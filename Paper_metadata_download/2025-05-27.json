[
    {
        "paper": {
            "id": "2505.17894",
            "authors": [
                {
                    "_id": "683577db7733c0f27e945847",
                    "user": {
                        "_id": "65276c7911a8a521c91bc10f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
                        "isPro": false,
                        "fullname": "Khalil Hennara",
                        "user": "Hennara",
                        "type": "user"
                    },
                    "name": "Khalil Hennara",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-27T09:23:49.563Z",
                    "hidden": false
                },
                {
                    "_id": "683577db7733c0f27e945848",
                    "user": {
                        "_id": "6496df4b3c64d75523a11973",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6496df4b3c64d75523a11973/I_Qn5-3Czngle-NsGmabO.jpeg",
                        "isPro": false,
                        "fullname": "Muhammad Hreden",
                        "user": "hr99",
                        "type": "user"
                    },
                    "name": "Muhammad Hreden",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T10:03:33.725Z",
                    "hidden": false
                },
                {
                    "_id": "683577db7733c0f27e945849",
                    "user": {
                        "_id": "63aa7667769a10efc404fbbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aa7667769a10efc404fbbc/tn8ZxUmTEMS0Gze7_F7JL.jpeg",
                        "isPro": false,
                        "fullname": "Mohamed Motasim Hamed",
                        "user": "Moatasem444",
                        "type": "user"
                    },
                    "name": "Mohamed Motaism Hamed",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-27T08:29:16.168Z",
                    "hidden": false
                },
                {
                    "_id": "683577db7733c0f27e94584a",
                    "user": {
                        "_id": "65704741e1cfce1764ce652e",
                        "avatarUrl": "/avatars/9189aaf417426af4ebe381ed364a6c0e.svg",
                        "isPro": false,
                        "fullname": "Zeina Aldallal",
                        "user": "ZeinaD",
                        "type": "user"
                    },
                    "name": "Zeina Aldallal",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-27T08:29:16.168Z",
                    "hidden": false
                },
                {
                    "_id": "683577db7733c0f27e94584b",
                    "user": {
                        "_id": "650b64fb443b2a59d4d432e3",
                        "avatarUrl": "/avatars/12f284e007e8b5cddeadf4491a0a1fe8.svg",
                        "isPro": false,
                        "fullname": "Sara Chrouf",
                        "user": "SaraChrouf",
                        "type": "user"
                    },
                    "name": "Sara Chrouf",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-27T15:45:02.476Z",
                    "hidden": false
                },
                {
                    "_id": "683577db7733c0f27e94584c",
                    "name": "Safwan AlModhayan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T13:42:21.000Z",
            "submittedOnDailyAt": "2025-05-27T07:07:04.517Z",
            "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a\n  Small Language Model",
            "submittedOnDailyBy": {
                "_id": "65276c7911a8a521c91bc10f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
                "isPro": false,
                "fullname": "Khalil Hennara",
                "user": "Hennara",
                "type": "user"
            },
            "summary": "We introduce Mutarjim, a compact yet powerful language model for\nbidirectional Arabic-English translation. While large-scale LLMs have shown\nimpressive progress in natural language processing tasks, including machine\ntranslation, smaller models. Leveraging this insight, we developed Mutarjim\nbased on Kuwain-1.5B , a language model tailored for both Arabic and English.\nDespite its modest size, Mutarjim outperforms much larger models on several\nestablished benchmarks, achieved through an optimized two-phase training\napproach and a carefully curated, high-quality training corpus.. Experimental\nresults show that Mutarjim rivals models up to 20 times larger while\nsignificantly reducing computational costs and training requirements. We also\nintroduce Tarjama-25, a new benchmark designed to overcome limitations in\nexisting Arabic-English benchmarking datasets, such as domain narrowness, short\nsentence lengths, and English-source bias. Tarjama-25 comprises 5,000\nexpert-reviewed sentence pairs and spans a wide range of domains, offering a\nmore comprehensive and balanced evaluation framework. Notably, Mutarjim\nachieves state-of-the-art performance on the English-to-Arabic task in\nTarjama-25, surpassing even significantly larger and proprietary models like\nGPT-4o mini. We publicly release Tarjama-25 to support future research and\nadvance the evaluation of Arabic-English translation systems.",
            "upvotes": 175,
            "discussionId": "683577dc7733c0f27e94588d",
            "ai_summary": "Mutarjim is a compact Arabic-English translation model that outperforms larger models on established benchmarks and achieves state-of-the-art performance on a new comprehensive Tarjama-25 benchmark.",
            "ai_keywords": [
                "language model",
                "bidirectional Arabic-English translation",
                "LLMs",
                "Kuwain-1.5B",
                "two-phase training",
                "high-quality training corpus",
                "Tarjama-25",
                "domain narrowness",
                "English-source bias",
                "GPT-4"
            ]
        },
        "publishedAt": "2025-05-23T09:42:21.000Z",
        "title": "Mutarjim: Advancing Bidirectional Arabic-English Translation with a\n  Small Language Model",
        "summary": "We introduce Mutarjim, a compact yet powerful language model for\nbidirectional Arabic-English translation. While large-scale LLMs have shown\nimpressive progress in natural language processing tasks, including machine\ntranslation, smaller models. Leveraging this insight, we developed Mutarjim\nbased on Kuwain-1.5B , a language model tailored for both Arabic and English.\nDespite its modest size, Mutarjim outperforms much larger models on several\nestablished benchmarks, achieved through an optimized two-phase training\napproach and a carefully curated, high-quality training corpus.. Experimental\nresults show that Mutarjim rivals models up to 20 times larger while\nsignificantly reducing computational costs and training requirements. We also\nintroduce Tarjama-25, a new benchmark designed to overcome limitations in\nexisting Arabic-English benchmarking datasets, such as domain narrowness, short\nsentence lengths, and English-source bias. Tarjama-25 comprises 5,000\nexpert-reviewed sentence pairs and spans a wide range of domains, offering a\nmore comprehensive and balanced evaluation framework. Notably, Mutarjim\nachieves state-of-the-art performance on the English-to-Arabic task in\nTarjama-25, surpassing even significantly larger and proprietary models like\nGPT-4o mini. We publicly release Tarjama-25 to support future research and\nadvance the evaluation of Arabic-English translation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17894.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "65276c7911a8a521c91bc10f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65276c7911a8a521c91bc10f/39dbuUtqQTJERTJ_WkxSh.jpeg",
            "fullname": "Khalil Hennara",
            "name": "Hennara",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19147",
            "authors": [
                {
                    "_id": "68353258d005e45149d2d384",
                    "user": {
                        "_id": "66a0caa1a7a6ed88ad1c0ddf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66a0caa1a7a6ed88ad1c0ddf/WoOP24-ruuHy4ryNhRp0D.jpeg",
                        "isPro": false,
                        "fullname": "Xuyang Liu",
                        "user": "xuyang-liu16",
                        "type": "user"
                    },
                    "name": "Xuyang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T08:23:05.932Z",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d385",
                    "user": {
                        "_id": "653b8c3e97a4d71d950e2f20",
                        "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
                        "isPro": false,
                        "fullname": "Zichen Wen",
                        "user": "zichenwen",
                        "type": "user"
                    },
                    "name": "Zichen Wen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:50:45.710Z",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d386",
                    "user": {
                        "_id": "66968099c952e09a4cb29f78",
                        "avatarUrl": "/avatars/bd3a361fe5315e26e9ae328071704eed.svg",
                        "isPro": false,
                        "fullname": "Wang",
                        "user": "Steven-Shaobo",
                        "type": "user"
                    },
                    "name": "Shaobo Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:50:41.853Z",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d387",
                    "user": {
                        "_id": "652f8642338c761caf474169",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/mq5jjqqNaFxVboWGDEocJ.jpeg",
                        "isPro": false,
                        "fullname": "Junjie Chen",
                        "user": "coderchen01",
                        "type": "user"
                    },
                    "name": "Junjie Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:50:48.148Z",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d388",
                    "user": {
                        "_id": "679f280ffb07d74f084520b6",
                        "avatarUrl": "/avatars/b378000f68c7faf8d4fee8074dd2db5b.svg",
                        "isPro": false,
                        "fullname": "Zhishan Tao",
                        "user": "Pppeach33",
                        "type": "user"
                    },
                    "name": "Zhishan Tao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:50:43.758Z",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d389",
                    "name": "Yubo Wang",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d38a",
                    "user": {
                        "_id": "64abcbfde144ba0eb9bb8419",
                        "avatarUrl": "/avatars/6ccea0e755bad384aaabd5c455bd962e.svg",
                        "isPro": false,
                        "fullname": "Xiangqi Jin",
                        "user": "Lueci4er",
                        "type": "user"
                    },
                    "name": "Xiangqi Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:21:21.961Z",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d38b",
                    "name": "Chang Zou",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d38c",
                    "name": "Yiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d38d",
                    "name": "Chenfei Liao",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d38e",
                    "name": "Xu Zheng",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d38f",
                    "name": "Honggang Chen",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d390",
                    "name": "Weijia Li",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d391",
                    "name": "Xuming Hu",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d392",
                    "user": {
                        "_id": "63f9fca8d4349b157a109eec",
                        "avatarUrl": "/avatars/fa1f2ae7972d7cde99dab178136ccbb0.svg",
                        "isPro": false,
                        "fullname": "Conghui He",
                        "user": "conghui",
                        "type": "user"
                    },
                    "name": "Conghui He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:22:15.329Z",
                    "hidden": false
                },
                {
                    "_id": "68353258d005e45149d2d393",
                    "user": {
                        "_id": "642ec9831d1737803dc1c30a",
                        "avatarUrl": "/avatars/c9ded838bad09004c15a27200e66a108.svg",
                        "isPro": false,
                        "fullname": "linfeng zhang",
                        "user": "linfengZ",
                        "type": "user"
                    },
                    "name": "Linfeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:22:07.787Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T13:51:17.000Z",
            "submittedOnDailyAt": "2025-05-27T02:06:05.849Z",
            "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
            "submittedOnDailyBy": {
                "_id": "653b8c3e97a4d71d950e2f20",
                "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
                "isPro": false,
                "fullname": "Zichen Wen",
                "user": "zichenwen",
                "type": "user"
            },
            "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\nwe argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement.",
            "upvotes": 124,
            "discussionId": "68353259d005e45149d2d3c0",
            "projectPage": "https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression",
            "githubRepo": "https://github.com/xuyang-liu16/Awesome-Token-level-Model-Compression",
            "ai_summary": "The focus in AI research is shifting from model-centric to data-centric compression, with token compression identified as key to improving efficiency in handling long-context scenarios.",
            "ai_keywords": [
                "large language models",
                "multi-modal LLMs",
                "self-attention",
                "token compression",
                "long-context AI",
                "mathematical framework",
                "model efficiency",
                "long-context overhead",
                "current challenges",
                "future directions"
            ]
        },
        "publishedAt": "2025-05-25T09:51:17.000Z",
        "title": "Shifting AI Efficiency From Model-Centric to Data-Centric Compression",
        "summary": "The rapid advancement of large language models (LLMs) and multi-modal LLMs\n(MLLMs) has historically relied on model-centric scaling through increasing\nparameter counts from millions to hundreds of billions to drive performance\ngains. However, as we approach hardware limits on model size, the dominant\ncomputational bottleneck has fundamentally shifted to the quadratic cost of\nself-attention over long token sequences, now driven by ultra-long text\ncontexts, high-resolution images, and extended videos. In this position paper,\nwe argue that the focus of research for efficient AI is shifting from\nmodel-centric compression to data-centric compression. We position token\ncompression as the new frontier, which improves AI efficiency via reducing the\nnumber of tokens during model training or inference. Through comprehensive\nanalysis, we first examine recent developments in long-context AI across\nvarious domains and establish a unified mathematical framework for existing\nmodel efficiency strategies, demonstrating why token compression represents a\ncrucial paradigm shift in addressing long-context overhead. Subsequently, we\nsystematically review the research landscape of token compression, analyzing\nits fundamental benefits and identifying its compelling advantages across\ndiverse scenarios. Furthermore, we provide an in-depth analysis of current\nchallenges in token compression research and outline promising future\ndirections. Ultimately, our work aims to offer a fresh perspective on AI\nefficiency, synthesize existing research, and catalyze innovative developments\nto address the challenges that increasing context lengths pose to the AI\ncommunity's advancement.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19147.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "653b8c3e97a4d71d950e2f20",
            "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
            "fullname": "Zichen Wen",
            "name": "zichenwen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19297",
            "authors": [
                {
                    "_id": "68354c05f7b44d5d505262c7",
                    "user": {
                        "_id": "63725a2eacef705233c62876",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63725a2eacef705233c62876/QlRm8oq7O8THzUhATYQlH.jpeg",
                        "isPro": false,
                        "fullname": "Valerii",
                        "user": "sharfikeg",
                        "type": "user"
                    },
                    "name": "Valerii Startsev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:51:23.082Z",
                    "hidden": false
                },
                {
                    "_id": "68354c05f7b44d5d505262c8",
                    "user": {
                        "_id": "682605d2a677fa26bd17440a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/682605d2a677fa26bd17440a/QiqSKhYiz7XIfKaXHcEX2.png",
                        "isPro": false,
                        "fullname": "Alexander Ustyuzhanin",
                        "user": "a-ustyuzhanin",
                        "type": "user"
                    },
                    "name": "Alexander Ustyuzhanin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:01:13.393Z",
                    "hidden": false
                },
                {
                    "_id": "68354c05f7b44d5d505262c9",
                    "user": {
                        "_id": "643322f86c2a26ae66d4eb66",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643322f86c2a26ae66d4eb66/niMw4prKr2pK9xdWlcWLt.jpeg",
                        "isPro": false,
                        "fullname": "Alexey",
                        "user": "alexkkir",
                        "type": "user"
                    },
                    "name": "Alexey Kirillov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T15:50:00.349Z",
                    "hidden": false
                },
                {
                    "_id": "68354c05f7b44d5d505262ca",
                    "user": {
                        "_id": "62b6cc49752323892323bc04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b6cc49752323892323bc04/gGBld1KJIP9AIpd81L3PC.jpeg",
                        "isPro": true,
                        "fullname": "Dmitry Baranchuk",
                        "user": "dbaranchuk",
                        "type": "user"
                    },
                    "name": "Dmitry Baranchuk",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:01:25.481Z",
                    "hidden": false
                },
                {
                    "_id": "68354c05f7b44d5d505262cb",
                    "user": {
                        "_id": "64aeb6aa59d35c5f8180ba7c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64aeb6aa59d35c5f8180ba7c/udqEsHBhzjZl_ShvgjD5D.jpeg",
                        "isPro": false,
                        "fullname": "Sergey Kastryulin",
                        "user": "snk4tr",
                        "type": "user"
                    },
                    "name": "Sergey Kastryulin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:01:31.712Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T20:08:20.000Z",
            "submittedOnDailyAt": "2025-05-27T07:55:09.983Z",
            "title": "Alchemist: Turning Public Text-to-Image Data into Generative Gold",
            "submittedOnDailyBy": {
                "_id": "63725a2eacef705233c62876",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63725a2eacef705233c62876/QlRm8oq7O8THzUhATYQlH.jpeg",
                "isPro": false,
                "fullname": "Valerii",
                "user": "sharfikeg",
                "type": "user"
            },
            "summary": "Pre-training equips text-to-image (T2I) models with broad world knowledge,\nbut this alone is often insufficient to achieve high aesthetic quality and\nalignment. Consequently, supervised fine-tuning (SFT) is crucial for further\nrefinement. However, its effectiveness highly depends on the quality of the\nfine-tuning dataset. Existing public SFT datasets frequently target narrow\ndomains (e.g., anime or specific art styles), and the creation of high-quality,\ngeneral-purpose SFT datasets remains a significant challenge. Current curation\nmethods are often costly and struggle to identify truly impactful samples. This\nchallenge is further complicated by the scarcity of public general-purpose\ndatasets, as leading models often rely on large, proprietary, and poorly\ndocumented internal data, hindering broader research progress. This paper\nintroduces a novel methodology for creating general-purpose SFT datasets by\nleveraging a pre-trained generative model as an estimator of high-impact\ntraining samples. We apply this methodology to construct and release Alchemist,\na compact (3,350 samples) yet highly effective SFT dataset. Experiments\ndemonstrate that Alchemist substantially improves the generative quality of\nfive public T2I models while preserving diversity and style. Additionally, we\nrelease the fine-tuned models' weights to the public.",
            "upvotes": 57,
            "discussionId": "68354c07f7b44d5d50526322",
            "ai_summary": "A new method using a pre-trained generative model helps construct a high-impact SFT dataset, Alchemist, which improves the generative quality of text-to-image models while maintaining diversity.",
            "ai_keywords": [
                "text-to-image",
                "fine-tuning",
                "pre-trained generative model",
                "general-purpose datasets",
                "aesthetic quality",
                "alignment",
                "curated datasets"
            ]
        },
        "publishedAt": "2025-05-25T16:08:20.000Z",
        "title": "Alchemist: Turning Public Text-to-Image Data into Generative Gold",
        "summary": "Pre-training equips text-to-image (T2I) models with broad world knowledge,\nbut this alone is often insufficient to achieve high aesthetic quality and\nalignment. Consequently, supervised fine-tuning (SFT) is crucial for further\nrefinement. However, its effectiveness highly depends on the quality of the\nfine-tuning dataset. Existing public SFT datasets frequently target narrow\ndomains (e.g., anime or specific art styles), and the creation of high-quality,\ngeneral-purpose SFT datasets remains a significant challenge. Current curation\nmethods are often costly and struggle to identify truly impactful samples. This\nchallenge is further complicated by the scarcity of public general-purpose\ndatasets, as leading models often rely on large, proprietary, and poorly\ndocumented internal data, hindering broader research progress. This paper\nintroduces a novel methodology for creating general-purpose SFT datasets by\nleveraging a pre-trained generative model as an estimator of high-impact\ntraining samples. We apply this methodology to construct and release Alchemist,\na compact (3,350 samples) yet highly effective SFT dataset. Experiments\ndemonstrate that Alchemist substantially improves the generative quality of\nfive public T2I models while preserving diversity and style. Additionally, we\nrelease the fine-tuned models' weights to the public.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19297.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63725a2eacef705233c62876",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63725a2eacef705233c62876/QlRm8oq7O8THzUhATYQlH.jpeg",
            "fullname": "Valerii",
            "name": "sharfikeg",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19457",
            "authors": [
                {
                    "_id": "683536ec70d215849adfc236",
                    "user": {
                        "_id": "6440f70f1a80f6d83cadfd16",
                        "avatarUrl": "/avatars/04790922837dac81747e80bd0ee0a1cf.svg",
                        "isPro": false,
                        "fullname": "luguilong",
                        "user": "guilong",
                        "type": "user"
                    },
                    "name": "Guilong Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:23:29.797Z",
                    "hidden": false
                },
                {
                    "_id": "683536ec70d215849adfc237",
                    "user": {
                        "_id": "672b138db4215fd3888e0a8f",
                        "avatarUrl": "/avatars/e90fe671a1db66401db88429fae9a763.svg",
                        "isPro": false,
                        "fullname": "guo",
                        "user": "xuntao",
                        "type": "user"
                    },
                    "name": "Xuntao Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:23:39.616Z",
                    "hidden": false
                },
                {
                    "_id": "683536ec70d215849adfc238",
                    "user": {
                        "_id": "6555df426947208b7741b637",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6555df426947208b7741b637/b7ply-HyaPKXrPvRNh21K.jpeg",
                        "isPro": false,
                        "fullname": "Rongjunchen Zhang",
                        "user": "Tinker250",
                        "type": "user"
                    },
                    "name": "Rongjunchen Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-27T03:52:17.018Z",
                    "hidden": false
                },
                {
                    "_id": "683536ec70d215849adfc239",
                    "user": {
                        "_id": "648add6aff6123185eb185a8",
                        "avatarUrl": "/avatars/e37dfa680c1bb86c721165f03eb79e97.svg",
                        "isPro": false,
                        "fullname": "WNQzhu",
                        "user": "Qlisp",
                        "type": "user"
                    },
                    "name": "Wenqiao Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:49:28.216Z",
                    "hidden": false
                },
                {
                    "_id": "683536ec70d215849adfc23a",
                    "name": "Ji Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/48jI0LlYjRwO4-0kHRV0V.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/atuM30TNh72kJtm8zGxoc.png"
            ],
            "publishedAt": "2025-05-26T03:23:02.000Z",
            "submittedOnDailyAt": "2025-05-27T02:28:08.336Z",
            "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for\n  Evaluating LLMs",
            "submittedOnDailyBy": {
                "_id": "6555df426947208b7741b637",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6555df426947208b7741b637/b7ply-HyaPKXrPvRNh21K.jpeg",
                "isPro": false,
                "fullname": "Rongjunchen Zhang",
                "user": "Tinker250",
                "type": "user"
            },
            "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench.",
            "upvotes": 55,
            "discussionId": "683536f170d215849adfc35e",
            "projectPage": "https://hithink-research.github.io/BizFinBench/",
            "githubRepo": "https://github.com/HiThink-Research/BizFinBench",
            "ai_summary": "BizFinBench is a benchmark for evaluating large language models in financial applications, revealing distinct performance patterns across various tasks.",
            "ai_keywords": [
                "large language models",
                "BizFinBench",
                "numerical calculation",
                "reasoning",
                "information extraction",
                "prediction recognition",
                "knowledge-based question answering",
                "IteraJudge",
                "Claude-3.5-Sonnet",
                "DeepSeek-R1",
                "Qwen2.5-VL-3B",
                "ChatGPT-o3",
                "Gemini-2.0-Flash",
                "Qwen3-1.7B"
            ]
        },
        "publishedAt": "2025-05-25T23:23:02.000Z",
        "title": "BizFinBench: A Business-Driven Real-World Financial Benchmark for\n  Evaluating LLMs",
        "summary": "Large language models excel in general tasks, yet assessing their reliability\nin logic-heavy, precision-critical domains like finance, law, and healthcare\nremains challenging. To address this, we introduce BizFinBench, the first\nbenchmark specifically designed to evaluate LLMs in real-world financial\napplications. BizFinBench consists of 6,781 well-annotated queries in Chinese,\nspanning five dimensions: numerical calculation, reasoning, information\nextraction, prediction recognition, and knowledge-based question answering,\ngrouped into nine fine-grained categories. The benchmark includes both\nobjective and subjective metrics. We also introduce IteraJudge, a novel LLM\nevaluation method that reduces bias when LLMs serve as evaluators in objective\nmetrics. We benchmark 25 models, including both proprietary and open-source\nsystems. Extensive experiments show that no model dominates across all tasks.\nOur evaluation reveals distinct capability patterns: (1) In Numerical\nCalculation, Claude-3.5-Sonnet (63.18) and DeepSeek-R1 (64.04) lead, while\nsmaller models like Qwen2.5-VL-3B (15.92) lag significantly; (2) In Reasoning,\nproprietary models dominate (ChatGPT-o3: 83.58, Gemini-2.0-Flash: 81.15), with\nopen-source models trailing by up to 19.49 points; (3) In Information\nExtraction, the performance spread is the largest, with DeepSeek-R1 scoring\n71.46, while Qwen3-1.7B scores 11.23; (4) In Prediction Recognition,\nperformance variance is minimal, with top models scoring between 39.16 and\n50.00. We find that while current LLMs handle routine finance queries\ncompetently, they struggle with complex scenarios requiring cross-concept\nreasoning. BizFinBench offers a rigorous, business-aligned benchmark for future\nresearch. The code and dataset are available at\nhttps://github.com/HiThink-Research/BizFinBench.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/48jI0LlYjRwO4-0kHRV0V.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6555df426947208b7741b637/atuM30TNh72kJtm8zGxoc.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19457.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6555df426947208b7741b637",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6555df426947208b7741b637/b7ply-HyaPKXrPvRNh21K.jpeg",
            "fullname": "Rongjunchen Zhang",
            "name": "Tinker250",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19250",
            "authors": [
                {
                    "_id": "68355ce06a9c239ada09f97b",
                    "user": {
                        "_id": "66e0404662d6ab4f1107580f",
                        "avatarUrl": "/avatars/ef71694fea5482078a637a3869e30d19.svg",
                        "isPro": false,
                        "fullname": "Yi Wang",
                        "user": "Yi53",
                        "type": "user"
                    },
                    "name": "Yi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:49:06.169Z",
                    "hidden": false
                },
                {
                    "_id": "68355ce06a9c239ada09f97c",
                    "user": {
                        "_id": "68356f5db243fb809813a715",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68356f5db243fb809813a715/grhHvANfDRp75rMJxWlQo.jpeg",
                        "isPro": false,
                        "fullname": "LiuJunxiao",
                        "user": "master-lan",
                        "type": "user"
                    },
                    "name": "Junxiao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:55:01.261Z",
                    "hidden": false
                },
                {
                    "_id": "68355ce06a9c239ada09f97d",
                    "user": {
                        "_id": "65080dc63fc966d1bbba485d",
                        "avatarUrl": "/avatars/347890233f2316e7f7a04d652b2378bb.svg",
                        "isPro": false,
                        "fullname": "Shimao Zhang",
                        "user": "Shimao-Zhang",
                        "type": "user"
                    },
                    "name": "Shimao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:34:46.475Z",
                    "hidden": false
                },
                {
                    "_id": "68355ce06a9c239ada09f97e",
                    "name": "Jiajun Chen",
                    "hidden": false
                },
                {
                    "_id": "68355ce06a9c239ada09f97f",
                    "name": "Shujian Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T17:58:50.000Z",
            "submittedOnDailyAt": "2025-05-27T07:16:47.391Z",
            "title": "PATS: Process-Level Adaptive Thinking Mode Switching",
            "submittedOnDailyBy": {
                "_id": "66e0404662d6ab4f1107580f",
                "avatarUrl": "/avatars/ef71694fea5482078a637a3869e30d19.svg",
                "isPro": false,
                "fullname": "Yi Wang",
                "user": "Yi53",
                "type": "user"
            },
            "summary": "Current large-language models (LLMs) typically adopt a fixed reasoning\nstrategy, either simple or complex, for all questions, regardless of their\ndifficulty. This neglect of variation in task and reasoning process complexity\nleads to an imbalance between performance and efficiency. Existing methods\nattempt to implement training-free fast-slow thinking system switching to\nhandle problems of varying difficulty, but are limited by coarse-grained\nsolution-level strategy adjustments. To address this issue, we propose a novel\nreasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),\nwhich enables LLMs to dynamically adjust their reasoning strategy based on the\ndifficulty of each step, optimizing the balance between accuracy and\ncomputational efficiency. Our approach integrates Process Reward Models (PRMs)\nwith Beam Search, incorporating progressive mode switching and bad-step penalty\nmechanisms. Experiments on diverse mathematical benchmarks demonstrate that our\nmethodology achieves high accuracy while maintaining moderate token usage. This\nstudy emphasizes the significance of process-level, difficulty-aware reasoning\nstrategy adaptation, offering valuable insights into efficient inference for\nLLMs.",
            "upvotes": 43,
            "discussionId": "68355ce16a9c239ada09f9a9",
            "githubRepo": "https://github.com/NJUNLP/PATS",
            "ai_summary": "PATS enhances LLM efficiency by dynamically adjusting reasoning strategies based on task difficulty, leveraging PRMs and Beam Search.",
            "ai_keywords": [
                "large-language models (LLMs)",
                "reasoning strategy",
                "task and reasoning process complexity",
                "training-free fast-slow thinking system switching",
                "Process-Level Adaptive Thinking Mode Switching (PATS)",
                "Process Reward Models (PRMs)",
                "Beam Search",
                "progressive mode switching",
                "bad-step penalty mechanisms",
                "mathematical benchmarks",
                "process-level",
                "difficulty-aware reasoning strategy adaptation"
            ]
        },
        "publishedAt": "2025-05-25T13:58:50.000Z",
        "title": "PATS: Process-Level Adaptive Thinking Mode Switching",
        "summary": "Current large-language models (LLMs) typically adopt a fixed reasoning\nstrategy, either simple or complex, for all questions, regardless of their\ndifficulty. This neglect of variation in task and reasoning process complexity\nleads to an imbalance between performance and efficiency. Existing methods\nattempt to implement training-free fast-slow thinking system switching to\nhandle problems of varying difficulty, but are limited by coarse-grained\nsolution-level strategy adjustments. To address this issue, we propose a novel\nreasoning paradigm: Process-Level Adaptive Thinking Mode Switching (PATS),\nwhich enables LLMs to dynamically adjust their reasoning strategy based on the\ndifficulty of each step, optimizing the balance between accuracy and\ncomputational efficiency. Our approach integrates Process Reward Models (PRMs)\nwith Beam Search, incorporating progressive mode switching and bad-step penalty\nmechanisms. Experiments on diverse mathematical benchmarks demonstrate that our\nmethodology achieves high accuracy while maintaining moderate token usage. This\nstudy emphasizes the significance of process-level, difficulty-aware reasoning\nstrategy adaptation, offering valuable insights into efficient inference for\nLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19250.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66e0404662d6ab4f1107580f",
            "avatarUrl": "/avatars/ef71694fea5482078a637a3869e30d19.svg",
            "fullname": "Yi Wang",
            "name": "Yi53",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16348",
            "authors": [
                {
                    "_id": "6835365d2925bc8bb23a57c7",
                    "user": {
                        "_id": "636b529ef796304dd67d139c",
                        "avatarUrl": "/avatars/7a64d5095fcb1da558b52ad48177ad76.svg",
                        "isPro": false,
                        "fullname": "Taeyoon Kwon",
                        "user": "Connoriginal",
                        "type": "user"
                    },
                    "name": "Taeyoon Kwon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:50:26.210Z",
                    "hidden": false
                },
                {
                    "_id": "6835365d2925bc8bb23a57c8",
                    "name": "Dongwook Choi",
                    "hidden": false
                },
                {
                    "_id": "6835365d2925bc8bb23a57c9",
                    "name": "Sunghwan Kim",
                    "hidden": false
                },
                {
                    "_id": "6835365d2925bc8bb23a57ca",
                    "name": "Hyojun Kim",
                    "hidden": false
                },
                {
                    "_id": "6835365d2925bc8bb23a57cb",
                    "user": {
                        "_id": "6420f4f55bccaa42484496e5",
                        "avatarUrl": "/avatars/4996ba26955f8423c946b1ecd3989964.svg",
                        "isPro": false,
                        "fullname": "Seung Jun Moon",
                        "user": "Lune-Blue",
                        "type": "user"
                    },
                    "name": "Seungjun Moon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:24:40.306Z",
                    "hidden": false
                },
                {
                    "_id": "6835365d2925bc8bb23a57cc",
                    "user": {
                        "_id": "64b72a408ba7d6c922c73054",
                        "avatarUrl": "/avatars/6d9797430bc36f05fb950b84aa6a9374.svg",
                        "isPro": false,
                        "fullname": "Beong Woo Kwak",
                        "user": "bwookwak",
                        "type": "user"
                    },
                    "name": "Beong-woo Kwak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:24:46.911Z",
                    "hidden": false
                },
                {
                    "_id": "6835365d2925bc8bb23a57cd",
                    "user": {
                        "_id": "658a57b4126b8d7eae07b983",
                        "avatarUrl": "/avatars/8d908cb3da697793564d24206a333782.svg",
                        "isPro": false,
                        "fullname": "Kuan-Hao Huang",
                        "user": "ej0cl6",
                        "type": "user"
                    },
                    "name": "Kuan-Hao Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:24:53.502Z",
                    "hidden": false
                },
                {
                    "_id": "6835365d2925bc8bb23a57ce",
                    "user": {
                        "_id": "682e91865fa8c5df85b3d8e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XWTfZoOjCdMnqaFEBBYWe.png",
                        "isPro": false,
                        "fullname": "Jinyoung Yeo",
                        "user": "jinyeo",
                        "type": "user"
                    },
                    "name": "Jinyoung Yeo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:25:01.610Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T08:00:10.000Z",
            "submittedOnDailyAt": "2025-05-27T02:20:10.067Z",
            "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for\n  Personalized Assistance",
            "submittedOnDailyBy": {
                "_id": "636b529ef796304dd67d139c",
                "avatarUrl": "/avatars/7a64d5095fcb1da558b52ad48177ad76.svg",
                "isPro": false,
                "fullname": "Taeyoon Kwon",
                "user": "Connoriginal",
                "type": "user"
            },
            "summary": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO",
            "upvotes": 42,
            "discussionId": "683536612925bc8bb23a58e1",
            "projectPage": "https://connoriginal.github.io/MEMENTO/",
            "githubRepo": "https://github.com/Connoriginal/MEMENTO",
            "ai_summary": "MEMENTO evaluates personalized memory utilization in embodied agents, revealing limitations in understanding user semantics and routines.",
            "ai_keywords": [
                "embodied agents",
                "large language models (LLMs)",
                "object rearrangement tasks",
                "user semantics",
                "prior interaction history",
                "memory utilization",
                "personalized assistance",
                "goal interpretation",
                "object semantics",
                "user patterns"
            ]
        },
        "publishedAt": "2025-05-22T04:00:10.000Z",
        "title": "Embodied Agents Meet Personalization: Exploring Memory Utilization for\n  Personalized Assistance",
        "summary": "Embodied agents empowered by large language models (LLMs) have shown strong\nperformance in household object rearrangement tasks. However, these tasks\nprimarily focus on single-turn interactions with simplified instructions, which\ndo not truly reflect the challenges of providing meaningful assistance to\nusers. To provide personalized assistance, embodied agents must understand the\nunique semantics that users assign to the physical world (e.g., favorite cup,\nbreakfast routine) by leveraging prior interaction history to interpret\ndynamic, real-world instructions. Yet, the effectiveness of embodied agents in\nutilizing memory for personalized assistance remains largely underexplored. To\naddress this gap, we present MEMENTO, a personalized embodied agent evaluation\nframework designed to comprehensively assess memory utilization capabilities to\nprovide personalized assistance. Our framework consists of a two-stage memory\nevaluation process design that enables quantifying the impact of memory\nutilization on task performance. This process enables the evaluation of agents'\nunderstanding of personalized knowledge in object rearrangement tasks by\nfocusing on its role in goal interpretation: (1) the ability to identify target\nobjects based on personal meaning (object semantics), and (2) the ability to\ninfer object-location configurations from consistent user patterns, such as\nroutines (user patterns). Our experiments across various LLMs reveal\nsignificant limitations in memory utilization, with even frontier models like\nGPT-4o experiencing a 30.5% performance drop when required to reference\nmultiple memories, particularly in tasks involving user patterns. These\nfindings, along with our detailed analyses and case studies, provide valuable\ninsights for future research in developing more effective personalized embodied\nagents. Project website: https://connoriginal.github.io/MEMENTO",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16348.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636b529ef796304dd67d139c",
            "avatarUrl": "/avatars/7a64d5095fcb1da558b52ad48177ad76.svg",
            "fullname": "Taeyoon Kwon",
            "name": "Connoriginal",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19914",
            "authors": [
                {
                    "_id": "68353e41f995630ab88c198b",
                    "user": {
                        "_id": "606ed1884ffe81d1e03e81e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1639375346654-606ed1884ffe81d1e03e81e5.png",
                        "isPro": false,
                        "fullname": "Jiangjie Chen",
                        "user": "jiangjiechen",
                        "type": "user"
                    },
                    "name": "Jiangjie Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:49:21.006Z",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c198c",
                    "user": {
                        "_id": "636b36351340f879a2ec2bb1",
                        "avatarUrl": "/avatars/260a1c15f9c14c967125469072020946.svg",
                        "isPro": false,
                        "fullname": "QianyuHe",
                        "user": "Abbey4799",
                        "type": "user"
                    },
                    "name": "Qianyu He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:49:23.290Z",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c198d",
                    "user": {
                        "_id": "62d62b333bf5e059f7d2b286",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg",
                        "isPro": false,
                        "fullname": "Siyu Yuan",
                        "user": "siyuyuan",
                        "type": "user"
                    },
                    "name": "Siyu Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T10:14:02.514Z",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c198e",
                    "name": "Aili Chen",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c198f",
                    "name": "Zhicheng Cai",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c1990",
                    "name": "Weinan Dai",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c1991",
                    "name": "Hongli Yu",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c1992",
                    "name": "Qiying Yu",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c1993",
                    "name": "Xuefeng Li",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c1994",
                    "name": "Jiaze Chen",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c1995",
                    "name": "Hao Zhou",
                    "hidden": false
                },
                {
                    "_id": "68353e41f995630ab88c1996",
                    "name": "Mingxuan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T12:40:31.000Z",
            "submittedOnDailyAt": "2025-05-27T02:57:13.989Z",
            "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with\n  Synthetic Verifiable Puzzles",
            "submittedOnDailyBy": {
                "_id": "62d62b333bf5e059f7d2b286",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg",
                "isPro": false,
                "fullname": "Siyu Yuan",
                "user": "siyuyuan",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.",
            "upvotes": 33,
            "discussionId": "68353e42f995630ab88c19dc",
            "ai_summary": "Enigmata is a comprehensive suite for improving LLMs in puzzle reasoning through scalable multi-task RL training, leading to better performance on benchmarks and advanced math tasks.",
            "ai_keywords": [
                "Large Language Models",
                "OpenAI's o1",
                "DeepSeek's R1",
                "Reinforcement Learning with Verifiable Rewards",
                "Enigmata",
                "Enigmata-Eval",
                "multi-task RL training",
                "puzzle reasoning",
                "rule-based verifier",
                "ARC-AGI",
                "Qwen2.5-32B-Enigmata",
                "Seed1.5-Thinking",
                "AIME",
                "BeyondAIME",
                "GPQA"
            ]
        },
        "publishedAt": "2025-05-26T08:40:31.000Z",
        "title": "Enigmata: Scaling Logical Reasoning in Large Language Models with\n  Synthetic Verifiable Puzzles",
        "summary": "Large Language Models (LLMs), such as OpenAI's o1 and DeepSeek's R1, excel at\nadvanced reasoning tasks like math and coding via Reinforcement Learning with\nVerifiable Rewards (RLVR), but still struggle with puzzles solvable by humans\nwithout domain knowledge. We introduce Enigmata, the first comprehensive suite\ntailored for improving LLMs with puzzle reasoning skills. It includes 36 tasks\nacross seven categories, each with 1) a generator that produces unlimited\nexamples with controllable difficulty and 2) a rule-based verifier for\nautomatic evaluation. This generator-verifier design supports scalable,\nmulti-task RL training, fine-grained analysis, and seamless RLVR integration.\nWe further propose Enigmata-Eval, a rigorous benchmark, and develop optimized\nmulti-task RLVR strategies. Our trained model, Qwen2.5-32B-Enigmata,\nconsistently surpasses o3-mini-high and o1 on the puzzle reasoning benchmarks\nlike Enigmata-Eval, ARC-AGI (32.8%), and ARC-AGI 2 (0.6%). It also generalizes\nwell to out-of-domain puzzle benchmarks and mathematical reasoning, with little\nmulti-tasking trade-off. When trained on larger models like Seed1.5-Thinking\n(20B activated parameters and 200B total parameters), puzzle data from Enigmata\nfurther boosts SoTA performance on advanced math and STEM reasoning tasks such\nas AIME (2024-2025), BeyondAIME and GPQA (Diamond), showing nice generalization\nbenefits of Enigmata. This work offers a unified, controllable framework for\nadvancing logical reasoning in LLMs. Resources of this work can be found at\nhttps://seed-enigmata.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19914.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62d62b333bf5e059f7d2b286",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1668513815771-62d62b333bf5e059f7d2b286.jpeg",
            "fullname": "Siyu Yuan",
            "name": "siyuyuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20259",
            "authors": [
                {
                    "_id": "6835346b2fdc5f8e8ea1e3cf",
                    "name": "Haoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "6835346b2fdc5f8e8ea1e3d0",
                    "name": "Zeyu Qin",
                    "hidden": false
                },
                {
                    "_id": "6835346b2fdc5f8e8ea1e3d1",
                    "name": "Yifei Zhao",
                    "hidden": false
                },
                {
                    "_id": "6835346b2fdc5f8e8ea1e3d2",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "6835346b2fdc5f8e8ea1e3d3",
                    "name": "Min Lin",
                    "hidden": false
                },
                {
                    "_id": "6835346b2fdc5f8e8ea1e3d4",
                    "name": "Xueqian Wang",
                    "hidden": false
                },
                {
                    "_id": "6835346b2fdc5f8e8ea1e3d5",
                    "user": {
                        "_id": "63d91b6d255ef6add20e1b38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                        "isPro": false,
                        "fullname": "Tianyu Pang",
                        "user": "P2333",
                        "type": "user"
                    },
                    "name": "Tianyu Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:31:57.554Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:40:40.000Z",
            "submittedOnDailyAt": "2025-05-27T02:13:04.341Z",
            "title": "Lifelong Safety Alignment for Language Models",
            "submittedOnDailyBy": {
                "_id": "63d91b6d255ef6add20e1b38",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                "isPro": false,
                "fullname": "Tianyu Pang",
                "user": "P2333",
                "type": "user"
            },
            "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment.",
            "upvotes": 22,
            "discussionId": "6835346c2fdc5f8e8ea1e407",
            "githubRepo": "https://github.com/sail-sg/LifelongSafetyAlignment",
            "ai_summary": "A lifecycle safety alignment framework employs a Meta-Attacker and Defender to adapt LLMs to novel jailbreaking strategies, improving robustness in deployment.",
            "ai_keywords": [
                "LLMs",
                "jailbreaking attacks",
                "safety alignment",
                "lifelong safety alignment framework",
                "Meta-Attacker",
                "Defender",
                "GPT-4o API",
                "attack success rate",
                "transfer attack success rate",
                "single-turn attacks"
            ]
        },
        "publishedAt": "2025-05-26T13:40:40.000Z",
        "title": "Lifelong Safety Alignment for Language Models",
        "summary": "LLMs have made impressive progress, but their growing capabilities also\nexpose them to highly flexible jailbreaking attacks designed to bypass safety\nalignment. While many existing defenses focus on known types of attacks, it is\nmore critical to prepare LLMs for unseen attacks that may arise during\ndeployment. To address this, we propose a lifelong safety alignment framework\nthat enables LLMs to continuously adapt to new and evolving jailbreaking\nstrategies. Our framework introduces a competitive setup between two\ncomponents: a Meta-Attacker, trained to actively discover novel jailbreaking\nstrategies, and a Defender, trained to resist them. To effectively warm up the\nMeta-Attacker, we first leverage the GPT-4o API to extract key insights from a\nlarge collection of jailbreak-related research papers. Through iterative\ntraining, the first iteration Meta-Attacker achieves a 73% attack success rate\n(ASR) on RR and a 57% transfer ASR on LAT using only single-turn attacks.\nMeanwhile, the Defender progressively improves its robustness and ultimately\nreduces the Meta-Attacker's success rate to just 7%, enabling safer and more\nreliable deployment of LLMs in open-ended environments. The code is available\nat https://github.com/sail-sg/LifelongSafetyAlignment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20259.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63d91b6d255ef6add20e1b38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
            "fullname": "Tianyu Pang",
            "name": "P2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19439",
            "authors": [
                {
                    "_id": "68355784bb7d114755346770",
                    "user": {
                        "_id": "6835378ad0fbc64a8e718c45",
                        "avatarUrl": "/avatars/ab0553b3114cc4cfad9eaa034af3241d.svg",
                        "isPro": false,
                        "fullname": "xinrihui",
                        "user": "xinrihui",
                        "type": "user"
                    },
                    "name": "Rihui Xin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:01:54.708Z",
                    "hidden": false
                },
                {
                    "_id": "68355784bb7d114755346771",
                    "name": "Han Liu",
                    "hidden": false
                },
                {
                    "_id": "68355784bb7d114755346772",
                    "user": {
                        "_id": "67123ec5b832202bddc6e74a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/M87nMhWm9LXi0MqxODXVK.png",
                        "isPro": false,
                        "fullname": "Zecheng Wang",
                        "user": "wangzech",
                        "type": "user"
                    },
                    "name": "Zecheng Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:02:02.524Z",
                    "hidden": false
                },
                {
                    "_id": "68355784bb7d114755346773",
                    "name": "Yupeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68355784bb7d114755346774",
                    "user": {
                        "_id": "643e8b572263cdc630f80fee",
                        "avatarUrl": "/avatars/93b11c28a58c5db71afd44e782493140.svg",
                        "isPro": false,
                        "fullname": "Dianbo Sui",
                        "user": "suidianbo",
                        "type": "user"
                    },
                    "name": "Dianbo Sui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:02:22.250Z",
                    "hidden": false
                },
                {
                    "_id": "68355784bb7d114755346775",
                    "name": "Xiaolin Hu",
                    "hidden": false
                },
                {
                    "_id": "68355784bb7d114755346776",
                    "name": "Bingning Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T02:56:22.000Z",
            "submittedOnDailyAt": "2025-05-27T04:41:46.037Z",
            "title": "Surrogate Signals from Format and Length: Reinforcement Learning for\n  Solving Mathematical Problems without Ground Truth Answers",
            "submittedOnDailyBy": {
                "_id": "62e52483a944e2a56cd2c6ca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
                "isPro": false,
                "fullname": "Jiejun Tan",
                "user": "zstanjj",
                "type": "user"
            },
            "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses.",
            "upvotes": 20,
            "discussionId": "68355785bb7d1147553467b8",
            "ai_summary": "The research demonstrates that using format and length as surrogate signals can improve LLMs' performance in mathematical problem-solving, matching or surpassing traditional methods without extensive ground truth data.",
            "ai_keywords": [
                "Large Language Models",
                "Reinforcement Learning",
                "mathematical problem-solving",
                "GRPO algorithm",
                "format correctness",
                "length-based rewards",
                "AIME2024"
            ]
        },
        "publishedAt": "2025-05-25T22:56:22.000Z",
        "title": "Surrogate Signals from Format and Length: Reinforcement Learning for\n  Solving Mathematical Problems without Ground Truth Answers",
        "summary": "Large Language Models have achieved remarkable success in natural language\nprocessing tasks, with Reinforcement Learning playing a key role in adapting\nthem to specific applications. However, obtaining ground truth answers for\ntraining LLMs in mathematical problem-solving is often challenging, costly, and\nsometimes unfeasible. This research delves into the utilization of format and\nlength as surrogate signals to train LLMs for mathematical problem-solving,\nbypassing the need for traditional ground truth answers.Our study shows that a\nreward function centered on format correctness alone can yield performance\nimprovements comparable to the standard GRPO algorithm in early phases.\nRecognizing the limitations of format-only rewards in the later phases, we\nincorporate length-based rewards. The resulting GRPO approach, leveraging\nformat-length surrogate signals, not only matches but surpasses the performance\nof the standard GRPO algorithm relying on ground truth answers in certain\nscenarios, achieving 40.0\\% accuracy on AIME2024 with a 7B base model. Through\nsystematic exploration and experimentation, this research not only offers a\npractical solution for training LLMs to solve mathematical problems and\nreducing the dependence on extensive ground truth data collection, but also\nreveals the essence of why our label-free approach succeeds: base model is like\nan excellent student who has already mastered mathematical and logical\nreasoning skills, but performs poorly on the test paper, it simply needs to\ndevelop good answering habits to achieve outstanding results in exams , in\nother words, to unlock the capabilities it already possesses.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19439.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e52483a944e2a56cd2c6ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
            "fullname": "Jiejun Tan",
            "name": "zstanjj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20139",
            "authors": [
                {
                    "_id": "6835744884d4600675a4449c",
                    "name": "Jialin Yang",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a4449d",
                    "user": {
                        "_id": "62567c86d444a9b5a0ec51c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
                        "isPro": false,
                        "fullname": "Dongfu Jiang",
                        "user": "DongfuJiang",
                        "type": "user"
                    },
                    "name": "Dongfu Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:58:01.881Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a4449e",
                    "user": {
                        "_id": "6700154f223c62ec8845e3f9",
                        "avatarUrl": "/avatars/3327214709785440fee68f8df641ad84.svg",
                        "isPro": false,
                        "fullname": "he",
                        "user": "lipenghe",
                        "type": "user"
                    },
                    "name": "Lipeng He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:57:55.410Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a4449f",
                    "user": {
                        "_id": "630d73387dacb93b335b00e9",
                        "avatarUrl": "/avatars/2ded3550aedf77a58175a66319d01820.svg",
                        "isPro": false,
                        "fullname": "Sherman Siu",
                        "user": "shermansiu",
                        "type": "user"
                    },
                    "name": "Sherman Siu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:57:49.231Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a0",
                    "name": "Yuxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a1",
                    "user": {
                        "_id": "66c6853c13962d19a8395499",
                        "avatarUrl": "/avatars/3066df4244a064e49880c19fd83c9426.svg",
                        "isPro": false,
                        "fullname": "disen-liao",
                        "user": "Dliao",
                        "type": "user"
                    },
                    "name": "Disen Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:57:33.699Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a2",
                    "user": {
                        "_id": "66349404f2c753240d02952a",
                        "avatarUrl": "/avatars/4f207cf5807d9629b9f4f7d13875b840.svg",
                        "isPro": false,
                        "fullname": "ZhuofengLi",
                        "user": "ZhuofengLi",
                        "type": "user"
                    },
                    "name": "Zhuofeng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:57:27.171Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a3",
                    "name": "Huaye Zeng",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a4",
                    "user": {
                        "_id": "6721451d41cc176331607843",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/HQ6_0b2Y-X1ORS6tAz2cq.png",
                        "isPro": false,
                        "fullname": "Yiming Jia",
                        "user": "jymmmmm",
                        "type": "user"
                    },
                    "name": "Yiming Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:57:12.783Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a5",
                    "name": "Haozhe Wang",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a6",
                    "user": {
                        "_id": "6669ffeecf27eed0f2f2581e",
                        "avatarUrl": "/avatars/6a1012cce15da12fa3bd6f9e0f87d100.svg",
                        "isPro": false,
                        "fullname": "Benjamin Schneider",
                        "user": "BenSchneider",
                        "type": "user"
                    },
                    "name": "Benjamin Schneider",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:56:55.022Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a7",
                    "user": {
                        "_id": "673b578551d8d86ed0abcb7c",
                        "avatarUrl": "/avatars/41dbd6103456569efb707601930fec7e.svg",
                        "isPro": false,
                        "fullname": "chiruan",
                        "user": "chiruan",
                        "type": "user"
                    },
                    "name": "Chi Ruan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:56:34.920Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a8",
                    "user": {
                        "_id": "65c387c807a1445dfe1e9452",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c387c807a1445dfe1e9452/t0VnwQh2wRZ9W_UGTZ8zt.jpeg",
                        "isPro": false,
                        "fullname": "Wentao Ma",
                        "user": "tonymwt",
                        "type": "user"
                    },
                    "name": "Wentao Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T15:49:46.788Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444a9",
                    "user": {
                        "_id": "6114ea319ae90b69cb29fc92",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6114ea319ae90b69cb29fc92/uw2n4iiXGD5v5iUocmbxE.jpeg",
                        "isPro": false,
                        "fullname": "Zhiheng Lyu",
                        "user": "cogito233",
                        "type": "user"
                    },
                    "name": "Zhiheng Lyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:56:27.752Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444aa",
                    "name": "Yifei Wang",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444ab",
                    "name": "Yi Lu",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444ac",
                    "user": {
                        "_id": "65fa5fa1180d06d37c845d51",
                        "avatarUrl": "/avatars/a04eb59f0a3a3b5800ae443a8eaab785.svg",
                        "isPro": false,
                        "fullname": "Quy Duc Do",
                        "user": "qd2do",
                        "type": "user"
                    },
                    "name": "Quy Duc Do",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:55:41.859Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444ad",
                    "user": {
                        "_id": "64778fb8168cb428e00f69b0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64778fb8168cb428e00f69b0/D_XYg74zHG9K3HUJj_gD4.jpeg",
                        "isPro": true,
                        "fullname": "Ziyan Jiang",
                        "user": "ziyjiang",
                        "type": "user"
                    },
                    "name": "Ziyan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:55:33.868Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444ae",
                    "user": {
                        "_id": "65358802a920f38780b3248a",
                        "avatarUrl": "/avatars/9415510b598079973c2b0436ad12db9c.svg",
                        "isPro": false,
                        "fullname": "Ping Nie",
                        "user": "pingnieuk",
                        "type": "user"
                    },
                    "name": "Ping Nie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:55:11.300Z",
                    "hidden": false
                },
                {
                    "_id": "6835744884d4600675a444af",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:54:56.382Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T15:40:42.000Z",
            "submittedOnDailyAt": "2025-05-27T06:46:45.076Z",
            "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural\n  Outputs",
            "submittedOnDailyBy": {
                "_id": "62567c86d444a9b5a0ec51c1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
                "isPro": false,
                "fullname": "Dongfu Jiang",
                "user": "DongfuJiang",
                "type": "user"
            },
            "summary": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures.",
            "upvotes": 15,
            "discussionId": "6835744884d4600675a444d3",
            "ai_summary": "StructEval benchmarks Large Language Models for generating and converting structured outputs, highlighting performance gaps and challenges in producing visual content.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "StructEval",
                "structured outputs",
                "JSON",
                "YAML",
                "CSV",
                "HTML",
                "React",
                "SVG",
                "generation tasks",
                "conversion tasks",
                "format adherence",
                "structural correctness"
            ]
        },
        "publishedAt": "2025-05-26T11:40:42.000Z",
        "title": "StructEval: Benchmarking LLMs' Capabilities to Generate Structural\n  Outputs",
        "summary": "As Large Language Models (LLMs) become integral to software development\nworkflows, their ability to generate structured outputs has become critically\nimportant. We introduce StructEval, a comprehensive benchmark for evaluating\nLLMs' capabilities in producing both non-renderable (JSON, YAML, CSV) and\nrenderable (HTML, React, SVG) structured formats. Unlike prior benchmarks,\nStructEval systematically evaluates structural fidelity across diverse formats\nthrough two paradigms: 1) generation tasks, producing structured output from\nnatural language prompts, and 2) conversion tasks, translating between\nstructured formats. Our benchmark encompasses 18 formats and 44 types of task,\nwith novel metrics for format adherence and structural correctness. Results\nreveal significant performance gaps, even state-of-the-art models like o1-mini\nachieve only 75.58 average score, with open-source alternatives lagging\napproximately 10 points behind. We find generation tasks more challenging than\nconversion tasks, and producing correct visual content more difficult than\ngenerating text-only structures.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20139.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62567c86d444a9b5a0ec51c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62567c86d444a9b5a0ec51c1/1vXJf2uGztPcXpkwyTBr6.png",
            "fullname": "Dongfu Jiang",
            "name": "DongfuJiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20256",
            "authors": [
                {
                    "_id": "68352e44c829f2ea1e0484b5",
                    "name": "Hao Zhong",
                    "hidden": false
                },
                {
                    "_id": "68352e44c829f2ea1e0484b6",
                    "name": "Muzhi Zhu",
                    "hidden": false
                },
                {
                    "_id": "68352e44c829f2ea1e0484b7",
                    "name": "Zongze Du",
                    "hidden": false
                },
                {
                    "_id": "68352e44c829f2ea1e0484b8",
                    "name": "Zheng Huang",
                    "hidden": false
                },
                {
                    "_id": "68352e44c829f2ea1e0484b9",
                    "user": {
                        "_id": "646efd223dd912a539e0bd46",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
                        "isPro": false,
                        "fullname": "Canyu Zhao",
                        "user": "Canyu",
                        "type": "user"
                    },
                    "name": "Canyu Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:50:50.579Z",
                    "hidden": false
                },
                {
                    "_id": "68352e44c829f2ea1e0484ba",
                    "name": "Mingyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68352e44c829f2ea1e0484bb",
                    "name": "Wen Wang",
                    "hidden": false
                },
                {
                    "_id": "68352e44c829f2ea1e0484bc",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "68352e44c829f2ea1e0484bd",
                    "name": "Chunhua Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:34:06.000Z",
            "submittedOnDailyAt": "2025-05-27T01:47:33.260Z",
            "title": "Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System\n  Collaboration",
            "submittedOnDailyBy": {
                "_id": "632179745fc60c44fd91fc33",
                "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
                "isPro": false,
                "fullname": "zhumuzhi",
                "user": "Z-MU-Z",
                "type": "user"
            },
            "summary": "Long-horizon video-audio reasoning and fine-grained pixel understanding\nimpose conflicting requirements on omnimodal models: dense temporal coverage\ndemands many low-resolution frames, whereas precise grounding calls for\nhigh-resolution inputs. We tackle this trade-off with a two-system\narchitecture: a Global Reasoning System selects informative keyframes and\nrewrites the task at low spatial cost, while a Detail Understanding System\nperforms pixel-level grounding on the selected high-resolution snippets.\nBecause ``optimal'' keyframe selection and reformulation are ambiguous and hard\nto supervise, we formulate them as a reinforcement learning (RL) problem and\npresent Omni-R1, an end-to-end RL framework built on Group Relative Policy\nOptimization. Omni-R1 trains the Global Reasoning System through hierarchical\nrewards obtained via online collaboration with the Detail Understanding System,\nrequiring only one epoch of RL on small task splits.\n  Experiments on two challenging benchmarks, namely Referring Audio-Visual\nSegmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show\nthat Omni-R1 not only surpasses strong supervised baselines but also\noutperforms specialized state-of-the-art models, while substantially improving\nout-of-domain generalization and mitigating multimodal hallucination. Our\nresults demonstrate the first successful application of RL to large-scale\nomnimodal reasoning and highlight a scalable path toward universally foundation\nmodels.",
            "upvotes": 13,
            "discussionId": "68352e47c829f2ea1e048539",
            "projectPage": "https://aim-uofa.github.io/OmniR1/",
            "githubRepo": "https://github.com/aim-uofa/Omni-R1",
            "ai_summary": "An end-to-end reinforcement learning framework, Omni-R1, achieves superior performance in long-horizon video-audio reasoning and fine-grained pixel understanding tasks by combining global reasoning and detail understanding systems.",
            "ai_keywords": [
                "reinforcement learning",
                "Group Relative Policy Optimization",
                "hierarchical rewards",
                "Referring Audio-Visual Segmentation",
                "Reasoning Video Object Segmentation",
                "out-of-domain generalization",
                "multimodal hallucination",
                "universally foundation models"
            ]
        },
        "publishedAt": "2025-05-26T13:34:06.000Z",
        "title": "Omni-R1: Reinforcement Learning for Omnimodal Reasoning via Two-System\n  Collaboration",
        "summary": "Long-horizon video-audio reasoning and fine-grained pixel understanding\nimpose conflicting requirements on omnimodal models: dense temporal coverage\ndemands many low-resolution frames, whereas precise grounding calls for\nhigh-resolution inputs. We tackle this trade-off with a two-system\narchitecture: a Global Reasoning System selects informative keyframes and\nrewrites the task at low spatial cost, while a Detail Understanding System\nperforms pixel-level grounding on the selected high-resolution snippets.\nBecause ``optimal'' keyframe selection and reformulation are ambiguous and hard\nto supervise, we formulate them as a reinforcement learning (RL) problem and\npresent Omni-R1, an end-to-end RL framework built on Group Relative Policy\nOptimization. Omni-R1 trains the Global Reasoning System through hierarchical\nrewards obtained via online collaboration with the Detail Understanding System,\nrequiring only one epoch of RL on small task splits.\n  Experiments on two challenging benchmarks, namely Referring Audio-Visual\nSegmentation (RefAVS) and Reasoning Video Object Segmentation (REVOS), show\nthat Omni-R1 not only surpasses strong supervised baselines but also\noutperforms specialized state-of-the-art models, while substantially improving\nout-of-domain generalization and mitigating multimodal hallucination. Our\nresults demonstrate the first successful application of RL to large-scale\nomnimodal reasoning and highlight a scalable path toward universally foundation\nmodels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20256.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "632179745fc60c44fd91fc33",
            "avatarUrl": "/avatars/37d4fefbcc19f091dccffefec9706de2.svg",
            "fullname": "zhumuzhi",
            "name": "Z-MU-Z",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.13136",
            "authors": [
                {
                    "_id": "683597e271c57c48d73d78a0",
                    "name": "Anton Ehrmanntraut",
                    "hidden": false
                },
                {
                    "_id": "683597e271c57c48d73d78a1",
                    "user": {
                        "_id": "61a36cba39c7c71f0bda980e",
                        "avatarUrl": "/avatars/3a351b7ea31e82f350193415cc5b256e.svg",
                        "isPro": false,
                        "fullname": "Julia Wunderle ",
                        "user": "Julia287",
                        "type": "user"
                    },
                    "name": "Julia Wunderle",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T12:30:53.989Z",
                    "hidden": false
                },
                {
                    "_id": "683597e271c57c48d73d78a2",
                    "user": {
                        "_id": "6070431e1a4c4d313032558b",
                        "avatarUrl": "/avatars/142102e7e14886a0ff34af7959ca2f8c.svg",
                        "isPro": false,
                        "fullname": "Jan Pfister",
                        "user": "JanPf",
                        "type": "user"
                    },
                    "name": "Jan Pfister",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T12:13:08.065Z",
                    "hidden": false
                },
                {
                    "_id": "683597e271c57c48d73d78a3",
                    "name": "Fotis Jannidis",
                    "hidden": false
                },
                {
                    "_id": "683597e271c57c48d73d78a4",
                    "name": "Andreas Hotho",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6070431e1a4c4d313032558b/Kt0FIJ5XM4X1jLhdWDAT0.png"
            ],
            "publishedAt": "2025-05-19T14:07:20.000Z",
            "submittedOnDailyAt": "2025-05-27T09:16:53.025Z",
            "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch",
            "submittedOnDailyBy": {
                "_id": "6070431e1a4c4d313032558b",
                "avatarUrl": "/avatars/142102e7e14886a0ff34af7959ca2f8c.svg",
                "isPro": false,
                "fullname": "Jan Pfister",
                "user": "JanPf",
                "type": "user"
            },
            "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.",
            "upvotes": 13,
            "discussionId": "683597e471c57c48d73d791b",
            "ai_summary": "ModernGBERT and LL\\\"aMmlein2Vec, new German encoder models, outperform existing models in terms of performance and parameter-efficiency across various NLP tasks.",
            "ai_keywords": [
                "encoder models",
                "ModernBERT",
                "LLM2Vec",
                "natural language understanding",
                "text embedding",
                "long-context reasoning",
                "parameter-efficiency",
                "German NLP"
            ]
        },
        "publishedAt": "2025-05-19T10:07:20.000Z",
        "title": "ModernGBERT: German-only 1B Encoder Model Trained from Scratch",
        "summary": "Despite the prominence of decoder-only language models, encoders remain\ncrucial for resource-constrained applications. We introduce ModernGBERT (134M,\n1B), a fully transparent family of German encoder models trained from scratch,\nincorporating architectural innovations from ModernBERT. To evaluate the\npractical trade-offs of training encoders from scratch, we also present\nLL\\\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German\ndecoder-only models via LLM2Vec. We benchmark all models on natural language\nunderstanding, text embedding, and long-context reasoning tasks, enabling a\ncontrolled comparison between dedicated encoders and converted decoders. Our\nresults show that ModernGBERT 1B outperforms prior state-of-the-art German\nencoders as well as encoders adapted via LLM2Vec, with regard to performance\nand parameter-efficiency. All models, training data, checkpoints and code are\npublicly available, advancing the German NLP ecosystem with transparent,\nhigh-performance encoder models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6070431e1a4c4d313032558b/Kt0FIJ5XM4X1jLhdWDAT0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13136.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6070431e1a4c4d313032558b",
            "avatarUrl": "/avatars/142102e7e14886a0ff34af7959ca2f8c.svg",
            "fullname": "Jan Pfister",
            "name": "JanPf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19788",
            "authors": [
                {
                    "_id": "68352630363d6fd2fff5d07f",
                    "name": "Zihao Zeng",
                    "hidden": false
                },
                {
                    "_id": "68352630363d6fd2fff5d080",
                    "user": {
                        "_id": "6721dacfc5309c08451d21d5",
                        "avatarUrl": "/avatars/ac8be5ac8b8ee5b5533214e526b72dad.svg",
                        "isPro": false,
                        "fullname": "Huang Xuyao",
                        "user": "ElysiaTrue",
                        "type": "user"
                    },
                    "name": "Xuyao Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:51:44.216Z",
                    "hidden": false
                },
                {
                    "_id": "68352630363d6fd2fff5d081",
                    "name": "Boxiu Li",
                    "hidden": false
                },
                {
                    "_id": "68352630363d6fd2fff5d082",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68352630363d6fd2fff5d083",
                    "name": "Zhijie Deng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T10:18:57.000Z",
            "submittedOnDailyAt": "2025-05-27T01:59:02.853Z",
            "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured\n  Multi-Turn Decomposition",
            "submittedOnDailyBy": {
                "_id": "6721dacfc5309c08451d21d5",
                "avatarUrl": "/avatars/ac8be5ac8b8ee5b5533214e526b72dad.svg",
                "isPro": false,
                "fullname": "Huang Xuyao",
                "user": "ElysiaTrue",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy\nChain-of-Thought (CoT) to derive the final answer, suffering from high\nfirst-token and overall latency. Typically, the CoT of LRMs mixes multiple\nthinking units; each unit attempts to produce a candidate answer to the\noriginal query. Hence, a natural idea to improve efficiency is to reduce the\nunit number. Yet, the fact that the thinking units in vanilla CoT cannot be\nexplicitly managed renders doing so challenging. This paper introduces\nMulti-Turn Decomposition (MinD) to decode conventional CoT into a sequence of\nexplicit, structured, and turn-wise interactions to bridge the gap. In MinD,\nthe model provides a multi-turn response to the query, where each turn embraces\na thinking unit and yields a corresponding answer. The subsequent turns can\nreflect, verify, revise, or explore alternative approaches to both the thinking\nand answer parts of earlier ones. This not only makes the answer delivered more\nswiftly, but also enables explicit controls over the iterative reasoning\nprocess (i.e., users may halt or continue at any turn). We follow a supervised\nfine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We\nfirst rephrase the outputs of an LRM into multi-turn formats by prompting\nanother LLM, and then tune the LRM with such data. Observing that the tuned\nmodel tends to consume even more tokens than the original one (probably due to\nthat the multi-turn formats introduce additional answer tokens), we advocate\nleveraging RL algorithms like GRPO to prioritize correct outputs with fewer\nturns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up\nto ~70% reduction in both output token usage and time to first token (TTFT),\nwhile maintaining competitive performance on reasoning benchmarks such as\nMATH-500, AIME24, AMC23, and GPQA-Diamond.",
            "upvotes": 12,
            "discussionId": "68352631363d6fd2fff5d0b9",
            "ai_summary": "Multi-Turn Decomposition improves efficiency in large reasoning models by breaking down chain-of-thought into manageable turns, reducing token usage and latency while maintaining performance.",
            "ai_keywords": [
                "Chain-of-Thought",
                "large reasoning models",
                "multi-turn decomposition",
                "thinking units",
                "iterative reasoning process",
                "supervised fine-tuning",
                "reinforcement learning",
                "MATH dataset",
                "R1-Distill models",
                "MATH-500",
                "AIME24",
                "AMC23",
                "GPQA-Diamond"
            ]
        },
        "publishedAt": "2025-05-26T06:18:57.000Z",
        "title": "Done Is Better than Perfect: Unlocking Efficient Reasoning by Structured\n  Multi-Turn Decomposition",
        "summary": "Large Reasoning Models (LRMs) are criticized for the excessively lengthy\nChain-of-Thought (CoT) to derive the final answer, suffering from high\nfirst-token and overall latency. Typically, the CoT of LRMs mixes multiple\nthinking units; each unit attempts to produce a candidate answer to the\noriginal query. Hence, a natural idea to improve efficiency is to reduce the\nunit number. Yet, the fact that the thinking units in vanilla CoT cannot be\nexplicitly managed renders doing so challenging. This paper introduces\nMulti-Turn Decomposition (MinD) to decode conventional CoT into a sequence of\nexplicit, structured, and turn-wise interactions to bridge the gap. In MinD,\nthe model provides a multi-turn response to the query, where each turn embraces\na thinking unit and yields a corresponding answer. The subsequent turns can\nreflect, verify, revise, or explore alternative approaches to both the thinking\nand answer parts of earlier ones. This not only makes the answer delivered more\nswiftly, but also enables explicit controls over the iterative reasoning\nprocess (i.e., users may halt or continue at any turn). We follow a supervised\nfine-tuning (SFT) then reinforcement learning (RL) paradigm to realize MinD. We\nfirst rephrase the outputs of an LRM into multi-turn formats by prompting\nanother LLM, and then tune the LRM with such data. Observing that the tuned\nmodel tends to consume even more tokens than the original one (probably due to\nthat the multi-turn formats introduce additional answer tokens), we advocate\nleveraging RL algorithms like GRPO to prioritize correct outputs with fewer\nturns. Trained on the MATH dataset using R1-Distill models, MinD can achieve up\nto ~70% reduction in both output token usage and time to first token (TTFT),\nwhile maintaining competitive performance on reasoning benchmarks such as\nMATH-500, AIME24, AMC23, and GPQA-Diamond.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19788.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6721dacfc5309c08451d21d5",
            "avatarUrl": "/avatars/ac8be5ac8b8ee5b5533214e526b72dad.svg",
            "fullname": "Huang Xuyao",
            "name": "ElysiaTrue",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18822",
            "authors": [
                {
                    "_id": "683528d8c682e155a8b9a80f",
                    "user": {
                        "_id": "64ce05c631c655ff8a2e183c",
                        "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
                        "isPro": false,
                        "fullname": "Shijue Huang",
                        "user": "JoeYing",
                        "type": "user"
                    },
                    "name": "Shijue Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T10:03:37.508Z",
                    "hidden": false
                },
                {
                    "_id": "683528d8c682e155a8b9a810",
                    "user": {
                        "_id": "65f906e5c3dbdcae83ff7aac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
                        "isPro": false,
                        "fullname": "Hongru Wang",
                        "user": "Merlin-Hongru",
                        "type": "user"
                    },
                    "name": "Hongru Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:07:34.709Z",
                    "hidden": false
                },
                {
                    "_id": "683528d8c682e155a8b9a811",
                    "user": {
                        "_id": "643f956635e2b54a42e7feba",
                        "avatarUrl": "/avatars/c6185f81ae8499ae866ad451c1cbf43b.svg",
                        "isPro": false,
                        "fullname": "Wanjun Zhong",
                        "user": "WanjunZhong",
                        "type": "user"
                    },
                    "name": "Wanjun Zhong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:07:08.589Z",
                    "hidden": false
                },
                {
                    "_id": "683528d8c682e155a8b9a812",
                    "user": {
                        "_id": "64264095ba51f8a2136946a0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
                        "isPro": false,
                        "fullname": "Zhaochen Su",
                        "user": "Warrieryes",
                        "type": "user"
                    },
                    "name": "Zhaochen Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:07:15.418Z",
                    "hidden": false
                },
                {
                    "_id": "683528d8c682e155a8b9a813",
                    "name": "Jiazhan Feng",
                    "hidden": false
                },
                {
                    "_id": "683528d8c682e155a8b9a814",
                    "name": "Bowen Cao",
                    "hidden": false
                },
                {
                    "_id": "683528d8c682e155a8b9a815",
                    "name": "Yi R. Fung",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T18:46:50.000Z",
            "submittedOnDailyAt": "2025-05-27T07:17:54.612Z",
            "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via\n  Difficulty-Aware Budgeting",
            "submittedOnDailyBy": {
                "_id": "64ce05c631c655ff8a2e183c",
                "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
                "isPro": false,
                "fullname": "Shijue Huang",
                "user": "JoeYing",
                "type": "user"
            },
            "summary": "Modern large reasoning models demonstrate impressive problem-solving\ncapabilities by employing sophisticated reasoning strategies. However, they\noften struggle to balance efficiency and effectiveness, frequently generating\nunnecessarily lengthy reasoning chains for simple problems. In this work, we\npropose AdaCtrl, a novel framework to support both difficulty-aware adaptive\nreasoning budget allocation and explicit user control over reasoning depth.\nAdaCtrl dynamically adjusts its reasoning length based on self-assessed problem\ndifficulty, while also allowing users to manually control the budget to\nprioritize either efficiency or effectiveness. This is achieved through a\ntwo-stage training pipeline: an initial cold-start fine-tuning phase to instill\nthe ability to self-aware difficulty and adjust reasoning budget, followed by a\ndifficulty-aware reinforcement learning (RL) stage that refines the model's\nadaptive reasoning strategies and calibrates its difficulty assessments based\non its evolving capabilities during online training. To enable intuitive user\ninteraction, we design explicit length-triggered tags that function as a\nnatural interface for budget control. Empirical results show that AdaCtrl\nadapts reasoning length based on estimated difficulty, compared to the standard\ntraining baseline that also incorporates fine-tuning and RL, it yields\nperformance improvements and simultaneously reduces response length by 10.06%\nand 12.14% on the more challenging AIME2024 and AIME2025 datasets, which\nrequire elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K\ndatasets, where more concise responses are sufficient. Furthermore, AdaCtrl\nenables precise user control over the reasoning budget, allowing for tailored\nresponses to meet specific needs.",
            "upvotes": 12,
            "discussionId": "683528d9c682e155a8b9a852",
            "ai_summary": "AdaCtrl, a novel framework, dynamically adjusts reasoning length based on problem difficulty and user control, improving performance and reducing response length across various datasets compared to standard training methods.",
            "ai_keywords": [
                "AdaCtrl",
                "adaptive reasoning budget allocation",
                "self-assessed problem difficulty",
                "difficulty-aware reinforcement learning",
                "two-stage training pipeline",
                "cold-start fine-tuning"
            ]
        },
        "publishedAt": "2025-05-24T14:46:50.000Z",
        "title": "AdaCtrl: Towards Adaptive and Controllable Reasoning via\n  Difficulty-Aware Budgeting",
        "summary": "Modern large reasoning models demonstrate impressive problem-solving\ncapabilities by employing sophisticated reasoning strategies. However, they\noften struggle to balance efficiency and effectiveness, frequently generating\nunnecessarily lengthy reasoning chains for simple problems. In this work, we\npropose AdaCtrl, a novel framework to support both difficulty-aware adaptive\nreasoning budget allocation and explicit user control over reasoning depth.\nAdaCtrl dynamically adjusts its reasoning length based on self-assessed problem\ndifficulty, while also allowing users to manually control the budget to\nprioritize either efficiency or effectiveness. This is achieved through a\ntwo-stage training pipeline: an initial cold-start fine-tuning phase to instill\nthe ability to self-aware difficulty and adjust reasoning budget, followed by a\ndifficulty-aware reinforcement learning (RL) stage that refines the model's\nadaptive reasoning strategies and calibrates its difficulty assessments based\non its evolving capabilities during online training. To enable intuitive user\ninteraction, we design explicit length-triggered tags that function as a\nnatural interface for budget control. Empirical results show that AdaCtrl\nadapts reasoning length based on estimated difficulty, compared to the standard\ntraining baseline that also incorporates fine-tuning and RL, it yields\nperformance improvements and simultaneously reduces response length by 10.06%\nand 12.14% on the more challenging AIME2024 and AIME2025 datasets, which\nrequire elaborate reasoning, and by 62.05% and 91.04% on the MATH500 and GSM8K\ndatasets, where more concise responses are sufficient. Furthermore, AdaCtrl\nenables precise user control over the reasoning budget, allowing for tailored\nresponses to meet specific needs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18822.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ce05c631c655ff8a2e183c",
            "avatarUrl": "/avatars/f2de7f8a1348b05f46946085e3e9718e.svg",
            "fullname": "Shijue Huang",
            "name": "JoeYing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18759",
            "authors": [
                {
                    "_id": "683551c54f3166e8677b43bb",
                    "name": "Ruichen Zhang",
                    "hidden": false
                },
                {
                    "_id": "683551c54f3166e8677b43bc",
                    "name": "Rana Muhammad Shahroz Khan",
                    "hidden": false
                },
                {
                    "_id": "683551c54f3166e8677b43bd",
                    "name": "Zhen Tan",
                    "hidden": false
                },
                {
                    "_id": "683551c54f3166e8677b43be",
                    "user": {
                        "_id": "6474e1afb68461d5cf7c41cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
                        "isPro": false,
                        "fullname": "Dawei Li",
                        "user": "wjldw",
                        "type": "user"
                    },
                    "name": "Dawei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:49:14.749Z",
                    "hidden": false
                },
                {
                    "_id": "683551c54f3166e8677b43bf",
                    "name": "Song Wang",
                    "hidden": false
                },
                {
                    "_id": "683551c54f3166e8677b43c0",
                    "name": "Tianlong Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T15:54:19.000Z",
            "submittedOnDailyAt": "2025-05-27T04:17:34.631Z",
            "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT\n  Distillation",
            "submittedOnDailyBy": {
                "_id": "6474e1afb68461d5cf7c41cc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
                "isPro": false,
                "fullname": "Dawei Li",
                "user": "wjldw",
                "type": "user"
            },
            "summary": "Data-centric distillation, including data augmentation, selection, and\nmixing, offers a promising path to creating smaller, more efficient student\nLarge Language Models (LLMs) that retain strong reasoning abilities. However,\nthere still lacks a comprehensive benchmark to systematically assess the effect\nof each distillation approach. This paper introduces DC-CoT, the first\ndata-centric benchmark that investigates data manipulation in chain-of-thought\n(CoT) distillation from method, model and data perspectives. Utilizing various\nteacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student\narchitectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of\nthese data manipulations on student model performance across multiple reasoning\ndatasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)\ngeneralization, and cross-domain transfer. Our findings aim to provide\nactionable insights and establish best practices for optimizing CoT\ndistillation through data-centric techniques, ultimately facilitating the\ndevelopment of more accessible and capable reasoning models. The dataset can be\nfound at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is\nshared in https://anonymous.4open.science/r/DC-COT-FF4C/.",
            "upvotes": 12,
            "discussionId": "683551c64f3166e8677b4424",
            "ai_summary": "DC-CoT provides a comprehensive benchmark for assessing data-centric distillation techniques in chain-of-thought distillation, focusing on performance and generalization across different models and datasets.",
            "ai_keywords": [
                "data-centric distillation",
                "data augmentation",
                "data selection",
                "data mixing",
                "chain-of-thought (CoT)",
                "in-distribution (IID)",
                "out-of-distribution (OOD)",
                "cross-domain transfer"
            ]
        },
        "publishedAt": "2025-05-24T11:54:19.000Z",
        "title": "The Quest for Efficient Reasoning: A Data-Centric Benchmark to CoT\n  Distillation",
        "summary": "Data-centric distillation, including data augmentation, selection, and\nmixing, offers a promising path to creating smaller, more efficient student\nLarge Language Models (LLMs) that retain strong reasoning abilities. However,\nthere still lacks a comprehensive benchmark to systematically assess the effect\nof each distillation approach. This paper introduces DC-CoT, the first\ndata-centric benchmark that investigates data manipulation in chain-of-thought\n(CoT) distillation from method, model and data perspectives. Utilizing various\nteacher models (e.g., o4-mini, Gemini-Pro, Claude-3.5) and student\narchitectures (e.g., 3B, 7B parameters), we rigorously evaluate the impact of\nthese data manipulations on student model performance across multiple reasoning\ndatasets, with a focus on in-distribution (IID) and out-of-distribution (OOD)\ngeneralization, and cross-domain transfer. Our findings aim to provide\nactionable insights and establish best practices for optimizing CoT\ndistillation through data-centric techniques, ultimately facilitating the\ndevelopment of more accessible and capable reasoning models. The dataset can be\nfound at https://huggingface.co/datasets/rana-shahroz/DC-COT, while our code is\nshared in https://anonymous.4open.science/r/DC-COT-FF4C/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18759.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6474e1afb68461d5cf7c41cc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
            "fullname": "Dawei Li",
            "name": "wjldw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20152",
            "authors": [
                {
                    "_id": "6835385ebd4d4208167d15ac",
                    "name": "Kai Sun",
                    "hidden": false
                },
                {
                    "_id": "6835385ebd4d4208167d15ad",
                    "user": {
                        "_id": "64ed568ccf6118a9379a61b8",
                        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
                        "isPro": false,
                        "fullname": "Yushi Bai",
                        "user": "bys0318",
                        "type": "user"
                    },
                    "name": "Yushi Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:06:45.273Z",
                    "hidden": false
                },
                {
                    "_id": "6835385ebd4d4208167d15ae",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "6835385ebd4d4208167d15af",
                    "user": {
                        "_id": "66cdd285c51a915bd5f2d017",
                        "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
                        "isPro": false,
                        "fullname": "Jiajie Zhang",
                        "user": "NeoZ123",
                        "type": "user"
                    },
                    "name": "Jiajie Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:06:22.005Z",
                    "hidden": false
                },
                {
                    "_id": "6835385ebd4d4208167d15b0",
                    "name": "Ji Qi",
                    "hidden": false
                },
                {
                    "_id": "6835385ebd4d4208167d15b1",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "6835385ebd4d4208167d15b2",
                    "user": {
                        "_id": "65df8cbc2705d9672f55d1aa",
                        "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
                        "isPro": false,
                        "fullname": "Juanzi Li",
                        "user": "juanli",
                        "type": "user"
                    },
                    "name": "Juanzi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:06:29.209Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T15:55:28.000Z",
            "submittedOnDailyAt": "2025-05-27T02:29:13.927Z",
            "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "66cdd285c51a915bd5f2d017",
                "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
                "isPro": false,
                "fullname": "Jiajie Zhang",
                "user": "NeoZ123",
                "type": "user"
            },
            "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.",
            "upvotes": 11,
            "discussionId": "6835385fbd4d4208167d15f0",
            "ai_summary": "A novel hard negative contrastive learning framework improves geometric reasoning in Large Multimodal Models, significantly enhancing their performance compared to existing models.",
            "ai_keywords": [
                "contrastively trained visual encoders",
                "Large Multimodal Models",
                "geometric problem-solving",
                "hard negative contrastive learning",
                "generation-based hard negatives",
                "rule-based negatives",
                "retrieval-based negatives",
                "CLIP",
                "MMCLIP",
                "multimodal math clip",
                "MMGeoLM",
                "geometric reasoning benchmarks"
            ]
        },
        "publishedAt": "2025-05-26T11:55:28.000Z",
        "title": "Hard Negative Contrastive Learning for Fine-Grained Geometric\n  Understanding in Large Multimodal Models",
        "summary": "Benefiting from contrastively trained visual encoders on large-scale natural\nscene images, Large Multimodal Models (LMMs) have achieved remarkable\nperformance across various visual perception tasks. However, the inherent\nlimitations of contrastive learning upon summarized descriptions fundamentally\nrestrict the capabilities of models in meticulous reasoning, particularly in\ncrucial scenarios of geometric problem-solving. To enhance geometric\nunderstanding, we propose a novel hard negative contrastive learning framework\nfor the vision encoder, which combines image-based contrastive learning using\ngeneration-based hard negatives created by perturbing diagram generation code,\nand text-based contrastive learning using rule-based negatives derived from\nmodified geometric descriptions and retrieval-based negatives selected based on\ncaption similarity. We train CLIP using our strong negative learning method,\nnamely MMCLIP (Multimodal Math CLIP), and subsequently train an LMM for\ngeometric problem-solving. Experiments show that our trained model, MMGeoLM,\nsignificantly outperforms other open-source models on three geometric reasoning\nbenchmarks. Even with a size of 7B, it can rival powerful closed-source models\nlike GPT-4o. We further study the impact of different negative sample\nconstruction methods and the number of negative samples on the geometric\nreasoning performance of LMM, yielding fruitful conclusions. The code and\ndataset are available at https://github.com/THU-KEG/MMGeoLM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20152.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66cdd285c51a915bd5f2d017",
            "avatarUrl": "/avatars/14e5794307e4672b1b51d26b31227e0f.svg",
            "fullname": "Jiajie Zhang",
            "name": "NeoZ123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20046",
            "authors": [
                {
                    "_id": "6835ca4b375575936dca6572",
                    "user": {
                        "_id": "633b423e5df91da9ceafe40a",
                        "avatarUrl": "/avatars/6d8c31c5f3e64d8a085b6e1228bcc44d.svg",
                        "isPro": false,
                        "fullname": "le.zhang",
                        "user": "le723z",
                        "type": "user"
                    },
                    "name": "Le Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T15:48:59.328Z",
                    "hidden": false
                },
                {
                    "_id": "6835ca4b375575936dca6573",
                    "name": "Bo Wang",
                    "hidden": false
                },
                {
                    "_id": "6835ca4b375575936dca6574",
                    "name": "Xipeng Qiu",
                    "hidden": false
                },
                {
                    "_id": "6835ca4b375575936dca6575",
                    "name": "Siva Reddy",
                    "hidden": false
                },
                {
                    "_id": "6835ca4b375575936dca6576",
                    "name": "Aishwarya Agrawal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T14:31:48.000Z",
            "submittedOnDailyAt": "2025-05-27T12:51:41.882Z",
            "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "633b423e5df91da9ceafe40a",
                "avatarUrl": "/avatars/6d8c31c5f3e64d8a085b6e1228bcc44d.svg",
                "isPro": false,
                "fullname": "le.zhang",
                "user": "le723z",
                "type": "user"
            },
            "summary": "We present REARANK, a large language model (LLM)-based listwise reasoning\nreranking agent. REARANK explicitly reasons before reranking, significantly\nimproving both performance and interpretability. Leveraging reinforcement\nlearning and data augmentation, REARANK achieves substantial improvements over\nbaseline models across popular information retrieval benchmarks, notably\nrequiring only 179 annotated samples. Built on top of Qwen2.5-7B, our\nREARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and\nout-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT\nbenchmarks. These results underscore the effectiveness of our approach and\nhighlight how reinforcement learning can enhance LLM reasoning capabilities in\nreranking.",
            "upvotes": 11,
            "discussionId": "6835ca4c375575936dca65b0",
            "githubRepo": "https://github.com/lezhang7/Rearank",
            "ai_summary": "REARANK, a reinforcement learning-enhanced large language model for listwise reasoning,outperforms baseline models and even surpasses GPT-4 on reasoning-intensive benchmarks with minimal data.",
            "ai_keywords": [
                "large language model",
                "listwise reasoning",
                "reinforcement learning",
                "data augmentation",
                "reranking",
                "information retrieval benchmarks",
                "BRIGHT benchmarks"
            ]
        },
        "publishedAt": "2025-05-26T10:31:48.000Z",
        "title": "REARANK: Reasoning Re-ranking Agent via Reinforcement Learning",
        "summary": "We present REARANK, a large language model (LLM)-based listwise reasoning\nreranking agent. REARANK explicitly reasons before reranking, significantly\nimproving both performance and interpretability. Leveraging reinforcement\nlearning and data augmentation, REARANK achieves substantial improvements over\nbaseline models across popular information retrieval benchmarks, notably\nrequiring only 179 annotated samples. Built on top of Qwen2.5-7B, our\nREARANK-7B demonstrates performance comparable to GPT-4 on both in-domain and\nout-of-domain benchmarks and even surpasses GPT-4 on reasoning-intensive BRIGHT\nbenchmarks. These results underscore the effectiveness of our approach and\nhighlight how reinforcement learning can enhance LLM reasoning capabilities in\nreranking.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20046.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633b423e5df91da9ceafe40a",
            "avatarUrl": "/avatars/6d8c31c5f3e64d8a085b6e1228bcc44d.svg",
            "fullname": "le.zhang",
            "name": "le723z",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19640",
            "authors": [
                {
                    "_id": "68353e3f9f4e0a0f0496d0c6",
                    "user": {
                        "_id": "6555a124a6554059711b58a2",
                        "avatarUrl": "/avatars/222bb6b8f252d6c2bbd4cf35a54fc1c9.svg",
                        "isPro": false,
                        "fullname": "Roy",
                        "user": "RRoy233",
                        "type": "user"
                    },
                    "name": "Roy Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:02:55.735Z",
                    "hidden": false
                },
                {
                    "_id": "68353e3f9f4e0a0f0496d0c7",
                    "name": "David Qiu",
                    "hidden": false
                },
                {
                    "_id": "68353e3f9f4e0a0f0496d0c8",
                    "user": {
                        "_id": "66f6df6329dad7fec7cf26f2",
                        "avatarUrl": "/avatars/e576fbeca6c5770905408316bd9bc0f2.svg",
                        "isPro": false,
                        "fullname": "Deepak Gopinath",
                        "user": "dpacgopinath",
                        "type": "user"
                    },
                    "name": "Deepak Gopinath",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:03:07.418Z",
                    "hidden": false
                },
                {
                    "_id": "68353e3f9f4e0a0f0496d0c9",
                    "name": "Dong Lin",
                    "hidden": false
                },
                {
                    "_id": "68353e3f9f4e0a0f0496d0ca",
                    "user": {
                        "_id": "6488a8e65815252b02b948c8",
                        "avatarUrl": "/avatars/bb1ff64841c75b8d331986709b0a0a93.svg",
                        "isPro": false,
                        "fullname": "Yanchao Sun",
                        "user": "yanchao-sun",
                        "type": "user"
                    },
                    "name": "Yanchao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:03:16.312Z",
                    "hidden": false
                },
                {
                    "_id": "68353e3f9f4e0a0f0496d0cb",
                    "name": "Chong Wang",
                    "hidden": false
                },
                {
                    "_id": "68353e3f9f4e0a0f0496d0cc",
                    "user": {
                        "_id": "66711f329c609c2484c76d80",
                        "avatarUrl": "/avatars/717e0ce3e8693e99da6d61199ea712f3.svg",
                        "isPro": false,
                        "fullname": "Saloni Potdar",
                        "user": "spotdar",
                        "type": "user"
                    },
                    "name": "Saloni Potdar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T13:03:28.257Z",
                    "hidden": false
                },
                {
                    "_id": "68353e3f9f4e0a0f0496d0cd",
                    "name": "Bhuwan Dhingra",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T07:58:17.000Z",
            "submittedOnDailyAt": "2025-05-27T02:56:39.316Z",
            "title": "Interleaved Reasoning for Large Language Models via Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6555a124a6554059711b58a2",
                "avatarUrl": "/avatars/222bb6b8f252d6c2bbd4cf35a54fc1c9.svg",
                "isPro": false,
                "fullname": "Roy",
                "user": "RRoy233",
                "type": "user"
            },
            "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling.",
            "upvotes": 10,
            "discussionId": "68353e409f4e0a0f0496d0fb",
            "ai_summary": "A reinforcement learning-guided training paradigm enhances large language models' reasoning efficiency and performance for multi-hop questions by interleaving thinking and answering.",
            "ai_keywords": [
                "chain-of-thought",
                "large language models",
                "reasoning capabilities",
                "time-to-first-token",
                "reinforcement learning",
                "interleaved reasoning",
                "rule-based reward",
                "reward modeling",
                "multi-hop questions",
                "think-answer reasoning",
                "Pass@1 accuracy",
                "MATH",
                "GPQA",
                "MMLU",
                "PPO",
                "GRPO",
                "REINFORCE++"
            ]
        },
        "publishedAt": "2025-05-26T03:58:17.000Z",
        "title": "Interleaved Reasoning for Large Language Models via Reinforcement\n  Learning",
        "summary": "Long chain-of-thought (CoT) significantly enhances large language models'\n(LLM) reasoning capabilities. However, the extensive reasoning traces lead to\ninefficiencies and an increased time-to-first-token (TTFT). We propose a novel\ntraining paradigm that uses reinforcement learning (RL) to guide reasoning LLMs\nto interleave thinking and answering for multi-hop questions. We observe that\nmodels inherently possess the ability to perform interleaved reasoning, which\ncan be further enhanced through RL. We introduce a simple yet effective\nrule-based reward to incentivize correct intermediate steps, which guides the\npolicy model toward correct reasoning paths by leveraging intermediate signals\ngenerated during interleaved reasoning. Extensive experiments conducted across\nfive diverse datasets and three RL algorithms (PPO, GRPO, and REINFORCE++)\ndemonstrate consistent improvements over traditional think-answer reasoning,\nwithout requiring external tools. Specifically, our approach reduces TTFT by\nover 80% on average and improves up to 19.3% in Pass@1 accuracy. Furthermore,\nour method, trained solely on question answering and logical reasoning\ndatasets, exhibits strong generalization ability to complex reasoning datasets\nsuch as MATH, GPQA, and MMLU. Additionally, we conduct in-depth analysis to\nreveal several valuable insights into conditional reward modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19640.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6555a124a6554059711b58a2",
            "avatarUrl": "/avatars/222bb6b8f252d6c2bbd4cf35a54fc1c9.svg",
            "fullname": "Roy",
            "name": "RRoy233",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19386",
            "authors": [
                {
                    "_id": "6835dc82635029ffb93dffea",
                    "name": "Nate Gillman",
                    "hidden": false
                },
                {
                    "_id": "6835dc82635029ffb93dffeb",
                    "name": "Charles Herrmann",
                    "hidden": false
                },
                {
                    "_id": "6835dc82635029ffb93dffec",
                    "name": "Michael Freeman",
                    "hidden": false
                },
                {
                    "_id": "6835dc82635029ffb93dffed",
                    "name": "Daksh Aggarwal",
                    "hidden": false
                },
                {
                    "_id": "6835dc82635029ffb93dffee",
                    "name": "Evan Luo",
                    "hidden": false
                },
                {
                    "_id": "6835dc82635029ffb93dffef",
                    "name": "Deqing Sun",
                    "hidden": false
                },
                {
                    "_id": "6835dc82635029ffb93dfff0",
                    "name": "Chen Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T01:04:02.000Z",
            "submittedOnDailyAt": "2025-05-27T14:09:51.881Z",
            "title": "Force Prompting: Video Generation Models Can Learn and Generalize\n  Physics-based Control Signals",
            "submittedOnDailyBy": {
                "_id": "634305b6e2647466b42aed71",
                "avatarUrl": "/avatars/d2b796b3bc0d15a10ff64fa4a43980a9.svg",
                "isPro": false,
                "fullname": "Nate Gillman",
                "user": "nate-gillman",
                "type": "user"
            },
            "summary": "Recent advances in video generation models have sparked interest in world\nmodels capable of simulating realistic environments. While navigation has been\nwell-explored, physically meaningful interactions that mimic real-world forces\nremain largely understudied. In this work, we investigate using physical forces\nas a control signal for video generation and propose force prompts which enable\nusers to interact with images through both localized point forces, such as\npoking a plant, and global wind force fields, such as wind blowing on fabric.\nWe demonstrate that these force prompts can enable videos to respond\nrealistically to physical control signals by leveraging the visual and motion\nprior in the original pretrained model, without using any 3D asset or physics\nsimulator at inference. The primary challenge of force prompting is the\ndifficulty in obtaining high quality paired force-video training data, both in\nthe real world due to the difficulty of obtaining force signals, and in\nsynthetic data due to limitations in the visual quality and domain diversity of\nphysics simulators. Our key finding is that video generation models can\ngeneralize remarkably well when adapted to follow physical force conditioning\nfrom videos synthesized by Blender, even with limited demonstrations of few\nobjects. Our method can generate videos which simulate forces across diverse\ngeometries, settings, and materials. We also try to understand the source of\nthis generalization and perform ablations that reveal two key elements: visual\ndiversity and the use of specific text keywords during training. Our approach\nis trained on only around 15k training examples for a single day on four A100\nGPUs, and outperforms existing methods on force adherence and physics realism,\nbringing world models closer to real-world physics interactions. We release all\ndatasets, code, weights, and interactive video demos at our project page.",
            "upvotes": 9,
            "discussionId": "6835dc86635029ffb93e00de",
            "projectPage": "https://force-prompting.github.io/",
            "githubRepo": "https://github.com/brown-palm/force-prompting",
            "ai_summary": "Force prompts enable video generation models to simulate realistic physical interactions using pretrained models and force conditioning from Blender-generated videos.",
            "ai_keywords": [
                "force prompts",
                "video generation models",
                "physical forces",
                "control signal",
                "visual and motion prior",
                "force adherence",
                "physics realism",
                "Blender-generated videos",
                "visual diversity",
                "text keywords"
            ]
        },
        "publishedAt": "2025-05-25T21:04:02.000Z",
        "title": "Force Prompting: Video Generation Models Can Learn and Generalize\n  Physics-based Control Signals",
        "summary": "Recent advances in video generation models have sparked interest in world\nmodels capable of simulating realistic environments. While navigation has been\nwell-explored, physically meaningful interactions that mimic real-world forces\nremain largely understudied. In this work, we investigate using physical forces\nas a control signal for video generation and propose force prompts which enable\nusers to interact with images through both localized point forces, such as\npoking a plant, and global wind force fields, such as wind blowing on fabric.\nWe demonstrate that these force prompts can enable videos to respond\nrealistically to physical control signals by leveraging the visual and motion\nprior in the original pretrained model, without using any 3D asset or physics\nsimulator at inference. The primary challenge of force prompting is the\ndifficulty in obtaining high quality paired force-video training data, both in\nthe real world due to the difficulty of obtaining force signals, and in\nsynthetic data due to limitations in the visual quality and domain diversity of\nphysics simulators. Our key finding is that video generation models can\ngeneralize remarkably well when adapted to follow physical force conditioning\nfrom videos synthesized by Blender, even with limited demonstrations of few\nobjects. Our method can generate videos which simulate forces across diverse\ngeometries, settings, and materials. We also try to understand the source of\nthis generalization and perform ablations that reveal two key elements: visual\ndiversity and the use of specific text keywords during training. Our approach\nis trained on only around 15k training examples for a single day on four A100\nGPUs, and outperforms existing methods on force adherence and physics realism,\nbringing world models closer to real-world physics interactions. We release all\ndatasets, code, weights, and interactive video demos at our project page.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19386.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634305b6e2647466b42aed71",
            "avatarUrl": "/avatars/d2b796b3bc0d15a10ff64fa4a43980a9.svg",
            "fullname": "Nate Gillman",
            "name": "nate-gillman",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.19955",
            "authors": [
                {
                    "_id": "68354470650d51732c992a4e",
                    "user": {
                        "_id": "61166c4328c98bfd5b92e7c5",
                        "avatarUrl": "/avatars/f4bb0f0cc2c5b84428c28bddaa479b61.svg",
                        "isPro": false,
                        "fullname": "Hui Chen",
                        "user": "chchenhui",
                        "type": "user"
                    },
                    "name": "Hui Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T08:47:12.560Z",
                    "hidden": false
                },
                {
                    "_id": "68354470650d51732c992a4f",
                    "user": {
                        "_id": "6530cf34e7535baecd9620a7",
                        "avatarUrl": "/avatars/e6058a932d88e42b4957734f653cbcfd.svg",
                        "isPro": false,
                        "fullname": "Miao Xiong",
                        "user": "happymio",
                        "type": "user"
                    },
                    "name": "Miao Xiong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T10:03:35.714Z",
                    "hidden": false
                },
                {
                    "_id": "68354470650d51732c992a50",
                    "name": "Yujie Lu",
                    "hidden": false
                },
                {
                    "_id": "68354470650d51732c992a51",
                    "name": "Wei Han",
                    "hidden": false
                },
                {
                    "_id": "68354470650d51732c992a52",
                    "name": "Ailin Deng",
                    "hidden": false
                },
                {
                    "_id": "68354470650d51732c992a53",
                    "name": "Yufei He",
                    "hidden": false
                },
                {
                    "_id": "68354470650d51732c992a54",
                    "name": "Jiaying Wu",
                    "hidden": false
                },
                {
                    "_id": "68354470650d51732c992a55",
                    "name": "Yibo Li",
                    "hidden": false
                },
                {
                    "_id": "68354470650d51732c992a56",
                    "name": "Yue Liu",
                    "hidden": false
                },
                {
                    "_id": "68354470650d51732c992a57",
                    "user": {
                        "_id": "651d8032c50012d33e914f2f",
                        "avatarUrl": "/avatars/0a44c9f51fc50ce86582e328c361ea00.svg",
                        "isPro": false,
                        "fullname": "Bryan Hooi",
                        "user": "bhooi",
                        "type": "user"
                    },
                    "name": "Bryan Hooi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:53:11.481Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T13:18:37.000Z",
            "submittedOnDailyAt": "2025-05-27T07:30:37.766Z",
            "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
            "submittedOnDailyBy": {
                "_id": "61166c4328c98bfd5b92e7c5",
                "avatarUrl": "/avatars/f4bb0f0cc2c5b84428c28bddaa479b61.svg",
                "isPro": false,
                "fullname": "Hui Chen",
                "user": "chchenhui",
                "type": "user"
            },
            "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
            "upvotes": 8,
            "discussionId": "68354471650d51732c992a81",
            "githubRepo": "https://github.com/chchenhui/mlrbench",
            "ai_summary": "MLR-Bench evaluates AI agents in scientific research through modular stages, revealing that while LLMs perform well in ideation and writing, coding agents often produce unreliable experimental results.",
            "ai_keywords": [
                "MLR-Bench",
                "MLR-Judge",
                "MLR-Agent",
                "LLM-based reviewers",
                "review rubrics",
                "research evaluation",
                "idea generation",
                "proposal formulation",
                "experimentation",
                "paper writing"
            ]
        },
        "publishedAt": "2025-05-26T09:18:37.000Z",
        "title": "MLR-Bench: Evaluating AI Agents on Open-Ended Machine Learning Research",
        "summary": "Recent advancements in AI agents have demonstrated their growing potential to\ndrive and support scientific discovery. In this work, we introduce MLR-Bench, a\ncomprehensive benchmark for evaluating AI agents on open-ended machine learning\nresearch. MLR-Bench includes three key components: (1) 201 research tasks\nsourced from NeurIPS, ICLR, and ICML workshops covering diverse ML topics; (2)\nMLR-Judge, an automated evaluation framework combining LLM-based reviewers with\ncarefully designed review rubrics to assess research quality; and (3)\nMLR-Agent, a modular agent scaffold capable of completing research tasks\nthrough four stages: idea generation, proposal formulation, experimentation,\nand paper writing. Our framework supports both stepwise assessment across these\ndistinct research stages, and end-to-end evaluation of the final research\npaper. We then use MLR-Bench to evaluate six frontier LLMs and an advanced\ncoding agent, finding that while LLMs are effective at generating coherent\nideas and well-structured papers, current coding agents frequently (e.g., in\n80% of the cases) produce fabricated or invalidated experimental\nresults--posing a major barrier to scientific reliability. We validate\nMLR-Judge through human evaluation, showing high agreement with expert\nreviewers, supporting its potential as a scalable tool for research evaluation.\nWe open-source MLR-Bench to help the community benchmark, diagnose, and improve\nAI research agents toward trustworthy and transparent scientific discovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19955.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61166c4328c98bfd5b92e7c5",
            "avatarUrl": "/avatars/f4bb0f0cc2c5b84428c28bddaa479b61.svg",
            "fullname": "Hui Chen",
            "name": "chchenhui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19103",
            "authors": [
                {
                    "_id": "68354dbfb524d75f61faf04e",
                    "user": {
                        "_id": "658436f5c73f74776b19198a",
                        "avatarUrl": "/avatars/3f1d76af6fc0405d663c9294318fe83e.svg",
                        "isPro": false,
                        "fullname": "Iddo Yosha",
                        "user": "iyosha",
                        "type": "user"
                    },
                    "name": "Iddo Yosha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T15:49:57.797Z",
                    "hidden": false
                },
                {
                    "_id": "68354dbfb524d75f61faf04f",
                    "user": {
                        "_id": "65a8eb64c5ffe1d01910ab3e",
                        "avatarUrl": "/avatars/3a11fe375c4c469c6ef5272eaeb4ff67.svg",
                        "isPro": false,
                        "fullname": "Dorin Shteyman",
                        "user": "DorinSht",
                        "type": "user"
                    },
                    "name": "Dorin Shteyman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T15:49:53.004Z",
                    "hidden": false
                },
                {
                    "_id": "68354dbfb524d75f61faf050",
                    "name": "Yossi Adi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T11:45:08.000Z",
            "submittedOnDailyAt": "2025-05-27T14:12:23.679Z",
            "title": "WHISTRESS: Enriching Transcriptions with Sentence Stress Detection",
            "submittedOnDailyBy": {
                "_id": "66b9bc2dacdbc1d0b39c3b50",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
                "isPro": false,
                "fullname": "Gallil Maimon",
                "user": "gallilmaimon",
                "type": "user"
            },
            "summary": "Spoken language conveys meaning not only through words but also through\nintonation, emotion, and emphasis. Sentence stress, the emphasis placed on\nspecific words within a sentence, is crucial for conveying speaker intent and\nhas been extensively studied in linguistics. In this work, we introduce\nWHISTRESS, an alignment-free approach for enhancing transcription systems with\nsentence stress detection. To support this task, we propose TINYSTRESS-15K, a\nscalable, synthetic training data for the task of sentence stress detection\nwhich resulted from a fully automated dataset creation process. We train\nWHISTRESS on TINYSTRESS-15K and evaluate it against several competitive\nbaselines. Our results show that WHISTRESS outperforms existing methods while\nrequiring no additional input priors during training or inference. Notably,\ndespite being trained on synthetic data, WHISTRESS demonstrates strong\nzero-shot generalization across diverse benchmarks. Project page:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/whistress.",
            "upvotes": 8,
            "discussionId": "68354dbfb524d75f61faf073",
            "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/whistress/",
            "githubRepo": "https://github.com/slp-rl/WhiStress",
            "ai_summary": "WHISTRESS is an alignment-free method for sentence stress detection trained on synthetic data, outperforming existing methods and generalizing well to diverse benchmarks.",
            "ai_keywords": [
                "alignment-free",
                "sentence stress detection",
                "TINYSTRESS-15K",
                "synthetic training data",
                "zero-shot generalization"
            ]
        },
        "publishedAt": "2025-05-25T07:45:08.000Z",
        "title": "WHISTRESS: Enriching Transcriptions with Sentence Stress Detection",
        "summary": "Spoken language conveys meaning not only through words but also through\nintonation, emotion, and emphasis. Sentence stress, the emphasis placed on\nspecific words within a sentence, is crucial for conveying speaker intent and\nhas been extensively studied in linguistics. In this work, we introduce\nWHISTRESS, an alignment-free approach for enhancing transcription systems with\nsentence stress detection. To support this task, we propose TINYSTRESS-15K, a\nscalable, synthetic training data for the task of sentence stress detection\nwhich resulted from a fully automated dataset creation process. We train\nWHISTRESS on TINYSTRESS-15K and evaluate it against several competitive\nbaselines. Our results show that WHISTRESS outperforms existing methods while\nrequiring no additional input priors during training or inference. Notably,\ndespite being trained on synthetic data, WHISTRESS demonstrates strong\nzero-shot generalization across diverse benchmarks. Project page:\nhttps://pages.cs.huji.ac.il/adiyoss-lab/whistress.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19103.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "fullname": "Gallil Maimon",
            "name": "gallilmaimon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20278",
            "authors": [
                {
                    "_id": "68353261bc28496925a185c9",
                    "user": {
                        "_id": "64d0d6684dfd5df70744b237",
                        "avatarUrl": "/avatars/4ea57bfd407e8cb727c624f64af75478.svg",
                        "isPro": false,
                        "fullname": "Chang",
                        "user": "Hoyeon",
                        "type": "user"
                    },
                    "name": "Hoyeon Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:58:28.992Z",
                    "hidden": false
                },
                {
                    "_id": "68353261bc28496925a185ca",
                    "name": "Jinho Park",
                    "hidden": false
                },
                {
                    "_id": "68353261bc28496925a185cb",
                    "user": {
                        "_id": "6486e2e3c4799f43eaf3602c",
                        "avatarUrl": "/avatars/cfd3055e1f2604d07475b31f33d43d52.svg",
                        "isPro": false,
                        "fullname": "Hanseul Cho",
                        "user": "HanseulJo",
                        "type": "user"
                    },
                    "name": "Hanseul Cho",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:59:21.074Z",
                    "hidden": false
                },
                {
                    "_id": "68353261bc28496925a185cc",
                    "user": {
                        "_id": "606ae0adb579bb0e88515311",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617617060380-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Sohee Yang",
                        "user": "soheeyang",
                        "type": "user"
                    },
                    "name": "Sohee Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:59:14.789Z",
                    "hidden": false
                },
                {
                    "_id": "68353261bc28496925a185cd",
                    "user": {
                        "_id": "6485d617097c4b9f75d350e9",
                        "avatarUrl": "/avatars/8dfc0fa03a7d5846fd38014beafb0381.svg",
                        "isPro": false,
                        "fullname": "Miyoung Ko",
                        "user": "miyoungko",
                        "type": "user"
                    },
                    "name": "Miyoung Ko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:59:07.932Z",
                    "hidden": false
                },
                {
                    "_id": "68353261bc28496925a185ce",
                    "user": {
                        "_id": "647eaaf61a1fcad2fdc5d1ef",
                        "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
                        "isPro": false,
                        "fullname": "Hyeonbin Hwang ",
                        "user": "hbin0701",
                        "type": "user"
                    },
                    "name": "Hyeonbin Hwang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:59:02.515Z",
                    "hidden": false
                },
                {
                    "_id": "68353261bc28496925a185cf",
                    "name": "Seungpil Won",
                    "hidden": false
                },
                {
                    "_id": "68353261bc28496925a185d0",
                    "name": "Dohaeng Lee",
                    "hidden": false
                },
                {
                    "_id": "68353261bc28496925a185d1",
                    "user": {
                        "_id": "65386bbe6d1c2659e102bbbb",
                        "avatarUrl": "/avatars/6ed42e65f64cd72c7b3615c24beedb01.svg",
                        "isPro": false,
                        "fullname": "Youbin Ahn",
                        "user": "youbina",
                        "type": "user"
                    },
                    "name": "Youbin Ahn",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:58:47.652Z",
                    "hidden": false
                },
                {
                    "_id": "68353261bc28496925a185d2",
                    "user": {
                        "_id": "621f05ba970615ad5861ceb1",
                        "avatarUrl": "/avatars/7e1902aa71369a524afda9b0a9e88e22.svg",
                        "isPro": false,
                        "fullname": "Minjoon Seo",
                        "user": "minjoon",
                        "type": "user"
                    },
                    "name": "Minjoon Seo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:58:41.960Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:55:15.000Z",
            "submittedOnDailyAt": "2025-05-27T02:04:40.615Z",
            "title": "The Coverage Principle: A Framework for Understanding Compositional\n  Generalization",
            "submittedOnDailyBy": {
                "_id": "64d0d6684dfd5df70744b237",
                "avatarUrl": "/avatars/4ea57bfd407e8cb727c624f64af75478.svg",
                "isPro": false,
                "fullname": "Chang",
                "user": "Hoyeon",
                "type": "user"
            },
            "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\nmechanism-based taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality.",
            "upvotes": 7,
            "discussionId": "68353261bc28496925a185ef",
            "ai_summary": "The coverage principle highlights limitations in Transformers' compositional generalization, emphasizing the need for new architectures or training methods to achieve systematic compositionality by distinguishing different mechanisms of generalization.",
            "ai_keywords": [
                "coverage principle",
                "data-centric framework",
                "sequential application",
                "pattern matching",
                "compositional generalization",
                "Transformers",
                "two-hop generalization",
                "token set size",
                "training data efficiency",
                "context-dependent state representations",
                "performance",
                "interoperability",
                "Chain-of-Thought supervision",
                "multi-hop tasks",
                "path ambiguity",
                "structure-based",
                "property-based",
                "shared-operator",
                "architectural innovations"
            ]
        },
        "publishedAt": "2025-05-26T13:55:15.000Z",
        "title": "The Coverage Principle: A Framework for Understanding Compositional\n  Generalization",
        "summary": "Large language models excel at pattern matching, yet often fall short in\nsystematic compositional generalization. We propose the coverage principle: a\ndata-centric framework showing that models relying primarily on pattern\nmatching for compositional tasks cannot reliably generalize beyond substituting\nfragments that yield identical results when used in the same contexts. We\ndemonstrate that this framework has a strong predictive power for the\ngeneralization capabilities of Transformers. First, we derive and empirically\nconfirm that the training data required for two-hop generalization grows at\nleast quadratically with the token set size, and the training data efficiency\ndoes not improve with 20x parameter scaling. Second, for compositional tasks\nwith path ambiguity where one variable affects the output through multiple\ncomputational paths, we show that Transformers learn context-dependent state\nrepresentations that undermine both performance and interoperability. Third,\nChain-of-Thought supervision improves training data efficiency for multi-hop\ntasks but still struggles with path ambiguity. Finally, we outline a\nmechanism-based taxonomy that distinguishes three ways neural networks\ncan generalize: structure-based (bounded by coverage), property-based\n(leveraging algebraic invariances), and shared-operator (through function\nreuse). This conceptual lens contextualizes our results and highlights where\nnew architectural ideas are needed to achieve systematic compositionally.\nOverall, the coverage principle provides a unified lens for understanding\ncompositional reasoning, and underscores the need for fundamental architectural\nor training innovations to achieve truly systematic compositionality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20278.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64d0d6684dfd5df70744b237",
            "avatarUrl": "/avatars/4ea57bfd407e8cb727c624f64af75478.svg",
            "fullname": "Chang",
            "name": "Hoyeon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19223",
            "authors": [
                {
                    "_id": "68357a21d0fbc64a8e829088",
                    "name": "Fengqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e829089",
                    "name": "Rongzhen Wang",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e82908a",
                    "name": "Shen Nie",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e82908b",
                    "user": {
                        "_id": "67513d6d3b8586521cda5d76",
                        "avatarUrl": "/avatars/0f95cc5c23a0a1da289aa785bd33b616.svg",
                        "isPro": false,
                        "fullname": "Xiaolu  Zhang",
                        "user": "xiaolu0714",
                        "type": "user"
                    },
                    "name": "Xiaolu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:45:40.970Z",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e82908c",
                    "name": "Chunwei Wu",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e82908d",
                    "name": "Jun Hu",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e82908e",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e82908f",
                    "user": {
                        "_id": "65fcad0ba0d7adc40b54fac2",
                        "avatarUrl": "/avatars/7564b5642378fddb46ec3b5ae57c0402.svg",
                        "isPro": false,
                        "fullname": "Jianfei Chen",
                        "user": "surfingtomchen",
                        "type": "user"
                    },
                    "name": "Jianfei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:45:00.594Z",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e829090",
                    "user": {
                        "_id": "657a651e1433ea7d44de6397",
                        "avatarUrl": "/avatars/ccfc76f94595a38ff4a80f77c911eabf.svg",
                        "isPro": false,
                        "fullname": "Yankai Lin",
                        "user": "lyk423",
                        "type": "user"
                    },
                    "name": "Yankai Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:44:53.835Z",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e829091",
                    "user": {
                        "_id": "64b8c89052b7353d8c6a1013",
                        "avatarUrl": "/avatars/cd59fffe81f6b07b4519540b8ff3d95f.svg",
                        "isPro": false,
                        "fullname": "Ji-Rong Wen",
                        "user": "jrwen",
                        "type": "user"
                    },
                    "name": "Ji-Rong Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:44:47.347Z",
                    "hidden": false
                },
                {
                    "_id": "68357a21d0fbc64a8e829092",
                    "user": {
                        "_id": "64c07b488e2612254361153b",
                        "avatarUrl": "/avatars/ade0f783cc4c2d3e73f402637f595471.svg",
                        "isPro": false,
                        "fullname": "chongxuan li",
                        "user": "zhenxuan00",
                        "type": "user"
                    },
                    "name": "Chongxuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T08:44:37.114Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/HQWTRZ5gL3-RFJ6PSJ3NC.jpeg"
            ],
            "publishedAt": "2025-05-25T16:36:20.000Z",
            "submittedOnDailyAt": "2025-05-27T07:14:06.300Z",
            "title": "LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language\n  Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
                "isPro": false,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "While Masked Diffusion Models (MDMs), such as LLaDA, present a promising\nparadigm for language modeling, there has been relatively little effort in\naligning these models with human preferences via reinforcement learning. The\nchallenge primarily arises from the high variance in Evidence Lower Bound\n(ELBO)-based likelihood estimates required for preference optimization. To\naddress this issue, we propose Variance-Reduced Preference Optimization (VRPO),\na framework that formally analyzes the variance of ELBO estimators and derives\nbounds on both the bias and variance of preference optimization gradients.\nBuilding on this theoretical foundation, we introduce unbiased variance\nreduction strategies, including optimal Monte Carlo budget allocation and\nantithetic sampling, that significantly improve the performance of MDM\nalignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA,\nand the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor\nconsistently and significantly across mathematical (GSM8K +4.7), code\n(HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard\n+4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical\nperformance compared to strong language MDMs and ARMs. Project page:\nhttps://ml-gsai.github.io/LLaDA-1.5-Demo/.",
            "upvotes": 7,
            "discussionId": "68357a21d0fbc64a8e8290ba",
            "ai_summary": "VRPO is a variance-reduced preference optimization framework for Masked Diffusion Models that significantly enhances their alignment with human preferences and performance across various benchmarks.",
            "ai_keywords": [
                "Masked Diffusion Models",
                "LLaDA",
                "Variance-Reduced Preference Optimization",
                "VRPO",
                "Evidence Lower Bound",
                "ELBO",
                "bias",
                "variance",
                "preference optimization",
                "unbiased variance reduction",
                "optimal Monte Carlo budget allocation",
                "antithetic sampling",
                "GSM8K",
                "HumanEval",
                "MBPP",
                "IFEval",
                "Arena-Hard",
                "ARMs"
            ]
        },
        "publishedAt": "2025-05-25T12:36:20.000Z",
        "title": "LLaDA 1.5: Variance-Reduced Preference Optimization for Large Language\n  Diffusion Models",
        "summary": "While Masked Diffusion Models (MDMs), such as LLaDA, present a promising\nparadigm for language modeling, there has been relatively little effort in\naligning these models with human preferences via reinforcement learning. The\nchallenge primarily arises from the high variance in Evidence Lower Bound\n(ELBO)-based likelihood estimates required for preference optimization. To\naddress this issue, we propose Variance-Reduced Preference Optimization (VRPO),\na framework that formally analyzes the variance of ELBO estimators and derives\nbounds on both the bias and variance of preference optimization gradients.\nBuilding on this theoretical foundation, we introduce unbiased variance\nreduction strategies, including optimal Monte Carlo budget allocation and\nantithetic sampling, that significantly improve the performance of MDM\nalignment. We demonstrate the effectiveness of VRPO by applying it to LLaDA,\nand the resulting model, LLaDA 1.5, outperforms its SFT-only predecessor\nconsistently and significantly across mathematical (GSM8K +4.7), code\n(HumanEval +3.0, MBPP +1.8), and alignment benchmarks (IFEval +4.0, Arena-Hard\n+4.3). Furthermore, LLaDA 1.5 demonstrates a highly competitive mathematical\nperformance compared to strong language MDMs and ARMs. Project page:\nhttps://ml-gsai.github.io/LLaDA-1.5-Demo/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/HQWTRZ5gL3-RFJ6PSJ3NC.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19223.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 703
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.19731",
            "authors": [
                {
                    "_id": "683588a1650d51732cab05de",
                    "user": {
                        "_id": "6262880c5eb4fa93219f0064",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6262880c5eb4fa93219f0064/6yyBvRK4Oh7OhjaaweaVN.jpeg",
                        "isPro": false,
                        "fullname": "Daniil Tiapkin",
                        "user": "dtiapkin",
                        "type": "user"
                    },
                    "name": "Daniil Tiapkin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T10:03:29.430Z",
                    "hidden": false
                },
                {
                    "_id": "683588a1650d51732cab05df",
                    "name": "Daniele Calandriello",
                    "hidden": false
                },
                {
                    "_id": "683588a1650d51732cab05e0",
                    "name": "Denis Belomestny",
                    "hidden": false
                },
                {
                    "_id": "683588a1650d51732cab05e1",
                    "name": "Eric Moulines",
                    "hidden": false
                },
                {
                    "_id": "683588a1650d51732cab05e2",
                    "name": "Alexey Naumov",
                    "hidden": false
                },
                {
                    "_id": "683588a1650d51732cab05e3",
                    "user": {
                        "_id": "629f3b18ee05727ce328ccbe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669189789447-629f3b18ee05727ce328ccbe.jpeg",
                        "isPro": false,
                        "fullname": "Kashif Rasul",
                        "user": "kashif",
                        "type": "user"
                    },
                    "name": "Kashif Rasul",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-27T09:40:50.326Z",
                    "hidden": false
                },
                {
                    "_id": "683588a1650d51732cab05e4",
                    "user": {
                        "_id": "651e97156d92456bdf5ace6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e97156d92456bdf5ace6b/KKfdZGPAcWPdqycp9SulH.jpeg",
                        "isPro": false,
                        "fullname": "Michal Valko",
                        "user": "misovalko",
                        "type": "user"
                    },
                    "name": "Michal Valko",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-27T10:00:09.731Z",
                    "hidden": false
                },
                {
                    "_id": "683588a1650d51732cab05e5",
                    "name": "Pierre Menard",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6262880c5eb4fa93219f0064/F7nWvm2sO5QXTLRnB1k6e.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6262880c5eb4fa93219f0064/Fuzzt-cOiMSOCevLVjkW-.png"
            ],
            "publishedAt": "2025-05-26T09:17:32.000Z",
            "submittedOnDailyAt": "2025-05-27T08:13:23.799Z",
            "title": "Accelerating Nash Learning from Human Feedback via Mirror Prox",
            "submittedOnDailyBy": {
                "_id": "6262880c5eb4fa93219f0064",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6262880c5eb4fa93219f0064/6yyBvRK4Oh7OhjaaweaVN.jpeg",
                "isPro": false,
                "fullname": "Daniil Tiapkin",
                "user": "dtiapkin",
                "type": "user"
            },
            "summary": "Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on\nreward models, frequently assuming preference structures like the Bradley-Terry\nmodel, which may not accurately capture the complexities of real human\npreferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF)\noffers a more direct alternative by framing the problem as finding a Nash\nequilibrium of a game defined by these preferences. In this work, we introduce\nNash Mirror Prox (Nash-MP), an online NLHF algorithm that leverages\nthe Mirror Prox optimization scheme to achieve fast and stable convergence to\nthe Nash equilibrium. Our theoretical analysis establishes that Nash-MP\nexhibits last-iterate linear convergence towards the beta-regularized Nash\nequilibrium. Specifically, we prove that the KL-divergence to the optimal\npolicy decreases at a rate of order (1+2beta)^{-N/2}, where N is a number\nof preference queries. We further demonstrate last-iterate linear convergence\nfor the exploitability gap and uniformly for the span semi-norm of\nlog-probabilities, with all these rates being independent of the size of the\naction space. Furthermore, we propose and analyze an approximate version of\nNash-MP where proximal steps are estimated using stochastic policy gradients,\nmaking the algorithm closer to applications. Finally, we detail a practical\nimplementation strategy for fine-tuning large language models and present\nexperiments that demonstrate its competitive performance and compatibility with\nexisting methods.",
            "upvotes": 6,
            "discussionId": "683588a2650d51732cab0612",
            "ai_summary": "Nash Mirror Prox is an online algorithm for Nash Learning from Human Feedback that achieves linear convergence to the Nash equilibrium and is applicable for fine-tuning language models.",
            "ai_keywords": [
                "Nash Learning from Human Feedback",
                "Nash Mirror Prox",
                "Mirror Prox",
                "KL-divergence",
                "Nash equilibrium",
                "exploitability gap",
                "span semi-norm",
                "log-probabilities",
                "stochastic policy gradients",
                "fine-tuning",
                "large language models"
            ]
        },
        "publishedAt": "2025-05-26T05:17:32.000Z",
        "title": "Accelerating Nash Learning from Human Feedback via Mirror Prox",
        "summary": "Traditional Reinforcement Learning from Human Feedback (RLHF) often relies on\nreward models, frequently assuming preference structures like the Bradley-Terry\nmodel, which may not accurately capture the complexities of real human\npreferences (e.g., intransitivity). Nash Learning from Human Feedback (NLHF)\noffers a more direct alternative by framing the problem as finding a Nash\nequilibrium of a game defined by these preferences. In this work, we introduce\nNash Mirror Prox (Nash-MP), an online NLHF algorithm that leverages\nthe Mirror Prox optimization scheme to achieve fast and stable convergence to\nthe Nash equilibrium. Our theoretical analysis establishes that Nash-MP\nexhibits last-iterate linear convergence towards the beta-regularized Nash\nequilibrium. Specifically, we prove that the KL-divergence to the optimal\npolicy decreases at a rate of order (1+2beta)^{-N/2}, where N is a number\nof preference queries. We further demonstrate last-iterate linear convergence\nfor the exploitability gap and uniformly for the span semi-norm of\nlog-probabilities, with all these rates being independent of the size of the\naction space. Furthermore, we propose and analyze an approximate version of\nNash-MP where proximal steps are estimated using stochastic policy gradients,\nmaking the algorithm closer to applications. Finally, we detail a practical\nimplementation strategy for fine-tuning large language models and present\nexperiments that demonstrate its competitive performance and compatibility with\nexisting methods.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6262880c5eb4fa93219f0064/F7nWvm2sO5QXTLRnB1k6e.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6262880c5eb4fa93219f0064/Fuzzt-cOiMSOCevLVjkW-.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19731.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6262880c5eb4fa93219f0064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6262880c5eb4fa93219f0064/6yyBvRK4Oh7OhjaaweaVN.jpeg",
            "fullname": "Daniil Tiapkin",
            "name": "dtiapkin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18773",
            "authors": [
                {
                    "_id": "6835727f9da2b91fb4e30473",
                    "name": "Jamie Hayes",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e30474",
                    "user": {
                        "_id": "6475c2794766357252e69e9f",
                        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                        "isPro": false,
                        "fullname": "i",
                        "user": "iliashum",
                        "type": "user"
                    },
                    "name": "Ilia Shumailov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:44:17.787Z",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e30475",
                    "name": "Christopher A. Choquette-Choo",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e30476",
                    "user": {
                        "_id": "654a4a3b6a46bad8c0f2ab80",
                        "avatarUrl": "/avatars/08caf74525ea3e46fe58c20eb1f3d64d.svg",
                        "isPro": false,
                        "fullname": "Matthew Jagielski",
                        "user": "jagielski",
                        "type": "user"
                    },
                    "name": "Matthew Jagielski",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:44:27.202Z",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e30477",
                    "name": "George Kaissis",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e30478",
                    "name": "Katherine Lee",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e30479",
                    "name": "Milad Nasr",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e3047a",
                    "user": {
                        "_id": "633a01c6c0b7fd23493cb934",
                        "avatarUrl": "/avatars/8592c548b2f8324c43c5fed2599e6074.svg",
                        "isPro": false,
                        "fullname": "Sahra Ghalebikesabi",
                        "user": "sahra",
                        "type": "user"
                    },
                    "name": "Sahra Ghalebikesabi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:44:52.007Z",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e3047b",
                    "name": "Niloofar Mireshghallah",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e3047c",
                    "name": "Meenatchi Sundaram Mutu Selva Annamalai",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e3047d",
                    "user": {
                        "_id": "6579c19a7a0f9b8a1abdccf1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vhsOT4mneeJX0KtFi-uDb.jpeg",
                        "isPro": false,
                        "fullname": "Igor Shilov",
                        "user": "ffuuugor",
                        "type": "user"
                    },
                    "name": "Igor Shilov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:45:06.168Z",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e3047e",
                    "name": "Matthieu Meeus",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e3047f",
                    "user": {
                        "_id": "6694f6b3284bcf975861b818",
                        "avatarUrl": "/avatars/37cd86306142027663b6471b0033ef58.svg",
                        "isPro": false,
                        "fullname": "Yves-Alexandre de Montjoye",
                        "user": "yvesalexandre",
                        "type": "user"
                    },
                    "name": "Yves-Alexandre de Montjoye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:45:18.016Z",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e30480",
                    "name": "Franziska Boenisch",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e30481",
                    "name": "Adam Dziedzic",
                    "hidden": false
                },
                {
                    "_id": "6835727f9da2b91fb4e30482",
                    "user": {
                        "_id": "663c3b587e7bc3d3e4a54ffb",
                        "avatarUrl": "/avatars/681abf7e4a85184667015cefefa226c6.svg",
                        "isPro": false,
                        "fullname": "A. Feder Cooper",
                        "user": "pasta41",
                        "type": "user"
                    },
                    "name": "A. Feder Cooper",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-27T20:00:14.988Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T16:23:43.000Z",
            "submittedOnDailyAt": "2025-05-27T06:39:29.837Z",
            "title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately)\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training reference models (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small-scale models and\ndatasets. However, weaker attacks have been shown to be brittle - achieving\nclose-to-arbitrary success - and insights from strong attacks in simplified\nsettings do not translate to today's LLMs. These challenges have prompted an\nimportant question: are the limitations observed in prior work due to attack\ndesign choices, or are MIAs fundamentally ineffective on LLMs? We address this\nquestion by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures\nranging from 10M to 1B parameters, training reference models on over 20B tokens\nfrom the C4 dataset. Our results advance the understanding of MIAs on LLMs in\nthree key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their\neffectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;\nand, (3) the relationship between MIA success and related privacy metrics is\nnot as straightforward as prior work has suggested.",
            "upvotes": 6,
            "discussionId": "683572809da2b91fb4e30513",
            "ai_summary": "Scaling LiRA membership inference attacks to large pre-trained language models shows that while these attacks can succeed, their effectiveness is limited and does not definitively correlate with privacy metrics.",
            "ai_keywords": [
                "membership inference attacks",
                "MIAs",
                "reference models",
                "fine-tuning attacks",
                "pre-trained language models",
                "LLMs",
                "LiRA",
                "GPT-2",
                "tokens",
                "C4 dataset",
                "AUC",
                "privacy metrics"
            ]
        },
        "publishedAt": "2025-05-24T12:23:43.000Z",
        "title": "Strong Membership Inference Attacks on Massive Datasets and (Moderately)\n  Large Language Models",
        "summary": "State-of-the-art membership inference attacks (MIAs) typically require\ntraining many reference models, making it difficult to scale these attacks to\nlarge pre-trained language models (LLMs). As a result, prior research has\neither relied on weaker attacks that avoid training reference models (e.g.,\nfine-tuning attacks), or on stronger attacks applied to small-scale models and\ndatasets. However, weaker attacks have been shown to be brittle - achieving\nclose-to-arbitrary success - and insights from strong attacks in simplified\nsettings do not translate to today's LLMs. These challenges have prompted an\nimportant question: are the limitations observed in prior work due to attack\ndesign choices, or are MIAs fundamentally ineffective on LLMs? We address this\nquestion by scaling LiRA - one of the strongest MIAs - to GPT-2 architectures\nranging from 10M to 1B parameters, training reference models on over 20B tokens\nfrom the C4 dataset. Our results advance the understanding of MIAs on LLMs in\nthree key ways: (1) strong MIAs can succeed on pre-trained LLMs; (2) their\neffectiveness, however, remains limited (e.g., AUC<0.7) in practical settings;\nand, (3) the relationship between MIA success and related privacy metrics is\nnot as straightforward as prior work has suggested.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18773.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18384",
            "authors": [
                {
                    "_id": "68354f30d795fadab0623699",
                    "user": {
                        "_id": "65319bd7f85995389d4f019c",
                        "avatarUrl": "/avatars/657858b8435b220c9a29918c0dae9c6d.svg",
                        "isPro": false,
                        "fullname": "Boyi Wei",
                        "user": "boyiwei",
                        "type": "user"
                    },
                    "name": "Boyi Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:49:18.459Z",
                    "hidden": false
                },
                {
                    "_id": "68354f30d795fadab062369a",
                    "user": {
                        "_id": "65a6869121943858bd7c8ec6",
                        "avatarUrl": "/avatars/179fa8fb6aa8fbd960aae9c3b4c201a0.svg",
                        "isPro": false,
                        "fullname": "Benedikt Stroebl",
                        "user": "benediktstroebl",
                        "type": "user"
                    },
                    "name": "Benedikt Stroebl",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:43:02.035Z",
                    "hidden": false
                },
                {
                    "_id": "68354f30d795fadab062369b",
                    "user": {
                        "_id": "639a21a19416d271ac6a2b80",
                        "avatarUrl": "/avatars/00f644d5fb82222999e1b7d1bd2afcfa.svg",
                        "isPro": false,
                        "fullname": "Jiacen Xu",
                        "user": "c0ldstudy",
                        "type": "user"
                    },
                    "name": "Jiacen Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T15:49:51.045Z",
                    "hidden": false
                },
                {
                    "_id": "68354f30d795fadab062369c",
                    "user": {
                        "_id": "6633e97bf5fcd0f26eead20e",
                        "avatarUrl": "/avatars/4b12aeaf2b1960386e8ac500a93acf59.svg",
                        "isPro": false,
                        "fullname": "Joie Zhang",
                        "user": "joie-zhang",
                        "type": "user"
                    },
                    "name": "Joie Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:43:39.808Z",
                    "hidden": false
                },
                {
                    "_id": "68354f30d795fadab062369d",
                    "name": "Zhou Li",
                    "hidden": false
                },
                {
                    "_id": "68354f30d795fadab062369e",
                    "user": {
                        "_id": "617aafa1ff3db6021d069787",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1644543485859-617aafa1ff3db6021d069787.jpeg",
                        "isPro": false,
                        "fullname": "Peter Henderson",
                        "user": "breakend",
                        "type": "user"
                    },
                    "name": "Peter Henderson",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:43:32.665Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T21:18:59.000Z",
            "submittedOnDailyAt": "2025-05-27T04:06:40.688Z",
            "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents",
            "submittedOnDailyBy": {
                "_id": "65319bd7f85995389d4f019c",
                "avatarUrl": "/avatars/657858b8435b220c9a29918c0dae9c6d.svg",
                "isPro": false,
                "fullname": "Boyi Wei",
                "user": "boyiwei",
                "type": "user"
            },
            "summary": "Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk.",
            "upvotes": 5,
            "discussionId": "68354f30d795fadab06236fe",
            "githubRepo": "https://github.com/boyiwei/Dynamic-Risk-Assessment",
            "ai_summary": "Adversaries can significantly enhance foundation model capabilities in offensive cybersecurity with limited computational resources, underscoring the need for dynamic threat model assessments.",
            "ai_keywords": [
                "foundation models",
                "autonomous programmers",
                "offensive cybersecurity",
                "model audits",
                "cybersecurity risks",
                "verifiers",
                "financial incentives",
                "iterative improvement",
                "threat model",
                "stateful environments",
                "non-stateful environments",
                "compute budget",
                "InterCode CTF"
            ]
        },
        "publishedAt": "2025-05-23T17:18:59.000Z",
        "title": "Dynamic Risk Assessments for Offensive Cybersecurity Agents",
        "summary": "Foundation models are increasingly becoming better autonomous programmers,\nraising the prospect that they could also automate dangerous offensive\ncyber-operations. Current frontier model audits probe the cybersecurity risks\nof such agents, but most fail to account for the degrees of freedom available\nto adversaries in the real world. In particular, with strong verifiers and\nfinancial incentives, agents for offensive cybersecurity are amenable to\niterative improvement by would-be adversaries. We argue that assessments should\ntake into account an expanded threat model in the context of cybersecurity,\nemphasizing the varying degrees of freedom that an adversary may possess in\nstateful and non-stateful environments within a fixed compute budget. We show\nthat even with a relatively small compute budget (8 H100 GPU Hours in our\nstudy), adversaries can improve an agent's cybersecurity capability on\nInterCode CTF by more than 40\\% relative to the baseline -- without any\nexternal assistance. These results highlight the need to evaluate agents'\ncybersecurity risk in a dynamic manner, painting a more representative picture\nof risk.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18384.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65319bd7f85995389d4f019c",
            "avatarUrl": "/avatars/657858b8435b220c9a29918c0dae9c6d.svg",
            "fullname": "Boyi Wei",
            "name": "boyiwei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.15804",
            "authors": [
                {
                    "_id": "6835adccc9704b79b7f18dd0",
                    "user": {
                        "_id": "65def3cfe8e604a7b6f39681",
                        "avatarUrl": "/avatars/c1ce791f5513d934c5f6426bd17e4fbd.svg",
                        "isPro": false,
                        "fullname": "lzz",
                        "user": "lizongzhao",
                        "type": "user"
                    },
                    "name": "Zongzhao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:49:37.146Z",
                    "hidden": false
                },
                {
                    "_id": "6835adccc9704b79b7f18dd1",
                    "user": {
                        "_id": "61bb00f6c4ac95d207b25f1b",
                        "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg",
                        "isPro": false,
                        "fullname": "Zongyang Ma",
                        "user": "zyma",
                        "type": "user"
                    },
                    "name": "Zongyang Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:49:26.103Z",
                    "hidden": false
                },
                {
                    "_id": "6835adccc9704b79b7f18dd2",
                    "name": "Mingze Li",
                    "hidden": false
                },
                {
                    "_id": "6835adccc9704b79b7f18dd3",
                    "user": {
                        "_id": "67d92d6061f369297a4a225a",
                        "avatarUrl": "/avatars/af71c847d8122c827b22ce52d4c5af71.svg",
                        "isPro": false,
                        "fullname": "Songyou Li",
                        "user": "Wthinker",
                        "type": "user"
                    },
                    "name": "Songyou Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:49:52.377Z",
                    "hidden": false
                },
                {
                    "_id": "6835adccc9704b79b7f18dd4",
                    "name": "Yu Rong",
                    "hidden": false
                },
                {
                    "_id": "6835adccc9704b79b7f18dd5",
                    "user": {
                        "_id": "67a5a25269f568c7eb4173cd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IFzcHm_K8s2UxTRCC79Xf.png",
                        "isPro": false,
                        "fullname": "Tingyang Xu",
                        "user": "xuty007",
                        "type": "user"
                    },
                    "name": "Tingyang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:49:58.919Z",
                    "hidden": false
                },
                {
                    "_id": "6835adccc9704b79b7f18dd6",
                    "name": "Ziqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6835adccc9704b79b7f18dd7",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "6835adccc9704b79b7f18dd8",
                    "name": "Wenbing Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T17:57:38.000Z",
            "submittedOnDailyAt": "2025-05-27T11:11:13.707Z",
            "title": "STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs",
            "submittedOnDailyBy": {
                "_id": "61bb00f6c4ac95d207b25f1b",
                "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg",
                "isPro": false,
                "fullname": "Zongyang Ma",
                "user": "zyma",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.",
            "upvotes": 5,
            "discussionId": "6835adcdc9704b79b7f18e23",
            "githubRepo": "https://github.com/zongzhao23/STAR-R1",
            "ai_summary": "STAR-R1, a novel RL framework with a fine-grained reward mechanism, enhances spatial reasoning in multimodal large language models by addressing limitations in traditional SFT and sparse-reward RL.",
            "ai_keywords": [
                "Transformation-Driven Visual Reasoning",
                "Supervised Fine-Tuning",
                "Reinforcement Learning",
                "single-stage RL",
                "fine-grained reward mechanism",
                "partial correctness",
                "excessive enumeration",
                "spatial reasoning",
                "multimodal large language models"
            ]
        },
        "publishedAt": "2025-05-21T13:57:38.000Z",
        "title": "STAR-R1: Spatial TrAnsformation Reasoning by Reinforcing Multimodal LLMs",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities across diverse tasks, yet they lag significantly behind humans in\nspatial reasoning. We investigate this gap through Transformation-Driven Visual\nReasoning (TVR), a challenging task requiring identification of object\ntransformations across images under varying viewpoints. While traditional\nSupervised Fine-Tuning (SFT) fails to generate coherent reasoning paths in\ncross-view settings, sparse-reward Reinforcement Learning (RL) suffers from\ninefficient exploration and slow convergence. To address these limitations, we\npropose STAR-R1, a novel framework that integrates a single-stage RL paradigm\nwith a fine-grained reward mechanism tailored for TVR. Specifically, STAR-R1\nrewards partial correctness while penalizing excessive enumeration and passive\ninaction, enabling efficient exploration and precise reasoning. Comprehensive\nevaluations demonstrate that STAR-R1 achieves state-of-the-art performance\nacross all 11 metrics, outperforming SFT by 23% in cross-view scenarios.\nFurther analysis reveals STAR-R1's anthropomorphic behavior and highlights its\nunique ability to compare all objects for improving spatial reasoning. Our work\nprovides critical insights in advancing the research of MLLMs and reasoning\nmodels. The codes, model weights, and data will be publicly available at\nhttps://github.com/zongzhao23/STAR-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15804.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61bb00f6c4ac95d207b25f1b",
            "avatarUrl": "/avatars/3b6eba701d64518d6f694942f5b2e9a9.svg",
            "fullname": "Zongyang Ma",
            "name": "zyma",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19084",
            "authors": [
                {
                    "_id": "6835334e0c0aff775f3eb6e2",
                    "user": {
                        "_id": "640c64779e5247967ff1e0b2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678533946170-640c64779e5247967ff1e0b2.jpeg",
                        "isPro": false,
                        "fullname": "Yifeng Xu",
                        "user": "xyfJASON",
                        "type": "user"
                    },
                    "name": "Yifeng Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T08:15:08.170Z",
                    "hidden": false
                },
                {
                    "_id": "6835334e0c0aff775f3eb6e3",
                    "name": "Zhenliang He",
                    "hidden": false
                },
                {
                    "_id": "6835334e0c0aff775f3eb6e4",
                    "name": "Meina Kan",
                    "hidden": false
                },
                {
                    "_id": "6835334e0c0aff775f3eb6e5",
                    "name": "Shiguang Shan",
                    "hidden": false
                },
                {
                    "_id": "6835334e0c0aff775f3eb6e6",
                    "name": "Xilin Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/Mqa2jx5wM-f5Fc1pdd6sz.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/-ubqvKPbhrxAjIKnj7dPU.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/Y4ub2WOy0Adp1TfcT90R3.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/lvdM2FUHFI2ut7cgELTh9.jpeg"
            ],
            "publishedAt": "2025-05-25T10:40:52.000Z",
            "submittedOnDailyAt": "2025-05-27T07:40:01.653Z",
            "title": "Jodi: Unification of Visual Generation and Understanding via Joint\n  Modeling",
            "submittedOnDailyBy": {
                "_id": "640c64779e5247967ff1e0b2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678533946170-640c64779e5247967ff1e0b2.jpeg",
                "isPro": false,
                "fullname": "Yifeng Xu",
                "user": "xyfJASON",
                "type": "user"
            },
            "summary": "Visual generation and understanding are two deeply interconnected aspects of\nhuman intelligence, yet they have been traditionally treated as separate tasks\nin machine learning. In this paper, we propose Jodi, a diffusion framework that\nunifies visual generation and understanding by jointly modeling the image\ndomain and multiple label domains. Specifically, Jodi is built upon a linear\ndiffusion transformer along with a role switch mechanism, which enables it to\nperform three particular types of tasks: (1) joint generation, where the model\nsimultaneously generates images and multiple labels; (2) controllable\ngeneration, where images are generated conditioned on any combination of\nlabels; and (3) image perception, where multiple labels can be predicted at\nonce from a given image. Furthermore, we present the Joint-1.6M dataset, which\ncontains 200,000 high-quality images collected from public sources, automatic\nlabels for 7 visual domains, and LLM-generated captions. Extensive experiments\ndemonstrate that Jodi excels in both generation and understanding tasks and\nexhibits strong extensibility to a wider range of visual domains. Code is\navailable at https://github.com/VIPL-GENUN/Jodi.",
            "upvotes": 4,
            "discussionId": "683533510c0aff775f3eb7ab",
            "ai_summary": "Jodi, a diffusion framework using a linear diffusion transformer and role switch mechanism, unifies visual generation and understanding, performing joint, controllable, and perceptual tasks effectively across multiple visual domains.",
            "ai_keywords": [
                "diffusion framework",
                "linear diffusion transformer",
                "role switch mechanism",
                "joint generation",
                "controllable generation",
                "image perception",
                "Joint-1.6M dataset",
                "visual domains",
                "LLM-generated captions"
            ]
        },
        "publishedAt": "2025-05-25T06:40:52.000Z",
        "title": "Jodi: Unification of Visual Generation and Understanding via Joint\n  Modeling",
        "summary": "Visual generation and understanding are two deeply interconnected aspects of\nhuman intelligence, yet they have been traditionally treated as separate tasks\nin machine learning. In this paper, we propose Jodi, a diffusion framework that\nunifies visual generation and understanding by jointly modeling the image\ndomain and multiple label domains. Specifically, Jodi is built upon a linear\ndiffusion transformer along with a role switch mechanism, which enables it to\nperform three particular types of tasks: (1) joint generation, where the model\nsimultaneously generates images and multiple labels; (2) controllable\ngeneration, where images are generated conditioned on any combination of\nlabels; and (3) image perception, where multiple labels can be predicted at\nonce from a given image. Furthermore, we present the Joint-1.6M dataset, which\ncontains 200,000 high-quality images collected from public sources, automatic\nlabels for 7 visual domains, and LLM-generated captions. Extensive experiments\ndemonstrate that Jodi excels in both generation and understanding tasks and\nexhibits strong extensibility to a wider range of visual domains. Code is\navailable at https://github.com/VIPL-GENUN/Jodi.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/Mqa2jx5wM-f5Fc1pdd6sz.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/-ubqvKPbhrxAjIKnj7dPU.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/Y4ub2WOy0Adp1TfcT90R3.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/640c64779e5247967ff1e0b2/lvdM2FUHFI2ut7cgELTh9.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19084.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640c64779e5247967ff1e0b2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678533946170-640c64779e5247967ff1e0b2.jpeg",
            "fullname": "Yifeng Xu",
            "name": "xyfJASON",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18926",
            "authors": [
                {
                    "_id": "6835fb623d51d52701b0eaeb",
                    "name": "Jingxuan Xu",
                    "hidden": false
                },
                {
                    "_id": "6835fb623d51d52701b0eaec",
                    "name": "Hong Huang",
                    "hidden": false
                },
                {
                    "_id": "6835fb623d51d52701b0eaed",
                    "name": "Chuhang Zou",
                    "hidden": false
                },
                {
                    "_id": "6835fb623d51d52701b0eaee",
                    "name": "Manolis Savva",
                    "hidden": false
                },
                {
                    "_id": "6835fb623d51d52701b0eaef",
                    "name": "Yunchao Wei",
                    "hidden": false
                },
                {
                    "_id": "6835fb623d51d52701b0eaf0",
                    "name": "Wuyang Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64df97c628d5d234ce0bf83c/uyWW3AbZQIcLfthwruoRc.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64df97c628d5d234ce0bf83c/4tjO6kRMK9zv8-GOGtczG.png"
            ],
            "publishedAt": "2025-05-25T01:27:18.000Z",
            "submittedOnDailyAt": "2025-05-27T16:23:36.909Z",
            "title": "Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time",
            "submittedOnDailyBy": {
                "_id": "64df97c628d5d234ce0bf83c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64df97c628d5d234ce0bf83c/BIEH7Ep4ffytiAcUIPFe-.jpeg",
                "isPro": false,
                "fullname": "Wuyang Chen",
                "user": "wuyangchen",
                "type": "user"
            },
            "summary": "We propose a neural physics system for real-time, interactive fluid\nsimulations. Traditional physics-based methods, while accurate, are\ncomputationally intensive and suffer from latency issues. Recent\nmachine-learning methods reduce computational costs while preserving fidelity;\nyet most still fail to satisfy the latency constraints for real-time use and\nlack support for interactive applications. To bridge this gap, we introduce a\nnovel hybrid method that integrates numerical simulation, neural physics, and\ngenerative control. Our neural physics jointly pursues low-latency simulation\nand high physical fidelity by employing a fallback safeguard to classical\nnumerical solvers. Furthermore, we develop a diffusion-based controller that is\ntrained using a reverse modeling strategy to generate external dynamic force\nfields for fluid manipulation. Our system demonstrates robust performance\nacross diverse 2D/3D scenarios, material types, and obstacle interactions,\nachieving real-time simulations at high frame rates (11~29% latency) while\nenabling fluid control guided by user-friendly freehand sketches. We present a\nsignificant step towards practical, controllable, and physically plausible\nfluid simulations for real-time interactive applications. We promise to release\nboth models and data upon acceptance.",
            "upvotes": 4,
            "discussionId": "6835fb643d51d52701b0eb83",
            "projectPage": "https://hybridmpm.github.io/",
            "ai_summary": "A hybrid neural physics system with diffusion-based control achieves real-time, interactive fluid simulations with low latency and high fidelity.",
            "ai_keywords": [
                "neural physics",
                "hybrid method",
                "numerical simulation",
                "real-time simulations",
                "diffusion-based controller",
                "reverse modeling strategy",
                "fluid manipulation",
                "high frame rates",
                "freehand sketches",
                "physically plausible fluid simulations"
            ]
        },
        "publishedAt": "2025-05-24T21:27:18.000Z",
        "title": "Hybrid Neural-MPM for Interactive Fluid Simulations in Real-Time",
        "summary": "We propose a neural physics system for real-time, interactive fluid\nsimulations. Traditional physics-based methods, while accurate, are\ncomputationally intensive and suffer from latency issues. Recent\nmachine-learning methods reduce computational costs while preserving fidelity;\nyet most still fail to satisfy the latency constraints for real-time use and\nlack support for interactive applications. To bridge this gap, we introduce a\nnovel hybrid method that integrates numerical simulation, neural physics, and\ngenerative control. Our neural physics jointly pursues low-latency simulation\nand high physical fidelity by employing a fallback safeguard to classical\nnumerical solvers. Furthermore, we develop a diffusion-based controller that is\ntrained using a reverse modeling strategy to generate external dynamic force\nfields for fluid manipulation. Our system demonstrates robust performance\nacross diverse 2D/3D scenarios, material types, and obstacle interactions,\nachieving real-time simulations at high frame rates (11~29% latency) while\nenabling fluid control guided by user-friendly freehand sketches. We present a\nsignificant step towards practical, controllable, and physically plausible\nfluid simulations for real-time interactive applications. We promise to release\nboth models and data upon acceptance.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64df97c628d5d234ce0bf83c/uyWW3AbZQIcLfthwruoRc.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64df97c628d5d234ce0bf83c/4tjO6kRMK9zv8-GOGtczG.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18926.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64df97c628d5d234ce0bf83c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64df97c628d5d234ce0bf83c/BIEH7Ep4ffytiAcUIPFe-.jpeg",
            "fullname": "Wuyang Chen",
            "name": "wuyangchen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18116",
            "authors": [
                {
                    "_id": "68360e6c4e92d78afe73c338",
                    "name": "Huayu Chen",
                    "hidden": false
                },
                {
                    "_id": "68360e6c4e92d78afe73c339",
                    "name": "Kaiwen Zheng",
                    "hidden": false
                },
                {
                    "_id": "68360e6c4e92d78afe73c33a",
                    "name": "Qinsheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68360e6c4e92d78afe73c33b",
                    "name": "Ganqu Cui",
                    "hidden": false
                },
                {
                    "_id": "68360e6c4e92d78afe73c33c",
                    "name": "Yin Cui",
                    "hidden": false
                },
                {
                    "_id": "68360e6c4e92d78afe73c33d",
                    "name": "Haotian Ye",
                    "hidden": false
                },
                {
                    "_id": "68360e6c4e92d78afe73c33e",
                    "name": "Tsung-Yi Lin",
                    "hidden": false
                },
                {
                    "_id": "68360e6c4e92d78afe73c33f",
                    "name": "Ming-Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "68360e6c4e92d78afe73c340",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "68360e6c4e92d78afe73c341",
                    "name": "Haoxiang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T17:17:40.000Z",
            "submittedOnDailyAt": "2025-05-27T19:46:50.248Z",
            "title": "Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "638fb8cf2380ffd99caf8c2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fb8cf2380ffd99caf8c2a/7juUomHJ4gH1HIJj43qE6.jpeg",
                "isPro": false,
                "fullname": "Haoxiang Wang",
                "user": "Haoxiang-Wang",
                "type": "user"
            },
            "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.",
            "upvotes": 4,
            "discussionId": "68360e6c4e92d78afe73c383",
            "projectPage": "https://research.nvidia.com/labs/dir/Negative-aware-Fine-Tuning/",
            "ai_summary": "Negative-aware Fine-Tuning (NFT) enhances LLMs' math abilities using supervised learning with negative feedback, achieving performance comparable to RL methods.",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "Supervised Learning (SL)",
                "Negative-aware Fine-Tuning (NFT)",
                "binary verifier signals",
                "negative answers",
                "implicit negative policy",
                "policy optimization",
                "math reasoning tasks",
                "Rejection sampling Fine-Tuning",
                "GRPO",
                "DAPO",
                "strict-on-policy training"
            ]
        },
        "publishedAt": "2025-05-23T13:17:40.000Z",
        "title": "Bridging Supervised Learning and Reinforcement Learning in Math\n  Reasoning",
        "summary": "Reinforcement Learning (RL) has played a central role in the recent surge of\nLLMs' math abilities by enabling self-improvement through binary verifier\nsignals. In contrast, Supervised Learning (SL) is rarely considered for such\nverification-driven training, largely due to its heavy reliance on reference\nanswers and inability to reflect on mistakes. In this work, we challenge the\nprevailing notion that self-improvement is exclusive to RL and propose\nNegative-aware Fine-Tuning (NFT) -- a supervised approach that enables LLMs to\nreflect on their failures and improve autonomously with no external teachers.\nIn online training, instead of throwing away self-generated negative answers,\nNFT constructs an implicit negative policy to model them. This implicit policy\nis parameterized with the same positive LLM we target to optimize on positive\ndata, enabling direct policy optimization on all LLMs' generations. We conduct\nexperiments on 7B and 32B models in math reasoning tasks. Results consistently\nshow that through the additional leverage of negative feedback, NFT\nsignificantly improves over SL baselines like Rejection sampling Fine-Tuning,\nmatching or even surpassing leading RL algorithms like GRPO and DAPO.\nFurthermore, we demonstrate that NFT and GRPO are actually equivalent in\nstrict-on-policy training, even though they originate from entirely different\ntheoretical foundations. Our experiments and theoretical findings bridge the\ngap between SL and RL methods in binary-feedback learning systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18116.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638fb8cf2380ffd99caf8c2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638fb8cf2380ffd99caf8c2a/7juUomHJ4gH1HIJj43qE6.jpeg",
            "fullname": "Haoxiang Wang",
            "name": "Haoxiang-Wang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20294",
            "authors": [
                {
                    "_id": "683552b7d34b8e5da4d9dfe3",
                    "user": {
                        "_id": "653cb25c394886efebf9971a",
                        "avatarUrl": "/avatars/bca0a20c305e178a3f316581a2636cb6.svg",
                        "isPro": false,
                        "fullname": "Xiao Chen",
                        "user": "Xiao-HF",
                        "type": "user"
                    },
                    "name": "Xiao Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:49:12.484Z",
                    "hidden": false
                },
                {
                    "_id": "683552b7d34b8e5da4d9dfe4",
                    "user": {
                        "_id": "64e6d9d229a548f66aff6e5b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
                        "isPro": false,
                        "fullname": "Tai Wang",
                        "user": "taiwang",
                        "type": "user"
                    },
                    "name": "Tai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:47:38.917Z",
                    "hidden": false
                },
                {
                    "_id": "683552b7d34b8e5da4d9dfe5",
                    "user": {
                        "_id": "679891b44a7043fef939ebf9",
                        "avatarUrl": "/avatars/4b0c9bd4c207faf81a13d5b24cbf7864.svg",
                        "isPro": false,
                        "fullname": "Quanyi Li",
                        "user": "kok123",
                        "type": "user"
                    },
                    "name": "Quanyi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:47:23.428Z",
                    "hidden": false
                },
                {
                    "_id": "683552b7d34b8e5da4d9dfe6",
                    "name": "Tao Huang",
                    "hidden": false
                },
                {
                    "_id": "683552b7d34b8e5da4d9dfe7",
                    "user": {
                        "_id": "65783ee6ee33d547aecc3ffc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                        "isPro": false,
                        "fullname": "Jiangmiao Pang",
                        "user": "Jiangmiao",
                        "type": "user"
                    },
                    "name": "Jiangmiao Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:47:14.318Z",
                    "hidden": false
                },
                {
                    "_id": "683552b7d34b8e5da4d9dfe8",
                    "user": {
                        "_id": "67631f57abfbd60470d4b3c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5VrG0IZjYrDLiatOr4y06.png",
                        "isPro": false,
                        "fullname": "Tianfan Xue",
                        "user": "littlemouse9",
                        "type": "user"
                    },
                    "name": "Tianfan Xue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:47:08.189Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:59:52.000Z",
            "submittedOnDailyAt": "2025-05-27T06:49:31.312Z",
            "title": "GLEAM: Learning Generalizable Exploration Policy for Active Mapping in\n  Complex 3D Indoor Scenes",
            "submittedOnDailyBy": {
                "_id": "653cb25c394886efebf9971a",
                "avatarUrl": "/avatars/bca0a20c305e178a3f316581a2636cb6.svg",
                "isPro": false,
                "fullname": "Xiao Chen",
                "user": "Xiao-HF",
                "type": "user"
            },
            "summary": "Generalizable active mapping in complex unknown environments remains a\ncritical challenge for mobile robots. Existing methods, constrained by\ninsufficient training data and conservative exploration strategies, exhibit\nlimited generalizability across scenes with diverse layouts and complex\nconnectivity. To enable scalable training and reliable evaluation, we introduce\nGLEAM-Bench, the first large-scale benchmark designed for generalizable active\nmapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.\nBuilding upon this foundation, we propose GLEAM, a unified generalizable\nexploration policy for active mapping. Its superior generalizability comes\nmainly from our semantic representations, long-term navigable goals, and\nrandomized strategies. It significantly outperforms state-of-the-art methods,\nachieving 66.50% coverage (+9.49%) with efficient trajectories and improved\nmapping accuracy on 128 unseen complex scenes. Project page:\nhttps://xiao-chen.tech/gleam/.",
            "upvotes": 3,
            "discussionId": "683552b9d34b8e5da4d9e050",
            "projectPage": "https://xiao-chen.tech/gleam/",
            "githubRepo": "https://github.com/zjwzcx/GLEAM",
            "ai_summary": "A new benchmark and policy, GLEAM-Bench and GLEAM, improve the scalability and reliability of active mapping in complex environments through semantic representations and efficient exploration strategies.",
            "ai_keywords": [
                "active mapping",
                "generalizable exploration",
                "semantic representations",
                "navigable goals",
                "randomized strategies",
                "3D scenes",
                "synthetic datasets",
                "real-scan datasets",
                "benchmark",
                "mapping accuracy",
                "coverage",
                "trajectories"
            ]
        },
        "publishedAt": "2025-05-26T13:59:52.000Z",
        "title": "GLEAM: Learning Generalizable Exploration Policy for Active Mapping in\n  Complex 3D Indoor Scenes",
        "summary": "Generalizable active mapping in complex unknown environments remains a\ncritical challenge for mobile robots. Existing methods, constrained by\ninsufficient training data and conservative exploration strategies, exhibit\nlimited generalizability across scenes with diverse layouts and complex\nconnectivity. To enable scalable training and reliable evaluation, we introduce\nGLEAM-Bench, the first large-scale benchmark designed for generalizable active\nmapping with 1,152 diverse 3D scenes from synthetic and real-scan datasets.\nBuilding upon this foundation, we propose GLEAM, a unified generalizable\nexploration policy for active mapping. Its superior generalizability comes\nmainly from our semantic representations, long-term navigable goals, and\nrandomized strategies. It significantly outperforms state-of-the-art methods,\nachieving 66.50% coverage (+9.49%) with efficient trajectories and improved\nmapping accuracy on 128 unseen complex scenes. Project page:\nhttps://xiao-chen.tech/gleam/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20294.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "653cb25c394886efebf9971a",
            "avatarUrl": "/avatars/bca0a20c305e178a3f316581a2636cb6.svg",
            "fullname": "Xiao Chen",
            "name": "Xiao-HF",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19056",
            "authors": [
                {
                    "_id": "68357c8ef0b7aba41a858b61",
                    "user": {
                        "_id": "65f6eff66396309f02a18dab",
                        "avatarUrl": "/avatars/3b315ecbe2815859ca7fb16d277740c8.svg",
                        "isPro": false,
                        "fullname": "Harethah Abu Shairah",
                        "user": "HarethahMo",
                        "type": "user"
                    },
                    "name": "Harethah Abu Shairah",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:45:40.779Z",
                    "hidden": false
                },
                {
                    "_id": "68357c8ef0b7aba41a858b62",
                    "user": {
                        "_id": "642b51385bf2355d02a23d15",
                        "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
                        "isPro": true,
                        "fullname": "Hasan Abed Al Kader Hammoud",
                        "user": "hammh0a",
                        "type": "user"
                    },
                    "name": "Hasan Abed Al Kader Hammoud",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:45:54.956Z",
                    "hidden": false
                },
                {
                    "_id": "68357c8ef0b7aba41a858b63",
                    "user": {
                        "_id": "6808bf97ffadd78ec71cb721",
                        "avatarUrl": "/avatars/9adca3142c06b8f69889fcbe85fa374d.svg",
                        "isPro": false,
                        "fullname": "Bernard Ghanem",
                        "user": "bernardghanem",
                        "type": "user"
                    },
                    "name": "Bernard Ghanem",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:46:01.582Z",
                    "hidden": false
                },
                {
                    "_id": "68357c8ef0b7aba41a858b64",
                    "name": "George Turkiyyah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T09:18:24.000Z",
            "submittedOnDailyAt": "2025-05-27T07:21:18.247Z",
            "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks",
            "submittedOnDailyBy": {
                "_id": "642b51385bf2355d02a23d15",
                "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
                "isPro": true,
                "fullname": "Hasan Abed Al Kader Hammoud",
                "user": "hammh0a",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance.",
            "upvotes": 3,
            "discussionId": "68357c8ff0b7aba41a858b96",
            "ai_summary": "Modifying models to generate justified refusals through fine-tuning on an extended-refusal dataset mitigates ablation attacks while maintaining high refusal rates and general performance.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "ablation",
                "latent direction",
                "refusal behavior",
                "extended-refusal dataset",
                "Llama-2-7B-Chat",
                "Qwen2.5-Instruct",
                "parameter-efficient fine-tuning"
            ]
        },
        "publishedAt": "2025-05-25T05:18:24.000Z",
        "title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks",
        "summary": "Large language models (LLMs) are typically aligned to comply with safety\nguidelines by refusing harmful instructions. A recent attack, termed\nabliteration, isolates and suppresses the single latent direction most\nresponsible for refusal behavior, enabling the model to generate unethical\ncontent. We propose a defense that modifies how models generate refusals. We\nconstruct an extended-refusal dataset that contains harmful prompts with a full\nresponse that justifies the reason for refusal. We then fine-tune\nLlama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our\nextended-refusal dataset, and evaluate the resulting systems on a set of\nharmful prompts. In our experiments, extended-refusal models maintain high\nrefusal rates, dropping at most by 10%, whereas baseline models' refusal rates\ndrop by 70-80% after abliteration. A broad evaluation of safety and utility\nshows that extended-refusal fine-tuning neutralizes the abliteration attack\nwhile preserving general performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19056.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642b51385bf2355d02a23d15",
            "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
            "fullname": "Hasan Abed Al Kader Hammoud",
            "name": "hammh0a",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18323",
            "authors": [
                {
                    "_id": "683573bc0830dfc67834f1b5",
                    "name": "Nicolas Kchler",
                    "hidden": false
                },
                {
                    "_id": "683573bc0830dfc67834f1b6",
                    "name": "Ivan Petrov",
                    "hidden": false
                },
                {
                    "_id": "683573bc0830dfc67834f1b7",
                    "name": "Conrad Grobler",
                    "hidden": false
                },
                {
                    "_id": "683573bc0830dfc67834f1b8",
                    "user": {
                        "_id": "6475c2794766357252e69e9f",
                        "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                        "isPro": false,
                        "fullname": "i",
                        "user": "iliashum",
                        "type": "user"
                    },
                    "name": "Ilia Shumailov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:41:35.703Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T19:28:45.000Z",
            "submittedOnDailyAt": "2025-05-27T06:44:57.820Z",
            "title": "Architectural Backdoors for Within-Batch Data Stealing and Model\n  Inference Manipulation",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "For nearly a decade the academic community has investigated backdoors in\nneural networks, primarily focusing on classification tasks where adversaries\nmanipulate the model prediction. While demonstrably malicious, the immediate\nreal-world impact of such prediction-altering attacks has remained unclear. In\nthis paper we introduce a novel and significantly more potent class of\nbackdoors that builds upon recent advancements in architectural backdoors. We\ndemonstrate how these backdoors can be specifically engineered to exploit\nbatched inference, a common technique for hardware utilization, enabling\nlarge-scale user data manipulation and theft. By targeting the batching\nprocess, these architectural backdoors facilitate information leakage between\nconcurrent user requests and allow attackers to fully control model responses\ndirected at other users within the same batch. In other words, an attacker who\ncan change the model architecture can set and steal model inputs and outputs of\nother users within the same batch. We show that such attacks are not only\nfeasible but also alarmingly effective, can be readily injected into prevalent\nmodel architectures, and represent a truly malicious threat to user privacy and\nsystem integrity. Critically, to counteract this new class of vulnerabilities,\nwe propose a deterministic mitigation strategy that provides formal guarantees\nagainst this new attack vector, unlike prior work that relied on Large Language\nModels to find the backdoors. Our mitigation strategy employs a novel\nInformation Flow Control mechanism that analyzes the model graph and proves\nnon-interference between different user inputs within the same batch. Using our\nmitigation strategy we perform a large scale analysis of models hosted through\nHugging Face and find over 200 models that introduce (unintended) information\nleakage between batch entries due to the use of dynamic quantization.",
            "upvotes": 3,
            "discussionId": "683573bc0830dfc67834f212",
            "ai_summary": "A novel class of backdoors in neural network architectures exploits batched inference to enable large-scale data manipulation, demonstrating information leakage and control over user inputs and outputs, with a proposed mitigation strategy using Information Flow Control.",
            "ai_keywords": [
                "backdoors",
                "neural networks",
                "classification tasks",
                "batched inference",
                "hardware utilization",
                "information leakage",
                "mitagation strategy",
                "Information Flow Control",
                "Hugging Face",
                "dynamic quantization"
            ]
        },
        "publishedAt": "2025-05-23T15:28:45.000Z",
        "title": "Architectural Backdoors for Within-Batch Data Stealing and Model\n  Inference Manipulation",
        "summary": "For nearly a decade the academic community has investigated backdoors in\nneural networks, primarily focusing on classification tasks where adversaries\nmanipulate the model prediction. While demonstrably malicious, the immediate\nreal-world impact of such prediction-altering attacks has remained unclear. In\nthis paper we introduce a novel and significantly more potent class of\nbackdoors that builds upon recent advancements in architectural backdoors. We\ndemonstrate how these backdoors can be specifically engineered to exploit\nbatched inference, a common technique for hardware utilization, enabling\nlarge-scale user data manipulation and theft. By targeting the batching\nprocess, these architectural backdoors facilitate information leakage between\nconcurrent user requests and allow attackers to fully control model responses\ndirected at other users within the same batch. In other words, an attacker who\ncan change the model architecture can set and steal model inputs and outputs of\nother users within the same batch. We show that such attacks are not only\nfeasible but also alarmingly effective, can be readily injected into prevalent\nmodel architectures, and represent a truly malicious threat to user privacy and\nsystem integrity. Critically, to counteract this new class of vulnerabilities,\nwe propose a deterministic mitigation strategy that provides formal guarantees\nagainst this new attack vector, unlike prior work that relied on Large Language\nModels to find the backdoors. Our mitigation strategy employs a novel\nInformation Flow Control mechanism that analyzes the model graph and proves\nnon-interference between different user inputs within the same batch. Using our\nmitigation strategy we perform a large scale analysis of models hosted through\nHugging Face and find over 200 models that introduce (unintended) information\nleakage between batch entries due to the use of dynamic quantization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18323.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/db428715dfd2239df2aeaaff1282323f.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16984",
            "authors": [
                {
                    "_id": "68347b01021ab1eaaf09dfec",
                    "user": {
                        "_id": "67b0b9ec55810ecdb365aa7b",
                        "avatarUrl": "/avatars/1f4e72981e05f9b79a78fae1171e2524.svg",
                        "isPro": true,
                        "fullname": "Mingyang Liu",
                        "user": "liumy2010",
                        "type": "user"
                    },
                    "name": "Mingyang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:53:53.037Z",
                    "hidden": false
                },
                {
                    "_id": "68347b01021ab1eaaf09dfed",
                    "name": "Gabriele Farina",
                    "hidden": false
                },
                {
                    "_id": "68347b01021ab1eaaf09dfee",
                    "name": "Asuman Ozdaglar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T17:53:57.000Z",
            "submittedOnDailyAt": "2025-05-27T12:45:59.644Z",
            "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "67b0b9ec55810ecdb365aa7b",
                "avatarUrl": "/avatars/1f4e72981e05f9b79a78fae1171e2524.svg",
                "isPro": true,
                "fullname": "Mingyang Liu",
                "user": "liumy2010",
                "type": "user"
            },
            "summary": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks.",
            "upvotes": 3,
            "discussionId": "68347b02021ab1eaaf09e01e",
            "githubRepo": "https://github.com/liumy2010/UFT",
            "ai_summary": "A new post-training method, Unified Fine-Tuning (UFT), improves upon supervised and reinforcement fine-tuning for large language models by combining their benefits, achieving better generalization and faster convergence.",
            "ai_keywords": [
                "supervised fine-tuning",
                "reinforcement fine-tuning",
                "Unified Fine-Tuning",
                "long-horizon reasoning tasks",
                "sample complexity bottleneck"
            ]
        },
        "publishedAt": "2025-05-22T13:53:57.000Z",
        "title": "UFT: Unifying Supervised and Reinforcement Fine-Tuning",
        "summary": "Post-training has demonstrated its importance in enhancing the reasoning\ncapabilities of large language models (LLMs). The primary post-training methods\ncan be categorized into supervised fine-tuning (SFT) and reinforcement\nfine-tuning (RFT). SFT is efficient and well-suited for small language models,\nbut it may lead to overfitting and limit the reasoning abilities of larger\nmodels. In contrast, RFT generally yields better generalization but depends\nheavily on the strength of the base model. To address the limitations of SFT\nand RFT, we propose Unified Fine-Tuning (UFT), a novel post-training paradigm\nthat unifies SFT and RFT into a single, integrated process. UFT enables the\nmodel to effectively explore solutions while incorporating informative\nsupervision signals, bridging the gap between memorizing and thinking\nunderlying existing methods. Notably, UFT outperforms both SFT and RFT in\ngeneral, regardless of model sizes. Furthermore, we theoretically prove that\nUFT breaks RFT's inherent exponential sample complexity bottleneck, showing for\nthe first time that unified training can exponentially accelerate convergence\non long-horizon reasoning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16984.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67b0b9ec55810ecdb365aa7b",
            "avatarUrl": "/avatars/1f4e72981e05f9b79a78fae1171e2524.svg",
            "fullname": "Mingyang Liu",
            "name": "liumy2010",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16886",
            "authors": [
                {
                    "_id": "6835ba5b5ff50d406844b4cd",
                    "name": "Nour Jedidi",
                    "hidden": false
                },
                {
                    "_id": "6835ba5b5ff50d406844b4ce",
                    "name": "Yung-Sung Chuang",
                    "hidden": false
                },
                {
                    "_id": "6835ba5b5ff50d406844b4cf",
                    "name": "James Glass",
                    "hidden": false
                },
                {
                    "_id": "6835ba5b5ff50d406844b4d0",
                    "name": "Jimmy Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T16:41:37.000Z",
            "submittedOnDailyAt": "2025-05-27T11:47:59.602Z",
            "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?",
            "submittedOnDailyBy": {
                "_id": "663113ed3e584ce1d96b1d32",
                "avatarUrl": "/avatars/28154c3c0ac3bb100dceed1c760d5fd3.svg",
                "isPro": false,
                "fullname": "Nour Jedidi",
                "user": "njedidi",
                "type": "user"
            },
            "summary": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.",
            "upvotes": 3,
            "discussionId": "6835ba5b5ff50d406844b4f3",
            "ai_summary": "Reasoning-based rerankers using Large Language Models do not improve accuracy compared to standard rerankers and are outperformed even when their reasoning process is disabled due to overly polarized relevance scores.",
            "ai_keywords": [
                "Large Language Models",
                "pointwise rerankers",
                "reasoning process"
            ]
        },
        "publishedAt": "2025-05-22T12:41:37.000Z",
        "title": "Don't \"Overthink\" Passage Reranking: Is Reasoning Truly Necessary?",
        "summary": "With the growing success of reasoning models across complex natural language\ntasks, researchers in the Information Retrieval (IR) community have begun\nexploring how similar reasoning capabilities can be integrated into passage\nrerankers built on Large Language Models (LLMs). These methods typically employ\nan LLM to produce an explicit, step-by-step reasoning process before arriving\nat a final relevance prediction. But, does reasoning actually improve reranking\naccuracy? In this paper, we dive deeper into this question, studying the impact\nof the reasoning process by comparing reasoning-based pointwise rerankers\n(ReasonRR) to standard, non-reasoning pointwise rerankers (StandardRR) under\nidentical training conditions, and observe that StandardRR generally\noutperforms ReasonRR. Building on this observation, we then study the\nimportance of reasoning to ReasonRR by disabling its reasoning process\n(ReasonRR-NoReason), and find that ReasonRR-NoReason is surprisingly more\neffective than ReasonRR. Examining the cause of this result, our findings\nreveal that reasoning-based rerankers are limited by the LLM's reasoning\nprocess, which pushes it toward polarized relevance scores and thus fails to\nconsider the partial relevance of passages, a key factor for the accuracy of\npointwise rerankers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16886.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "663113ed3e584ce1d96b1d32",
            "avatarUrl": "/avatars/28154c3c0ac3bb100dceed1c760d5fd3.svg",
            "fullname": "Nour Jedidi",
            "name": "njedidi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.16312",
            "authors": [
                {
                    "_id": "6830109ea20ebb4738e76931",
                    "user": {
                        "_id": "6747d38098fe79433a8c4580",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BrcsTfusqfu9p9uv1NeX6.png",
                        "isPro": false,
                        "fullname": "Jiawei Liu",
                        "user": "Jiawei1222",
                        "type": "user"
                    },
                    "name": "Jiawei Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:15:09.111Z",
                    "hidden": false
                },
                {
                    "_id": "6830109ea20ebb4738e76932",
                    "name": "Qisi Chen",
                    "hidden": false
                },
                {
                    "_id": "6830109ea20ebb4738e76933",
                    "user": {
                        "_id": "65d8b0f0661492b25c6623de",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d8b0f0661492b25c6623de/c6LPDse8NIV_3BHIu8dYe.png",
                        "isPro": false,
                        "fullname": "Jianshu Zhang",
                        "user": "Sterzhang",
                        "type": "user"
                    },
                    "name": "Jianshu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:42:26.545Z",
                    "hidden": false
                },
                {
                    "_id": "6830109ea20ebb4738e76934",
                    "name": "Quan Liu",
                    "hidden": false
                },
                {
                    "_id": "6830109ea20ebb4738e76935",
                    "user": {
                        "_id": "66ed026076a8038cb4ae6053",
                        "avatarUrl": "/avatars/99b6527da6b66c6b5df3fc8261587322.svg",
                        "isPro": false,
                        "fullname": "Defu Lian",
                        "user": "dove-ustc",
                        "type": "user"
                    },
                    "name": "Defu Lian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:42:48.468Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T07:07:43.000Z",
            "submittedOnDailyAt": "2025-05-27T05:04:30.217Z",
            "title": "EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via\n  Action Pruning",
            "submittedOnDailyBy": {
                "_id": "6747d38098fe79433a8c4580",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BrcsTfusqfu9p9uv1NeX6.png",
                "isPro": false,
                "fullname": "Jiawei Liu",
                "user": "Jiawei1222",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) excel at complex reasoning through search\nalgorithms, yet current strategies often suffer from massive token consumption\ndue to redundant exploration of semantically equivalent steps. Existing\nsemantic similarity methods struggle to accurately identify such equivalence in\ndomain-specific contexts like mathematical reasoning. To address this, we\npropose EquivPruner, a simple yet effective approach that identifies and prunes\nsemantically equivalent actions during LLM reasoning search. We also introduce\nMathEquiv, the first dataset we created for mathematical statement equivalence,\nwhich enables the training of a lightweight equivalence detector. Extensive\nexperiments across various models and tasks demonstrate that EquivPruner\nsignificantly reduces token consumption, improving searching efficiency and\noften bolstering reasoning accuracy. For instance, when applied to\nQwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by\n48.1\\% while also improving accuracy. Our code is available at\nhttps://github.com/Lolo1222/EquivPruner.",
            "upvotes": 3,
            "discussionId": "6830109fa20ebb4738e769a3",
            "githubRepo": "https://github.com/Lolo1222/EquivPruner",
            "ai_summary": "EquivPruner reduces token consumption and improves reasoning accuracy by pruning semantically equivalent actions in LLM searches, leveraging a new dataset for mathematical equivalence.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "semantic similarity",
                "semantically equivalent actions",
                "EquivPruner",
                "MathEquiv",
                "equivalence detector",
                "GSM8K"
            ]
        },
        "publishedAt": "2025-05-22T03:07:43.000Z",
        "title": "EquivPruner: Boosting Efficiency and Quality in LLM-Based Search via\n  Action Pruning",
        "summary": "Large Language Models (LLMs) excel at complex reasoning through search\nalgorithms, yet current strategies often suffer from massive token consumption\ndue to redundant exploration of semantically equivalent steps. Existing\nsemantic similarity methods struggle to accurately identify such equivalence in\ndomain-specific contexts like mathematical reasoning. To address this, we\npropose EquivPruner, a simple yet effective approach that identifies and prunes\nsemantically equivalent actions during LLM reasoning search. We also introduce\nMathEquiv, the first dataset we created for mathematical statement equivalence,\nwhich enables the training of a lightweight equivalence detector. Extensive\nexperiments across various models and tasks demonstrate that EquivPruner\nsignificantly reduces token consumption, improving searching efficiency and\noften bolstering reasoning accuracy. For instance, when applied to\nQwen2.5-Math-7B-Instruct on GSM8K, EquivPruner reduced token consumption by\n48.1\\% while also improving accuracy. Our code is available at\nhttps://github.com/Lolo1222/EquivPruner.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16312.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6747d38098fe79433a8c4580",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/BrcsTfusqfu9p9uv1NeX6.png",
            "fullname": "Jiawei Liu",
            "name": "Jiawei1222",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18454",
            "authors": [
                {
                    "_id": "683606b33e467355094d95bc",
                    "name": "Zhenrui Yue",
                    "hidden": false
                },
                {
                    "_id": "683606b33e467355094d95bd",
                    "name": "Bowen Jin",
                    "hidden": false
                },
                {
                    "_id": "683606b33e467355094d95be",
                    "name": "Huimin Zeng",
                    "hidden": false
                },
                {
                    "_id": "683606b33e467355094d95bf",
                    "name": "Honglei Zhuang",
                    "hidden": false
                },
                {
                    "_id": "683606b33e467355094d95c0",
                    "name": "Zhen Qin",
                    "hidden": false
                },
                {
                    "_id": "683606b33e467355094d95c1",
                    "name": "Jinsung Yoon",
                    "hidden": false
                },
                {
                    "_id": "683606b33e467355094d95c2",
                    "name": "Lanyu Shang",
                    "hidden": false
                },
                {
                    "_id": "683606b33e467355094d95c3",
                    "name": "Jiawei Han",
                    "hidden": false
                },
                {
                    "_id": "683606b33e467355094d95c4",
                    "name": "Dong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T01:26:16.000Z",
            "submittedOnDailyAt": "2025-05-27T17:09:01.317Z",
            "title": "Hybrid Latent Reasoning via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "644c4eff9a7c19cfaf0c8f4a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/2gHtfhPFjSsnwxVtBnxe2.jpeg",
                "isPro": false,
                "fullname": "Zhenrui Yue",
                "user": "yuezrhb",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have introduced latent\nreasoning as a promising alternative to autoregressive reasoning. By performing\ninternal computation with hidden states from previous steps, latent reasoning\nbenefit from more informative features rather than sampling a discrete\nchain-of-thought (CoT) path. Yet latent reasoning approaches are often\nincompatible with LLMs, as their continuous paradigm conflicts with the\ndiscrete nature of autoregressive generation. Moreover, these methods rely on\nCoT traces for training and thus fail to exploit the inherent reasoning\npatterns of LLMs. In this work, we explore latent reasoning by leveraging the\nintrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we\nintroduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid\nlatent reasoning approach that (1) integrates prior hidden states into sampled\ntokens with a learnable gating mechanism, and (2) initializes training with\npredominantly token embeddings while progressively incorporating more hidden\nfeatures. This design maintains LLMs' generative capabilities and incentivizes\nhybrid reasoning using both discrete and continuous representations. In\naddition, the hybrid HRPO introduces stochasticity into latent reasoning via\ntoken sampling, thereby enabling RL-based optimization without requiring CoT\ntrajectories. Extensive evaluations across diverse benchmarks show that HRPO\noutperforms prior methods in both knowledge- and reasoning-intensive tasks.\nFurthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing\nbehaviors like cross-lingual patterns and shorter completion lengths,\nhighlighting the potential of our RL-based approach and offer insights for\nfuture work in latent reasoning.",
            "upvotes": 2,
            "discussionId": "683606b53e467355094d9628",
            "ai_summary": "Hybrid reasoning policy optimization (HRPO) leverages reinforcement learning to integrate latent reasoning with large language models, enhancing performance in knowledge- and reasoning-intensive tasks while maintaining interpretability.",
            "ai_keywords": [
                "latent reasoning",
                "autoregressive reasoning",
                "hidden states",
                "chain-of-thought (CoT)",
                "reinforcement learning (RL)",
                "hybrid reasoning",
                "HRPO",
                "token embeddings",
                "generative capabilities",
                "discrete representations",
                "continuous representations",
                "stochasticity",
                "token sampling",
                "cross-lingual patterns",
                "completion lengths"
            ]
        },
        "publishedAt": "2025-05-23T21:26:16.000Z",
        "title": "Hybrid Latent Reasoning via Reinforcement Learning",
        "summary": "Recent advances in large language models (LLMs) have introduced latent\nreasoning as a promising alternative to autoregressive reasoning. By performing\ninternal computation with hidden states from previous steps, latent reasoning\nbenefit from more informative features rather than sampling a discrete\nchain-of-thought (CoT) path. Yet latent reasoning approaches are often\nincompatible with LLMs, as their continuous paradigm conflicts with the\ndiscrete nature of autoregressive generation. Moreover, these methods rely on\nCoT traces for training and thus fail to exploit the inherent reasoning\npatterns of LLMs. In this work, we explore latent reasoning by leveraging the\nintrinsic capabilities of LLMs via reinforcement learning (RL). To this end, we\nintroduce hybrid reasoning policy optimization (HRPO), an RL-based hybrid\nlatent reasoning approach that (1) integrates prior hidden states into sampled\ntokens with a learnable gating mechanism, and (2) initializes training with\npredominantly token embeddings while progressively incorporating more hidden\nfeatures. This design maintains LLMs' generative capabilities and incentivizes\nhybrid reasoning using both discrete and continuous representations. In\naddition, the hybrid HRPO introduces stochasticity into latent reasoning via\ntoken sampling, thereby enabling RL-based optimization without requiring CoT\ntrajectories. Extensive evaluations across diverse benchmarks show that HRPO\noutperforms prior methods in both knowledge- and reasoning-intensive tasks.\nFurthermore, HRPO-trained LLMs remain interpretable and exhibit intriguing\nbehaviors like cross-lingual patterns and shorter completion lengths,\nhighlighting the potential of our RL-based approach and offer insights for\nfuture work in latent reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18454.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644c4eff9a7c19cfaf0c8f4a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/2gHtfhPFjSsnwxVtBnxe2.jpeg",
            "fullname": "Zhenrui Yue",
            "name": "yuezrhb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18283",
            "authors": [
                {
                    "_id": "68358855d34b8e5da4e82fe6",
                    "user": {
                        "_id": "66de7dcb21f901b049895e33",
                        "avatarUrl": "/avatars/a8beb6ef83afae6e40fdcf3a1fb08a62.svg",
                        "isPro": false,
                        "fullname": "Jianghao",
                        "user": "JianghaoWu",
                        "type": "user"
                    },
                    "name": "Jianghao Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T10:03:31.484Z",
                    "hidden": false
                },
                {
                    "_id": "68358855d34b8e5da4e82fe7",
                    "user": {
                        "_id": "67467d9bf7b7f6fffeee8f09",
                        "avatarUrl": "/avatars/da5dd1acd79478540c32b34060710e1b.svg",
                        "isPro": false,
                        "fullname": "tangfeilong",
                        "user": "tfl01",
                        "type": "user"
                    },
                    "name": "Feilong Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:39:04.112Z",
                    "hidden": false
                },
                {
                    "_id": "68358855d34b8e5da4e82fe8",
                    "name": "Yulong Li",
                    "hidden": false
                },
                {
                    "_id": "68358855d34b8e5da4e82fe9",
                    "name": "Ming Hu",
                    "hidden": false
                },
                {
                    "_id": "68358855d34b8e5da4e82fea",
                    "name": "Haochen Xue",
                    "hidden": false
                },
                {
                    "_id": "68358855d34b8e5da4e82feb",
                    "name": "Shoaib Jameel",
                    "hidden": false
                },
                {
                    "_id": "68358855d34b8e5da4e82fec",
                    "name": "Yutong Xie",
                    "hidden": false
                },
                {
                    "_id": "68358855d34b8e5da4e82fed",
                    "user": {
                        "_id": "65ade1c114d782df06d92798",
                        "avatarUrl": "/avatars/f47dffe30dd2af2ae774309fea017efe.svg",
                        "isPro": false,
                        "fullname": "Imran Razzak",
                        "user": "imranrazzakunsw",
                        "type": "user"
                    },
                    "name": "Imran Razzak",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:39:16.051Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T18:28:59.000Z",
            "submittedOnDailyAt": "2025-05-27T08:52:11.509Z",
            "title": "TAGS: A Test-Time Generalist-Specialist Framework with\n  Retrieval-Augmented Reasoning and Verification",
            "submittedOnDailyBy": {
                "_id": "66de7dcb21f901b049895e33",
                "avatarUrl": "/avatars/a8beb6ef83afae6e40fdcf3a1fb08a62.svg",
                "isPro": false,
                "fullname": "Jianghao",
                "user": "JianghaoWu",
                "type": "user"
            },
            "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS.",
            "upvotes": 2,
            "discussionId": "68358855d34b8e5da4e8301c",
            "ai_summary": "TAGS, a test-time framework combining generalist and specialist models with hierarchical retrieval and reliability scoring, enhances medical LLM reasoning without fine-tuning.",
            "ai_keywords": [
                "Chain-of-Thought prompting",
                "large language models (LLMs)",
                "zero-shot medical reasoning",
                "fine-tuning",
                "parameter updates",
                "hierarchical retrieval mechanism",
                "reliability scorer",
                "MedQA benchmarks",
                "GPT-4o",
                "DeepSeek-R1"
            ]
        },
        "publishedAt": "2025-05-23T14:28:59.000Z",
        "title": "TAGS: A Test-Time Generalist-Specialist Framework with\n  Retrieval-Augmented Reasoning and Verification",
        "summary": "Recent advances such as Chain-of-Thought prompting have significantly\nimproved large language models (LLMs) in zero-shot medical reasoning. However,\nprompting-based methods often remain shallow and unstable, while fine-tuned\nmedical LLMs suffer from poor generalization under distribution shifts and\nlimited adaptability to unseen clinical scenarios. To address these\nlimitations, we present TAGS, a test-time framework that combines a broadly\ncapable generalist with a domain-specific specialist to offer complementary\nperspectives without any model fine-tuning or parameter updates. To support\nthis generalist-specialist reasoning process, we introduce two auxiliary\nmodules: a hierarchical retrieval mechanism that provides multi-scale exemplars\nby selecting examples based on both semantic and rationale-level similarity,\nand a reliability scorer that evaluates reasoning consistency to guide final\nanswer aggregation. TAGS achieves strong performance across nine MedQA\nbenchmarks, boosting GPT-4o accuracy by 13.8%, DeepSeek-R1 by 16.8%, and\nimproving a vanilla 7B model from 14.1% to 23.9%. These results surpass several\nfine-tuned medical LLMs, without any parameter updates. The code will be\navailable at https://github.com/JianghaoWu/TAGS.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18283.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66de7dcb21f901b049895e33",
            "avatarUrl": "/avatars/a8beb6ef83afae6e40fdcf3a1fb08a62.svg",
            "fullname": "Jianghao",
            "name": "JianghaoWu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20297",
            "authors": [
                {
                    "_id": "68364bbf8bb68ffc201e7d83",
                    "name": "Qinyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68364bbf8bb68ffc201e7d84",
                    "name": "Jaskirat Singh",
                    "hidden": false
                },
                {
                    "_id": "68364bbf8bb68ffc201e7d85",
                    "name": "Ming Xu",
                    "hidden": false
                },
                {
                    "_id": "68364bbf8bb68ffc201e7d86",
                    "name": "Akshay Asthana",
                    "hidden": false
                },
                {
                    "_id": "68364bbf8bb68ffc201e7d87",
                    "name": "Stephen Gould",
                    "hidden": false
                },
                {
                    "_id": "68364bbf8bb68ffc201e7d88",
                    "name": "Liang Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:59:57.000Z",
            "submittedOnDailyAt": "2025-05-27T22:10:48.351Z",
            "title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and\nHarmon adopt diffusion sampling to improve the quality of image generation.\nHowever, this strategy leads to low inference efficiency, because it usually\ntakes 50 to 100 steps for diffusion to sample a token. This paper explores how\nto effectively address this issue. Our key motivation is that as more tokens\nare generated during the autoregressive process, subsequent tokens follow more\nconstrained distributions and are easier to sample. To intuitively explain, if\na model has generated part of a dog, the remaining tokens must complete the dog\nand thus are more constrained. Empirical evidence supports our motivation: at\nlater generation stages, the next tokens can be well predicted by a multilayer\nperceptron, exhibit low variance, and follow closer-to-straight-line denoising\npaths from noise to tokens. Based on our finding, we introduce diffusion step\nannealing (DiSA), a training-free method which gradually uses fewer diffusion\nsteps as more tokens are generated, e.g., using 50 steps at the beginning and\ngradually decreasing to 5 steps at later stages. Because DiSA is derived from\nour finding specific to diffusion in autoregressive models, it is complementary\nto existing acceleration methods designed for diffusion alone. DiSA can be\nimplemented in only a few lines of code on existing models, and albeit simple,\nachieves 5-10times faster inference for MAR and Harmon and 1.4-2.5times\nfor FlowAR and xAR, while maintaining the generation quality.",
            "upvotes": 1,
            "discussionId": "68364bc28bb68ffc201e7e3a",
            "ai_summary": "Diffusion step annealing enhances inference efficiency in autoregressive models by reducing the number of diffusion steps as more tokens are generated, preserving quality.",
            "ai_keywords": [
                "autoregressive models",
                "MAR",
                "FlowAR",
                "xAR",
                "Harmon",
                "diffusion sampling",
                "diffusion step annealing",
                "DiSA",
                "multilayer perceptron",
                "denoising paths"
            ]
        },
        "publishedAt": "2025-05-26T13:59:57.000Z",
        "title": "DiSA: Diffusion Step Annealing in Autoregressive Image Generation",
        "summary": "An increasing number of autoregressive models, such as MAR, FlowAR, xAR, and\nHarmon adopt diffusion sampling to improve the quality of image generation.\nHowever, this strategy leads to low inference efficiency, because it usually\ntakes 50 to 100 steps for diffusion to sample a token. This paper explores how\nto effectively address this issue. Our key motivation is that as more tokens\nare generated during the autoregressive process, subsequent tokens follow more\nconstrained distributions and are easier to sample. To intuitively explain, if\na model has generated part of a dog, the remaining tokens must complete the dog\nand thus are more constrained. Empirical evidence supports our motivation: at\nlater generation stages, the next tokens can be well predicted by a multilayer\nperceptron, exhibit low variance, and follow closer-to-straight-line denoising\npaths from noise to tokens. Based on our finding, we introduce diffusion step\nannealing (DiSA), a training-free method which gradually uses fewer diffusion\nsteps as more tokens are generated, e.g., using 50 steps at the beginning and\ngradually decreasing to 5 steps at later stages. Because DiSA is derived from\nour finding specific to diffusion in autoregressive models, it is complementary\nto existing acceleration methods designed for diffusion alone. DiSA can be\nimplemented in only a few lines of code on existing models, and albeit simple,\nachieves 5-10times faster inference for MAR and Harmon and 1.4-2.5times\nfor FlowAR and xAR, while maintaining the generation quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20297.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6956
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20290",
            "authors": [
                {
                    "_id": "683534ac4ab4471ff376dfca",
                    "user": {
                        "_id": "6606d5370c87906d8e287fd6",
                        "avatarUrl": "/avatars/b01d9f9a2f1b3f3d18944f48e36cb54c.svg",
                        "isPro": false,
                        "fullname": "vincent liu",
                        "user": "vincentjliu",
                        "type": "user"
                    },
                    "name": "Vincent Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:50:29.968Z",
                    "hidden": false
                },
                {
                    "_id": "683534ac4ab4471ff376dfcb",
                    "name": "Ademi Adeniji",
                    "hidden": false
                },
                {
                    "_id": "683534ac4ab4471ff376dfcc",
                    "name": "Haotian Zhan",
                    "hidden": false
                },
                {
                    "_id": "683534ac4ab4471ff376dfcd",
                    "name": "Raunaq Bhirangi",
                    "hidden": false
                },
                {
                    "_id": "683534ac4ab4471ff376dfce",
                    "name": "Pieter Abbeel",
                    "hidden": false
                },
                {
                    "_id": "683534ac4ab4471ff376dfcf",
                    "name": "Lerrel Pinto",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:59:17.000Z",
            "submittedOnDailyAt": "2025-05-27T12:56:23.611Z",
            "title": "EgoZero: Robot Learning from Smart Glasses",
            "submittedOnDailyBy": {
                "_id": "6606d5370c87906d8e287fd6",
                "avatarUrl": "/avatars/b01d9f9a2f1b3f3d18944f48e36cb54c.svg",
                "isPro": false,
                "fullname": "vincent liu",
                "user": "vincentjliu",
                "type": "user"
            },
            "summary": "Despite recent progress in general purpose robotics, robot policies still lag\nfar behind basic human capabilities in the real world. Humans interact\nconstantly with the physical world, yet this rich data resource remains largely\nuntapped in robot learning. We propose EgoZero, a minimal system that learns\nrobust manipulation policies from human demonstrations captured with Project\nAria smart glasses, and zero robot data. EgoZero enables: (1)\nextraction of complete, robot-executable actions from in-the-wild, egocentric,\nhuman demonstrations, (2) compression of human visual observations into\nmorphology-agnostic state representations, and (3) closed-loop policy learning\nthat generalizes morphologically, spatially, and semantically. We deploy\nEgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot\ntransfer with 70% success rate over 7 manipulation tasks and only 20 minutes of\ndata collection per task. Our results suggest that in-the-wild human data can\nserve as a scalable foundation for real-world robot learning - paving the way\ntoward a future of abundant, diverse, and naturalistic training data for\nrobots. Code and videos are available at https://egozero-robot.github.io.",
            "upvotes": 1,
            "discussionId": "683534ae4ab4471ff376e023",
            "ai_summary": "EgoZero learns robust manipulation policies for robots using in-the-wild human demonstrations and zero robot data, enabling zero-shot transfer across diverse tasks.",
            "ai_keywords": [
                "EgoZero",
                "human demonstrations",
                "Project Aria smart glasses",
                "robot-executable actions",
                "egocentric",
                "morphology-agnostic state representations",
                "closed-loop policy learning",
                "zero-shot transfer",
                "gripper Franka Panda robot"
            ]
        },
        "publishedAt": "2025-05-26T13:59:17.000Z",
        "title": "EgoZero: Robot Learning from Smart Glasses",
        "summary": "Despite recent progress in general purpose robotics, robot policies still lag\nfar behind basic human capabilities in the real world. Humans interact\nconstantly with the physical world, yet this rich data resource remains largely\nuntapped in robot learning. We propose EgoZero, a minimal system that learns\nrobust manipulation policies from human demonstrations captured with Project\nAria smart glasses, and zero robot data. EgoZero enables: (1)\nextraction of complete, robot-executable actions from in-the-wild, egocentric,\nhuman demonstrations, (2) compression of human visual observations into\nmorphology-agnostic state representations, and (3) closed-loop policy learning\nthat generalizes morphologically, spatially, and semantically. We deploy\nEgoZero policies on a gripper Franka Panda robot and demonstrate zero-shot\ntransfer with 70% success rate over 7 manipulation tasks and only 20 minutes of\ndata collection per task. Our results suggest that in-the-wild human data can\nserve as a scalable foundation for real-world robot learning - paving the way\ntoward a future of abundant, diverse, and naturalistic training data for\nrobots. Code and videos are available at https://egozero-robot.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20290.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6606d5370c87906d8e287fd6",
            "avatarUrl": "/avatars/b01d9f9a2f1b3f3d18944f48e36cb54c.svg",
            "fullname": "vincent liu",
            "name": "vincentjliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20236",
            "authors": [
                {
                    "_id": "68365bc843cbbe13e0fabdad",
                    "name": "Weihao Xuan",
                    "hidden": false
                },
                {
                    "_id": "68365bc843cbbe13e0fabdae",
                    "name": "Qingcheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "68365bc843cbbe13e0fabdaf",
                    "name": "Heli Qi",
                    "hidden": false
                },
                {
                    "_id": "68365bc843cbbe13e0fabdb0",
                    "name": "Junjue Wang",
                    "hidden": false
                },
                {
                    "_id": "68365bc843cbbe13e0fabdb1",
                    "name": "Naoto Yokoya",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:16:36.000Z",
            "submittedOnDailyAt": "2025-05-27T23:12:11.079Z",
            "title": "Seeing is Believing, but How Much? A Comprehensive Analysis of\n  Verbalized Calibration in Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "63bc77661374e3ef9135735f",
                "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg",
                "isPro": false,
                "fullname": "Qingcheng Zeng",
                "user": "qcz",
                "type": "user"
            },
            "summary": "Uncertainty quantification is essential for assessing the reliability and\ntrustworthiness of modern AI systems. Among existing approaches, verbalized\nuncertainty, where models express their confidence through natural language,\nhas emerged as a lightweight and interpretable solution in large language\nmodels (LLMs). However, its effectiveness in vision-language models (VLMs)\nremains insufficiently studied. In this work, we conduct a comprehensive\nevaluation of verbalized confidence in VLMs, spanning three model categories,\nfour task domains, and three evaluation scenarios. Our results show that\ncurrent VLMs often display notable miscalibration across diverse tasks and\nsettings. Notably, visual reasoning models (i.e., thinking with images)\nconsistently exhibit better calibration, suggesting that modality-specific\nreasoning is critical for reliable uncertainty estimation. To further address\ncalibration challenges, we introduce Visual Confidence-Aware Prompting, a\ntwo-stage prompting strategy that improves confidence alignment in multimodal\nsettings. Overall, our study highlights the inherent miscalibration in VLMs\nacross modalities. More broadly, our findings underscore the fundamental\nimportance of modality alignment and model faithfulness in advancing reliable\nmultimodal systems.",
            "upvotes": 1,
            "discussionId": "68365bc943cbbe13e0fabdee",
            "ai_summary": "The study evaluates verbalized confidence in vision-language models, revealing miscalibration across tasks and suggesting that modality-specific reasoning and Visual Confidence-Aware Prompting can improve reliability.",
            "ai_keywords": [
                "vision-language models",
                "verbalized uncertainty",
                "natural language",
                "visual reasoning models",
                "multimodal settings",
                "confidence calibration",
                "Visual Confidence-Aware Prompting",
                "modality alignment",
                "model faithfulness"
            ]
        },
        "publishedAt": "2025-05-26T13:16:36.000Z",
        "title": "Seeing is Believing, but How Much? A Comprehensive Analysis of\n  Verbalized Calibration in Vision-Language Models",
        "summary": "Uncertainty quantification is essential for assessing the reliability and\ntrustworthiness of modern AI systems. Among existing approaches, verbalized\nuncertainty, where models express their confidence through natural language,\nhas emerged as a lightweight and interpretable solution in large language\nmodels (LLMs). However, its effectiveness in vision-language models (VLMs)\nremains insufficiently studied. In this work, we conduct a comprehensive\nevaluation of verbalized confidence in VLMs, spanning three model categories,\nfour task domains, and three evaluation scenarios. Our results show that\ncurrent VLMs often display notable miscalibration across diverse tasks and\nsettings. Notably, visual reasoning models (i.e., thinking with images)\nconsistently exhibit better calibration, suggesting that modality-specific\nreasoning is critical for reliable uncertainty estimation. To further address\ncalibration challenges, we introduce Visual Confidence-Aware Prompting, a\ntwo-stage prompting strategy that improves confidence alignment in multimodal\nsettings. Overall, our study highlights the inherent miscalibration in VLMs\nacross modalities. More broadly, our findings underscore the fundamental\nimportance of modality alignment and model faithfulness in advancing reliable\nmultimodal systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20236.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63bc77661374e3ef9135735f",
            "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg",
            "fullname": "Qingcheng Zeng",
            "name": "qcz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20225",
            "authors": [
                {
                    "_id": "6835d87e46c0a9c0f2cac425",
                    "name": "Hao Kang",
                    "hidden": false
                },
                {
                    "_id": "6835d87e46c0a9c0f2cac426",
                    "name": "Zichun Yu",
                    "hidden": false
                },
                {
                    "_id": "6835d87e46c0a9c0f2cac427",
                    "name": "Chenyan Xiong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:06:25.000Z",
            "submittedOnDailyAt": "2025-05-27T13:52:28.746Z",
            "title": "FLAME-MoE: A Transparent End-to-End Research Platform for\n  Mixture-of-Experts Language Models",
            "submittedOnDailyBy": {
                "_id": "649a943f575e60d3a87cfcdf",
                "avatarUrl": "/avatars/879f79628a30dddeab6e36e7d82bdbaa.svg",
                "isPro": false,
                "fullname": " Zichun Yu",
                "user": "yuzc19",
                "type": "user"
            },
            "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE.",
            "upvotes": 1,
            "discussionId": "6835d87f46c0a9c0f2cac47f",
            "ai_summary": "FLAME-MoE is an open-source research suite for MoE architectures in LLMs, providing tools to investigate scaling, routing, and expert behavior with reproducible experiments.",
            "ai_keywords": [
                "Mixture-of-Experts (MoE)",
                "decoder-only models",
                "top-8 gating",
                "shared experts",
                "training trace transparency",
                "co-activation matrices",
                "routing behavior"
            ]
        },
        "publishedAt": "2025-05-26T13:06:25.000Z",
        "title": "FLAME-MoE: A Transparent End-to-End Research Platform for\n  Mixture-of-Experts Language Models",
        "summary": "Recent large language models such as Gemini-1.5, DeepSeek-V3, and Llama-4\nincreasingly adopt Mixture-of-Experts (MoE) architectures, which offer strong\nefficiency-performance trade-offs by activating only a fraction of the model\nper token. Yet academic researchers still lack a fully open, end-to-end MoE\nplatform for investigating scaling, routing, and expert behavior. We release\nFLAME-MoE, a completely open-source research suite composed of seven\ndecoder-only models, ranging from 38M to 1.7B active parameters, whose\narchitecture--64 experts with top-8 gating and 2 shared experts--closely\nreflects modern production LLMs. All training data pipelines, scripts, logs,\nand checkpoints are publicly available to enable reproducible experimentation.\nAcross six evaluation tasks, FLAME-MoE improves average accuracy by up to 3.4\npoints over dense baselines trained with identical FLOPs. Leveraging full\ntraining trace transparency, we present initial analyses showing that (i)\nexperts increasingly specialize on distinct token subsets, (ii) co-activation\nmatrices remain sparse, reflecting diverse expert usage, and (iii) routing\nbehavior stabilizes early in training. All code, training logs, and model\ncheckpoints are available at https://github.com/cmu-flame/FLAME-MoE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20225.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "649a943f575e60d3a87cfcdf",
            "avatarUrl": "/avatars/879f79628a30dddeab6e36e7d82bdbaa.svg",
            "fullname": " Zichun Yu",
            "name": "yuzc19",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.19800",
            "authors": [
                {
                    "_id": "68356e736bb42c7e99d2d266",
                    "user": {
                        "_id": "5f04bd384ec31d33a72116d1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1594145966049-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Zaid Alyafeai",
                        "user": "Zaid",
                        "type": "user"
                    },
                    "name": "Zaid Alyafeai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T08:47:10.858Z",
                    "hidden": false
                },
                {
                    "_id": "68356e736bb42c7e99d2d267",
                    "name": "Maged S. Al-Shaibani",
                    "hidden": false
                },
                {
                    "_id": "68356e736bb42c7e99d2d268",
                    "user": {
                        "_id": "6808bf97ffadd78ec71cb721",
                        "avatarUrl": "/avatars/9adca3142c06b8f69889fcbe85fa374d.svg",
                        "isPro": false,
                        "fullname": "Bernard Ghanem",
                        "user": "bernardghanem",
                        "type": "user"
                    },
                    "name": "Bernard Ghanem",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T12:42:05.974Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T10:31:26.000Z",
            "submittedOnDailyAt": "2025-05-27T07:09:18.429Z",
            "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs",
            "submittedOnDailyBy": {
                "_id": "5f04bd384ec31d33a72116d1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1594145966049-noauth.jpeg",
                "isPro": false,
                "fullname": "Zaid Alyafeai",
                "user": "Zaid",
                "type": "user"
            },
            "summary": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.",
            "upvotes": 1,
            "discussionId": "68356e746bb42c7e99d2d2af",
            "projectPage": "https://ivul-kaust.github.io/MOLE/blog",
            "githubRepo": "https://github.com/IVUL-KAUST/MOLE",
            "ai_summary": "A framework leveraging Large Language Models (LLMs) is introduced for automatic metadata extraction from scientific papers, with a focus on datasets from languages other than Arabic.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "schema-driven methodology",
                "benchmark",
                "few-shot learning",
                "web browsing integration"
            ]
        },
        "publishedAt": "2025-05-26T06:31:26.000Z",
        "title": "MOLE: Metadata Extraction and Validation in Scientific Papers Using LLMs",
        "summary": "Metadata extraction is essential for cataloging and preserving datasets,\nenabling effective research discovery and reproducibility, especially given the\ncurrent exponential growth in scientific research. While Masader (Alyafeai et\nal.,2021) laid the groundwork for extracting a wide range of metadata\nattributes from Arabic NLP datasets' scholarly articles, it relies heavily on\nmanual annotation. In this paper, we present MOLE, a framework that leverages\nLarge Language Models (LLMs) to automatically extract metadata attributes from\nscientific papers covering datasets of languages other than Arabic. Our\nschema-driven methodology processes entire documents across multiple input\nformats and incorporates robust validation mechanisms for consistent output.\nAdditionally, we introduce a new benchmark to evaluate the research progress on\nthis task. Through systematic analysis of context length, few-shot learning,\nand web browsing integration, we demonstrate that modern LLMs show promising\nresults in automating this task, highlighting the need for further future work\nimprovements to ensure consistent and reliable performance. We release the\ncode: https://github.com/IVUL-KAUST/MOLE and dataset:\nhttps://huggingface.co/datasets/IVUL-KAUST/MOLE for the research community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19800.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f04bd384ec31d33a72116d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1594145966049-noauth.jpeg",
            "fullname": "Zaid Alyafeai",
            "name": "Zaid",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 48
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19440",
            "authors": [
                {
                    "_id": "6835c1e4bc28496925c784de",
                    "name": "Shashata Sawmya",
                    "hidden": false
                },
                {
                    "_id": "6835c1e4bc28496925c784df",
                    "name": "Micah Adler",
                    "hidden": false
                },
                {
                    "_id": "6835c1e4bc28496925c784e0",
                    "name": "Nir Shavit",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T02:59:54.000Z",
            "submittedOnDailyAt": "2025-05-27T12:18:12.612Z",
            "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale\n  in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "5e4b943a37cb5b49818287b5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e4b943a37cb5b49818287b5/Sd8QOGFY-_B1z4UlFTuPf.jpeg",
                "isPro": false,
                "fullname": "Shayekh Islam",
                "user": "shayekh",
                "type": "user"
            },
            "summary": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models.",
            "upvotes": 1,
            "discussionId": "6835c1e5bc28496925c7852d",
            "ai_summary": "The study examines interpretable categorical features in large language models, using sparse autoencoders to identify semantic concept emergence over time, across layers, and varying sizes, revealing spatial feature reactivation.",
            "ai_keywords": [
                "large language models",
                "interpretable categorical features",
                "training checkpoints",
                "transformer layers",
                "varying model sizes",
                "sparse autoencoders",
                "mechanistic interpretability",
                "neural activations",
                "semantic concepts",
                "representational dynamics"
            ]
        },
        "publishedAt": "2025-05-25T22:59:54.000Z",
        "title": "The Birth of Knowledge: Emergent Features across Time, Space, and Scale\n  in Large Language Models",
        "summary": "This paper studies the emergence of interpretable categorical features within\nlarge language models (LLMs), analyzing their behavior across training\ncheckpoints (time), transformer layers (space), and varying model sizes\n(scale). Using sparse autoencoders for mechanistic interpretability, we\nidentify when and where specific semantic concepts emerge within neural\nactivations. Results indicate clear temporal and scale-specific thresholds for\nfeature emergence across multiple domains. Notably, spatial analysis reveals\nunexpected semantic reactivation, with early-layer features re-emerging at\nlater layers, challenging standard assumptions about representational dynamics\nin transformer models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19440.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e4b943a37cb5b49818287b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e4b943a37cb5b49818287b5/Sd8QOGFY-_B1z4UlFTuPf.jpeg",
            "fullname": "Shayekh Islam",
            "name": "shayekh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.19415",
            "authors": [
                {
                    "_id": "68360763eccf5a50adcddade",
                    "name": "Hang Hua",
                    "hidden": false
                },
                {
                    "_id": "68360763eccf5a50adcddadf",
                    "name": "Ziyun Zeng",
                    "hidden": false
                },
                {
                    "_id": "68360763eccf5a50adcddae0",
                    "name": "Yizhi Song",
                    "hidden": false
                },
                {
                    "_id": "68360763eccf5a50adcddae1",
                    "name": "Yunlong Tang",
                    "hidden": false
                },
                {
                    "_id": "68360763eccf5a50adcddae2",
                    "name": "Liu He",
                    "hidden": false
                },
                {
                    "_id": "68360763eccf5a50adcddae3",
                    "name": "Daniel Aliaga",
                    "hidden": false
                },
                {
                    "_id": "68360763eccf5a50adcddae4",
                    "name": "Wei Xiong",
                    "hidden": false
                },
                {
                    "_id": "68360763eccf5a50adcddae5",
                    "name": "Jiebo Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T02:07:24.000Z",
            "submittedOnDailyAt": "2025-05-27T17:12:08.308Z",
            "title": "MMIG-Bench: Towards Comprehensive and Explainable Evaluation of\n  Multi-Modal Image Generation Models",
            "submittedOnDailyBy": {
                "_id": "639f8277beb95d698de007dd",
                "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg",
                "isPro": false,
                "fullname": "HangHua",
                "user": "hhua2",
                "type": "user"
            },
            "summary": "Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and\nGemini 2.5 Pro excel at following complex instructions, editing images and\nmaintaining concept consistency. However, they are still evaluated by disjoint\ntoolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning,\nand customized image generation benchmarks that overlook compositional\nsemantics and common knowledge. We propose MMIG-Bench, a comprehensive\nMulti-Modal Image Generation Benchmark that unifies these tasks by pairing\n4,850 richly annotated text prompts with 1,750 multi-view reference images\nacross 380 subjects, spanning humans, animals, objects, and artistic styles.\nMMIG-Bench is equipped with a three-level evaluation framework: (1) low-level\nmetrics for visual artifacts and identity preservation of objects; (2) novel\nAspect Matching Score (AMS): a VQA-based mid-level metric that delivers\nfine-grained prompt-image alignment and shows strong correlation with human\njudgments; and (3) high-level metrics for aesthetics and human preference.\nUsing MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5\nPro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human\nratings, yielding in-depth insights into architecture and data design. We will\nrelease the dataset and evaluation code to foster rigorous, unified evaluation\nand accelerate future innovations in multi-modal image generation.",
            "upvotes": 1,
            "discussionId": "6836076aeccf5a50adcddcfc",
            "ai_summary": "A comprehensive benchmark, MMIG-Bench, evaluates multi-modal image generators using text prompts and reference images, providing detailed insights through low-level, mid-level, and high-level metrics.",
            "ai_keywords": [
                "multi-modal image generators",
                "GPT-4o",
                "Gemini 2.0 Flash",
                "Gemini 2.5 Pro",
                "MMIG-Bench",
                "text-to-image (T2I)",
                "multi-view reference images",
                "Aspect Matching Score (AMS)",
                "VQA-based",
                "visual artifacts",
                "identity preservation",
                "aesthetics",
                "human preference"
            ]
        },
        "publishedAt": "2025-05-25T22:07:24.000Z",
        "title": "MMIG-Bench: Towards Comprehensive and Explainable Evaluation of\n  Multi-Modal Image Generation Models",
        "summary": "Recent multimodal image generators such as GPT-4o, Gemini 2.0 Flash, and\nGemini 2.5 Pro excel at following complex instructions, editing images and\nmaintaining concept consistency. However, they are still evaluated by disjoint\ntoolkits: text-to-image (T2I) benchmarks that lacks multi-modal conditioning,\nand customized image generation benchmarks that overlook compositional\nsemantics and common knowledge. We propose MMIG-Bench, a comprehensive\nMulti-Modal Image Generation Benchmark that unifies these tasks by pairing\n4,850 richly annotated text prompts with 1,750 multi-view reference images\nacross 380 subjects, spanning humans, animals, objects, and artistic styles.\nMMIG-Bench is equipped with a three-level evaluation framework: (1) low-level\nmetrics for visual artifacts and identity preservation of objects; (2) novel\nAspect Matching Score (AMS): a VQA-based mid-level metric that delivers\nfine-grained prompt-image alignment and shows strong correlation with human\njudgments; and (3) high-level metrics for aesthetics and human preference.\nUsing MMIG-Bench, we benchmark 17 state-of-the-art models, including Gemini 2.5\nPro, FLUX, DreamBooth, and IP-Adapter, and validate our metrics with 32k human\nratings, yielding in-depth insights into architecture and data design. We will\nrelease the dataset and evaluation code to foster rigorous, unified evaluation\nand accelerate future innovations in multi-modal image generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19415.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "639f8277beb95d698de007dd",
            "avatarUrl": "/avatars/57f223ccd9d3cb03166ccf0e41361c58.svg",
            "fullname": "HangHua",
            "name": "hhua2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18497",
            "authors": [
                {
                    "_id": "68365b399951c3c658a05903",
                    "name": "Kefan Yu",
                    "hidden": false
                },
                {
                    "_id": "68365b399951c3c658a05904",
                    "name": "Qingcheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "68365b399951c3c658a05905",
                    "name": "Weihao Xuan",
                    "hidden": false
                },
                {
                    "_id": "68365b399951c3c658a05906",
                    "name": "Wanxin Li",
                    "hidden": false
                },
                {
                    "_id": "68365b399951c3c658a05907",
                    "name": "Jingyi Wu",
                    "hidden": false
                },
                {
                    "_id": "68365b399951c3c658a05908",
                    "name": "Rob Voigt",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T04:24:59.000Z",
            "submittedOnDailyAt": "2025-05-27T23:10:41.614Z",
            "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic\n  Competence in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "63bc77661374e3ef9135735f",
                "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg",
                "isPro": false,
                "fullname": "Qingcheng Zeng",
                "user": "qcz",
                "type": "user"
            },
            "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution (Sravanthi et\nal. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which\nrequire substantial pragmatic understanding. However, how LLMs acquire this\ncompetence throughout the training process remains poorly understood. In this\nwork, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of\nalternatives, designed to evaluate whether LLMs at different training stages\ncan accurately infer nuanced speaker intentions. Each instance pairs two\ncontextually appropriate but pragmatically distinct continuations, enabling\nfine-grained assessment of both pragmatic interpretation and contrastive\nreasoning. We systematically evaluate 22 LLMs across key training stages:\npre-training, supervised fine-tuning (SFT), and preference optimization, to\nexamine the development of pragmatic competence. Our results show that even\nbase models exhibit notable sensitivity to pragmatic cues, which improves\nconsistently with increases in model and data scale. Additionally, SFT and RLHF\ncontribute further gains, particularly in cognitive-pragmatic reasoning. These\nfindings highlight pragmatic competence as an emergent and compositional\nproperty of LLM training and offer new insights for aligning models with human\ncommunicative norms.",
            "upvotes": 1,
            "discussionId": "68365b3a9951c3c658a05945",
            "ai_summary": "ALTPRAG dataset evaluates pragmatic competence in LLMs across training stages, showing improvements with increased scale and specific training methods.",
            "ai_keywords": [
                "LLMs",
                "social intelligence tasks",
                "implicature resolution",
                "theory-of-mind reasoning",
                "pragmatic understanding",
                "alternative",
                "pragmatic concept",
                "pragmatic interpretation",
                "contrastive reasoning",
                "pre-training",
                "supervised fine-tuning",
                "preference optimization",
                "cognitive-pragmatic reasoning",
                "RLHF",
                "pragmatic cues",
                "emergent property",
                "compositional property"
            ]
        },
        "publishedAt": "2025-05-24T00:24:59.000Z",
        "title": "The Pragmatic Mind of Machines: Tracing the Emergence of Pragmatic\n  Competence in Large Language Models",
        "summary": "Current large language models (LLMs) have demonstrated emerging capabilities\nin social intelligence tasks, including implicature resolution (Sravanthi et\nal. (2024)) and theory-of-mind reasoning (Shapira et al. (2024)), both of which\nrequire substantial pragmatic understanding. However, how LLMs acquire this\ncompetence throughout the training process remains poorly understood. In this\nwork, we introduce ALTPRAG, a dataset grounded in the pragmatic concept of\nalternatives, designed to evaluate whether LLMs at different training stages\ncan accurately infer nuanced speaker intentions. Each instance pairs two\ncontextually appropriate but pragmatically distinct continuations, enabling\nfine-grained assessment of both pragmatic interpretation and contrastive\nreasoning. We systematically evaluate 22 LLMs across key training stages:\npre-training, supervised fine-tuning (SFT), and preference optimization, to\nexamine the development of pragmatic competence. Our results show that even\nbase models exhibit notable sensitivity to pragmatic cues, which improves\nconsistently with increases in model and data scale. Additionally, SFT and RLHF\ncontribute further gains, particularly in cognitive-pragmatic reasoning. These\nfindings highlight pragmatic competence as an emergent and compositional\nproperty of LLM training and offer new insights for aligning models with human\ncommunicative norms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18497.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bc77661374e3ef9135735f",
            "avatarUrl": "/avatars/94b04545ed9d30bfe58691672a0b5618.svg",
            "fullname": "Qingcheng Zeng",
            "name": "qcz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18291",
            "authors": [
                {
                    "_id": "6835e8411900f053ee40f208",
                    "name": "Zifu Wan",
                    "hidden": false
                },
                {
                    "_id": "6835e8411900f053ee40f209",
                    "name": "Yaqi Xie",
                    "hidden": false
                },
                {
                    "_id": "6835e8411900f053ee40f20a",
                    "name": "Ce Zhang",
                    "hidden": false
                },
                {
                    "_id": "6835e8411900f053ee40f20b",
                    "name": "Zhiqiu Lin",
                    "hidden": false
                },
                {
                    "_id": "6835e8411900f053ee40f20c",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "6835e8411900f053ee40f20d",
                    "name": "Simon Stepputtis",
                    "hidden": false
                },
                {
                    "_id": "6835e8411900f053ee40f20e",
                    "name": "Deva Ramanan",
                    "hidden": false
                },
                {
                    "_id": "6835e8411900f053ee40f20f",
                    "name": "Katia Sycara",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T18:36:13.000Z",
            "submittedOnDailyAt": "2025-05-27T15:01:52.177Z",
            "title": "InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning",
            "submittedOnDailyBy": {
                "_id": "65493c5b8fde27109badf365",
                "avatarUrl": "/avatars/8ff7e84f7fe3cefd80c344b17986c5b3.svg",
                "isPro": false,
                "fullname": "Zifu Wan",
                "user": "zifuwan",
                "type": "user"
            },
            "summary": "Large multimodal foundation models, particularly in the domains of language\nand vision, have significantly advanced various tasks, including robotics,\nautonomous driving, information retrieval, and grounding. However, many of\nthese models perceive objects as indivisible, overlooking the components that\nconstitute them. Understanding these components and their associated\naffordances provides valuable insights into an object's functionality, which is\nfundamental for performing a wide range of tasks. In this work, we introduce a\nnovel real-world benchmark, InstructPart, comprising hand-labeled part\nsegmentation annotations and task-oriented instructions to evaluate the\nperformance of current models in understanding and executing part-level tasks\nwithin everyday contexts. Through our experiments, we demonstrate that\ntask-oriented part segmentation remains a challenging problem, even for\nstate-of-the-art Vision-Language Models (VLMs). In addition to our benchmark,\nwe introduce a simple baseline that achieves a twofold performance improvement\nthrough fine-tuning with our dataset. With our dataset and benchmark, we aim to\nfacilitate research on task-oriented part segmentation and enhance the\napplicability of VLMs across various domains, including robotics, virtual\nreality, information retrieval, and other related fields. Project website:\nhttps://zifuwan.github.io/InstructPart/.",
            "upvotes": 1,
            "discussionId": "6835e8471900f053ee40f369",
            "projectPage": "https://zifuwan.github.io/InstructPart/",
            "githubRepo": "https://github.com/zifuwan/InstructPart/tree/dataset",
            "ai_summary": "A new benchmark, InstructPart, and a task-oriented part segmentation dataset are introduced to evaluate and improve the performance of Vision-Language Models in real-world contexts.",
            "ai_keywords": [
                "Vision-Language Models",
                "part segmentation",
                "part-level tasks",
                "task-oriented instructions",
                "fine-tuning"
            ]
        },
        "publishedAt": "2025-05-23T14:36:13.000Z",
        "title": "InstructPart: Task-Oriented Part Segmentation with Instruction Reasoning",
        "summary": "Large multimodal foundation models, particularly in the domains of language\nand vision, have significantly advanced various tasks, including robotics,\nautonomous driving, information retrieval, and grounding. However, many of\nthese models perceive objects as indivisible, overlooking the components that\nconstitute them. Understanding these components and their associated\naffordances provides valuable insights into an object's functionality, which is\nfundamental for performing a wide range of tasks. In this work, we introduce a\nnovel real-world benchmark, InstructPart, comprising hand-labeled part\nsegmentation annotations and task-oriented instructions to evaluate the\nperformance of current models in understanding and executing part-level tasks\nwithin everyday contexts. Through our experiments, we demonstrate that\ntask-oriented part segmentation remains a challenging problem, even for\nstate-of-the-art Vision-Language Models (VLMs). In addition to our benchmark,\nwe introduce a simple baseline that achieves a twofold performance improvement\nthrough fine-tuning with our dataset. With our dataset and benchmark, we aim to\nfacilitate research on task-oriented part segmentation and enhance the\napplicability of VLMs across various domains, including robotics, virtual\nreality, information retrieval, and other related fields. Project website:\nhttps://zifuwan.github.io/InstructPart/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18291.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65493c5b8fde27109badf365",
            "avatarUrl": "/avatars/8ff7e84f7fe3cefd80c344b17986c5b3.svg",
            "fullname": "Zifu Wan",
            "name": "zifuwan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.16968",
            "authors": [
                {
                    "_id": "683656aefd55e753bf26ed3e",
                    "name": "Ahmed Heakl",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed3f",
                    "name": "Sarim Hashmi",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed40",
                    "name": "Gustavo Bertolo Stahl",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed41",
                    "name": "Seung Hun Eddie Han",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed42",
                    "name": "Salman Khan",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed43",
                    "name": "Abdulrahman Mahmoud",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/708iXv1imkCQMovtdMXZK.png"
            ],
            "publishedAt": "2025-05-22T17:48:53.000Z",
            "submittedOnDailyAt": "2025-05-27T22:50:50.968Z",
            "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
            "submittedOnDailyBy": {
                "_id": "656864e12d73834278a8dea7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                "isPro": true,
                "fullname": "Ahmed Heakl",
                "user": "ahmedheakl",
                "type": "user"
            },
            "summary": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\nleftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train the CASS family of domain-specific language\nmodels, achieving 95% source translation accuracy and 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, and Hipify. Our generated code matches native performance in\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress in GPU compiler tooling, binary\ncompatibility, and LLM-guided hardware translation. Dataset and benchmark are\non\nhttps://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}},\nwith code at\nhttps://github.com/GustavoStahl/CASS{blue{GitHub}}.",
            "upvotes": 1,
            "discussionId": "683656b0fd55e753bf26edf7",
            "githubRepo": "https://github.com/GustavoStahl/CASS",
            "ai_summary": "CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.",
            "ai_keywords": [
                "cross-architecture GPU code transpilation",
                "CASS",
                "CUDA",
                "HIP",
                "Nvidia SASS",
                "AMD RDNA3",
                "domain-specific language models",
                "source translation accuracy",
                "assembly translation accuracy",
                "native performance",
                "CASS-Bench",
                "GPU compiler tooling",
                "binary compatibility",
                "LLM-guided hardware translation"
            ]
        },
        "publishedAt": "2025-05-22T13:48:53.000Z",
        "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
        "summary": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\nleftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train the CASS family of domain-specific language\nmodels, achieving 95% source translation accuracy and 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, and Hipify. Our generated code matches native performance in\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress in GPU compiler tooling, binary\ncompatibility, and LLM-guided hardware translation. Dataset and benchmark are\non\nhttps://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}},\nwith code at\nhttps://github.com/GustavoStahl/CASS{blue{GitHub}}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/708iXv1imkCQMovtdMXZK.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16968.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "fullname": "Ahmed Heakl",
            "name": "ahmedheakl",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 39
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.14071",
            "authors": [
                {
                    "_id": "6836384a1ec776c1b0fc0d7f",
                    "name": "Woody Haosheng Gan",
                    "hidden": false
                },
                {
                    "_id": "6836384a1ec776c1b0fc0d80",
                    "name": "Deqing Fu",
                    "hidden": false
                },
                {
                    "_id": "6836384a1ec776c1b0fc0d81",
                    "name": "Julian Asilis",
                    "hidden": false
                },
                {
                    "_id": "6836384a1ec776c1b0fc0d82",
                    "name": "Ollie Liu",
                    "hidden": false
                },
                {
                    "_id": "6836384a1ec776c1b0fc0d83",
                    "name": "Dani Yogatama",
                    "hidden": false
                },
                {
                    "_id": "6836384a1ec776c1b0fc0d84",
                    "name": "Vatsal Sharan",
                    "hidden": false
                },
                {
                    "_id": "6836384a1ec776c1b0fc0d85",
                    "name": "Robin Jia",
                    "hidden": false
                },
                {
                    "_id": "6836384a1ec776c1b0fc0d86",
                    "name": "Willie Neiswanger",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T08:23:08.000Z",
            "submittedOnDailyAt": "2025-05-27T20:40:33.421Z",
            "title": "Textual Steering Vectors Can Improve Visual Understanding in Multimodal\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "63c8454e46421a2efe82709d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
                "isPro": true,
                "fullname": "Deqing Fu",
                "user": "deqing",
                "type": "user"
            },
            "summary": "Steering methods have emerged as effective and targeted tools for guiding\nlarge language models' (LLMs) behavior without modifying their parameters.\nMultimodal large language models (MLLMs), however, do not currently enjoy the\nsame suite of techniques, due in part to their recency and architectural\ndiversity. Inspired by this gap, we investigate whether MLLMs can be steered\nusing vectors derived from their text-only LLM backbone, via sparse\nautoencoders (SAEs), mean shift, and linear probing. We find that text-derived\nsteering consistently enhances multimodal accuracy across diverse MLLM\narchitectures and visual tasks. In particular, mean shift boosts spatial\nrelationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to\n+3.3%, outperforming prompting and exhibiting strong generalization to\nout-of-distribution datasets. These results highlight textual steering vectors\nas a powerful, efficient mechanism for enhancing grounding in MLLMs with\nminimal additional data collection and computational overhead.",
            "upvotes": 1,
            "discussionId": "6836384b1ec776c1b0fc0dcb",
            "ai_summary": "Text-derived steering via sparse autoencoders, mean shift, and linear probing enhances multimodal accuracy in large language models without requiring parameter modifications or significant additional data or computation.",
            "ai_keywords": [
                "sparse autoencoders",
                "mean shift",
                "linear probing",
                "multimodal large language models",
                "CV-Bench",
                "spatial relationship accuracy",
                "counting accuracy",
                "textual steering vectors",
                "grounding"
            ]
        },
        "publishedAt": "2025-05-20T04:23:08.000Z",
        "title": "Textual Steering Vectors Can Improve Visual Understanding in Multimodal\n  Large Language Models",
        "summary": "Steering methods have emerged as effective and targeted tools for guiding\nlarge language models' (LLMs) behavior without modifying their parameters.\nMultimodal large language models (MLLMs), however, do not currently enjoy the\nsame suite of techniques, due in part to their recency and architectural\ndiversity. Inspired by this gap, we investigate whether MLLMs can be steered\nusing vectors derived from their text-only LLM backbone, via sparse\nautoencoders (SAEs), mean shift, and linear probing. We find that text-derived\nsteering consistently enhances multimodal accuracy across diverse MLLM\narchitectures and visual tasks. In particular, mean shift boosts spatial\nrelationship accuracy on CV-Bench by up to +7.3% and counting accuracy by up to\n+3.3%, outperforming prompting and exhibiting strong generalization to\nout-of-distribution datasets. These results highlight textual steering vectors\nas a powerful, efficient mechanism for enhancing grounding in MLLMs with\nminimal additional data collection and computational overhead.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14071.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c8454e46421a2efe82709d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
            "fullname": "Deqing Fu",
            "name": "deqing",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.12737",
            "authors": [
                {
                    "_id": "683572b271c57c48d732da5e",
                    "name": "Hongjoon Ahn",
                    "hidden": false
                },
                {
                    "_id": "683572b271c57c48d732da5f",
                    "name": "Heewoong Choi",
                    "hidden": false
                },
                {
                    "_id": "683572b271c57c48d732da60",
                    "user": {
                        "_id": "644e78e8d6001776ed76bff0",
                        "avatarUrl": "/avatars/04af3c60f0fd186723ec5691e7465ace.svg",
                        "isPro": false,
                        "fullname": "Jisu Han",
                        "user": "JisuHann",
                        "type": "user"
                    },
                    "name": "Jisu Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T12:13:32.222Z",
                    "hidden": false
                },
                {
                    "_id": "683572b271c57c48d732da61",
                    "name": "Taesup Moon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T05:51:11.000Z",
            "submittedOnDailyAt": "2025-05-27T10:45:26.877Z",
            "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "644e78e8d6001776ed76bff0",
                "avatarUrl": "/avatars/04af3c60f0fd186723ec5691e7465ace.svg",
                "isPro": false,
                "fullname": "Jisu Han",
                "user": "JisuHann",
                "type": "user"
            },
            "summary": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical\nlearning paradigm where goal-reaching policies are trained from abundant\nunlabeled (reward-free) datasets without additional environment interaction.\nHowever, offline GCRL still struggles with long-horizon tasks, even with recent\nadvances that employ hierarchical policy structures, such as HIQL. By\nidentifying the root cause of this challenge, we observe the following\ninsights: First, performance bottlenecks mainly stem from the high-level\npolicy's inability to generate appropriate subgoals. Second, when learning the\nhigh-level policy in the long-horizon regime, the sign of the advantage signal\nfrequently becomes incorrect. Thus, we argue that improving the value function\nto produce a clear advantage signal for learning the high-level policy is\nessential. In this paper, we propose a simple yet effective solution:\nOption-aware Temporally Abstracted value learning, dubbed OTA, which\nincorporates temporal abstraction into the temporal-difference learning\nprocess. By modifying the value update to be option-aware, the proposed\nlearning scheme contracts the effective horizon length, enabling better\nadvantage estimates even in long-horizon regimes. We experimentally show that\nthe high-level policy extracted using the OTA value function achieves strong\nperformance on complex tasks from OGBench, a recently proposed offline GCRL\nbenchmark, including maze navigation and visual robotic manipulation\nenvironments.",
            "upvotes": 1,
            "discussionId": "683572b471c57c48d732dad1",
            "ai_summary": "Option-aware Temporally Abstracted (OTA) value learning improves offline goal-conditioned reinforcement learning performance by refining the high-level policy through better advantage estimates in long-horizon settings.",
            "ai_keywords": [
                "goal-conditioned reinforcement learning",
                "offline GCRL",
                "high-level policy",
                "subgoals",
                "advantage signal",
                "value function",
                "temporal abstraction",
                "temporal-difference learning",
                "OGBench",
                "maze navigation",
                "visual robotic manipulation"
            ]
        },
        "publishedAt": "2025-05-19T01:51:11.000Z",
        "title": "Option-aware Temporally Abstracted Value for Offline Goal-Conditioned\n  Reinforcement Learning",
        "summary": "Offline goal-conditioned reinforcement learning (GCRL) offers a practical\nlearning paradigm where goal-reaching policies are trained from abundant\nunlabeled (reward-free) datasets without additional environment interaction.\nHowever, offline GCRL still struggles with long-horizon tasks, even with recent\nadvances that employ hierarchical policy structures, such as HIQL. By\nidentifying the root cause of this challenge, we observe the following\ninsights: First, performance bottlenecks mainly stem from the high-level\npolicy's inability to generate appropriate subgoals. Second, when learning the\nhigh-level policy in the long-horizon regime, the sign of the advantage signal\nfrequently becomes incorrect. Thus, we argue that improving the value function\nto produce a clear advantage signal for learning the high-level policy is\nessential. In this paper, we propose a simple yet effective solution:\nOption-aware Temporally Abstracted value learning, dubbed OTA, which\nincorporates temporal abstraction into the temporal-difference learning\nprocess. By modifying the value update to be option-aware, the proposed\nlearning scheme contracts the effective horizon length, enabling better\nadvantage estimates even in long-horizon regimes. We experimentally show that\nthe high-level policy extracted using the OTA value function achieves strong\nperformance on complex tasks from OGBench, a recently proposed offline GCRL\nbenchmark, including maze navigation and visual robotic manipulation\nenvironments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12737.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644e78e8d6001776ed76bff0",
            "avatarUrl": "/avatars/04af3c60f0fd186723ec5691e7465ace.svg",
            "fullname": "Jisu Han",
            "name": "JisuHann",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]