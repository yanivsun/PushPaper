[
    {
        "paper": {
            "id": "2505.19897",
            "authors": [
                {
                    "_id": "6835e75d649a767a0771f8a4",
                    "user": {
                        "_id": "6064a0eeb1703ddba0d458b9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                        "isPro": false,
                        "fullname": "Qiushi",
                        "user": "QiushiSun",
                        "type": "user"
                    },
                    "name": "Qiushi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:14.344Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a5",
                    "name": "Zhoumianze Liu",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a6",
                    "name": "Chang Ma",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a7",
                    "user": {
                        "_id": "642b9861bb77f8456634b048",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/ZT-oJrw5BsADC-gZT_i25.jpeg",
                        "isPro": false,
                        "fullname": "Zichen Ding",
                        "user": "heroding77",
                        "type": "user"
                    },
                    "name": "Zichen Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:08.025Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a8",
                    "user": {
                        "_id": "64e6cf78ecce34cb442dc889",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
                        "isPro": false,
                        "fullname": "Fangzhi Xu",
                        "user": "xufangzhi",
                        "type": "user"
                    },
                    "name": "Fangzhi Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:12.170Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8a9",
                    "name": "Zhangyue Yin",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8aa",
                    "user": {
                        "_id": "64a7c6e223622f7f189bcbe1",
                        "avatarUrl": "/avatars/4f13a7ed0d2b8d8dfff7dc650e46450a.svg",
                        "isPro": false,
                        "fullname": "haiteng zhao",
                        "user": "haitengzhao",
                        "type": "user"
                    },
                    "name": "Haiteng Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:14:29.511Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8ab",
                    "name": "Zhenyu Wu",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8ac",
                    "user": {
                        "_id": "63340dbbd92c5842ae71d1e9",
                        "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
                        "isPro": false,
                        "fullname": "Kanzhi Cheng",
                        "user": "cckevinn",
                        "type": "user"
                    },
                    "name": "Kanzhi Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:03.364Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8ad",
                    "name": "Zhaoyang Liu",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8ae",
                    "name": "Jianing Wang",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8af",
                    "name": "Qintong Li",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b0",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b1",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b2",
                    "user": {
                        "_id": "6776ae0c91b4c75dac91249c",
                        "avatarUrl": "/avatars/a43139f65ad7086426d9757a1bcb7080.svg",
                        "isPro": false,
                        "fullname": "Oran Feng",
                        "user": "xiachongfeng",
                        "type": "user"
                    },
                    "name": "Xiachong Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:05.718Z",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b3",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b4",
                    "name": "Ben Kao",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b5",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b6",
                    "name": "Biqing Qi",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b7",
                    "name": "Lingpeng Kong",
                    "hidden": false
                },
                {
                    "_id": "6835e75d649a767a0771f8b8",
                    "name": "Zhiyong Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/kBkmkf8TUS9QTLLNJh9Cc.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/MuRvbGQbB4n2WBymZqGh1.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/hpztzin1L2YJpACBBIbdb.gif"
            ],
            "publishedAt": "2025-05-26T12:27:27.000Z",
            "submittedOnDailyAt": "2025-05-28T12:25:48.176Z",
            "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows",
            "submittedOnDailyBy": {
                "_id": "6064a0eeb1703ddba0d458b9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
                "isPro": false,
                "fullname": "Qiushi",
                "user": "QiushiSun",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/.",
            "upvotes": 84,
            "discussionId": "6835e760649a767a0771f984",
            "projectPage": "https://qiushisun.github.io/ScienceBoard-Home/",
            "githubRepo": "https://github.com/OS-Copilot/ScienceBoard",
            "ai_summary": "ScienceBoard provides a realistic scientific workflow environment and benchmark to evaluate the performance of LLM-based agents, demonstrating their current limitations in complex scientific tasks.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "computer-using agents",
                "scientific discovery",
                "dynamic workflows",
                "integrated professional software",
                "autonomous interaction",
                "ScienceBoard",
                "benchmark",
                "evaluation",
                "state-of-the-art backbones",
                "GPT-4o",
                "Claude 3.7",
                "UI-TARS",
                "biochemistry",
                "astronomy",
                "geoinformatics",
                "design principles"
            ]
        },
        "publishedAt": "2025-05-26T08:27:27.000Z",
        "title": "ScienceBoard: Evaluating Multimodal Autonomous Agents in Realistic\n  Scientific Workflows",
        "summary": "Large Language Models (LLMs) have extended their impact beyond Natural\nLanguage Processing, substantially fostering the development of\ninterdisciplinary research. Recently, various LLM-based agents have been\ndeveloped to assist scientific discovery progress across multiple aspects and\ndomains. Among these, computer-using agents, capable of interacting with\noperating systems as humans do, are paving the way to automated scientific\nproblem-solving and addressing routines in researchers' workflows. Recognizing\nthe transformative potential of these agents, we introduce ScienceBoard, which\nencompasses two complementary contributions: (i) a realistic, multi-domain\nenvironment featuring dynamic and visually rich scientific workflows with\nintegrated professional software, where agents can autonomously interact via\ndifferent interfaces to accelerate complex research tasks and experiments; and\n(ii) a challenging benchmark of 169 high-quality, rigorously validated\nreal-world tasks curated by humans, spanning scientific-discovery workflows in\ndomains such as biochemistry, astronomy, and geoinformatics. Extensive\nevaluations of agents with state-of-the-art backbones (e.g., GPT-4o, Claude\n3.7, UI-TARS) show that, despite some promising results, they still fall short\nof reliably assisting scientists in complex workflows, achieving only a 15%\noverall success rate. In-depth analysis further provides valuable insights for\naddressing current agent limitations and more effective design principles,\npaving the way to build more capable agents for scientific discovery. Our code,\nenvironment, and benchmark are at\nhttps://qiushisun.github.io/ScienceBoard-Home/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/kBkmkf8TUS9QTLLNJh9Cc.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/MuRvbGQbB4n2WBymZqGh1.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/6064a0eeb1703ddba0d458b9/hpztzin1L2YJpACBBIbdb.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19897.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6064a0eeb1703ddba0d458b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1617207525789-noauth.png",
            "fullname": "Qiushi",
            "name": "QiushiSun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19641",
            "authors": [
                {
                    "_id": "683686a4bec1d6dbb3d8728d",
                    "user": {
                        "_id": "6493fbb3085e14d7933b936d",
                        "avatarUrl": "/avatars/85723bedac9e81fecc33b36ff94ecada.svg",
                        "isPro": false,
                        "fullname": "Junteng Liu",
                        "user": "Junteng",
                        "type": "user"
                    },
                    "name": "Junteng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:12.066Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d8728e",
                    "user": {
                        "_id": "64813e4b3fb124fc98503a7e",
                        "avatarUrl": "/avatars/f871b7d84f04f827041d4a23cb1cdc9f.svg",
                        "isPro": false,
                        "fullname": "Yuanxiang Fan",
                        "user": "ShiroFFF",
                        "type": "user"
                    },
                    "name": "Yuanxiang Fan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:18.907Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d8728f",
                    "name": "Zhuo Jiang",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87290",
                    "name": "Han Ding",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87291",
                    "name": "Yongyi Hu",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87292",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87293",
                    "name": "Yiqi Shi",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87294",
                    "name": "Shitong Weng",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87295",
                    "name": "Aili Chen",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87296",
                    "user": {
                        "_id": "62bf99cae140faaa85335ab8",
                        "avatarUrl": "/avatars/bcd2aa1823995b385096e2a68ce3e071.svg",
                        "isPro": false,
                        "fullname": "Shiqi Chen",
                        "user": "ShiqiChen",
                        "type": "user"
                    },
                    "name": "Shiqi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:58.772Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87297",
                    "user": {
                        "_id": "660e95ac3ee7a71ef0107347",
                        "avatarUrl": "/avatars/f96fb06158e8953a0d5898021ea1fe35.svg",
                        "isPro": false,
                        "fullname": "Yu-Nan Huang",
                        "user": "YN83",
                        "type": "user"
                    },
                    "name": "Yunan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:45.974Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87298",
                    "user": {
                        "_id": "62f3baf389ded29a9f93876e",
                        "avatarUrl": "/avatars/6c6cfbee17782bb9b37ccfc79e2f15b2.svg",
                        "isPro": false,
                        "fullname": "Mozhi Zhang",
                        "user": "zhangmozhi",
                        "type": "user"
                    },
                    "name": "Mozhi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:40.524Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d87299",
                    "name": "Pengyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d8729a",
                    "user": {
                        "_id": "63390ce41718795719635b1e",
                        "avatarUrl": "/avatars/ad03a2b349f01c1ac1fedfb95d02d43e.svg",
                        "isPro": false,
                        "fullname": "JunjieYan",
                        "user": "JunjieYan",
                        "type": "user"
                    },
                    "name": "Junjie Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:34.200Z",
                    "hidden": false
                },
                {
                    "_id": "683686a4bec1d6dbb3d8729b",
                    "user": {
                        "_id": "615f34ec3f6d24d67c1b5c78",
                        "avatarUrl": "/avatars/6dcff6477993d9e57c5cb92b6f95eb66.svg",
                        "isPro": false,
                        "fullname": "Junxian He",
                        "user": "jxhe",
                        "type": "user"
                    },
                    "name": "Junxian He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:27.539Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T07:59:36.000Z",
            "submittedOnDailyAt": "2025-05-28T06:37:58.638Z",
            "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond",
            "submittedOnDailyBy": {
                "_id": "676e38ad04af5bec20bc9faf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/676e38ad04af5bec20bc9faf/AG8Q9wAUzGtPWyjd5QO2l.jpeg",
                "isPro": false,
                "fullname": "MiniMax",
                "user": "MiniMax-AI",
                "type": "user"
            },
            "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",
            "upvotes": 43,
            "discussionId": "683686a5bec1d6dbb3d872c8",
            "projectPage": "https://huggingface.co/datasets/MiniMaxAI/SynLogic",
            "githubRepo": "https://github.com/MiniMax-AI/SynLogic",
            "ai_summary": "SynLogic, a data synthesis framework, enhances the logical reasoning capabilities of Large Language Models through RL, achieving state-of-the-art performance and improving generalization across various domains.",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "Large Language Models (LLMs)",
                "Logical Reasoning",
                "Data Synthesis",
                "BBEH",
                "Mixed Training",
                "DeepSeek-R1",
                "DeepSeek-R1-Distill-Qwen-32B",
                "DeepSeek-R1-Zero-Qwen-32B"
            ]
        },
        "publishedAt": "2025-05-26T03:59:36.000Z",
        "title": "SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning\n  Logical Reasoning and Beyond",
        "summary": "Recent advances such as OpenAI-o1 and DeepSeek R1 have demonstrated the\npotential of Reinforcement Learning (RL) to enhance reasoning abilities in\nLarge Language Models (LLMs). While open-source replication efforts have\nprimarily focused on mathematical and coding domains, methods and resources for\ndeveloping general reasoning capabilities remain underexplored. This gap is\npartly due to the challenge of collecting diverse and verifiable reasoning data\nsuitable for RL. We hypothesize that logical reasoning is critical for\ndeveloping general reasoning capabilities, as logic forms a fundamental\nbuilding block of reasoning. In this work, we present SynLogic, a data\nsynthesis framework and dataset that generates diverse logical reasoning data\nat scale, encompassing 35 diverse logical reasoning tasks. The SynLogic\napproach enables controlled synthesis of data with adjustable difficulty and\nquantity. Importantly, all examples can be verified by simple rules, making\nthem ideally suited for RL with verifiable rewards. In our experiments, we\nvalidate the effectiveness of RL training on the SynLogic dataset based on 7B\nand 32B models. SynLogic leads to state-of-the-art logical reasoning\nperformance among open-source datasets, surpassing DeepSeek-R1-Distill-Qwen-32B\nby 6 points on BBEH. Furthermore, mixing SynLogic data with mathematical and\ncoding tasks improves the training efficiency of these domains and\nsignificantly enhances reasoning generalization. Notably, our mixed training\nmodel outperforms DeepSeek-R1-Zero-Qwen-32B across multiple benchmarks. These\nfindings position SynLogic as a valuable resource for advancing the broader\nreasoning capabilities of LLMs. We open-source both the data synthesis pipeline\nand the SynLogic dataset at https://github.com/MiniMax-AI/SynLogic.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19641.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "676e38ad04af5bec20bc9faf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/676e38ad04af5bec20bc9faf/AG8Q9wAUzGtPWyjd5QO2l.jpeg",
            "fullname": "MiniMax",
            "name": "MiniMax-AI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 143
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21189",
            "authors": [
                {
                    "_id": "6836babd75a4c5486bac4149",
                    "user": {
                        "_id": "672e0638ee49faac3ad53af7",
                        "avatarUrl": "/avatars/3a273b4beba8286309296a1e25bc34a9.svg",
                        "isPro": false,
                        "fullname": "Gleb Mezentsev",
                        "user": "glebzok",
                        "type": "user"
                    },
                    "name": "Gleb Mezentsev",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-28T07:43:05.521Z",
                    "hidden": false
                },
                {
                    "_id": "6836babd75a4c5486bac414a",
                    "user": {
                        "_id": "6169a581d05945bfd8718dfa",
                        "avatarUrl": "/avatars/1892ab06a7ddb557232777de3cbec470.svg",
                        "isPro": false,
                        "fullname": "Ivan Oseledets",
                        "user": "oseledets",
                        "type": "user"
                    },
                    "name": "Ivan Oseledets",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:15:02.162Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/672e0638ee49faac3ad53af7/M2gWNMdYYUjkCuamqmHJ4.png",
                "https://cdn-uploads.huggingface.co/production/uploads/672e0638ee49faac3ad53af7/ZAl4A9HCN24-VRZwER0H1.png",
                "https://cdn-uploads.huggingface.co/production/uploads/672e0638ee49faac3ad53af7/zFXr1JmOc8jHoNtVJoiSM.png"
            ],
            "publishedAt": "2025-05-27T13:39:24.000Z",
            "submittedOnDailyAt": "2025-05-28T06:03:21.363Z",
            "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation",
            "submittedOnDailyBy": {
                "_id": "672e0638ee49faac3ad53af7",
                "avatarUrl": "/avatars/3a273b4beba8286309296a1e25bc34a9.svg",
                "isPro": false,
                "fullname": "Gleb Mezentsev",
                "user": "glebzok",
                "type": "user"
            },
            "summary": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.",
            "upvotes": 40,
            "discussionId": "6836babe75a4c5486bac4170",
            "githubRepo": "https://github.com/Glebzok/OneStepLLMGeneration",
            "ai_summary": "LLMs can generate long text segments in a single forward pass using learned embeddings, revealing a capability for multi-token generation without iterative decoding.",
            "ai_keywords": [
                "large language models",
                "autoregressive generation",
                "input embedding",
                "frozen LLMs",
                "multi-token generation",
                "iterative decoding",
                "learned embeddings",
                "embedding space",
                "dedicated encoder"
            ]
        },
        "publishedAt": "2025-05-27T09:39:24.000Z",
        "title": "Exploring the Latent Capacity of LLMs for One-Step Text Generation",
        "summary": "A recent study showed that large language models (LLMs) can reconstruct\nsurprisingly long texts - up to thousands of tokens - via autoregressive\ngeneration from just one specially trained input embedding. In this work, we\nexplore whether such reconstruction is possible without autoregression. We show\nthat frozen LLMs can generate hundreds of accurate tokens in just one forward\npass, when provided with only two learned embeddings. This reveals a surprising\nand underexplored capability of LLMs - multi-token generation without iterative\ndecoding. We investigate the behaviour of these embeddings and provide insight\ninto the type of information they encode. We also empirically show that\nalthough these representations are not unique for a given text, they form\nconnected and local regions in embedding space - a property that suggests the\npotential of learning a dedicated encoder into that space.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/672e0638ee49faac3ad53af7/M2gWNMdYYUjkCuamqmHJ4.png",
            "https://cdn-uploads.huggingface.co/production/uploads/672e0638ee49faac3ad53af7/ZAl4A9HCN24-VRZwER0H1.png",
            "https://cdn-uploads.huggingface.co/production/uploads/672e0638ee49faac3ad53af7/zFXr1JmOc8jHoNtVJoiSM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21189.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "672e0638ee49faac3ad53af7",
            "avatarUrl": "/avatars/3a273b4beba8286309296a1e25bc34a9.svg",
            "fullname": "Gleb Mezentsev",
            "name": "glebzok",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20325",
            "authors": [
                {
                    "_id": "6836abd6871c9d398c136ee0",
                    "user": {
                        "_id": "6830c4be7f72826192827659",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0hTKkO_zytpy9W8_0s291.png",
                        "isPro": false,
                        "fullname": "Amirhosein Ghasemabadi",
                        "user": "AmirhoseinGH",
                        "type": "user"
                    },
                    "name": "Amirhosein Ghasemabadi",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-28T07:04:37.500Z",
                    "hidden": false
                },
                {
                    "_id": "6836abd6871c9d398c136ee1",
                    "user": {
                        "_id": "661c391720b47b0daddfcc5a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RwvDJRtlEEPch27xId9Cb.png",
                        "isPro": false,
                        "fullname": "Keith G. Mills",
                        "user": "kgmills",
                        "type": "user"
                    },
                    "name": "Keith G. Mills",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-28T06:48:14.969Z",
                    "hidden": false
                },
                {
                    "_id": "6836abd6871c9d398c136ee2",
                    "name": "Baochun Li",
                    "hidden": false
                },
                {
                    "_id": "6836abd6871c9d398c136ee3",
                    "name": "Di Niu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/GtZcRKolbuU_GHGX2wpXa.png"
            ],
            "publishedAt": "2025-05-23T18:19:09.000Z",
            "submittedOnDailyAt": "2025-05-28T16:20:06.840Z",
            "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence",
            "submittedOnDailyBy": {
                "_id": "6830c4be7f72826192827659",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0hTKkO_zytpy9W8_0s291.png",
                "isPro": false,
                "fullname": "Amirhosein Ghasemabadi",
                "user": "AmirhoseinGH",
                "type": "user"
            },
            "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.",
            "upvotes": 39,
            "discussionId": "6836abd7871c9d398c136f12",
            "githubRepo": "https://github.com/Amirhosein-gh98/Guided-by-Gut",
            "ai_summary": "The Guided by Gut (GG) framework enhances LLM reasoning efficiently using intrinsic signals and token-level confidence, outperforming PRM-based methods with faster inference and lower memory usage.",
            "ai_keywords": [
                "Test-Time Scaling (TTS)",
                "Large Language Model (LLM)",
                "Process Reward Models (PRMs)",
                "Best-of-N (BoN)",
                "tree search",
                "intrinsic LLM signals",
                "token-level confidence",
                "step novelty",
                "targeted reinforcement learning fine-tuning"
            ]
        },
        "publishedAt": "2025-05-23T14:19:09.000Z",
        "title": "Guided by Gut: Efficient Test-Time Scaling with Reinforced Intrinsic\n  Confidence",
        "summary": "Test-Time Scaling (TTS) methods for enhancing Large Language Model (LLM)\nreasoning often incur substantial computational costs, primarily due to\nextensive reliance on external Process Reward Models (PRMs) or sampling methods\nlike Best-of-N (BoN). This paper introduces Guided by Gut (GG), an efficient\nself-guided TTS framework that achieves PRM-level performance without costly\nexternal verifier models. Our method employs a lightweight tree search guided\nsolely by intrinsic LLM signals, token-level confidence and step novelty. One\ncritical innovation is improving the reliability of internal confidence\nestimates via a targeted reinforcement learning fine-tuning phase. Empirical\nevaluations on challenging mathematical reasoning benchmarks demonstrate that\nGG enables smaller models (e.g., 1.5B parameters) to achieve accuracy matching\nor surpassing significantly larger models (e.g., 32B-70B parameters), while\nreducing GPU memory usage by up to 10x. Compared to PRM-based methods, GG\nachieves comparable accuracy with 8x faster inference speeds and 4-5x lower\nmemory usage. Additionally, GG reduces KV cache memory usage by approximately\n50% compared to the BoN strategy, facilitating more efficient and practical\ndeployment of TTS techniques.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6830c4be7f72826192827659/GtZcRKolbuU_GHGX2wpXa.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20325.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6830c4be7f72826192827659",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0hTKkO_zytpy9W8_0s291.png",
            "fullname": "Amirhosein Ghasemabadi",
            "name": "AmirhoseinGH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21496",
            "authors": [
                {
                    "_id": "683698f3c32e462c40a9188f",
                    "user": {
                        "_id": "666aa99cd1652853e4f9a8b9",
                        "avatarUrl": "/avatars/7cd5a0c34b5ccb8eff5a353d88d15a93.svg",
                        "isPro": false,
                        "fullname": "HanXiao",
                        "user": "HanXiao1999",
                        "type": "user"
                    },
                    "name": "Han Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:56:45.845Z",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91890",
                    "user": {
                        "_id": "65446e3fdc39aa9fabba8fd5",
                        "avatarUrl": "/avatars/41321bd37d007387d1d363b6b4fd2079.svg",
                        "isPro": false,
                        "fullname": "guozhi wang",
                        "user": "juice-wang",
                        "type": "user"
                    },
                    "name": "Guozhi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:17:24.039Z",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91891",
                    "user": {
                        "_id": "6458ce236fa580137af5aa95",
                        "avatarUrl": "/avatars/db65a7332e375eb5daad5c1b076b1e3b.svg",
                        "isPro": false,
                        "fullname": "Yuxiang Chai",
                        "user": "Yuxiang007",
                        "type": "user"
                    },
                    "name": "Yuxiang Chai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:17:29.742Z",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91892",
                    "user": {
                        "_id": "64b0bfef2f2f9c345b87e673",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7O3zCr_IyGvtz0BUdKNKi.jpeg",
                        "isPro": false,
                        "fullname": "Zimu Lu",
                        "user": "luzimu",
                        "type": "user"
                    },
                    "name": "Zimu Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:17:35.790Z",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91893",
                    "user": {
                        "_id": "66026c9068d519ed32519e9c",
                        "avatarUrl": "/avatars/8fa051312c713772e5b8ba65989ff7f5.svg",
                        "isPro": false,
                        "fullname": "Weifeng Lin",
                        "user": "Afeng-x",
                        "type": "user"
                    },
                    "name": "Weifeng Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:17:49.451Z",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91894",
                    "name": "Hao He",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91895",
                    "name": "Lue Fan",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91896",
                    "user": {
                        "_id": "66d459ca1dbd780574b42e2d",
                        "avatarUrl": "/avatars/d573a92487628bf6c727b77eedf80b9d.svg",
                        "isPro": false,
                        "fullname": "bian liuyang",
                        "user": "liuyangbian",
                        "type": "user"
                    },
                    "name": "Liuyang Bian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:17:55.352Z",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91897",
                    "name": "Rui Hu",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91898",
                    "name": "Liang Liu",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a91899",
                    "name": "Shuai Ren",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a9189a",
                    "user": {
                        "_id": "67134b1ce58b557752c54a1c",
                        "avatarUrl": "/avatars/5cc07748dac424a6f5c7ca09f1c45068.svg",
                        "isPro": false,
                        "fullname": "yafei wen",
                        "user": "wolf1110",
                        "type": "user"
                    },
                    "name": "Yafei Wen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:18:16.701Z",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a9189b",
                    "name": "Xiaoxin Chen",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a9189c",
                    "user": {
                        "_id": "637de1520d5bb06fbe5207a9",
                        "avatarUrl": "/avatars/1090851217270c5a858b13e013356d4f.svg",
                        "isPro": false,
                        "fullname": "AJ.Zhou",
                        "user": "AJZhou",
                        "type": "user"
                    },
                    "name": "Aojun Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:56:48.292Z",
                    "hidden": false
                },
                {
                    "_id": "683698f3c32e462c40a9189d",
                    "user": {
                        "_id": "65c04e9c27a5fdca81abcbd9",
                        "avatarUrl": "/avatars/12a155683c824fa23da4a9e2bed4f64e.svg",
                        "isPro": false,
                        "fullname": "Hongsheng LI",
                        "user": "hsli-cuhk",
                        "type": "user"
                    },
                    "name": "Hongsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:18:35.337Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T17:58:06.000Z",
            "submittedOnDailyAt": "2025-05-28T03:33:04.170Z",
            "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\n  Mobile GUI Agents",
            "submittedOnDailyBy": {
                "_id": "637de1520d5bb06fbe5207a9",
                "avatarUrl": "/avatars/1090851217270c5a858b13e013356d4f.svg",
                "isPro": false,
                "fullname": "AJ.Zhou",
                "user": "AJZhou",
                "type": "user"
            },
            "summary": "In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie.",
            "upvotes": 34,
            "discussionId": "683698f5c32e462c40a9192d",
            "githubRepo": "https://github.com/Euphoria16/UI-Genie",
            "ai_summary": "UI-Genie framework addresses GUI agent challenges through a reward model with image-text architecture and a self-improvement pipeline, achieving state-of-the-art performance on multiple benchmarks.",
            "ai_keywords": [
                "image-text interleaved architecture",
                "GUI agents",
                "reward model",
                "self-improving pipeline",
                "rule-based verification",
                "controlled trajectory corruption",
                "hard negative mining",
                "reward-guided exploration",
                "outcome verification",
                "dynamic environments",
                "synthetic trajectory generation"
            ]
        },
        "publishedAt": "2025-05-27T13:58:06.000Z",
        "title": "UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based\n  Mobile GUI Agents",
        "summary": "In this paper, we introduce UI-Genie, a self-improving framework addressing\ntwo key challenges in GUI agents: verification of trajectory outcome is\nchallenging and high-quality training data are not scalable. These challenges\nare addressed by a reward model and a self-improving pipeline, respectively.\nThe reward model, UI-Genie-RM, features an image-text interleaved architecture\nthat efficiently pro- cesses historical context and unifies action-level and\ntask-level rewards. To sup- port the training of UI-Genie-RM, we develop\ndeliberately-designed data genera- tion strategies including rule-based\nverification, controlled trajectory corruption, and hard negative mining. To\naddress the second challenge, a self-improvement pipeline progressively expands\nsolvable complex GUI tasks by enhancing both the agent and reward models\nthrough reward-guided exploration and outcome verification in dynamic\nenvironments. For training the model, we generate UI- Genie-RM-517k and\nUI-Genie-Agent-16k, establishing the first reward-specific dataset for GUI\nagents while demonstrating high-quality synthetic trajectory gen- eration\nwithout manual annotation. Experimental results show that UI-Genie achieves\nstate-of-the-art performance across multiple GUI agent benchmarks with three\ngenerations of data-model self-improvement. We open-source our complete\nframework implementation and generated datasets to facilitate further research\nin https://github.com/Euphoria16/UI-Genie.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21496.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637de1520d5bb06fbe5207a9",
            "avatarUrl": "/avatars/1090851217270c5a858b13e013356d4f.svg",
            "fullname": "AJ.Zhou",
            "name": "AJZhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17952",
            "authors": [
                {
                    "_id": "6836e9b4481e0bfedacc61bc",
                    "user": {
                        "_id": "631b9ff5824f2502e3557c7e",
                        "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
                        "isPro": true,
                        "fullname": "liu",
                        "user": "che111",
                        "type": "user"
                    },
                    "name": "Che Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:37:55.201Z",
                    "hidden": false
                },
                {
                    "_id": "6836e9b4481e0bfedacc61bd",
                    "name": "Haozhe Wang",
                    "hidden": false
                },
                {
                    "_id": "6836e9b4481e0bfedacc61be",
                    "user": {
                        "_id": "66588c8a338165aad1516756",
                        "avatarUrl": "/avatars/c6539b4ef65f465f6f762628d6921be6.svg",
                        "isPro": false,
                        "fullname": "JZPeterPan",
                        "user": "JZPeterPan",
                        "type": "user"
                    },
                    "name": "Jiazhen Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:14:16.399Z",
                    "hidden": false
                },
                {
                    "_id": "6836e9b4481e0bfedacc61bf",
                    "name": "Zhongwei Wan",
                    "hidden": false
                },
                {
                    "_id": "6836e9b4481e0bfedacc61c0",
                    "name": "Yong Dai",
                    "hidden": false
                },
                {
                    "_id": "6836e9b4481e0bfedacc61c1",
                    "name": "Fangzhen Lin",
                    "hidden": false
                },
                {
                    "_id": "6836e9b4481e0bfedacc61c2",
                    "user": {
                        "_id": "67c973029d03a429bb2bbf99",
                        "avatarUrl": "/avatars/d05246e3d1e54e13acd1959155931dac.svg",
                        "isPro": false,
                        "fullname": "Bai",
                        "user": "baiwenjia",
                        "type": "user"
                    },
                    "name": "Wenjia Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:48:49.737Z",
                    "hidden": false
                },
                {
                    "_id": "6836e9b4481e0bfedacc61c3",
                    "name": "Daniel Rueckert",
                    "hidden": false
                },
                {
                    "_id": "6836e9b4481e0bfedacc61c4",
                    "name": "Rossella Arcucci",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T14:27:37.000Z",
            "submittedOnDailyAt": "2025-05-28T09:17:41.869Z",
            "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with\n  Minimalist Rule-Based RL",
            "submittedOnDailyBy": {
                "_id": "631b9ff5824f2502e3557c7e",
                "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
                "isPro": true,
                "fullname": "liu",
                "user": "che111",
                "type": "user"
            },
            "summary": "Improving performance on complex tasks and enabling interpretable decision\nmaking in large language models (LLMs), especially for clinical applications,\nrequires effective reasoning. Yet this remains challenging without supervised\nfine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from\nclosed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the\nfirst medical LLM to show that reasoning capability can emerge purely through\nreinforcement learning (RL), using minimalist rule-based rewards on public\nmultiple-choice QA datasets, without relying on SFT or distilled CoT data.\nAlphaMed achieves state-of-the-art results on six medical QA benchmarks,\noutperforming models trained with conventional SFT+RL pipelines. On challenging\nbenchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source\nmodels such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the\nfactors behind this success, we conduct a comprehensive data-centric analysis\nguided by three questions: (i) Can minimalist rule-based RL incentivize\nreasoning without distilled CoT supervision? (ii) How do dataset quantity and\ndiversity impact reasoning? (iii) How does question difficulty shape the\nemergence and generalization of reasoning? Our findings show that dataset\ninformativeness is a key driver of reasoning performance, and that minimalist\nRL on informative, multiple-choice QA data is effective at inducing reasoning\nwithout CoT supervision. We also observe divergent trends across benchmarks,\nunderscoring limitations in current evaluation and the need for more\nchallenging, reasoning-oriented medical QA benchmarks.",
            "upvotes": 18,
            "discussionId": "6836e9b5481e0bfedacc61ff",
            "projectPage": "https://cheliu-computation.github.io/AlphaM/",
            "ai_summary": "AlphaMed, a medical LLM, demonstrates superior reasoning capabilities through reinforcement learning using minimalist rule-based rewards, surpassing conventionally trained models on medical QA benchmarks.",
            "ai_keywords": [
                "reinforcement learning",
                "rule-based rewards",
                "medical LLM",
                "chain-of-thought",
                "reasoning capability",
                "multiple-choice QA datasets",
                "dataset informativeness",
                "medical QA benchmarks"
            ]
        },
        "publishedAt": "2025-05-23T10:27:37.000Z",
        "title": "Beyond Distillation: Pushing the Limits of Medical LLM Reasoning with\n  Minimalist Rule-Based RL",
        "summary": "Improving performance on complex tasks and enabling interpretable decision\nmaking in large language models (LLMs), especially for clinical applications,\nrequires effective reasoning. Yet this remains challenging without supervised\nfine-tuning (SFT) on costly chain-of-thought (CoT) data distilled from\nclosed-source models (e.g., GPT-4o). In this work, we present AlphaMed, the\nfirst medical LLM to show that reasoning capability can emerge purely through\nreinforcement learning (RL), using minimalist rule-based rewards on public\nmultiple-choice QA datasets, without relying on SFT or distilled CoT data.\nAlphaMed achieves state-of-the-art results on six medical QA benchmarks,\noutperforming models trained with conventional SFT+RL pipelines. On challenging\nbenchmarks (e.g., MedXpert), AlphaMed even surpasses larger or closed-source\nmodels such as DeepSeek-V3-671B and Claude-3.5-Sonnet. To understand the\nfactors behind this success, we conduct a comprehensive data-centric analysis\nguided by three questions: (i) Can minimalist rule-based RL incentivize\nreasoning without distilled CoT supervision? (ii) How do dataset quantity and\ndiversity impact reasoning? (iii) How does question difficulty shape the\nemergence and generalization of reasoning? Our findings show that dataset\ninformativeness is a key driver of reasoning performance, and that minimalist\nRL on informative, multiple-choice QA data is effective at inducing reasoning\nwithout CoT supervision. We also observe divergent trends across benchmarks,\nunderscoring limitations in current evaluation and the need for more\nchallenging, reasoning-oriented medical QA benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17952.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "fullname": "liu",
            "name": "che111",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21334",
            "authors": [
                {
                    "_id": "6836a5a9bec1d6dbb3e10454",
                    "user": {
                        "_id": "6696755fd26a65bd255184d3",
                        "avatarUrl": "/avatars/8d46c21a7b23f0100a7e3385fea61edf.svg",
                        "isPro": false,
                        "fullname": "Kele Shao",
                        "user": "keleshao",
                        "type": "user"
                    },
                    "name": "Kele Shao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:56:19.978Z",
                    "hidden": false
                },
                {
                    "_id": "6836a5a9bec1d6dbb3e10455",
                    "name": "Keda Tao",
                    "hidden": false
                },
                {
                    "_id": "6836a5a9bec1d6dbb3e10456",
                    "name": "Can Qin",
                    "hidden": false
                },
                {
                    "_id": "6836a5a9bec1d6dbb3e10457",
                    "name": "Haoxuan You",
                    "hidden": false
                },
                {
                    "_id": "6836a5a9bec1d6dbb3e10458",
                    "name": "Yang Sui",
                    "hidden": false
                },
                {
                    "_id": "6836a5a9bec1d6dbb3e10459",
                    "user": {
                        "_id": "62b624f3b52bef716e248fd7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b624f3b52bef716e248fd7/AllcccKH-eBWduA8KVnOQ.png",
                        "isPro": false,
                        "fullname": "Huan Wang",
                        "user": "Huan-WhoRegisteredMyName",
                        "type": "user"
                    },
                    "name": "Huan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T10:11:24.697Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T15:28:45.000Z",
            "submittedOnDailyAt": "2025-05-28T04:30:49.475Z",
            "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
            "submittedOnDailyBy": {
                "_id": "67a4a26d5e65aa63c6d30e68",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
                "isPro": false,
                "fullname": "Sicheng Feng",
                "user": "FSCCS",
                "type": "user"
            },
            "summary": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference.",
            "upvotes": 17,
            "discussionId": "6836a5a9bec1d6dbb3e1048a",
            "ai_summary": "HoliTom combines outer-LLM pruning through global temporal segmentation with inner-LLM token similarity-based merging to significantly reduce computational inefficiency in video LLMs without sacrificing performance.",
            "ai_keywords": [
                "video LLMs",
                "video tokens",
                "FastV",
                "inner-LLM pruning",
                "outer-LLM pruning",
                "global redundancy-aware temporal segmentation",
                "spatial-temporal merging",
                "HoliTom",
                "token similarity-based merging",
                "LLaVA-OneVision-7B",
                "Time-To-First-Token (TTFT)",
                "decoding throughput"
            ]
        },
        "publishedAt": "2025-05-27T11:28:45.000Z",
        "title": "HoliTom: Holistic Token Merging for Fast Video Large Language Models",
        "summary": "Video large language models (video LLMs) excel at video comprehension but\nface significant computational inefficiency due to redundant video tokens.\nExisting token pruning methods offer solutions. However, approaches operating\nwithin the LLM (inner-LLM pruning), such as FastV, incur intrinsic\ncomputational overhead in shallow layers. In contrast, methods performing token\npruning before the LLM (outer-LLM pruning) primarily address spatial redundancy\nwithin individual frames or limited temporal windows, neglecting the crucial\nglobal temporal dynamics and correlations across longer video sequences. This\nleads to sub-optimal spatio-temporal reduction and does not leverage video\ncompressibility fully. Crucially, the synergistic potential and mutual\ninfluence of combining these strategies remain unexplored. To further reduce\nredundancy, we introduce HoliTom, a novel training-free holistic token merging\nframework. HoliTom employs outer-LLM pruning through global redundancy-aware\ntemporal segmentation, followed by spatial-temporal merging to reduce visual\ntokens by over 90%, significantly alleviating the LLM's computational burden.\nComplementing this, we introduce a robust inner-LLM token similarity-based\nmerging approach, designed for superior performance and compatibility with\nouter-LLM pruning. Evaluations demonstrate our method's promising\nefficiency-performance trade-off on LLaVA-OneVision-7B, reducing computational\ncosts to 6.9% of FLOPs while maintaining 99.1% of the original performance.\nFurthermore, we achieve a 2.28x reduction in Time-To-First-Token (TTFT) and a\n1.32x acceleration in decoding throughput, highlighting the practical benefits\nof our integrated pruning approach for efficient video LLMs inference.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21334.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a4a26d5e65aa63c6d30e68",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a4a26d5e65aa63c6d30e68/GtodlJGw-_IL2DTXQTucz.jpeg",
            "fullname": "Sicheng Feng",
            "name": "FSCCS",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.14064",
            "authors": [
                {
                    "_id": "682d92a2e1fd895b29ed2b46",
                    "user": {
                        "_id": "6717cef5dca8fbc18c5c4216",
                        "avatarUrl": "/avatars/8c6e420bb4ece42b07e107ba3199fed2.svg",
                        "isPro": false,
                        "fullname": "Cosmin",
                        "user": "ci-ber",
                        "type": "user"
                    },
                    "name": "Cosmin I. Bercea",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-21T08:45:26.580Z",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b47",
                    "user": {
                        "_id": "6526523fa754153bd4da45f3",
                        "avatarUrl": "/avatars/848b234fcd59d9885ddb673ffea20a63.svg",
                        "isPro": false,
                        "fullname": "RioJune",
                        "user": "RioJune",
                        "type": "user"
                    },
                    "name": "Jun Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T12:11:19.995Z",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b48",
                    "name": "Philipp Raffler",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b49",
                    "name": "Evamaria O. Riedel",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b4a",
                    "name": "Lena Schmitzer",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b4b",
                    "name": "Angela Kurz",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b4c",
                    "name": "Felix Bitzer",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b4d",
                    "name": "Paula Romller",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b4e",
                    "name": "Julian Canisius",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b4f",
                    "name": "Mirjam L. Beyrle",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b50",
                    "name": "Che Liu",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b51",
                    "name": "Wenjia Bai",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b52",
                    "name": "Bernhard Kainz",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b53",
                    "name": "Julia A. Schnabel",
                    "hidden": false
                },
                {
                    "_id": "682d92a2e1fd895b29ed2b54",
                    "name": "Benedikt Wiestler",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T08:10:57.000Z",
            "submittedOnDailyAt": "2025-05-28T09:16:16.519Z",
            "title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in\n  Brain MRI",
            "submittedOnDailyBy": {
                "_id": "631b9ff5824f2502e3557c7e",
                "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
                "isPro": true,
                "fullname": "liu",
                "user": "che111",
                "type": "user"
            },
            "summary": "In many real-world applications, deployed models encounter inputs that differ\nfrom the data seen during training. Out-of-distribution detection identifies\nwhether an input stems from an unseen distribution, while open-world\nrecognition flags such inputs to ensure the system remains robust as\never-emerging, previously unknown categories appear and must be addressed\nwithout retraining. Foundation and vision-language models are pre-trained on\nlarge and diverse datasets with the expectation of broad generalization across\ndomains, including medical imaging. However, benchmarking these models on test\nsets with only a few common outlier types silently collapses the evaluation\nback to a closed-set problem, masking failures on rare or truly novel\nconditions encountered in clinical use.\n  We therefore present NOVA, a challenging, real-life evaluation-only\nbenchmark of sim900 brain MRI scans that span 281 rare pathologies and\nheterogeneous acquisition protocols. Each case includes rich clinical\nnarratives and double-blinded expert bounding-box annotations. Together, these\nenable joint assessment of anomaly localisation, visual captioning, and\ndiagnostic reasoning. Because NOVA is never used for training, it serves as an\nextreme stress-test of out-of-distribution generalisation: models must bridge\na distribution gap both in sample appearance and in semantic space. Baseline\nresults with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and\nQwen2.5-VL-72B) reveal substantial performance drops across all tasks,\nestablishing NOVA as a rigorous testbed for advancing models that can detect,\nlocalize, and reason about truly unknown anomalies.",
            "upvotes": 16,
            "discussionId": "682d92a6e1fd895b29ed2cc2",
            "projectPage": "https://huggingface.co/datasets/Ano-2090/Nova",
            "ai_summary": "NOVA is a benchmark for evaluating vision-language models on rare, clinically relevant MRI pathologies, challenging their out-of-distribution and open-world recognition capabilities.",
            "ai_keywords": [
                "out-of-distribution detection",
                "open-world recognition",
                "foundation models",
                "vision-language models",
                "brain MRI scans",
                "rare pathologies",
                "heterogeneous acquisition protocols",
                "clinical narratives",
                "bounding-box annotations",
                "joint assessment",
                "anomaly localization",
                "visual captioning",
                "diagnostic reasoning",
                "extreme stress-test",
                "out-of-distribution generalisation",
                "distribution gap",
                "sample appearance",
                "semantic space"
            ]
        },
        "publishedAt": "2025-05-20T04:10:57.000Z",
        "title": "NOVA: A Benchmark for Anomaly Localization and Clinical Reasoning in\n  Brain MRI",
        "summary": "In many real-world applications, deployed models encounter inputs that differ\nfrom the data seen during training. Out-of-distribution detection identifies\nwhether an input stems from an unseen distribution, while open-world\nrecognition flags such inputs to ensure the system remains robust as\never-emerging, previously unknown categories appear and must be addressed\nwithout retraining. Foundation and vision-language models are pre-trained on\nlarge and diverse datasets with the expectation of broad generalization across\ndomains, including medical imaging. However, benchmarking these models on test\nsets with only a few common outlier types silently collapses the evaluation\nback to a closed-set problem, masking failures on rare or truly novel\nconditions encountered in clinical use.\n  We therefore present NOVA, a challenging, real-life evaluation-only\nbenchmark of sim900 brain MRI scans that span 281 rare pathologies and\nheterogeneous acquisition protocols. Each case includes rich clinical\nnarratives and double-blinded expert bounding-box annotations. Together, these\nenable joint assessment of anomaly localisation, visual captioning, and\ndiagnostic reasoning. Because NOVA is never used for training, it serves as an\nextreme stress-test of out-of-distribution generalisation: models must bridge\na distribution gap both in sample appearance and in semantic space. Baseline\nresults with leading vision-language models (GPT-4o, Gemini 2.0 Flash, and\nQwen2.5-VL-72B) reveal substantial performance drops across all tasks,\nestablishing NOVA as a rigorous testbed for advancing models that can detect,\nlocalize, and reason about truly unknown anomalies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14064.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "fullname": "liu",
            "name": "che111",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21505",
            "authors": [
                {
                    "_id": "68369d9f8a36b9fa7f340c86",
                    "user": {
                        "_id": "65080dc63fc966d1bbba485d",
                        "avatarUrl": "/avatars/347890233f2316e7f7a04d652b2378bb.svg",
                        "isPro": false,
                        "fullname": "Shimao Zhang",
                        "user": "Shimao-Zhang",
                        "type": "user"
                    },
                    "name": "Shimao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:56:40.548Z",
                    "hidden": false
                },
                {
                    "_id": "68369d9f8a36b9fa7f340c87",
                    "user": {
                        "_id": "643525ea0b30bd434ea15363",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643525ea0b30bd434ea15363/7sAzllfWUPtt68NY1gDLj.png",
                        "isPro": false,
                        "fullname": "Jackie Lai",
                        "user": "DreamW1ngs",
                        "type": "user"
                    },
                    "name": "Zhejian Lai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:56:38.385Z",
                    "hidden": false
                },
                {
                    "_id": "68369d9f8a36b9fa7f340c88",
                    "user": {
                        "_id": "65cc78118ebd392213fc9e16",
                        "avatarUrl": "/avatars/859735d5abdb6f2c6f26bd8db034a1ce.svg",
                        "isPro": false,
                        "fullname": "Xiang Liu",
                        "user": "VincentLx",
                        "type": "user"
                    },
                    "name": "Xiang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T12:12:17.068Z",
                    "hidden": false
                },
                {
                    "_id": "68369d9f8a36b9fa7f340c89",
                    "name": "Shuaijie She",
                    "hidden": false
                },
                {
                    "_id": "68369d9f8a36b9fa7f340c8a",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "68369d9f8a36b9fa7f340c8b",
                    "name": "Yeyun Gong",
                    "hidden": false
                },
                {
                    "_id": "68369d9f8a36b9fa7f340c8c",
                    "name": "Shujian Huang",
                    "hidden": false
                },
                {
                    "_id": "68369d9f8a36b9fa7f340c8d",
                    "name": "Jiajun Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T17:59:52.000Z",
            "submittedOnDailyAt": "2025-05-28T04:14:23.406Z",
            "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective",
            "submittedOnDailyBy": {
                "_id": "65080dc63fc966d1bbba485d",
                "avatarUrl": "/avatars/347890233f2316e7f7a04d652b2378bb.svg",
                "isPro": false,
                "fullname": "Shimao Zhang",
                "user": "Shimao-Zhang",
                "type": "user"
            },
            "summary": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.",
            "upvotes": 15,
            "discussionId": "68369da08a36b9fa7f340cba",
            "ai_summary": "The research proposes a finer-grained neuron identification algorithm for detecting language-specific and language-agnostic neurons in LLMs, and investigates the impact on multilingual alignment and capabilities through analysis of multilingual understanding, shared semantic reasoning, multilingual output transformation, and vocabulary space outputting.",
            "ai_keywords": [
                "multilingual alignment",
                "language-specific neurons",
                "language-agnostic neurons",
                "shared semantic space",
                "multilingual output space",
                "vocabulary space",
                "neuron identification algorithm",
                "spontaneous multilingual alignment"
            ]
        },
        "publishedAt": "2025-05-27T13:59:52.000Z",
        "title": "How does Alignment Enhance LLMs' Multilingual Capabilities? A Language\n  Neurons Perspective",
        "summary": "Multilingual Alignment is an effective and representative paradigm to enhance\nLLMs' multilingual capabilities, which transfers the capabilities from the\nhigh-resource languages to the low-resource languages. Meanwhile, some\nresearches on language-specific neurons reveal that there are language-specific\nneurons that are selectively activated in LLMs when processing different\nlanguages. This provides a new perspective to analyze and understand LLMs'\nmechanisms more specifically in multilingual scenarios. In this work, we\npropose a new finer-grained neuron identification algorithm, which detects\nlanguage neurons~(including language-specific neurons and language-related\nneurons) and language-agnostic neurons. Furthermore, based on the\ndistributional characteristics of different types of neurons, we divide the\nLLMs' internal process for multilingual inference into four parts: (1)\nmultilingual understanding, (2) shared semantic space reasoning, (3)\nmultilingual output space transformation, and (4) vocabulary space outputting.\nAdditionally, we systematically analyze the models before and after alignment\nwith a focus on different types of neurons. We also analyze the phenomenon of\n''Spontaneous Multilingual Alignment''. Overall, our work conducts a\ncomprehensive investigation based on different types of neurons, providing\nempirical results and valuable insights for better understanding multilingual\nalignment and multilingual capabilities of LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21505.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65080dc63fc966d1bbba485d",
            "avatarUrl": "/avatars/347890233f2316e7f7a04d652b2378bb.svg",
            "fullname": "Shimao Zhang",
            "name": "Shimao-Zhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21493",
            "authors": [
                {
                    "_id": "68372228726e45ea3f19fd59",
                    "name": "Xiangxin Zhou",
                    "hidden": false
                },
                {
                    "_id": "68372228726e45ea3f19fd5a",
                    "user": {
                        "_id": "65f5392c68b8e0cb3c9977a2",
                        "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
                        "isPro": false,
                        "fullname": "Zichen",
                        "user": "lkevinzc",
                        "type": "user"
                    },
                    "name": "Zichen Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:13:48.854Z",
                    "hidden": false
                },
                {
                    "_id": "68372228726e45ea3f19fd5b",
                    "name": "Anya Sims",
                    "hidden": false
                },
                {
                    "_id": "68372228726e45ea3f19fd5c",
                    "name": "Haonan Wang",
                    "hidden": false
                },
                {
                    "_id": "68372228726e45ea3f19fd5d",
                    "name": "Tianyu Pang",
                    "hidden": false
                },
                {
                    "_id": "68372228726e45ea3f19fd5e",
                    "name": "Chongxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68372228726e45ea3f19fd5f",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "68372228726e45ea3f19fd60",
                    "name": "Min Lin",
                    "hidden": false
                },
                {
                    "_id": "68372228726e45ea3f19fd61",
                    "name": "Chao Du",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T17:56:27.000Z",
            "submittedOnDailyAt": "2025-05-28T13:23:39.167Z",
            "title": "Reinforcing General Reasoning without Verifiers",
            "submittedOnDailyBy": {
                "_id": "65f5392c68b8e0cb3c9977a2",
                "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
                "isPro": false,
                "fullname": "Zichen",
                "user": "lkevinzc",
                "type": "user"
            },
            "summary": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree.",
            "upvotes": 14,
            "discussionId": "68372229726e45ea3f19fd90",
            "githubRepo": "https://github.com/sail-sg/VeriFree",
            "ai_summary": "A verifier-free method (VeriFree) is introduced to extend reinforcement learning training of large language models to general reasoning domains, improving efficiency and performance compared to verifier-based methods.",
            "ai_keywords": [
                "DeepSeek-R1-Zero",
                "reinforcement learning",
                "RL",
                "large language models",
                "LLMs",
                "VeriFree",
                "MMLU-Pro",
                "GPQA",
                "SuperGPQA"
            ]
        },
        "publishedAt": "2025-05-27T13:56:27.000Z",
        "title": "Reinforcing General Reasoning without Verifiers",
        "summary": "The recent paradigm shift towards training large language models (LLMs) using\nDeepSeek-R1-Zero-style reinforcement learning (RL) on verifiable rewards has\nled to impressive advancements in code and mathematical reasoning. However,\nthis methodology is limited to tasks where rule-based answer verification is\npossible and does not naturally extend to real-world domains such as chemistry,\nhealthcare, engineering, law, biology, business, and economics. Current\npractical workarounds use an additional LLM as a model-based verifier; however,\nthis introduces issues such as reliance on a strong verifier LLM,\nsusceptibility to reward hacking, and the practical burden of maintaining the\nverifier model in memory during training. To address this and extend\nDeepSeek-R1-Zero-style training to general reasoning domains, we propose a\nverifier-free method (VeriFree) that bypasses answer verification and instead\nuses RL to directly maximize the probability of generating the reference\nanswer. We compare VeriFree with verifier-based methods and demonstrate that,\nin addition to its significant practical benefits and reduced compute\nrequirements, VeriFree matches and even surpasses verifier-based methods on\nextensive evaluations across MMLU-Pro, GPQA, SuperGPQA, and math-related\nbenchmarks. Moreover, we provide insights into this method from multiple\nperspectives: as an elegant integration of training both the policy and\nimplicit verifier in a unified model, and as a variational optimization\napproach. Code is available at https://github.com/sail-sg/VeriFree.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21493.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f5392c68b8e0cb3c9977a2",
            "avatarUrl": "/avatars/aa64772475098e8a135c13072fde6744.svg",
            "fullname": "Zichen",
            "name": "lkevinzc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16901",
            "authors": [
                {
                    "_id": "68369babaffae1c74f432a1e",
                    "name": "Hongyuan Tao",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a1f",
                    "user": {
                        "_id": "6620d5bdcc2bc496fd6c79e6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6620d5bdcc2bc496fd6c79e6/-xaFiByQrXdYgpscpUgR2.png",
                        "isPro": false,
                        "fullname": "Ying ZHANG",
                        "user": "JJYDXFS",
                        "type": "user"
                    },
                    "name": "Ying Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:14:20.825Z",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a20",
                    "user": {
                        "_id": "67b875489107c46e94ffcc16",
                        "avatarUrl": "/avatars/3435476b12ec63775e3de4b6bec3bba7.svg",
                        "isPro": false,
                        "fullname": "Zhenhao Tang",
                        "user": "Darian616",
                        "type": "user"
                    },
                    "name": "Zhenhao Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:39:47.865Z",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a21",
                    "name": "Hongen Peng",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a22",
                    "name": "Xukun Zhu",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a23",
                    "name": "Bingchang Liu",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a24",
                    "user": {
                        "_id": "67ae0b740b0b4c1400f5ce13",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/XO8Sw-nuSy-5s7OR5Z0kl.png",
                        "isPro": false,
                        "fullname": "yingguang",
                        "user": "yangyingguang",
                        "type": "user"
                    },
                    "name": "Yingguang Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:40:15.278Z",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a25",
                    "user": {
                        "_id": "6430bdd8cd31d174a9f900fb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
                        "isPro": false,
                        "fullname": "Ziyin Zhang",
                        "user": "Geralt-Targaryen",
                        "type": "user"
                    },
                    "name": "Ziyin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:56:43.238Z",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a26",
                    "name": "Zhaogui Xu",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a27",
                    "name": "Haipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a28",
                    "user": {
                        "_id": "63521e1dfe367c0d9b155007",
                        "avatarUrl": "/avatars/b22804fc63b507fd60191486b17cdf7c.svg",
                        "isPro": false,
                        "fullname": "Linchao Zhu",
                        "user": "ffmpbgrnn",
                        "type": "user"
                    },
                    "name": "Linchao Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:40:42.330Z",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a29",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a2a",
                    "user": {
                        "_id": "6837243353739ee0d588d04c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/CYU8xxEHxxGkwNVCpua6X.png",
                        "isPro": false,
                        "fullname": "Hang Yu",
                        "user": "fhlyhv",
                        "type": "user"
                    },
                    "name": "Hang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:14:18.595Z",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a2b",
                    "name": "Jianguo Li",
                    "hidden": false
                },
                {
                    "_id": "68369babaffae1c74f432a2c",
                    "name": "Peng Di",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T17:00:55.000Z",
            "submittedOnDailyAt": "2025-05-28T04:02:51.324Z",
            "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks",
            "submittedOnDailyBy": {
                "_id": "6430bdd8cd31d174a9f900fb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
                "isPro": false,
                "fullname": "Ziyin Zhang",
                "user": "Geralt-Targaryen",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) have shown promise in\nfunction-level code generation, yet repository-level software engineering tasks\nremain challenging. Current solutions predominantly rely on proprietary LLM\nagents, which introduce unpredictability and limit accessibility, raising\nconcerns about data privacy and model customization. This paper investigates\nwhether open-source LLMs can effectively address repository-level tasks without\nrequiring agent-based approaches. We demonstrate this is possible by enabling\nLLMs to comprehend functions and files within codebases through their semantic\ninformation and structural dependencies. To this end, we introduce Code Graph\nModels (CGMs), which integrate repository code graph structures into the LLM's\nattention mechanism and map node attributes to the LLM's input space using a\nspecialized adapter. When combined with an agentless graph RAG framework, our\napproach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark\nusing the open-source Qwen2.5-72B model. This performance ranks first among\nopen weight models, second among methods with open-source systems, and eighth\noverall, surpassing the previous best open-source model-based method by 12.33%.",
            "upvotes": 14,
            "discussionId": "68369bacaffae1c74f432a82",
            "githubRepo": "https://github.com/codefuse-ai/CodeFuse-CGM",
            "ai_summary": "Open-source Code Graph Models enhance repository-level code generation tasks by integrating code graph structures into LLMs' attention mechanisms, achieving high performance without agent-based approaches.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "function-level code generation",
                "repository-level software engineering",
                "Code Graph Models",
                "CGMs",
                "attention mechanism",
                "SWE-bench Lite benchmark",
                "Qwen2.5-72B model",
                "graph RAG framework"
            ]
        },
        "publishedAt": "2025-05-22T13:00:55.000Z",
        "title": "Code Graph Model (CGM): A Graph-Integrated Large Language Model for\n  Repository-Level Software Engineering Tasks",
        "summary": "Recent advances in Large Language Models (LLMs) have shown promise in\nfunction-level code generation, yet repository-level software engineering tasks\nremain challenging. Current solutions predominantly rely on proprietary LLM\nagents, which introduce unpredictability and limit accessibility, raising\nconcerns about data privacy and model customization. This paper investigates\nwhether open-source LLMs can effectively address repository-level tasks without\nrequiring agent-based approaches. We demonstrate this is possible by enabling\nLLMs to comprehend functions and files within codebases through their semantic\ninformation and structural dependencies. To this end, we introduce Code Graph\nModels (CGMs), which integrate repository code graph structures into the LLM's\nattention mechanism and map node attributes to the LLM's input space using a\nspecialized adapter. When combined with an agentless graph RAG framework, our\napproach achieves a 43.00% resolution rate on the SWE-bench Lite benchmark\nusing the open-source Qwen2.5-72B model. This performance ranks first among\nopen weight models, second among methods with open-source systems, and eighth\noverall, surpassing the previous best open-source model-based method by 12.33%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16901.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6430bdd8cd31d174a9f900fb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
            "fullname": "Ziyin Zhang",
            "name": "Geralt-Targaryen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17332",
            "authors": [
                {
                    "_id": "68349ace56ca97d1b2f1eb7c",
                    "name": "Hitesh Laxmichand Patel",
                    "hidden": false
                },
                {
                    "_id": "68349ace56ca97d1b2f1eb7d",
                    "name": "Amit Agarwal",
                    "hidden": false
                },
                {
                    "_id": "68349ace56ca97d1b2f1eb7e",
                    "user": {
                        "_id": "655c848048320febfc45db74",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-An-36AfvExD_EfLFXHOP.jpeg",
                        "isPro": false,
                        "fullname": "Arion Das",
                        "user": "ariondas",
                        "type": "user"
                    },
                    "name": "Arion Das",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T07:53:29.445Z",
                    "hidden": false
                },
                {
                    "_id": "68349ace56ca97d1b2f1eb7f",
                    "name": "Bhargava Kumar",
                    "hidden": false
                },
                {
                    "_id": "68349ace56ca97d1b2f1eb80",
                    "name": "Srikant Panda",
                    "hidden": false
                },
                {
                    "_id": "68349ace56ca97d1b2f1eb81",
                    "name": "Priyaranjan Pattnayak",
                    "hidden": false
                },
                {
                    "_id": "68349ace56ca97d1b2f1eb82",
                    "name": "Taki Hasan Rafi",
                    "hidden": false
                },
                {
                    "_id": "68349ace56ca97d1b2f1eb83",
                    "name": "Tejaswini Kumar",
                    "hidden": false
                },
                {
                    "_id": "68349ace56ca97d1b2f1eb84",
                    "name": "Dong-Kyu Chae",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655c848048320febfc45db74/N_UgYvbInwPwa_qIs0T42.png",
                "https://cdn-uploads.huggingface.co/production/uploads/655c848048320febfc45db74/XZ_unJDELPP-GuG7gJxCD.png"
            ],
            "publishedAt": "2025-05-22T22:56:58.000Z",
            "submittedOnDailyAt": "2025-05-28T16:09:34.651Z",
            "title": "SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for\n  Enterprise Use",
            "submittedOnDailyBy": {
                "_id": "655c848048320febfc45db74",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-An-36AfvExD_EfLFXHOP.jpeg",
                "isPro": false,
                "fullname": "Arion Das",
                "user": "ariondas",
                "type": "user"
            },
            "summary": "Enterprise customers are increasingly adopting Large Language Models (LLMs)\nfor critical communication tasks, such as drafting emails, crafting sales\npitches, and composing casual messages. Deploying such models across different\nregions requires them to understand diverse cultural and linguistic contexts\nand generate safe and respectful responses. For enterprise applications, it is\ncrucial to mitigate reputational risks, maintain trust, and ensure compliance\nby effectively identifying and handling unsafe or offensive language. To\naddress this, we introduce SweEval, a benchmark simulating real-world scenarios\nwith variations in tone (positive or negative) and context (formal or\ninformal). The prompts explicitly instruct the model to include specific swear\nwords while completing the task. This benchmark evaluates whether LLMs comply\nwith or resist such inappropriate instructions and assesses their alignment\nwith ethical frameworks, cultural nuances, and language comprehension\ncapabilities. In order to advance research in building ethically aligned AI\nsystems for enterprise use and beyond, we release the dataset and code:\nhttps://github.com/amitbcp/multilingual_profanity.",
            "upvotes": 13,
            "discussionId": "68349acf56ca97d1b2f1ebba",
            "projectPage": "https://github.com/amitbcp/multilingual_profanity",
            "githubRepo": "https://github.com/amitbcp/multilingual_profanity",
            "ai_summary": "SweEval is a benchmark for evaluating Large Language Models' compliance with ethical guidelines and cultural nuances when instructed to include offensive language.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "SweEval",
                "benchmark",
                "ethical alignment",
                "cultural nuances",
                "language comprehension",
                "ethical frameworks"
            ]
        },
        "publishedAt": "2025-05-22T18:56:58.000Z",
        "title": "SweEval: Do LLMs Really Swear? A Safety Benchmark for Testing Limits for\n  Enterprise Use",
        "summary": "Enterprise customers are increasingly adopting Large Language Models (LLMs)\nfor critical communication tasks, such as drafting emails, crafting sales\npitches, and composing casual messages. Deploying such models across different\nregions requires them to understand diverse cultural and linguistic contexts\nand generate safe and respectful responses. For enterprise applications, it is\ncrucial to mitigate reputational risks, maintain trust, and ensure compliance\nby effectively identifying and handling unsafe or offensive language. To\naddress this, we introduce SweEval, a benchmark simulating real-world scenarios\nwith variations in tone (positive or negative) and context (formal or\ninformal). The prompts explicitly instruct the model to include specific swear\nwords while completing the task. This benchmark evaluates whether LLMs comply\nwith or resist such inappropriate instructions and assesses their alignment\nwith ethical frameworks, cultural nuances, and language comprehension\ncapabilities. In order to advance research in building ethically aligned AI\nsystems for enterprise use and beyond, we release the dataset and code:\nhttps://github.com/amitbcp/multilingual_profanity.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/655c848048320febfc45db74/N_UgYvbInwPwa_qIs0T42.png",
            "https://cdn-uploads.huggingface.co/production/uploads/655c848048320febfc45db74/XZ_unJDELPP-GuG7gJxCD.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17332.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "655c848048320febfc45db74",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-An-36AfvExD_EfLFXHOP.jpeg",
            "fullname": "Arion Das",
            "name": "ariondas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20793",
            "authors": [
                {
                    "_id": "68372ea2ad0fc935fff6bd1e",
                    "name": "Juan A. Rodriguez",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd1f",
                    "name": "Haotian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd20",
                    "name": "Abhay Puri",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd21",
                    "name": "Aarash Feizi",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd22",
                    "name": "Rishav Pramanik",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd23",
                    "name": "Pascal Wichmann",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd24",
                    "name": "Arnab Mondal",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd25",
                    "name": "Mohammad Reza Samsami",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd26",
                    "user": {
                        "_id": "60edf5e03203a5daf7d3912e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60edf5e03203a5daf7d3912e/F3PpWaZf78lsa0ZdkjzTO.jpeg",
                        "isPro": false,
                        "fullname": "Rabiul Awal",
                        "user": "rabiulawal",
                        "type": "user"
                    },
                    "name": "Rabiul Awal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T20:10:16.612Z",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd27",
                    "name": "Perouz Taslakian",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd28",
                    "name": "Spandana Gella",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd29",
                    "name": "Sai Rajeswar",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd2a",
                    "name": "David Vazquez",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd2b",
                    "name": "Christopher Pal",
                    "hidden": false
                },
                {
                    "_id": "68372ea2ad0fc935fff6bd2c",
                    "name": "Marco Pedersoli",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/jwADHUBkcm-muR9Kq-ExP.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/AwbAvWpJN4gcmw5B-aHcu.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/14hs0RgVthaWJ0LFiQi4V.png"
            ],
            "publishedAt": "2025-05-27T06:56:00.000Z",
            "submittedOnDailyAt": "2025-05-28T14:15:34.744Z",
            "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation",
            "submittedOnDailyBy": {
                "_id": "63a614d264f470027818b066",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
                "isPro": false,
                "fullname": "Juan A. Rodriguez",
                "user": "joanrodai",
                "type": "user"
            },
            "summary": "Scalable Vector Graphics (SVG) offer a powerful format for representing\nvisual designs as interpretable code. Recent advances in vision-language models\n(VLMs) have enabled high-quality SVG generation by framing the problem as a\ncode generation task and leveraging large-scale pretraining. VLMs are\nparticularly suitable for this task as they capture both global semantics and\nfine-grained visual patterns, while transferring knowledge across vision,\nnatural language, and code domains. However, existing VLM approaches often\nstruggle to produce faithful and efficient SVGs because they never observe the\nrendered images during training. Although differentiable rendering for\nautoregressive SVG code generation remains unavailable, rendered outputs can\nstill be compared to original inputs, enabling evaluative feedback suitable for\nreinforcement learning (RL). We introduce RLRF(Reinforcement Learning from\nRendering Feedback), an RL method that enhances SVG generation in\nautoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an\ninput image, the model generates SVG roll-outs that are rendered and compared\nto the original image to compute a reward. This visual fidelity feedback guides\nthe model toward producing more accurate, efficient, and semantically coherent\nSVGs. RLRF significantly outperforms supervised fine-tuning, addressing common\nfailure modes and enabling precise, high-quality SVG generation with strong\nstructural understanding and generalization.",
            "upvotes": 8,
            "discussionId": "68372ea4ad0fc935fff6bd8b",
            "ai_summary": "RLRF, a reinforcement learning method utilizing rendering feedback, enhances SVG generation in VLMs, improving accuracy and efficiency by comparing rendered SVGs to original images.",
            "ai_keywords": [
                "vision-language models",
                "code generation",
                "large-scale pretraining",
                "global semantics",
                "fine-grained visual patterns",
                "differentiable rendering",
                "reinforcement learning",
                "autoregressive VLMs",
                "SVG roll-outs",
                "visual fidelity feedback",
                "supervised fine-tuning",
                "structural understanding",
                "generalization"
            ]
        },
        "publishedAt": "2025-05-27T02:56:00.000Z",
        "title": "Rendering-Aware Reinforcement Learning for Vector Graphics Generation",
        "summary": "Scalable Vector Graphics (SVG) offer a powerful format for representing\nvisual designs as interpretable code. Recent advances in vision-language models\n(VLMs) have enabled high-quality SVG generation by framing the problem as a\ncode generation task and leveraging large-scale pretraining. VLMs are\nparticularly suitable for this task as they capture both global semantics and\nfine-grained visual patterns, while transferring knowledge across vision,\nnatural language, and code domains. However, existing VLM approaches often\nstruggle to produce faithful and efficient SVGs because they never observe the\nrendered images during training. Although differentiable rendering for\nautoregressive SVG code generation remains unavailable, rendered outputs can\nstill be compared to original inputs, enabling evaluative feedback suitable for\nreinforcement learning (RL). We introduce RLRF(Reinforcement Learning from\nRendering Feedback), an RL method that enhances SVG generation in\nautoregressive VLMs by leveraging feedback from rendered SVG outputs. Given an\ninput image, the model generates SVG roll-outs that are rendered and compared\nto the original image to compute a reward. This visual fidelity feedback guides\nthe model toward producing more accurate, efficient, and semantically coherent\nSVGs. RLRF significantly outperforms supervised fine-tuning, addressing common\nfailure modes and enabling precise, high-quality SVG generation with strong\nstructural understanding and generalization.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/jwADHUBkcm-muR9Kq-ExP.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/AwbAvWpJN4gcmw5B-aHcu.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63a614d264f470027818b066/14hs0RgVthaWJ0LFiQi4V.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20793.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a614d264f470027818b066",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a614d264f470027818b066/Q5Eih2VqD4NaHB_MkNp32.jpeg",
            "fullname": "Juan A. Rodriguez",
            "name": "joanrodai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20287",
            "authors": [
                {
                    "_id": "6835d1587ba57211f250371d",
                    "user": {
                        "_id": "6496f5754a3c31df8e3139f6",
                        "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg",
                        "isPro": false,
                        "fullname": "Zhongwei Zhang",
                        "user": "zzwustc",
                        "type": "user"
                    },
                    "name": "Zhongwei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-27T15:48:53.306Z",
                    "hidden": false
                },
                {
                    "_id": "6835d1587ba57211f250371e",
                    "name": "Fuchen Long",
                    "hidden": false
                },
                {
                    "_id": "6835d1587ba57211f250371f",
                    "name": "Zhaofan Qiu",
                    "hidden": false
                },
                {
                    "_id": "6835d1587ba57211f2503720",
                    "name": "Yingwei Pan",
                    "hidden": false
                },
                {
                    "_id": "6835d1587ba57211f2503721",
                    "name": "Wu Liu",
                    "hidden": false
                },
                {
                    "_id": "6835d1587ba57211f2503722",
                    "name": "Ting Yao",
                    "hidden": false
                },
                {
                    "_id": "6835d1587ba57211f2503723",
                    "name": "Tao Mei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/wbsP5g2PSgJ5Uv1ZEK-Jw.mp4"
            ],
            "publishedAt": "2025-05-26T17:59:03.000Z",
            "submittedOnDailyAt": "2025-05-28T14:32:08.559Z",
            "title": "MotionPro: A Precise Motion Controller for Image-to-Video Generation",
            "submittedOnDailyBy": {
                "_id": "6496f5754a3c31df8e3139f6",
                "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg",
                "isPro": false,
                "fullname": "Zhongwei Zhang",
                "user": "zzwustc",
                "type": "user"
            },
            "summary": "Animating images with interactive motion control has garnered popularity for\nimage-to-video (I2V) generation. Modern approaches typically rely on large\nGaussian kernels to extend motion trajectories as condition without explicitly\ndefining movement region, leading to coarse motion control and failing to\ndisentangle object and camera moving. To alleviate these, we present MotionPro,\na precise motion controller that novelly leverages region-wise trajectory and\nmotion mask to regulate fine-grained motion synthesis and identify target\nmotion category (i.e., object or camera moving), respectively. Technically,\nMotionPro first estimates the flow maps on each training video via a tracking\nmodel, and then samples the region-wise trajectories to simulate inference\nscenario. Instead of extending flow through large Gaussian kernels, our\nregion-wise trajectory approach enables more precise control by directly\nutilizing trajectories within local regions, thereby effectively characterizing\nfine-grained movements. A motion mask is simultaneously derived from the\npredicted flow maps to capture the holistic motion dynamics of the movement\nregions. To pursue natural motion control, MotionPro further strengthens video\ndenoising by incorporating both region-wise trajectories and motion mask\nthrough feature modulation. More remarkably, we meticulously construct a\nbenchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for\nthe evaluation of both fine-grained and object-level I2V motion control.\nExtensive experiments conducted on WebVid-10M and MC-Bench demonstrate the\neffectiveness of MotionPro. Please refer to our project page for more results:\nhttps://zhw-zhang.github.io/MotionPro-page/.",
            "upvotes": 8,
            "discussionId": "6835d15c7ba57211f2503825",
            "projectPage": "https://zhw-zhang.github.io/MotionPro-page/",
            "githubRepo": "https://github.com/HiDream-ai/MotionPro",
            "ai_summary": "MotionPro uses region-wise trajectories and motion masks for precise image-to-video generation, enhancing motion control and disentangling object and camera movement.",
            "ai_keywords": [
                "Gaussian kernels",
                "motion control",
                "motion trajectories",
                "motion mask",
                "flow maps",
                "tracking model",
                "feature modulation",
                "fine-grained motion",
                "MC-Bench",
                "WebVid-10M",
                "video denoising"
            ]
        },
        "publishedAt": "2025-05-26T13:59:03.000Z",
        "title": "MotionPro: A Precise Motion Controller for Image-to-Video Generation",
        "summary": "Animating images with interactive motion control has garnered popularity for\nimage-to-video (I2V) generation. Modern approaches typically rely on large\nGaussian kernels to extend motion trajectories as condition without explicitly\ndefining movement region, leading to coarse motion control and failing to\ndisentangle object and camera moving. To alleviate these, we present MotionPro,\na precise motion controller that novelly leverages region-wise trajectory and\nmotion mask to regulate fine-grained motion synthesis and identify target\nmotion category (i.e., object or camera moving), respectively. Technically,\nMotionPro first estimates the flow maps on each training video via a tracking\nmodel, and then samples the region-wise trajectories to simulate inference\nscenario. Instead of extending flow through large Gaussian kernels, our\nregion-wise trajectory approach enables more precise control by directly\nutilizing trajectories within local regions, thereby effectively characterizing\nfine-grained movements. A motion mask is simultaneously derived from the\npredicted flow maps to capture the holistic motion dynamics of the movement\nregions. To pursue natural motion control, MotionPro further strengthens video\ndenoising by incorporating both region-wise trajectories and motion mask\nthrough feature modulation. More remarkably, we meticulously construct a\nbenchmark, i.e., MC-Bench, with 1.1K user-annotated image-trajectory pairs, for\nthe evaluation of both fine-grained and object-level I2V motion control.\nExtensive experiments conducted on WebVid-10M and MC-Bench demonstrate the\neffectiveness of MotionPro. Please refer to our project page for more results:\nhttps://zhw-zhang.github.io/MotionPro-page/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6496f5754a3c31df8e3139f6/wbsP5g2PSgJ5Uv1ZEK-Jw.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20287.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6496f5754a3c31df8e3139f6",
            "avatarUrl": "/avatars/cf789d1986f976373c82b2976df4542a.svg",
            "fullname": "Zhongwei Zhang",
            "name": "zzwustc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17613",
            "authors": [
                {
                    "_id": "68361038450a48f228ff555e",
                    "user": {
                        "_id": "65750954a90ae2daaec7ab93",
                        "avatarUrl": "/avatars/43ea342d9a10caeadc0183afc3848f5a.svg",
                        "isPro": false,
                        "fullname": "Jihan Yao",
                        "user": "yaojh18",
                        "type": "user"
                    },
                    "name": "Jihan Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:14:25.358Z",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff555f",
                    "user": {
                        "_id": "62b1474bdcbad6848a91a54e",
                        "avatarUrl": "/avatars/d7308899b46232cad4a48a0e876449a8.svg",
                        "isPro": false,
                        "fullname": "Yushi Hu",
                        "user": "yushihu",
                        "type": "user"
                    },
                    "name": "Yushi Hu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-27T19:21:19.936Z",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5560",
                    "name": "Yujie Yi",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5561",
                    "name": "Bin Han",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5562",
                    "name": "Shangbin Feng",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5563",
                    "name": "Guang Yang",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5564",
                    "name": "Bingbing Wen",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5565",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5566",
                    "name": "Lucy Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5567",
                    "name": "Yulia Tsvetkov",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5568",
                    "name": "Noah A. Smith",
                    "hidden": false
                },
                {
                    "_id": "68361038450a48f228ff5569",
                    "name": "Banghua Zhu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62b1474bdcbad6848a91a54e/qamj7EZHD6KuwusK6g9aH.png"
            ],
            "publishedAt": "2025-05-23T08:21:28.000Z",
            "submittedOnDailyAt": "2025-05-28T15:56:24.554Z",
            "title": "MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask\n  Multimodal Generation",
            "submittedOnDailyBy": {
                "_id": "62b1474bdcbad6848a91a54e",
                "avatarUrl": "/avatars/d7308899b46232cad4a48a0e876449a8.svg",
                "isPro": false,
                "fullname": "Yushi Hu",
                "user": "yushihu",
                "type": "user"
            },
            "summary": "Automatically evaluating multimodal generation presents a significant\nchallenge, as automated metrics often struggle to align reliably with human\nevaluation, especially for complex tasks that involve multiple modalities. To\naddress this, we present MMMG, a comprehensive and human-aligned benchmark for\nmultimodal generation across 4 modality combinations (image, audio, interleaved\ntext and image, interleaved text and audio), with a focus on tasks that present\nsignificant challenges for generation models, while still enabling reliable\nautomatic evaluation through a combination of models and programs. MMMG\nencompasses 49 tasks (including 29 newly developed ones), each with a carefully\ndesigned evaluation pipeline, and 937 instructions to systematically assess\nreasoning, controllability, and other key capabilities of multimodal generation\nmodels. Extensive validation demonstrates that MMMG is highly aligned with\nhuman evaluation, achieving an average agreement of 94.3%. Benchmarking results\non 24 multimodal generation models reveal that even though the state-of-the-art\nmodel, GPT Image, achieves 78.3% accuracy for image generation, it falls short\non multimodal reasoning and interleaved generation. Furthermore, results\nsuggest considerable headroom for improvement in audio generation, highlighting\nan important direction for future research.",
            "upvotes": 8,
            "discussionId": "6836103e450a48f228ff56d2",
            "projectPage": "https://yaojh18.github.io/mmmg-leaderboard/",
            "githubRepo": "https://github.com/yaojh18/MMMG",
            "ai_summary": "MMMG is a comprehensive benchmark for multimodal generation, offering 49 tasks and 937 instructions to align automatic evaluation with human judgment, revealing areas for improvement in reasoning and audio generation.",
            "ai_keywords": [
                "multimodal generation",
                "evaluation pipeline",
                "human-aligned benchmark",
                "multimodal reasoning",
                "interleaved generation",
                "audio generation",
                "GPT Image"
            ]
        },
        "publishedAt": "2025-05-23T04:21:28.000Z",
        "title": "MMMG: a Comprehensive and Reliable Evaluation Suite for Multitask\n  Multimodal Generation",
        "summary": "Automatically evaluating multimodal generation presents a significant\nchallenge, as automated metrics often struggle to align reliably with human\nevaluation, especially for complex tasks that involve multiple modalities. To\naddress this, we present MMMG, a comprehensive and human-aligned benchmark for\nmultimodal generation across 4 modality combinations (image, audio, interleaved\ntext and image, interleaved text and audio), with a focus on tasks that present\nsignificant challenges for generation models, while still enabling reliable\nautomatic evaluation through a combination of models and programs. MMMG\nencompasses 49 tasks (including 29 newly developed ones), each with a carefully\ndesigned evaluation pipeline, and 937 instructions to systematically assess\nreasoning, controllability, and other key capabilities of multimodal generation\nmodels. Extensive validation demonstrates that MMMG is highly aligned with\nhuman evaluation, achieving an average agreement of 94.3%. Benchmarking results\non 24 multimodal generation models reveal that even though the state-of-the-art\nmodel, GPT Image, achieves 78.3% accuracy for image generation, it falls short\non multimodal reasoning and interleaved generation. Furthermore, results\nsuggest considerable headroom for improvement in audio generation, highlighting\nan important direction for future research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62b1474bdcbad6848a91a54e/qamj7EZHD6KuwusK6g9aH.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17613.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b1474bdcbad6848a91a54e",
            "avatarUrl": "/avatars/d7308899b46232cad4a48a0e876449a8.svg",
            "fullname": "Yushi Hu",
            "name": "yushihu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21097",
            "authors": [
                {
                    "_id": "6837361b4b3a674ac2a2c27a",
                    "name": "Stephen Chung",
                    "hidden": false
                },
                {
                    "_id": "6837361b4b3a674ac2a2c27b",
                    "name": "Wenyu Du",
                    "hidden": false
                },
                {
                    "_id": "6837361b4b3a674ac2a2c27c",
                    "name": "Jie Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T12:22:46.000Z",
            "submittedOnDailyAt": "2025-05-28T14:43:44.272Z",
            "title": "Thinker: Learning to Think Fast and Slow",
            "submittedOnDailyBy": {
                "_id": "624c3d2ca19f20b197761ba9",
                "avatarUrl": "/avatars/7a64b81c29f4f6700fa18effc5616865.svg",
                "isPro": false,
                "fullname": "Wenyu Du",
                "user": "wydu",
                "type": "user"
            },
            "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training.",
            "upvotes": 7,
            "discussionId": "6837361c4b3a674ac2a2c2a6",
            "ai_summary": "A four-stage QA task modification, inspired by Dual Process Theory, improves the accuracy of LLMs in math and coding by separating intuition and deliberation.",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "Question-Answering (QA)",
                "DeepSeek R1",
                "Dual Process Theory",
                "Fast Thinking",
                "Verification",
                "Slow Thinking",
                "Summarization"
            ]
        },
        "publishedAt": "2025-05-27T08:22:46.000Z",
        "title": "Thinker: Learning to Think Fast and Slow",
        "summary": "Recent studies show that the reasoning capabilities of Large Language Models\n(LLMs) can be improved by applying Reinforcement Learning (RL) to\nquestion-answering (QA) tasks in areas such as math and coding. With a long\ncontext length, LLMs may learn to perform search, as indicated by the\nself-correction behavior observed in DeepSeek R1. However, this search behavior\nis often imprecise and lacks confidence, resulting in long, redundant responses\nand highlighting deficiencies in intuition and verification. Inspired by the\nDual Process Theory in psychology, we introduce a simple modification to the QA\ntask that includes four stages: Fast Thinking, where the LLM must answer within\na strict token budget; Verification, where the model evaluates its initial\nresponse; Slow Thinking, where it refines the initial response with more\ndeliberation; and Summarization, where it distills the refinement from the\nprevious stage into precise steps. Our proposed task improves average accuracy\nfrom 24.9% to 27.9% for Qwen2.5-1.5B, and from 45.9% to 49.8% for\nDeepSeek-R1-Qwen-1.5B. Notably, for Qwen2.5-1.5B, the Fast Thinking mode alone\nachieves 26.8% accuracy using fewer than 1000 tokens, demonstrating substantial\ninference efficiency gains. These findings suggest that intuition and\ndeliberative reasoning are distinct, complementary systems benefiting from\ntargeted training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21097.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "624c3d2ca19f20b197761ba9",
            "avatarUrl": "/avatars/7a64b81c29f4f6700fa18effc5616865.svg",
            "fullname": "Wenyu Du",
            "name": "wydu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20650",
            "authors": [
                {
                    "_id": "68376120fa4820e924fd2b35",
                    "user": {
                        "_id": "65d76cc5b9b7b8bf88faa916",
                        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                        "isPro": true,
                        "fullname": "Yan Wang",
                        "user": "YanAdjeNole",
                        "type": "user"
                    },
                    "name": "Yan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T20:10:14.599Z",
                    "hidden": false
                },
                {
                    "_id": "68376120fa4820e924fd2b36",
                    "name": "Yang Ren",
                    "hidden": false
                },
                {
                    "_id": "68376120fa4820e924fd2b37",
                    "name": "Lingfei Qian",
                    "hidden": false
                },
                {
                    "_id": "68376120fa4820e924fd2b38",
                    "name": "Xueqing Peng",
                    "hidden": false
                },
                {
                    "_id": "68376120fa4820e924fd2b39",
                    "name": "Keyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68376120fa4820e924fd2b3a",
                    "name": "Yi Han",
                    "hidden": false
                },
                {
                    "_id": "68376120fa4820e924fd2b3b",
                    "name": "Dongji Feng",
                    "hidden": false
                },
                {
                    "_id": "68376120fa4820e924fd2b3c",
                    "name": "Xiao-Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "68376120fa4820e924fd2b3d",
                    "name": "Jimin Huang",
                    "hidden": false
                },
                {
                    "_id": "68376120fa4820e924fd2b3e",
                    "name": "Qianqian Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T02:55:53.000Z",
            "submittedOnDailyAt": "2025-05-28T17:53:44.285Z",
            "title": "FinTagging: An LLM-ready Benchmark for Extracting and Structuring\n  Financial Information",
            "submittedOnDailyBy": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
            },
            "summary": "We introduce FinTagging, the first full-scope, table-aware XBRL benchmark\ndesigned to evaluate the structured information extraction and semantic\nalignment capabilities of large language models (LLMs) in the context of\nXBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL\ntagging as flat multi-class classification and focus solely on narrative text,\nFinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for\nfinancial entity extraction and FinCL for taxonomy-driven concept alignment. It\nrequires models to jointly extract facts and align them with the full 10k+\nUS-GAAP taxonomy across both unstructured text and structured tables, enabling\nrealistic, fine-grained evaluation. We assess a diverse set of LLMs under\nzero-shot settings, systematically analyzing their performance on both subtasks\nand overall tagging accuracy. Our results reveal that, while LLMs demonstrate\nstrong generalization in information extraction, they struggle with\nfine-grained concept alignment, particularly in disambiguating closely related\ntaxonomy entries. These findings highlight the limitations of existing LLMs in\nfully automating XBRL tagging and underscore the need for improved semantic\nreasoning and schema-aware modeling to meet the demands of accurate financial\ndisclosure. Code is available at our GitHub repository and data is at our\nHugging Face repository.",
            "upvotes": 7,
            "discussionId": "68376121fa4820e924fd2b70",
            "githubRepo": "https://github.com/The-FinAI/FinTagging",
            "ai_summary": "FinTagging evaluates LLMs for structured information extraction and semantic alignment in XBRL financial reporting, revealing challenges in fine-grained concept alignment.",
            "ai_keywords": [
                "FinTagging",
                "XBRL",
                "structured information extraction",
                "semantic alignment",
                "large language models",
                "FinNI",
                "FinCL",
                "US-GAAP taxonomy",
                "zero-shot settings"
            ]
        },
        "publishedAt": "2025-05-26T22:55:53.000Z",
        "title": "FinTagging: An LLM-ready Benchmark for Extracting and Structuring\n  Financial Information",
        "summary": "We introduce FinTagging, the first full-scope, table-aware XBRL benchmark\ndesigned to evaluate the structured information extraction and semantic\nalignment capabilities of large language models (LLMs) in the context of\nXBRL-based financial reporting. Unlike prior benchmarks that oversimplify XBRL\ntagging as flat multi-class classification and focus solely on narrative text,\nFinTagging decomposes the XBRL tagging problem into two subtasks: FinNI for\nfinancial entity extraction and FinCL for taxonomy-driven concept alignment. It\nrequires models to jointly extract facts and align them with the full 10k+\nUS-GAAP taxonomy across both unstructured text and structured tables, enabling\nrealistic, fine-grained evaluation. We assess a diverse set of LLMs under\nzero-shot settings, systematically analyzing their performance on both subtasks\nand overall tagging accuracy. Our results reveal that, while LLMs demonstrate\nstrong generalization in information extraction, they struggle with\nfine-grained concept alignment, particularly in disambiguating closely related\ntaxonomy entries. These findings highlight the limitations of existing LLMs in\nfully automating XBRL tagging and underscore the need for improved semantic\nreasoning and schema-aware modeling to meet the demands of accurate financial\ndisclosure. Code is available at our GitHub repository and data is at our\nHugging Face repository.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20650.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "fullname": "Yan Wang",
            "name": "YanAdjeNole",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20426",
            "authors": [
                {
                    "_id": "683718530ab97eef8cefc1ce",
                    "name": "Yunlong Tang",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1cf",
                    "name": "Pinxin Liu",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d0",
                    "name": "Mingqian Feng",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d1",
                    "name": "Zhangyun Tan",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d2",
                    "name": "Rui Mao",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d3",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d4",
                    "name": "Jing Bi",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d5",
                    "name": "Yunzhong Xiao",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d6",
                    "name": "Susan Liang",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d7",
                    "name": "Hang Hua",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d8",
                    "name": "Ali Vosoughi",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1d9",
                    "name": "Luchuan Song",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1da",
                    "name": "Zeliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "683718530ab97eef8cefc1db",
                    "name": "Chenliang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T18:20:22.000Z",
            "submittedOnDailyAt": "2025-05-28T12:36:44.124Z",
            "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive\n  Benchmark for Perspective Perception, Reasoning, and Robustness",
            "submittedOnDailyBy": {
                "_id": "6344c87f0f69ad8aa61dfcf6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344c87f0f69ad8aa61dfcf6/m4QiQ_4c6GfOIZZFICy7T.jpeg",
                "isPro": false,
                "fullname": "Yolo Y. Tang",
                "user": "yunlong10",
                "type": "user"
            },
            "summary": "Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/",
            "upvotes": 6,
            "discussionId": "6837185b0ab97eef8cefc41d",
            "ai_summary": "MMPerspective evaluates large language models' understanding of perspective geometry through tasks involving perception, reasoning, and robustness, revealing limitations in spatial reasoning and the benefits of chain-of-thought prompting.",
            "ai_keywords": [
                "multimodal large language models",
                "MMPerspective",
                "perspective geometry",
                "image instances",
                "question-answer pairs",
                "vanishing point perception",
                "perspective type reasoning",
                "line relationship understanding",
                "3D space",
                "invariance",
                "compositional reasoning",
                "spatial consistency",
                "robustness",
                "chain-of-thought prompting",
                "vision-language systems"
            ]
        },
        "publishedAt": "2025-05-26T14:20:22.000Z",
        "title": "MMPerspective: Do MLLMs Understand Perspective? A Comprehensive\n  Benchmark for Perspective Perception, Reasoning, and Robustness",
        "summary": "Understanding perspective is fundamental to human visual perception, yet the\nextent to which multimodal large language models (MLLMs) internalize\nperspective geometry remains unclear. We introduce MMPerspective, the first\nbenchmark specifically designed to systematically evaluate MLLMs' understanding\nof perspective through 10 carefully crafted tasks across three complementary\ndimensions: Perspective Perception, Reasoning, and Robustness. Our benchmark\ncomprises 2,711 real-world and synthetic image instances with 5,083\nquestion-answer pairs that probe key capabilities, such as vanishing point\nperception and counting, perspective type reasoning, line relationship\nunderstanding in 3D space, invariance to perspective-preserving\ntransformations, etc. Through a comprehensive evaluation of 43 state-of-the-art\nMLLMs, we uncover significant limitations: while models demonstrate competence\non surface-level perceptual tasks, they struggle with compositional reasoning\nand maintaining spatial consistency under perturbations. Our analysis further\nreveals intriguing patterns between model architecture, scale, and perspective\ncapabilities, highlighting both robustness bottlenecks and the benefits of\nchain-of-thought prompting. MMPerspective establishes a valuable testbed for\ndiagnosing and advancing spatial understanding in vision-language systems.\nResources available at: https://yunlong10.github.io/MMPerspective/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20426.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6344c87f0f69ad8aa61dfcf6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6344c87f0f69ad8aa61dfcf6/m4QiQ_4c6GfOIZZFICy7T.jpeg",
            "fullname": "Yolo Y. Tang",
            "name": "yunlong10",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21178",
            "authors": [
                {
                    "_id": "6836b5fccbd5554d2038732c",
                    "user": {
                        "_id": "617051728db4a760d912d81f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617051728db4a760d912d81f/4dKG176GPk2pPRK7ctaT-.jpeg",
                        "isPro": false,
                        "fullname": "Mingyang Song",
                        "user": "Nickyang",
                        "type": "user"
                    },
                    "name": "Mingyang Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:55:33.180Z",
                    "hidden": false
                },
                {
                    "_id": "6836b5fccbd5554d2038732d",
                    "name": "Mao Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T13:29:51.000Z",
            "submittedOnDailyAt": "2025-05-28T05:37:42.879Z",
            "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "617051728db4a760d912d81f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617051728db4a760d912d81f/4dKG176GPk2pPRK7ctaT-.jpeg",
                "isPro": false,
                "fullname": "Mingyang Song",
                "user": "Nickyang",
                "type": "user"
            },
            "summary": "As test-time scaling becomes a pivotal research frontier in Large Language\nModels (LLMs) development, contemporary and advanced post-training\nmethodologies increasingly focus on extending the generation length of long\nChain-of-Thought (CoT) responses to enhance reasoning capabilities toward\nDeepSeek R1-like performance. However, recent studies reveal a persistent\noverthinking phenomenon in state-of-the-art reasoning models, manifesting as\nexcessive redundancy or repetitive thinking patterns in long CoT responses. To\naddress this issue, in this paper, we propose a simple yet effective two-stage\nreinforcement learning framework for achieving concise reasoning in LLMs, named\nConciseR. Specifically, the first stage, using more training steps, aims to\nincentivize the model's reasoning capabilities via Group Relative Policy\nOptimization with clip-higher and dynamic sampling components (GRPO++), and the\nsecond stage, using fewer training steps, explicitly enforces conciseness and\nimproves efficiency via Length-aware Group Relative Policy Optimization\n(L-GRPO). Significantly, ConciseR only optimizes response length once all\nrollouts of a sample are correct, following the \"walk before you run\"\nprinciple. Extensive experimental results demonstrate that our ConciseR model,\nwhich generates more concise CoT reasoning responses, outperforms recent\nstate-of-the-art reasoning models with zero RL paradigm across AIME 2024,\nMATH-500, AMC 2023, Minerva, and Olympiad benchmarks.",
            "upvotes": 5,
            "discussionId": "6836b5fccbd5554d20387357",
            "ai_summary": "A reinforcement learning framework, ConciseR, is proposed to enhance the conciseness and efficiency of reasoning in LLMs through a two-stage optimization process.",
            "ai_keywords": [
                "Large Language Models",
                "Chain-of-Thought",
                "DeepSeek R1",
                "reinforcement learning",
                "Group Relative Policy Optimization",
                "clip-higher",
                "dynamic sampling",
                "Length-aware Group Relative Policy Optimization",
                "concise reasoning",
                "rollouts",
                "AIME 2024",
                "MATH-500",
                "AMC 2023",
                "Minerva",
                "Olympiad benchmarks"
            ]
        },
        "publishedAt": "2025-05-27T09:29:51.000Z",
        "title": "Walk Before You Run! Concise LLM Reasoning via Reinforcement Learning",
        "summary": "As test-time scaling becomes a pivotal research frontier in Large Language\nModels (LLMs) development, contemporary and advanced post-training\nmethodologies increasingly focus on extending the generation length of long\nChain-of-Thought (CoT) responses to enhance reasoning capabilities toward\nDeepSeek R1-like performance. However, recent studies reveal a persistent\noverthinking phenomenon in state-of-the-art reasoning models, manifesting as\nexcessive redundancy or repetitive thinking patterns in long CoT responses. To\naddress this issue, in this paper, we propose a simple yet effective two-stage\nreinforcement learning framework for achieving concise reasoning in LLMs, named\nConciseR. Specifically, the first stage, using more training steps, aims to\nincentivize the model's reasoning capabilities via Group Relative Policy\nOptimization with clip-higher and dynamic sampling components (GRPO++), and the\nsecond stage, using fewer training steps, explicitly enforces conciseness and\nimproves efficiency via Length-aware Group Relative Policy Optimization\n(L-GRPO). Significantly, ConciseR only optimizes response length once all\nrollouts of a sample are correct, following the \"walk before you run\"\nprinciple. Extensive experimental results demonstrate that our ConciseR model,\nwhich generates more concise CoT reasoning responses, outperforms recent\nstate-of-the-art reasoning models with zero RL paradigm across AIME 2024,\nMATH-500, AMC 2023, Minerva, and Olympiad benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21178.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "617051728db4a760d912d81f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617051728db4a760d912d81f/4dKG176GPk2pPRK7ctaT-.jpeg",
            "fullname": "Mingyang Song",
            "name": "Nickyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19433",
            "authors": [
                {
                    "_id": "6836b4b68ec432fdc7e38251",
                    "name": "Peijie Dong",
                    "hidden": false
                },
                {
                    "_id": "6836b4b68ec432fdc7e38252",
                    "name": "Zhenheng Tang",
                    "hidden": false
                },
                {
                    "_id": "6836b4b68ec432fdc7e38253",
                    "user": {
                        "_id": "63024676056ec3a2a8714b24",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661093436322-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiang Liu",
                        "user": "Dominic789654",
                        "type": "user"
                    },
                    "name": "Xiang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T12:12:19.286Z",
                    "hidden": false
                },
                {
                    "_id": "6836b4b68ec432fdc7e38254",
                    "name": "Lujun Li",
                    "hidden": false
                },
                {
                    "_id": "6836b4b68ec432fdc7e38255",
                    "name": "Xiaowen Chu",
                    "hidden": false
                },
                {
                    "_id": "6836b4b68ec432fdc7e38256",
                    "name": "Bo Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T02:49:07.000Z",
            "submittedOnDailyAt": "2025-05-28T05:31:59.506Z",
            "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic\n  Capabilities in LLM Compression",
            "submittedOnDailyBy": {
                "_id": "6395f845aec00abff778ad31",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6395f845aec00abff778ad31/bZkAlchSvqER1HgBKmcHI.jpeg",
                "isPro": false,
                "fullname": "PeijieDong",
                "user": "pprp",
                "type": "user"
            },
            "summary": "Post-training compression reduces the computational and memory costs of large\nlanguage models (LLMs), enabling resource-efficient deployment. However,\nexisting compression benchmarks only focus on language modeling (e.g.,\nperplexity) and natural language understanding tasks (e.g., GLUE accuracy),\nignoring the agentic capabilities - workflow, tool use/function call,\nlong-context understanding and real-world application. We introduce the Agent\nCompression Benchmark (ACBench), the first comprehensive benchmark for\nevaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)\n12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,\nNeedle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ)\nand pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),\nstandard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).\nOur experiments reveal compression tradeoffs: 4-bit quantization preserves\nworkflow generation and tool use (1%-3% drop) but degrades real-world\napplication accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation\nand Energy to systematize analysis. ACBench provides actionable insights for\noptimizing LLM compression in agentic scenarios. The code can be found in\nhttps://github.com/pprp/ACBench.",
            "upvotes": 5,
            "discussionId": "6836b4b88ec432fdc7e382ad",
            "ai_summary": "ACBench evaluates the impact of compression on the agentic capabilities of large language models, focusing on workflow generation, tool use, long-context understanding, and real-world application.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "post-training compression",
                "computational costs",
                "memory costs",
                "resource-efficient deployment",
                "language modeling",
                "perplexity",
                "natural language understanding",
                "GLUE accuracy",
                "Agent Compression Benchmark",
                "ACBench",
                "WorfBench",
                "Needle-in-Haystack",
                "quantization",
                "GPTQ",
                "AWQ",
                "pruning",
                "Wanda",
                "SparseGPT",
                "DeepSeek-R1-Distill",
                "ERank",
                "Top-k Ranking Correlation",
                "Energy",
                "agentic capabilities",
                "workflow generation",
                "tool use",
                "long-context understanding",
                "real-world application"
            ]
        },
        "publishedAt": "2025-05-25T22:49:07.000Z",
        "title": "Can Compressed LLMs Truly Act? An Empirical Evaluation of Agentic\n  Capabilities in LLM Compression",
        "summary": "Post-training compression reduces the computational and memory costs of large\nlanguage models (LLMs), enabling resource-efficient deployment. However,\nexisting compression benchmarks only focus on language modeling (e.g.,\nperplexity) and natural language understanding tasks (e.g., GLUE accuracy),\nignoring the agentic capabilities - workflow, tool use/function call,\nlong-context understanding and real-world application. We introduce the Agent\nCompression Benchmark (ACBench), the first comprehensive benchmark for\nevaluating how compression impacts LLMs' agentic abilities. ACBench spans (1)\n12 tasks across 4 capabilities (e.g., WorfBench for workflow generation,\nNeedle-in-Haystack for long-context retrieval), (2) quantization (GPTQ, AWQ)\nand pruning (Wanda, SparseGPT), and (3) 15 models, including small (Gemma-2B),\nstandard (Qwen2.5 7B-32B), and distilled reasoning LLMs (DeepSeek-R1-Distill).\nOur experiments reveal compression tradeoffs: 4-bit quantization preserves\nworkflow generation and tool use (1%-3% drop) but degrades real-world\napplication accuracy by 10%-15%. We introduce ERank, Top-k Ranking Correlation\nand Energy to systematize analysis. ACBench provides actionable insights for\noptimizing LLM compression in agentic scenarios. The code can be found in\nhttps://github.com/pprp/ACBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19433.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6395f845aec00abff778ad31",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6395f845aec00abff778ad31/bZkAlchSvqER1HgBKmcHI.jpeg",
            "fullname": "PeijieDong",
            "name": "pprp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20321",
            "authors": [
                {
                    "_id": "68370a124a68a691a29331cb",
                    "user": {
                        "_id": "672e33ad6ff4cb430622f915",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_HBL0daPqqdN8rNU6l25i.png",
                        "isPro": false,
                        "fullname": "Mathew Koretsky",
                        "user": "mkoretsky1",
                        "type": "user"
                    },
                    "name": "Mathew J. Koretsky",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:14:08.928Z",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331cc",
                    "name": "Maya Willey",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331cd",
                    "name": "Adi Asija",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331ce",
                    "name": "Owen Bianchi",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331cf",
                    "name": "Chelsea X. Alvarado",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331d0",
                    "name": "Tanay Nayak",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331d1",
                    "name": "Nicole Kuznetsov",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331d2",
                    "name": "Sungwon Kim",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331d3",
                    "name": "Mike A. Nalls",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331d4",
                    "user": {
                        "_id": "5f6540c65e78cc6b0ed3199d",
                        "avatarUrl": "/avatars/0280d4df417855965a0964d22766c012.svg",
                        "isPro": false,
                        "fullname": "Daniel Khashabi",
                        "user": "danyaljj",
                        "type": "user"
                    },
                    "name": "Daniel Khashabi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:14:06.658Z",
                    "hidden": false
                },
                {
                    "_id": "68370a124a68a691a29331d5",
                    "name": "Faraz Faghri",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T17:58:07.000Z",
            "submittedOnDailyAt": "2025-05-28T11:45:03.838Z",
            "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases",
            "submittedOnDailyBy": {
                "_id": "672e33ad6ff4cb430622f915",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_HBL0daPqqdN8rNU6l25i.png",
                "isPro": false,
                "fullname": "Mathew Koretsky",
                "user": "mkoretsky1",
                "type": "user"
            },
            "summary": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql.",
            "upvotes": 5,
            "discussionId": "68370a124a68a691a2933216",
            "ai_summary": "BiomedSQL evaluates scientific reasoning in text-to-SQL tasks using a large biomedical knowledge base, highlighting performance gaps in existing models.",
            "ai_keywords": [
                "text-to-SQL",
                "biomedical knowledge base",
                "gene-disease associations",
                "causal inference",
                "omics data",
                "drug approval records",
                "genome-wide significance",
                "effect directionality",
                "trial phase filtering",
                "multi-step agent",
                "expert baseline"
            ]
        },
        "publishedAt": "2025-05-23T13:58:07.000Z",
        "title": "BiomedSQL: Text-to-SQL for Scientific Reasoning on Biomedical Knowledge\n  Bases",
        "summary": "Biomedical researchers increasingly rely on large-scale structured databases\nfor complex analytical tasks. However, current text-to-SQL systems often\nstruggle to map qualitative scientific questions into executable SQL,\nparticularly when implicit domain reasoning is required. We introduce\nBiomedSQL, the first benchmark explicitly designed to evaluate scientific\nreasoning in text-to-SQL generation over a real-world biomedical knowledge\nbase. BiomedSQL comprises 68,000 question/SQL query/answer triples grounded in\na harmonized BigQuery knowledge base that integrates gene-disease associations,\ncausal inference from omics data, and drug approval records. Each question\nrequires models to infer domain-specific criteria, such as genome-wide\nsignificance thresholds, effect directionality, or trial phase filtering,\nrather than rely on syntactic translation alone. We evaluate a range of open-\nand closed-source LLMs across prompting strategies and interaction paradigms.\nOur results reveal a substantial performance gap: GPT-o3-mini achieves 59.0%\nexecution accuracy, while our custom multi-step agent, BMSQL, reaches 62.6%,\nboth well below the expert baseline of 90.0%. BiomedSQL provides a new\nfoundation for advancing text-to-SQL systems capable of supporting scientific\ndiscovery through robust reasoning over structured biomedical knowledge bases.\nOur dataset is publicly available at\nhttps://huggingface.co/datasets/NIH-CARD/BiomedSQL, and our code is open-source\nat https://github.com/NIH-CARD/biomedsql.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20321.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672e33ad6ff4cb430622f915",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_HBL0daPqqdN8rNU6l25i.png",
            "fullname": "Mathew Koretsky",
            "name": "mkoretsky1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18134",
            "authors": [
                {
                    "_id": "6837123c7b41ef0965ce8f44",
                    "name": "Alex L. Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837123c7b41ef0965ce8f45",
                    "name": "Thomas L. Griffiths",
                    "hidden": false
                },
                {
                    "_id": "6837123c7b41ef0965ce8f46",
                    "name": "Karthik R. Narasimhan",
                    "hidden": false
                },
                {
                    "_id": "6837123c7b41ef0965ce8f47",
                    "name": "Ofir Press",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65b7a5657817e067a5ad45d8/nunstyNGiJMn1HUseOV5Q.mp4"
            ],
            "publishedAt": "2025-05-23T17:43:27.000Z",
            "submittedOnDailyAt": "2025-05-28T12:11:58.194Z",
            "title": "VideoGameBench: Can Vision-Language Models complete popular video games?",
            "submittedOnDailyBy": {
                "_id": "65b7a5657817e067a5ad45d8",
                "avatarUrl": "/avatars/f8792352f16be80bd138ffc911138be1.svg",
                "isPro": false,
                "fullname": "Ofir Press",
                "user": "ofirpress",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.",
            "upvotes": 5,
            "discussionId": "6837123f7b41ef0965ce8fef",
            "ai_summary": "VideoGameBench evaluates vision-language models' abilities in real-time video game interaction using only visual inputs and high-level objectives, highlighting challenges in human-like skills.",
            "ai_keywords": [
                "vision-language models",
                "VideoGameBench",
                "real-time interaction",
                "visual inputs",
                "high-level objectives",
                "inference latency",
                "VideoGameBench Lite",
                "Gemini 2.5 Pro"
            ]
        },
        "publishedAt": "2025-05-23T13:43:27.000Z",
        "title": "VideoGameBench: Can Vision-Language Models complete popular video games?",
        "summary": "Vision-language models (VLMs) have achieved strong results on coding and math\nbenchmarks that are challenging for humans, yet their ability to perform tasks\nthat come naturally to humans--such as perception, spatial navigation, and\nmemory management--remains understudied. Real video games are crafted to be\nintuitive for humans to learn and master by leveraging innate inductive biases,\nmaking them an ideal testbed for evaluating such capabilities in VLMs. To this\nend, we introduce VideoGameBench, a benchmark consisting of 10 popular video\ngames from the 1990s that VLMs directly interact with in real-time.\nVideoGameBench challenges models to complete entire games with access to only\nraw visual inputs and a high-level description of objectives and controls, a\nsignificant departure from existing setups that rely on game-specific\nscaffolding and auxiliary information. We keep three of the games secret to\nencourage solutions that generalize to unseen environments. Our experiments\nshow that frontier vision-language models struggle to progress beyond the\nbeginning of each game. We find inference latency to be a major limitation of\nfrontier models in the real-time setting; therefore, we introduce\nVideoGameBench Lite, a setting where the game pauses while waiting for the LM's\nnext action. The best performing model, Gemini 2.5 Pro, completes only 0.48% of\nVideoGameBench and 1.6% of VideoGameBench Lite. We hope that the formalization\nof the human skills mentioned above into this benchmark motivates progress in\nthese research directions.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65b7a5657817e067a5ad45d8/nunstyNGiJMn1HUseOV5Q.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18134.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b7a5657817e067a5ad45d8",
            "avatarUrl": "/avatars/f8792352f16be80bd138ffc911138be1.svg",
            "fullname": "Ofir Press",
            "name": "ofirpress",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.11277",
            "authors": [
                {
                    "_id": "6834f8d4bb7d1147551cc8f6",
                    "user": {
                        "_id": "63edd2d1f765928ceeb49057",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676530369930-noauth.png",
                        "isPro": false,
                        "fullname": "Yaorui SHI",
                        "user": "yrshi",
                        "type": "user"
                    },
                    "name": "Yaorui Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T09:00:51.919Z",
                    "hidden": false
                },
                {
                    "_id": "6834f8d4bb7d1147551cc8f7",
                    "name": "Shihan Li",
                    "hidden": false
                },
                {
                    "_id": "6834f8d4bb7d1147551cc8f8",
                    "name": "Chang Wu",
                    "hidden": false
                },
                {
                    "_id": "6834f8d4bb7d1147551cc8f9",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6834f8d4bb7d1147551cc8fa",
                    "name": "Junfeng Fang",
                    "hidden": false
                },
                {
                    "_id": "6834f8d4bb7d1147551cc8fb",
                    "user": {
                        "_id": "6340c214181f5648a58f5f82",
                        "avatarUrl": "/avatars/2ee353d8ec2b0ebc5b58d0641beddc26.svg",
                        "isPro": false,
                        "fullname": "Hengxing Cai",
                        "user": "enjoy",
                        "type": "user"
                    },
                    "name": "Hengxing Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:35:49.890Z",
                    "hidden": false
                },
                {
                    "_id": "6834f8d4bb7d1147551cc8fc",
                    "name": "An Zhang",
                    "hidden": false
                },
                {
                    "_id": "6834f8d4bb7d1147551cc8fd",
                    "name": "Xiang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-16T14:11:29.000Z",
            "submittedOnDailyAt": "2025-05-28T05:09:33.318Z",
            "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs",
            "submittedOnDailyBy": {
                "_id": "63edd2d1f765928ceeb49057",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676530369930-noauth.png",
                "isPro": false,
                "fullname": "Yaorui SHI",
                "user": "yrshi",
                "type": "user"
            },
            "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.",
            "upvotes": 5,
            "discussionId": "6834f8d4bb7d1147551cc93f",
            "githubRepo": "https://github.com/syr-cn/AutoRefine",
            "ai_summary": "AutoRefine, a reinforcement learning framework for large language models, enhances retrieval-augmented reasoning by iteratively refining knowledge and optimizing searches, leading to improved performance in complex question-answering tasks.",
            "ai_keywords": [
                "Large language models",
                "Retrieval-augmented reasoning",
                "AutoRefine",
                "Reinforcement learning",
                "Search-and-refine-during-think",
                "Reinforcement learning post-training",
                "Group relative policy optimization",
                "Single-hop QA benchmarks",
                "Multi-hop QA benchmarks"
            ]
        },
        "publishedAt": "2025-05-16T10:11:29.000Z",
        "title": "Search and Refine During Think: Autonomous Retrieval-Augmented Reasoning\n  of LLMs",
        "summary": "Large language models have demonstrated impressive reasoning capabilities but\nare inherently limited by their knowledge reservoir. Retrieval-augmented\nreasoning mitigates this limitation by allowing LLMs to query external\nresources, but existing methods often retrieve irrelevant or noisy information,\nhindering accurate reasoning. In this paper, we propose AutoRefine, a\nreinforcement learning post-training framework that adopts a new\n``search-and-refine-during-think'' paradigm. AutoRefine introduces explicit\nknowledge refinement steps between successive search calls, enabling the model\nto iteratively filter, distill, and organize evidence before generating an\nanswer. Furthermore, we incorporate tailored retrieval-specific rewards\nalongside answer correctness rewards using group relative policy optimization.\nExperiments on single-hop and multi-hop QA benchmarks demonstrate that\nAutoRefine significantly outperforms existing approaches, particularly in\ncomplex, multi-hop reasoning scenarios. Detailed analysis shows that AutoRefine\nissues frequent, higher-quality searches and synthesizes evidence effectively.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11277.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63edd2d1f765928ceeb49057",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676530369930-noauth.png",
            "fullname": "Yaorui SHI",
            "name": "yrshi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21471",
            "authors": [
                {
                    "_id": "6836ee74ce6ceb4c0d4ad9b9",
                    "name": "Zijun Liu",
                    "hidden": false
                },
                {
                    "_id": "6836ee74ce6ceb4c0d4ad9ba",
                    "user": {
                        "_id": "65d73ac991dbd0f8b399ab73",
                        "avatarUrl": "/avatars/c73ab59bd20a8de796e4f58a4147c4c9.svg",
                        "isPro": false,
                        "fullname": "Zhennan Wan",
                        "user": "zhennan1",
                        "type": "user"
                    },
                    "name": "Zhennan Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T12:12:21.052Z",
                    "hidden": false
                },
                {
                    "_id": "6836ee74ce6ceb4c0d4ad9bb",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "6836ee74ce6ceb4c0d4ad9bc",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "6836ee74ce6ceb4c0d4ad9bd",
                    "name": "Ji Zhang",
                    "hidden": false
                },
                {
                    "_id": "6836ee74ce6ceb4c0d4ad9be",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6836ee74ce6ceb4c0d4ad9bf",
                    "name": "Yang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T17:45:04.000Z",
            "submittedOnDailyAt": "2025-05-28T11:12:01.361Z",
            "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration",
            "submittedOnDailyBy": {
                "_id": "65d73ac991dbd0f8b399ab73",
                "avatarUrl": "/avatars/c73ab59bd20a8de796e4f58a4147c4c9.svg",
                "isPro": false,
                "fullname": "Zhennan Wan",
                "user": "zhennan1",
                "type": "user"
            },
            "summary": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\nExtAgents, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$boldsymbol{inftyBench+}, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls within or exceeds the context window$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications.",
            "upvotes": 4,
            "discussionId": "6836ee79ce6ceb4c0d4adb34",
            "projectPage": "https://github.com/THUNLP-MT/ExtAgents",
            "githubRepo": "https://github.com/THUNLP-MT/ExtAgents",
            "ai_summary": "The ExtAgents multi-agent framework enhances the scalability of inference-time knowledge integration in large language models, improving performance without increasing the context window.",
            "ai_keywords": [
                "large language models",
                "post-training techniques",
                "reasoning",
                "information seeking",
                "context window",
                "multi-agent framework",
                "knowledge synchronization",
                "reasoning processes",
                "multi-hop question answering",
                "long survey generation",
                "parallelism"
            ]
        },
        "publishedAt": "2025-05-27T13:45:04.000Z",
        "title": "Scaling External Knowledge Input Beyond Context Windows of LLMs via\n  Multi-Agent Collaboration",
        "summary": "With the rapid advancement of post-training techniques for reasoning and\ninformation seeking, large language models (LLMs) can incorporate a large\nquantity of retrieved knowledge to solve complex tasks. However, the limited\ncontext window of LLMs obstructs scaling the amount of external knowledge\ninput, prohibiting further improvement, especially for tasks requiring\nsignificant amount of external knowledge. Existing context window extension\nmethods inevitably cause information loss. LLM-based multi-agent methods emerge\nas a new paradigm to handle massive input in a distributional manner, where we\nidentify two core bottlenecks in existing knowledge synchronization and\nreasoning processes. In this work, we develop a multi-agent framework,\nExtAgents, to overcome the bottlenecks and enable better scalability\nin inference-time knowledge integration without longer-context training.\nBenchmarked with our enhanced multi-hop question answering test,\n$boldsymbol{inftyBench+}, and other public test sets including\nlong survey generation, ExtAgents significantly enhances the performance over\nexisting non-training methods with the same amount of external knowledge input,\nregardless of whether it falls within or exceeds the context window$.\nMoreover, the method maintains high efficiency due to high parallelism. Further\nstudy in the coordination of LLM agents on increasing external knowledge input\ncould benefit real-world applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21471.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d73ac991dbd0f8b399ab73",
            "avatarUrl": "/avatars/c73ab59bd20a8de796e4f58a4147c4c9.svg",
            "fullname": "Zhennan Wan",
            "name": "zhennan1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20561",
            "authors": [
                {
                    "_id": "6836a46b2a5e3993a42e3e3f",
                    "user": {
                        "_id": "661213f894e0b3bff3e80c69",
                        "avatarUrl": "/avatars/d8febbb081825bf91e487aa8bad3a391.svg",
                        "isPro": false,
                        "fullname": "Shenao Zhang",
                        "user": "ZhangShenao",
                        "type": "user"
                    },
                    "name": "Shenao Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:55:47.352Z",
                    "hidden": false
                },
                {
                    "_id": "6836a46b2a5e3993a42e3e40",
                    "user": {
                        "_id": "62cb0f38b189b6164d2a7900",
                        "avatarUrl": "/avatars/8bdf9705822fba3518e25db45d894535.svg",
                        "isPro": false,
                        "fullname": "Yaqing Wang",
                        "user": "yaqingwang90",
                        "type": "user"
                    },
                    "name": "Yaqing Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:55:53.241Z",
                    "hidden": false
                },
                {
                    "_id": "6836a46b2a5e3993a42e3e41",
                    "name": "Yinxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "6836a46b2a5e3993a42e3e42",
                    "name": "Tianqi Liu",
                    "hidden": false
                },
                {
                    "_id": "6836a46b2a5e3993a42e3e43",
                    "name": "Peter Grabowski",
                    "hidden": false
                },
                {
                    "_id": "6836a46b2a5e3993a42e3e44",
                    "user": {
                        "_id": "65a5d95d0797583a68438f49",
                        "avatarUrl": "/avatars/266f78b2678fcddac51c75f6fe33f118.svg",
                        "isPro": false,
                        "fullname": "Eugene Ie",
                        "user": "eugeneie",
                        "type": "user"
                    },
                    "name": "Eugene Ie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:56:22.038Z",
                    "hidden": false
                },
                {
                    "_id": "6836a46b2a5e3993a42e3e45",
                    "name": "Zhaoran Wang",
                    "hidden": false
                },
                {
                    "_id": "6836a46b2a5e3993a42e3e46",
                    "user": {
                        "_id": "63531848fc9b7ad0a53fa564",
                        "avatarUrl": "/avatars/d8df470349aa976741604921cb408440.svg",
                        "isPro": false,
                        "fullname": "Yunxuan Li",
                        "user": "eric008",
                        "type": "user"
                    },
                    "name": "Yunxuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:56:39.817Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T22:51:00.000Z",
            "submittedOnDailyAt": "2025-05-28T04:22:53.793Z",
            "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "661213f894e0b3bff3e80c69",
                "avatarUrl": "/avatars/d8febbb081825bf91e487aa8bad3a391.svg",
                "isPro": false,
                "fullname": "Shenao Zhang",
                "user": "ZhangShenao",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have\nexhibited strong reasoning capabilities and emergent reflective behaviors, such\nas backtracking and error correction. However, conventional Markovian RL\nconfines exploration to the training phase to learn an optimal deterministic\npolicy and depends on the history contexts only through the current state.\nTherefore, it remains unclear whether reflective reasoning will emerge during\nMarkovian RL training, or why they are beneficial at test time. To remedy this,\nwe recast reflective exploration within the Bayes-Adaptive RL framework, which\nexplicitly optimizes the expected return under a posterior distribution over\nMarkov decision processes. This Bayesian formulation inherently incentivizes\nboth reward-maximizing exploitation and information-gathering exploration via\nbelief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and\nswitch strategies based on the observed outcomes, offering principled guidance\non when and how the model should reflectively explore. Empirical results on\nboth synthetic and mathematical reasoning tasks demonstrate that BARL\noutperforms standard Markovian RL approaches at test time, achieving superior\ntoken efficiency with improved exploration effectiveness. Our code is available\nat https://github.com/shenao-zhang/BARL.",
            "upvotes": 4,
            "discussionId": "6836a46c2a5e3993a42e3e79",
            "githubRepo": "https://github.com/shenao-zhang/BARL",
            "ai_summary": "BARL, a Bayes-Adaptive RL framework, enhances LLM performance by integrating reflective reasoning and efficient exploration, leading to better token efficiency and effectiveness in test scenarios.",
            "ai_keywords": [
                "Reinforcement Learning (RL)",
                "backtracking",
                "error correction",
                "Markovian RL",
                "Bayes-Adaptive RL",
                "expected return",
                "posterior distribution",
                "Markov decision processes",
                "belief updates",
                "BARL",
                "token efficiency",
                "exploration effectiveness"
            ]
        },
        "publishedAt": "2025-05-26T18:51:00.000Z",
        "title": "Beyond Markovian: Reflective Exploration via Bayes-Adaptive RL for LLM\n  Reasoning",
        "summary": "Large Language Models (LLMs) trained via Reinforcement Learning (RL) have\nexhibited strong reasoning capabilities and emergent reflective behaviors, such\nas backtracking and error correction. However, conventional Markovian RL\nconfines exploration to the training phase to learn an optimal deterministic\npolicy and depends on the history contexts only through the current state.\nTherefore, it remains unclear whether reflective reasoning will emerge during\nMarkovian RL training, or why they are beneficial at test time. To remedy this,\nwe recast reflective exploration within the Bayes-Adaptive RL framework, which\nexplicitly optimizes the expected return under a posterior distribution over\nMarkov decision processes. This Bayesian formulation inherently incentivizes\nboth reward-maximizing exploitation and information-gathering exploration via\nbelief updates. Our resulting algorithm, BARL, instructs the LLM to stitch and\nswitch strategies based on the observed outcomes, offering principled guidance\non when and how the model should reflectively explore. Empirical results on\nboth synthetic and mathematical reasoning tasks demonstrate that BARL\noutperforms standard Markovian RL approaches at test time, achieving superior\ntoken efficiency with improved exploration effectiveness. Our code is available\nat https://github.com/shenao-zhang/BARL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20561.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661213f894e0b3bff3e80c69",
            "avatarUrl": "/avatars/d8febbb081825bf91e487aa8bad3a391.svg",
            "fullname": "Shenao Zhang",
            "name": "ZhangShenao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18657",
            "authors": [
                {
                    "_id": "6836db002cbd03bbddcc8696",
                    "name": "Xu Zheng",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc8697",
                    "user": {
                        "_id": "6806464ed918f6d2fee2bc8b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
                        "isPro": false,
                        "fullname": "Chenfei Liao",
                        "user": "Chenfei-Liao",
                        "type": "user"
                    },
                    "name": "Chenfei Liao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T10:11:13.295Z",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc8698",
                    "name": "Yuqian Fu",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc8699",
                    "name": "Kaiyu Lei",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc869a",
                    "name": "Yuanhuiyi Lyu",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc869b",
                    "name": "Lutao Jiang",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc869c",
                    "name": "Bin Ren",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc869d",
                    "name": "Jialei Chen",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc869e",
                    "name": "Jiawen Wang",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc869f",
                    "name": "Chengxin Li",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc86a0",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc86a1",
                    "name": "Danda Pani Paudel",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc86a2",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc86a3",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc86a4",
                    "name": "Nicu Sebe",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc86a5",
                    "name": "Dacheng Tao",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc86a6",
                    "name": "Luc Van Gool",
                    "hidden": false
                },
                {
                    "_id": "6836db002cbd03bbddcc86a7",
                    "name": "Xuming Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T11:49:31.000Z",
            "submittedOnDailyAt": "2025-05-28T08:16:07.597Z",
            "title": "MLLMs are Deeply Affected by Modality Bias",
            "submittedOnDailyBy": {
                "_id": "6806464ed918f6d2fee2bc8b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
                "isPro": false,
                "fullname": "Chenfei Liao",
                "user": "Chenfei-Liao",
                "type": "user"
            },
            "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have shown\npromising results in integrating diverse modalities such as texts and images.\nMLLMs are heavily influenced by modality bias, often relying on language while\nunder-utilizing other modalities like visual inputs. This position paper argues\nthat MLLMs are deeply affected by modality bias. Firstly, we diagnose the\ncurrent state of modality bias, highlighting its manifestations across various\ntasks. Secondly, we propose a systematic research road-map related to modality\nbias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and\noffer actionable suggestions for future research to mitigate it. To\nsubstantiate these findings, we conduct experiments that demonstrate the\ninfluence of each factor: 1. Data Characteristics: Language data is compact and\nabstract, while visual data is redundant and complex, creating an inherent\nimbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The\ndominance of pretrained language models in MLLMs leads to overreliance on\nlanguage and neglect of visual information. 3. Training Objectives: Current\nobjectives often fail to promote balanced cross-modal alignment, resulting in\nshortcut learning biased toward language. These findings highlight the need for\nbalanced training strategies and model architectures to better integrate\nmultiple modalities in MLLMs. We call for interdisciplinary efforts to tackle\nthese challenges and drive innovation in MLLM research. Our work provides a\nfresh perspective on modality bias in MLLMs and offers insights for developing\nmore robust and generalizable multimodal systems-advancing progress toward\nArtificial General Intelligence.",
            "upvotes": 4,
            "discussionId": "6836db012cbd03bbddcc86d8",
            "ai_summary": "MLLMs exhibit modality bias, favoring language over other modalities like visual inputs, which impedes balanced multimodal integration and necessitates research into balanced strategies and architectures.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "modality bias",
                "language data",
                "visual data",
                "pretrained language models",
                "cross-modal alignment",
                "shortcut learning",
                "balanced training strategies"
            ]
        },
        "publishedAt": "2025-05-24T07:49:31.000Z",
        "title": "MLLMs are Deeply Affected by Modality Bias",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have shown\npromising results in integrating diverse modalities such as texts and images.\nMLLMs are heavily influenced by modality bias, often relying on language while\nunder-utilizing other modalities like visual inputs. This position paper argues\nthat MLLMs are deeply affected by modality bias. Firstly, we diagnose the\ncurrent state of modality bias, highlighting its manifestations across various\ntasks. Secondly, we propose a systematic research road-map related to modality\nbias in MLLMs. Thirdly, we identify key factors of modality bias in MLLMs and\noffer actionable suggestions for future research to mitigate it. To\nsubstantiate these findings, we conduct experiments that demonstrate the\ninfluence of each factor: 1. Data Characteristics: Language data is compact and\nabstract, while visual data is redundant and complex, creating an inherent\nimbalance in learning dynamics. 2. Imbalanced Backbone Capabilities: The\ndominance of pretrained language models in MLLMs leads to overreliance on\nlanguage and neglect of visual information. 3. Training Objectives: Current\nobjectives often fail to promote balanced cross-modal alignment, resulting in\nshortcut learning biased toward language. These findings highlight the need for\nbalanced training strategies and model architectures to better integrate\nmultiple modalities in MLLMs. We call for interdisciplinary efforts to tackle\nthese challenges and drive innovation in MLLM research. Our work provides a\nfresh perspective on modality bias in MLLMs and offers insights for developing\nmore robust and generalizable multimodal systems-advancing progress toward\nArtificial General Intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18657.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6806464ed918f6d2fee2bc8b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6806464ed918f6d2fee2bc8b/rgpG2oO0m6PT0KltCF_Wf.jpeg",
            "fullname": "Chenfei Liao",
            "name": "Chenfei-Liao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17005",
            "authors": [
                {
                    "_id": "683469f0df7cbb5c08a0498a",
                    "name": "Huatong Song",
                    "hidden": false
                },
                {
                    "_id": "683469f0df7cbb5c08a0498b",
                    "name": "Jinhao Jiang",
                    "hidden": false
                },
                {
                    "_id": "683469f0df7cbb5c08a0498c",
                    "name": "Wenqing Tian",
                    "hidden": false
                },
                {
                    "_id": "683469f0df7cbb5c08a0498d",
                    "name": "Zhipeng Chen",
                    "hidden": false
                },
                {
                    "_id": "683469f0df7cbb5c08a0498e",
                    "name": "Yuhuan Wu",
                    "hidden": false
                },
                {
                    "_id": "683469f0df7cbb5c08a0498f",
                    "name": "Jiahao Zhao",
                    "hidden": false
                },
                {
                    "_id": "683469f0df7cbb5c08a04990",
                    "user": {
                        "_id": "6703ac76ea890f0ca5b225eb",
                        "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
                        "isPro": false,
                        "fullname": "Yingqian Min",
                        "user": "EliverQ",
                        "type": "user"
                    },
                    "name": "Yingqian Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T09:00:55.996Z",
                    "hidden": false
                },
                {
                    "_id": "683469f0df7cbb5c08a04991",
                    "name": "Wayne Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "683469f0df7cbb5c08a04992",
                    "name": "Lei Fang",
                    "hidden": false
                },
                {
                    "_id": "683469f0df7cbb5c08a04993",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T17:58:26.000Z",
            "submittedOnDailyAt": "2025-05-28T06:24:31.531Z",
            "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6703ac76ea890f0ca5b225eb",
                "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
                "isPro": false,
                "fullname": "Yingqian Min",
                "user": "EliverQ",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.",
            "upvotes": 4,
            "discussionId": "683469f1df7cbb5c08a049b2",
            "ai_summary": "R1-Searcher++, a novel framework, enhances LLMs by adaptively integrating internal and external knowledge through two-stage training, improving retrieval-augmented reasoning efficiency and performance.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "hallucinations",
                "Retrieval-Augmented Generation (RAG)",
                "R1-Searcher++",
                "SFT Cold-start",
                "Reinforcement Learning (RL)",
                "outcome-supervision",
                "reward mechanism",
                "memorization mechanism",
                "retrieval-augmented reasoning"
            ]
        },
        "publishedAt": "2025-05-22T13:58:26.000Z",
        "title": "R1-Searcher++: Incentivizing the Dynamic Knowledge Acquisition of LLMs\n  via Reinforcement Learning",
        "summary": "Large Language Models (LLMs) are powerful but prone to hallucinations due to\nstatic knowledge. Retrieval-Augmented Generation (RAG) helps by injecting\nexternal information, but current methods often are costly, generalize poorly,\nor ignore the internal knowledge of the model. In this paper, we introduce\nR1-Searcher++, a novel framework designed to train LLMs to adaptively leverage\nboth internal and external knowledge sources. R1-Searcher++ employs a two-stage\ntraining strategy: an initial SFT Cold-start phase for preliminary format\nlearning, followed by RL for Dynamic Knowledge Acquisition. The RL stage uses\noutcome-supervision to encourage exploration, incorporates a reward mechanism\nfor internal knowledge utilization, and integrates a memorization mechanism to\ncontinuously assimilate retrieved information, thereby enriching the model's\ninternal knowledge. By leveraging internal knowledge and external search\nengine, the model continuously improves its capabilities, enabling efficient\nretrieval-augmented reasoning. Our experiments demonstrate that R1-Searcher++\noutperforms previous RAG and reasoning methods and achieves efficient\nretrieval. The code is available at\nhttps://github.com/RUCAIBox/R1-Searcher-plus.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17005.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6703ac76ea890f0ca5b225eb",
            "avatarUrl": "/avatars/5f56c49a1940143d47dd484782a4abbf.svg",
            "fullname": "Yingqian Min",
            "name": "EliverQ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22172",
            "authors": [
                {
                    "_id": "6837b720612e76702eb756b1",
                    "name": "Xiang Huang",
                    "hidden": false
                },
                {
                    "_id": "6837b720612e76702eb756b2",
                    "name": "Ting-En Lin",
                    "hidden": false
                },
                {
                    "_id": "6837b720612e76702eb756b3",
                    "name": "Feiteng Fang",
                    "hidden": false
                },
                {
                    "_id": "6837b720612e76702eb756b4",
                    "name": "Yuchuan Wu",
                    "hidden": false
                },
                {
                    "_id": "6837b720612e76702eb756b5",
                    "name": "Hangyu Li",
                    "hidden": false
                },
                {
                    "_id": "6837b720612e76702eb756b6",
                    "name": "Yuzhong Qu",
                    "hidden": false
                },
                {
                    "_id": "6837b720612e76702eb756b7",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6837b720612e76702eb756b8",
                    "name": "Yongbin Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T09:44:27.000Z",
            "submittedOnDailyAt": "2025-05-28T23:54:19.732Z",
            "title": "Reverse Preference Optimization for Complex Instruction Following",
            "submittedOnDailyBy": {
                "_id": "66168d11123f393614f7c908",
                "avatarUrl": "/avatars/754779a228cb2ff31d21eb1221c576de.svg",
                "isPro": false,
                "fullname": "xiang huang",
                "user": "xianghuang",
                "type": "user"
            },
            "summary": "Instruction following (IF) is a critical capability for large language models\n(LLMs). However, handling complex instructions with multiple constraints\nremains challenging. Previous methods typically select preference pairs based\non the number of constraints they satisfy, introducing noise where chosen\nexamples may fail to follow some constraints and rejected examples may excel in\ncertain respects over the chosen ones. To address the challenge of aligning\nwith multiple preferences, we propose a simple yet effective method called\nReverse Preference Optimization (RPO). It mitigates noise in preference pairs\nby dynamically reversing the constraints within the instruction to ensure the\nchosen response is perfect, alleviating the burden of extensive sampling and\nfiltering to collect perfect responses. Besides, reversal also enlarges the gap\nbetween chosen and rejected responses, thereby clarifying the optimization\ndirection and making it more robust to noise. We evaluate RPO on two multi-turn\nIF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over\nthe DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.\nMoreover, RPO scales effectively across model sizes (8B to 70B parameters),\nwith the 70B RPO model surpassing GPT-4o.",
            "upvotes": 3,
            "discussionId": "6837b721612e76702eb756f0"
        },
        "publishedAt": "2025-05-28T05:44:27.000Z",
        "title": "Reverse Preference Optimization for Complex Instruction Following",
        "summary": "Instruction following (IF) is a critical capability for large language models\n(LLMs). However, handling complex instructions with multiple constraints\nremains challenging. Previous methods typically select preference pairs based\non the number of constraints they satisfy, introducing noise where chosen\nexamples may fail to follow some constraints and rejected examples may excel in\ncertain respects over the chosen ones. To address the challenge of aligning\nwith multiple preferences, we propose a simple yet effective method called\nReverse Preference Optimization (RPO). It mitigates noise in preference pairs\nby dynamically reversing the constraints within the instruction to ensure the\nchosen response is perfect, alleviating the burden of extensive sampling and\nfiltering to collect perfect responses. Besides, reversal also enlarges the gap\nbetween chosen and rejected responses, thereby clarifying the optimization\ndirection and making it more robust to noise. We evaluate RPO on two multi-turn\nIF benchmarks, Sysbench and Multi-IF, demonstrating average improvements over\nthe DPO baseline of 4.6 and 2.5 points (on Llama-3.1 8B), respectively.\nMoreover, RPO scales effectively across model sizes (8B to 70B parameters),\nwith the 70B RPO model surpassing GPT-4o.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22172.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66168d11123f393614f7c908",
            "avatarUrl": "/avatars/754779a228cb2ff31d21eb1221c576de.svg",
            "fullname": "xiang huang",
            "name": "xianghuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.19973",
            "authors": [
                {
                    "_id": "6836a33e64f38b5bf439f9be",
                    "user": {
                        "_id": "64d3db80aea0ccb1b4975d95",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
                        "isPro": false,
                        "fullname": "Bilel Cherif",
                        "user": "Neo111x",
                        "type": "user"
                    },
                    "name": "Bilel Cherif",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:56:58.406Z",
                    "hidden": false
                },
                {
                    "_id": "6836a33e64f38b5bf439f9bf",
                    "user": {
                        "_id": "679d3d70663f033e1a728c17",
                        "avatarUrl": "/avatars/4ee5580b614c48e3cbb8e123b0ae01a2.svg",
                        "isPro": false,
                        "fullname": "Tamas Bisztray",
                        "user": "Sh1n1g4m1",
                        "type": "user"
                    },
                    "name": "Tamas Bisztray",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:57:03.816Z",
                    "hidden": false
                },
                {
                    "_id": "6836a33e64f38b5bf439f9c0",
                    "user": {
                        "_id": "66f1a481fa0f7d6823e95149",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66f1a481fa0f7d6823e95149/2_r8tJZSr95KkezLNDRu7.jpeg",
                        "isPro": false,
                        "fullname": "Richard A. Dubniczky",
                        "user": "dubniczky",
                        "type": "user"
                    },
                    "name": "Richard A. Dubniczky",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:57:10.313Z",
                    "hidden": false
                },
                {
                    "_id": "6836a33e64f38b5bf439f9c1",
                    "user": {
                        "_id": "672bb53e7b58518d356e2e18",
                        "avatarUrl": "/avatars/2945ef62c8efcb7da5a6f1f084589448.svg",
                        "isPro": false,
                        "fullname": "aaesha aldahmani",
                        "user": "aaeshaaldahmani",
                        "type": "user"
                    },
                    "name": "Aaesha Aldahmani",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:57:17.034Z",
                    "hidden": false
                },
                {
                    "_id": "6836a33e64f38b5bf439f9c2",
                    "name": "Saeed Alshehhi",
                    "hidden": false
                },
                {
                    "_id": "6836a33e64f38b5bf439f9c3",
                    "user": {
                        "_id": "65ae586052e1b2aae48e01fb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ae586052e1b2aae48e01fb/Ga6PC31XgXlfzCteqInNp.png",
                        "isPro": false,
                        "fullname": "Norbert Tihanyi",
                        "user": "tihanyin",
                        "type": "user"
                    },
                    "name": "Norbert Tihanyi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:57:26.722Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T13:35:37.000Z",
            "submittedOnDailyAt": "2025-05-28T04:17:09.043Z",
            "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in\n  Digital Forensics and Incident Response",
            "submittedOnDailyBy": {
                "_id": "64d3db80aea0ccb1b4975d95",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
                "isPro": false,
                "fullname": "Bilel Cherif",
                "user": "Neo111x",
                "type": "user"
            },
            "summary": "Digital Forensics and Incident Response (DFIR) involves analyzing digital\nevidence to support legal investigations. Large Language Models (LLMs) offer\nnew opportunities in DFIR tasks such as log analysis and memory forensics, but\ntheir susceptibility to errors and hallucinations raises concerns in\nhigh-stakes contexts. Despite growing interest, there is no comprehensive\nbenchmark to evaluate LLMs across both theoretical and practical DFIR domains.\nTo address this gap, we present DFIR-Metric, a benchmark with three components:\n(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice\nquestions sourced from industry-standard certifications and official\ndocumentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing\nmulti-step reasoning and evidence correlation; and (3) Practical Analysis: 500\ndisk and memory forensics cases from the NIST Computer Forensics Tool Testing\nProgram (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their\naccuracy and consistency across trials. We also introduce a new metric, the\nTask Understanding Score (TUS), designed to more effectively evaluate models in\nscenarios where they achieve near-zero accuracy. This benchmark offers a\nrigorous, reproducible foundation for advancing AI in digital forensics. All\nscripts, artifacts, and results are available on the project website at\nhttps://github.com/DFIR-Metric.",
            "upvotes": 3,
            "discussionId": "6836a33f64f38b5bf439f9f2",
            "projectPage": "https://github.com/DFIR-Metric",
            "githubRepo": "https://github.com/DFIR-Metric",
            "ai_summary": "DFIR-Metric evaluates Large Language Models for digital forensics using a comprehensive benchmark with knowledge assessments, realistic forensic challenges, and practical analysis cases, introducing a Task Understanding Score for near-zero accuracy scenarios.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "DFIR-Metric",
                "Knowledge Assessment",
                "Realistic Forensic Challenges",
                "Practical Analysis",
                "Task Understanding Score"
            ]
        },
        "publishedAt": "2025-05-26T09:35:37.000Z",
        "title": "DFIR-Metric: A Benchmark Dataset for Evaluating Large Language Models in\n  Digital Forensics and Incident Response",
        "summary": "Digital Forensics and Incident Response (DFIR) involves analyzing digital\nevidence to support legal investigations. Large Language Models (LLMs) offer\nnew opportunities in DFIR tasks such as log analysis and memory forensics, but\ntheir susceptibility to errors and hallucinations raises concerns in\nhigh-stakes contexts. Despite growing interest, there is no comprehensive\nbenchmark to evaluate LLMs across both theoretical and practical DFIR domains.\nTo address this gap, we present DFIR-Metric, a benchmark with three components:\n(1) Knowledge Assessment: a set of 700 expert-reviewed multiple-choice\nquestions sourced from industry-standard certifications and official\ndocumentation; (2) Realistic Forensic Challenges: 150 CTF-style tasks testing\nmulti-step reasoning and evidence correlation; and (3) Practical Analysis: 500\ndisk and memory forensics cases from the NIST Computer Forensics Tool Testing\nProgram (CFTT). We evaluated 14 LLMs using DFIR-Metric, analyzing both their\naccuracy and consistency across trials. We also introduce a new metric, the\nTask Understanding Score (TUS), designed to more effectively evaluate models in\nscenarios where they achieve near-zero accuracy. This benchmark offers a\nrigorous, reproducible foundation for advancing AI in digital forensics. All\nscripts, artifacts, and results are available on the project website at\nhttps://github.com/DFIR-Metric.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19973.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d3db80aea0ccb1b4975d95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
            "fullname": "Bilel Cherif",
            "name": "Neo111x",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19650",
            "authors": [
                {
                    "_id": "6836c8391314d4ac39aebebb",
                    "user": {
                        "_id": "63835dc85c83390fc7527849",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63835dc85c83390fc7527849/axIViCepzduN3IfXWmj5A.png",
                        "isPro": false,
                        "fullname": "Kong",
                        "user": "friedrichor",
                        "type": "user"
                    },
                    "name": "Fanheng Kong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:41:20.041Z",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebebc",
                    "name": "Jingyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebebd",
                    "name": "Yahui Liu",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebebe",
                    "name": "Hongzhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebebf",
                    "name": "Shi Feng",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebec0",
                    "user": {
                        "_id": "649596a46d1cf21b50ed0ba7",
                        "avatarUrl": "/avatars/47fec77d7067a2fc5fa8088a3b9af0f5.svg",
                        "isPro": false,
                        "fullname": "Yang Xiaocui",
                        "user": "YangXiaocui",
                        "type": "user"
                    },
                    "name": "Xiaocui Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:29:53.979Z",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebec1",
                    "name": "Daling Wang",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebec2",
                    "name": "Yu Tian",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebec3",
                    "name": "Victoria W.",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebec4",
                    "user": {
                        "_id": "67c5945da1661d5fa6f29adb",
                        "avatarUrl": "/avatars/62561f3875c0c251cae949acc38d72dc.svg",
                        "isPro": false,
                        "fullname": "Fuzheng Zhang",
                        "user": "Edrex",
                        "type": "user"
                    },
                    "name": "Fuzheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:30:12.107Z",
                    "hidden": false
                },
                {
                    "_id": "6836c8391314d4ac39aebec5",
                    "user": {
                        "_id": "67c6c570cf87e2d2ebfc81aa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c6c570cf87e2d2ebfc81aa/7qAstZtIT86Uwrz3u_anv.jpeg",
                        "isPro": false,
                        "fullname": "Guorui Zhou",
                        "user": "GuoruiZhou",
                        "type": "user"
                    },
                    "name": "Guorui Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:30:18.800Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T08:09:44.000Z",
            "submittedOnDailyAt": "2025-05-28T07:22:30.002Z",
            "title": "Modality Curation: Building Universal Embeddings for Advanced Multimodal\n  Information Retrieval",
            "submittedOnDailyBy": {
                "_id": "63835dc85c83390fc7527849",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63835dc85c83390fc7527849/axIViCepzduN3IfXWmj5A.png",
                "isPro": false,
                "fullname": "Kong",
                "user": "friedrichor",
                "type": "user"
            },
            "summary": "Multimodal information retrieval (MIR) faces inherent challenges due to the\nheterogeneity of data sources and the complexity of cross-modal alignment.\nWhile previous studies have identified modal gaps in feature spaces, a\nsystematic approach to address these challenges remains unexplored. In this\nwork, we introduce UNITE, a universal framework that tackles these challenges\nthrough two critical yet underexplored aspects: data curation and\nmodality-aware training configurations. Our work provides the first\ncomprehensive analysis of how modality-specific data properties influence\ndownstream task performance across diverse scenarios. Moreover, we propose\nModal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive\nrelationships among the instances of different modalities. Our framework\nachieves state-of-the-art results on multiple multimodal retrieval benchmarks,\noutperforming existing methods by notable margins. Through extensive\nexperiments, we demonstrate that strategic modality curation and tailored\ntraining protocols are pivotal for robust cross-modal representation learning.\nThis work not only advances MIR performance but also provides a foundational\nblueprint for future research in multimodal systems. Our project is available\nat https://friedrichor.github.io/projects/UNITE.",
            "upvotes": 3,
            "discussionId": "6836c8391314d4ac39aebf03",
            "projectPage": "https://friedrichor.github.io/projects/UNITE",
            "githubRepo": "https://github.com/friedrichor/UNITE",
            "ai_summary": "UNITE addresses challenges in multimodal information retrieval through data curation and modality-aware training, achieving state-of-the-art results across benchmarks with Modal-Aware Masked Contrastive Learning.",
            "ai_keywords": [
                "MAMCL",
                "Modal-Aware Masked Contrastive Learning",
                "multimodal information retrieval",
                "modality-specific data properties",
                "cross-modal alignment",
                "cross-modal representation learning"
            ]
        },
        "publishedAt": "2025-05-26T04:09:44.000Z",
        "title": "Modality Curation: Building Universal Embeddings for Advanced Multimodal\n  Information Retrieval",
        "summary": "Multimodal information retrieval (MIR) faces inherent challenges due to the\nheterogeneity of data sources and the complexity of cross-modal alignment.\nWhile previous studies have identified modal gaps in feature spaces, a\nsystematic approach to address these challenges remains unexplored. In this\nwork, we introduce UNITE, a universal framework that tackles these challenges\nthrough two critical yet underexplored aspects: data curation and\nmodality-aware training configurations. Our work provides the first\ncomprehensive analysis of how modality-specific data properties influence\ndownstream task performance across diverse scenarios. Moreover, we propose\nModal-Aware Masked Contrastive Learning (MAMCL) to mitigate the competitive\nrelationships among the instances of different modalities. Our framework\nachieves state-of-the-art results on multiple multimodal retrieval benchmarks,\noutperforming existing methods by notable margins. Through extensive\nexperiments, we demonstrate that strategic modality curation and tailored\ntraining protocols are pivotal for robust cross-modal representation learning.\nThis work not only advances MIR performance but also provides a foundational\nblueprint for future research in multimodal systems. Our project is available\nat https://friedrichor.github.io/projects/UNITE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19650.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63835dc85c83390fc7527849",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63835dc85c83390fc7527849/axIViCepzduN3IfXWmj5A.png",
            "fullname": "Kong",
            "name": "friedrichor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21499",
            "authors": [
                {
                    "_id": "683681c189cf92972059d4e8",
                    "user": {
                        "_id": "63a85367353e10031a8becaa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1Zb_T08yt0jJCHTe-ahGE.png",
                        "isPro": false,
                        "fullname": "NicerWang",
                        "user": "NicerWang",
                        "type": "user"
                    },
                    "name": "Haowei Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:57:25.748Z",
                    "hidden": false
                },
                {
                    "_id": "683681c189cf92972059d4e9",
                    "name": "Junjie Wang",
                    "hidden": false
                },
                {
                    "_id": "683681c189cf92972059d4ea",
                    "user": {
                        "_id": "64c6627d5671d42e0adfad56",
                        "avatarUrl": "/avatars/8b98054b2911b86dcc4856a15306e60f.svg",
                        "isPro": false,
                        "fullname": "jiaxiaojunQAQ",
                        "user": "jiaxiaojunQAQ",
                        "type": "user"
                    },
                    "name": "Xiaojun Jia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:30:39.901Z",
                    "hidden": false
                },
                {
                    "_id": "683681c189cf92972059d4eb",
                    "name": "Rupeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "683681c189cf92972059d4ec",
                    "name": "Mingyang Li",
                    "hidden": false
                },
                {
                    "_id": "683681c189cf92972059d4ed",
                    "name": "Zhe Liu",
                    "hidden": false
                },
                {
                    "_id": "683681c189cf92972059d4ee",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "683681c189cf92972059d4ef",
                    "name": "Qing Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63a85367353e10031a8becaa/kQIRYd68CHrhr5RPT5btB.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63a85367353e10031a8becaa/XOc3pNTpuIg-CX6EG6ZxI.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63a85367353e10031a8becaa/TX8xGG2Ub9HCMijqlX27O.png"
            ],
            "publishedAt": "2025-05-27T17:59:05.000Z",
            "submittedOnDailyAt": "2025-05-28T07:39:29.041Z",
            "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery",
            "submittedOnDailyBy": {
                "_id": "63a85367353e10031a8becaa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1Zb_T08yt0jJCHTe-ahGE.png",
                "isPro": false,
                "fullname": "NicerWang",
                "user": "NicerWang",
                "type": "user"
            },
            "summary": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject.",
            "upvotes": 2,
            "discussionId": "683681c289cf92972059d534",
            "ai_summary": "AdInject is a novel real-world black-box attack method leveraging internet advertising to inject malicious content into vision-language model-based web agents, demonstrating significant vulnerability in web agent security.",
            "ai_keywords": [
                "vision-language model",
                "web agents",
                "adversarial environmental injection attacks",
                "AdInject",
                "internet advertising",
                "black-box agent",
                "static malicious content",
                "user intent",
                "attack success rates"
            ]
        },
        "publishedAt": "2025-05-27T13:59:05.000Z",
        "title": "AdInject: Real-World Black-Box Attacks on Web Agents via Advertising\n  Delivery",
        "summary": "Vision-Language Model (VLM) based Web Agents represent a significant step\ntowards automating complex tasks by simulating human-like interaction with\nwebsites. However, their deployment in uncontrolled web environments introduces\nsignificant security vulnerabilities. Existing research on adversarial\nenvironmental injection attacks often relies on unrealistic assumptions, such\nas direct HTML manipulation, knowledge of user intent, or access to agent model\nparameters, limiting their practical applicability. In this paper, we propose\nAdInject, a novel and real-world black-box attack method that leverages the\ninternet advertising delivery to inject malicious content into the Web Agent's\nenvironment. AdInject operates under a significantly more realistic threat\nmodel than prior work, assuming a black-box agent, static malicious content\nconstraints, and no specific knowledge of user intent. AdInject includes\nstrategies for designing malicious ad content aimed at misleading agents into\nclicking, and a VLM-based ad content optimization technique that infers\npotential user intents from the target website's context and integrates these\nintents into the ad content to make it appear more relevant or critical to the\nagent's task, thus enhancing attack effectiveness. Experimental evaluations\ndemonstrate the effectiveness of AdInject, attack success rates exceeding 60%\nin most scenarios and approaching 100% in certain cases. This strongly\ndemonstrates that prevalent advertising delivery constitutes a potent and\nreal-world vector for environment injection attacks against Web Agents. This\nwork highlights a critical vulnerability in Web Agent security arising from\nreal-world environment manipulation channels, underscoring the urgent need for\ndeveloping robust defense mechanisms against such threats. Our code is\navailable at https://github.com/NicerWang/AdInject.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a85367353e10031a8becaa/kQIRYd68CHrhr5RPT5btB.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63a85367353e10031a8becaa/XOc3pNTpuIg-CX6EG6ZxI.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63a85367353e10031a8becaa/TX8xGG2Ub9HCMijqlX27O.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21499.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a85367353e10031a8becaa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1Zb_T08yt0jJCHTe-ahGE.png",
            "fullname": "NicerWang",
            "name": "NicerWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20286",
            "authors": [
                {
                    "_id": "6835d921b0527cd0fd9bce41",
                    "user": {
                        "_id": "65271df5abd7795aaa1fd86e",
                        "avatarUrl": "/avatars/99e1aabaefcf5e7f438817375cd1ddef.svg",
                        "isPro": false,
                        "fullname": "Jiahao Qiu",
                        "user": "jiahaoq",
                        "type": "user"
                    },
                    "name": "Jiahao Qiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:14:34.766Z",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce42",
                    "name": "Xuan Qi",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce43",
                    "name": "Tongcheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce44",
                    "user": {
                        "_id": "674500b57a76d46e9141af8b",
                        "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
                        "isPro": false,
                        "fullname": "Xinzhe Juan",
                        "user": "ChrisJuan",
                        "type": "user"
                    },
                    "name": "Xinzhe Juan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:44:34.452Z",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce45",
                    "user": {
                        "_id": "66b42f6717f9cb4e8d70afd2",
                        "avatarUrl": "/avatars/3829e8c7044fac861405d1a48c02aab5.svg",
                        "isPro": false,
                        "fullname": "Jiacheng Guo",
                        "user": "gjcjcg",
                        "type": "user"
                    },
                    "name": "Jiacheng Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:44:54.618Z",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce46",
                    "name": "Yifu Lu",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce47",
                    "name": "Yimin Wang",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce48",
                    "user": {
                        "_id": "6657f2041d83f3ee61bf414d",
                        "avatarUrl": "/avatars/e678fbbba2e93536dfc702e8ae629a95.svg",
                        "isPro": false,
                        "fullname": "zixin",
                        "user": "yaozixin",
                        "type": "user"
                    },
                    "name": "Zixin Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:45:30.842Z",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce49",
                    "user": {
                        "_id": "66e2624a436a1798365e4581",
                        "avatarUrl": "/avatars/6c605807d34faa8fb505e135a4b47776.svg",
                        "isPro": false,
                        "fullname": "Qihan Ren",
                        "user": "jasonrqh",
                        "type": "user"
                    },
                    "name": "Qihan Ren",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:45:41.780Z",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce4a",
                    "name": "Xun Jiang",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce4b",
                    "name": "Xing Zhou",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce4c",
                    "user": {
                        "_id": "657fe7a8504da7f6f30a2832",
                        "avatarUrl": "/avatars/65987e3cba449b5d250616510ee11f33.svg",
                        "isPro": false,
                        "fullname": "Dongrui Liu",
                        "user": "Max9803",
                        "type": "user"
                    },
                    "name": "Dongrui Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:46:00.401Z",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce4d",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce4e",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce4f",
                    "user": {
                        "_id": "64a02d1625af3c141c2653da",
                        "avatarUrl": "/avatars/d9a79815188020f795ab7aa64707dfeb.svg",
                        "isPro": false,
                        "fullname": "huangkaixuan",
                        "user": "huangkaixuan",
                        "type": "user"
                    },
                    "name": "Kaixuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:46:55.581Z",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce50",
                    "user": {
                        "_id": "638b13c0c1d591879698f4e2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638b13c0c1d591879698f4e2/X8X4EWMXuzhBpG62wO2xS.jpeg",
                        "isPro": false,
                        "fullname": "Shilong Liu",
                        "user": "ShilongLiu",
                        "type": "user"
                    },
                    "name": "Shilong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:47:10.123Z",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce51",
                    "user": {
                        "_id": "65f906e5c3dbdcae83ff7aac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f906e5c3dbdcae83ff7aac/mdjiVkLDJgJcGLwv0rMe4.jpeg",
                        "isPro": false,
                        "fullname": "Hongru Wang",
                        "user": "Merlin-Hongru",
                        "type": "user"
                    },
                    "name": "Hongru Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:47:27.857Z",
                    "hidden": false
                },
                {
                    "_id": "6835d921b0527cd0fd9bce52",
                    "user": {
                        "_id": "6599415e8c8ac79295e0b5e3",
                        "avatarUrl": "/avatars/85500bc8d2cd51444adcc19b1f8db313.svg",
                        "isPro": false,
                        "fullname": "Mengdi Wang",
                        "user": "Edify-Kd2024",
                        "type": "user"
                    },
                    "name": "Mengdi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:47:58.042Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:58:53.000Z",
            "submittedOnDailyAt": "2025-05-28T11:08:59.890Z",
            "title": "Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal\n  Predefinition and Maximal Self-Evolution",
            "submittedOnDailyBy": {
                "_id": "674500b57a76d46e9141af8b",
                "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
                "isPro": false,
                "fullname": "Xinzhe Juan",
                "user": "ChrisJuan",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have enabled agents to\nautonomously perform complex, open-ended tasks. However, many existing\nframeworks depend heavily on manually predefined tools and workflows, which\nhinder their adaptability, scalability, and generalization across domains. In\nthis work, we introduce Alita--a generalist agent designed with the principle\nof \"Simplicity is the ultimate sophistication,\" enabling scalable agentic\nreasoning through minimal predefinition and maximal self-evolution. For minimal\npredefinition, Alita is equipped with only one component for direct\nproblem-solving, making it much simpler and neater than previous approaches\nthat relied heavily on hand-crafted, elaborate tools and workflows. This clean\ndesign enhances its potential to generalize to challenging questions, without\nbeing limited by tools. For Maximal self-evolution, we enable the creativity of\nAlita by providing a suite of general-purpose components to autonomously\nconstruct, refine, and reuse external capabilities by generating task-related\nmodel context protocols (MCPs) from open source, which contributes to scalable\nagentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3\naccuracy, which is top-ranking among general-purpose agents, on the GAIA\nbenchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on\nMathvista and PathVQA, outperforming many agent systems with far greater\ncomplexity. More details will be updated at\nhttps://github.com/CharlesQ9/Alita{https://github.com/CharlesQ9/Alita}.",
            "upvotes": 2,
            "discussionId": "6835d922b0527cd0fd9bce79",
            "ai_summary": "Alita, a simplicity-driven generalist agent, achieves high performance across multiple benchmarks through minimal predefinition and self-evolution using task-related model context protocols.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "generalist agent",
                "scalable agentic reasoning",
                "minimal predefinition",
                "self-evolution",
                "task-related model context protocols",
                "MCPs",
                "GAIA benchmark",
                "Mathvista",
                "PathVQA"
            ]
        },
        "publishedAt": "2025-05-26T13:58:53.000Z",
        "title": "Alita: Generalist Agent Enabling Scalable Agentic Reasoning with Minimal\n  Predefinition and Maximal Self-Evolution",
        "summary": "Recent advances in large language models (LLMs) have enabled agents to\nautonomously perform complex, open-ended tasks. However, many existing\nframeworks depend heavily on manually predefined tools and workflows, which\nhinder their adaptability, scalability, and generalization across domains. In\nthis work, we introduce Alita--a generalist agent designed with the principle\nof \"Simplicity is the ultimate sophistication,\" enabling scalable agentic\nreasoning through minimal predefinition and maximal self-evolution. For minimal\npredefinition, Alita is equipped with only one component for direct\nproblem-solving, making it much simpler and neater than previous approaches\nthat relied heavily on hand-crafted, elaborate tools and workflows. This clean\ndesign enhances its potential to generalize to challenging questions, without\nbeing limited by tools. For Maximal self-evolution, we enable the creativity of\nAlita by providing a suite of general-purpose components to autonomously\nconstruct, refine, and reuse external capabilities by generating task-related\nmodel context protocols (MCPs) from open source, which contributes to scalable\nagentic reasoning. Notably, Alita achieves 75.15% pass@1 and 87.27% pass@3\naccuracy, which is top-ranking among general-purpose agents, on the GAIA\nbenchmark validation dataset, 74.00% and 52.00% pass@1, respectively, on\nMathvista and PathVQA, outperforming many agent systems with far greater\ncomplexity. More details will be updated at\nhttps://github.com/CharlesQ9/Alita{https://github.com/CharlesQ9/Alita}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20286.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "674500b57a76d46e9141af8b",
            "avatarUrl": "/avatars/e70243c31e2ac9f000542e8504e65b51.svg",
            "fullname": "Xinzhe Juan",
            "name": "ChrisJuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20279",
            "authors": [
                {
                    "_id": "6837611a3216d45c6288350c",
                    "name": "Zhiwen Fan",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c6288350d",
                    "name": "Jian Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c6288350e",
                    "name": "Renjie Li",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c6288350f",
                    "name": "Junge Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883510",
                    "name": "Runjin Chen",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883511",
                    "name": "Hezhen Hu",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883512",
                    "name": "Kevin Wang",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883513",
                    "name": "Huaizhi Qu",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883514",
                    "name": "Dilin Wang",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883515",
                    "name": "Zhicheng Yan",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883516",
                    "name": "Hongyu Xu",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883517",
                    "name": "Justin Theiss",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883518",
                    "name": "Tianlong Chen",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c62883519",
                    "name": "Jiachen Li",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c6288351a",
                    "name": "Zhengzhong Tu",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c6288351b",
                    "name": "Zhangyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6837611a3216d45c6288351c",
                    "name": "Rakesh Ranjan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:56:30.000Z",
            "submittedOnDailyAt": "2025-05-28T17:47:15.811Z",
            "title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D\n  Reconstruction",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and\nvideos has motivated extending these models to understand 3D scenes, aiming for\nhuman-like visual-spatial intelligence. Nevertheless, achieving deep spatial\nunderstanding comparable to human capabilities poses significant challenges in\nmodel encoding and data acquisition. Existing methods frequently depend on\nexternal depth sensors for geometry capture or utilize off-the-shelf algorithms\nfor pre-constructing 3D maps, thereby limiting their scalability, especially\nwith prevalent monocular video inputs and for time-sensitive applications. In\nthis work, we introduce VLM-3R, a unified framework for Vision-Language Models\n(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes\nmonocular video frames by employing a geometry encoder to derive implicit 3D\ntokens that represent spatial understanding. Leveraging our Spatial-Visual-View\nFusion and over 200K curated 3D reconstructive instruction tuning\nquestion-answer (QA) pairs, VLM-3R effectively aligns real-world spatial\ncontext with language instructions. This enables monocular 3D spatial\nassistance and embodied reasoning. To facilitate the evaluation of temporal\nreasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,\nfeaturing over 138.6K QA pairs across five distinct tasks focused on evolving\nspatial relationships. Extensive experiments demonstrate that our model,\nVLM-3R, not only facilitates robust visual-spatial reasoning but also enables\nthe understanding of temporal 3D context changes, excelling in both accuracy\nand scalability.",
            "upvotes": 2,
            "discussionId": "6837611b3216d45c62883568",
            "ai_summary": "VLM-3R, a framework for Vision-Language Models, incorporates 3D reconstructive instruction tuning to process monocular video frames and perform embodied reasoning with robust visual-spatial and temporal contextual understanding.",
            "ai_keywords": [
                "Large Multimodal Models",
                "Vision-Language Models",
                "3D Reconstructive instruction tuning",
                "geometry encoder",
                "implicit 3D tokens",
                "Spatial-Visual-View Fusion",
                "Vision-Spatial-Temporal Intelligence benchmark",
                "embodied reasoning"
            ]
        },
        "publishedAt": "2025-05-26T13:56:30.000Z",
        "title": "VLM-3R: Vision-Language Models Augmented with Instruction-Aligned 3D\n  Reconstruction",
        "summary": "The rapid advancement of Large Multimodal Models (LMMs) for 2D images and\nvideos has motivated extending these models to understand 3D scenes, aiming for\nhuman-like visual-spatial intelligence. Nevertheless, achieving deep spatial\nunderstanding comparable to human capabilities poses significant challenges in\nmodel encoding and data acquisition. Existing methods frequently depend on\nexternal depth sensors for geometry capture or utilize off-the-shelf algorithms\nfor pre-constructing 3D maps, thereby limiting their scalability, especially\nwith prevalent monocular video inputs and for time-sensitive applications. In\nthis work, we introduce VLM-3R, a unified framework for Vision-Language Models\n(VLMs) that incorporates 3D Reconstructive instruction tuning. VLM-3R processes\nmonocular video frames by employing a geometry encoder to derive implicit 3D\ntokens that represent spatial understanding. Leveraging our Spatial-Visual-View\nFusion and over 200K curated 3D reconstructive instruction tuning\nquestion-answer (QA) pairs, VLM-3R effectively aligns real-world spatial\ncontext with language instructions. This enables monocular 3D spatial\nassistance and embodied reasoning. To facilitate the evaluation of temporal\nreasoning, we introduce the Vision-Spatial-Temporal Intelligence benchmark,\nfeaturing over 138.6K QA pairs across five distinct tasks focused on evolving\nspatial relationships. Extensive experiments demonstrate that our model,\nVLM-3R, not only facilitates robust visual-spatial reasoning but also enables\nthe understanding of temporal 3D context changes, excelling in both accuracy\nand scalability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20279.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 873
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.19094",
            "authors": [
                {
                    "_id": "683524e492151d8beb27f7eb",
                    "user": {
                        "_id": "64afb1759f6af3d885e79596",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64afb1759f6af3d885e79596/U0C_zSEhpY4HKRZbL3EYr.png",
                        "isPro": false,
                        "fullname": "Chuming Shen",
                        "user": "justairr",
                        "type": "user"
                    },
                    "name": "Chuming Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-27T14:44:01.542Z",
                    "hidden": false
                },
                {
                    "_id": "683524e492151d8beb27f7ec",
                    "name": "Wei Wei",
                    "hidden": false
                },
                {
                    "_id": "683524e492151d8beb27f7ed",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "683524e492151d8beb27f7ee",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T11:11:06.000Z",
            "submittedOnDailyAt": "2025-05-28T15:24:24.006Z",
            "title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and\n  Verifiable Rewards",
            "submittedOnDailyBy": {
                "_id": "64afb1759f6af3d885e79596",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64afb1759f6af3d885e79596/U0C_zSEhpY4HKRZbL3EYr.png",
                "isPro": false,
                "fullname": "Chuming Shen",
                "user": "justairr",
                "type": "user"
            },
            "summary": "DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text\ndomain through stable reinforcement learning (RL). Recently, in the multimodal\ndomain, works have begun to directly apply RL to generate R1-like free-form\nreasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks\nshare an intrinsically different nature from textual tasks, which heavily rely\non the understanding of the input image to solve the problem. Therefore, such\nfree-form reasoning faces two critical limitations in the VQA task: (1)\nExtended reasoning chains diffuse visual focus away from task-critical regions,\ndegrading answer accuracy. (2) Unverifiable intermediate steps amplify\npolicy-gradient variance and computational costs overhead. To address these\nissues, in this paper, we introduce SATORI (Spatially\nAnchored Task Optimization with\nReInforcement Learning), which decomposes VQA into three\nverifiable stages, including global image captioning, region localization, and\nanswer prediction, each supplying explicit reward signals. Furthermore, we also\nintroduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and\nbounding-boxes to facilitate training. Experiments demonstrate consistent\nperformance improvements across seven VQA benchmarks, achieving up to 15.7%\nimprovement in accuracy in accuracy compared to the R1-like baseline. Our\nanalysis of the attention map confirms enhanced focus on critical regions,\nwhich brings improvements in accuracy. Our code is available at\nhttps://github.com/justairr/SATORI-R1.",
            "upvotes": 2,
            "discussionId": "683524e592151d8beb27f847",
            "ai_summary": "SATORI decomposes VQA into verifiable stages with explicit rewards to enhance focus on critical regions and reduce policy-gradient variance, achieving significant performance improvements.",
            "ai_keywords": [
                "stable reinforcement learning",
                "multimodal domain",
                "Visual Question Answering (VQA)",
                "extended reasoning chains",
                "policy-gradient variance",
                "global image captioning",
                "region localization",
                "answer prediction",
                "attention map",
                "VQA-Verify"
            ]
        },
        "publishedAt": "2025-05-25T07:11:06.000Z",
        "title": "SATORI-R1: Incentivizing Multimodal Reasoning with Spatial Grounding and\n  Verifiable Rewards",
        "summary": "DeepSeek-R1 has demonstrated powerful reasoning capabilities in the text\ndomain through stable reinforcement learning (RL). Recently, in the multimodal\ndomain, works have begun to directly apply RL to generate R1-like free-form\nreasoning for Visual Question Answering (VQA) tasks. However, multimodal tasks\nshare an intrinsically different nature from textual tasks, which heavily rely\non the understanding of the input image to solve the problem. Therefore, such\nfree-form reasoning faces two critical limitations in the VQA task: (1)\nExtended reasoning chains diffuse visual focus away from task-critical regions,\ndegrading answer accuracy. (2) Unverifiable intermediate steps amplify\npolicy-gradient variance and computational costs overhead. To address these\nissues, in this paper, we introduce SATORI (Spatially\nAnchored Task Optimization with\nReInforcement Learning), which decomposes VQA into three\nverifiable stages, including global image captioning, region localization, and\nanswer prediction, each supplying explicit reward signals. Furthermore, we also\nintroduce VQA-Verify, a 12k dataset annotated with answer-aligned captions and\nbounding-boxes to facilitate training. Experiments demonstrate consistent\nperformance improvements across seven VQA benchmarks, achieving up to 15.7%\nimprovement in accuracy in accuracy compared to the R1-like baseline. Our\nanalysis of the attention map confirms enhanced focus on critical regions,\nwhich brings improvements in accuracy. Our code is available at\nhttps://github.com/justairr/SATORI-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19094.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64afb1759f6af3d885e79596",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64afb1759f6af3d885e79596/U0C_zSEhpY4HKRZbL3EYr.png",
            "fullname": "Chuming Shen",
            "name": "justairr",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17908",
            "authors": [
                {
                    "_id": "6836ab69f5ad887c90517254",
                    "user": {
                        "_id": "6476025bfd4c874fa84b6a28",
                        "avatarUrl": "/avatars/b5ca2039731fec9d8e95d6d15b7719f5.svg",
                        "isPro": false,
                        "fullname": "litao Guo",
                        "user": "Riversideli",
                        "type": "user"
                    },
                    "name": "Litao Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:43:50.189Z",
                    "hidden": false
                },
                {
                    "_id": "6836ab69f5ad887c90517255",
                    "user": {
                        "_id": "64b4ab62eec33e27dcd733b5",
                        "avatarUrl": "/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg",
                        "isPro": false,
                        "fullname": "Xinli XU",
                        "user": "Xxlbigbrother",
                        "type": "user"
                    },
                    "name": "Xinli Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:43:41.201Z",
                    "hidden": false
                },
                {
                    "_id": "6836ab69f5ad887c90517256",
                    "user": {
                        "_id": "6340333a733f9eef46913dc8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6340333a733f9eef46913dc8/BwhKgkqbuuxWsJv2s_JEH.png",
                        "isPro": false,
                        "fullname": "luozhou wang",
                        "user": "wileewang",
                        "type": "user"
                    },
                    "name": "Luozhou Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:43:35.119Z",
                    "hidden": false
                },
                {
                    "_id": "6836ab69f5ad887c90517257",
                    "user": {
                        "_id": "6332e2689bf698ce68a22e8c",
                        "avatarUrl": "/avatars/c1922acfda2e6d2fe7b03194a404eb10.svg",
                        "isPro": true,
                        "fullname": "JIANTAO LIN",
                        "user": "LTT",
                        "type": "user"
                    },
                    "name": "Jiantao Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:43:28.714Z",
                    "hidden": false
                },
                {
                    "_id": "6836ab69f5ad887c90517258",
                    "name": "Jinsong Zhou",
                    "hidden": false
                },
                {
                    "_id": "6836ab69f5ad887c90517259",
                    "name": "Zixin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6836ab69f5ad887c9051725a",
                    "name": "Bolan Su",
                    "hidden": false
                },
                {
                    "_id": "6836ab69f5ad887c9051725b",
                    "user": {
                        "_id": "655cba1d87b67834000590e8",
                        "avatarUrl": "/avatars/3bd43b7c9351f65b8f38f4c8237a0146.svg",
                        "isPro": false,
                        "fullname": "Yingcong Chen",
                        "user": "yingcongchen",
                        "type": "user"
                    },
                    "name": "Ying-Cong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:42:54.177Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T13:53:03.000Z",
            "submittedOnDailyAt": "2025-05-28T04:52:42.710Z",
            "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and\n  Reactive Feedback",
            "submittedOnDailyBy": {
                "_id": "64b4ab62eec33e27dcd733b5",
                "avatarUrl": "/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg",
                "isPro": false,
                "fullname": "Xinli XU",
                "user": "Xxlbigbrother",
                "type": "user"
            },
            "summary": "With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind",
            "upvotes": 2,
            "discussionId": "6836ab6af5ad887c905172c2",
            "projectPage": "https://litaoguo.github.io/ComfyMind.github.io/",
            "githubRepo": "https://github.com/LitaoGuo/ComfyMind",
            "ai_summary": "ComfyMind, a collaborative AI system built on ComfyUI, enhances generative workflows with a Semantic Workflow Interface and Search Tree Planning mechanism, outperforming existing open-source systems across generation, editing, and reasoning tasks.",
            "ai_keywords": [
                "Semantic Workflow Interface",
                "Search Tree Planning mechanism",
                "generative models",
                "general-purpose generation",
                "generative workflows"
            ]
        },
        "publishedAt": "2025-05-23T09:53:03.000Z",
        "title": "ComfyMind: Toward General-Purpose Generation via Tree-Based Planning and\n  Reactive Feedback",
        "summary": "With the rapid advancement of generative models, general-purpose generation\nhas gained increasing attention as a promising approach to unify diverse tasks\nacross modalities within a single system. Despite this progress, existing\nopen-source frameworks often remain fragile and struggle to support complex\nreal-world applications due to the lack of structured workflow planning and\nexecution-level feedback. To address these limitations, we present ComfyMind, a\ncollaborative AI system designed to enable robust and scalable general-purpose\ngeneration, built on the ComfyUI platform. ComfyMind introduces two core\ninnovations: Semantic Workflow Interface (SWI) that abstracts low-level node\ngraphs into callable functional modules described in natural language, enabling\nhigh-level composition and reducing structural errors; Search Tree Planning\nmechanism with localized feedback execution, which models generation as a\nhierarchical decision process and allows adaptive correction at each stage.\nTogether, these components improve the stability and flexibility of complex\ngenerative workflows. We evaluate ComfyMind on three public benchmarks:\nComfyBench, GenEval, and Reason-Edit, which span generation, editing, and\nreasoning tasks. Results show that ComfyMind consistently outperforms existing\nopen-source baselines and achieves performance comparable to GPT-Image-1.\nComfyMind paves a promising path for the development of open-source\ngeneral-purpose generative AI systems. Project page:\nhttps://github.com/LitaoGuo/ComfyMind",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17908.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64b4ab62eec33e27dcd733b5",
            "avatarUrl": "/avatars/0a9bf220c9a5efe7279f9b287b087d36.svg",
            "fullname": "Xinli XU",
            "name": "Xxlbigbrother",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16673",
            "authors": [
                {
                    "_id": "6836164664810fd39f82cf6e",
                    "user": {
                        "_id": "6590e03454f8826173ed5ee6",
                        "avatarUrl": "/avatars/f5e59d3e58c28a99f2ff39267ca51cdb.svg",
                        "isPro": false,
                        "fullname": "Huanjin Yao",
                        "user": "HuanjinYao",
                        "type": "user"
                    },
                    "name": "Huanjin Yao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:36:18.475Z",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf6f",
                    "user": {
                        "_id": "66841df7f0236757f437be00",
                        "avatarUrl": "/avatars/930d58451152bfd90a0e5549552790c0.svg",
                        "isPro": false,
                        "fullname": "Qixiang Yin",
                        "user": "Lucasqixiang",
                        "type": "user"
                    },
                    "name": "Qixiang Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:36:25.597Z",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf70",
                    "name": "Jingyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf71",
                    "name": "Min Yang",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf72",
                    "name": "Yibo Wang",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf73",
                    "name": "Wenhao Wu",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf74",
                    "name": "Fei Su",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf75",
                    "name": "Li Shen",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf76",
                    "name": "Minghui Qiu",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf77",
                    "name": "Dacheng Tao",
                    "hidden": false
                },
                {
                    "_id": "6836164664810fd39f82cf78",
                    "user": {
                        "_id": "67f3a9ec327c0599a0e7c2f0",
                        "avatarUrl": "/avatars/41674aad656c49487cb18fdd2a44c07c.svg",
                        "isPro": false,
                        "fullname": "Huang jiaxing",
                        "user": "WisteriaWilson",
                        "type": "user"
                    },
                    "name": "Jiaxing Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:37:33.188Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T13:39:32.000Z",
            "submittedOnDailyAt": "2025-05-28T04:19:38.170Z",
            "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large\n  Language Models via Share-GRPO",
            "submittedOnDailyBy": {
                "_id": "6590e03454f8826173ed5ee6",
                "avatarUrl": "/avatars/f5e59d3e58c28a99f2ff39267ca51cdb.svg",
                "isPro": false,
                "fullname": "Huanjin Yao",
                "user": "HuanjinYao",
                "type": "user"
            },
            "summary": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL.",
            "upvotes": 2,
            "discussionId": "6836164764810fd39f82cfba",
            "ai_summary": "Share-GRPO, a novel reinforcement learning approach, enhances Multimodal Large Language Models by expanding the question space, sharing diverse reasoning trajectories, and hierarchical advantage computation.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "reinforcement learning",
                "sparse reward",
                "advantage vanishing",
                "Share-GRPO",
                "data transformation techniques",
                "reasoning trajectories",
                "question space",
                "hierarchical advantage computation"
            ]
        },
        "publishedAt": "2025-05-22T09:39:32.000Z",
        "title": "R1-ShareVL: Incentivizing Reasoning Capability of Multimodal Large\n  Language Models via Share-GRPO",
        "summary": "In this work, we aim to incentivize the reasoning ability of Multimodal Large\nLanguage Models (MLLMs) via reinforcement learning (RL) and develop an\neffective approach that mitigates the sparse reward and advantage vanishing\nissues during RL. To this end, we propose Share-GRPO, a novel RL approach that\ntackle these issues by exploring and sharing diverse reasoning trajectories\nover expanded question space. Specifically, Share-GRPO first expands the\nquestion space for a given question via data transformation techniques, and\nthen encourages MLLM to effectively explore diverse reasoning trajectories over\nthe expanded question space and shares the discovered reasoning trajectories\nacross the expanded questions during RL. In addition, Share-GRPO also shares\nreward information during advantage computation, which estimates solution\nadvantages hierarchically across and within question variants, allowing more\naccurate estimation of relative advantages and improving the stability of\npolicy training. Extensive evaluations over six widely-used reasoning\nbenchmarks showcase the superior performance of our method. Code will be\navailable at https://github.com/HJYao00/R1-ShareVL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16673.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6590e03454f8826173ed5ee6",
            "avatarUrl": "/avatars/f5e59d3e58c28a99f2ff39267ca51cdb.svg",
            "fullname": "Huanjin Yao",
            "name": "HuanjinYao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22633",
            "authors": [
                {
                    "_id": "6837b67765209bfdd650594e",
                    "name": "Yida Xue",
                    "hidden": false
                },
                {
                    "_id": "6837b67765209bfdd650594f",
                    "name": "Zhen Bi",
                    "hidden": false
                },
                {
                    "_id": "6837b67765209bfdd6505950",
                    "name": "Jinnan Yang",
                    "hidden": false
                },
                {
                    "_id": "6837b67765209bfdd6505951",
                    "name": "Jungang Lou",
                    "hidden": false
                },
                {
                    "_id": "6837b67765209bfdd6505952",
                    "name": "Huajun Chen",
                    "hidden": false
                },
                {
                    "_id": "6837b67765209bfdd6505953",
                    "name": "Ningyu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:50:21.000Z",
            "submittedOnDailyAt": "2025-05-28T23:51:16.375Z",
            "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence.",
            "upvotes": 1,
            "discussionId": "6837b67865209bfdd6505997"
        },
        "publishedAt": "2025-05-28T13:50:21.000Z",
        "title": "Spatial Knowledge Graph-Guided Multimodal Synthesis",
        "summary": "Recent advances in multimodal large language models (MLLMs) have\nsignificantly enhanced their capabilities; however, their spatial perception\nabilities remain a notable limitation. To address this challenge, multimodal\ndata synthesis offers a promising solution. Yet, ensuring that synthesized data\nadhere to spatial common sense is a non-trivial task. In this work, we\nintroduce SKG2Data, a novel multimodal synthesis approach guided by spatial\nknowledge graphs, grounded in the concept of knowledge-to-data generation.\nSKG2Data automatically constructs a Spatial Knowledge Graph (SKG) to emulate\nhuman-like perception of spatial directions and distances, which is\nsubsequently utilized to guide multimodal data synthesis. Extensive experiments\ndemonstrate that data synthesized from diverse types of spatial knowledge,\nincluding direction and distance, not only enhance the spatial perception and\nreasoning abilities of MLLMs but also exhibit strong generalization\ncapabilities. We hope that the idea of knowledge-based data synthesis can\nadvance the development of spatial intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22633.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22096",
            "authors": [
                {
                    "_id": "6837b7fdb7eacc2ac90c6acb",
                    "name": "Jinheon Baek",
                    "hidden": false
                },
                {
                    "_id": "6837b7fdb7eacc2ac90c6acc",
                    "name": "Horst Samulowitz",
                    "hidden": false
                },
                {
                    "_id": "6837b7fdb7eacc2ac90c6acd",
                    "name": "Oktie Hassanzadeh",
                    "hidden": false
                },
                {
                    "_id": "6837b7fdb7eacc2ac90c6ace",
                    "name": "Dharmashankar Subramanian",
                    "hidden": false
                },
                {
                    "_id": "6837b7fdb7eacc2ac90c6acf",
                    "name": "Sola Shirai",
                    "hidden": false
                },
                {
                    "_id": "6837b7fdb7eacc2ac90c6ad0",
                    "name": "Alfio Gliozzo",
                    "hidden": false
                },
                {
                    "_id": "6837b7fdb7eacc2ac90c6ad1",
                    "name": "Debarun Bhattacharjya",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T08:17:58.000Z",
            "submittedOnDailyAt": "2025-05-28T23:58:15.706Z",
            "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL",
            "submittedOnDailyBy": {
                "_id": "63036b6c5c70c21d0ea79d48",
                "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
                "isPro": false,
                "fullname": "Jinheon Baek",
                "user": "jinheon",
                "type": "user"
            },
            "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially.",
            "upvotes": 1,
            "discussionId": "6837b7feb7eacc2ac90c6af5"
        },
        "publishedAt": "2025-05-28T04:17:58.000Z",
        "title": "Knowledge Base Construction for Knowledge-Augmented Text-to-SQL",
        "summary": "Text-to-SQL aims to translate natural language queries into SQL statements,\nwhich is practical as it enables anyone to easily retrieve the desired\ninformation from databases. Recently, many existing approaches tackle this\nproblem with Large Language Models (LLMs), leveraging their strong capability\nin understanding user queries and generating corresponding SQL code. Yet, the\nparametric knowledge in LLMs might be limited to covering all the diverse and\ndomain-specific queries that require grounding in various database schemas,\nwhich makes generated SQLs less accurate oftentimes. To tackle this, we propose\nconstructing the knowledge base for text-to-SQL, a foundational source of\nknowledge, from which we retrieve and generate the necessary knowledge for\ngiven queries. In particular, unlike existing approaches that either manually\nannotate knowledge or generate only a few pieces of knowledge for each query,\nour knowledge base is comprehensive, which is constructed based on a\ncombination of all the available questions and their associated database\nschemas along with their relevant knowledge, and can be reused for unseen\ndatabases from different datasets and domains. We validate our approach on\nmultiple text-to-SQL datasets, considering both the overlapping and\nnon-overlapping database scenarios, where it outperforms relevant baselines\nsubstantially.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22096.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63036b6c5c70c21d0ea79d48",
            "avatarUrl": "/avatars/a7eb03f5cbd4eaa09fe807bbed8bc0f7.svg",
            "fullname": "Jinheon Baek",
            "name": "jinheon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21062",
            "authors": [
                {
                    "_id": "6836dbe003f1a07f6f9fe0db",
                    "user": {
                        "_id": "65abccc9fd4261f5313f1c34",
                        "avatarUrl": "/avatars/b4f09703e1fcd83ab3fc31a116dccde9.svg",
                        "isPro": false,
                        "fullname": "Davide",
                        "user": "davidelobba",
                        "type": "user"
                    },
                    "name": "Davide Lobba",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-28T09:52:31.885Z",
                    "hidden": false
                },
                {
                    "_id": "6836dbe003f1a07f6f9fe0dc",
                    "name": "Fulvio Sanguigni",
                    "hidden": false
                },
                {
                    "_id": "6836dbe003f1a07f6f9fe0dd",
                    "name": "Bin Ren",
                    "hidden": false
                },
                {
                    "_id": "6836dbe003f1a07f6f9fe0de",
                    "name": "Marcella Cornia",
                    "hidden": false
                },
                {
                    "_id": "6836dbe003f1a07f6f9fe0df",
                    "name": "Rita Cucchiara",
                    "hidden": false
                },
                {
                    "_id": "6836dbe003f1a07f6f9fe0e0",
                    "name": "Nicu Sebe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T11:47:51.000Z",
            "submittedOnDailyAt": "2025-05-28T22:08:24.244Z",
            "title": "Inverse Virtual Try-On: Generating Multi-Category Product-Style Images\n  from Clothed Individuals",
            "submittedOnDailyBy": {
                "_id": "65abccc9fd4261f5313f1c34",
                "avatarUrl": "/avatars/b4f09703e1fcd83ab3fc31a116dccde9.svg",
                "isPro": false,
                "fullname": "Davide",
                "user": "davidelobba",
                "type": "user"
            },
            "summary": "While virtual try-on (VTON) systems aim to render a garment onto a target\nperson image, this paper tackles the novel task of virtual try-off (VTOFF),\nwhich addresses the inverse problem: generating standardized product images of\ngarments from real-world photos of clothed individuals. Unlike VTON, which must\nresolve diverse pose and style variations, VTOFF benefits from a consistent and\nwell-defined output format -- typically a flat, lay-down-style representation\nof the garment -- making it a promising tool for data generation and dataset\nenhancement. However, existing VTOFF approaches face two major limitations: (i)\ndifficulty in disentangling garment features from occlusions and complex poses,\noften leading to visual artifacts, and (ii) restricted applicability to\nsingle-category garments (e.g., upper-body clothes only), limiting\ngeneralization. To address these challenges, we present Text-Enhanced\nMUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a\ndual DiT-based backbone with a modified multimodal attention mechanism for\nrobust garment feature extraction. Our architecture is designed to receive\ngarment information from multiple modalities like images, text, and masks to\nwork in a multi-category setting. Finally, we propose an additional alignment\nmodule to further refine the generated visual details. Experiments on VITON-HD\nand Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the\nVTOFF task, significantly improving both visual quality and fidelity to the\ntarget garments.",
            "upvotes": 1,
            "discussionId": "6836dbe303f1a07f6f9fe1c5",
            "projectPage": "https://temu-vtoff-page.github.io/",
            "githubRepo": "https://github.com/davidelobba/TEMU-VTOFF",
            "ai_summary": "TEMU-VTOFF, a novel multi-modal architecture using DiT and multimodal attention, achieves state-of-the-art performance on virtual try-off tasks by disentangling garment features and handling multiple garment categories.",
            "ai_keywords": [
                "DiT",
                "multimodal attention mechanism",
                "garment feature extraction",
                "multi-category virtual try-off",
                "alignment module"
            ]
        },
        "publishedAt": "2025-05-27T07:47:51.000Z",
        "title": "Inverse Virtual Try-On: Generating Multi-Category Product-Style Images\n  from Clothed Individuals",
        "summary": "While virtual try-on (VTON) systems aim to render a garment onto a target\nperson image, this paper tackles the novel task of virtual try-off (VTOFF),\nwhich addresses the inverse problem: generating standardized product images of\ngarments from real-world photos of clothed individuals. Unlike VTON, which must\nresolve diverse pose and style variations, VTOFF benefits from a consistent and\nwell-defined output format -- typically a flat, lay-down-style representation\nof the garment -- making it a promising tool for data generation and dataset\nenhancement. However, existing VTOFF approaches face two major limitations: (i)\ndifficulty in disentangling garment features from occlusions and complex poses,\noften leading to visual artifacts, and (ii) restricted applicability to\nsingle-category garments (e.g., upper-body clothes only), limiting\ngeneralization. To address these challenges, we present Text-Enhanced\nMUlti-category Virtual Try-Off (TEMU-VTOFF), a novel architecture featuring a\ndual DiT-based backbone with a modified multimodal attention mechanism for\nrobust garment feature extraction. Our architecture is designed to receive\ngarment information from multiple modalities like images, text, and masks to\nwork in a multi-category setting. Finally, we propose an additional alignment\nmodule to further refine the generated visual details. Experiments on VITON-HD\nand Dress Code datasets show that TEMU-VTOFF sets a new state-of-the-art on the\nVTOFF task, significantly improving both visual quality and fidelity to the\ntarget garments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21062.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65abccc9fd4261f5313f1c34",
            "avatarUrl": "/avatars/b4f09703e1fcd83ab3fc31a116dccde9.svg",
            "fullname": "Davide",
            "name": "davidelobba",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20162",
            "authors": [
                {
                    "_id": "68371955b0224460143a69bf",
                    "user": {
                        "_id": "645410c50b50e97d3722f1d2",
                        "avatarUrl": "/avatars/e8e079981316b26dbf0f4ada6dcdc8c6.svg",
                        "isPro": false,
                        "fullname": "Alexander Panfilov",
                        "user": "kotekjedi",
                        "type": "user"
                    },
                    "name": "Alexander Panfilov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:13:54.321Z",
                    "hidden": false
                },
                {
                    "_id": "68371955b0224460143a69c0",
                    "name": "Paul Kassianik",
                    "hidden": false
                },
                {
                    "_id": "68371955b0224460143a69c1",
                    "name": "Maksym Andriushchenko",
                    "hidden": false
                },
                {
                    "_id": "68371955b0224460143a69c2",
                    "name": "Jonas Geiping",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T16:05:41.000Z",
            "submittedOnDailyAt": "2025-05-28T12:46:28.427Z",
            "title": "Capability-Based Scaling Laws for LLM Red-Teaming",
            "submittedOnDailyBy": {
                "_id": "645410c50b50e97d3722f1d2",
                "avatarUrl": "/avatars/e8e079981316b26dbf0f4ada6dcdc8c6.svg",
                "isPro": false,
                "fullname": "Alexander Panfilov",
                "user": "kotekjedi",
                "type": "user"
            },
            "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers.",
            "upvotes": 1,
            "discussionId": "68371956b0224460143a6a15",
            "ai_summary": "Red-teaming with large language models reveals that attack success drops sharply when the target model's capabilities exceed the attacker's, highlighting the need for new strategies to assess and mitigate future risks.",
            "ai_keywords": [
                "large language models",
                "red-teaming",
                "prompt-engineering",
                "LLM-based jailbreak attacks",
                "MMLU-Pro benchmark",
                "jailbreaking scaling law"
            ]
        },
        "publishedAt": "2025-05-26T12:05:41.000Z",
        "title": "Capability-Based Scaling Laws for LLM Red-Teaming",
        "summary": "As large language models grow in capability and agency, identifying\nvulnerabilities through red-teaming becomes vital for safe deployment. However,\ntraditional prompt-engineering approaches may prove ineffective once\nred-teaming turns into a weak-to-strong problem, where target models surpass\nred-teamers in capabilities. To study this shift, we frame red-teaming through\nthe lens of the capability gap between attacker and target. We evaluate more\nthan 500 attacker-target pairs using LLM-based jailbreak attacks that mimic\nhuman red-teamers across diverse families, sizes, and capability levels. Three\nstrong trends emerge: (i) more capable models are better attackers, (ii) attack\nsuccess drops sharply once the target's capability exceeds the attacker's, and\n(iii) attack success rates correlate with high performance on social science\nsplits of the MMLU-Pro benchmark. From these trends, we derive a jailbreaking\nscaling law that predicts attack success for a fixed target based on\nattacker-target capability gap. These findings suggest that fixed-capability\nattackers (e.g., humans) may become ineffective against future models,\nincreasingly capable open-source models amplify risks for existing systems, and\nmodel providers must accurately measure and control models' persuasive and\nmanipulative abilities to limit their effectiveness as attackers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20162.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645410c50b50e97d3722f1d2",
            "avatarUrl": "/avatars/e8e079981316b26dbf0f4ada6dcdc8c6.svg",
            "fullname": "Alexander Panfilov",
            "name": "kotekjedi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19377",
            "authors": [
                {
                    "_id": "6836a0e48a36b9fa7f34ef2d",
                    "user": {
                        "_id": "64c1f02bb9d81735a12a9ef6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c1f02bb9d81735a12a9ef6/DvamojbJQhiBMFOiVVvBO.jpeg",
                        "isPro": false,
                        "fullname": "Zichong Meng",
                        "user": "cr8br0ze",
                        "type": "user"
                    },
                    "name": "Zichong Meng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:56:36.129Z",
                    "hidden": false
                },
                {
                    "_id": "6836a0e48a36b9fa7f34ef2e",
                    "name": "Zeyu Han",
                    "hidden": false
                },
                {
                    "_id": "6836a0e48a36b9fa7f34ef2f",
                    "user": {
                        "_id": "65dddb434b1db09751c164bd",
                        "avatarUrl": "/avatars/f6b6fb4ae081785f070aad107c9e6449.svg",
                        "isPro": false,
                        "fullname": "Xiaogang Peng",
                        "user": "Eric-Peng",
                        "type": "user"
                    },
                    "name": "Xiaogang Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:28:47.091Z",
                    "hidden": false
                },
                {
                    "_id": "6836a0e48a36b9fa7f34ef30",
                    "name": "Yiming Xie",
                    "hidden": false
                },
                {
                    "_id": "6836a0e48a36b9fa7f34ef31",
                    "user": {
                        "_id": "6825fa72a85bada3f4c9230f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/4PIZwf33EKmy_VBXqpJMC.png",
                        "isPro": false,
                        "fullname": "Huaizu Jiang",
                        "user": "playerkk",
                        "type": "user"
                    },
                    "name": "Huaizu Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:28:29.420Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T00:36:00.000Z",
            "submittedOnDailyAt": "2025-05-28T04:06:59.905Z",
            "title": "Absolute Coordinates Make Motion Generation Easy",
            "submittedOnDailyBy": {
                "_id": "64c1f02bb9d81735a12a9ef6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c1f02bb9d81735a12a9ef6/DvamojbJQhiBMFOiVVvBO.jpeg",
                "isPro": false,
                "fullname": "Zichong Meng",
                "user": "cr8br0ze",
                "type": "user"
            },
            "summary": "State-of-the-art text-to-motion generation models rely on the\nkinematic-aware, local-relative motion representation popularized by HumanML3D,\nwhich encodes motion relative to the pelvis and to the previous frame with\nbuilt-in redundancy. While this design simplifies training for earlier\ngeneration models, it introduces critical limitations for diffusion models and\nhinders applicability to downstream tasks. In this work, we revisit the motion\nrepresentation and propose a radically simplified and long-abandoned\nalternative for text-to-motion generation: absolute joint coordinates in global\nspace. Through systematic analysis of design choices, we show that this\nformulation achieves significantly higher motion fidelity, improved text\nalignment, and strong scalability, even with a simple Transformer backbone and\nno auxiliary kinematic-aware losses. Moreover, our formulation naturally\nsupports downstream tasks such as text-driven motion control and\ntemporal/spatial editing without additional task-specific reengineering and\ncostly classifier guidance generation from control signals. Finally, we\ndemonstrate promising generalization to directly generate SMPL-H mesh vertices\nin motion from text, laying a strong foundation for future research and\nmotion-related applications.",
            "upvotes": 1,
            "discussionId": "6836a0e58a36b9fa7f34ef72",
            "ai_summary": "Absolute joint coordinates in global space improve motion fidelity, text alignment, and scalability for text-to-motion generation, supporting downstream tasks with a simple Transformer backbone.",
            "ai_keywords": [
                "kinematic-aware",
                "local-relative motion representation",
                "HumanML3D",
                "absolute joint coordinates",
                "global space",
                "diffusion models",
                "text-to-motion generation",
                "motion fidelity",
                "text alignment",
                "Transformer backbone",
                "downstream tasks",
                "text-driven motion control",
                "temporal editing",
                "spatial editing",
                "SMPL-H mesh vertices"
            ]
        },
        "publishedAt": "2025-05-25T20:36:00.000Z",
        "title": "Absolute Coordinates Make Motion Generation Easy",
        "summary": "State-of-the-art text-to-motion generation models rely on the\nkinematic-aware, local-relative motion representation popularized by HumanML3D,\nwhich encodes motion relative to the pelvis and to the previous frame with\nbuilt-in redundancy. While this design simplifies training for earlier\ngeneration models, it introduces critical limitations for diffusion models and\nhinders applicability to downstream tasks. In this work, we revisit the motion\nrepresentation and propose a radically simplified and long-abandoned\nalternative for text-to-motion generation: absolute joint coordinates in global\nspace. Through systematic analysis of design choices, we show that this\nformulation achieves significantly higher motion fidelity, improved text\nalignment, and strong scalability, even with a simple Transformer backbone and\nno auxiliary kinematic-aware losses. Moreover, our formulation naturally\nsupports downstream tasks such as text-driven motion control and\ntemporal/spatial editing without additional task-specific reengineering and\ncostly classifier guidance generation from control signals. Finally, we\ndemonstrate promising generalization to directly generate SMPL-H mesh vertices\nin motion from text, laying a strong foundation for future research and\nmotion-related applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19377.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c1f02bb9d81735a12a9ef6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c1f02bb9d81735a12a9ef6/DvamojbJQhiBMFOiVVvBO.jpeg",
            "fullname": "Zichong Meng",
            "name": "cr8br0ze",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19235",
            "authors": [
                {
                    "_id": "68373a5d8209dbdcf52d9723",
                    "name": "Qinsi Wang",
                    "hidden": false
                },
                {
                    "_id": "68373a5d8209dbdcf52d9724",
                    "name": "Hancheng Ye",
                    "hidden": false
                },
                {
                    "_id": "68373a5d8209dbdcf52d9725",
                    "name": "Ming-Yu Chung",
                    "hidden": false
                },
                {
                    "_id": "68373a5d8209dbdcf52d9726",
                    "name": "Yudong Liu",
                    "hidden": false
                },
                {
                    "_id": "68373a5d8209dbdcf52d9727",
                    "name": "Yueqian Lin",
                    "hidden": false
                },
                {
                    "_id": "68373a5d8209dbdcf52d9728",
                    "name": "Martin Kuo",
                    "hidden": false
                },
                {
                    "_id": "68373a5d8209dbdcf52d9729",
                    "name": "Mingyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "68373a5d8209dbdcf52d972a",
                    "name": "Jianyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68373a5d8209dbdcf52d972b",
                    "name": "Yiran Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T17:16:34.000Z",
            "submittedOnDailyAt": "2025-05-28T15:03:26.509Z",
            "title": "CoreMatching: A Co-adaptive Sparse Inference Framework with Token and\n  Neuron Pruning for Comprehensive Acceleration of Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "66128e8b7e0e7a64652dbbdf",
                "avatarUrl": "/avatars/b72ea5b14b55ff3af920c06b69a60b3f.svg",
                "isPro": false,
                "fullname": "Wang",
                "user": "Qinsi1",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) excel across diverse tasks but suffer from high\ninference costs in time and memory. Token sparsity mitigates inefficiencies in\ntoken usage, while neuron sparsity reduces high-dimensional computations, both\noffering promising solutions to enhance efficiency. Recently, these two\nsparsity paradigms have evolved largely in parallel, fostering the prevailing\nassumption that they function independently. However, a fundamental yet\nunderexplored question remains: Do they truly operate in isolation, or is there\na deeper underlying interplay that has yet to be uncovered? In this paper, we\nconduct the first comprehensive investigation into this question. By\nintroducing and analyzing the matching mechanism between Core Neurons and Core\nTokens, we found that key neurons and tokens for inference mutually influence\nand reinforce each other. Building on this insight, we propose CoreMatching, a\nco-adaptive sparse inference framework, which leverages the synergy between\ntoken and neuron sparsity to enhance inference efficiency. Through theoretical\nanalysis and efficiency evaluations, we demonstrate that the proposed method\nsurpasses state-of-the-art baselines on ten image understanding tasks and three\nhardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs\nreduction and a 10x overall speedup. Code is released at\nhttps://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.",
            "upvotes": 1,
            "discussionId": "68373a5f8209dbdcf52d97a1",
            "ai_summary": "A core-matching framework enhances inference efficiency in vision-language models by leveraging the synergy between token and neuron sparsity, outperforming baselines across multiple tasks and devices.",
            "ai_keywords": [
                "vision-language models",
                "token sparsity",
                "neuron sparsity",
                "inference efficiency",
                "core neurons",
                "core tokens",
                "adaptive sparse inference framework",
                "FLOPs reduction"
            ]
        },
        "publishedAt": "2025-05-25T13:16:34.000Z",
        "title": "CoreMatching: A Co-adaptive Sparse Inference Framework with Token and\n  Neuron Pruning for Comprehensive Acceleration of Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) excel across diverse tasks but suffer from high\ninference costs in time and memory. Token sparsity mitigates inefficiencies in\ntoken usage, while neuron sparsity reduces high-dimensional computations, both\noffering promising solutions to enhance efficiency. Recently, these two\nsparsity paradigms have evolved largely in parallel, fostering the prevailing\nassumption that they function independently. However, a fundamental yet\nunderexplored question remains: Do they truly operate in isolation, or is there\na deeper underlying interplay that has yet to be uncovered? In this paper, we\nconduct the first comprehensive investigation into this question. By\nintroducing and analyzing the matching mechanism between Core Neurons and Core\nTokens, we found that key neurons and tokens for inference mutually influence\nand reinforce each other. Building on this insight, we propose CoreMatching, a\nco-adaptive sparse inference framework, which leverages the synergy between\ntoken and neuron sparsity to enhance inference efficiency. Through theoretical\nanalysis and efficiency evaluations, we demonstrate that the proposed method\nsurpasses state-of-the-art baselines on ten image understanding tasks and three\nhardware devices. Notably, on the NVIDIA Titan Xp, it achieved 5x FLOPs\nreduction and a 10x overall speedup. Code is released at\nhttps://github.com/wangqinsi1/2025-ICML-CoreMatching/tree/main.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19235.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66128e8b7e0e7a64652dbbdf",
            "avatarUrl": "/avatars/b72ea5b14b55ff3af920c06b69a60b3f.svg",
            "fullname": "Wang",
            "name": "Qinsi1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17855",
            "authors": [
                {
                    "_id": "68370a318cd8af122276627d",
                    "name": "Jingyi Sun",
                    "hidden": false
                },
                {
                    "_id": "68370a318cd8af122276627e",
                    "name": "Greta Warren",
                    "hidden": false
                },
                {
                    "_id": "68370a318cd8af122276627f",
                    "name": "Irina Shklovski",
                    "hidden": false
                },
                {
                    "_id": "68370a318cd8af1222766280",
                    "name": "Isabelle Augenstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T13:06:43.000Z",
            "submittedOnDailyAt": "2025-05-28T11:36:24.156Z",
            "title": "Explaining Sources of Uncertainty in Automated Fact-Checking",
            "submittedOnDailyBy": {
                "_id": "6516bc688fe117b68e465d0b",
                "avatarUrl": "/avatars/f58af261b0fea1fd6507027f6f1ab6bb.svg",
                "isPro": false,
                "fullname": "Jingyi Sun",
                "user": "jpd459",
                "type": "user"
            },
            "summary": "Understanding sources of a model's uncertainty regarding its predictions is\ncrucial for effective human-AI collaboration. Prior work proposes using\nnumerical uncertainty or hedges (\"I'm not sure, but ...\"), which do not explain\nuncertainty that arises from conflicting evidence, leaving users unable to\nresolve disagreements or rely on the output. We introduce CLUE\n(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the\nfirst framework to generate natural language explanations of model uncertainty\nby (i) identifying relationships between spans of text that expose\nclaim-evidence or inter-evidence conflicts and agreements that drive the\nmodel's predictive uncertainty in an unsupervised way, and (ii) generating\nexplanations via prompting and attention steering that verbalize these critical\ninteractions. Across three language models and two fact-checking datasets, we\nshow that CLUE produces explanations that are more faithful to the model's\nuncertainty and more consistent with fact-checking decisions than prompting for\nuncertainty explanations without span-interaction guidance. Human evaluators\njudge our explanations to be more helpful, more informative, less redundant,\nand more logically consistent with the input than this baseline. CLUE requires\nno fine-tuning or architectural changes, making it plug-and-play for any\nwhite-box language model. By explicitly linking uncertainty to evidence\nconflicts, it offers practical support for fact-checking and generalises\nreadily to other tasks that require reasoning over complex information.",
            "upvotes": 1,
            "discussionId": "68370a328cd8af12227662b9",
            "ai_summary": "CLUE generates natural language explanations for a language model's uncertainty by identifying and explaining conflicts and agreements in text spans, enhancing the clarity and helpfulness of explanations in tasks like fact-checking.",
            "ai_keywords": [
                "knowledge graphs",
                "span-interaction guidance",
                "claim-evidence",
                "inter-evidence conflicts",
                "agreements",
                "predictive uncertainty",
                "prompting",
                "attention steering",
                "fact-checking datasets",
                "white-box language models"
            ]
        },
        "publishedAt": "2025-05-23T09:06:43.000Z",
        "title": "Explaining Sources of Uncertainty in Automated Fact-Checking",
        "summary": "Understanding sources of a model's uncertainty regarding its predictions is\ncrucial for effective human-AI collaboration. Prior work proposes using\nnumerical uncertainty or hedges (\"I'm not sure, but ...\"), which do not explain\nuncertainty that arises from conflicting evidence, leaving users unable to\nresolve disagreements or rely on the output. We introduce CLUE\n(Conflict-and-Agreement-aware Language-model Uncertainty Explanations), the\nfirst framework to generate natural language explanations of model uncertainty\nby (i) identifying relationships between spans of text that expose\nclaim-evidence or inter-evidence conflicts and agreements that drive the\nmodel's predictive uncertainty in an unsupervised way, and (ii) generating\nexplanations via prompting and attention steering that verbalize these critical\ninteractions. Across three language models and two fact-checking datasets, we\nshow that CLUE produces explanations that are more faithful to the model's\nuncertainty and more consistent with fact-checking decisions than prompting for\nuncertainty explanations without span-interaction guidance. Human evaluators\njudge our explanations to be more helpful, more informative, less redundant,\nand more logically consistent with the input than this baseline. CLUE requires\nno fine-tuning or architectural changes, making it plug-and-play for any\nwhite-box language model. By explicitly linking uncertainty to evidence\nconflicts, it offers practical support for fact-checking and generalises\nreadily to other tasks that require reasoning over complex information.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17855.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6516bc688fe117b68e465d0b",
            "avatarUrl": "/avatars/f58af261b0fea1fd6507027f6f1ab6bb.svg",
            "fullname": "Jingyi Sun",
            "name": "jpd459",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17639",
            "authors": [
                {
                    "_id": "683733543eb396dd7b9c8287",
                    "user": {
                        "_id": "6527c063e86758eb6ca800a1",
                        "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
                        "isPro": false,
                        "fullname": "JarvisPei",
                        "user": "Eleven-P",
                        "type": "user"
                    },
                    "name": "Zehua Pei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:13:42.952Z",
                    "hidden": false
                },
                {
                    "_id": "683733543eb396dd7b9c8288",
                    "name": "Ying Zhang",
                    "hidden": false
                },
                {
                    "_id": "683733543eb396dd7b9c8289",
                    "name": "Hui-Ling Zhen",
                    "hidden": false
                },
                {
                    "_id": "683733543eb396dd7b9c828a",
                    "name": "Xianzhi Yu",
                    "hidden": false
                },
                {
                    "_id": "683733543eb396dd7b9c828b",
                    "name": "Wulong Liu",
                    "hidden": false
                },
                {
                    "_id": "683733543eb396dd7b9c828c",
                    "name": "Sinno Jialin Pan",
                    "hidden": false
                },
                {
                    "_id": "683733543eb396dd7b9c828d",
                    "name": "Mingxuan Yuan",
                    "hidden": false
                },
                {
                    "_id": "683733543eb396dd7b9c828e",
                    "name": "Bei Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T08:59:16.000Z",
            "submittedOnDailyAt": "2025-05-28T14:37:14.539Z",
            "title": "PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and\n  Retrieval",
            "submittedOnDailyBy": {
                "_id": "6527c063e86758eb6ca800a1",
                "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
                "isPro": false,
                "fullname": "JarvisPei",
                "user": "Eleven-P",
                "type": "user"
            },
            "summary": "Mixture-of-experts (MoE) architectures enable scaling large language models\n(LLMs) to vast parameter counts without a proportional rise in computational\ncosts. However, the significant memory demands of large MoE models hinder their\ndeployment across various computational environments, from cloud servers to\nconsumer devices. This study first demonstrates pronounced task-specific\nspecialization in expert activation patterns within MoE layers. Building on\nthis, we introduce PreMoe, a novel framework that enables efficient deployment\nof massive MoE models in memory-constrained environments. PreMoe features two\nmain components: probabilistic expert pruning (PEP) and task-adaptive expert\nretrieval (TAER). PEP employs a new metric, the task-conditioned expected\nselection score (TCESS), derived from router logits to quantify expert\nimportance for specific tasks, thereby identifying a minimal set of critical\nexperts. TAER leverages these task-specific expert importance profiles for\nefficient inference. It pre-computes and stores compact expert patterns for\ndiverse tasks. When a user query is received, TAER rapidly identifies the most\nrelevant stored task pattern and reconstructs the model by loading only the\nsmall subset of experts crucial for that task. This approach dramatically\nreduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B\nmaintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\%\nexpert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning\n(87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and\n81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64\n(390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly\navailable at https://github.com/JarvisPei/PreMoe.",
            "upvotes": 1,
            "discussionId": "683733553eb396dd7b9c82c5",
            "ai_summary": "PreMoe framework enables efficient deployment of large MoE language models in memory-constrained environments by pruning and retrieving task-specific experts.",
            "ai_keywords": [
                "mixture-of-experts (MoE)",
                "large language models (LLMs)",
                "probabilistic expert pruning (PEP)",
                "task-adaptive expert retrieval (TAER)",
                "task-conditioned expected selection score (TCESS)",
                "DeepSeek-R1",
                "Pangu-Ultra-MoE",
                "MATH500",
                "AIME24"
            ]
        },
        "publishedAt": "2025-05-23T04:59:16.000Z",
        "title": "PreMoe: Lightening MoEs on Constrained Memory by Expert Pruning and\n  Retrieval",
        "summary": "Mixture-of-experts (MoE) architectures enable scaling large language models\n(LLMs) to vast parameter counts without a proportional rise in computational\ncosts. However, the significant memory demands of large MoE models hinder their\ndeployment across various computational environments, from cloud servers to\nconsumer devices. This study first demonstrates pronounced task-specific\nspecialization in expert activation patterns within MoE layers. Building on\nthis, we introduce PreMoe, a novel framework that enables efficient deployment\nof massive MoE models in memory-constrained environments. PreMoe features two\nmain components: probabilistic expert pruning (PEP) and task-adaptive expert\nretrieval (TAER). PEP employs a new metric, the task-conditioned expected\nselection score (TCESS), derived from router logits to quantify expert\nimportance for specific tasks, thereby identifying a minimal set of critical\nexperts. TAER leverages these task-specific expert importance profiles for\nefficient inference. It pre-computes and stores compact expert patterns for\ndiverse tasks. When a user query is received, TAER rapidly identifies the most\nrelevant stored task pattern and reconstructs the model by loading only the\nsmall subset of experts crucial for that task. This approach dramatically\nreduces the memory footprint across all deployment scenarios. DeepSeek-R1 671B\nmaintains 97.2\\% accuracy on MATH500 when pruned to 8/128 configuration (50\\%\nexpert reduction), and still achieves 72.0\\% with aggressive 8/32 pruning\n(87.5\\% expert reduction). Pangu-Ultra-MoE 718B achieves 97.15\\% on MATH500 and\n81.3\\% on AIME24 with 8/128 pruning, while even more aggressive pruning to 4/64\n(390GB memory) preserves 96.95\\% accuracy on MATH500. We make our code publicly\navailable at https://github.com/JarvisPei/PreMoe.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17639.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6527c063e86758eb6ca800a1",
            "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
            "fullname": "JarvisPei",
            "name": "Eleven-P",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17190",
            "authors": [
                {
                    "_id": "6836cd5bc65dcde2f95d674f",
                    "user": {
                        "_id": "67ee9e42c4ff6510f47b8c29",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Sl0EyXleUwjSFHLa1qsve.png",
                        "isPro": false,
                        "fullname": "Baran Hashemi",
                        "user": "Baran47",
                        "type": "user"
                    },
                    "name": "Baran Hashemi",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-28T08:54:13.357Z",
                    "hidden": false
                },
                {
                    "_id": "6836cd5bc65dcde2f95d6750",
                    "name": "Kurt Pasque",
                    "hidden": false
                },
                {
                    "_id": "6836cd5bc65dcde2f95d6751",
                    "name": "Chris Teska",
                    "hidden": false
                },
                {
                    "_id": "6836cd5bc65dcde2f95d6752",
                    "name": "Ruriko Yoshida",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T18:01:25.000Z",
            "submittedOnDailyAt": "2025-05-28T07:21:35.327Z",
            "title": "Tropical Attention: Neural Algorithmic Reasoning for Combinatorial\n  Algorithms",
            "submittedOnDailyBy": {
                "_id": "67ee9e42c4ff6510f47b8c29",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Sl0EyXleUwjSFHLa1qsve.png",
                "isPro": false,
                "fullname": "Baran Hashemi",
                "user": "Baran47",
                "type": "user"
            },
            "summary": "Dynamic programming (DP) algorithms for combinatorial optimization problems\nwork with taking maximization, minimization, and classical addition in their\nrecursion algorithms. The associated value functions correspond to convex\npolyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning\nmodels, however, rely on softmax-normalized dot-product attention where the\nsmooth exponential weighting blurs these sharp polyhedral structures and\ncollapses when evaluated on out-of-distribution (OOD) settings. We introduce\nTropical attention, a novel attention function that operates natively in the\nmax-plus semiring of tropical geometry. We prove that Tropical attention can\napproximate tropical circuits of DP-type combinatorial algorithms. We then\npropose that using Tropical transformers enhances empirical OOD performance in\nboth length generalization and value generalization, on algorithmic reasoning\ntasks, surpassing softmax baselines while remaining stable under adversarial\nattacks. We also present adversarial-attack generalization as a third axis for\nNeural Algorithmic Reasoning benchmarking. Our results demonstrate that\nTropical attention restores the sharp, scale-invariant reasoning absent from\nsoftmax.",
            "upvotes": 1,
            "discussionId": "6836cd5cc65dcde2f95d679d",
            "ai_summary": "Tropical attention, a novel attention mechanism operating in the max-plus semiring, enhances Neural Algorithmic Reasoning models by improving out-of-distribution performance and robustness to adversarial attacks compared to softmax attention.",
            "ai_keywords": [
                "tropical attention",
                "max-plus semiring",
                "tropical geometry",
                "tropical circuits",
                "Neural Algorithmic Reasoning",
                "out-of-distribution",
                "adversarial attacks"
            ]
        },
        "publishedAt": "2025-05-22T14:01:25.000Z",
        "title": "Tropical Attention: Neural Algorithmic Reasoning for Combinatorial\n  Algorithms",
        "summary": "Dynamic programming (DP) algorithms for combinatorial optimization problems\nwork with taking maximization, minimization, and classical addition in their\nrecursion algorithms. The associated value functions correspond to convex\npolyhedra in the max plus semiring. Existing Neural Algorithmic Reasoning\nmodels, however, rely on softmax-normalized dot-product attention where the\nsmooth exponential weighting blurs these sharp polyhedral structures and\ncollapses when evaluated on out-of-distribution (OOD) settings. We introduce\nTropical attention, a novel attention function that operates natively in the\nmax-plus semiring of tropical geometry. We prove that Tropical attention can\napproximate tropical circuits of DP-type combinatorial algorithms. We then\npropose that using Tropical transformers enhances empirical OOD performance in\nboth length generalization and value generalization, on algorithmic reasoning\ntasks, surpassing softmax baselines while remaining stable under adversarial\nattacks. We also present adversarial-attack generalization as a third axis for\nNeural Algorithmic Reasoning benchmarking. Our results demonstrate that\nTropical attention restores the sharp, scale-invariant reasoning absent from\nsoftmax.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17190.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67ee9e42c4ff6510f47b8c29",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Sl0EyXleUwjSFHLa1qsve.png",
            "fullname": "Baran Hashemi",
            "name": "Baran47",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.15561",
            "authors": [
                {
                    "_id": "6836c1815b96c1925376ecce",
                    "user": {
                        "_id": "62bfff6788fdef8ecde8c45b",
                        "avatarUrl": "/avatars/637da88b8dce01bdc2a4c1319cc882e3.svg",
                        "isPro": false,
                        "fullname": "Florin Cuconasu",
                        "user": "florin-hf",
                        "type": "user"
                    },
                    "name": "Florin Cuconasu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:55:27.298Z",
                    "hidden": false
                },
                {
                    "_id": "6836c1815b96c1925376eccf",
                    "user": {
                        "_id": "652028f9974423bd3ea33ae2",
                        "avatarUrl": "/avatars/50b749f9a95a3b0fb7a2bfcf17c34295.svg",
                        "isPro": false,
                        "fullname": "Simone Filice",
                        "user": "filice",
                        "type": "user"
                    },
                    "name": "Simone Filice",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:25:37.486Z",
                    "hidden": false
                },
                {
                    "_id": "6836c1815b96c1925376ecd0",
                    "user": {
                        "_id": "63062839df993a789e649e00",
                        "avatarUrl": "/avatars/48a57ab7458a0c525da76b289a36a9f6.svg",
                        "isPro": false,
                        "fullname": "Guy Horowitz",
                        "user": "guyhoro95",
                        "type": "user"
                    },
                    "name": "Guy Horowitz",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:25:43.599Z",
                    "hidden": false
                },
                {
                    "_id": "6836c1815b96c1925376ecd1",
                    "user": {
                        "_id": "677fa709d34d476c057cd4cb",
                        "avatarUrl": "/avatars/54fe4086b5f1e7af7f573d971f01a2c2.svg",
                        "isPro": false,
                        "fullname": "Yoelle Maarek",
                        "user": "yoelle",
                        "type": "user"
                    },
                    "name": "Yoelle Maarek",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:25:49.349Z",
                    "hidden": false
                },
                {
                    "_id": "6836c1815b96c1925376ecd2",
                    "user": {
                        "_id": "6075d282134c000d1ae10d54",
                        "avatarUrl": "/avatars/8691622e1f6325bc378b2268c119b9a0.svg",
                        "isPro": false,
                        "fullname": "Fabrizio Silvestri",
                        "user": "fabrizio-silvestri",
                        "type": "user"
                    },
                    "name": "Fabrizio Silvestri",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:25:54.460Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62bfff6788fdef8ecde8c45b/lHSYQcfCsocdozBuiX2Ug.png",
                "https://cdn-uploads.huggingface.co/production/uploads/62bfff6788fdef8ecde8c45b/nNkrTZvqVcPjqYmc1snJd.png"
            ],
            "publishedAt": "2025-05-21T14:18:01.000Z",
            "submittedOnDailyAt": "2025-05-28T06:29:50.476Z",
            "title": "Do RAG Systems Suffer From Positional Bias?",
            "submittedOnDailyBy": {
                "_id": "62bfff6788fdef8ecde8c45b",
                "avatarUrl": "/avatars/637da88b8dce01bdc2a4c1319cc882e3.svg",
                "isPro": false,
                "fullname": "Florin Cuconasu",
                "user": "florin-hf",
                "type": "user"
            },
            "summary": "Retrieval Augmented Generation enhances LLM accuracy by adding passages\nretrieved from an external corpus to the LLM prompt. This paper investigates\nhow positional bias - the tendency of LLMs to weight information differently\nbased on its position in the prompt - affects not only the LLM's capability to\ncapitalize on relevant passages, but also its susceptibility to distracting\npassages. Through extensive experiments on three benchmarks, we show how\nstate-of-the-art retrieval pipelines, while attempting to retrieve relevant\npassages, systematically bring highly distracting ones to the top ranks, with\nover 60% of queries containing at least one highly distracting passage among\nthe top-10 retrieved passages. As a result, the impact of the LLM positional\nbias, which in controlled settings is often reported as very prominent by\nrelated works, is actually marginal in real scenarios since both relevant and\ndistracting passages are, in turn, penalized. Indeed, our findings reveal that\nsophisticated strategies that attempt to rearrange the passages based on LLM\npositional preferences do not perform better than random shuffling.",
            "upvotes": 1,
            "discussionId": "6836c1815b96c1925376ecf7",
            "ai_summary": "Retrieval Augmented Generation suffers from high distraction from top-ranked passages, rendering LLM positional bias less impactful than previously thought.",
            "ai_keywords": [
                "Retrieval Augmented Generation",
                "LLM",
                "positional bias",
                "relevant passages",
                "distracting passages",
                "retrieval pipelines"
            ]
        },
        "publishedAt": "2025-05-21T10:18:01.000Z",
        "title": "Do RAG Systems Suffer From Positional Bias?",
        "summary": "Retrieval Augmented Generation enhances LLM accuracy by adding passages\nretrieved from an external corpus to the LLM prompt. This paper investigates\nhow positional bias - the tendency of LLMs to weight information differently\nbased on its position in the prompt - affects not only the LLM's capability to\ncapitalize on relevant passages, but also its susceptibility to distracting\npassages. Through extensive experiments on three benchmarks, we show how\nstate-of-the-art retrieval pipelines, while attempting to retrieve relevant\npassages, systematically bring highly distracting ones to the top ranks, with\nover 60% of queries containing at least one highly distracting passage among\nthe top-10 retrieved passages. As a result, the impact of the LLM positional\nbias, which in controlled settings is often reported as very prominent by\nrelated works, is actually marginal in real scenarios since both relevant and\ndistracting passages are, in turn, penalized. Indeed, our findings reveal that\nsophisticated strategies that attempt to rearrange the passages based on LLM\npositional preferences do not perform better than random shuffling.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62bfff6788fdef8ecde8c45b/lHSYQcfCsocdozBuiX2Ug.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62bfff6788fdef8ecde8c45b/nNkrTZvqVcPjqYmc1snJd.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15561.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62bfff6788fdef8ecde8c45b",
            "avatarUrl": "/avatars/637da88b8dce01bdc2a4c1319cc882e3.svg",
            "fullname": "Florin Cuconasu",
            "name": "florin-hf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21501",
            "authors": [
                {
                    "_id": "68373c9bb93aa1e23ae48a27",
                    "name": "Yinjie Chen",
                    "hidden": false
                },
                {
                    "_id": "68373c9bb93aa1e23ae48a28",
                    "name": "Zipeng Yan",
                    "hidden": false
                },
                {
                    "_id": "68373c9bb93aa1e23ae48a29",
                    "name": "Chong Zhou",
                    "hidden": false
                },
                {
                    "_id": "68373c9bb93aa1e23ae48a2a",
                    "name": "Bo Dai",
                    "hidden": false
                },
                {
                    "_id": "68373c9bb93aa1e23ae48a2b",
                    "user": {
                        "_id": "64b6ce23dbbd1f2cdb624d56",
                        "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
                        "isPro": false,
                        "fullname": "Andrew Luo",
                        "user": "aluo-x",
                        "type": "user"
                    },
                    "name": "Andrew F. Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:13:35.893Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T17:59:41.000Z",
            "submittedOnDailyAt": "2025-05-28T15:11:54.078Z",
            "title": "Vision Transformers with Self-Distilled Registers",
            "submittedOnDailyBy": {
                "_id": "64b6ce23dbbd1f2cdb624d56",
                "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
                "isPro": false,
                "fullname": "Andrew Luo",
                "user": "aluo-x",
                "type": "user"
            },
            "summary": "Vision Transformers (ViTs) have emerged as the dominant architecture for\nvisual processing tasks, demonstrating excellent scalability with increased\ntraining data and model size. However, recent work has identified the emergence\nof artifact tokens in ViTs that are incongruous with the local semantics. These\nanomalous tokens degrade ViT performance in tasks that require fine-grained\nlocalization or structural coherence. An effective mitigation of this issue is\nto the addition of register tokens to ViTs, which implicitly \"absorb\" the\nartifact term during training. Given the availability of various large-scale\npre-trained ViTs, in this paper we aim at equipping them with such register\ntokens without the need of re-training them from scratch, which is infeasible\nconsidering their size. Specifically, we propose Post Hoc Registers (PH-Reg),\nan efficient self-distillation method that integrates registers into an\nexisting ViT without requiring additional labeled data and full retraining.\nPH-Reg initializes both teacher and student networks from the same pre-trained\nViT. The teacher remains frozen and unmodified, while the student is augmented\nwith randomly initialized register tokens. By applying test-time augmentation\nto the teacher's inputs, we generate denoised dense embeddings free of\nartifacts, which are then used to optimize only a small subset of unlocked\nstudent weights. We show that our approach can effectively reduce the number of\nartifact tokens, improving the segmentation and depth prediction of the student\nViT under zero-shot and linear probing.",
            "upvotes": 0,
            "discussionId": "68373c9db93aa1e23ae48a7d",
            "ai_summary": "Post Hoc Registers, a self-distillation method, integrates registers into pre-trained Vision Transformers to reduce artifact tokens, enhancing segmentation and depth prediction.",
            "ai_keywords": [
                "vision transformers",
                "artifact tokens",
                "register tokens",
                "self-distillation",
                "test-time augmentation",
                "denoised dense embeddings",
                "segmentation",
                "depth prediction",
                "zero-shot learning",
                "linear probing"
            ]
        },
        "publishedAt": "2025-05-27T13:59:41.000Z",
        "title": "Vision Transformers with Self-Distilled Registers",
        "summary": "Vision Transformers (ViTs) have emerged as the dominant architecture for\nvisual processing tasks, demonstrating excellent scalability with increased\ntraining data and model size. However, recent work has identified the emergence\nof artifact tokens in ViTs that are incongruous with the local semantics. These\nanomalous tokens degrade ViT performance in tasks that require fine-grained\nlocalization or structural coherence. An effective mitigation of this issue is\nto the addition of register tokens to ViTs, which implicitly \"absorb\" the\nartifact term during training. Given the availability of various large-scale\npre-trained ViTs, in this paper we aim at equipping them with such register\ntokens without the need of re-training them from scratch, which is infeasible\nconsidering their size. Specifically, we propose Post Hoc Registers (PH-Reg),\nan efficient self-distillation method that integrates registers into an\nexisting ViT without requiring additional labeled data and full retraining.\nPH-Reg initializes both teacher and student networks from the same pre-trained\nViT. The teacher remains frozen and unmodified, while the student is augmented\nwith randomly initialized register tokens. By applying test-time augmentation\nto the teacher's inputs, we generate denoised dense embeddings free of\nartifacts, which are then used to optimize only a small subset of unlocked\nstudent weights. We show that our approach can effectively reduce the number of\nartifact tokens, improving the segmentation and depth prediction of the student\nViT under zero-shot and linear probing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21501.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b6ce23dbbd1f2cdb624d56",
            "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
            "fullname": "Andrew Luo",
            "name": "aluo-x",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20052",
            "authors": [
                {
                    "_id": "6836f9a6d44ecc28c70cc038",
                    "user": {
                        "_id": "61faa58a11deaebce017da4a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670851610124-61faa58a11deaebce017da4a.jpeg",
                        "isPro": false,
                        "fullname": "Hazem Essam",
                        "user": "hazemessam",
                        "type": "user"
                    },
                    "name": "Hazem Alsamkary",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T12:12:23.522Z",
                    "hidden": false
                },
                {
                    "_id": "6836f9a6d44ecc28c70cc039",
                    "user": {
                        "_id": "63959596401bd73e467e1fc2",
                        "avatarUrl": "/avatars/963d073e9d5ee414d5c185faf9d8d4a6.svg",
                        "isPro": false,
                        "fullname": "Mohamed Elshaffei",
                        "user": "shaffei",
                        "type": "user"
                    },
                    "name": "Mohamed Elshaffei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:27:38.964Z",
                    "hidden": false
                },
                {
                    "_id": "6836f9a6d44ecc28c70cc03a",
                    "user": {
                        "_id": "62f96db84588fe31f43aceb9",
                        "avatarUrl": "/avatars/7577f071b5a030f04f6ab0df46e5e246.svg",
                        "isPro": false,
                        "fullname": "Mohamed Elkerdawy",
                        "user": "melkerdawy",
                        "type": "user"
                    },
                    "name": "Mohamed Elkerdawy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:27:45.103Z",
                    "hidden": false
                },
                {
                    "_id": "6836f9a6d44ecc28c70cc03b",
                    "name": "Ahmed Elnaggar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T14:41:10.000Z",
            "submittedOnDailyAt": "2025-05-28T10:29:19.212Z",
            "title": "Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion\n  Enhances Protein Representations",
            "submittedOnDailyBy": {
                "_id": "61faa58a11deaebce017da4a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670851610124-61faa58a11deaebce017da4a.jpeg",
                "isPro": false,
                "fullname": "Hazem Essam",
                "user": "hazemessam",
                "type": "user"
            },
            "summary": "Protein language models (PLMs) have emerged as powerful tools to detect\ncomplex patterns of protein sequences. However, the capability of PLMs to fully\ncapture information on protein sequences might be limited by focusing on single\npre-training tasks. Although adding data modalities or supervised objectives\ncan improve the performance of PLMs, pre-training often remains focused on\ndenoising corrupted sequences. To push the boundaries of PLMs, our research\ninvestigated a multi-task pre-training strategy. We developed Ankh3, a model\njointly optimized on two objectives: masked language modeling with multiple\nmasking probabilities and protein sequence completion relying only on protein\nsequences as input. This multi-task pre-training demonstrated that PLMs can\nlearn richer and more generalizable representations solely from protein\nsequences. The results demonstrated improved performance in downstream tasks,\nsuch as secondary structure prediction, fluorescence, GB1 fitness, and contact\nprediction. The integration of multiple tasks gave the model a more\ncomprehensive understanding of protein properties, leading to more robust and\naccurate predictions.",
            "upvotes": 0,
            "discussionId": "6836f9a7d44ecc28c70cc05d",
            "ai_summary": "A multi-task pre-training strategy for protein language models improves their performance on downstream protein prediction tasks by learning richer representations from sequence data alone.",
            "ai_keywords": [
                "protein language models",
                "pre-training",
                "masked language modeling",
                "protein sequence completion",
                "secondary structure prediction",
                "fluorescence",
                "fitness",
                "contact prediction"
            ]
        },
        "publishedAt": "2025-05-26T10:41:10.000Z",
        "title": "Ankh3: Multi-Task Pretraining with Sequence Denoising and Completion\n  Enhances Protein Representations",
        "summary": "Protein language models (PLMs) have emerged as powerful tools to detect\ncomplex patterns of protein sequences. However, the capability of PLMs to fully\ncapture information on protein sequences might be limited by focusing on single\npre-training tasks. Although adding data modalities or supervised objectives\ncan improve the performance of PLMs, pre-training often remains focused on\ndenoising corrupted sequences. To push the boundaries of PLMs, our research\ninvestigated a multi-task pre-training strategy. We developed Ankh3, a model\njointly optimized on two objectives: masked language modeling with multiple\nmasking probabilities and protein sequence completion relying only on protein\nsequences as input. This multi-task pre-training demonstrated that PLMs can\nlearn richer and more generalizable representations solely from protein\nsequences. The results demonstrated improved performance in downstream tasks,\nsuch as secondary structure prediction, fluorescence, GB1 fitness, and contact\nprediction. The integration of multiple tasks gave the model a more\ncomprehensive understanding of protein properties, leading to more robust and\naccurate predictions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20052.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61faa58a11deaebce017da4a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670851610124-61faa58a11deaebce017da4a.jpeg",
            "fullname": "Hazem Essam",
            "name": "hazemessam",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20036",
            "authors": [
                {
                    "_id": "6835523ac0003bc4022e0113",
                    "user": {
                        "_id": "61faa58a11deaebce017da4a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670851610124-61faa58a11deaebce017da4a.jpeg",
                        "isPro": false,
                        "fullname": "Hazem Essam",
                        "user": "hazemessam",
                        "type": "user"
                    },
                    "name": "Hazem Alsamkary",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T12:12:12.065Z",
                    "hidden": false
                },
                {
                    "_id": "6835523ac0003bc4022e0114",
                    "user": {
                        "_id": "63959596401bd73e467e1fc2",
                        "avatarUrl": "/avatars/963d073e9d5ee414d5c185faf9d8d4a6.svg",
                        "isPro": false,
                        "fullname": "Mohamed Elshaffei",
                        "user": "shaffei",
                        "type": "user"
                    },
                    "name": "Mohamed Elshaffei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:26:53.712Z",
                    "hidden": false
                },
                {
                    "_id": "6835523ac0003bc4022e0115",
                    "user": {
                        "_id": "63a1923af30c4642277456e6",
                        "avatarUrl": "/avatars/d1f06afe2f4e3cb30d014045d8c62c28.svg",
                        "isPro": false,
                        "fullname": "Mohamed Soudy",
                        "user": "MohmedSoudy",
                        "type": "user"
                    },
                    "name": "Mohamed Soudy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:26:59.250Z",
                    "hidden": false
                },
                {
                    "_id": "6835523ac0003bc4022e0116",
                    "name": "Sara Ossman",
                    "hidden": false
                },
                {
                    "_id": "6835523ac0003bc4022e0117",
                    "name": "Abdallah Amr",
                    "hidden": false
                },
                {
                    "_id": "6835523ac0003bc4022e0118",
                    "name": "Nehal Adel Abdelsalam",
                    "hidden": false
                },
                {
                    "_id": "6835523ac0003bc4022e0119",
                    "user": {
                        "_id": "62f96db84588fe31f43aceb9",
                        "avatarUrl": "/avatars/7577f071b5a030f04f6ab0df46e5e246.svg",
                        "isPro": false,
                        "fullname": "Mohamed Elkerdawy",
                        "user": "melkerdawy",
                        "type": "user"
                    },
                    "name": "Mohamed Elkerdawy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:27:15.295Z",
                    "hidden": false
                },
                {
                    "_id": "6835523ac0003bc4022e011a",
                    "user": {
                        "_id": "5f16d73c925b9863e28ad430",
                        "avatarUrl": "/avatars/2f61e47eb057effee4490d62a67371c6.svg",
                        "isPro": false,
                        "fullname": "Ahmed Elnaggar",
                        "user": "agemagician",
                        "type": "user"
                    },
                    "name": "Ahmed Elnaggar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:27:28.070Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T14:23:08.000Z",
            "submittedOnDailyAt": "2025-05-28T10:30:17.414Z",
            "title": "Beyond Simple Concatenation: Fairly Assessing PLM Architectures for\n  Multi-Chain Protein-Protein Interactions Prediction",
            "submittedOnDailyBy": {
                "_id": "61faa58a11deaebce017da4a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670851610124-61faa58a11deaebce017da4a.jpeg",
                "isPro": false,
                "fullname": "Hazem Essam",
                "user": "hazemessam",
                "type": "user"
            },
            "summary": "Protein-protein interactions (PPIs) are fundamental to numerous cellular\nprocesses, and their characterization is vital for understanding disease\nmechanisms and guiding drug discovery. While protein language models (PLMs)\nhave demonstrated remarkable success in predicting protein structure and\nfunction, their application to sequence-based PPI binding affinity prediction\nremains relatively underexplored. This gap is often attributed to the scarcity\nof high-quality, rigorously refined datasets and the reliance on simple\nstrategies for concatenating protein representations. In this work, we address\nthese limitations. First, we introduce a meticulously curated version of the\nPPB-Affinity dataset of a total of 8,207 unique protein-protein interaction\nentries, by resolving annotation inconsistencies and duplicate entries for\nmulti-chain protein interactions. This dataset incorporates a stringent, less\nthan or equal to 30%, sequence identity threshold to ensure robust splitting\ninto training, validation, and test sets, minimizing data leakage. Second, we\npropose and systematically evaluate four architectures for adapting PLMs to PPI\nbinding affinity prediction: embeddings concatenation (EC), sequences\nconcatenation (SC), hierarchical pooling (HP), and pooled attention addition\n(PAD). These architectures were assessed using two training methods: full\nfine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM\nfeatures. Our comprehensive experiments across multiple leading PLMs (ProtT5,\nESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures\nconsistently outperform conventional concatenation methods, achieving up to 12%\nincrease in terms of Spearman correlation. These results highlight the\nnecessity of sophisticated architectural designs to fully exploit the\ncapabilities of PLMs for nuanced PPI binding affinity prediction.",
            "upvotes": 0,
            "discussionId": "6835523ac0003bc4022e0148",
            "githubRepo": "https://github.com/Proteinea/ppiseq",
            "ai_summary": "The study introduces a curated PPB-Affinity dataset and evaluates four architectural designs for adapting protein language models to predict protein-protein interaction binding affinity, demonstrating that hierarchical pooling and pooled attention addition architectures perform better than concatenation methods.",
            "ai_keywords": [
                "protein language models",
                "PPI binding affinity prediction",
                "embeddings concatenation",
                "sequences concatenation",
                "hierarchical pooling",
                "pooled attention addition",
                "ProtT5",
                "ESM2",
                "Ankh",
                "Ankh2",
                "ESM3",
                "Spearman correlation"
            ]
        },
        "publishedAt": "2025-05-26T10:23:08.000Z",
        "title": "Beyond Simple Concatenation: Fairly Assessing PLM Architectures for\n  Multi-Chain Protein-Protein Interactions Prediction",
        "summary": "Protein-protein interactions (PPIs) are fundamental to numerous cellular\nprocesses, and their characterization is vital for understanding disease\nmechanisms and guiding drug discovery. While protein language models (PLMs)\nhave demonstrated remarkable success in predicting protein structure and\nfunction, their application to sequence-based PPI binding affinity prediction\nremains relatively underexplored. This gap is often attributed to the scarcity\nof high-quality, rigorously refined datasets and the reliance on simple\nstrategies for concatenating protein representations. In this work, we address\nthese limitations. First, we introduce a meticulously curated version of the\nPPB-Affinity dataset of a total of 8,207 unique protein-protein interaction\nentries, by resolving annotation inconsistencies and duplicate entries for\nmulti-chain protein interactions. This dataset incorporates a stringent, less\nthan or equal to 30%, sequence identity threshold to ensure robust splitting\ninto training, validation, and test sets, minimizing data leakage. Second, we\npropose and systematically evaluate four architectures for adapting PLMs to PPI\nbinding affinity prediction: embeddings concatenation (EC), sequences\nconcatenation (SC), hierarchical pooling (HP), and pooled attention addition\n(PAD). These architectures were assessed using two training methods: full\nfine-tuning and a lightweight approach employing ConvBERT heads over frozen PLM\nfeatures. Our comprehensive experiments across multiple leading PLMs (ProtT5,\nESM2, Ankh, Ankh2, and ESM3) demonstrated that the HP and PAD architectures\nconsistently outperform conventional concatenation methods, achieving up to 12%\nincrease in terms of Spearman correlation. These results highlight the\nnecessity of sophisticated architectural designs to fully exploit the\ncapabilities of PLMs for nuanced PPI binding affinity prediction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20036.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61faa58a11deaebce017da4a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670851610124-61faa58a11deaebce017da4a.jpeg",
            "fullname": "Hazem Essam",
            "name": "hazemessam",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19954",
            "authors": [
                {
                    "_id": "6836ffd1ec0f4b6889526488",
                    "user": {
                        "_id": "65be17eb52edc430281c6a72",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65be17eb52edc430281c6a72/ZeSxgA25KnJ8pqvvy6rdl.png",
                        "isPro": false,
                        "fullname": "Andrew Zamai",
                        "user": "andrewzamai",
                        "type": "user"
                    },
                    "name": "Andrew Zamai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:26:16.791Z",
                    "hidden": false
                },
                {
                    "_id": "6836ffd1ec0f4b6889526489",
                    "user": {
                        "_id": "64897d743262c9f1566efb22",
                        "avatarUrl": "/avatars/daa78961fe34d598ae5030efb6f431f8.svg",
                        "isPro": true,
                        "fullname": "Nathanal Fijalkow",
                        "user": "nathanael-fijalkow",
                        "type": "user"
                    },
                    "name": "Nathanael Fijalkow",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:26:22.686Z",
                    "hidden": false
                },
                {
                    "_id": "6836ffd1ec0f4b688952648a",
                    "user": {
                        "_id": "67924a56c9b9adc06d1f51a4",
                        "avatarUrl": "/avatars/41d032829ed112f0b56a76c4f7d5861b.svg",
                        "isPro": false,
                        "fullname": "BorisMansencal",
                        "user": "BorisMansencal",
                        "type": "user"
                    },
                    "name": "Boris Mansencal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:26:28.609Z",
                    "hidden": false
                },
                {
                    "_id": "6836ffd1ec0f4b688952648b",
                    "user": {
                        "_id": "664c867efc66a5a3eee5f16a",
                        "avatarUrl": "/avatars/913aacea32932e54b9144a69995e6733.svg",
                        "isPro": false,
                        "fullname": "Laurent Simon",
                        "user": "lcysimon",
                        "type": "user"
                    },
                    "name": "Laurent Simon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:26:35.638Z",
                    "hidden": false
                },
                {
                    "_id": "6836ffd1ec0f4b688952648c",
                    "user": {
                        "_id": "6641e54c24555ed542945a5a",
                        "avatarUrl": "/avatars/a04a19a3bd2aad05acec7e675e597415.svg",
                        "isPro": false,
                        "fullname": "Eloi Navet",
                        "user": "eloinavet",
                        "type": "user"
                    },
                    "name": "Eloi Navet",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T12:26:41.553Z",
                    "hidden": false
                },
                {
                    "_id": "6836ffd1ec0f4b688952648d",
                    "name": "Pierrick Coupe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T13:18:32.000Z",
            "submittedOnDailyAt": "2025-05-28T10:53:55.289Z",
            "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via\n  Reinforcement-Optimized LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "65be17eb52edc430281c6a72",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65be17eb52edc430281c6a72/ZeSxgA25KnJ8pqvvy6rdl.png",
                "isPro": false,
                "fullname": "Andrew Zamai",
                "user": "andrewzamai",
                "type": "user"
            },
            "summary": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions.",
            "upvotes": 0,
            "discussionId": "6836ffd2ec0f4b6889526538",
            "ai_summary": "A framework using modular pipelines and reinforcement learning enhances the diagnostic clarity of deep learning models for neurodegenerative dementias by generating causally grounded explanations.",
            "ai_keywords": [
                "Convolutional Neural Networks",
                "Vision Transformers",
                "brain MRIs",
                "Frontotemporal dementia",
                "Alzheimer's disease",
                "Large Language Models",
                "reinforcement learning",
                "modular pipeline"
            ]
        },
        "publishedAt": "2025-05-26T09:18:32.000Z",
        "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via\n  Reinforcement-Optimized LLM Reasoning",
        "summary": "The differential diagnosis of neurodegenerative dementias is a challenging\nclinical task, mainly because of the overlap in symptom presentation and the\nsimilarity of patterns observed in structural neuroimaging. To improve\ndiagnostic efficiency and accuracy, deep learning-based methods such as\nConvolutional Neural Networks and Vision Transformers have been proposed for\nthe automatic classification of brain MRIs. However, despite their strong\npredictive performance, these models find limited clinical utility due to their\nopaque decision making. In this work, we propose a framework that integrates\ntwo core components to enhance diagnostic transparency. First, we introduce a\nmodular pipeline for converting 3D T1-weighted brain MRIs into textual\nradiology reports. Second, we explore the potential of modern Large Language\nModels (LLMs) to assist clinicians in the differential diagnosis between\nFrontotemporal dementia subtypes, Alzheimer's disease, and normal aging based\non the generated reports. To bridge the gap between predictive accuracy and\nexplainability, we employ reinforcement learning to incentivize diagnostic\nreasoning in LLMs. Without requiring supervised reasoning traces or\ndistillation from larger models, our approach enables the emergence of\nstructured diagnostic rationales grounded in neuroimaging findings. Unlike\npost-hoc explainability methods that retrospectively justify model decisions,\nour framework generates diagnostic rationales as part of the inference\nprocess-producing causally grounded explanations that inform and guide the\nmodel's decision-making process. In doing so, our framework matches the\ndiagnostic performance of existing deep learning methods while offering\nrationales that support its diagnostic conclusions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19954.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65be17eb52edc430281c6a72",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65be17eb52edc430281c6a72/ZeSxgA25KnJ8pqvvy6rdl.png",
            "fullname": "Andrew Zamai",
            "name": "andrewzamai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    }
]