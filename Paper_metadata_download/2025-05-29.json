[
    {
        "paper": {
            "id": "2505.22617",
            "authors": [
                {
                    "_id": "6837cd8fc537d91527323667",
                    "user": {
                        "_id": "650eba9555dc1e841746f132",
                        "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                        "isPro": false,
                        "fullname": "Ganqu Cui",
                        "user": "ganqu",
                        "type": "user"
                    },
                    "name": "Ganqu Cui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:09.467Z",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323668",
                    "user": {
                        "_id": "66e3f8fb5d97b5bb46923444",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/DW806I00-00oQAYvD4ocQ.png",
                        "isPro": false,
                        "fullname": "Yuchen Zhang",
                        "user": "YucZhang2003",
                        "type": "user"
                    },
                    "name": "Yuchen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:14.207Z",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323669",
                    "user": {
                        "_id": "65352acb7139c5dd8d9a8590",
                        "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
                        "isPro": false,
                        "fullname": "JiachengChen",
                        "user": "JC-Chen",
                        "type": "user"
                    },
                    "name": "Jiacheng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:20.736Z",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366a",
                    "name": "Lifan Yuan",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366b",
                    "name": "Zhi Wang",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366c",
                    "user": {
                        "_id": "622474f38dc6b0b64f5e903d",
                        "avatarUrl": "/avatars/d6b60a014277a8ec7d564163c5f644aa.svg",
                        "isPro": false,
                        "fullname": "Yuxin Zuo",
                        "user": "yuxinzuo",
                        "type": "user"
                    },
                    "name": "Yuxin Zuo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:12.022Z",
                    "hidden": true
                },
                {
                    "_id": "6837cd8fc537d9152732366d",
                    "user": {
                        "_id": "662f638ba9891e43cc4c5125",
                        "avatarUrl": "/avatars/77c22de5511f9b85d98ec75fb0b5e9be.svg",
                        "isPro": false,
                        "fullname": "Li Haozhan",
                        "user": "Haozhan72",
                        "type": "user"
                    },
                    "name": "Haozhan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:22.720Z",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366e",
                    "name": "Yuchen Fan",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d9152732366f",
                    "name": "Huayu Chen",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323670",
                    "name": "Weize Chen",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323671",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323672",
                    "name": "Hao Peng",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323673",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323674",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323675",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323676",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "6837cd8fc537d91527323677",
                    "user": {
                        "_id": "60cf4bcb1ce3775ebb86e5d5",
                        "avatarUrl": "/avatars/12bcd18d215abf91f297f93007733148.svg",
                        "isPro": false,
                        "fullname": "Ning Ding",
                        "user": "stingning",
                        "type": "user"
                    },
                    "name": "Ning Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:16.809Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:38:45.000Z",
            "submittedOnDailyAt": "2025-05-29T01:38:57.501Z",
            "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "650eba9555dc1e841746f132",
                "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
                "isPro": false,
                "fullname": "Ganqu Cui",
                "user": "ganqu",
                "type": "user"
            },
            "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
            "upvotes": 83,
            "discussionId": "6837cd90c537d9152732369d",
            "githubRepo": "https://github.com/PRIME-RL/Entropy-Mechanism-of-RL",
            "ai_summary": "Entropy dynamics in reinforcement learning with large language models are investigated to prevent policy entropy collapse and improve exploration.",
            "ai_keywords": [
                "policy entropy",
                "reinforcement learning",
                "LLMs",
                "entropy intervention",
                "transformation equation",
                "policy performance",
                "entropy dynamics",
                "covariance",
                "action probability",
                "logits",
                "advantage",
                "Policy Gradient",
                "Clip-Cov",
                "KL-Cov"
            ]
        },
        "publishedAt": "2025-05-28T13:38:45.000Z",
        "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language\n  Models",
        "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with\nLLMs, namely the collapse of policy entropy. Such phenomenon is consistently\nobserved across vast RL runs without entropy intervention, where the policy\nentropy dropped sharply at the early training stage, this diminished\nexploratory ability is always accompanied with the saturation of policy\nperformance. In practice, we establish a transformation equation R=-a*e^H+b\nbetween entropy H and downstream performance R. This empirical law strongly\nindicates that, the policy performance is traded from policy entropy, thus\nbottlenecked by its exhaustion, and the ceiling is fully predictable H=0,\nR=-a+b. Our finding necessitates entropy management for continuous exploration\ntoward scaling compute for RL. To this end, we investigate entropy dynamics\nboth theoretically and empirically. Our derivation highlights that, the change\nin policy entropy is driven by the covariance between action probability and\nthe change in logits, which is proportional to its advantage when using Policy\nGradient-like algorithms. Empirical study shows that, the values of covariance\nterm and entropy differences matched exactly, supporting the theoretical\nconclusion. Moreover, the covariance term stays mostly positive throughout\ntraining, further explaining why policy entropy would decrease monotonically.\nThrough understanding the mechanism behind entropy dynamics, we motivate to\ncontrol entropy by restricting the update of high-covariance tokens.\nSpecifically, we propose two simple yet effective techniques, namely Clip-Cov\nand KL-Cov, which clip and apply KL penalty to tokens with high covariances\nrespectively. Experiments show that these methods encourage exploration, thus\nhelping policy escape entropy collapse and achieve better downstream\nperformance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22617.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "650eba9555dc1e841746f132",
            "avatarUrl": "/avatars/af6f5ee78f161d25ec0afc45d2def8eb.svg",
            "fullname": "Ganqu Cui",
            "name": "ganqu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20411",
            "authors": [
                {
                    "_id": "683735aff42cc8a1d260e677",
                    "user": {
                        "_id": "654e5e094319c75e3e1b6cbc",
                        "avatarUrl": "/avatars/a8889036fa38f80f2d45aea8d1471395.svg",
                        "isPro": false,
                        "fullname": "Ibragim",
                        "user": "ibragim-bad",
                        "type": "user"
                    },
                    "name": "Ibragim Badertdinov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:24.705Z",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e678",
                    "user": {
                        "_id": "644e9ffcd6001776ed77d874",
                        "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
                        "isPro": false,
                        "fullname": "Alexander",
                        "user": "djalexj",
                        "type": "user"
                    },
                    "name": "Alexander Golubev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:13:38.030Z",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e679",
                    "name": "Maksim Nekrashevich",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67a",
                    "user": {
                        "_id": "66214f06d55e5f677e0da9f2",
                        "avatarUrl": "/avatars/dc3b8de4a72b0d10f6bb90594488a9ec.svg",
                        "isPro": false,
                        "fullname": "Anton",
                        "user": "mrshevan",
                        "type": "user"
                    },
                    "name": "Anton Shevtsov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:18:57.150Z",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67b",
                    "user": {
                        "_id": "65e48cb3a4e46e644ec1277d",
                        "avatarUrl": "/avatars/dd2bf04a6f81bf0a0892080af5d485b2.svg",
                        "isPro": false,
                        "fullname": "Simon Karasik",
                        "user": "sbkarasik",
                        "type": "user"
                    },
                    "name": "Simon Karasik",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:26.644Z",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67c",
                    "name": "Andrei Andriushchenko",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67d",
                    "name": "Maria Trofimova",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67e",
                    "name": "Daria Litvintseva",
                    "hidden": false
                },
                {
                    "_id": "683735aff42cc8a1d260e67f",
                    "name": "Boris Yangel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T18:01:00.000Z",
            "submittedOnDailyAt": "2025-05-29T06:59:04.261Z",
            "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
            "submittedOnDailyBy": {
                "_id": "644e9ffcd6001776ed77d874",
                "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
                "isPro": false,
                "fullname": "Alexander",
                "user": "djalexj",
                "type": "user"
            },
            "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
            "upvotes": 63,
            "discussionId": "683735b0f42cc8a1d260e69f",
            "ai_summary": "A novel pipeline extracts real-world, interactive software engineering tasks from GitHub to create SWE-rebench, improving the evaluation of reinforcement learning models in SWE.",
            "ai_keywords": [
                "LLM-based agents",
                "reinforcement learning",
                "software engineering tasks",
                "GitHub repositories",
                "SWE-rebench",
                "contamination-free benchmark",
                "SWE-bench Verified"
            ]
        },
        "publishedAt": "2025-05-26T14:01:00.000Z",
        "title": "SWE-rebench: An Automated Pipeline for Task Collection and\n  Decontaminated Evaluation of Software Engineering Agents",
        "summary": "LLM-based agents have shown promising capabilities in a growing range of\nsoftware engineering (SWE) tasks. However, advancing this field faces two\ncritical challenges. First, high-quality training data is scarce, especially\ndata that reflects real-world SWE scenarios, where agents must interact with\ndevelopment environments, execute code and adapt behavior based on the outcomes\nof their actions. Existing datasets are either limited to one-shot code\ngeneration or comprise small, manually curated collections of interactive\ntasks, lacking both scale and diversity. Second, the lack of fresh interactive\nSWE tasks affects evaluation of rapidly improving models, as static benchmarks\nquickly become outdated due to contamination issues. To address these\nlimitations, we introduce a novel, automated, and scalable pipeline to\ncontinuously extract real-world interactive SWE tasks from diverse GitHub\nrepositories. Using this pipeline, we construct SWE-rebench, a public dataset\ncomprising over 21,000 interactive Python-based SWE tasks, suitable for\nreinforcement learning of SWE agents at scale. Additionally, we use continuous\nsupply of fresh tasks collected using SWE-rebench methodology to build a\ncontamination-free benchmark for agentic software engineering. We compare\nresults of various LLMs on this benchmark to results on SWE-bench Verified and\nshow that performance of some language models might be inflated due to\ncontamination issues.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20411.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644e9ffcd6001776ed77d874",
            "avatarUrl": "/avatars/b93e02caf929679b7e9bc589eed0b689.svg",
            "fullname": "Alexander",
            "name": "djalexj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22651",
            "authors": [
                {
                    "_id": "6837ffdd1bfb4a669ad6de09",
                    "user": {
                        "_id": "662678dfdd43e904ef1dcd03",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
                        "isPro": false,
                        "fullname": "Yi Ding",
                        "user": "Tuwhy",
                        "type": "user"
                    },
                    "name": "Yi Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:19.811Z",
                    "hidden": false
                },
                {
                    "_id": "6837ffdd1bfb4a669ad6de0a",
                    "name": "Ruqi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:58:03.000Z",
            "submittedOnDailyAt": "2025-05-29T06:18:44.515Z",
            "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "662678dfdd43e904ef1dcd03",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
                "isPro": false,
                "fullname": "Yi Ding",
                "user": "Tuwhy",
                "type": "user"
            },
            "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\nbeta for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.",
            "upvotes": 44,
            "discussionId": "6837ffdf1bfb4a669ad6de71",
            "projectPage": "https://dripnowhy.github.io/Sherlock/",
            "githubRepo": "https://github.com/DripNowhy/Sherlock",
            "ai_summary": "Sherlock, a self-correction and self-improvement framework for reasoning vision-language models, enhances accuracy across benchmarks using limited annotated data.",
            "ai_keywords": [
                "vision-language models",
                "self-correction",
                "trajectory-level self-correction",
                "preference data",
                "visual perturbation",
                "dynamic beta",
                "Llama3.2-Vision-11B",
                "LLaVA-CoT",
                "Mulberry",
                "LlamaV-o1"
            ]
        },
        "publishedAt": "2025-05-28T13:58:03.000Z",
        "title": "Sherlock: Self-Correcting Reasoning in Vision-Language Models",
        "summary": "Reasoning Vision-Language Models (VLMs) have shown promising performance on\ncomplex multimodal tasks. However, they still face significant challenges: they\nare highly sensitive to reasoning errors, require large volumes of annotated\ndata or accurate verifiers, and struggle to generalize beyond specific domains.\nTo address these limitations, we explore self-correction as a strategy to\nenhance reasoning VLMs. We first conduct an in-depth analysis of reasoning\nVLMs' self-correction abilities and identify key gaps. Based on our findings,\nwe introduce Sherlock, a self-correction and self-improvement training\nframework. Sherlock introduces a trajectory-level self-correction objective, a\npreference data construction method based on visual perturbation, and a dynamic\nbeta for preference tuning. Once the model acquires self-correction\ncapabilities using only 20k randomly sampled annotated data, it continues to\nself-improve without external supervision. Built on the Llama3.2-Vision-11B\nmodel, Sherlock achieves remarkable results across eight benchmarks, reaching\nan average accuracy of 64.1 with direct generation and 65.4 after\nself-correction. It outperforms LLaVA-CoT (63.2), Mulberry (63.9), and\nLlamaV-o1 (63.4) while using less than 20% of the annotated data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22651.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662678dfdd43e904ef1dcd03",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662678dfdd43e904ef1dcd03/9bbnrnsRV3ylSuZXwcSNv.jpeg",
            "fullname": "Yi Ding",
            "name": "Tuwhy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21136",
            "authors": [
                {
                    "_id": "6837c91ec790885f338b8f27",
                    "user": {
                        "_id": "66c0a08bac74db25de8427ec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
                        "isPro": false,
                        "fullname": "Jintao Zhang",
                        "user": "jt-zhang",
                        "type": "user"
                    },
                    "name": "Jintao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:22.525Z",
                    "hidden": false
                },
                {
                    "_id": "6837c91ec790885f338b8f28",
                    "name": "Xiaoming Xu",
                    "hidden": false
                },
                {
                    "_id": "6837c91ec790885f338b8f29",
                    "name": "Jia Wei",
                    "hidden": false
                },
                {
                    "_id": "6837c91ec790885f338b8f2a",
                    "name": "Haofeng Huang",
                    "hidden": false
                },
                {
                    "_id": "6837c91ec790885f338b8f2b",
                    "name": "Pengle Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837c91ec790885f338b8f2c",
                    "name": "Chendong Xiang",
                    "hidden": false
                },
                {
                    "_id": "6837c91ec790885f338b8f2d",
                    "name": "Jun Zhu",
                    "hidden": false
                },
                {
                    "_id": "6837c91ec790885f338b8f2e",
                    "name": "Jianfei Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
            ],
            "publishedAt": "2025-05-27T12:50:36.000Z",
            "submittedOnDailyAt": "2025-05-29T01:12:49.349Z",
            "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
            "submittedOnDailyBy": {
                "_id": "66c0a08bac74db25de8427ec",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
                "isPro": false,
                "fullname": "Jintao Zhang",
                "user": "jt-zhang",
                "type": "user"
            },
            "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.",
            "upvotes": 33,
            "discussionId": "6837c923c790885f338b90e5",
            "projectPage": "https://github.com/thu-ml/SageAttention",
            "githubRepo": "https://github.com/thu-ml/SageAttention",
            "ai_summary": "SageAttention2++ improves attention efficiency by using FP8 Matmul in FP16, achieving a 3.9x speedup over FlashAttention without losing accuracy.",
            "ai_keywords": [
                "attention",
                "time complexity",
                "sequence length",
                "quantization",
                "matrix multiplications",
                "Matmul",
                "FP8",
                "FP16",
                "SageAttention2",
                "SageAttention2++",
                "FlashAttention",
                "image generation",
                "video generation"
            ]
        },
        "publishedAt": "2025-05-27T08:50:36.000Z",
        "title": "SageAttention2++: A More Efficient Implementation of SageAttention2",
        "summary": "The efficiency of attention is critical because its time complexity grows\nquadratically with sequence length. SageAttention2 addresses this by utilizing\nquantization to accelerate matrix multiplications (Matmul) in attention. To\nfurther accelerate SageAttention2, we propose to utilize the faster instruction\nof FP8 Matmul accumulated in FP16. The instruction is 2x faster than the FP8\nMatmul used in SageAttention2. Our experiments show that SageAttention2++\nachieves a 3.9x speedup over FlashAttention while maintaining the same\nattention accuracy as SageAttention2. This means SageAttention2++ effectively\naccelerates various models, including those for language, image, and video\ngeneration, with negligible end-to-end metrics loss. The code will be available\nat https://github.com/thu-ml/SageAttention.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66c0a08bac74db25de8427ec/e7t1XOditivMjvbyyouoc.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21136.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66c0a08bac74db25de8427ec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c0a08bac74db25de8427ec/9D-piDBZqSt6KNkHImmkv.jpeg",
            "fullname": "Jintao Zhang",
            "name": "jt-zhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22457",
            "authors": [
                {
                    "_id": "68380912e9c1608de91e23f3",
                    "name": "Haonan Wang",
                    "hidden": false
                },
                {
                    "_id": "68380912e9c1608de91e23f4",
                    "name": "Hongfu Liu",
                    "hidden": false
                },
                {
                    "_id": "68380912e9c1608de91e23f5",
                    "name": "Xiangyan Liu",
                    "hidden": false
                },
                {
                    "_id": "68380912e9c1608de91e23f6",
                    "name": "Chao Du",
                    "hidden": false
                },
                {
                    "_id": "68380912e9c1608de91e23f7",
                    "name": "Kenji Kawaguchi",
                    "hidden": false
                },
                {
                    "_id": "68380912e9c1608de91e23f8",
                    "name": "Ye Wang",
                    "hidden": false
                },
                {
                    "_id": "68380912e9c1608de91e23f9",
                    "name": "Tianyu Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T15:13:34.000Z",
            "submittedOnDailyAt": "2025-05-29T05:44:17.072Z",
            "title": "Fostering Video Reasoning via Next-Event Prediction",
            "submittedOnDailyBy": {
                "_id": "63d91b6d255ef6add20e1b38",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
                "isPro": false,
                "fullname": "Tianyu Pang",
                "user": "P2333",
                "type": "user"
            },
            "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
            "upvotes": 27,
            "discussionId": "68380913e9c1608de91e2430",
            "githubRepo": "https://github.com/sail-sg/Video-Next-Event-Prediction",
            "ai_summary": "Next-event prediction (NEP) is proposed as a learning task to enable MLLMs to reason temporally over video inputs, using future video segments as a self-supervised signal.",
            "ai_keywords": [
                "next-token prediction",
                "next-event prediction (NEP)",
                "LLMs",
                "MLLMs",
                "video question answering",
                "video captioning",
                "temporal reasoning",
                "future video segments",
                "past frames",
                "video segments",
                "V1-33K",
                "video instruction-tuning strategies",
                "FutureBench"
            ]
        },
        "publishedAt": "2025-05-28T11:13:34.000Z",
        "title": "Fostering Video Reasoning via Next-Event Prediction",
        "summary": "Next-token prediction serves as the foundational learning task enabling\nreasoning in LLMs. But what should the learning task be when aiming to equip\nMLLMs with temporal reasoning capabilities over video inputs? Existing tasks\nsuch as video question answering often rely on annotations from humans or much\nstronger MLLMs, while video captioning tends to entangle temporal reasoning\nwith spatial information. To address this gap, we propose next-event prediction\n(NEP), a learning task that harnesses future video segments as a rich,\nself-supervised signal to foster temporal reasoning. We segment each video into\npast and future frames: the MLLM takes the past frames as input and predicts a\nsummary of events derived from the future frames, thereby encouraging the model\nto reason temporally in order to complete the task. To support this task, we\ncurate V1-33K, a dataset comprising 33,000 automatically extracted video\nsegments spanning diverse real-world scenarios. We further explore a range of\nvideo instruction-tuning strategies to study their effects on temporal\nreasoning. To evaluate progress, we introduce FutureBench to assess coherence\nin predicting unseen future events. Experiments validate that NEP offers a\nscalable and effective training paradigm for fostering temporal reasoning in\nMLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22457.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d91b6d255ef6add20e1b38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675921369867-63d91b6d255ef6add20e1b38.jpeg",
            "fullname": "Tianyu Pang",
            "name": "P2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18600",
            "authors": [
                {
                    "_id": "6837fe7664391bba7e477747",
                    "user": {
                        "_id": "6628efe14e1fa854f48d3a28",
                        "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
                        "isPro": false,
                        "fullname": "Sangwoo Kim",
                        "user": "bryanswkim",
                        "type": "user"
                    },
                    "name": "Bryan Sangwoo Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:19:04.953Z",
                    "hidden": false
                },
                {
                    "_id": "6837fe7664391bba7e477748",
                    "name": "Jeongsol Kim",
                    "hidden": false
                },
                {
                    "_id": "6837fe7664391bba7e477749",
                    "name": "Jong Chul Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T08:50:08.000Z",
            "submittedOnDailyAt": "2025-05-29T05:07:53.112Z",
            "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment",
            "submittedOnDailyBy": {
                "_id": "6628efe14e1fa854f48d3a28",
                "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
                "isPro": false,
                "fullname": "Sangwoo Kim",
                "user": "bryanswkim",
                "type": "user"
            },
            "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity. Project Page:\nhttps://bryanswkim.github.io/chain-of-zoom/ .",
            "upvotes": 22,
            "discussionId": "6837fe7864391bba7e47779b",
            "projectPage": "https://bryanswkim.github.io/chain-of-zoom/",
            "githubRepo": "https://github.com/bryanswkim/Chain-of-Zoom",
            "ai_summary": "Chain-of-Zoom (CoZ) enhances single-image super-resolution models by using an autoregressive chain of intermediate scale-states and multi-scale-aware prompts to achieve extreme magnifications with high quality.",
            "ai_keywords": [
                "single-image super-resolution",
                "Chain-of-Zoom",
                "autoregressive chain",
                "multi-scale-aware prompts",
                "backbone SR model",
                "diffusion SR model",
                "prompt extractor",
                "Generalized Reward Policy Optimization",
                "critic VLM"
            ]
        },
        "publishedAt": "2025-05-24T04:50:08.000Z",
        "title": "Chain-of-Zoom: Extreme Super-Resolution via Scale Autoregression and\n  Preference Alignment",
        "summary": "Modern single-image super-resolution (SISR) models deliver photo-realistic\nresults at the scale factors on which they are trained, but collapse when asked\nto magnify far beyond that regime. We address this scalability bottleneck with\nChain-of-Zoom (CoZ), a model-agnostic framework that factorizes SISR into an\nautoregressive chain of intermediate scale-states with multi-scale-aware\nprompts. CoZ repeatedly re-uses a backbone SR model, decomposing the\nconditional probability into tractable sub-problems to achieve extreme\nresolutions without additional training. Because visual cues diminish at high\nmagnifications, we augment each zoom step with multi-scale-aware text prompts\ngenerated by a vision-language model (VLM). The prompt extractor itself is\nfine-tuned using Generalized Reward Policy Optimization (GRPO) with a critic\nVLM, aligning text guidance towards human preference. Experiments show that a\nstandard 4x diffusion SR model wrapped in CoZ attains beyond 256x enlargement\nwith high perceptual quality and fidelity. Project Page:\nhttps://bryanswkim.github.io/chain-of-zoom/ .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18600.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6628efe14e1fa854f48d3a28",
            "avatarUrl": "/avatars/aa5421149a07a82b5c2a25978f9b6926.svg",
            "fullname": "Sangwoo Kim",
            "name": "bryanswkim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19075",
            "authors": [
                {
                    "_id": "6837fc484d14d7c8800e8b9c",
                    "user": {
                        "_id": "64c3732de6c3860fba66ceb0",
                        "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
                        "isPro": false,
                        "fullname": "JaeminKim",
                        "user": "kjm981995",
                        "type": "user"
                    },
                    "name": "Jaemin Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:35.589Z",
                    "hidden": false
                },
                {
                    "_id": "6837fc484d14d7c8800e8b9d",
                    "name": "Hangeol Chang",
                    "hidden": false
                },
                {
                    "_id": "6837fc484d14d7c8800e8b9e",
                    "name": "Hyunmin Hwang",
                    "hidden": false
                },
                {
                    "_id": "6837fc484d14d7c8800e8b9f",
                    "name": "Choonghan Kim",
                    "hidden": false
                },
                {
                    "_id": "6837fc484d14d7c8800e8ba0",
                    "name": "Jong Chul Ye",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T10:19:10.000Z",
            "submittedOnDailyAt": "2025-05-29T04:49:39.731Z",
            "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs",
            "submittedOnDailyBy": {
                "_id": "64c3732de6c3860fba66ceb0",
                "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
                "isPro": false,
                "fullname": "JaeminKim",
                "user": "kjm981995",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR",
            "upvotes": 17,
            "discussionId": "6837fc494d14d7c8800e8be6",
            "ai_summary": "UniR, a lightweight reasoning module, enhances Large Language Models with specialized reasoning abilities through modular composition, improving performance and generalization at lower computational costs.",
            "ai_keywords": [
                "Large Language Models",
                "Parameter-Efficient Fine-Tuning",
                "Universal Reasoner",
                "trajectory-level signals",
                "token-level guidance",
                "additive structure",
                "modular composition",
                "Llama3.2",
                "mathematical reasoning",
                "machine translation",
                "cost-efficient",
                "adaptable",
                "robust"
            ]
        },
        "publishedAt": "2025-05-25T06:19:10.000Z",
        "title": "Universal Reasoner: A Single, Composable Plug-and-Play Reasoner for\n  Frozen LLMs",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable general\ncapabilities, but enhancing skills such as reasoning often demands substantial\ncomputational resources and may compromise their generalization. While\nParameter-Efficient Fine-Tuning (PEFT) methods offer a more resource-conscious\nalternative, they typically requires retraining for each LLM backbone due to\narchitectural dependencies. To address these challenges, here we propose\nUniversal Reasoner (UniR) - a single, lightweight, composable, and\nplug-and-play reasoning module that can be used with any frozen LLM to endow it\nwith specialized reasoning capabilities. Specifically, UniR decomposes the\nreward into a standalone reasoning module that is trained independently using\npredefined rewards, effectively translating trajectory-level signals into\ntoken-level guidance. Once trained, UniR can be combined with any frozen LLM at\ninference time by simply adding its output logits to those of the LLM backbone.\nThis additive structure naturally enables modular composition: multiple UniR\nmodules trained for different tasks can be jointly applied by summing their\nlogits, enabling complex reasoning via composition. Experimental results on\nmathematical reasoning and machine translation tasks show that UniR\nsignificantly outperforms existing baseline fine-tuning methods using the\nLlama3.2 model. Furthermore, UniR demonstrates strong weak-to-strong\ngeneralization: reasoning modules trained on smaller models effectively guide\nmuch larger LLMs. This makes UniR a cost-efficient, adaptable, and robust\nsolution for enhancing reasoning in LLMs without compromising their core\ncapabilities. Code is open-sourced at https://github.com/hangeol/UniR",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19075.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c3732de6c3860fba66ceb0",
            "avatarUrl": "/avatars/785783ca08687923053eac641326281f.svg",
            "fullname": "JaeminKim",
            "name": "kjm981995",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22232",
            "authors": [
                {
                    "_id": "683815574d9866c160e88670",
                    "name": "Mehdi Ali",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88671",
                    "user": {
                        "_id": "62fa1d95e8c9c532aa75331c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
                        "isPro": false,
                        "fullname": "Manuel Brack",
                        "user": "mbrack",
                        "type": "user"
                    },
                    "name": "Manuel Brack",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:14.826Z",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88672",
                    "name": "Max Lbbering",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88673",
                    "name": "Elias Wendt",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88674",
                    "name": "Abbas Goher Khan",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88675",
                    "name": "Richard Rutmann",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88676",
                    "name": "Alex Jude",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88677",
                    "name": "Maurice Kraus",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88678",
                    "name": "Alexander Arno Weber",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88679",
                    "name": "Felix Stollenwerk",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e8867a",
                    "name": "David Kaczr",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e8867b",
                    "name": "Florian Mai",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e8867c",
                    "name": "Lucie Flek",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e8867d",
                    "name": "Rafet Sifa",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e8867e",
                    "name": "Nicolas Flores-Herr",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e8867f",
                    "name": "Joachim Khler",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88680",
                    "name": "Patrick Schramowski",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88681",
                    "name": "Michael Fromm",
                    "hidden": false
                },
                {
                    "_id": "683815574d9866c160e88682",
                    "name": "Kristian Kersting",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T11:06:54.000Z",
            "submittedOnDailyAt": "2025-05-29T07:38:07.888Z",
            "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining\n  Data Filtering with Language Models",
            "submittedOnDailyBy": {
                "_id": "62fa1d95e8c9c532aa75331c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
                "isPro": false,
                "fullname": "Manuel Brack",
                "user": "mbrack",
                "type": "user"
            },
            "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.",
            "upvotes": 15,
            "discussionId": "683815594d9866c160e88708",
            "projectPage": "https://huggingface.co/spaces/Jackal-AI/JQL",
            "githubRepo": "https://github.com/JQL-AI/JQL-Annotation-Pipeline",
            "ai_summary": "JQL systematically curates high-quality multilingual training data using pretrained multilingual embeddings, outperforming heuristic methods and improving downstream model training across diverse languages.",
            "ai_keywords": [
                "pretraining",
                "large language models",
                "multilingual datasets",
                "heuristic filtering methods",
                "JQL",
                "lightweight annotators",
                "multilingual embeddings",
                "cross-lingual transferability",
                "annotation pipeline",
                "data retention rates",
                "multilingual data curation"
            ]
        },
        "publishedAt": "2025-05-28T07:06:54.000Z",
        "title": "Judging Quality Across Languages: A Multilingual Approach to Pretraining\n  Data Filtering with Language Models",
        "summary": "High-quality multilingual training data is essential for effectively\npretraining large language models (LLMs). Yet, the availability of suitable\nopen-source multilingual datasets remains limited. Existing state-of-the-art\ndatasets mostly rely on heuristic filtering methods, restricting both their\ncross-lingual transferability and scalability. Here, we introduce JQL, a\nsystematic approach that efficiently curates diverse and high-quality\nmultilingual data at scale while significantly reducing computational demands.\nJQL distills LLMs' annotation capabilities into lightweight annotators based on\npretrained multilingual embeddings. These models exhibit robust multilingual\nand cross-lingual performance, even for languages and scripts unseen during\ntraining. Evaluated empirically across 35 languages, the resulting annotation\npipeline substantially outperforms current heuristic filtering methods like\nFineweb2. JQL notably enhances downstream model training quality and increases\ndata retention rates. Our research provides practical insights and valuable\nresources for multilingual data curation, raising the standards of multilingual\ndataset development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22232.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62fa1d95e8c9c532aa75331c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62fa1d95e8c9c532aa75331c/WFfk_n8gOj845pSkfdazA.jpeg",
            "fullname": "Manuel Brack",
            "name": "mbrack",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21887",
            "authors": [
                {
                    "_id": "6837e3400aa18c6f96fe4876",
                    "user": {
                        "_id": "656864e12d73834278a8dea7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                        "isPro": true,
                        "fullname": "Ahmed Heakl",
                        "user": "ahmedheakl",
                        "type": "user"
                    },
                    "name": "Ahmed Heakl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:49.456Z",
                    "hidden": false
                },
                {
                    "_id": "6837e3400aa18c6f96fe4877",
                    "name": "Yahia Salaheldin Shaaban",
                    "hidden": false
                },
                {
                    "_id": "6837e3400aa18c6f96fe4878",
                    "name": "Martin Takac",
                    "hidden": false
                },
                {
                    "_id": "6837e3400aa18c6f96fe4879",
                    "name": "Salem Lahlou",
                    "hidden": false
                },
                {
                    "_id": "6837e3400aa18c6f96fe487a",
                    "name": "Zangir Iklassov",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
            ],
            "publishedAt": "2025-05-28T02:03:31.000Z",
            "submittedOnDailyAt": "2025-05-29T03:02:06.399Z",
            "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem",
            "submittedOnDailyBy": {
                "_id": "656864e12d73834278a8dea7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                "isPro": true,
                "fullname": "Ahmed Heakl",
                "user": "ahmedheakl",
                "type": "user"
            },
            "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.",
            "upvotes": 14,
            "discussionId": "6837e3410aa18c6f96fe48b8",
            "githubRepo": "https://github.com/yehias21/vrp-benchmarks",
            "ai_summary": "SVRPBench introduces a new benchmark for vehicle routing under uncertainty, simulating realistic urban conditions and highlighting the limitations of state-of-the-art RL solvers.",
            "ai_keywords": [
                "vehicle routing",
                "SVRPBench",
                "time-dependent congestion",
                "log-normal delays",
                "probabilistic accidents",
                "multi-depot",
                "multi-vehicle",
                "state-of-the-art RL solvers",
                "POMO",
                "AM",
                "distributional shift",
                "classical methods",
                "metaheuristic methods"
            ]
        },
        "publishedAt": "2025-05-27T22:03:31.000Z",
        "title": "SVRPBench: A Realistic Benchmark for Stochastic Vehicle Routing Problem",
        "summary": "Robust routing under uncertainty is central to real-world logistics, yet most\nbenchmarks assume static, idealized settings. We present SVRPBench, the first\nopen benchmark to capture high-fidelity stochastic dynamics in vehicle routing\nat urban scale. Spanning more than 500 instances with up to 1000 customers, it\nsimulates realistic delivery conditions: time-dependent congestion, log-normal\ndelays, probabilistic accidents, and empirically grounded time windows for\nresidential and commercial clients. Our pipeline generates diverse,\nconstraint-rich scenarios, including multi-depot and multi-vehicle setups.\nBenchmarking reveals that state-of-the-art RL solvers like POMO and AM degrade\nby over 20% under distributional shift, while classical and metaheuristic\nmethods remain robust. To enable reproducible research, we release the dataset\nand evaluation suite. SVRPBench challenges the community to design solvers that\ngeneralize beyond synthetic assumptions and adapt to real-world uncertainty.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Xa51WJrOIV-xl7A-Ry4t2.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/eS_bEDm2o2WAbn3YPS2XR.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/jaG4WahYyrQGaalZObNew.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/mMKY5jn0SCJPxxR8_mssx.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21887.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "fullname": "Ahmed Heakl",
            "name": "ahmedheakl",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 39
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22129",
            "authors": [
                {
                    "_id": "6837dae6e9b21653755a05d4",
                    "user": {
                        "_id": "64c71a5647418a0a59e5c7cb",
                        "avatarUrl": "/avatars/a99ab24c0c19b1399d2e6795fb9d7000.svg",
                        "isPro": false,
                        "fullname": "Jinhong Ni",
                        "user": "mcleanie",
                        "type": "user"
                    },
                    "name": "Jinhong Ni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:03.476Z",
                    "hidden": false
                },
                {
                    "_id": "6837dae6e9b21653755a05d5",
                    "user": {
                        "_id": "65434daa5a36a8774d0e2271",
                        "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
                        "isPro": false,
                        "fullname": "Allen Zhang",
                        "user": "allencbzhang",
                        "type": "user"
                    },
                    "name": "Chang-Bin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:59.930Z",
                    "hidden": false
                },
                {
                    "_id": "6837dae6e9b21653755a05d6",
                    "name": "Qiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837dae6e9b21653755a05d7",
                    "name": "Jing Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T08:54:04.000Z",
            "submittedOnDailyAt": "2025-05-29T02:38:27.338Z",
            "title": "What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?",
            "submittedOnDailyBy": {
                "_id": "65434daa5a36a8774d0e2271",
                "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
                "isPro": false,
                "fullname": "Allen Zhang",
                "user": "allencbzhang",
                "type": "user"
            },
            "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released.",
            "upvotes": 13,
            "discussionId": "6837daece9b21653755a0791",
            "ai_summary": "Analysis of fine-tuning diffusion models for panoramic image generation reveals distinct roles of attention module matrices and introduces UniPano, a memory-efficient and speed-enhanced baseline framework.",
            "ai_keywords": [
                "text-to-image diffusion models",
                "Stable Diffusion",
                "low-rank adaptation",
                "pre-trained diffusion models",
                "attention modules",
                "query matrices",
                "key matrices",
                "value matrices",
                "output weight matrices",
                "panoramic image generation",
                "domain gap",
                "common information",
                "pre-trained knowledge",
                "UniPano",
                "end-to-end panorama generation",
                "memory usage",
                "training time"
            ]
        },
        "publishedAt": "2025-05-28T04:54:04.000Z",
        "title": "What Makes for Text to 360-degree Panorama Generation with Stable\n  Diffusion?",
        "summary": "Recent prosperity of text-to-image diffusion models, e.g. Stable Diffusion,\nhas stimulated research to adapt them to 360-degree panorama generation. Prior\nwork has demonstrated the feasibility of using conventional low-rank adaptation\ntechniques on pre-trained diffusion models to generate panoramic images.\nHowever, the substantial domain gap between perspective and panoramic images\nraises questions about the underlying mechanisms enabling this empirical\nsuccess. We hypothesize and examine that the trainable counterparts exhibit\ndistinct behaviors when fine-tuned on panoramic data, and such an adaptation\nconceals some intrinsic mechanism to leverage the prior knowledge within the\npre-trained diffusion models. Our analysis reveals the following: 1) the query\nand key matrices in the attention modules are responsible for common\ninformation that can be shared between the panoramic and perspective domains,\nthus are less relevant to panorama generation; and 2) the value and output\nweight matrices specialize in adapting pre-trained knowledge to the panoramic\ndomain, playing a more critical role during fine-tuning for panorama\ngeneration. We empirically verify these insights by introducing a simple\nframework called UniPano, with the objective of establishing an elegant\nbaseline for future research. UniPano not only outperforms existing methods but\nalso significantly reduces memory usage and training time compared to prior\ndual-branch approaches, making it scalable for end-to-end panorama generation\nwith higher resolution. The code will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22129.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65434daa5a36a8774d0e2271",
            "avatarUrl": "/avatars/abc3ddec72072121130d581e32cd9045.svg",
            "fullname": "Allen Zhang",
            "name": "allencbzhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22202",
            "authors": [
                {
                    "_id": "6837eff312d1f7a138bd09b3",
                    "name": "Hyeonbin Hwang",
                    "hidden": false
                },
                {
                    "_id": "6837eff312d1f7a138bd09b4",
                    "name": "Byeongguk Jeon",
                    "hidden": false
                },
                {
                    "_id": "6837eff312d1f7a138bd09b5",
                    "user": {
                        "_id": "6469949654873f0043b09c22",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
                        "isPro": false,
                        "fullname": "Seungone Kim",
                        "user": "seungone",
                        "type": "user"
                    },
                    "name": "Seungone Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:19:02.694Z",
                    "hidden": false
                },
                {
                    "_id": "6837eff312d1f7a138bd09b6",
                    "name": "Jiyeon Kim",
                    "hidden": false
                },
                {
                    "_id": "6837eff312d1f7a138bd09b7",
                    "name": "Hoyeon Chang",
                    "hidden": false
                },
                {
                    "_id": "6837eff312d1f7a138bd09b8",
                    "name": "Sohee Yang",
                    "hidden": false
                },
                {
                    "_id": "6837eff312d1f7a138bd09b9",
                    "name": "Seungpil Won",
                    "hidden": false
                },
                {
                    "_id": "6837eff312d1f7a138bd09ba",
                    "name": "Dohaeng Lee",
                    "hidden": false
                },
                {
                    "_id": "6837eff312d1f7a138bd09bb",
                    "name": "Youbin Ahn",
                    "hidden": false
                },
                {
                    "_id": "6837eff312d1f7a138bd09bc",
                    "name": "Minjoon Seo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T10:28:35.000Z",
            "submittedOnDailyAt": "2025-05-29T04:00:37.847Z",
            "title": "Let's Predict Sentence by Sentence",
            "submittedOnDailyBy": {
                "_id": "647eaaf61a1fcad2fdc5d1ef",
                "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
                "isPro": false,
                "fullname": "Hyeonbin Hwang ",
                "user": "hbin0701",
                "type": "user"
            },
            "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.",
            "upvotes": 12,
            "discussionId": "6837eff412d1f7a138bd0a3e",
            "githubRepo": "https://github.com/hbin0701/pred-sent",
            "ai_summary": "Pretrained language models can adapt to operate in sentence space, reason over structured semantic units, and achieve competitive performance with Chain-of-Thought while reducing inference-time FLOPs.",
            "ai_keywords": [
                "autoregressive language models",
                "semantic embeddings",
                "contextual embeddings",
                "next-sentence prediction",
                "Chain-of-Thought",
                "SentenceLens"
            ]
        },
        "publishedAt": "2025-05-28T06:28:35.000Z",
        "title": "Let's Predict Sentence by Sentence",
        "summary": "Autoregressive language models (LMs) generate one token at a time, yet human\nreasoning operates over higher-level abstractions - sentences, propositions,\nand concepts. This contrast raises a central question- Can LMs likewise learn\nto reason over structured semantic units rather than raw token sequences? In\nthis work, we investigate whether pretrained LMs can be lifted into such\nabstract reasoning spaces by building on their learned representations. We\npresent a framework that adapts a pretrained token-level LM to operate in\nsentence space by autoregressively predicting continuous embeddings of next\nsentences. We explore two embedding paradigms inspired by classical\nrepresentation learning: 1) semantic embeddings, learned via autoencoding to\npreserve surface meaning; and 2) contextual embeddings, trained via\nnext-sentence prediction to encode anticipatory structure. We evaluate both\nunder two inference regimes: Discretized, which decodes each predicted\nembedding into text before re-encoding; and Continuous, which reasons entirely\nin embedding space for improved efficiency. Across four domains - mathematics,\nlogic, commonsense, and planning - contextual embeddings under continuous\ninference show competitive performance with Chain-of-Thought (CoT) while\nreducing inference-time FLOPs on average by half. We also present early signs\nof scalability and modular adaptation. Finally, to visualize latent\ntrajectories, we introduce SentenceLens, a diagnostic tool that decodes\nintermediate model states into interpretable sentences. Together, our results\nindicate that pretrained LMs can effectively transition to abstract, structured\nreasoning within latent embedding spaces.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22202.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647eaaf61a1fcad2fdc5d1ef",
            "avatarUrl": "/avatars/596d7a61d6e6227d38b661210a32fed0.svg",
            "fullname": "Hyeonbin Hwang ",
            "name": "hbin0701",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.19187",
            "authors": [
                {
                    "_id": "6837210455e9bab4e9c302b1",
                    "user": {
                        "_id": "6002c316698168af3bb9f4a6",
                        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
                        "isPro": false,
                        "fullname": "yangxiao",
                        "user": "YangXiao-nlp",
                        "type": "user"
                    },
                    "name": "Yang Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:13:52.090Z",
                    "hidden": false
                },
                {
                    "_id": "6837210455e9bab4e9c302b2",
                    "name": "Jiashuo Wang",
                    "hidden": false
                },
                {
                    "_id": "6837210455e9bab4e9c302b3",
                    "name": "Ruifeng Yuan",
                    "hidden": false
                },
                {
                    "_id": "6837210455e9bab4e9c302b4",
                    "name": "Chunpu Xu",
                    "hidden": false
                },
                {
                    "_id": "6837210455e9bab4e9c302b5",
                    "name": "Kaishuai Xu",
                    "hidden": false
                },
                {
                    "_id": "6837210455e9bab4e9c302b6",
                    "name": "Wenjie Li",
                    "hidden": false
                },
                {
                    "_id": "6837210455e9bab4e9c302b7",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T15:17:57.000Z",
            "submittedOnDailyAt": "2025-05-29T01:05:08.605Z",
            "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling",
            "submittedOnDailyBy": {
                "_id": "6002c316698168af3bb9f4a6",
                "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
                "isPro": false,
                "fullname": "yangxiao",
                "user": "YangXiao-nlp",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.",
            "upvotes": 12,
            "discussionId": "6837210555e9bab4e9c302f2",
            "ai_summary": "A framework called PIR refines the importance of reasoning steps in large language models by pruning low-importance functional elements, leading to more concise reasoning chains with improved accuracy and reduced computational demands.",
            "ai_keywords": [
                "large language models",
                "chain-of-thought",
                "large reasoning models",
                "progressive reasoning",
                "functional elements",
                "perplexity-based importance refinement",
                "token usage",
                "reasoning benchmarks",
                "AIME",
                "AMC",
                "GPQA Diamond"
            ]
        },
        "publishedAt": "2025-05-25T11:17:57.000Z",
        "title": "LIMOPro: Reasoning Refinement for Efficient and Effective Test-time\n  Scaling",
        "summary": "Large language models (LLMs) have demonstrated remarkable reasoning\ncapabilities through test-time scaling approaches, particularly when fine-tuned\nwith chain-of-thought (CoT) data distilled from more powerful large reasoning\nmodels (LRMs). However, these reasoning chains often contain verbose elements\nthat mirror human problem-solving, categorized as progressive reasoning (the\nessential solution development path) and functional elements (verification\nprocesses, alternative solution approaches, and error corrections). While\nprogressive reasoning is crucial, the functional elements significantly\nincrease computational demands during test-time inference. We introduce PIR\n(Perplexity-based Importance Refinement), a principled framework that\nquantitatively evaluates the importance of each reasoning step based on its\nimpact on answer prediction confidence. PIR systematically identifies and\nselectively prunes only low-importance functional steps while preserving\nprogressive reasoning components, creating optimized training data that\nmaintains the integrity of the core solution path while reducing verbosity.\nModels fine-tuned on PIR-optimized data exhibit superior test-time scaling\nproperties, generating more concise reasoning chains while achieving improved\naccuracy (+0.9\\% to +6.6\\%) with significantly reduced token usage (-3\\% to\n-41\\%) across challenging reasoning benchmarks (AIME, AMC, and GPQA Diamond).\nOur approach demonstrates strong generalizability across different model sizes,\ndata sources, and token budgets, offering a practical solution for deploying\nreasoning-capable LLMs in scenarios where efficient test-time scaling, response\ntime, and computational efficiency are valuable constraints.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19187.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "fullname": "yangxiao",
            "name": "YangXiao-nlp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18882",
            "authors": [
                {
                    "_id": "6835fd8e869613d0fd6dc2fc",
                    "user": {
                        "_id": "6543463f91a22f5a1189181a",
                        "avatarUrl": "/avatars/6010ef307918e57e44adf2f60080fe04.svg",
                        "isPro": false,
                        "fullname": "Yuchen Wu",
                        "user": "wick1d",
                        "type": "user"
                    },
                    "name": "Yuchen Wu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-27T17:59:44.169Z",
                    "hidden": false
                },
                {
                    "_id": "6835fd8e869613d0fd6dc2fd",
                    "user": {
                        "_id": "65822da0117b524ef3e8dbcc",
                        "avatarUrl": "/avatars/8d3b0eed5144ff452ca2a49a0f67d7c4.svg",
                        "isPro": false,
                        "fullname": "Edward Sun",
                        "user": "EdwardoSunny",
                        "type": "user"
                    },
                    "name": "Edward Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:18:48.711Z",
                    "hidden": false
                },
                {
                    "_id": "6835fd8e869613d0fd6dc2fe",
                    "name": "Kaijie Zhu",
                    "hidden": false
                },
                {
                    "_id": "6835fd8e869613d0fd6dc2ff",
                    "name": "Jianxun Lian",
                    "hidden": false
                },
                {
                    "_id": "6835fd8e869613d0fd6dc300",
                    "name": "Jose Hernandez-Orallo",
                    "hidden": false
                },
                {
                    "_id": "6835fd8e869613d0fd6dc301",
                    "name": "Aylin Caliskan",
                    "hidden": false
                },
                {
                    "_id": "6835fd8e869613d0fd6dc302",
                    "name": "Jindong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T21:37:10.000Z",
            "submittedOnDailyAt": "2025-05-29T17:00:58.092Z",
            "title": "Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent\n  Approach",
            "submittedOnDailyBy": {
                "_id": "6543463f91a22f5a1189181a",
                "avatarUrl": "/avatars/6010ef307918e57e44adf2f60080fe04.svg",
                "isPro": false,
                "fullname": "Yuchen Wu",
                "user": "wick1d",
                "type": "user"
            },
            "summary": "Large language models (LLMs) typically generate identical or similar\nresponses for all users given the same prompt, posing serious safety risks in\nhigh-stakes applications where user vulnerabilities differ widely. Existing\nsafety evaluations primarily rely on context-independent metrics - such as\nfactuality, bias, or toxicity - overlooking the fact that the same response may\ncarry divergent risks depending on the user's background or condition. We\nintroduce personalized safety to fill this gap and present PENGUIN - a\nbenchmark comprising 14,000 scenarios across seven sensitive domains with both\ncontext-rich and context-free variants. Evaluating six leading LLMs, we\ndemonstrate that personalized user information significantly improves safety\nscores by 43.2%, confirming the effectiveness of personalization in safety\nalignment. However, not all context attributes contribute equally to safety\nenhancement. To address this, we develop RAISE - a training-free, two-stage\nagent framework that strategically acquires user-specific background. RAISE\nimproves safety scores by up to 31.6% over six vanilla LLMs, while maintaining\na low interaction cost of just 2.7 user queries on average. Our findings\nhighlight the importance of selective information gathering in safety-critical\ndomains and offer a practical solution for personalizing LLM responses without\nmodel retraining. This work establishes a foundation for safety research that\nadapts to individual user contexts rather than assuming a universal harm\nstandard.",
            "upvotes": 12,
            "discussionId": "6835fd90869613d0fd6dc37f",
            "projectPage": "https://chibadaisuki.github.io/personalized-safety.io/",
            "githubRepo": "https://github.com/Chibadaisuki/Personalized-Safety-in-LLMs",
            "ai_summary": "Introducing personalized safety for LLMs through PENGUIN and RAISE frameworks Enhances safety scores by leveraging user-specific information without retraining models.",
            "ai_keywords": [
                "large language models (LLMs)",
                "personalized safety",
                "PENGUIN",
                "context-rich",
                "context-free",
                "safety alignment",
                "RAISE",
                "user-specific background",
                "safety scores",
                "selective information gathering"
            ]
        },
        "publishedAt": "2025-05-24T17:37:10.000Z",
        "title": "Personalized Safety in LLMs: A Benchmark and A Planning-Based Agent\n  Approach",
        "summary": "Large language models (LLMs) typically generate identical or similar\nresponses for all users given the same prompt, posing serious safety risks in\nhigh-stakes applications where user vulnerabilities differ widely. Existing\nsafety evaluations primarily rely on context-independent metrics - such as\nfactuality, bias, or toxicity - overlooking the fact that the same response may\ncarry divergent risks depending on the user's background or condition. We\nintroduce personalized safety to fill this gap and present PENGUIN - a\nbenchmark comprising 14,000 scenarios across seven sensitive domains with both\ncontext-rich and context-free variants. Evaluating six leading LLMs, we\ndemonstrate that personalized user information significantly improves safety\nscores by 43.2%, confirming the effectiveness of personalization in safety\nalignment. However, not all context attributes contribute equally to safety\nenhancement. To address this, we develop RAISE - a training-free, two-stage\nagent framework that strategically acquires user-specific background. RAISE\nimproves safety scores by up to 31.6% over six vanilla LLMs, while maintaining\na low interaction cost of just 2.7 user queries on average. Our findings\nhighlight the importance of selective information gathering in safety-critical\ndomains and offer a practical solution for personalizing LLM responses without\nmodel retraining. This work establishes a foundation for safety research that\nadapts to individual user contexts rather than assuming a universal harm\nstandard.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18882.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6543463f91a22f5a1189181a",
            "avatarUrl": "/avatars/6010ef307918e57e44adf2f60080fe04.svg",
            "fullname": "Yuchen Wu",
            "name": "wick1d",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18227",
            "authors": [
                {
                    "_id": "683866e4ae92c7d496fdd866",
                    "name": "Zhenglun Kong",
                    "hidden": false
                },
                {
                    "_id": "683866e4ae92c7d496fdd867",
                    "user": {
                        "_id": "6552d3f69fa2f7205a1f94a2",
                        "avatarUrl": "/avatars/8b857fe89586b25b54864da91ce1b35a.svg",
                        "isPro": false,
                        "fullname": "Yize Li",
                        "user": "yeezlee",
                        "type": "user"
                    },
                    "name": "Yize Li",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-29T15:39:45.735Z",
                    "hidden": false
                },
                {
                    "_id": "683866e4ae92c7d496fdd868",
                    "name": "Fanhu Zeng",
                    "hidden": false
                },
                {
                    "_id": "683866e4ae92c7d496fdd869",
                    "name": "Lei Xin",
                    "hidden": false
                },
                {
                    "_id": "683866e4ae92c7d496fdd86a",
                    "name": "Shvat Messica",
                    "hidden": false
                },
                {
                    "_id": "683866e4ae92c7d496fdd86b",
                    "name": "Xue Lin",
                    "hidden": false
                },
                {
                    "_id": "683866e4ae92c7d496fdd86c",
                    "name": "Pu Zhao",
                    "hidden": false
                },
                {
                    "_id": "683866e4ae92c7d496fdd86d",
                    "name": "Manolis Kellis",
                    "hidden": false
                },
                {
                    "_id": "683866e4ae92c7d496fdd86e",
                    "name": "Hao Tang",
                    "hidden": false
                },
                {
                    "_id": "683866e4ae92c7d496fdd86f",
                    "name": "Marinka Zitnik",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T11:30:30.000Z",
            "submittedOnDailyAt": "2025-05-29T12:26:37.311Z",
            "title": "Token Reduction Should Go Beyond Efficiency in Generative Models -- From\n  Vision, Language to Multimodality",
            "submittedOnDailyBy": {
                "_id": "5f2c36551ebc8c6ede2f0e53",
                "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
                "isPro": false,
                "fullname": "Tony Kong",
                "user": "TonyK",
                "type": "user"
            },
            "summary": "In Transformer architectures, tokens\\textemdash discrete units derived from\nraw data\\textemdash are formed by segmenting inputs into fixed-length chunks.\nEach token is then mapped to an embedding, enabling parallel attention\ncomputations while preserving the input's essential information. Due to the\nquadratic computational complexity of transformer self-attention mechanisms,\ntoken reduction has primarily been used as an efficiency strategy. This is\nespecially true in single vision and language domains, where it helps balance\ncomputational costs, memory usage, and inference latency. Despite these\nadvances, this paper argues that token reduction should transcend its\ntraditional efficiency-oriented role in the era of large generative models.\nInstead, we position it as a fundamental principle in generative modeling,\ncritically influencing both model architecture and broader applications.\nSpecifically, we contend that across vision, language, and multimodal systems,\ntoken reduction can: (i) facilitate deeper multimodal integration and\nalignment, (ii) mitigate \"overthinking\" and hallucinations, (iii) maintain\ncoherence over long inputs, and (iv) enhance training stability, etc. We\nreframe token reduction as more than an efficiency measure. By doing so, we\noutline promising future directions, including algorithm design, reinforcement\nlearning-guided token reduction, token optimization for in-context learning,\nand broader ML and scientific domains. We highlight its potential to drive new\nmodel architectures and learning strategies that improve robustness, increase\ninterpretability, and better align with the objectives of generative modeling.",
            "upvotes": 12,
            "discussionId": "683866e5ae92c7d496fdd8a6",
            "projectPage": "https://github.com/ZLKong/Awesome-Token-Compression-Reduction/tree/main?tab=readme-ov-file",
            "githubRepo": "https://github.com/ZLKong/Awesome-Token-Compression-Reduction/tree/main",
            "ai_summary": "Token reduction in Transformer models, beyond efficiency, enhances multimodal integration, reduces hallucinations, and improves training stability in generative modeling.",
            "ai_keywords": [
                "Transformer architectures",
                "token reduction",
                "self-attention mechanisms",
                "multimodal integration",
                "hallucinations",
                "training stability",
                "generative modeling"
            ]
        },
        "publishedAt": "2025-05-23T07:30:30.000Z",
        "title": "Token Reduction Should Go Beyond Efficiency in Generative Models -- From\n  Vision, Language to Multimodality",
        "summary": "In Transformer architectures, tokens\\textemdash discrete units derived from\nraw data\\textemdash are formed by segmenting inputs into fixed-length chunks.\nEach token is then mapped to an embedding, enabling parallel attention\ncomputations while preserving the input's essential information. Due to the\nquadratic computational complexity of transformer self-attention mechanisms,\ntoken reduction has primarily been used as an efficiency strategy. This is\nespecially true in single vision and language domains, where it helps balance\ncomputational costs, memory usage, and inference latency. Despite these\nadvances, this paper argues that token reduction should transcend its\ntraditional efficiency-oriented role in the era of large generative models.\nInstead, we position it as a fundamental principle in generative modeling,\ncritically influencing both model architecture and broader applications.\nSpecifically, we contend that across vision, language, and multimodal systems,\ntoken reduction can: (i) facilitate deeper multimodal integration and\nalignment, (ii) mitigate \"overthinking\" and hallucinations, (iii) maintain\ncoherence over long inputs, and (iv) enhance training stability, etc. We\nreframe token reduction as more than an efficiency measure. By doing so, we\noutline promising future directions, including algorithm design, reinforcement\nlearning-guided token reduction, token optimization for in-context learning,\nand broader ML and scientific domains. We highlight its potential to drive new\nmodel architectures and learning strategies that improve robustness, increase\ninterpretability, and better align with the objectives of generative modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18227.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "5f2c36551ebc8c6ede2f0e53",
            "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
            "fullname": "Tony Kong",
            "name": "TonyK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17663",
            "authors": [
                {
                    "_id": "6833c6ff97966d18e7b995b0",
                    "user": {
                        "_id": "6002c316698168af3bb9f4a6",
                        "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
                        "isPro": false,
                        "fullname": "yangxiao",
                        "user": "YangXiao-nlp",
                        "type": "user"
                    },
                    "name": "Yang Xiao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T19:14:41.639Z",
                    "hidden": false
                },
                {
                    "_id": "6833c6ff97966d18e7b995b1",
                    "name": "Jiashuo Wang",
                    "hidden": false
                },
                {
                    "_id": "6833c6ff97966d18e7b995b2",
                    "name": "Qiancheng Xu",
                    "hidden": false
                },
                {
                    "_id": "6833c6ff97966d18e7b995b3",
                    "name": "Changhe Song",
                    "hidden": false
                },
                {
                    "_id": "6833c6ff97966d18e7b995b4",
                    "name": "Chunpu Xu",
                    "hidden": false
                },
                {
                    "_id": "6833c6ff97966d18e7b995b5",
                    "name": "Yi Cheng",
                    "hidden": false
                },
                {
                    "_id": "6833c6ff97966d18e7b995b6",
                    "name": "Wenjie Li",
                    "hidden": false
                },
                {
                    "_id": "6833c6ff97966d18e7b995b7",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T09:27:40.000Z",
            "submittedOnDailyAt": "2025-05-29T01:05:57.079Z",
            "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States",
            "submittedOnDailyBy": {
                "_id": "6002c316698168af3bb9f4a6",
                "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
                "isPro": false,
                "fullname": "yangxiao",
                "user": "YangXiao-nlp",
                "type": "user"
            },
            "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present DynToM, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.",
            "upvotes": 12,
            "discussionId": "6833c70097966d18e7b99616",
            "ai_summary": "The DynToM benchmark evaluates LLMs' ability to track and understand the temporal progression of mental states, revealing significant gaps compared to human performance.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Theory of Mind (ToM)",
                "DynToM",
                "social contexts",
                "mental states",
                "temporal progression",
                "evaluation framework",
                "state-of-the-art LLMs",
                "performance gap",
                "dynamic mental states"
            ]
        },
        "publishedAt": "2025-05-23T05:27:40.000Z",
        "title": "Towards Dynamic Theory of Mind: Evaluating LLM Adaptation to Temporal\n  Evolution of Human States",
        "summary": "As Large Language Models (LLMs) increasingly participate in human-AI\ninteractions, evaluating their Theory of Mind (ToM) capabilities - particularly\ntheir ability to track dynamic mental states - becomes crucial. While existing\nbenchmarks assess basic ToM abilities, they predominantly focus on static\nsnapshots of mental states, overlooking the temporal evolution that\ncharacterizes real-world social interactions. We present DynToM, a\nnovel benchmark specifically designed to evaluate LLMs' ability to understand\nand track the temporal progression of mental states across interconnected\nscenarios. Through a systematic four-step framework, we generate 1,100 social\ncontexts encompassing 5,500 scenarios and 78,100 questions, each validated for\nrealism and quality. Our comprehensive evaluation of ten state-of-the-art LLMs\nreveals that their average performance underperforms humans by 44.7\\%, with\nperformance degrading significantly when tracking and reasoning about the shift\nof mental states. This performance gap highlights fundamental limitations in\ncurrent LLMs' ability to model the dynamic nature of human mental states.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17663.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6002c316698168af3bb9f4a6",
            "avatarUrl": "/avatars/1028860b59951be04e94b126b0985dc0.svg",
            "fullname": "yangxiao",
            "name": "YangXiao-nlp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.11821",
            "authors": [
                {
                    "_id": "6832def11f8965bd58214baf",
                    "name": "Siliang Zeng",
                    "hidden": false
                },
                {
                    "_id": "6832def11f8965bd58214bb0",
                    "user": {
                        "_id": "662a037140363679757cc292",
                        "avatarUrl": "/avatars/fc41455247cc0744f24ac9255293ddf5.svg",
                        "isPro": false,
                        "fullname": "Quan Wei",
                        "user": "quanwei0",
                        "type": "user"
                    },
                    "name": "Quan Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:18:40.227Z",
                    "hidden": false
                },
                {
                    "_id": "6832def11f8965bd58214bb1",
                    "name": "William Brown",
                    "hidden": false
                },
                {
                    "_id": "6832def11f8965bd58214bb2",
                    "name": "Oana Frunza",
                    "hidden": false
                },
                {
                    "_id": "6832def11f8965bd58214bb3",
                    "name": "Yuriy Nevmyvaka",
                    "hidden": false
                },
                {
                    "_id": "6832def11f8965bd58214bb4",
                    "name": "Mingyi Hong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-17T04:09:46.000Z",
            "submittedOnDailyAt": "2025-05-29T18:50:27.142Z",
            "title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit\n  Assignment",
            "submittedOnDailyBy": {
                "_id": "662a037140363679757cc292",
                "avatarUrl": "/avatars/fc41455247cc0744f24ac9255293ddf5.svg",
                "isPro": false,
                "fullname": "Quan Wei",
                "user": "quanwei0",
                "type": "user"
            },
            "summary": "This paper investigates approaches to enhance the reasoning capabilities of\nLarge Language Model (LLM) agents using Reinforcement Learning (RL).\nSpecifically, we focus on multi-turn tool-use scenarios, which can be naturally\nmodeled as Markov Decision Processes (MDPs). While existing approaches often\ntrain multi-turn LLM agents with trajectory-level advantage estimation in\nbandit settings, they struggle with turn-level credit assignment across\nmultiple decision steps, limiting their performance on multi-turn reasoning\ntasks. To address this, we introduce a fine-grained turn-level advantage\nestimation strategy to enable more precise credit assignment in multi-turn\nagent interactions. The strategy is general and can be incorporated into\nvarious RL algorithms such as Group Relative Preference Optimization (GRPO).\nOur experimental evaluation on multi-turn reasoning and search-based tool-use\ntasks with GRPO implementations highlights the effectiveness of the MDP\nframework and the turn-level credit assignment in advancing the multi-turn\nreasoning capabilities of LLM agents in complex decision-making settings. Our\nmethod achieves 100% success in tool execution and 50% accuracy in exact answer\nmatching, significantly outperforming baselines, which fail to invoke tools and\nachieve only 20-30% exact match accuracy.",
            "upvotes": 12,
            "discussionId": "6832def21f8965bd58214be7",
            "githubRepo": "https://github.com/SiliangZeng/Multi-Turn-RL-Agent",
            "ai_summary": "Reinforcement Learning with turn-level credit assignment enhances Large Language Model reasoning capabilities in multi-turn tool-use scenarios.",
            "ai_keywords": [
                "Large Language Model",
                "Reinforcement Learning",
                "multi-turn tool-use",
                "Markov Decision Processes",
                "trajectory-level advantage estimation",
                "turn-level credit assignment",
                "Group Relative Preference Optimization",
                "multi-turn reasoning",
                "search-based tool-use",
                "tool execution",
                "exact answer matching"
            ]
        },
        "publishedAt": "2025-05-17T00:09:46.000Z",
        "title": "Reinforcing Multi-Turn Reasoning in LLM Agents via Turn-Level Credit\n  Assignment",
        "summary": "This paper investigates approaches to enhance the reasoning capabilities of\nLarge Language Model (LLM) agents using Reinforcement Learning (RL).\nSpecifically, we focus on multi-turn tool-use scenarios, which can be naturally\nmodeled as Markov Decision Processes (MDPs). While existing approaches often\ntrain multi-turn LLM agents with trajectory-level advantage estimation in\nbandit settings, they struggle with turn-level credit assignment across\nmultiple decision steps, limiting their performance on multi-turn reasoning\ntasks. To address this, we introduce a fine-grained turn-level advantage\nestimation strategy to enable more precise credit assignment in multi-turn\nagent interactions. The strategy is general and can be incorporated into\nvarious RL algorithms such as Group Relative Preference Optimization (GRPO).\nOur experimental evaluation on multi-turn reasoning and search-based tool-use\ntasks with GRPO implementations highlights the effectiveness of the MDP\nframework and the turn-level credit assignment in advancing the multi-turn\nreasoning capabilities of LLM agents in complex decision-making settings. Our\nmethod achieves 100% success in tool execution and 50% accuracy in exact answer\nmatching, significantly outperforming baselines, which fail to invoke tools and\nachieve only 20-30% exact match accuracy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.11821.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662a037140363679757cc292",
            "avatarUrl": "/avatars/fc41455247cc0744f24ac9255293ddf5.svg",
            "fullname": "Quan Wei",
            "name": "quanwei0",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22019",
            "authors": [
                {
                    "_id": "6837ed297d00cf0a04677bc1",
                    "user": {
                        "_id": "657429d833e5a4bf5b278615",
                        "avatarUrl": "/avatars/ed7e28c1b9a7bed1cad864c992cdcc69.svg",
                        "isPro": false,
                        "fullname": "QiuchenWang",
                        "user": "autumncc",
                        "type": "user"
                    },
                    "name": "Qiuchen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T09:40:19.039Z",
                    "hidden": false
                },
                {
                    "_id": "6837ed297d00cf0a04677bc2",
                    "name": "Ruixue Ding",
                    "hidden": false
                },
                {
                    "_id": "6837ed297d00cf0a04677bc3",
                    "user": {
                        "_id": "665d652e0f35c005de892108",
                        "avatarUrl": "/avatars/240bebdc7fdc6d50719c65de0e3cf1cd.svg",
                        "isPro": false,
                        "fullname": "Yu Zeng",
                        "user": "YuZeng260",
                        "type": "user"
                    },
                    "name": "Yu Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:42.906Z",
                    "hidden": false
                },
                {
                    "_id": "6837ed297d00cf0a04677bc4",
                    "name": "Zehui Chen",
                    "hidden": false
                },
                {
                    "_id": "6837ed297d00cf0a04677bc5",
                    "user": {
                        "_id": "64b02ec0e5000ae8a572ced5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
                        "isPro": false,
                        "fullname": "Lin Chen",
                        "user": "Lin-Chen",
                        "type": "user"
                    },
                    "name": "Lin Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:44.983Z",
                    "hidden": false
                },
                {
                    "_id": "6837ed297d00cf0a04677bc6",
                    "name": "Shihang Wang",
                    "hidden": false
                },
                {
                    "_id": "6837ed297d00cf0a04677bc7",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "6837ed297d00cf0a04677bc8",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6837ed297d00cf0a04677bc9",
                    "name": "Feng Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T06:30:51.000Z",
            "submittedOnDailyAt": "2025-05-29T03:46:05.539Z",
            "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64b02ec0e5000ae8a572ced5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
                "isPro": false,
                "fullname": "Lin Chen",
                "user": "Lin-Chen",
                "type": "user"
            },
            "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\nhttps://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.",
            "upvotes": 10,
            "discussionId": "6837ed297d00cf0a04677bf5",
            "githubRepo": "https://github.com/Alibaba-NLP/VRAG",
            "ai_summary": "VRAG-RL, a reinforcement learning framework, enhances reasoning and visual information handling in RAG methods by integrating visual perception tokens and employing specialized action spaces and rewards.",
            "ai_keywords": [
                "reinforcement learning",
                "VRAG-RL",
                "VLMs",
                "search engines",
                "visually rich information",
                "reasoning trajectories",
                "visual perception tokens",
                "action space",
                "query rewriting",
                "retrieval performance",
                "model-based reward"
            ]
        },
        "publishedAt": "2025-05-28T02:30:51.000Z",
        "title": "VRAG-RL: Empower Vision-Perception-Based RAG for Visually Rich\n  Information Understanding via Iterative Reasoning with Reinforcement Learning",
        "summary": "Effectively retrieving, reasoning and understanding visually rich information\nremains a challenge for RAG methods. Traditional text-based methods cannot\nhandle visual-related information. On the other hand, current vision-based RAG\napproaches are often limited by fixed pipelines and frequently struggle to\nreason effectively due to the insufficient activation of the fundamental\ncapabilities of models. As RL has been proven to be beneficial for model\nreasoning, we introduce VRAG-RL, a novel RL framework tailored for complex\nreasoning across visually rich information. With this framework, VLMs interact\nwith search engines, autonomously sampling single-turn or multi-turn reasoning\ntrajectories with the help of visual perception tokens and undergoing continual\noptimization based on these samples. Our approach highlights key limitations of\nRL in RAG domains: (i) Prior Multi-modal RAG approaches tend to merely\nincorporate images into the context, leading to insufficient reasoning token\nallocation and neglecting visual-specific perception; and (ii) When models\ninteract with search engines, their queries often fail to retrieve relevant\ninformation due to the inability to articulate requirements, thereby leading to\nsuboptimal performance. To address these challenges, we define an action space\ntailored for visually rich inputs, with actions including cropping and scaling,\nallowing the model to gather information from a coarse-to-fine perspective.\nFurthermore, to bridge the gap between users' original inquiries and the\nretriever, we employ a simple yet effective reward that integrates query\nrewriting and retrieval performance with a model-based reward. Our VRAG-RL\noptimizes VLMs for RAG tasks using specially designed RL strategies, aligning\nthe model with real-world applications. The code is available at\nhttps://github.com/Alibaba-NLP/VRAG{https://github.com/Alibaba-NLP/VRAG}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22019.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64b02ec0e5000ae8a572ced5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b02ec0e5000ae8a572ced5/6ifLntBU2ICQK7SW8WxKU.png",
            "fullname": "Lin Chen",
            "name": "Lin-Chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 89
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22525",
            "authors": [
                {
                    "_id": "6837da438f680552f7b86b28",
                    "user": {
                        "_id": "64bb5f9d8e051085bace4d1e",
                        "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
                        "isPro": true,
                        "fullname": "Ethan Chern",
                        "user": "ethanchern",
                        "type": "user"
                    },
                    "name": "Ethan Chern",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:05.613Z",
                    "hidden": false
                },
                {
                    "_id": "6837da438f680552f7b86b29",
                    "name": "Zhulin Hu",
                    "hidden": false
                },
                {
                    "_id": "6837da438f680552f7b86b2a",
                    "name": "Steffi Chern",
                    "hidden": false
                },
                {
                    "_id": "6837da438f680552f7b86b2b",
                    "name": "Siqi Kou",
                    "hidden": false
                },
                {
                    "_id": "6837da438f680552f7b86b2c",
                    "name": "Jiadi Su",
                    "hidden": false
                },
                {
                    "_id": "6837da438f680552f7b86b2d",
                    "name": "Yan Ma",
                    "hidden": false
                },
                {
                    "_id": "6837da438f680552f7b86b2e",
                    "name": "Zhijie Deng",
                    "hidden": false
                },
                {
                    "_id": "6837da438f680552f7b86b2f",
                    "name": "Pengfei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T16:12:45.000Z",
            "submittedOnDailyAt": "2025-05-29T02:24:51.876Z",
            "title": "Thinking with Generated Images",
            "submittedOnDailyBy": {
                "_id": "64bb5f9d8e051085bace4d1e",
                "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
                "isPro": true,
                "fullname": "Ethan Chern",
                "user": "ethanchern",
                "type": "user"
            },
            "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.",
            "upvotes": 9,
            "discussionId": "6837da468f680552f7b86bb2",
            "githubRepo": "https://github.com/GAIR-NLP/thinking-with-generated-images",
            "ai_summary": "Thinking with Generated Images allows large multimodal models to generate and critique intermediate visual steps, enhancing visual reasoning capabilities and achieving significant improvements in complex scenarios.",
            "ai_keywords": [
                "LMMs",
                "visual reasoning",
                "chain-of-thought",
                "vision generation",
                "intermediate visual subgoals",
                "self-critique",
                "multis-object scenarios",
                "biochemists",
                "architects",
                "forensic analysts",
                "basketball players"
            ]
        },
        "publishedAt": "2025-05-28T12:12:45.000Z",
        "title": "Thinking with Generated Images",
        "summary": "We present Thinking with Generated Images, a novel paradigm that\nfundamentally transforms how large multimodal models (LMMs) engage with visual\nreasoning by enabling them to natively think across text and vision modalities\nthrough spontaneous generation of intermediate visual thinking steps. Current\nvisual reasoning with LMMs is constrained to either processing fixed\nuser-provided images or reasoning solely through text-based chain-of-thought\n(CoT). Thinking with Generated Images unlocks a new dimension of cognitive\ncapability where models can actively construct intermediate visual thoughts,\ncritique their own visual hypotheses, and refine them as integral components of\ntheir reasoning process. We demonstrate the effectiveness of our approach\nthrough two complementary mechanisms: (1) vision generation with intermediate\nvisual subgoals, where models decompose complex visual tasks into manageable\ncomponents that are generated and integrated progressively, and (2) vision\ngeneration with self-critique, where models generate an initial visual\nhypothesis, analyze its shortcomings through textual reasoning, and produce\nrefined outputs based on their own critiques. Our experiments on vision\ngeneration benchmarks show substantial improvements over baseline approaches,\nwith our models achieving up to 50% (from 38% to 57%) relative improvement in\nhandling complex multi-object scenarios. From biochemists exploring novel\nprotein structures, and architects iterating on spatial designs, to forensic\nanalysts reconstructing crime scenes, and basketball players envisioning\nstrategic plays, our approach enables AI models to engage in the kind of visual\nimagination and iterative refinement that characterizes human creative,\nanalytical, and strategic thinking. We release our open-source suite at\nhttps://github.com/GAIR-NLP/thinking-with-generated-images.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22525.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64bb5f9d8e051085bace4d1e",
            "avatarUrl": "/avatars/15ccbb78c6131dfe46b7a9d8e7d1a31f.svg",
            "fullname": "Ethan Chern",
            "name": "ethanchern",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20779",
            "authors": [
                {
                    "_id": "6836b63be36a95ffd7c9743f",
                    "user": {
                        "_id": "63e8914accae1fe5c6176a55",
                        "avatarUrl": "/avatars/0d9cb1d5a3927ec52a425b15217a3f25.svg",
                        "isPro": false,
                        "fullname": "noy sternlicht",
                        "user": "noystl",
                        "type": "user"
                    },
                    "name": "Noy Sternlicht",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:55:29.488Z",
                    "hidden": false
                },
                {
                    "_id": "6836b63be36a95ffd7c97440",
                    "name": "Tom Hope",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63e8914accae1fe5c6176a55/ke4cBUYmoL9k-6vOJ5Svx.gif"
            ],
            "publishedAt": "2025-05-27T06:36:04.000Z",
            "submittedOnDailyAt": "2025-05-29T11:01:11.443Z",
            "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature",
            "submittedOnDailyBy": {
                "_id": "63e8914accae1fe5c6176a55",
                "avatarUrl": "/avatars/0d9cb1d5a3927ec52a425b15217a3f25.svg",
                "isPro": false,
                "fullname": "noy sternlicht",
                "user": "noystl",
                "type": "user"
            },
            "summary": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA",
            "upvotes": 9,
            "discussionId": "6836b63ee36a95ffd7c974f1",
            "projectPage": "https://noy-sternlicht.github.io/CHIMERA-Web",
            "githubRepo": "https://github.com/noy-sternlicht/CHIMERA-KB/tree/main",
            "ai_summary": "A large-scale knowledge base of recombination examples is built from scientific paper abstracts using an LLM-based extraction model to analyze and inspire new creative directions in AI.",
            "ai_keywords": [
                "LLM-based extraction model",
                "scientific hypothesis generation model"
            ]
        },
        "publishedAt": "2025-05-27T02:36:04.000Z",
        "title": "CHIMERA: A Knowledge Base of Idea Recombination in Scientific Literature",
        "summary": "A hallmark of human innovation is the process of recombination -- creating\noriginal ideas by integrating elements of existing mechanisms and concepts. In\nthis work, we automatically mine the scientific literature and build CHIMERA: a\nlarge-scale knowledge base (KB) of recombination examples. CHIMERA can be used\nto empirically explore at scale how scientists recombine concepts and take\ninspiration from different areas, or to train supervised machine learning\nmodels that learn to predict new creative cross-domain directions. To build\nthis KB, we present a novel information extraction task of extracting\nrecombination from scientific paper abstracts, collect a high-quality corpus of\nhundreds of manually annotated abstracts, and use it to train an LLM-based\nextraction model. The model is applied to a large corpus of papers in the AI\ndomain, yielding a KB of over 28K recombination examples. We analyze CHIMERA to\nexplore the properties of recombination in different subareas of AI. Finally,\nwe train a scientific hypothesis generation model using the KB, which predicts\nnew recombination directions that real-world researchers find inspiring. Our\ndata and code are available at https://github.cs.huji.ac.il/tomhope-lab/CHIMERA",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63e8914accae1fe5c6176a55/ke4cBUYmoL9k-6vOJ5Svx.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20779.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63e8914accae1fe5c6176a55",
            "avatarUrl": "/avatars/0d9cb1d5a3927ec52a425b15217a3f25.svg",
            "fullname": "noy sternlicht",
            "name": "noystl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21876",
            "authors": [
                {
                    "_id": "6837d80bf42b2aacfc26c460",
                    "name": "Zun Wang",
                    "hidden": false
                },
                {
                    "_id": "6837d80bf42b2aacfc26c461",
                    "name": "Jaemin Cho",
                    "hidden": false
                },
                {
                    "_id": "6837d80bf42b2aacfc26c462",
                    "name": "Jialu Li",
                    "hidden": false
                },
                {
                    "_id": "6837d80bf42b2aacfc26c463",
                    "name": "Han Lin",
                    "hidden": false
                },
                {
                    "_id": "6837d80bf42b2aacfc26c464",
                    "user": {
                        "_id": "652066649004117947e46ed6",
                        "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
                        "isPro": false,
                        "fullname": "Jaehong Yoon",
                        "user": "jaehong31",
                        "type": "user"
                    },
                    "name": "Jaehong Yoon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:42:07.590Z",
                    "hidden": false
                },
                {
                    "_id": "6837d80bf42b2aacfc26c465",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837d80bf42b2aacfc26c466",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T01:45:26.000Z",
            "submittedOnDailyAt": "2025-05-29T02:15:48.194Z",
            "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance",
            "submittedOnDailyBy": {
                "_id": "5ffe32d8942cf3533d364449",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
                "isPro": false,
                "fullname": "Jaemin Cho",
                "user": "j-min",
                "type": "user"
            },
            "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.",
            "upvotes": 8,
            "discussionId": "6837d810f42b2aacfc26c5ec",
            "projectPage": "https://zunwang1.github.io/Epic",
            "githubRepo": "https://github.com/wz0919/EPiC",
            "ai_summary": "EPiC is a framework for efficient 3D camera control in video diffusion models that constructs high-quality anchor videos through first-frame visibility masking, integrates them using a lightweight ControlNet module, and achieves state-of-the-art performance on I2V tasks with minimal resources.",
            "ai_keywords": [
                "anchor videos",
                "point cloud estimation",
                "camera trajectories",
                "diffusion models",
                "first-frame visibility",
                "EPiC",
                "ControlNet",
                "I2V training pairs",
                "rendering misalignments",
                "RealEstate10K",
                "MiraData"
            ]
        },
        "publishedAt": "2025-05-27T21:45:26.000Z",
        "title": "EPiC: Efficient Video Camera Control Learning with Precise Anchor-Video\n  Guidance",
        "summary": "Recent approaches on 3D camera control in video diffusion models (VDMs) often\ncreate anchor videos to guide diffusion models as a structured prior by\nrendering from estimated point clouds following annotated camera trajectories.\nHowever, errors inherent in point cloud estimation often lead to inaccurate\nanchor videos. Moreover, the requirement for extensive camera trajectory\nannotations further increases resource demands. To address these limitations,\nwe introduce EPiC, an efficient and precise camera control learning framework\nthat automatically constructs high-quality anchor videos without expensive\ncamera trajectory annotations. Concretely, we create highly precise anchor\nvideos for training by masking source videos based on first-frame visibility.\nThis approach ensures high alignment, eliminates the need for camera trajectory\nannotations, and thus can be readily applied to any in-the-wild video to\ngenerate image-to-video (I2V) training pairs. Furthermore, we introduce\nAnchor-ControlNet, a lightweight conditioning module that integrates anchor\nvideo guidance in visible regions to pretrained VDMs, with less than 1% of\nbackbone model parameters. By combining the proposed anchor video data and\nControlNet module, EPiC achieves efficient training with substantially fewer\nparameters, training steps, and less data, without requiring modifications to\nthe diffusion model backbone typically needed to mitigate rendering\nmisalignments. Although being trained on masking-based anchor videos, our\nmethod generalizes robustly to anchor videos made with point clouds during\ninference, enabling precise 3D-informed camera control. EPiC achieves SOTA\nperformance on RealEstate10K and MiraData for I2V camera control task,\ndemonstrating precise and robust camera control ability both quantitatively and\nqualitatively. Notably, EPiC also exhibits strong zero-shot generalization to\nvideo-to-video scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21876.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5ffe32d8942cf3533d364449",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
            "fullname": "Jaemin Cho",
            "name": "j-min",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18366",
            "authors": [
                {
                    "_id": "6838949f702f865b81701ff0",
                    "name": "Hansa Meghwani",
                    "hidden": false
                },
                {
                    "_id": "6838949f702f865b81701ff1",
                    "user": {
                        "_id": "6321a50a6b1992383fa00db1",
                        "avatarUrl": "/avatars/92311b4a5e8bcc8f5477747621334fc8.svg",
                        "isPro": false,
                        "fullname": "Amit Agarwal",
                        "user": "amitbcp",
                        "type": "user"
                    },
                    "name": "Amit Agarwal",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-29T17:08:47.973Z",
                    "hidden": false
                },
                {
                    "_id": "6838949f702f865b81701ff2",
                    "name": "Priyaranjan Pattnayak",
                    "hidden": false
                },
                {
                    "_id": "6838949f702f865b81701ff3",
                    "name": "Hitesh Laxmichand Patel",
                    "hidden": false
                },
                {
                    "_id": "6838949f702f865b81701ff4",
                    "name": "Srikant Panda",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T20:51:20.000Z",
            "submittedOnDailyAt": "2025-05-29T15:39:51.547Z",
            "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems",
            "submittedOnDailyBy": {
                "_id": "6321a50a6b1992383fa00db1",
                "avatarUrl": "/avatars/92311b4a5e8bcc8f5477747621334fc8.svg",
                "isPro": false,
                "fullname": "Amit Agarwal",
                "user": "amitbcp",
                "type": "user"
            },
            "summary": "Enterprise search systems often struggle to retrieve accurate,\ndomain-specific information due to semantic mismatches and overlapping\nterminologies. These issues can degrade the performance of downstream\napplications such as knowledge management, customer support, and\nretrieval-augmented generation agents. To address this challenge, we propose a\nscalable hard-negative mining framework tailored specifically for\ndomain-specific enterprise data. Our approach dynamically selects semantically\nchallenging but contextually irrelevant documents to enhance deployed\nre-ranking models.\n  Our method integrates diverse embedding models, performs dimensionality\nreduction, and uniquely selects hard negatives, ensuring computational\nefficiency and semantic precision. Evaluation on our proprietary enterprise\ncorpus (cloud services domain) demonstrates substantial improvements of 15\\% in\nMRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other\nnegative sampling techniques. Further validation on public domain-specific\ndatasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability\nand readiness for real-world applications.",
            "upvotes": 7,
            "discussionId": "6838949f702f865b81702023",
            "ai_summary": "A scalable hard-negative mining framework enhances domain-specific enterprise search by dynamically selecting semantically challenging irrelevant documents, improving re-ranking models' performance.",
            "ai_keywords": [
                "hard-negative mining",
                "domain-specific enterprise data",
                "semantically challenging documents",
                "contextually irrelevant documents",
                "re-ranking models",
                "embedding models",
                "dimensionality reduction",
                "MRR@3",
                "MRR@10",
                "FiQA",
                "Climate Fever",
                "TechQA"
            ]
        },
        "publishedAt": "2025-05-23T16:51:20.000Z",
        "title": "Hard Negative Mining for Domain-Specific Retrieval in Enterprise Systems",
        "summary": "Enterprise search systems often struggle to retrieve accurate,\ndomain-specific information due to semantic mismatches and overlapping\nterminologies. These issues can degrade the performance of downstream\napplications such as knowledge management, customer support, and\nretrieval-augmented generation agents. To address this challenge, we propose a\nscalable hard-negative mining framework tailored specifically for\ndomain-specific enterprise data. Our approach dynamically selects semantically\nchallenging but contextually irrelevant documents to enhance deployed\nre-ranking models.\n  Our method integrates diverse embedding models, performs dimensionality\nreduction, and uniquely selects hard negatives, ensuring computational\nefficiency and semantic precision. Evaluation on our proprietary enterprise\ncorpus (cloud services domain) demonstrates substantial improvements of 15\\% in\nMRR@3 and 19\\% in MRR@10 compared to state-of-the-art baselines and other\nnegative sampling techniques. Further validation on public domain-specific\ndatasets (FiQA, Climate Fever, TechQA) confirms our method's generalizability\nand readiness for real-world applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18366.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6321a50a6b1992383fa00db1",
            "avatarUrl": "/avatars/92311b4a5e8bcc8f5477747621334fc8.svg",
            "fullname": "Amit Agarwal",
            "name": "amitbcp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22613",
            "authors": [
                {
                    "_id": "6837d79d4d9866c160d8f43b",
                    "name": "Yuchi Wang",
                    "hidden": false
                },
                {
                    "_id": "6837d79d4d9866c160d8f43c",
                    "name": "Yishuo Cai",
                    "hidden": false
                },
                {
                    "_id": "6837d79d4d9866c160d8f43d",
                    "name": "Shuhuai Ren",
                    "hidden": false
                },
                {
                    "_id": "6837d79d4d9866c160d8f43e",
                    "name": "Sihan Yang",
                    "hidden": false
                },
                {
                    "_id": "6837d79d4d9866c160d8f43f",
                    "name": "Linli Yao",
                    "hidden": false
                },
                {
                    "_id": "6837d79d4d9866c160d8f440",
                    "name": "Yuanxin Liu",
                    "hidden": false
                },
                {
                    "_id": "6837d79d4d9866c160d8f441",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837d79d4d9866c160d8f442",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "6837d79d4d9866c160d8f443",
                    "name": "Xu Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:29:34.000Z",
            "submittedOnDailyAt": "2025-05-29T02:20:00.209Z",
            "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction",
            "submittedOnDailyBy": {
                "_id": "622842e296588dd1a2594746",
                "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
                "isPro": false,
                "fullname": "wangyuchi",
                "user": "YuchiWang",
                "type": "user"
            },
            "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.",
            "upvotes": 6,
            "discussionId": "6837d79e4d9866c160d8f471",
            "githubRepo": "https://github.com/wangyuchi369/RICO",
            "ai_summary": "A novel iterative framework, RICO, improves image caption accuracy by using visual reconstruction and a text-to-image model to refine discrepancies, while RICO-Flash enhances efficiency using DPO.",
            "ai_keywords": [
                "multimodal large language models",
                "text-to-image model",
                "DPO",
                "CapsBench",
                "CompreCap"
            ]
        },
        "publishedAt": "2025-05-28T13:29:34.000Z",
        "title": "RICO: Improving Accuracy and Completeness in Image Recaptioning via\n  Visual Reconstruction",
        "summary": "Image recaptioning is widely used to generate training datasets with enhanced\nquality for various multimodal tasks. Existing recaptioning methods typically\nrely on powerful multimodal large language models (MLLMs) to enhance textual\ndescriptions, but often suffer from inaccuracies due to hallucinations and\nincompleteness caused by missing fine-grained details. To address these\nlimitations, we propose RICO, a novel framework that refines captions through\nvisual reconstruction. Specifically, we leverage a text-to-image model to\nreconstruct a caption into a reference image, and prompt an MLLM to identify\ndiscrepancies between the original and reconstructed images to refine the\ncaption. This process is performed iteratively, further progressively promoting\nthe generation of more faithful and comprehensive descriptions. To mitigate the\nadditional computational cost induced by the iterative process, we introduce\nRICO-Flash, which learns to generate captions like RICO using DPO. Extensive\nexperiments demonstrate that our approach significantly improves caption\naccuracy and completeness, outperforms most baselines by approximately 10% on\nboth CapsBench and CompreCap. Code released at\nhttps://github.com/wangyuchi369/RICO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22613.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "622842e296588dd1a2594746",
            "avatarUrl": "/avatars/b96d0b49e4cff25d83fefc67b7cd1076.svg",
            "fullname": "wangyuchi",
            "name": "YuchiWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22338",
            "authors": [
                {
                    "_id": "6837c79576eac3fa930de19b",
                    "name": "Hanyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6837c79576eac3fa930de19c",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "6837c79576eac3fa930de19d",
                    "name": "Chaoyun Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837c79576eac3fa930de19e",
                    "name": "Tianjun Mao",
                    "hidden": false
                },
                {
                    "_id": "6837c79576eac3fa930de19f",
                    "name": "Si Qin",
                    "hidden": false
                },
                {
                    "_id": "6837c79576eac3fa930de1a0",
                    "name": "Qingwei Lin",
                    "hidden": false
                },
                {
                    "_id": "6837c79576eac3fa930de1a1",
                    "name": "Saravan Rajmohan",
                    "hidden": false
                },
                {
                    "_id": "6837c79576eac3fa930de1a2",
                    "name": "Dongmei Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T13:23:49.000Z",
            "submittedOnDailyAt": "2025-05-29T01:04:31.053Z",
            "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
            "submittedOnDailyBy": {
                "_id": "654dbac9938fbf1e696be8aa",
                "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                "isPro": false,
                "fullname": "Chaoyun Zhang",
                "user": "vyokky",
                "type": "user"
            },
            "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad",
            "upvotes": 6,
            "discussionId": "6837c79576eac3fa930de1dd",
            "ai_summary": "Text2Grad converts human textual feedback into span-level gradients to optimize language models precisely and efficiently.",
            "ai_keywords": [
                "RLHF",
                "reinforcement-learning",
                "free-form textual feedback",
                "span-level gradients",
                "token spans",
                "differentiable reward signals",
                "gradient updates",
                "span-level policy optimizer",
                "fine-grained reward model",
                "feedback-annotation pipeline"
            ]
        },
        "publishedAt": "2025-05-28T09:23:49.000Z",
        "title": "Text2Grad: Reinforcement Learning from Natural Language Feedback",
        "summary": "Traditional RLHF optimizes language models with coarse, scalar rewards that\nmask the fine-grained reasons behind success or failure, leading to slow and\nopaque learning. Recent work augments RL with textual critiques through\nprompting or reflection, improving interpretability but leaving model\nparameters untouched. We introduce Text2Grad, a reinforcement-learning paradigm\nthat turns free-form textual feedback into span-level gradients. Given human\n(or programmatic) critiques, Text2Grad aligns each feedback phrase with the\nrelevant token spans, converts these alignments into differentiable reward\nsignals, and performs gradient updates that directly refine the offending\nportions of the model's policy. This yields precise, feedback-conditioned\nadjustments instead of global nudges. Text2Grad is realized through three\ncomponents: (1) a high-quality feedback-annotation pipeline that pairs\ncritiques with token spans; (2) a fine-grained reward model that predicts\nspan-level reward on answer while generating explanatory critiques; and (3) a\nspan-level policy optimizer that back-propagates natural-language gradients.\nAcross summarization, code generation, and question answering, Text2Grad\nconsistently surpasses scalar-reward RL and prompt-only baselines, providing\nboth higher task metrics and richer interpretability. Our results demonstrate\nthat natural-language feedback, when converted to gradients, is a powerful\nsignal for fine-grained policy optimization. The code for our method is\navailable at https://github.com/microsoft/Text2Grad",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22338.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "fullname": "Chaoyun Zhang",
            "name": "vyokky",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22203",
            "authors": [
                {
                    "_id": "6837dcc41448b8bf0c91fa30",
                    "user": {
                        "_id": "6462def82a83863b97c0611e",
                        "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
                        "isPro": false,
                        "fullname": "Yuzhen Huang",
                        "user": "yuzhen17",
                        "type": "user"
                    },
                    "name": "Yuzhen Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:53.737Z",
                    "hidden": false
                },
                {
                    "_id": "6837dcc41448b8bf0c91fa31",
                    "name": "Weihao Zeng",
                    "hidden": false
                },
                {
                    "_id": "6837dcc41448b8bf0c91fa32",
                    "name": "Xingshan Zeng",
                    "hidden": false
                },
                {
                    "_id": "6837dcc41448b8bf0c91fa33",
                    "name": "Qi Zhu",
                    "hidden": false
                },
                {
                    "_id": "6837dcc41448b8bf0c91fa34",
                    "name": "Junxian He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T10:28:41.000Z",
            "submittedOnDailyAt": "2025-05-29T02:37:12.694Z",
            "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning",
            "submittedOnDailyBy": {
                "_id": "6462def82a83863b97c0611e",
                "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
                "isPro": false,
                "fullname": "Yuzhen Huang",
                "user": "yuzhen17",
                "type": "user"
            },
            "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.",
            "upvotes": 6,
            "discussionId": "6837dcc51448b8bf0c91fa54",
            "ai_summary": "The study examines the effectiveness and reliability of rule-based and model-based verifiers in reinforcement learning with verifiable reward, highlighting limitations and vulnerabilities in their use for mathematical reasoning tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "verifiable reward",
                "DeepSeek-R1",
                "mathematical reasoning",
                "rule-based verifiers",
                "reward systems",
                "model-based verifiers",
                "false negatives",
                "false positives"
            ]
        },
        "publishedAt": "2025-05-28T06:28:41.000Z",
        "title": "Pitfalls of Rule- and Model-based Verifiers -- A Case Study on\n  Mathematical Reasoning",
        "summary": "Trustworthy verifiers are essential for the success of reinforcement learning\nwith verifiable reward (RLVR), which is the core methodology behind various\nlarge reasoning models such as DeepSeek-R1. In complex domains like\nmathematical reasoning, rule-based verifiers have been widely adopted in\nprevious works to train strong reasoning models. However, the reliability of\nthese verifiers and their impact on the RL training process remain poorly\nunderstood. In this work, we take mathematical reasoning as a case study and\nconduct a comprehensive analysis of various verifiers in both static evaluation\nand RL training scenarios. First, we find that current open-source rule-based\nverifiers often fail to recognize equivalent answers presented in different\nformats across multiple commonly used mathematical datasets, resulting in\nnon-negligible false negative rates. This limitation adversely affects RL\ntraining performance and becomes more pronounced as the policy model gets\nstronger. Subsequently, we investigate model-based verifiers as a potential\nsolution to address these limitations. While the static evaluation shows that\nmodel-based verifiers achieve significantly higher verification accuracy,\nfurther analysis and RL training results imply that they are highly susceptible\nto hacking, where they misclassify certain patterns in responses as correct\n(i.e., false positives). This vulnerability is exploited during policy model\noptimization, leading to artificially inflated rewards. Our findings underscore\nthe unique risks inherent to both rule-based and model-based verifiers, aiming\nto offer valuable insights to develop more robust reward systems in\nreinforcement learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22203.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6462def82a83863b97c0611e",
            "avatarUrl": "/avatars/c03e9cc7d75b0266fcc56ecb6ee62148.svg",
            "fullname": "Yuzhen Huang",
            "name": "yuzhen17",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20589",
            "authors": [
                {
                    "_id": "6836b933f9baa9388e75e171",
                    "user": {
                        "_id": "62d05322dd7bdfc5e5c86298",
                        "avatarUrl": "/avatars/62879c8a8bbdbaaf63da27a44e90bd5f.svg",
                        "isPro": false,
                        "fullname": "Mahdi Pourmirzaei",
                        "user": "Mahdip72",
                        "type": "user"
                    },
                    "name": "Mahdi Pourmirzaei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:43:49.366Z",
                    "hidden": false
                },
                {
                    "_id": "6836b933f9baa9388e75e172",
                    "name": "Farzaneh Esmaili",
                    "hidden": false
                },
                {
                    "_id": "6836b933f9baa9388e75e173",
                    "name": "Salhuldin Alqarghuli",
                    "hidden": false
                },
                {
                    "_id": "6836b933f9baa9388e75e174",
                    "name": "Mohammadreza Pourmirzaei",
                    "hidden": false
                },
                {
                    "_id": "6836b933f9baa9388e75e175",
                    "name": "Ye Han",
                    "hidden": false
                },
                {
                    "_id": "6836b933f9baa9388e75e176",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "6836b933f9baa9388e75e177",
                    "name": "Mohsen Rezaei",
                    "hidden": false
                },
                {
                    "_id": "6836b933f9baa9388e75e178",
                    "name": "Duolin Wang",
                    "hidden": false
                },
                {
                    "_id": "6836b933f9baa9388e75e179",
                    "name": "Dong Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T23:50:36.000Z",
            "submittedOnDailyAt": "2025-05-29T15:11:42.522Z",
            "title": "Prot2Token: A Unified Framework for Protein Modeling via Next-Token\n  Prediction",
            "submittedOnDailyBy": {
                "_id": "62d05322dd7bdfc5e5c86298",
                "avatarUrl": "/avatars/62879c8a8bbdbaaf63da27a44e90bd5f.svg",
                "isPro": false,
                "fullname": "Mahdi Pourmirzaei",
                "user": "Mahdip72",
                "type": "user"
            },
            "summary": "The diverse nature of protein prediction tasks has traditionally necessitated\nspecialized models, hindering the development of broadly applicable and\ncomputationally efficient Protein Language Models (PLMs). In this work, we\nintroduce Prot2Token, a unified framework that overcomes these challenges by\nconverting a wide spectrum of protein-related predictions, from sequence-level\nproperties and residue-specific attributes to complex inter-protein\ninteractions, into a standardized next-token prediction format. At its core,\nProt2Token employs an autoregressive decoder, conditioned on embeddings from\npre-trained protein encoders and guided by learnable task tokens, to perform\ndiverse predictions. This architecture uniquely facilitates multi-task\nlearning, enabling a single model to master numerous tasks with improved\nefficiency. We present extensive experimental validation across a variety of\nbenchmarks, demonstrating Prot2Tokens strong predictive power in different\ntypes of protein-prediction tasks. Key results include significant speedups\n(e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or\nexceeding specialized approaches. Beyond that, we introduce an auxiliary\nself-supervised decoder pre-training approach to improve spatially sensitive\ntask performance. Prot2Token thus offers a significant step towards a\nversatile, high-throughput paradigm for protein modeling, promising to\naccelerate biological discovery and the development of novel therapeutics. The\ncode is available at https://github.com/mahdip72/prot2token .",
            "upvotes": 6,
            "discussionId": "6836b934f9baa9388e75e1c2",
            "githubRepo": "https://github.com/mahdip72/prot2token",
            "ai_summary": "Prot2Token unifies protein prediction tasks using an autoregressive decoder with task tokens, improving efficiency and accuracy across different benchmarks compared to specialized models.",
            "ai_keywords": [
                "Protein Language Models",
                "autoregressive decoder",
                "pre-trained protein encoders",
                "task tokens",
                "multi-task learning",
                "next-token prediction",
                "self-supervised decoder pre-training"
            ]
        },
        "publishedAt": "2025-05-26T19:50:36.000Z",
        "title": "Prot2Token: A Unified Framework for Protein Modeling via Next-Token\n  Prediction",
        "summary": "The diverse nature of protein prediction tasks has traditionally necessitated\nspecialized models, hindering the development of broadly applicable and\ncomputationally efficient Protein Language Models (PLMs). In this work, we\nintroduce Prot2Token, a unified framework that overcomes these challenges by\nconverting a wide spectrum of protein-related predictions, from sequence-level\nproperties and residue-specific attributes to complex inter-protein\ninteractions, into a standardized next-token prediction format. At its core,\nProt2Token employs an autoregressive decoder, conditioned on embeddings from\npre-trained protein encoders and guided by learnable task tokens, to perform\ndiverse predictions. This architecture uniquely facilitates multi-task\nlearning, enabling a single model to master numerous tasks with improved\nefficiency. We present extensive experimental validation across a variety of\nbenchmarks, demonstrating Prot2Tokens strong predictive power in different\ntypes of protein-prediction tasks. Key results include significant speedups\n(e.g., near 1000x over AlphaFold2 with MSA) and performance often matching or\nexceeding specialized approaches. Beyond that, we introduce an auxiliary\nself-supervised decoder pre-training approach to improve spatially sensitive\ntask performance. Prot2Token thus offers a significant step towards a\nversatile, high-throughput paradigm for protein modeling, promising to\naccelerate biological discovery and the development of novel therapeutics. The\ncode is available at https://github.com/mahdip72/prot2token .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20589.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d05322dd7bdfc5e5c86298",
            "avatarUrl": "/avatars/62879c8a8bbdbaaf63da27a44e90bd5f.svg",
            "fullname": "Mahdi Pourmirzaei",
            "name": "Mahdip72",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22960",
            "authors": [
                {
                    "_id": "68390972be7fee960c0b07ee",
                    "name": "Yongjin Yang",
                    "hidden": false
                },
                {
                    "_id": "68390972be7fee960c0b07ef",
                    "name": "Euiin Yi",
                    "hidden": false
                },
                {
                    "_id": "68390972be7fee960c0b07f0",
                    "name": "Jongwoo Ko",
                    "hidden": false
                },
                {
                    "_id": "68390972be7fee960c0b07f1",
                    "name": "Kimin Lee",
                    "hidden": false
                },
                {
                    "_id": "68390972be7fee960c0b07f2",
                    "name": "Zhijing Jin",
                    "hidden": false
                },
                {
                    "_id": "68390972be7fee960c0b07f3",
                    "name": "Se-Young Yun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T01:02:55.000Z",
            "submittedOnDailyAt": "2025-05-29T23:58:11.948Z",
            "title": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study\n  of Conditional Effectiveness",
            "submittedOnDailyBy": {
                "_id": "66027cead56cd39a44992631",
                "avatarUrl": "/avatars/3c8b679bc72f30c547782579e9592bb1.svg",
                "isPro": false,
                "fullname": "Euiin Yi",
                "user": "euiin",
                "type": "user"
            },
            "summary": "The remarkable growth in large language model (LLM) capabilities has spurred\nexploration into multi-agent systems, with debate frameworks emerging as a\npromising avenue for enhanced problem-solving. These multi-agent debate (MAD)\napproaches, where agents collaboratively present, critique, and refine\narguments, potentially offer improved reasoning, robustness, and diverse\nperspectives over monolithic models. Despite prior studies leveraging MAD, a\nsystematic understanding of its effectiveness compared to self-agent methods,\nparticularly under varying conditions, remains elusive. This paper seeks to\nfill this gap by conceptualizing MAD as a test-time computational scaling\ntechnique, distinguished by collaborative refinement and diverse exploration\ncapabilities. We conduct a comprehensive empirical investigation comparing MAD\nwith strong self-agent test-time scaling baselines on mathematical reasoning\nand safety-related tasks. Our study systematically examines the influence of\ntask difficulty, model scale, and agent diversity on MAD's performance. Key\nfindings reveal that, for mathematical reasoning, MAD offers limited advantages\nover self-agent scaling but becomes more effective with increased problem\ndifficulty and decreased model capability, while agent diversity shows little\nbenefit. Conversely, for safety tasks, MAD's collaborative refinement can\nincrease vulnerability, but incorporating diverse agent configurations\nfacilitates a gradual reduction in attack success through the collaborative\nrefinement process. We believe our findings provide critical guidance for the\nfuture development of more effective and strategically deployed MAD systems.",
            "upvotes": 5,
            "discussionId": "68390973be7fee960c0b0829"
        },
        "publishedAt": "2025-05-28T21:02:55.000Z",
        "title": "Revisiting Multi-Agent Debate as Test-Time Scaling: A Systematic Study\n  of Conditional Effectiveness",
        "summary": "The remarkable growth in large language model (LLM) capabilities has spurred\nexploration into multi-agent systems, with debate frameworks emerging as a\npromising avenue for enhanced problem-solving. These multi-agent debate (MAD)\napproaches, where agents collaboratively present, critique, and refine\narguments, potentially offer improved reasoning, robustness, and diverse\nperspectives over monolithic models. Despite prior studies leveraging MAD, a\nsystematic understanding of its effectiveness compared to self-agent methods,\nparticularly under varying conditions, remains elusive. This paper seeks to\nfill this gap by conceptualizing MAD as a test-time computational scaling\ntechnique, distinguished by collaborative refinement and diverse exploration\ncapabilities. We conduct a comprehensive empirical investigation comparing MAD\nwith strong self-agent test-time scaling baselines on mathematical reasoning\nand safety-related tasks. Our study systematically examines the influence of\ntask difficulty, model scale, and agent diversity on MAD's performance. Key\nfindings reveal that, for mathematical reasoning, MAD offers limited advantages\nover self-agent scaling but becomes more effective with increased problem\ndifficulty and decreased model capability, while agent diversity shows little\nbenefit. Conversely, for safety tasks, MAD's collaborative refinement can\nincrease vulnerability, but incorporating diverse agent configurations\nfacilitates a gradual reduction in attack success through the collaborative\nrefinement process. We believe our findings provide critical guidance for the\nfuture development of more effective and strategically deployed MAD systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22960.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66027cead56cd39a44992631",
            "avatarUrl": "/avatars/3c8b679bc72f30c547782579e9592bb1.svg",
            "fullname": "Euiin Yi",
            "name": "euiin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17330",
            "authors": [
                {
                    "_id": "68389760cf258e24a6079f6a",
                    "user": {
                        "_id": "6321a50a6b1992383fa00db1",
                        "avatarUrl": "/avatars/92311b4a5e8bcc8f5477747621334fc8.svg",
                        "isPro": false,
                        "fullname": "Amit Agarwal",
                        "user": "amitbcp",
                        "type": "user"
                    },
                    "name": "Amit Agarwal",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-29T17:20:33.175Z",
                    "hidden": false
                },
                {
                    "_id": "68389760cf258e24a6079f6b",
                    "name": "Srikant Panda",
                    "hidden": false
                },
                {
                    "_id": "68389760cf258e24a6079f6c",
                    "name": "Kulbhushan Pachauri",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T22:53:58.000Z",
            "submittedOnDailyAt": "2025-05-29T15:54:32.273Z",
            "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich\n  Document Understanding",
            "submittedOnDailyBy": {
                "_id": "6321a50a6b1992383fa00db1",
                "avatarUrl": "/avatars/92311b4a5e8bcc8f5477747621334fc8.svg",
                "isPro": false,
                "fullname": "Amit Agarwal",
                "user": "amitbcp",
                "type": "user"
            },
            "summary": "In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable\nand efficient model architecture for visually rich document understanding\n(VRDU) in few-shot settings. FS-DAG leverages domain-specific and\nlanguage/vision specific backbones within a modular framework to adapt to\ndiverse document types with minimal data. The model is robust to practical\nchallenges such as handling OCR errors, misspellings, and domain shifts, which\nare critical in real-world deployments. FS-DAG is highly performant with less\nthan 90M parameters, making it well-suited for complex real-world applications\nfor Information Extraction (IE) tasks where computational resources are\nlimited. We demonstrate FS-DAG's capability through extensive experiments for\ninformation extraction task, showing significant improvements in convergence\nspeed and performance compared to state-of-the-art methods. Additionally, this\nwork highlights the ongoing progress in developing smaller, more efficient\nmodels that do not compromise on performance. Code :\nhttps://github.com/oracle-samples/fs-dag",
            "upvotes": 5,
            "discussionId": "68389761cf258e24a6079fa9",
            "githubRepo": "https://github.com/oracle-samples/fs-dag",
            "ai_summary": "FS-DAG, a modular model architecture, efficiently adapts to diverse document types with few-shot learning for visually rich document understanding in resource-constrained environments.",
            "ai_keywords": [
                "Few Shot Domain Adapting Graph",
                "FS-DAG",
                "visually rich document understanding",
                "VRDU",
                "domain-specific backbones",
                "language/vision specific backbones",
                "domain shifts",
                "OCR errors",
                "misspellings",
                "Information Extraction",
                "IE",
                "parameter-efficient"
            ]
        },
        "publishedAt": "2025-05-22T18:53:58.000Z",
        "title": "FS-DAG: Few Shot Domain Adapting Graph Networks for Visually Rich\n  Document Understanding",
        "summary": "In this work, we propose Few Shot Domain Adapting Graph (FS-DAG), a scalable\nand efficient model architecture for visually rich document understanding\n(VRDU) in few-shot settings. FS-DAG leverages domain-specific and\nlanguage/vision specific backbones within a modular framework to adapt to\ndiverse document types with minimal data. The model is robust to practical\nchallenges such as handling OCR errors, misspellings, and domain shifts, which\nare critical in real-world deployments. FS-DAG is highly performant with less\nthan 90M parameters, making it well-suited for complex real-world applications\nfor Information Extraction (IE) tasks where computational resources are\nlimited. We demonstrate FS-DAG's capability through extensive experiments for\ninformation extraction task, showing significant improvements in convergence\nspeed and performance compared to state-of-the-art methods. Additionally, this\nwork highlights the ongoing progress in developing smaller, more efficient\nmodels that do not compromise on performance. Code :\nhttps://github.com/oracle-samples/fs-dag",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17330.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6321a50a6b1992383fa00db1",
            "avatarUrl": "/avatars/92311b4a5e8bcc8f5477747621334fc8.svg",
            "fullname": "Amit Agarwal",
            "name": "amitbcp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21960",
            "authors": [
                {
                    "_id": "6837dd74ec10479b9605da15",
                    "user": {
                        "_id": "637e1cf4f09bf2498c543a73",
                        "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
                        "isPro": false,
                        "fullname": "Senmao Li",
                        "user": "senmaonk",
                        "type": "user"
                    },
                    "name": "Senmao Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:51.659Z",
                    "hidden": false
                },
                {
                    "_id": "6837dd74ec10479b9605da16",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "6837dd74ec10479b9605da17",
                    "name": "Kai Wang",
                    "hidden": false
                },
                {
                    "_id": "6837dd74ec10479b9605da18",
                    "name": "Tao Liu",
                    "hidden": false
                },
                {
                    "_id": "6837dd74ec10479b9605da19",
                    "name": "Jiehang Xie",
                    "hidden": false
                },
                {
                    "_id": "6837dd74ec10479b9605da1a",
                    "name": "Joost van de Weijer",
                    "hidden": false
                },
                {
                    "_id": "6837dd74ec10479b9605da1b",
                    "name": "Fahad Shahbaz Khan",
                    "hidden": false
                },
                {
                    "_id": "6837dd74ec10479b9605da1c",
                    "name": "Shiqi Yang",
                    "hidden": false
                },
                {
                    "_id": "6837dd74ec10479b9605da1d",
                    "name": "Yaxing Wang",
                    "hidden": false
                },
                {
                    "_id": "6837dd74ec10479b9605da1e",
                    "name": "Jian Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T04:23:22.000Z",
            "submittedOnDailyAt": "2025-05-29T02:44:09.323Z",
            "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "637e1cf4f09bf2498c543a73",
                "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
                "isPro": false,
                "fullname": "Senmao Li",
                "user": "senmaonk",
                "type": "user"
            },
            "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.",
            "upvotes": 4,
            "discussionId": "6837dd78ec10479b9605db06",
            "githubRepo": "https://github.com/sen-mao/Loopfree",
            "ai_summary": "Time-independent Unified Encoder TiUE reduces inference time and improves diversity and quality in Text-to-Image diffusion models by sharing encoder features across decoder time steps.",
            "ai_keywords": [
                "Text-to-Image diffusion models",
                "inference speed",
                "image quality",
                "distilled T2I models",
                "UNet encoders",
                "decoders",
                "semantic information",
                "Time-independent Unified Encoder TiUE",
                "loop-free image generation",
                "one-pass scheme",
                "parallel sampling",
                "KL divergence",
                "perceptual realism",
                "LCM",
                "SD-Turbo",
                "SwiftBrushv2"
            ]
        },
        "publishedAt": "2025-05-28T00:23:22.000Z",
        "title": "One-Way Ticket:Time-Independent Unified Encoder for Distilling\n  Text-to-Image Diffusion Models",
        "summary": "Text-to-Image (T2I) diffusion models have made remarkable advancements in\ngenerative modeling; however, they face a trade-off between inference speed and\nimage quality, posing challenges for efficient deployment. Existing distilled\nT2I models can generate high-fidelity images with fewer sampling steps, but\noften struggle with diversity and quality, especially in one-step models. From\nour analysis, we observe redundant computations in the UNet encoders. Our\nfindings suggest that, for T2I diffusion models, decoders are more adept at\ncapturing richer and more explicit semantic information, while encoders can be\neffectively shared across decoders from diverse time steps. Based on these\nobservations, we introduce the first Time-independent Unified Encoder TiUE for\nthe student model UNet architecture, which is a loop-free image generation\napproach for distilling T2I diffusion models. Using a one-pass scheme, TiUE\nshares encoder features across multiple decoder time steps, enabling parallel\nsampling and significantly reducing inference time complexity. In addition, we\nincorporate a KL divergence term to regularize noise prediction, which enhances\nthe perceptual realism and diversity of the generated images. Experimental\nresults demonstrate that TiUE outperforms state-of-the-art methods, including\nLCM, SD-Turbo, and SwiftBrushv2, producing more diverse and realistic results\nwhile maintaining the computational efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21960.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637e1cf4f09bf2498c543a73",
            "avatarUrl": "/avatars/3c0e4e15602a384839bee0c23029f58a.svg",
            "fullname": "Senmao Li",
            "name": "senmaonk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18700",
            "authors": [
                {
                    "_id": "6837cc29bbee677da741aba7",
                    "name": "Chun Wang",
                    "hidden": false
                },
                {
                    "_id": "6837cc29bbee677da741aba8",
                    "name": "Xiaoran Pan",
                    "hidden": false
                },
                {
                    "_id": "6837cc29bbee677da741aba9",
                    "name": "Zihao Pan",
                    "hidden": false
                },
                {
                    "_id": "6837cc29bbee677da741abaa",
                    "name": "Haofan Wang",
                    "hidden": false
                },
                {
                    "_id": "6837cc29bbee677da741abab",
                    "name": "Yiren Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T13:48:57.000Z",
            "submittedOnDailyAt": "2025-05-29T01:25:51.194Z",
            "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains",
            "submittedOnDailyBy": {
                "_id": "64311a95034ecbefddd141ef",
                "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                "isPro": true,
                "fullname": "Yiren Song",
                "user": "yiren98",
                "type": "user"
            },
            "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.",
            "upvotes": 4,
            "discussionId": "6837cc2abbee677da741abf5",
            "ai_summary": "The GRE Suite enhances Visual Language Models with structured reasoning chains, improving geo-localization tasks through a multi-stage strategy and comprehensive evaluation benchmark.",
            "ai_keywords": [
                "Visual Language Models",
                "geo-localization",
                "reasoning chains",
                "GRE30K",
                "GRE model",
                "GREval-Bench",
                "multi-stage reasoning",
                "scene attributes",
                "local details",
                "semantic features",
                "coarse-grained localization",
                "fine-grained localization"
            ]
        },
        "publishedAt": "2025-05-24T09:48:57.000Z",
        "title": "GRE Suite: Geo-localization Inference via Fine-Tuned Vision-Language\n  Models and Enhanced Reasoning Chains",
        "summary": "Recent advances in Visual Language Models (VLMs) have demonstrated\nexceptional performance in visual reasoning tasks. However, geo-localization\npresents unique challenges, requiring the extraction of multigranular visual\ncues from images and their integration with external world knowledge for\nsystematic reasoning. Current approaches to geo-localization tasks often lack\nrobust reasoning mechanisms and explainability, limiting their effectiveness.\nTo address these limitations, we propose the Geo Reason Enhancement (GRE)\nSuite, a novel framework that augments VLMs with structured reasoning chains\nfor accurate and interpretable location inference. The GRE Suite is\nsystematically developed across three key dimensions: dataset, model, and\nbenchmark. First, we introduce GRE30K, a high-quality geo-localization\nreasoning dataset designed to facilitate fine-grained visual and contextual\nanalysis. Next, we present the GRE model, which employs a multi-stage reasoning\nstrategy to progressively infer scene attributes, local details, and semantic\nfeatures, thereby narrowing down potential geographic regions with enhanced\nprecision. Finally, we construct the Geo Reason Evaluation Benchmark\n(GREval-Bench), a comprehensive evaluation framework that assesses VLMs across\ndiverse urban, natural, and landmark scenes to measure both coarse-grained\n(e.g., country, continent) and fine-grained (e.g., city, street) localization\nperformance. Experimental results demonstrate that GRE significantly\noutperforms existing methods across all granularities of geo-localization\ntasks, underscoring the efficacy of reasoning-augmented VLMs in complex\ngeographic inference. Code and data will be released at\nhttps://github.com/Thorin215/GRE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18700.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64311a95034ecbefddd141ef",
            "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
            "fullname": "Yiren Song",
            "name": "yiren98",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.17870",
            "authors": [
                {
                    "_id": "6837f049023315653be65a88",
                    "user": {
                        "_id": "640f32f6ef5c6dcac8b094bd",
                        "avatarUrl": "/avatars/89b95837666ad696fe1f10808e4619b0.svg",
                        "isPro": false,
                        "fullname": "Shaina Raza",
                        "user": "Shainarazavi",
                        "type": "user"
                    },
                    "name": "Shaina Raza",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-29T05:27:38.248Z",
                    "hidden": false
                },
                {
                    "_id": "6837f049023315653be65a89",
                    "name": "Rizwan Qureshi",
                    "hidden": false
                },
                {
                    "_id": "6837f049023315653be65a8a",
                    "name": "Marcelo Lotif",
                    "hidden": false
                },
                {
                    "_id": "6837f049023315653be65a8b",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:40.928Z",
                    "hidden": false
                },
                {
                    "_id": "6837f049023315653be65a8c",
                    "name": "Deval Pandya",
                    "hidden": false
                },
                {
                    "_id": "6837f049023315653be65a8d",
                    "name": "Christos Emmanouilidis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T13:20:23.000Z",
            "submittedOnDailyAt": "2025-05-29T04:08:04.627Z",
            "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods",
            "submittedOnDailyBy": {
                "_id": "63a4754927f1f64ed7238dac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                "isPro": false,
                "fullname": "Aman Chadha",
                "user": "amanchadha",
                "type": "user"
            },
            "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.",
            "upvotes": 4,
            "discussionId": "6837f04a023315653be65ac6",
            "ai_summary": "A generative AI model is fine-tuned with labeled falsehoods to reduce misinformation generation, analogous to biological immunization.",
            "ai_keywords": [
                "generative AI models",
                "misinformation",
                "fine-tuning",
                "labeled falsehoods",
                "immunization",
                "fact-checked falsehoods",
                "supervised vaccine"
            ]
        },
        "publishedAt": "2025-05-23T09:20:23.000Z",
        "title": "Just as Humans Need Vaccines, So Do Models: Model Immunization to Combat\n  Falsehoods",
        "summary": "Generative AI models often learn and reproduce false information present in\ntheir training corpora. This position paper argues that, analogous to\nbiological immunization, where controlled exposure to a weakened pathogen\nbuilds immunity, AI models should be fine tuned on small, quarantined sets of\nexplicitly labeled falsehoods as a \"vaccine\" against misinformation. These\ncurated false examples are periodically injected during finetuning,\nstrengthening the model ability to recognize and reject misleading claims while\npreserving accuracy on truthful inputs. An illustrative case study shows that\nimmunized models generate substantially less misinformation than baselines. To\nour knowledge, this is the first training framework that treats fact checked\nfalsehoods themselves as a supervised vaccine, rather than relying on input\nperturbations or generic human feedback signals, to harden models against\nfuture misinformation. We also outline ethical safeguards and governance\ncontrols to ensure the safe use of false data. Model immunization offers a\nproactive paradigm for aligning AI systems with factuality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17870.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22664",
            "authors": [
                {
                    "_id": "6837befa0aa18c6f96f4a9a5",
                    "name": "Kaiyu Yue",
                    "hidden": false
                },
                {
                    "_id": "6837befa0aa18c6f96f4a9a6",
                    "name": "Vasu Singla",
                    "hidden": false
                },
                {
                    "_id": "6837befa0aa18c6f96f4a9a7",
                    "name": "Menglin Jia",
                    "hidden": false
                },
                {
                    "_id": "6837befa0aa18c6f96f4a9a8",
                    "name": "John Kirchenbauer",
                    "hidden": false
                },
                {
                    "_id": "6837befa0aa18c6f96f4a9a9",
                    "name": "Rifaa Qadri",
                    "hidden": false
                },
                {
                    "_id": "6837befa0aa18c6f96f4a9aa",
                    "name": "Zikui Cai",
                    "hidden": false
                },
                {
                    "_id": "6837befa0aa18c6f96f4a9ab",
                    "name": "Abhinav Bhatele",
                    "hidden": false
                },
                {
                    "_id": "6837befa0aa18c6f96f4a9ac",
                    "name": "Furong Huang",
                    "hidden": false
                },
                {
                    "_id": "6837befa0aa18c6f96f4a9ad",
                    "name": "Tom Goldstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:59:59.000Z",
            "submittedOnDailyAt": "2025-05-29T12:40:35.423Z",
            "title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates",
            "submittedOnDailyBy": {
                "_id": "640d0dbc8036cc2142273a83",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d0dbc8036cc2142273a83/TQHyWWMGuXJKEM6fTPf-z.png",
                "isPro": false,
                "fullname": "Kaiyu Yue",
                "user": "kaiyuyue",
                "type": "user"
            },
            "summary": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder.",
            "upvotes": 3,
            "discussionId": "6837befb0aa18c6f96f4a9fd",
            "githubRepo": "https://github.com/facebookresearch/zero",
            "ai_summary": "The approach of training vision encoders with small surrogate models before transferring them to large language models reduces training costs and enhances performance.",
            "ai_keywords": [
                "vision language models",
                "vision encoder",
                "large language model",
                "small language model",
                "surrogate models",
                "embedding space",
                "representation language",
                "zero-shot grafting"
            ]
        },
        "publishedAt": "2025-05-28T13:59:59.000Z",
        "title": "Zero-Shot Vision Encoder Grafting via LLM Surrogates",
        "summary": "Vision language models (VLMs) typically pair a modestly sized vision encoder\nwith a large language model (LLM), e.g., Llama-70B, making the decoder the\nprimary computational burden during training. To reduce costs, a potential\npromising strategy is to first train the vision encoder using a small language\nmodel before transferring it to the large one. We construct small \"surrogate\nmodels\" that share the same embedding space and representation language as the\nlarge target LLM by directly inheriting its shallow layers. Vision encoders\ntrained on the surrogate can then be directly transferred to the larger model,\na process we call zero-shot grafting -- when plugged directly into the\nfull-size target LLM, the grafted pair surpasses the encoder-surrogate pair\nand, on some benchmarks, even performs on par with full decoder training with\nthe target LLM. Furthermore, our surrogate training approach reduces overall\nVLM training costs by ~45% when using Llama-70B as the decoder.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22664.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640d0dbc8036cc2142273a83",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640d0dbc8036cc2142273a83/TQHyWWMGuXJKEM6fTPf-z.png",
            "fullname": "Kaiyu Yue",
            "name": "kaiyuyue",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21060",
            "authors": [
                {
                    "_id": "683818841902f641cc669774",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "683818841902f641cc669775",
                    "name": "Xiang Liu",
                    "hidden": false
                },
                {
                    "_id": "683818841902f641cc669776",
                    "name": "Peidong Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T11:47:15.000Z",
            "submittedOnDailyAt": "2025-05-29T06:49:42.295Z",
            "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": false,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.",
            "upvotes": 3,
            "discussionId": "6838188b1902f641cc669947",
            "ai_summary": "A novel feed-forward model achieves fast 3D stylization using sparse view images, maintaining multi-view consistency and high-quality style transfer while retaining reconstruction accuracy.",
            "ai_keywords": [
                "feed-forward reconstruction models",
                "3D stylization",
                "dense posed input images",
                "unposed sparse-view scene images",
                "branched architecture",
                "structure modeling",
                "appearance shading",
                "identity loss",
                "novel view synthesis",
                "in-domain datasets",
                "out-of-domain datasets",
                "multi-view consistency"
            ]
        },
        "publishedAt": "2025-05-27T07:47:15.000Z",
        "title": "Styl3R: Instant 3D Stylized Reconstruction for Arbitrary Scenes and\n  Styles",
        "summary": "Stylizing 3D scenes instantly while maintaining multi-view consistency and\nfaithfully resembling a style image remains a significant challenge. Current\nstate-of-the-art 3D stylization methods typically involve computationally\nintensive test-time optimization to transfer artistic features into a\npretrained 3D representation, often requiring dense posed input images. In\ncontrast, leveraging recent advances in feed-forward reconstruction models, we\ndemonstrate a novel approach to achieve direct 3D stylization in less than a\nsecond using unposed sparse-view scene images and an arbitrary style image. To\naddress the inherent decoupling between reconstruction and stylization, we\nintroduce a branched architecture that separates structure modeling and\nappearance shading, effectively preventing stylistic transfer from distorting\nthe underlying 3D scene structure. Furthermore, we adapt an identity loss to\nfacilitate pre-training our stylization model through the novel view synthesis\ntask. This strategy also allows our model to retain its original reconstruction\ncapabilities while being fine-tuned for stylization. Comprehensive evaluations,\nusing both in-domain and out-of-domain datasets, demonstrate that our approach\nproduces high-quality stylized 3D content that achieve a superior blend of\nstyle and scene appearance, while also outperforming existing methods in terms\nof multi-view consistency and efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21060.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 874
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20298",
            "authors": [
                {
                    "_id": "6838433e66c3531dee69e88b",
                    "name": "Jeonghun Baek",
                    "hidden": false
                },
                {
                    "_id": "6838433e66c3531dee69e88c",
                    "name": "Kazuki Egashira",
                    "hidden": false
                },
                {
                    "_id": "6838433e66c3531dee69e88d",
                    "name": "Shota Onohara",
                    "hidden": false
                },
                {
                    "_id": "6838433e66c3531dee69e88e",
                    "user": {
                        "_id": "6527b37c0ae663e384eb1b85",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
                        "isPro": true,
                        "fullname": "Atsuyuki Miyai",
                        "user": "AtsuMiyai",
                        "type": "user"
                    },
                    "name": "Atsuyuki Miyai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:19:13.446Z",
                    "hidden": false
                },
                {
                    "_id": "6838433e66c3531dee69e88f",
                    "name": "Yuki Imajuku",
                    "hidden": false
                },
                {
                    "_id": "6838433e66c3531dee69e890",
                    "name": "Hikaru Ikuta",
                    "hidden": false
                },
                {
                    "_id": "6838433e66c3531dee69e891",
                    "name": "Kiyoharu Aizawa",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:59:59.000Z",
            "submittedOnDailyAt": "2025-05-29T09:51:59.095Z",
            "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal\n  Manga Understanding",
            "submittedOnDailyBy": {
                "_id": "6527b37c0ae663e384eb1b85",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
                "isPro": true,
                "fullname": "Atsuyuki Miyai",
                "user": "AtsuMiyai",
                "type": "user"
            },
            "summary": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga.",
            "upvotes": 3,
            "discussionId": "6838434066c3531dee69e90e",
            "ai_summary": "Two new benchmarks, MangaOCR and MangaVQA, and a specialized model, MangaLMM, are introduced to evaluate and advance large multimodal models in understanding manga narratives.",
            "ai_keywords": [
                "Multimodal models",
                "MangaOCR",
                "MangaVQA",
                "contextual understanding",
                "visual question answering",
                "MangaLMM",
                "Qwen2.5-VL",
                "GPT-4o",
                "Gemini 2.5"
            ]
        },
        "publishedAt": "2025-05-26T13:59:59.000Z",
        "title": "MangaVQA and MangaLMM: A Benchmark and Specialized Model for Multimodal\n  Manga Understanding",
        "summary": "Manga, or Japanese comics, is a richly multimodal narrative form that blends\nimages and text in complex ways. Teaching large multimodal models (LMMs) to\nunderstand such narratives at a human-like level could help manga creators\nreflect on and refine their stories. To this end, we introduce two benchmarks\nfor multimodal manga understanding: MangaOCR, which targets in-page text\nrecognition, and MangaVQA, a novel benchmark designed to evaluate contextual\nunderstanding through visual question answering. MangaVQA consists of 526\nhigh-quality, manually constructed question-answer pairs, enabling reliable\nevaluation across diverse narrative and visual scenarios. Building on these\nbenchmarks, we develop MangaLMM, a manga-specialized model finetuned from the\nopen-source LMM Qwen2.5-VL to jointly handle both tasks. Through extensive\nexperiments, including comparisons with proprietary models such as GPT-4o and\nGemini 2.5, we assess how well LMMs understand manga. Our benchmark and model\nprovide a comprehensive foundation for evaluating and advancing LMMs in the\nrichly narrative domain of manga.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20298.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6527b37c0ae663e384eb1b85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
            "fullname": "Atsuyuki Miyai",
            "name": "AtsuMiyai",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19051",
            "authors": [
                {
                    "_id": "68383ce6023315653bf9c3be",
                    "user": {
                        "_id": "6526b8ebba9a8279c139616b",
                        "avatarUrl": "/avatars/09f6b677603a03be128996a0765233e6.svg",
                        "isPro": false,
                        "fullname": "Mahdi Nikdan",
                        "user": "mnikdan97",
                        "type": "user"
                    },
                    "name": "Mahdi Nikdan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:19:11.613Z",
                    "hidden": false
                },
                {
                    "_id": "68383ce6023315653bf9c3bf",
                    "name": "Vincent Cohen-Addad",
                    "hidden": false
                },
                {
                    "_id": "68383ce6023315653bf9c3c0",
                    "name": "Dan Alistarh",
                    "hidden": false
                },
                {
                    "_id": "68383ce6023315653bf9c3c1",
                    "name": "Vahab Mirrokni",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T09:08:00.000Z",
            "submittedOnDailyAt": "2025-05-29T09:30:16.629Z",
            "title": "Efficient Data Selection at Scale via Influence Distillation",
            "submittedOnDailyBy": {
                "_id": "6526b8ebba9a8279c139616b",
                "avatarUrl": "/avatars/09f6b677603a03be128996a0765233e6.svg",
                "isPro": false,
                "fullname": "Mahdi Nikdan",
                "user": "mnikdan97",
                "type": "user"
            },
            "summary": "Effective data selection is critical for efficient training of modern Large\nLanguage Models (LLMs). This paper introduces Influence Distillation, a novel,\nmathematically-justified framework for data selection that employs second-order\ninformation to optimally weight training samples. By distilling each sample's\ninfluence on a target distribution, our method assigns model-specific weights\nthat are used to select training data for LLM fine-tuning, guiding it toward\nstrong performance on the target domain. We derive these optimal weights for\nboth Gradient Descent and Adam optimizers. To ensure scalability and reduce\ncomputational cost, we propose a landmark-based approximation:\ninfluence is precisely computed for a small subset of \"landmark\" samples and\nthen efficiently propagated to all other samples to determine their weights. We\nvalidate Influence Distillation by applying it to instruction tuning on the\nTulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,\nacross several models from the Llama and Qwen families. Experiments show that\nInfluence Distillation matches or outperforms state-of-the-art performance\nwhile achieving up to 3.5times faster selection.",
            "upvotes": 3,
            "discussionId": "68383ce7023315653bf9c3ea",
            "githubRepo": "https://github.com/IST-DASLab/influence_distillation",
            "ai_summary": "Influence Distillation, using second-order information, optimally selects training data for LLM fine-tuning with landmark-based approximation, achieving faster and competitive performance on various tasks.",
            "ai_keywords": [
                "Influence Distillation",
                "second-order information",
                "model-specific weights",
                "Gradient Descent",
                "Adam optimizers",
                "landmark-based approximation",
                "LLM fine-tuning",
                "instruction tuning",
                "Tulu V2 dataset",
                "GSM8k",
                "SQuAD",
                "MMLU",
                "Llama",
                "Qwen"
            ]
        },
        "publishedAt": "2025-05-25T05:08:00.000Z",
        "title": "Efficient Data Selection at Scale via Influence Distillation",
        "summary": "Effective data selection is critical for efficient training of modern Large\nLanguage Models (LLMs). This paper introduces Influence Distillation, a novel,\nmathematically-justified framework for data selection that employs second-order\ninformation to optimally weight training samples. By distilling each sample's\ninfluence on a target distribution, our method assigns model-specific weights\nthat are used to select training data for LLM fine-tuning, guiding it toward\nstrong performance on the target domain. We derive these optimal weights for\nboth Gradient Descent and Adam optimizers. To ensure scalability and reduce\ncomputational cost, we propose a landmark-based approximation:\ninfluence is precisely computed for a small subset of \"landmark\" samples and\nthen efficiently propagated to all other samples to determine their weights. We\nvalidate Influence Distillation by applying it to instruction tuning on the\nTulu V2 dataset, targeting a range of tasks including GSM8k, SQuAD, and MMLU,\nacross several models from the Llama and Qwen families. Experiments show that\nInfluence Distillation matches or outperforms state-of-the-art performance\nwhile achieving up to 3.5times faster selection.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19051.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6526b8ebba9a8279c139616b",
            "avatarUrl": "/avatars/09f6b677603a03be128996a0765233e6.svg",
            "fullname": "Mahdi Nikdan",
            "name": "mnikdan97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17507",
            "authors": [
                {
                    "_id": "6833f24ed5c438959f7decf9",
                    "user": {
                        "_id": "619ef3f253061ce00477b09e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
                        "isPro": false,
                        "fullname": "Qiaosheng Chen",
                        "user": "cqsss",
                        "type": "user"
                    },
                    "name": "Qiaosheng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T09:01:00.198Z",
                    "hidden": false
                },
                {
                    "_id": "6833f24ed5c438959f7decfa",
                    "name": "Kaijia Huang",
                    "hidden": false
                },
                {
                    "_id": "6833f24ed5c438959f7decfb",
                    "name": "Xiao Zhou",
                    "hidden": false
                },
                {
                    "_id": "6833f24ed5c438959f7decfc",
                    "name": "Weiqing Luo",
                    "hidden": false
                },
                {
                    "_id": "6833f24ed5c438959f7decfd",
                    "name": "Yuanning Cui",
                    "hidden": false
                },
                {
                    "_id": "6833f24ed5c438959f7decfe",
                    "name": "Gong Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T06:00:20.000Z",
            "submittedOnDailyAt": "2025-05-29T00:53:38.693Z",
            "title": "Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph",
            "submittedOnDailyBy": {
                "_id": "619ef3f253061ce00477b09e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
                "isPro": false,
                "fullname": "Qiaosheng Chen",
                "user": "cqsss",
                "type": "user"
            },
            "summary": "The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.",
            "upvotes": 3,
            "discussionId": "6833f24ed5c438959f7ded31",
            "githubRepo": "https://github.com/nju-websoft/HuggingBench",
            "ai_summary": "HuggingKG, a large-scale knowledge graph, enhances open source ML resource management by enabling advanced queries and analyses via HuggingBench.",
            "ai_keywords": [
                "knowledge graph",
                "resource recommendation",
                "classification",
                "tracing",
                "multi-task benchmark"
            ]
        },
        "publishedAt": "2025-05-23T02:00:20.000Z",
        "title": "Benchmarking Recommendation, Classification, and Tracing Based on\n  Hugging Face Knowledge Graph",
        "summary": "The rapid growth of open source machine learning (ML) resources, such as\nmodels and datasets, has accelerated IR research. However, existing platforms\nlike Hugging Face do not explicitly utilize structured representations,\nlimiting advanced queries and analyses such as tracing model evolution and\nrecommending relevant datasets. To fill the gap, we construct HuggingKG, the\nfirst large-scale knowledge graph built from the Hugging Face community for ML\nresource management. With 2.6 million nodes and 6.2 million edges, HuggingKG\ncaptures domain-specific relations and rich textual attributes. It enables us\nto further present HuggingBench, a multi-task benchmark with three novel test\ncollections for IR tasks including resource recommendation, classification, and\ntracing. Our experiments reveal unique characteristics of HuggingKG and the\nderived tasks. Both resources are publicly available, expected to advance\nresearch in open source resource sharing and management.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17507.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "619ef3f253061ce00477b09e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619ef3f253061ce00477b09e/FknZhgQhV2_3aTqIKVsTo.jpeg",
            "fullname": "Qiaosheng Chen",
            "name": "cqsss",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.15813",
            "authors": [
                {
                    "_id": "68380974717461677df17514",
                    "name": "Muquan Yu",
                    "hidden": false
                },
                {
                    "_id": "68380974717461677df17515",
                    "name": "Mu Nan",
                    "hidden": false
                },
                {
                    "_id": "68380974717461677df17516",
                    "name": "Hossein Adeli",
                    "hidden": false
                },
                {
                    "_id": "68380974717461677df17517",
                    "name": "Jacob S. Prince",
                    "hidden": false
                },
                {
                    "_id": "68380974717461677df17518",
                    "name": "John A. Pyles",
                    "hidden": false
                },
                {
                    "_id": "68380974717461677df17519",
                    "name": "Leila Wehbe",
                    "hidden": false
                },
                {
                    "_id": "68380974717461677df1751a",
                    "name": "Margaret M. Henderson",
                    "hidden": false
                },
                {
                    "_id": "68380974717461677df1751b",
                    "name": "Michael J. Tarr",
                    "hidden": false
                },
                {
                    "_id": "68380974717461677df1751c",
                    "user": {
                        "_id": "64b6ce23dbbd1f2cdb624d56",
                        "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
                        "isPro": false,
                        "fullname": "Andrew Luo",
                        "user": "aluo-x",
                        "type": "user"
                    },
                    "name": "Andrew F. Luo",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-29T07:16:44.495Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
            ],
            "publishedAt": "2025-05-21T17:59:41.000Z",
            "submittedOnDailyAt": "2025-05-29T05:45:43.922Z",
            "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex",
            "submittedOnDailyBy": {
                "_id": "64b6ce23dbbd1f2cdb624d56",
                "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
                "isPro": false,
                "fullname": "Andrew Luo",
                "user": "aluo-x",
                "type": "user"
            },
            "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.",
            "upvotes": 3,
            "discussionId": "68380977717461677df17638",
            "ai_summary": "BraInCoRL employs a transformer-based in-context learning approach to model higher visual cortex neural responses with few-shot examples, demonstrating superior performance and generalizability across new subjects, stimuli, and datasets.",
            "ai_keywords": [
                "functional representations",
                "higher visual cortex",
                "artificial neural networks",
                "fMRI datasets",
                "in-context learning",
                "transformer architecture",
                "inductive bias",
                "voxelwise neural responses",
                "image features",
                "voxel activations",
                "test-time scaling",
                "interpretability",
                "natural language queries",
                "voxel selectivity"
            ]
        },
        "publishedAt": "2025-05-21T13:59:41.000Z",
        "title": "Meta-Learning an In-Context Transformer Model of Human Higher Visual\n  Cortex",
        "summary": "Understanding functional representations within higher visual cortex is a\nfundamental question in computational neuroscience. While artificial neural\nnetworks pretrained on large-scale datasets exhibit striking representational\nalignment with human neural responses, learning image-computable models of\nvisual cortex relies on individual-level, large-scale fMRI datasets. The\nnecessity for expensive, time-intensive, and often impractical data acquisition\nlimits the generalizability of encoders to new subjects and stimuli. BraInCoRL\nuses in-context learning to predict voxelwise neural responses from few-shot\nexamples without any additional finetuning for novel subjects and stimuli. We\nleverage a transformer architecture that can flexibly condition on a variable\nnumber of in-context image stimuli, learning an inductive bias over multiple\nsubjects. During training, we explicitly optimize the model for in-context\nlearning. By jointly conditioning on image features and voxel activations, our\nmodel learns to directly generate better performing voxelwise models of higher\nvisual cortex. We demonstrate that BraInCoRL consistently outperforms existing\nvoxelwise encoder designs in a low-data regime when evaluated on entirely novel\nimages, while also exhibiting strong test-time scaling behavior. The model also\ngeneralizes to an entirely new visual fMRI dataset, which uses different\nsubjects and fMRI data acquisition parameters. Further, BraInCoRL facilitates\nbetter interpretability of neural signals in higher visual cortex by attending\nto semantically relevant stimuli. Finally, we show that our framework enables\ninterpretable mappings from natural language queries to voxel selectivity.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b6ce23dbbd1f2cdb624d56/vCXz4Xv4AgHNNIC1S8aYf.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15813.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b6ce23dbbd1f2cdb624d56",
            "avatarUrl": "/avatars/022738ce8f76fa9545b3e363d7264b53.svg",
            "fullname": "Andrew Luo",
            "name": "aluo-x",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.12667",
            "authors": [
                {
                    "_id": "682dd41740c6417d995087de",
                    "user": {
                        "_id": "648dca31385b84261811505d",
                        "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
                        "isPro": false,
                        "fullname": "Zihan Su",
                        "user": "Sugewud",
                        "type": "user"
                    },
                    "name": "Zihan Su",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-05-28T16:17:54.970Z",
                    "hidden": false
                },
                {
                    "_id": "682dd41740c6417d995087df",
                    "name": "Xuerui Qiu",
                    "hidden": false
                },
                {
                    "_id": "682dd41740c6417d995087e0",
                    "name": "Hongbin Xu",
                    "hidden": false
                },
                {
                    "_id": "682dd41740c6417d995087e1",
                    "name": "Tangyu Jiang",
                    "hidden": false
                },
                {
                    "_id": "682dd41740c6417d995087e2",
                    "name": "Junhao Zhuang",
                    "hidden": false
                },
                {
                    "_id": "682dd41740c6417d995087e3",
                    "name": "Chun Yuan",
                    "hidden": false
                },
                {
                    "_id": "682dd41740c6417d995087e4",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "682dd41740c6417d995087e5",
                    "name": "Shengfeng He",
                    "hidden": false
                },
                {
                    "_id": "682dd41740c6417d995087e6",
                    "name": "Fei Richard Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T03:31:31.000Z",
            "submittedOnDailyAt": "2025-05-29T01:30:41.124Z",
            "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
            "submittedOnDailyBy": {
                "_id": "648dca31385b84261811505d",
                "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
                "isPro": false,
                "fullname": "Zihan Su",
                "user": "Sugewud",
                "type": "user"
            },
            "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication.",
            "upvotes": 3,
            "discussionId": "682dd41840c6417d99508847",
            "projectPage": "https://sugewud.github.io/Safe-Sora-project/",
            "githubRepo": "https://github.com/Sugewud/Safe-Sora",
            "ai_summary": "Safe-Sora embeds invisible watermarks into AI-generated videos using a hierarchical adaptive matching mechanism and a 3D wavelet transform-enhanced Mamba architecture, achieving top performance in video quality, watermark fidelity, and robustness.",
            "ai_keywords": [
                "generative watermarking",
                "hierarchical coarse-to-fine adaptive matching",
                "3D wavelet transform",
                "Mamba architecture",
                "spatiotemporal local scanning",
                "state space models",
                "watermark embedding",
                "watermark retrieval"
            ]
        },
        "publishedAt": "2025-05-18T23:31:31.000Z",
        "title": "Safe-Sora: Safe Text-to-Video Generation via Graphical Watermarking",
        "summary": "The explosive growth of generative video models has amplified the demand for\nreliable copyright preservation of AI-generated content. Despite its popularity\nin image synthesis, invisible generative watermarking remains largely\nunderexplored in video generation. To address this gap, we propose Safe-Sora,\nthe first framework to embed graphical watermarks directly into the video\ngeneration process. Motivated by the observation that watermarking performance\nis closely tied to the visual similarity between the watermark and cover\ncontent, we introduce a hierarchical coarse-to-fine adaptive matching\nmechanism. Specifically, the watermark image is divided into patches, each\nassigned to the most visually similar video frame, and further localized to the\noptimal spatial region for seamless embedding. To enable spatiotemporal fusion\nof watermark patches across video frames, we develop a 3D wavelet\ntransform-enhanced Mamba architecture with a novel spatiotemporal local\nscanning strategy, effectively modeling long-range dependencies during\nwatermark embedding and retrieval. To the best of our knowledge, this is the\nfirst attempt to apply state space models to watermarking, opening new avenues\nfor efficient and robust watermark protection. Extensive experiments\ndemonstrate that Safe-Sora achieves state-of-the-art performance in terms of\nvideo quality, watermark fidelity, and robustness, which is largely attributed\nto our proposals. We will release our code upon publication.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12667.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648dca31385b84261811505d",
            "avatarUrl": "/avatars/dfd124e3b5ffed8b0d3f429be0f7bdc0.svg",
            "fullname": "Zihan Su",
            "name": "Sugewud",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22645",
            "authors": [
                {
                    "_id": "6837dbed1233747046da00f5",
                    "user": {
                        "_id": "64c939307dba66c3a7e4d215",
                        "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
                        "isPro": false,
                        "fullname": "BruceLyu",
                        "user": "brucelyu",
                        "type": "user"
                    },
                    "name": "Hanjia Lyu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:19:00.877Z",
                    "hidden": false
                },
                {
                    "_id": "6837dbed1233747046da00f6",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "6837dbed1233747046da00f7",
                    "name": "Jian Kang",
                    "hidden": false
                },
                {
                    "_id": "6837dbed1233747046da00f8",
                    "name": "Allison Koenecke",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:56:49.000Z",
            "submittedOnDailyAt": "2025-05-29T02:33:57.876Z",
            "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
            "submittedOnDailyBy": {
                "_id": "64c939307dba66c3a7e4d215",
                "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
                "isPro": false,
                "fullname": "BruceLyu",
                "user": "brucelyu",
                "type": "user"
            },
            "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
            "upvotes": 2,
            "discussionId": "6837dbee1233747046da0125",
            "githubRepo": "https://github.com/brucelyu17/SC-TC-Bench",
            "ai_summary": "Research examines LLM performance biases between Simplified and Traditional Chinese in regional term and name choice tasks, attributing differences to training data and tokenization.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "LLM-facilitated decision-making",
                "regional term choice",
                "regional name choice",
                "open-sourced benchmark dataset",
                "SC-TC-Bench"
            ]
        },
        "publishedAt": "2025-05-28T13:56:49.000Z",
        "title": "Characterizing Bias: Benchmarking Large Language Models in Simplified\n  versus Traditional Chinese",
        "summary": "While the capabilities of Large Language Models (LLMs) have been studied in\nboth Simplified and Traditional Chinese, it is yet unclear whether LLMs exhibit\ndifferential performance when prompted in these two variants of written\nChinese. This understanding is critical, as disparities in the quality of LLM\nresponses can perpetuate representational harms by ignoring the different\ncultural contexts underlying Simplified versus Traditional Chinese, and can\nexacerbate downstream harms in LLM-facilitated decision-making in domains such\nas education or hiring. To investigate potential LLM performance disparities,\nwe design two benchmark tasks that reflect real-world scenarios: regional term\nchoice (prompting the LLM to name a described item which is referred to\ndifferently in Mainland China and Taiwan), and regional name choice (prompting\nthe LLM to choose who to hire from a list of names in both Simplified and\nTraditional Chinese). For both tasks, we audit the performance of 11 leading\ncommercial LLM services and open-sourced models -- spanning those primarily\ntrained on English, Simplified Chinese, or Traditional Chinese. Our analyses\nindicate that biases in LLM responses are dependent on both the task and\nprompting language: while most LLMs disproportionately favored Simplified\nChinese responses in the regional term choice task, they surprisingly favored\nTraditional Chinese names in the regional name choice task. We find that these\ndisparities may arise from differences in training data representation, written\ncharacter preferences, and tokenization of Simplified and Traditional Chinese.\nThese findings highlight the need for further analysis of LLM biases; as such,\nwe provide an open-sourced benchmark dataset to foster reproducible evaluations\nof future LLM behavior across Chinese language variants\n(https://github.com/brucelyu17/SC-TC-Bench).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22645.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c939307dba66c3a7e4d215",
            "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
            "fullname": "BruceLyu",
            "name": "brucelyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21191",
            "authors": [
                {
                    "_id": "6837eb38f09a146728a4b80f",
                    "name": "Junyan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b810",
                    "name": "Yubo Gao",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b811",
                    "name": "Yibo Yan",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b812",
                    "name": "Jungang Li",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b813",
                    "name": "Zhaorui Hou",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b814",
                    "name": "Sicheng Tao",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b815",
                    "name": "Shuliang Liu",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b816",
                    "name": "Song Dai",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b817",
                    "name": "Yonghua Hei",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b818",
                    "name": "Junzhuo Li",
                    "hidden": false
                },
                {
                    "_id": "6837eb38f09a146728a4b819",
                    "name": "Xuming Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T13:40:28.000Z",
            "submittedOnDailyAt": "2025-05-29T03:36:16.801Z",
            "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities",
            "submittedOnDailyBy": {
                "_id": "64b76528fdb702b3d8641514",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
                "isPro": false,
                "fullname": "Jungang Li",
                "user": "Jungang",
                "type": "user"
            },
            "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.",
            "upvotes": 2,
            "discussionId": "6837eb39f09a146728a4b872",
            "ai_summary": "The study investigates the role of sparse computational components in the instruction-following capabilities of Large Language Models through systematic analysis and introduces HexaInst and SPARCOM for better understanding.",
            "ai_keywords": [
                "Large Language Models",
                "fine-tuning",
                "instruction-following",
                "HexaInst",
                "SPARCOM",
                "sparse components",
                "neurons",
                "Mixture-of-Experts",
                "instruction execution",
                "computational adaptations"
            ]
        },
        "publishedAt": "2025-05-27T09:40:28.000Z",
        "title": "Unveiling Instruction-Specific Neurons & Experts: An Analytical\n  Framework for LLM's Instruction-Following Capabilities",
        "summary": "The finetuning of Large Language Models (LLMs) has significantly advanced\ntheir instruction-following capabilities, yet the underlying computational\nmechanisms driving these improvements remain poorly understood. This study\nsystematically examines how fine-tuning reconfigures LLM computations by\nisolating and analyzing instruction-specific sparse components, i.e., neurons\nin dense models and both neurons and experts in Mixture-of-Experts (MoE)\narchitectures. In particular, we introduce HexaInst, a carefully curated and\nbalanced instructional dataset spanning six distinct categories, and propose\nSPARCOM, a novel analytical framework comprising three key contributions: (1) a\nmethod for identifying these sparse components, (2) an evaluation of their\nfunctional generality and uniqueness, and (3) a systematic comparison of their\nalterations. Through experiments, we demonstrate functional generality,\nuniqueness, and the critical role of these components in instruction execution.\nBy elucidating the relationship between fine-tuning-induced adaptations and\nsparse computational substrates, this work provides deeper insights into how\nLLMs internalize instruction-following behavior for the trustworthy LLM\ncommunity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21191.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64b76528fdb702b3d8641514",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b76528fdb702b3d8641514/wCbEtpPEHRjS4sZSVK1gK.png",
            "fullname": "Jungang Li",
            "name": "Jungang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21582",
            "authors": [
                {
                    "_id": "68380c860fb1ddbe91ba0bf9",
                    "user": {
                        "_id": "631f5035c6b20f03c823c4ba",
                        "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
                        "isPro": false,
                        "fullname": "Christopher Knievel",
                        "user": "CKnievel",
                        "type": "user"
                    },
                    "name": "Christopher Knievel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:12.986Z",
                    "hidden": false
                },
                {
                    "_id": "68380c860fb1ddbe91ba0bfa",
                    "name": "Alexander Bernhardt",
                    "hidden": false
                },
                {
                    "_id": "68380c860fb1ddbe91ba0bfb",
                    "name": "Christian Bernhardt",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T10:07:05.000Z",
            "submittedOnDailyAt": "2025-05-29T06:18:15.842Z",
            "title": "AITEE -- Agentic Tutor for Electrical Engineering",
            "submittedOnDailyBy": {
                "_id": "631f5035c6b20f03c823c4ba",
                "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
                "isPro": false,
                "fullname": "Christopher Knievel",
                "user": "CKnievel",
                "type": "user"
            },
            "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.",
            "upvotes": 2,
            "discussionId": "68380c870fb1ddbe91ba0c55",
            "ai_summary": "An agent-based tutoring system for electrical engineering enhances learning through natural circuit interaction, context-retrieving generation, and guided questioning, demonstrating superior performance compared to baseline methods.",
            "ai_keywords": [
                "agent-based tutoring system",
                "large language models",
                "electrical circuits",
                "adapted circuit reconstruction",
                "graph-based similarity measure",
                "retrieval augmented generation",
                "parallel Spice simulation",
                "Socratic dialogue"
            ]
        },
        "publishedAt": "2025-05-27T06:07:05.000Z",
        "title": "AITEE -- Agentic Tutor for Electrical Engineering",
        "summary": "Intelligent tutoring systems combined with large language models offer a\npromising approach to address students' diverse needs and promote\nself-efficacious learning. While large language models possess good\nfoundational knowledge of electrical engineering basics, they remain\ninsufficiently capable of addressing specific questions about electrical\ncircuits. In this paper, we present AITEE, an agent-based tutoring system for\nelectrical engineering designed to accompany students throughout their learning\nprocess, offer individualized support, and promote self-directed learning.\nAITEE supports both hand-drawn and digital circuits through an adapted circuit\nreconstruction process, enabling natural interaction with students. Our novel\ngraph-based similarity measure identifies relevant context from lecture\nmaterials through a retrieval augmented generation approach, while parallel\nSpice simulation further enhances accuracy in applying solution methodologies.\nThe system implements a Socratic dialogue to foster learner autonomy through\nguided questioning. Experimental evaluations demonstrate that AITEE\nsignificantly outperforms baseline approaches in domain-specific knowledge\napplication, with even medium-sized LLM models showing acceptable performance.\nOur results highlight the potential of agentic tutors to deliver scalable,\npersonalized, and effective learning environments for electrical engineering\neducation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21582.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631f5035c6b20f03c823c4ba",
            "avatarUrl": "/avatars/5557b07bc7bd76f5752b5cf3d8e8f22f.svg",
            "fullname": "Christopher Knievel",
            "name": "CKnievel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20715",
            "authors": [
                {
                    "_id": "6837eef0e237f02cd3c87963",
                    "name": "Fuwen Luo",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c87964",
                    "name": "Shengfeng Lou",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c87965",
                    "name": "Chi Chen",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c87966",
                    "name": "Ziyue Wang",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c87967",
                    "name": "Chenliang Li",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c87968",
                    "name": "Weizhou Shen",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c87969",
                    "name": "Jiyue Guo",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c8796a",
                    "name": "Peng Li",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c8796b",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c8796c",
                    "name": "Ji Zhang",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c8796d",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6837eef0e237f02cd3c8796e",
                    "name": "Yang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T04:50:07.000Z",
            "submittedOnDailyAt": "2025-05-29T03:57:26.118Z",
            "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding",
            "submittedOnDailyBy": {
                "_id": "642086ed290342c5df85662d",
                "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
                "isPro": false,
                "fullname": "Chi Chen",
                "user": "carboncoo",
                "type": "user"
            },
            "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.",
            "upvotes": 2,
            "discussionId": "6837eef1e237f02cd3c8798d",
            "githubRepo": "https://github.com/THUNLP-MT/MUSEG",
            "ai_summary": "MUSEG, an RL-based method with timestamp-aware multi-segment grounding, significantly enhances the temporal understanding of large language models by improving alignment with video segments and demonstrating superior performance in temporal reasoning tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "timestamp-aware multi-segment grounding",
                "temporal understanding",
                "MUSEG",
                "phased rewards",
                "temporal grounding",
                "time-sensitive video QA"
            ]
        },
        "publishedAt": "2025-05-27T00:50:07.000Z",
        "title": "MUSEG: Reinforcing Video Temporal Understanding via Timestamp-Aware\n  Multi-Segment Grounding",
        "summary": "Video temporal understanding is crucial for multimodal large language models\n(MLLMs) to reason over events in videos. Despite recent advances in general\nvideo understanding, current MLLMs still struggle with fine-grained temporal\nreasoning. While reinforcement learning (RL) has been explored to address this\nissue recently, existing RL approaches remain limited in effectiveness. In this\nwork, we propose MUSEG, a novel RL-based method that enhances temporal\nunderstanding by introducing timestamp-aware multi-segment grounding. MUSEG\nenables MLLMs to align queries with multiple relevant video segments, promoting\nmore comprehensive temporal reasoning. To facilitate effective learning, we\ndesign a customized RL training recipe with phased rewards that progressively\nguides the model toward temporally grounded reasoning. Extensive experiments on\ntemporal grounding and time-sensitive video QA tasks demonstrate that MUSEG\nsignificantly outperforms existing methods and generalizes well across diverse\ntemporal understanding scenarios. View our project at\nhttps://github.com/THUNLP-MT/MUSEG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20715.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642086ed290342c5df85662d",
            "avatarUrl": "/avatars/915a4d7b89455ae97b8544c79286ddf8.svg",
            "fullname": "Chi Chen",
            "name": "carboncoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20444",
            "authors": [
                {
                    "_id": "68385abc8c8e3b721702d006",
                    "user": {
                        "_id": "64e86d6e7ef2c72cbcdb27b7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e86d6e7ef2c72cbcdb27b7/RLuJJ1xuO665LMWneVDZz.jpeg",
                        "isPro": false,
                        "fullname": "Haoran Li",
                        "user": "brian13",
                        "type": "user"
                    },
                    "name": "Haoran Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:19:23.117Z",
                    "hidden": false
                },
                {
                    "_id": "68385abc8c8e3b721702d007",
                    "name": "Yingjie Qin",
                    "hidden": false
                },
                {
                    "_id": "68385abc8c8e3b721702d008",
                    "name": "Baoyuan Ou",
                    "hidden": false
                },
                {
                    "_id": "68385abc8c8e3b721702d009",
                    "name": "Lai Xu",
                    "hidden": false
                },
                {
                    "_id": "68385abc8c8e3b721702d00a",
                    "name": "Ruiwen Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T18:37:40.000Z",
            "submittedOnDailyAt": "2025-05-29T12:00:50.440Z",
            "title": "HoPE: Hybrid of Position Embedding for Length Generalization in\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "64e86d6e7ef2c72cbcdb27b7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e86d6e7ef2c72cbcdb27b7/RLuJJ1xuO665LMWneVDZz.jpeg",
                "isPro": false,
                "fullname": "Haoran Li",
                "user": "brian13",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) have made significant progress in multimodal\ntasks. However, their performance often deteriorates in long-context scenarios,\nparticularly long videos. While Rotary Position Embedding (RoPE) has been\nwidely adopted for length generalization in Large Language Models (LLMs),\nextending vanilla RoPE to capture the intricate spatial-temporal dependencies\nin videos remains an unsolved challenge. Existing methods typically allocate\ndifferent frequencies within RoPE to encode 3D positional information. However,\nthese allocation strategies mainly rely on heuristics, lacking in-depth\ntheoretical analysis. In this paper, we first study how different allocation\nstrategies impact the long-context capabilities of VLMs. Our analysis reveals\nthat current multimodal RoPEs fail to reliably capture semantic similarities\nover extended contexts. To address this issue, we propose HoPE, a Hybrid of\nPosition Embedding designed to improve the long-context capabilities of VLMs.\nHoPE introduces a hybrid frequency allocation strategy for reliable semantic\nmodeling over arbitrarily long context, and a dynamic temporal scaling\nmechanism to facilitate robust learning and flexible inference across diverse\ncontext lengths. Extensive experiments across four video benchmarks on long\nvideo understanding and retrieval tasks demonstrate that HoPE consistently\noutperforms existing methods, confirming its effectiveness. Code is available\nat https://github.com/hrlics/HoPE.",
            "upvotes": 2,
            "discussionId": "68385abd8c8e3b721702d060",
            "githubRepo": "https://github.com/hrlics/HoPE",
            "ai_summary": "HoPE, a Hybrid of Position Embedding, enhances VLMs' long-context performance in videos through improved frequency allocation and dynamic temporal scaling.",
            "ai_keywords": [
                "Rotary Position Embedding",
                "RoPE",
                "Large Language Models",
                "LLMs",
                "multimodal RoPEs",
                "HoPE",
                "Hybrid of Position Embedding",
                "frequency allocation",
                "dynamic temporal scaling",
                "video benchmarks",
                "long video understanding",
                "retrieval tasks"
            ]
        },
        "publishedAt": "2025-05-26T14:37:40.000Z",
        "title": "HoPE: Hybrid of Position Embedding for Length Generalization in\n  Vision-Language Models",
        "summary": "Vision-Language Models (VLMs) have made significant progress in multimodal\ntasks. However, their performance often deteriorates in long-context scenarios,\nparticularly long videos. While Rotary Position Embedding (RoPE) has been\nwidely adopted for length generalization in Large Language Models (LLMs),\nextending vanilla RoPE to capture the intricate spatial-temporal dependencies\nin videos remains an unsolved challenge. Existing methods typically allocate\ndifferent frequencies within RoPE to encode 3D positional information. However,\nthese allocation strategies mainly rely on heuristics, lacking in-depth\ntheoretical analysis. In this paper, we first study how different allocation\nstrategies impact the long-context capabilities of VLMs. Our analysis reveals\nthat current multimodal RoPEs fail to reliably capture semantic similarities\nover extended contexts. To address this issue, we propose HoPE, a Hybrid of\nPosition Embedding designed to improve the long-context capabilities of VLMs.\nHoPE introduces a hybrid frequency allocation strategy for reliable semantic\nmodeling over arbitrarily long context, and a dynamic temporal scaling\nmechanism to facilitate robust learning and flexible inference across diverse\ncontext lengths. Extensive experiments across four video benchmarks on long\nvideo understanding and retrieval tasks demonstrate that HoPE consistently\noutperforms existing methods, confirming its effectiveness. Code is available\nat https://github.com/hrlics/HoPE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20444.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e86d6e7ef2c72cbcdb27b7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e86d6e7ef2c72cbcdb27b7/RLuJJ1xuO665LMWneVDZz.jpeg",
            "fullname": "Haoran Li",
            "name": "brian13",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23727",
            "authors": [
                {
                    "_id": "6839080fd73e6015a1acc507",
                    "name": "Song Wang",
                    "hidden": false
                },
                {
                    "_id": "6839080fd73e6015a1acc508",
                    "name": "Gongfan Fang",
                    "hidden": false
                },
                {
                    "_id": "6839080fd73e6015a1acc509",
                    "name": "Lingdong Kong",
                    "hidden": false
                },
                {
                    "_id": "6839080fd73e6015a1acc50a",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "6839080fd73e6015a1acc50b",
                    "name": "Jianyun Xu",
                    "hidden": false
                },
                {
                    "_id": "6839080fd73e6015a1acc50c",
                    "name": "Sheng Yang",
                    "hidden": false
                },
                {
                    "_id": "6839080fd73e6015a1acc50d",
                    "name": "Qiang Li",
                    "hidden": false
                },
                {
                    "_id": "6839080fd73e6015a1acc50e",
                    "name": "Jianke Zhu",
                    "hidden": false
                },
                {
                    "_id": "6839080fd73e6015a1acc50f",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:55:49.000Z",
            "submittedOnDailyAt": "2025-05-29T23:53:20.526Z",
            "title": "PixelThink: Towards Efficient Chain-of-Pixel Reasoning",
            "submittedOnDailyBy": {
                "_id": "66863d26e2b71e3d09189ae9",
                "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
                "isPro": false,
                "fullname": "Song Wang",
                "user": "songw-zju",
                "type": "user"
            },
            "summary": "Existing reasoning segmentation approaches typically fine-tune multimodal\nlarge language models (MLLMs) using image-text pairs and corresponding mask\nlabels. However, they exhibit limited generalization to out-of-distribution\nscenarios without an explicit reasoning process. Although recent efforts\nleverage reinforcement learning through group-relative policy optimization\n(GRPO) to enhance reasoning ability, they often suffer from overthinking -\nproducing uniformly verbose reasoning chains irrespective of task complexity.\nThis results in elevated computational costs and limited control over reasoning\nquality. To address this problem, we propose PixelThink, a simple yet effective\nscheme that integrates externally estimated task difficulty and internally\nmeasured model uncertainty to regulate reasoning generation within a\nreinforcement learning paradigm. The model learns to compress reasoning length\nin accordance with scene complexity and predictive confidence. To support\ncomprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark\nwith annotated reasoning references and difficulty scores, along with a suite\nof metrics designed to assess segmentation accuracy, reasoning quality, and\nefficiency jointly. Experimental results demonstrate that the proposed approach\nimproves both reasoning efficiency and overall segmentation performance. Our\nwork contributes novel perspectives towards efficient and interpretable\nmultimodal understanding. The code and model will be publicly available.",
            "upvotes": 1,
            "discussionId": "68390811d73e6015a1acc597",
            "projectPage": "https://pixelthink.github.io",
            "githubRepo": "https://github.com/songw-zju/PixelThink"
        },
        "publishedAt": "2025-05-29T13:55:49.000Z",
        "title": "PixelThink: Towards Efficient Chain-of-Pixel Reasoning",
        "summary": "Existing reasoning segmentation approaches typically fine-tune multimodal\nlarge language models (MLLMs) using image-text pairs and corresponding mask\nlabels. However, they exhibit limited generalization to out-of-distribution\nscenarios without an explicit reasoning process. Although recent efforts\nleverage reinforcement learning through group-relative policy optimization\n(GRPO) to enhance reasoning ability, they often suffer from overthinking -\nproducing uniformly verbose reasoning chains irrespective of task complexity.\nThis results in elevated computational costs and limited control over reasoning\nquality. To address this problem, we propose PixelThink, a simple yet effective\nscheme that integrates externally estimated task difficulty and internally\nmeasured model uncertainty to regulate reasoning generation within a\nreinforcement learning paradigm. The model learns to compress reasoning length\nin accordance with scene complexity and predictive confidence. To support\ncomprehensive evaluation, we introduce ReasonSeg-Diff, an extended benchmark\nwith annotated reasoning references and difficulty scores, along with a suite\nof metrics designed to assess segmentation accuracy, reasoning quality, and\nefficiency jointly. Experimental results demonstrate that the proposed approach\nimproves both reasoning efficiency and overall segmentation performance. Our\nwork contributes novel perspectives towards efficient and interpretable\nmultimodal understanding. The code and model will be publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23727.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66863d26e2b71e3d09189ae9",
            "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
            "fullname": "Song Wang",
            "name": "songw-zju",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22642",
            "authors": [
                {
                    "_id": "6838a05e80df874a4f2360d8",
                    "name": "Younggyo Seo",
                    "hidden": false
                },
                {
                    "_id": "6838a05e80df874a4f2360d9",
                    "name": "Carmelo Sferrazza",
                    "hidden": false
                },
                {
                    "_id": "6838a05e80df874a4f2360da",
                    "name": "Haoran Geng",
                    "hidden": false
                },
                {
                    "_id": "6838a05e80df874a4f2360db",
                    "name": "Michal Nauman",
                    "hidden": false
                },
                {
                    "_id": "6838a05e80df874a4f2360dc",
                    "name": "Zhao-Heng Yin",
                    "hidden": false
                },
                {
                    "_id": "6838a05e80df874a4f2360dd",
                    "name": "Pieter Abbeel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:55:26.000Z",
            "submittedOnDailyAt": "2025-05-29T16:29:47.737Z",
            "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid\n  Control",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": false,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has driven significant progress in robotics, but\nits complexity and long training times remain major bottlenecks. In this\nreport, we introduce FastTD3, a simple, fast, and capable RL algorithm that\nsignificantly speeds up training for humanoid robots in popular suites such as\nHumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably\nsimple: we train an off-policy TD3 agent with several modifications -- parallel\nsimulation, large-batch updates, a distributional critic, and carefully tuned\nhyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours\non a single A100 GPU, while remaining stable during training. We also provide a\nlightweight and easy-to-use implementation of FastTD3 to accelerate RL research\nin robotics.",
            "upvotes": 1,
            "discussionId": "6838a05f80df874a4f23610c",
            "ai_summary": "FastTD3, an enhanced RL algorithm with parallel simulation and distributional critic, significantly accelerates training for humanoid robots.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "FastTD3",
                "off-policy TD3",
                "parallel simulation",
                "large-batch updates",
                "distributional critic",
                "HumanoidBench",
                "IsaacLab",
                "MuJoCo Playground"
            ]
        },
        "publishedAt": "2025-05-28T13:55:26.000Z",
        "title": "FastTD3: Simple, Fast, and Capable Reinforcement Learning for Humanoid\n  Control",
        "summary": "Reinforcement learning (RL) has driven significant progress in robotics, but\nits complexity and long training times remain major bottlenecks. In this\nreport, we introduce FastTD3, a simple, fast, and capable RL algorithm that\nsignificantly speeds up training for humanoid robots in popular suites such as\nHumanoidBench, IsaacLab, and MuJoCo Playground. Our recipe is remarkably\nsimple: we train an off-policy TD3 agent with several modifications -- parallel\nsimulation, large-batch updates, a distributional critic, and carefully tuned\nhyperparameters. FastTD3 solves a range of HumanoidBench tasks in under 3 hours\non a single A100 GPU, while remaining stable during training. We also provide a\nlightweight and easy-to-use implementation of FastTD3 to accelerate RL research\nin robotics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22642.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6973
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22586",
            "authors": [
                {
                    "_id": "6838c514570877d07100f318",
                    "name": "Yoav Gur-Arieh",
                    "hidden": false
                },
                {
                    "_id": "6838c514570877d07100f319",
                    "name": "Clara Suslik",
                    "hidden": false
                },
                {
                    "_id": "6838c514570877d07100f31a",
                    "name": "Yihuai Hong",
                    "hidden": false
                },
                {
                    "_id": "6838c514570877d07100f31b",
                    "name": "Fazl Barez",
                    "hidden": false
                },
                {
                    "_id": "6838c514570877d07100f31c",
                    "name": "Mor Geva",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T16:58:23.000Z",
            "submittedOnDailyAt": "2025-05-29T19:06:19.966Z",
            "title": "Precise In-Parameter Concept Erasure in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "67609a46525a7cf186ca8ca4",
                "avatarUrl": "/avatars/f9027eca2181dee7dce899e7a590e803.svg",
                "isPro": false,
                "fullname": "Yoav Gur Arieh",
                "user": "yoavgurarieh",
                "type": "user"
            },
            "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.",
            "upvotes": 1,
            "discussionId": "6838c514570877d07100f35f",
            "githubRepo": "https://github.com/yoavgur/PISCES/",
            "ai_summary": "PISCES, a novel framework using feature-based in-parameter editing, effectively erases concepts from large language models with high specificity and robustness.",
            "ai_keywords": [
                "PISCES",
                "Precise In-parameter Suppression for Concept EraSure",
                "MLP vectors",
                "disentangler model",
                "automated interpretability techniques",
                "feature-based in-parameter editing"
            ]
        },
        "publishedAt": "2025-05-28T12:58:23.000Z",
        "title": "Precise In-Parameter Concept Erasure in Large Language Models",
        "summary": "Large language models (LLMs) often acquire knowledge during pretraining that\nis undesirable in downstream deployments, e.g., sensitive information or\ncopyrighted content. Existing approaches for removing such knowledge rely on\nfine-tuning, training low-rank adapters or fact-level editing, but these are\neither too coarse, too shallow, or ineffective. In this work, we propose PISCES\n(Precise In-parameter Suppression for Concept EraSure), a novel framework for\nprecisely erasing entire concepts from model parameters by directly editing\ndirections that encode them in parameter space. PISCES uses a disentangler\nmodel to decompose MLP vectors into interpretable features, identifies those\nassociated with a target concept using automated interpretability techniques,\nand removes them from model parameters. Experiments on Gemma 2 and Llama 3.1\nover various concepts show that PISCES achieves modest gains in efficacy over\nleading erasure methods, reducing accuracy on the target concept to as low as\n7.7%, while dramatically improving erasure specificity (by up to 31%) and\nrobustness (by up to 38%). Overall, these results demonstrate that\nfeature-based in-parameter editing enables a more precise and reliable approach\nfor removing conceptual knowledge in language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22586.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67609a46525a7cf186ca8ca4",
            "avatarUrl": "/avatars/f9027eca2181dee7dce899e7a590e803.svg",
            "fullname": "Yoav Gur Arieh",
            "name": "yoavgurarieh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18931",
            "authors": [
                {
                    "_id": "6838c9f0402ef780fdb23293",
                    "name": "Ryan Saklad",
                    "hidden": false
                },
                {
                    "_id": "6838c9f0402ef780fdb23294",
                    "name": "Aman Chadha",
                    "hidden": false
                },
                {
                    "_id": "6838c9f0402ef780fdb23295",
                    "name": "Oleg Pavlov",
                    "hidden": false
                },
                {
                    "_id": "6838c9f0402ef780fdb23296",
                    "name": "Raha Moraffah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T01:50:05.000Z",
            "submittedOnDailyAt": "2025-05-29T20:05:11.580Z",
            "title": "Can Large Language Models Infer Causal Relationships from Real-World\n  Text?",
            "submittedOnDailyBy": {
                "_id": "63a4754927f1f64ed7238dac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                "isPro": false,
                "fullname": "Aman Chadha",
                "user": "amanchadha",
                "type": "user"
            },
            "summary": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work primarily focuses on\nsynthetically generated texts which involve simple causal relationships\nexplicitly mentioned in the text. This fails to reflect the complexities of\nreal-world tasks. In this paper, we investigate whether LLMs are capable of\ninferring causal relationships from real-world texts. We develop a benchmark\ndrawn from real-world academic literature which includes diverse texts with\nrespect to length, complexity of relationships (different levels of\nexplicitness, number of events, and causal relationships), and domains and\nsub-domains. To the best of our knowledge, our benchmark is the first-ever\nreal-world dataset for this task. Our experiments on state-of-the-art LLMs\nevaluated on our proposed benchmark demonstrate significant challenges, with\nthe best-performing model achieving an average F1 score of only 0.477. Analysis\nreveals common pitfalls: difficulty with implicitly stated information, in\ndistinguishing relevant causal factors from surrounding contextual details, and\nwith connecting causally relevant information spread across lengthy textual\npassages. By systematically characterizing these deficiencies, our benchmark\noffers targeted insights for further research into advancing LLM causal\nreasoning.",
            "upvotes": 1,
            "discussionId": "6838c9f1402ef780fdb232d6",
            "ai_summary": "A benchmark for assessing LLMs' ability to infer causal relationships from real-world texts highlights significant challenges, revealing common pitfalls in handling implicit information and long-range connections.",
            "ai_keywords": [
                "large language models",
                "causal relationships",
                "real-world texts",
                "benchmark",
                "F1 score",
                "causal reasoning",
                "contextual details",
                "long-range connections"
            ]
        },
        "publishedAt": "2025-05-24T21:50:05.000Z",
        "title": "Can Large Language Models Infer Causal Relationships from Real-World\n  Text?",
        "summary": "Understanding and inferring causal relationships from texts is a core aspect\nof human cognition and is essential for advancing large language models (LLMs)\ntowards artificial general intelligence. Existing work primarily focuses on\nsynthetically generated texts which involve simple causal relationships\nexplicitly mentioned in the text. This fails to reflect the complexities of\nreal-world tasks. In this paper, we investigate whether LLMs are capable of\ninferring causal relationships from real-world texts. We develop a benchmark\ndrawn from real-world academic literature which includes diverse texts with\nrespect to length, complexity of relationships (different levels of\nexplicitness, number of events, and causal relationships), and domains and\nsub-domains. To the best of our knowledge, our benchmark is the first-ever\nreal-world dataset for this task. Our experiments on state-of-the-art LLMs\nevaluated on our proposed benchmark demonstrate significant challenges, with\nthe best-performing model achieving an average F1 score of only 0.477. Analysis\nreveals common pitfalls: difficulty with implicitly stated information, in\ndistinguishing relevant causal factors from surrounding contextual details, and\nwith connecting causally relevant information spread across lengthy textual\npassages. By systematically characterizing these deficiencies, our benchmark\noffers targeted insights for further research into advancing LLM causal\nreasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18931.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21862",
            "authors": [
                {
                    "_id": "68389ac5d011eaeee3d8e75a",
                    "user": {
                        "_id": "6701671ba48acf5f915ac060",
                        "avatarUrl": "/avatars/b830ab57457ed14780228a46d315f42f.svg",
                        "isPro": false,
                        "fullname": "Chenhui Zhao",
                        "user": "Zch0414",
                        "type": "user"
                    },
                    "name": "Chenhui Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:19:31.423Z",
                    "hidden": false
                },
                {
                    "_id": "68389ac5d011eaeee3d8e75b",
                    "name": "Yiwei Lyu",
                    "hidden": false
                },
                {
                    "_id": "68389ac5d011eaeee3d8e75c",
                    "name": "Asadur Chowdury",
                    "hidden": false
                },
                {
                    "_id": "68389ac5d011eaeee3d8e75d",
                    "name": "Edward Harake",
                    "hidden": false
                },
                {
                    "_id": "68389ac5d011eaeee3d8e75e",
                    "name": "Akhil Kondepudi",
                    "hidden": false
                },
                {
                    "_id": "68389ac5d011eaeee3d8e75f",
                    "name": "Akshay Rao",
                    "hidden": false
                },
                {
                    "_id": "68389ac5d011eaeee3d8e760",
                    "name": "Xinhai Hou",
                    "hidden": false
                },
                {
                    "_id": "68389ac5d011eaeee3d8e761",
                    "name": "Honglak Lee",
                    "hidden": false
                },
                {
                    "_id": "68389ac5d011eaeee3d8e762",
                    "name": "Todd Hollon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T01:16:34.000Z",
            "submittedOnDailyAt": "2025-05-29T16:06:37.919Z",
            "title": "Towards Scalable Language-Image Pre-training for 3D Medical Imaging",
            "submittedOnDailyBy": {
                "_id": "6701671ba48acf5f915ac060",
                "avatarUrl": "/avatars/b830ab57457ed14780228a46d315f42f.svg",
                "isPro": false,
                "fullname": "Chenhui Zhao",
                "user": "Zch0414",
                "type": "user"
            },
            "summary": "Language-image pre-training has demonstrated strong performance in 2D medical\nimaging, but its success in 3D modalities such as CT and MRI remains limited\ndue to the high computational demands of volumetric data, which pose a\nsignificant barrier to training on large-scale, uncurated clinical studies. In\nthis study, we introduce Hierarchical attention for Language-Image Pre-training\n(HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a\nlightweight hierarchical attention mechanism inspired by the natural hierarchy\nof radiology data: slice, scan, and study. This mechanism exhibits strong\ngeneralizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when\npre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables\ndirect training on uncurated datasets. Trained on 220K patients with 3.13\nmillion scans for brain MRI and 240K patients with 1.44 million scans for head\nCT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on\nthe proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and\n+6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These\nresults demonstrate that, with HLIP, directly pre-training on uncurated\nclinical datasets is a scalable and effective direction for language-image\npre-training in 3D medical imaging. The code is available at\nhttps://github.com/Zch0414/hlip",
            "upvotes": 0,
            "discussionId": "68389ac9d011eaeee3d8e860",
            "githubRepo": "https://github.com/Zch0414/hlip",
            "ai_summary": "Hierarchical attention mechanism for language-image pre-training in 3D medical imaging achieves state-of-the-art performance on uncurated clinical datasets.",
            "ai_keywords": [
                "Hierarchical attention",
                "language-image pre-training",
                "HLIP",
                "radiology data",
                "macro AUC",
                "balanced ACC",
                "Rad-ChestCT",
                "CT-RATE",
                "brain MRI",
                "head CT",
                "Pub-Brain-5",
                "RSNA",
                "CQ500"
            ]
        },
        "publishedAt": "2025-05-27T21:16:34.000Z",
        "title": "Towards Scalable Language-Image Pre-training for 3D Medical Imaging",
        "summary": "Language-image pre-training has demonstrated strong performance in 2D medical\nimaging, but its success in 3D modalities such as CT and MRI remains limited\ndue to the high computational demands of volumetric data, which pose a\nsignificant barrier to training on large-scale, uncurated clinical studies. In\nthis study, we introduce Hierarchical attention for Language-Image Pre-training\n(HLIP), a scalable pre-training framework for 3D medical imaging. HLIP adopts a\nlightweight hierarchical attention mechanism inspired by the natural hierarchy\nof radiology data: slice, scan, and study. This mechanism exhibits strong\ngeneralizability, e.g., +4.3% macro AUC on the Rad-ChestCT benchmark when\npre-trained on CT-RATE. Moreover, the computational efficiency of HLIP enables\ndirect training on uncurated datasets. Trained on 220K patients with 3.13\nmillion scans for brain MRI and 240K patients with 1.44 million scans for head\nCT, HLIP achieves state-of-the-art performance, e.g., +32.4% balanced ACC on\nthe proposed publicly available brain MRI benchmark Pub-Brain-5; +1.4% and\n+6.9% macro AUC on head CT benchmarks RSNA and CQ500, respectively. These\nresults demonstrate that, with HLIP, directly pre-training on uncurated\nclinical datasets is a scalable and effective direction for language-image\npre-training in 3D medical imaging. The code is available at\nhttps://github.com/Zch0414/hlip",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21862.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6701671ba48acf5f915ac060",
            "avatarUrl": "/avatars/b830ab57457ed14780228a46d315f42f.svg",
            "fullname": "Chenhui Zhao",
            "name": "Zch0414",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21649",
            "authors": [
                {
                    "_id": "68386daaa2d6f83cf1fb97a2",
                    "user": {
                        "_id": "64505b4a5fdcff143ad8593c",
                        "avatarUrl": "/avatars/1278a3f4c8c7be342bb6ab7ce4952ab7.svg",
                        "isPro": false,
                        "fullname": "Keanu Nichols",
                        "user": "kmn5409",
                        "type": "user"
                    },
                    "name": "Keanu Nichols",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T20:19:27.650Z",
                    "hidden": false
                },
                {
                    "_id": "68386daaa2d6f83cf1fb97a3",
                    "name": "Nazia Tasnim",
                    "hidden": false
                },
                {
                    "_id": "68386daaa2d6f83cf1fb97a4",
                    "name": "Yan Yuting",
                    "hidden": false
                },
                {
                    "_id": "68386daaa2d6f83cf1fb97a5",
                    "name": "Nicholas Ikechukwu",
                    "hidden": false
                },
                {
                    "_id": "68386daaa2d6f83cf1fb97a6",
                    "name": "Elva Zou",
                    "hidden": false
                },
                {
                    "_id": "68386daaa2d6f83cf1fb97a7",
                    "name": "Deepti Ghadiyaram",
                    "hidden": false
                },
                {
                    "_id": "68386daaa2d6f83cf1fb97a8",
                    "name": "Bryan Plummer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T18:22:44.000Z",
            "submittedOnDailyAt": "2025-05-29T19:30:42.472Z",
            "title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with\n  Fine-grained Multi-axis Perception Tasks",
            "submittedOnDailyBy": {
                "_id": "64505b4a5fdcff143ad8593c",
                "avatarUrl": "/avatars/1278a3f4c8c7be342bb6ab7ce4952ab7.svg",
                "isPro": false,
                "fullname": "Keanu Nichols",
                "user": "kmn5409",
                "type": "user"
            },
            "summary": "Object orientation understanding represents a fundamental challenge in visual\nperception critical for applications like robotic manipulation and augmented\nreality. Current vision-language benchmarks fail to isolate this capability,\noften conflating it with positional relationships and general scene\nunderstanding. We introduce DORI (Discriminative Orientation Reasoning\nIntelligence), a comprehensive benchmark establishing object orientation\nperception as a primary evaluation target. DORI assesses four dimensions of\norientation comprehension: frontal alignment, rotational transformations,\nrelative directional relationships, and canonical orientation understanding.\nThrough carefully curated tasks from 11 datasets spanning 67 object categories\nacross synthetic and real-world scenarios, DORI provides insights on how\nmulti-modal systems understand object orientations. Our evaluation of 15\nstate-of-the-art vision-language models reveals critical limitations: even the\nbest models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular\norientation judgments, with performance deteriorating for tasks requiring\nreference frame shifts or compound rotations. These findings demonstrate the\nneed for dedicated orientation representation mechanisms, as models show\nsystematic inability to perform precise angular estimations, track orientation\nchanges across viewpoints, and understand compound rotations - suggesting\nlimitations in their internal 3D spatial representations. As the first\ndiagnostic framework specifically designed for orientation awareness in\nmultimodal systems, DORI offers implications for improving robotic control, 3D\nscene reconstruction, and human-AI interaction in physical environments. DORI\ndata: https://huggingface.co/datasets/appledora/DORI-Benchmark",
            "upvotes": 0,
            "discussionId": "68386daca2d6f83cf1fb9891",
            "ai_summary": "DORI evaluates object orientation perception in vision-language models, revealing critical limitations in their ability to accurately estimate and track orientations, particularly with reference frame shifts and compound rotations.",
            "ai_keywords": [
                "discriminative orientation reasoning intelligence",
                "orientation comprehension",
                "frontal alignment",
                "rotational transformations",
                "relative directional relationships",
                "canonical orientation understanding",
                "vision-language models",
                "angular estimations",
                "3D spatial representations"
            ]
        },
        "publishedAt": "2025-05-27T14:22:44.000Z",
        "title": "Right Side Up? Disentangling Orientation Understanding in MLLMs with\n  Fine-grained Multi-axis Perception Tasks",
        "summary": "Object orientation understanding represents a fundamental challenge in visual\nperception critical for applications like robotic manipulation and augmented\nreality. Current vision-language benchmarks fail to isolate this capability,\noften conflating it with positional relationships and general scene\nunderstanding. We introduce DORI (Discriminative Orientation Reasoning\nIntelligence), a comprehensive benchmark establishing object orientation\nperception as a primary evaluation target. DORI assesses four dimensions of\norientation comprehension: frontal alignment, rotational transformations,\nrelative directional relationships, and canonical orientation understanding.\nThrough carefully curated tasks from 11 datasets spanning 67 object categories\nacross synthetic and real-world scenarios, DORI provides insights on how\nmulti-modal systems understand object orientations. Our evaluation of 15\nstate-of-the-art vision-language models reveals critical limitations: even the\nbest models achieve only 54.2% accuracy on coarse tasks and 33.0% on granular\norientation judgments, with performance deteriorating for tasks requiring\nreference frame shifts or compound rotations. These findings demonstrate the\nneed for dedicated orientation representation mechanisms, as models show\nsystematic inability to perform precise angular estimations, track orientation\nchanges across viewpoints, and understand compound rotations - suggesting\nlimitations in their internal 3D spatial representations. As the first\ndiagnostic framework specifically designed for orientation awareness in\nmultimodal systems, DORI offers implications for improving robotic control, 3D\nscene reconstruction, and human-AI interaction in physical environments. DORI\ndata: https://huggingface.co/datasets/appledora/DORI-Benchmark",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21649.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64505b4a5fdcff143ad8593c",
            "avatarUrl": "/avatars/1278a3f4c8c7be342bb6ab7ce4952ab7.svg",
            "fullname": "Keanu Nichols",
            "name": "kmn5409",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18149",
            "authors": [
                {
                    "_id": "6837e51d05c81fd7d7d1962e",
                    "user": {
                        "_id": "63ca499104c97982831127ec",
                        "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
                        "isPro": false,
                        "fullname": "Aradhye Agarwal",
                        "user": "aradhye",
                        "type": "user"
                    },
                    "name": "Aradhye Agarwal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:41:47.337Z",
                    "hidden": false
                },
                {
                    "_id": "6837e51d05c81fd7d7d1962f",
                    "name": "Ayan Sengupta",
                    "hidden": false
                },
                {
                    "_id": "6837e51d05c81fd7d7d19630",
                    "name": "Tanmoy Chakraborty",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T17:57:43.000Z",
            "submittedOnDailyAt": "2025-05-29T06:15:54.434Z",
            "title": "First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "63ca499104c97982831127ec",
                "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
                "isPro": false,
                "fullname": "Aradhye Agarwal",
                "user": "aradhye",
                "type": "user"
            },
            "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches n independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15%\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.",
            "upvotes": 0,
            "discussionId": "6837e51d05c81fd7d7d19659",
            "ai_summary": "First Finish Search improves accuracy in large language models by stopping inference at the first completed sample, significantly outperforming other decoding strategies in reasoning tasks.",
            "ai_keywords": [
                "Test-time scaling",
                "TTS",
                "dynamic allocation",
                "inference",
                "reasoning tasks",
                "First Finish Search",
                "FFS",
                "parallel decoding",
                "decoding strategies",
                "beam search",
                "majority voting",
                "budget forcing",
                "accuracy",
                "performance",
                "inference latency",
                "early stopping"
            ]
        },
        "publishedAt": "2025-05-23T13:57:43.000Z",
        "title": "First Finish Search: Efficient Test-Time Scaling in Large Language\n  Models",
        "summary": "Test-time scaling (TTS), which involves dynamic allocation of compute during\ninference, offers a promising way to improve reasoning in large language\nmodels. While existing TTS methods work well, they often rely on long decoding\npaths or require a large number of samples to be generated, increasing the\ntoken usage and inference latency. We observe the surprising fact that for\nreasoning tasks, shorter traces are much more likely to be correct than longer\nones. Motivated by this, we introduce First Finish Search (FFS), a\ntraining-free parallel decoding strategy that launches n independent samples\nand returns as soon as any one completes. We evaluate FFS alongside simple\ndecoding, beam search, majority voting, and budget forcing on four reasoning\nmodels (DeepSeek-R1, R1-Distill-Qwen-32B, QwQ-32B and Phi-4-Reasoning-Plus) and\nacross four datasets (AIME24, AIME25-I, AIME25-II and GPQA Diamond). With\nDeepSeek-R1, FFS achieves 82.23% accuracy on the AIME datasets, a 15%\nimprovement over DeepSeek-R1's standalone accuracy, nearly matching OpenAI's\no4-mini performance. Our theoretical analysis explains why stopping at the\nshortest trace is likely to yield a correct answer and identifies the\nconditions under which early stopping may be suboptimal. The elegance and\nsimplicity of FFS demonstrate that straightforward TTS strategies can perform\nremarkably well, revealing the untapped potential of simple approaches at\ninference time.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18149.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ca499104c97982831127ec",
            "avatarUrl": "/avatars/816a962c62c805adf7789fa8e398b01e.svg",
            "fullname": "Aradhye Agarwal",
            "name": "aradhye",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.12000",
            "authors": [
                {
                    "_id": "6838fb5e9deef11aa62251ee",
                    "name": "Tan-Hanh Pham",
                    "hidden": false
                },
                {
                    "_id": "6838fb5e9deef11aa62251ef",
                    "name": "Phu-Vinh Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6838fb5e9deef11aa62251f0",
                    "name": "Dang The Hung",
                    "hidden": false
                },
                {
                    "_id": "6838fb5e9deef11aa62251f1",
                    "name": "Bui Trong Duong",
                    "hidden": false
                },
                {
                    "_id": "6838fb5e9deef11aa62251f2",
                    "name": "Vu Nguyen Thanh",
                    "hidden": false
                },
                {
                    "_id": "6838fb5e9deef11aa62251f3",
                    "name": "Chris Ngo",
                    "hidden": false
                },
                {
                    "_id": "6838fb5e9deef11aa62251f4",
                    "name": "Tri Quang Truong",
                    "hidden": false
                },
                {
                    "_id": "6838fb5e9deef11aa62251f5",
                    "name": "Truong-Son Hy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-17T13:24:08.000Z",
            "submittedOnDailyAt": "2025-05-29T23:04:29.017Z",
            "title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ\n  Tests",
            "submittedOnDailyBy": {
                "_id": "6654fa747473f3c2572d2f8e",
                "avatarUrl": "/avatars/8f7c1ef9865f88d538b3f377e1eced0d.svg",
                "isPro": false,
                "fullname": "Tan-Hanh Pham",
                "user": "Hanhpt23",
                "type": "user"
            },
            "summary": "Although large Vision-Language Models (VLMs) have demonstrated remarkable\nperformance in a wide range of multimodal tasks, their true reasoning\ncapabilities on human IQ tests remain underexplored. To advance research on the\nfluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed\nto evaluate VLMs on standardized visual IQ tests. We focus on evaluating the\nreasoning capabilities of VLMs, which we argue are more important than the\naccuracy of the final prediction. **Our benchmark is visually centric,\nminimizing the dependence on unnecessary textual content**, thus encouraging\nmodels to derive answers primarily from image-based information rather than\nlearned textual knowledge. To this end, we manually collected and annotated 500\nvisual IQ questions to **prevent unintentional data leakage during training**.\nUnlike prior work that focuses primarily on the accuracy of the final answer,\nwe evaluate the reasoning ability of the models by assessing their explanations\nand the patterns used to solve each problem, along with the accuracy of the\nfinal prediction and human evaluation. Our experiments show that there are\nsubstantial performance disparities between tasks, with models such as\n`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest\naverage accuracies of 0.615, 0.578, and 0.548, respectively. However, all\nmodels struggle with 3D spatial and anagram reasoning tasks, highlighting\nsignificant limitations in current VLMs' general reasoning abilities. In terms\nof reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`\nachieved top averages of 0.696, 0.586, and 0.516, respectively. These results\nhighlight inconsistencies between the reasoning processes of the models and\ntheir final answers, emphasizing the importance of evaluating the accuracy of\nthe reasoning in addition to the final predictions.",
            "upvotes": 0,
            "discussionId": "6838fb5f9deef11aa6225257",
            "ai_summary": "IQBench evaluates the reasoning capabilities of Vision-Language Models on visual IQ tests, focusing on pattern recognition and explanation quality rather than final accuracy alone.",
            "ai_keywords": [
                "IQBench",
                "Vision-Language Models",
                "VLMs",
                "visual IQ tests",
                "reasoning capabilities",
                "pattern recognition",
                "explanation quality"
            ]
        },
        "publishedAt": "2025-05-17T09:24:08.000Z",
        "title": "IQBench: How \"Smart'' Are Vision-Language Models? A Study with Human IQ\n  Tests",
        "summary": "Although large Vision-Language Models (VLMs) have demonstrated remarkable\nperformance in a wide range of multimodal tasks, their true reasoning\ncapabilities on human IQ tests remain underexplored. To advance research on the\nfluid intelligence of VLMs, we introduce **IQBench**, a new benchmark designed\nto evaluate VLMs on standardized visual IQ tests. We focus on evaluating the\nreasoning capabilities of VLMs, which we argue are more important than the\naccuracy of the final prediction. **Our benchmark is visually centric,\nminimizing the dependence on unnecessary textual content**, thus encouraging\nmodels to derive answers primarily from image-based information rather than\nlearned textual knowledge. To this end, we manually collected and annotated 500\nvisual IQ questions to **prevent unintentional data leakage during training**.\nUnlike prior work that focuses primarily on the accuracy of the final answer,\nwe evaluate the reasoning ability of the models by assessing their explanations\nand the patterns used to solve each problem, along with the accuracy of the\nfinal prediction and human evaluation. Our experiments show that there are\nsubstantial performance disparities between tasks, with models such as\n`o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet` achieving the highest\naverage accuracies of 0.615, 0.578, and 0.548, respectively. However, all\nmodels struggle with 3D spatial and anagram reasoning tasks, highlighting\nsignificant limitations in current VLMs' general reasoning abilities. In terms\nof reasoning scores, `o4-mini`, `gemini-2.5-flash`, and `claude-3.7-sonnet`\nachieved top averages of 0.696, 0.586, and 0.516, respectively. These results\nhighlight inconsistencies between the reasoning processes of the models and\ntheir final answers, emphasizing the importance of evaluating the accuracy of\nthe reasoning in addition to the final predictions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.12000.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6654fa747473f3c2572d2f8e",
            "avatarUrl": "/avatars/8f7c1ef9865f88d538b3f377e1eced0d.svg",
            "fullname": "Tan-Hanh Pham",
            "name": "Hanhpt23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    }
]