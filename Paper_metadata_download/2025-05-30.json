[
    {
        "paper": {
            "id": "2505.23660",
            "authors": [
                {
                    "_id": "683906adf85de1fc563957d8",
                    "user": {
                        "_id": "648ac3d53470b17ccc90deaf",
                        "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
                        "isPro": false,
                        "fullname": "Ziteng Gao",
                        "user": "sebgao",
                        "type": "user"
                    },
                    "name": "Ziteng Gao",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-30T01:15:28.731Z",
                    "hidden": false
                },
                {
                    "_id": "683906adf85de1fc563957d9",
                    "user": {
                        "_id": "63a55320ce5763e06f78519c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671779060549-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Mike Shou",
                        "user": "mikeshou",
                        "type": "user"
                    },
                    "name": "Mike Zheng Shou",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-30T01:15:28.731Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:09:25.000Z",
            "submittedOnDailyAt": "2025-05-30T01:28:22.938Z",
            "title": "D-AR: Diffusion via Autoregressive Models",
            "submittedOnDailyBy": {
                "_id": "648ac3d53470b17ccc90deaf",
                "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
                "isPro": false,
                "fullname": "Ziteng Gao",
                "user": "sebgao",
                "type": "user"
            },
            "summary": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR",
            "upvotes": 20,
            "discussionId": "683906b0f85de1fc5639586b",
            "githubRepo": "https://github.com/showlab/D-AR",
            "ai_summary": "Diffusion via Autoregressive models (D-AR) recasts the image diffusion process as a standard autoregressive task, achieving high-quality image generation with consistent previews and layout control using a large language model backbone.",
            "ai_keywords": [
                "Diffusion via Autoregressive models",
                "autoregressive procedure",
                "next-token-prediction",
                "tokenizer",
                "discrete tokens",
                "diffusion denoising",
                "coarse-to-fine order",
                "autoregressive modeling",
                "autoregressive token generation",
                "FID",
                "Llama backbone"
            ]
        },
        "publishedAt": "2025-05-29T13:09:25.000Z",
        "title": "D-AR: Diffusion via Autoregressive Models",
        "summary": "This paper presents Diffusion via Autoregressive models (D-AR), a new\nparadigm recasting the image diffusion process as a vanilla autoregressive\nprocedure in the standard next-token-prediction fashion. We start by designing\nthe tokenizer that converts images into sequences of discrete tokens, where\ntokens in different positions can be decoded into different diffusion denoising\nsteps in the pixel space. Thanks to the diffusion properties, these tokens\nnaturally follow a coarse-to-fine order, which directly lends itself to\nautoregressive modeling. Therefore, we apply standard next-token prediction on\nthese tokens, without modifying any underlying designs (either causal masks or\ntraining/inference strategies), and such sequential autoregressive token\ngeneration directly mirrors the diffusion procedure in image space. That is,\nonce the autoregressive model generates an increment of tokens, we can directly\ndecode these tokens into the corresponding diffusion denoising step in the\nstreaming manner. Our pipeline naturally reveals several intriguing properties,\nfor example, it supports consistent previews when generating only a subset of\ntokens and enables zero-shot layout-controlled synthesis. On the standard\nImageNet benchmark, our method achieves 2.09 FID using a 775M Llama backbone\nwith 256 discrete tokens. We hope our work can inspire future research on\nunified autoregressive architectures of visual synthesis, especially with large\nlanguage models. Code and models will be available at\nhttps://github.com/showlab/D-AR",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23660.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648ac3d53470b17ccc90deaf",
            "avatarUrl": "/avatars/35b678baa7a91959f40f704c5de2a3e1.svg",
            "fullname": "Ziteng Gao",
            "name": "sebgao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23646",
            "authors": [
                {
                    "_id": "683946c845636acda08ed401",
                    "name": "Zijun Yao",
                    "hidden": false
                },
                {
                    "_id": "683946c845636acda08ed402",
                    "name": "Yantao Liu",
                    "hidden": false
                },
                {
                    "_id": "683946c845636acda08ed403",
                    "name": "Yanxu Chen",
                    "hidden": false
                },
                {
                    "_id": "683946c845636acda08ed404",
                    "name": "Jianhui Chen",
                    "hidden": false
                },
                {
                    "_id": "683946c845636acda08ed405",
                    "name": "Junfeng Fang",
                    "hidden": false
                },
                {
                    "_id": "683946c845636acda08ed406",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "683946c845636acda08ed407",
                    "name": "Juanzi Li",
                    "hidden": false
                },
                {
                    "_id": "683946c845636acda08ed408",
                    "name": "Tat-Seng Chua",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T16:53:41.000Z",
            "submittedOnDailyAt": "2025-05-30T04:31:18.157Z",
            "title": "Are Reasoning Models More Prone to Hallucination?",
            "submittedOnDailyBy": {
                "_id": "62e25e2247678ea5ce1b1786",
                "avatarUrl": "/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg",
                "isPro": false,
                "fullname": "Yantao",
                "user": "RicardoL1u",
                "type": "user"
            },
            "summary": "Recently evolved large reasoning models (LRMs) show powerful performance in\nsolving complex tasks with long chain-of-thought (CoT) reasoning capability. As\nthese LRMs are mostly developed by post-training on formal reasoning tasks,\nwhether they generalize the reasoning capability to help reduce hallucination\nin fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1\nreports increased performance on SimpleQA, a fact-seeking benchmark, while\nOpenAI-o3 observes even severer hallucination. This discrepancy naturally\nraises the following research question: Are reasoning models more prone to\nhallucination? This paper addresses the question from three perspectives. (1)\nWe first conduct a holistic evaluation for the hallucination in LRMs. Our\nanalysis reveals that LRMs undergo a full post-training pipeline with cold\nstart supervised fine-tuning (SFT) and verifiable reward RL generally alleviate\ntheir hallucination. In contrast, both distillation alone and RL training\nwithout cold start fine-tuning introduce more nuanced hallucinations. (2) To\nexplore why different post-training pipelines alters the impact on\nhallucination in LRMs, we conduct behavior analysis. We characterize two\ncritical cognitive behaviors that directly affect the factuality of a LRM: Flaw\nRepetition, where the surface-level reasoning attempts repeatedly follow the\nsame underlying flawed logic, and Think-Answer Mismatch, where the final answer\nfails to faithfully match the previous CoT process. (3) Further, we investigate\nthe mechanism behind the hallucination of LRMs from the perspective of model\nuncertainty. We find that increased hallucination of LRMs is usually associated\nwith the misalignment between model uncertainty and factual accuracy. Our work\nprovides an initial understanding of the hallucination in LRMs.",
            "upvotes": 19,
            "discussionId": "683946c845636acda08ed42a",
            "ai_summary": "Large reasoning models exhibit varying susceptibility to hallucination depending on post-training pipelines, revealing critical cognitive behaviors and uncertainty misalignment as contributing factors.",
            "ai_keywords": [
                "large reasoning models",
                "chain-of-thought",
                "post-training",
                "formal reasoning tasks",
                "fact-seeking tasks",
                "DeepSeek-R1",
                "OpenAI-o3",
                "hallucination",
                "cold start supervised fine-tuning",
                "verifiable reward RL",
                "distillation",
                "Flaw Repetition",
                "Think-Answer Mismatch",
                "model uncertainty",
                "factual accuracy"
            ]
        },
        "publishedAt": "2025-05-29T12:53:41.000Z",
        "title": "Are Reasoning Models More Prone to Hallucination?",
        "summary": "Recently evolved large reasoning models (LRMs) show powerful performance in\nsolving complex tasks with long chain-of-thought (CoT) reasoning capability. As\nthese LRMs are mostly developed by post-training on formal reasoning tasks,\nwhether they generalize the reasoning capability to help reduce hallucination\nin fact-seeking tasks remains unclear and debated. For instance, DeepSeek-R1\nreports increased performance on SimpleQA, a fact-seeking benchmark, while\nOpenAI-o3 observes even severer hallucination. This discrepancy naturally\nraises the following research question: Are reasoning models more prone to\nhallucination? This paper addresses the question from three perspectives. (1)\nWe first conduct a holistic evaluation for the hallucination in LRMs. Our\nanalysis reveals that LRMs undergo a full post-training pipeline with cold\nstart supervised fine-tuning (SFT) and verifiable reward RL generally alleviate\ntheir hallucination. In contrast, both distillation alone and RL training\nwithout cold start fine-tuning introduce more nuanced hallucinations. (2) To\nexplore why different post-training pipelines alters the impact on\nhallucination in LRMs, we conduct behavior analysis. We characterize two\ncritical cognitive behaviors that directly affect the factuality of a LRM: Flaw\nRepetition, where the surface-level reasoning attempts repeatedly follow the\nsame underlying flawed logic, and Think-Answer Mismatch, where the final answer\nfails to faithfully match the previous CoT process. (3) Further, we investigate\nthe mechanism behind the hallucination of LRMs from the perspective of model\nuncertainty. We find that increased hallucination of LRMs is usually associated\nwith the misalignment between model uncertainty and factual accuracy. Our work\nprovides an initial understanding of the hallucination in LRMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23646.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e25e2247678ea5ce1b1786",
            "avatarUrl": "/avatars/1bb32e7597a9b1c89c434cbf550b5382.svg",
            "fullname": "Yantao",
            "name": "RicardoL1u",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23380",
            "authors": [
                {
                    "_id": "683906f81bf7a7a94309c5a5",
                    "name": "Weijia Mao",
                    "hidden": false
                },
                {
                    "_id": "683906f81bf7a7a94309c5a6",
                    "name": "Zhenheng Yang",
                    "hidden": false
                },
                {
                    "_id": "683906f81bf7a7a94309c5a7",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T12:00:15.000Z",
            "submittedOnDailyAt": "2025-05-30T02:41:07.217Z",
            "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "63f320ee0be81bdc5d8ecb88",
                "avatarUrl": "/avatars/9d08cff6ed23a51887c869947bc03228.svg",
                "isPro": false,
                "fullname": "Mao Weijia",
                "user": "benzweijia",
                "type": "user"
            },
            "summary": "Unified multimodal large language models such as Show-o and Janus have\nachieved strong performance across both generation and understanding tasks.\nHowever, these models typically rely on large-scale datasets and require\nsubstantial computation during the pretraining stage. In addition, several\npost-training methods have been proposed, but they often depend on external\ndata or are limited to task-specific customization. In this work, we introduce\nUniRL, a self-improving post-training approach. Our approach enables the model\nto generate images from prompts and use them as training data in each\niteration, without relying on any external image data. Moreover, it enables the\ntwo tasks to enhance each other: the generated images are used for\nunderstanding, and the understanding results are used to supervise generation.\nWe explore supervised fine-tuning (SFT) and Group Relative Policy Optimization\n(GRPO) to optimize the models. UniRL offers three key advantages: (1) it\nrequires no external image data, as all training samples are generated by the\nmodel itself during training; (2) it not only improves individual task\nperformance, but also reduces the imbalance between generation and\nunderstanding; and (3) it requires only several additional training steps\nduring the post-training stage. We evaluate UniRL on top of Show-o and Janus,\nachieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and\nmodels will be released in https://github.com/showlab/UniRL.",
            "upvotes": 19,
            "discussionId": "683906f91bf7a7a94309c5dd",
            "ai_summary": "UniRL is a self-improving post-training method for unified multimodal large language models that uses generated images as training data, enhancing both generation and understanding tasks without external data.",
            "ai_keywords": [
                "Unified multimodal large language models",
                "Show-o",
                "Janus",
                "self-improving post-training",
                "prompts",
                "training data",
                "supervised fine-tuning",
                "Group Relative Policy Optimization",
                "GenEval score"
            ]
        },
        "publishedAt": "2025-05-29T08:00:15.000Z",
        "title": "UniRL: Self-Improving Unified Multimodal Models via Supervised and\n  Reinforcement Learning",
        "summary": "Unified multimodal large language models such as Show-o and Janus have\nachieved strong performance across both generation and understanding tasks.\nHowever, these models typically rely on large-scale datasets and require\nsubstantial computation during the pretraining stage. In addition, several\npost-training methods have been proposed, but they often depend on external\ndata or are limited to task-specific customization. In this work, we introduce\nUniRL, a self-improving post-training approach. Our approach enables the model\nto generate images from prompts and use them as training data in each\niteration, without relying on any external image data. Moreover, it enables the\ntwo tasks to enhance each other: the generated images are used for\nunderstanding, and the understanding results are used to supervise generation.\nWe explore supervised fine-tuning (SFT) and Group Relative Policy Optimization\n(GRPO) to optimize the models. UniRL offers three key advantages: (1) it\nrequires no external image data, as all training samples are generated by the\nmodel itself during training; (2) it not only improves individual task\nperformance, but also reduces the imbalance between generation and\nunderstanding; and (3) it requires only several additional training steps\nduring the post-training stage. We evaluate UniRL on top of Show-o and Janus,\nachieving a GenEval score of 0.77 for Show-o and 0.65 for Janus. Code and\nmodels will be released in https://github.com/showlab/UniRL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23380.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f320ee0be81bdc5d8ecb88",
            "avatarUrl": "/avatars/9d08cff6ed23a51887c869947bc03228.svg",
            "fullname": "Mao Weijia",
            "name": "benzweijia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22914",
            "authors": [
                {
                    "_id": "6839533ae7922379b361b3cb",
                    "name": "Maksim Kolodiazhnyi",
                    "hidden": false
                },
                {
                    "_id": "6839533ae7922379b361b3cc",
                    "name": "Denis Tarasov",
                    "hidden": false
                },
                {
                    "_id": "6839533ae7922379b361b3cd",
                    "user": {
                        "_id": "67d5a331eab66ce9cb01bae4",
                        "avatarUrl": "/avatars/3ed437c874889e8a6db66c3ef88a60c0.svg",
                        "isPro": false,
                        "fullname": "DMITRII ZHEMCHUZHNIKOV",
                        "user": "zhemchuzhnikov",
                        "type": "user"
                    },
                    "name": "Dmitrii Zhemchuzhnikov",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-30T06:42:03.218Z",
                    "hidden": false
                },
                {
                    "_id": "6839533ae7922379b361b3ce",
                    "name": "Alexander Nikulin",
                    "hidden": false
                },
                {
                    "_id": "6839533ae7922379b361b3cf",
                    "name": "Ilya Zisman",
                    "hidden": false
                },
                {
                    "_id": "6839533ae7922379b361b3d0",
                    "name": "Anna Vorontsova",
                    "hidden": false
                },
                {
                    "_id": "6839533ae7922379b361b3d1",
                    "name": "Anton Konushin",
                    "hidden": false
                },
                {
                    "_id": "6839533ae7922379b361b3d2",
                    "name": "Vladislav Kurenkov",
                    "hidden": false
                },
                {
                    "_id": "6839533ae7922379b361b3d3",
                    "name": "Danila Rukhovich",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T22:32:31.000Z",
            "submittedOnDailyAt": "2025-05-30T05:25:03.367Z",
            "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "665b10fb270e47e678f2ddf1",
                "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
                "isPro": false,
                "fullname": "max",
                "user": "maksimko123",
                "type": "user"
            },
            "summary": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.",
            "upvotes": 19,
            "discussionId": "6839533be7922379b361b418",
            "ai_summary": "A multi-modal CAD reconstruction model leveraging vision-language models and reinforcement learning achieves state-of-the-art performance across various datasets, including real-world ones.",
            "ai_keywords": [
                "vision-language models",
                "multi-modal",
                "large language model",
                "supervised fine-tuning",
                "reinforcement learning",
                "Group Relative Preference Optimization",
                "DeepCAD benchmark"
            ]
        },
        "publishedAt": "2025-05-28T18:32:31.000Z",
        "title": "cadrille: Multi-modal CAD Reconstruction with Online Reinforcement\n  Learning",
        "summary": "Computer-Aided Design (CAD) plays a central role in engineering and\nmanufacturing, making it possible to create precise and editable 3D models.\nUsing a variety of sensor or user-provided data as inputs for CAD\nreconstruction can democratize access to design applications. However, existing\nmethods typically focus on a single input modality, such as point clouds,\nimages, or text, which limits their generalizability and robustness. Leveraging\nrecent advances in vision-language models (VLM), we propose a multi-modal CAD\nreconstruction model that simultaneously processes all three input modalities.\nInspired by large language model (LLM) training paradigms, we adopt a two-stage\npipeline: supervised fine-tuning (SFT) on large-scale procedurally generated\ndata, followed by reinforcement learning (RL) fine-tuning using online\nfeedback, obtained programatically. Furthermore, we are the first to explore RL\nfine-tuning of LLMs for CAD tasks demonstrating that online RL algorithms such\nas Group Relative Preference Optimization (GRPO) outperform offline\nalternatives. In the DeepCAD benchmark, our SFT model outperforms existing\nsingle-modal approaches in all three input modalities simultaneously. More\nimportantly, after RL fine-tuning, cadrille sets new state-of-the-art on three\nchallenging datasets, including a real-world one.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22914.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "665b10fb270e47e678f2ddf1",
            "avatarUrl": "/avatars/1bc7a9211acf767f7bfca998c24315a0.svg",
            "fullname": "max",
            "name": "maksimko123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22618",
            "authors": [
                {
                    "_id": "683931e1b6280677f75edf09",
                    "name": "Chengyue Wu",
                    "hidden": false
                },
                {
                    "_id": "683931e1b6280677f75edf0a",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "683931e1b6280677f75edf0b",
                    "user": {
                        "_id": "64cf5e81a2e7f9ff61eb3a0c",
                        "avatarUrl": "/avatars/cbaa11daec9d4113bf7de93fe9b9ee86.svg",
                        "isPro": false,
                        "fullname": "scxue",
                        "user": "Cauthyyy",
                        "type": "user"
                    },
                    "name": "Shuchen Xue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:19.453Z",
                    "hidden": false
                },
                {
                    "_id": "683931e1b6280677f75edf0c",
                    "user": {
                        "_id": "650dac79b959b0e1d41d7378",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dac79b959b0e1d41d7378/mzbN0MFk3k8b94FQ40I7L.jpeg",
                        "isPro": false,
                        "fullname": "Zhijian Liu",
                        "user": "zhijianliu",
                        "type": "user"
                    },
                    "name": "Zhijian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:22.124Z",
                    "hidden": false
                },
                {
                    "_id": "683931e1b6280677f75edf0d",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "683931e1b6280677f75edf0e",
                    "name": "Ligeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "683931e1b6280677f75edf0f",
                    "name": "Ping Luo",
                    "hidden": false
                },
                {
                    "_id": "683931e1b6280677f75edf10",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "683931e1b6280677f75edf11",
                    "name": "Enze Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T17:39:15.000Z",
            "submittedOnDailyAt": "2025-05-30T02:50:04.196Z",
            "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
            "submittedOnDailyBy": {
                "_id": "633bd54b00732349209a18fe",
                "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
                "isPro": false,
                "fullname": "Shizhe Diao",
                "user": "shizhediao",
                "type": "user"
            },
            "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to 27.6times\nthroughput improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
            "upvotes": 18,
            "discussionId": "683931e2b6280677f75edf32",
            "ai_summary": "A novel block-wise approximate KV Cache and confidence-aware parallel decoding strategy improve the inference speed of diffusion-based large language models without significant quality loss.",
            "ai_keywords": [
                "diffusion-based large language models (Diffusion LLMs)",
                "non-autoregressive text generation",
                "parallel decoding",
                "Key-Value (KV) Cache",
                "bidirectional diffusion models",
                "token dependencies",
                "confidence-aware parallel decoding",
                "LLaDA",
                "Dream models",
                "LLM benchmarks"
            ]
        },
        "publishedAt": "2025-05-28T13:39:15.000Z",
        "title": "Fast-dLLM: Training-free Acceleration of Diffusion LLM by Enabling KV\n  Cache and Parallel Decoding",
        "summary": "Diffusion-based large language models (Diffusion LLMs) have shown promise for\nnon-autoregressive text generation with parallel decoding capabilities.\nHowever, the practical inference speed of open-sourced Diffusion LLMs often\nlags behind autoregressive models due to the lack of Key-Value (KV) Cache and\nquality degradation when decoding multiple tokens simultaneously. To bridge\nthis gap, we introduce a novel block-wise approximate KV Cache mechanism\ntailored for bidirectional diffusion models, enabling cache reuse with\nnegligible performance drop. Additionally, we identify the root cause of\ngeneration quality degradation in parallel decoding as the disruption of token\ndependencies under the conditional independence assumption. To address this, we\npropose a confidence-aware parallel decoding strategy that selectively decodes\ntokens exceeding a confidence threshold, mitigating dependency violations and\nmaintaining generation quality. Experimental results on LLaDA and Dream models\nacross multiple LLM benchmarks demonstrate up to 27.6times\nthroughput improvement with minimal accuracy loss, closing the performance gap\nwith autoregressive models and paving the way for practical deployment of\nDiffusion LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22618.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633bd54b00732349209a18fe",
            "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
            "fullname": "Shizhe Diao",
            "name": "shizhediao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23716",
            "authors": [
                {
                    "_id": "6839217e95fedc63bb4ae475",
                    "name": "Lihan Jiang",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae476",
                    "user": {
                        "_id": "65de9c6cf68c3d3bac330509",
                        "avatarUrl": "/avatars/150858545ea1a9e7c96d6f227093ac54.svg",
                        "isPro": false,
                        "fullname": "Yucheng Mao",
                        "user": "matthewmao",
                        "type": "user"
                    },
                    "name": "Yucheng Mao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:34.206Z",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae477",
                    "name": "Linning Xu",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae478",
                    "name": "Tao Lu",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae479",
                    "name": "Kerui Ren",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae47a",
                    "name": "Yichen Jin",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae47b",
                    "name": "Xudong Xu",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae47c",
                    "name": "Mulin Yu",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae47d",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae47e",
                    "name": "Feng Zhao",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae47f",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "6839217e95fedc63bb4ae480",
                    "name": "Bo Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:49:56.000Z",
            "submittedOnDailyAt": "2025-05-30T01:42:17.635Z",
            "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
            "submittedOnDailyBy": {
                "_id": "64a4ce8118f4e2529546daef",
                "avatarUrl": "/avatars/6d88aa68eccfa07d2009df405f957fd7.svg",
                "isPro": false,
                "fullname": "Jiang Lihan",
                "user": "lhjiang",
                "type": "user"
            },
            "summary": "We introduce AnySplat, a feed forward network for novel view synthesis from\nuncalibrated image collections. In contrast to traditional neural rendering\npipelines that demand known camera poses and per scene optimization, or recent\nfeed forward methods that buckle under the computational weight of dense views,\nour model predicts everything in one shot. A single forward pass yields a set\nof 3D Gaussian primitives encoding both scene geometry and appearance, and the\ncorresponding camera intrinsics and extrinsics for each input image. This\nunified design scales effortlessly to casually captured, multi view datasets\nwithout any pose annotations. In extensive zero shot evaluations, AnySplat\nmatches the quality of pose aware baselines in both sparse and dense view\nscenarios while surpassing existing pose free approaches. Moreover, it greatly\nreduce rendering latency compared to optimization based neural fields, bringing\nreal time novel view synthesis within reach for unconstrained capture\nsettings.Project page: https://city-super.github.io/anysplat/",
            "upvotes": 17,
            "discussionId": "6839217f95fedc63bb4ae4d0",
            "projectPage": "https://city-super.github.io/anysplat/",
            "ai_summary": "AnySplat is a feed forward network that performs novel view synthesis without camera poses, using 3D Gaussian primitives and unified design for efficiency and quality across sparse and dense datasets.",
            "ai_keywords": [
                "feed forward network",
                "novel view synthesis",
                "uncalibrated image collections",
                "3D Gaussian primitives",
                "camera intrinsics",
                "camera extrinsics",
                "neural rendering pipelines",
                "per scene optimization",
                "zero shot evaluations",
                "pose aware baselines",
                "pose free approaches",
                "rendering latency",
                "optimization based neural fields"
            ]
        },
        "publishedAt": "2025-05-29T13:49:56.000Z",
        "title": "AnySplat: Feed-forward 3D Gaussian Splatting from Unconstrained Views",
        "summary": "We introduce AnySplat, a feed forward network for novel view synthesis from\nuncalibrated image collections. In contrast to traditional neural rendering\npipelines that demand known camera poses and per scene optimization, or recent\nfeed forward methods that buckle under the computational weight of dense views,\nour model predicts everything in one shot. A single forward pass yields a set\nof 3D Gaussian primitives encoding both scene geometry and appearance, and the\ncorresponding camera intrinsics and extrinsics for each input image. This\nunified design scales effortlessly to casually captured, multi view datasets\nwithout any pose annotations. In extensive zero shot evaluations, AnySplat\nmatches the quality of pose aware baselines in both sparse and dense view\nscenarios while surpassing existing pose free approaches. Moreover, it greatly\nreduce rendering latency compared to optimization based neural fields, bringing\nreal time novel view synthesis within reach for unconstrained capture\nsettings.Project page: https://city-super.github.io/anysplat/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23716.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a4ce8118f4e2529546daef",
            "avatarUrl": "/avatars/6d88aa68eccfa07d2009df405f957fd7.svg",
            "fullname": "Jiang Lihan",
            "name": "lhjiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20088",
            "authors": [
                {
                    "_id": "683947458987f50a5ec45a01",
                    "name": "Nitay Calderon",
                    "hidden": false
                },
                {
                    "_id": "683947458987f50a5ec45a02",
                    "name": "Liat Ein-Dor",
                    "hidden": false
                },
                {
                    "_id": "683947458987f50a5ec45a03",
                    "name": "Roi Reichart",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/C8RHn7j61J4qQlZvXd52v.png"
            ],
            "publishedAt": "2025-05-26T15:01:56.000Z",
            "submittedOnDailyAt": "2025-05-30T04:25:05.621Z",
            "title": "Multi-Domain Explainability of Preferences",
            "submittedOnDailyBy": {
                "_id": "62d6a0c18faee0ac953c51fa",
                "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
                "isPro": false,
                "fullname": "Nitay Calderon",
                "user": "nitay",
                "type": "user"
            },
            "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs.",
            "upvotes": 17,
            "discussionId": "683947488987f50a5ec45a95",
            "ai_summary": "A new automated method using concept-based vectors and a Hierarchical Multi-Domain Regression model improves preference explanations and predictions for large language models.",
            "ai_keywords": [
                "LLM-as-a-Judge",
                "reward models",
                "large language models",
                "concept-based explanations",
                "domain-general effects",
                "domain-specific effects",
                "Hierarchical Multi-Domain Regression model",
                "preference prediction",
                "explainability"
            ]
        },
        "publishedAt": "2025-05-26T11:01:56.000Z",
        "title": "Multi-Domain Explainability of Preferences",
        "summary": "Preference mechanisms, such as human preference, LLM-as-a-Judge (LaaJ), and\nreward models, are central to aligning and evaluating large language models\n(LLMs). Yet, the underlying concepts that drive these preferences remain poorly\nunderstood. In this work, we propose a fully automated method for generating\nlocal and global concept-based explanations of preferences across multiple\ndomains. Our method utilizes an LLM to identify concepts that distinguish\nbetween chosen and rejected responses, and to represent them with concept-based\nvectors. To model the relationships between concepts and preferences, we\npropose a white-box Hierarchical Multi-Domain Regression model that captures\nboth domain-general and domain-specific effects. To evaluate our method, we\ncurate a dataset spanning eight challenging and diverse domains and explain\ntwelve mechanisms. Our method achieves strong preference prediction\nperformance, outperforming baselines while also being explainable.\nAdditionally, we assess explanations in two application-driven settings. First,\nguiding LLM outputs with concepts from LaaJ explanations yields responses that\nthose judges consistently prefer. Second, prompting LaaJs with concepts\nexplaining humans improves their preference predictions. Together, our work\nestablishes a new paradigm for explainability in the era of LLMs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62d6a0c18faee0ac953c51fa/C8RHn7j61J4qQlZvXd52v.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20088.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d6a0c18faee0ac953c51fa",
            "avatarUrl": "/avatars/ca818cebdb089a8d853c5bc4d5e0987b.svg",
            "fullname": "Nitay Calderon",
            "name": "nitay",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22255",
            "authors": [
                {
                    "_id": "68385ed47d00cf0a04845b23",
                    "name": "Vadim Kurochkin",
                    "hidden": false
                },
                {
                    "_id": "68385ed47d00cf0a04845b24",
                    "name": "Yaroslav Aksenov",
                    "hidden": false
                },
                {
                    "_id": "68385ed47d00cf0a04845b25",
                    "name": "Daniil Laptev",
                    "hidden": false
                },
                {
                    "_id": "68385ed47d00cf0a04845b26",
                    "name": "Daniil Gavrilov",
                    "hidden": false
                },
                {
                    "_id": "68385ed47d00cf0a04845b27",
                    "name": "Nikita Balagansky",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T11:41:11.000Z",
            "submittedOnDailyAt": "2025-05-30T08:53:19.780Z",
            "title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation",
            "submittedOnDailyBy": {
                "_id": "634c5f8cfb80cc6bcaf42c03",
                "avatarUrl": "/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg",
                "isPro": false,
                "fullname": "Daniil Laptev",
                "user": "dlaptev",
                "type": "user"
            },
            "summary": "Sparse Autoencoders (SAEs) have demonstrated significant promise in\ninterpreting the hidden states of language models by decomposing them into\ninterpretable latent directions. However, training SAEs at scale remains\nchallenging, especially when large dictionary sizes are used. While decoders\ncan leverage sparse-aware kernels for efficiency, encoders still require\ncomputationally intensive linear operations with large output dimensions. To\naddress this, we propose KronSAE, a novel architecture that factorizes the\nlatent representation via Kronecker product decomposition, drastically reducing\nmemory and computational overhead. Furthermore, we introduce mAND, a\ndifferentiable activation function approximating the binary AND operation,\nwhich improves interpretability and performance in our factorized framework.",
            "upvotes": 16,
            "discussionId": "68385ed57d00cf0a04845b60",
            "ai_summary": "KronSAE, a novel architecture using Kronecker product decomposition, enhances efficiency in training Sparse Autoencoders, while mAND, a differentiable binary AND function, improves interpretability and performance.",
            "ai_keywords": [
                "Sparse Autoencoders",
                "Kronecker product decomposition",
                "latent representation",
                "mAND",
                "differentiable activation function"
            ]
        },
        "publishedAt": "2025-05-28T07:41:11.000Z",
        "title": "Train Sparse Autoencoders Efficiently by Utilizing Features Correlation",
        "summary": "Sparse Autoencoders (SAEs) have demonstrated significant promise in\ninterpreting the hidden states of language models by decomposing them into\ninterpretable latent directions. However, training SAEs at scale remains\nchallenging, especially when large dictionary sizes are used. While decoders\ncan leverage sparse-aware kernels for efficiency, encoders still require\ncomputationally intensive linear operations with large output dimensions. To\naddress this, we propose KronSAE, a novel architecture that factorizes the\nlatent representation via Kronecker product decomposition, drastically reducing\nmemory and computational overhead. Furthermore, we introduce mAND, a\ndifferentiable activation function approximating the binary AND operation,\nwhich improves interpretability and performance in our factorized framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22255.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "634c5f8cfb80cc6bcaf42c03",
            "avatarUrl": "/avatars/1f37db0e70cbaf9707f4c8cbcee37ca0.svg",
            "fullname": "Daniil Laptev",
            "name": "dlaptev",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22759",
            "authors": [
                {
                    "_id": "6839680e3e5dd928f0576da0",
                    "name": "Sara Papi",
                    "hidden": false
                },
                {
                    "_id": "6839680e3e5dd928f0576da1",
                    "name": "Marco Gaido",
                    "hidden": false
                },
                {
                    "_id": "6839680e3e5dd928f0576da2",
                    "name": "Luisa Bentivogli",
                    "hidden": false
                },
                {
                    "_id": "6839680e3e5dd928f0576da3",
                    "name": "Alessio Brutti",
                    "hidden": false
                },
                {
                    "_id": "6839680e3e5dd928f0576da4",
                    "name": "Mauro Cettolo",
                    "hidden": false
                },
                {
                    "_id": "6839680e3e5dd928f0576da5",
                    "name": "Roberto Gretter",
                    "hidden": false
                },
                {
                    "_id": "6839680e3e5dd928f0576da6",
                    "name": "Marco Matassoni",
                    "hidden": false
                },
                {
                    "_id": "6839680e3e5dd928f0576da7",
                    "name": "Mohamed Nabih",
                    "hidden": false
                },
                {
                    "_id": "6839680e3e5dd928f0576da8",
                    "name": "Matteo Negri",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66309b3833ccd9e68c5d5171/Kzu5rlcLEhejnsYq_yWHV.png"
            ],
            "publishedAt": "2025-05-28T18:19:34.000Z",
            "submittedOnDailyAt": "2025-05-30T08:43:05.929Z",
            "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for\n  English and Italian",
            "submittedOnDailyBy": {
                "_id": "66309b3833ccd9e68c5d5171",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                "isPro": false,
                "fullname": "Sara Papi",
                "user": "spapi",
                "type": "user"
            },
            "summary": "The development of speech foundation models (SFMs) like Whisper and\nSeamlessM4T has significantly advanced the field of speech processing. However,\ntheir closed nature--with inaccessible training data and code--poses major\nreproducibility and fair evaluation challenges. While other domains have made\nsubstantial progress toward open science by developing fully transparent models\ntrained on open-source (OS) code and data, similar efforts in speech remain\nlimited. To fill this gap, we introduce FAMA, the first family of open science\nSFMs for English and Italian, trained on 150k+ hours of OS speech data.\nMoreover, we present a new dataset containing 16k hours of cleaned and\npseudo-labeled speech for both languages. Results show that FAMA achieves\ncompetitive performance compared to existing SFMs while being up to 8 times\nfaster. All artifacts, including code, datasets, and models, are released under\nOS-compliant licenses, promoting openness in speech technology research.",
            "upvotes": 15,
            "discussionId": "6839680f3e5dd928f0576dd3",
            "ai_summary": "FAMA, an open science family of speech foundation models, provides transparency and competitive performance by leveraging open-source training data and code.",
            "ai_keywords": [
                "speech foundation models",
                "Whisper",
                "SeamlessM4T",
                "open science",
                "open-source code",
                "open-source data",
                "FAMA",
                "pseudo-labeled speech"
            ]
        },
        "publishedAt": "2025-05-28T14:19:34.000Z",
        "title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for\n  English and Italian",
        "summary": "The development of speech foundation models (SFMs) like Whisper and\nSeamlessM4T has significantly advanced the field of speech processing. However,\ntheir closed nature--with inaccessible training data and code--poses major\nreproducibility and fair evaluation challenges. While other domains have made\nsubstantial progress toward open science by developing fully transparent models\ntrained on open-source (OS) code and data, similar efforts in speech remain\nlimited. To fill this gap, we introduce FAMA, the first family of open science\nSFMs for English and Italian, trained on 150k+ hours of OS speech data.\nMoreover, we present a new dataset containing 16k hours of cleaned and\npseudo-labeled speech for both languages. Results show that FAMA achieves\ncompetitive performance compared to existing SFMs while being up to 8 times\nfaster. All artifacts, including code, datasets, and models, are released under\nOS-compliant licenses, promoting openness in speech technology research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66309b3833ccd9e68c5d5171/Kzu5rlcLEhejnsYq_yWHV.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22759.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66309b3833ccd9e68c5d5171",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
            "fullname": "Sara Papi",
            "name": "spapi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23758",
            "authors": [
                {
                    "_id": "683925243a3289061eda69ee",
                    "user": {
                        "_id": "65454d7c117ecae648892170",
                        "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
                        "isPro": false,
                        "fullname": "Yusuf Dalva",
                        "user": "ydalva",
                        "type": "user"
                    },
                    "name": "Yusuf Dalva",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:26.107Z",
                    "hidden": false
                },
                {
                    "_id": "683925243a3289061eda69ef",
                    "name": "Hidir Yesiltepe",
                    "hidden": false
                },
                {
                    "_id": "683925243a3289061eda69f0",
                    "name": "Pinar Yanardag",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:59:46.000Z",
            "submittedOnDailyAt": "2025-05-30T01:56:15.021Z",
            "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with\n  Rectified Flow Transformers",
            "submittedOnDailyBy": {
                "_id": "65454d7c117ecae648892170",
                "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
                "isPro": false,
                "fullname": "Yusuf Dalva",
                "user": "ydalva",
                "type": "user"
            },
            "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.",
            "upvotes": 12,
            "discussionId": "683925263a3289061eda6a62",
            "projectPage": "https://lorashop.github.io/",
            "ai_summary": "LoRAShop, a framework for multi-concept image editing with LoRA models, leverages spatially coherent feature activation in Flux-style diffusion transformers to achieve seamless integration of multiple subjects or styles while preserving global context and identity.",
            "ai_keywords": [
                "Flux-style diffusion transformers",
                "concept-specific transformer features",
                "denoising process",
                "disentangled latent mask",
                "LoRA models",
                "identity preservation",
                "personalized diffusion models",
                "compositional visual storytelling",
                "rapid creative iteration"
            ]
        },
        "publishedAt": "2025-05-29T13:59:46.000Z",
        "title": "LoRAShop: Training-Free Multi-Concept Image Generation and Editing with\n  Rectified Flow Transformers",
        "summary": "We introduce LoRAShop, the first framework for multi-concept image editing\nwith LoRA models. LoRAShop builds on a key observation about the feature\ninteraction patterns inside Flux-style diffusion transformers: concept-specific\ntransformer features activate spatially coherent regions early in the denoising\nprocess. We harness this observation to derive a disentangled latent mask for\neach concept in a prior forward pass and blend the corresponding LoRA weights\nonly within regions bounding the concepts to be personalized. The resulting\nedits seamlessly integrate multiple subjects or styles into the original scene\nwhile preserving global context, lighting, and fine details. Our experiments\ndemonstrate that LoRAShop delivers better identity preservation compared to\nbaselines. By eliminating retraining and external constraints, LoRAShop turns\npersonalized diffusion models into a practical `photoshop-with-LoRAs' tool and\nopens new avenues for compositional visual storytelling and rapid creative\niteration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23758.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65454d7c117ecae648892170",
            "avatarUrl": "/avatars/83a7091a24bf86801176ca85234b417a.svg",
            "fullname": "Yusuf Dalva",
            "name": "ydalva",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21784",
            "authors": [
                {
                    "_id": "6838d672f527444e97b746db",
                    "name": "Tharindu Kumarage",
                    "hidden": false
                },
                {
                    "_id": "6838d672f527444e97b746dc",
                    "name": "Ninareh Mehrabi",
                    "hidden": false
                },
                {
                    "_id": "6838d672f527444e97b746dd",
                    "name": "Anil Ramakrishna",
                    "hidden": false
                },
                {
                    "_id": "6838d672f527444e97b746de",
                    "name": "Xinyan Zhao",
                    "hidden": false
                },
                {
                    "_id": "6838d672f527444e97b746df",
                    "name": "Richard Zemel",
                    "hidden": false
                },
                {
                    "_id": "6838d672f527444e97b746e0",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                },
                {
                    "_id": "6838d672f527444e97b746e1",
                    "name": "Aram Galstyan",
                    "hidden": false
                },
                {
                    "_id": "6838d672f527444e97b746e2",
                    "name": "Rahul Gupta",
                    "hidden": false
                },
                {
                    "_id": "6838d672f527444e97b746e3",
                    "name": "Charith Peris",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64779935950ad3b14ddb2a3b/cPraYttFf801ff2DLf49N.png"
            ],
            "publishedAt": "2025-05-27T21:34:40.000Z",
            "submittedOnDailyAt": "2025-05-30T16:56:55.728Z",
            "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for\n  Policy-embedded CoT Data Creation",
            "submittedOnDailyBy": {
                "_id": "64779935950ad3b14ddb2a3b",
                "avatarUrl": "/avatars/7e5f6880c3edf1c6cba8f017e2d82239.svg",
                "isPro": false,
                "fullname": "Tharindu S Kumarage",
                "user": "TharinduSK",
                "type": "user"
            },
            "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE",
            "upvotes": 12,
            "discussionId": "6838d673f527444e97b7471b",
            "ai_summary": "AIDSAFE uses multi-agent deliberation to create high-quality safety policy datasets, enhancing LLM safety without compromising utility.",
            "ai_keywords": [
                "LLMs",
                "safety reasoning",
                "policy-embedded chain-of-thought",
                "CoT",
                "data generation",
                "multi-agent deliberation",
                "data refiner",
                "supervised fine-tuning",
                "belief augmentation",
                "DPO training",
                "policy adherence",
                "reasoning quality",
                "safety generalization",
                "jailbreak robustness",
                "utility",
                "over-refusal accuracy"
            ]
        },
        "publishedAt": "2025-05-27T17:34:40.000Z",
        "title": "Towards Safety Reasoning in LLMs: AI-agentic Deliberation for\n  Policy-embedded CoT Data Creation",
        "summary": "Safety reasoning is a recent paradigm where LLMs reason over safety policies\nbefore generating responses, thereby mitigating limitations in existing safety\nmeasures such as over-refusal and jailbreak vulnerabilities. However,\nimplementing this paradigm is challenging due to the resource-intensive process\nof creating high-quality policy-embedded chain-of-thought (CoT) datasets while\nensuring reasoning remains accurate and free from hallucinations or policy\nconflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation\nfor Safety Reasoning, a novel data generation recipe that leverages multi-agent\ndeliberation to iteratively expand reasoning on safety policies. A data refiner\nstage in AIDSAFE ensures high-quality outputs by eliminating repetitive,\nredundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong\nfoundation for supervised fine-tuning (SFT)-based safety training.\nAdditionally, to address the need of preference data in alignment stages, such\nas DPO training, we introduce a supplemental recipe that uses belief\naugmentation to create distinct selected and rejected CoT samples. Our\nevaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy\nadherence and reasoning quality. Consequently, we show that fine-tuning\nopen-source LLMs on these CoTs can significantly improve safety generalization\nand jailbreak robustness while maintaining acceptable utility and over-refusal\naccuracy. AIDSAFE-generated CoT datasets can be found here:\nhttps://huggingface.co/datasets/AmazonScience/AIDSAFE",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64779935950ad3b14ddb2a3b/cPraYttFf801ff2DLf49N.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21784.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64779935950ad3b14ddb2a3b",
            "avatarUrl": "/avatars/7e5f6880c3edf1c6cba8f017e2d82239.svg",
            "fullname": "Tharindu S Kumarage",
            "name": "TharinduSK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23606",
            "authors": [
                {
                    "_id": "6839189f4a3a71a917b0514e",
                    "user": {
                        "_id": "656724074f6ec72017754d33",
                        "avatarUrl": "/avatars/e61de248f6f53719b2375077340dd033.svg",
                        "isPro": false,
                        "fullname": "QingyuShi",
                        "user": "QingyuShi",
                        "type": "user"
                    },
                    "name": "Qingyu Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:45.160Z",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b0514f",
                    "user": {
                        "_id": "63fccdac93b993a4ebd7789a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
                        "isPro": false,
                        "fullname": "Jinbin Bai",
                        "user": "BryanW",
                        "type": "user"
                    },
                    "name": "Jinbin Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:47.122Z",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b05150",
                    "name": "Zhuoran Zhao",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b05151",
                    "name": "Wenhao Chai",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b05152",
                    "name": "Kaidong Yu",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b05153",
                    "name": "Jianzong Wu",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b05154",
                    "name": "Shuangyong Song",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b05155",
                    "name": "Yunhai Tong",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b05156",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b05157",
                    "name": "Xuelong Li",
                    "hidden": false
                },
                {
                    "_id": "6839189f4a3a71a917b05158",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T16:15:48.000Z",
            "submittedOnDailyAt": "2025-05-30T01:06:53.607Z",
            "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
            "submittedOnDailyBy": {
                "_id": "63fccdac93b993a4ebd7789a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
                "isPro": false,
                "fullname": "Jinbin Bai",
                "user": "BryanW",
                "type": "user"
            },
            "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
            "upvotes": 11,
            "discussionId": "683918a14a3a71a917b051ea",
            "githubRepo": "https://github.com/M-E-AGI-Lab/Muddit",
            "ai_summary": "Muddit, a unified discrete diffusion transformer, achieves fast and high-quality generation across text and image modalities by integrating pretrained visual priors with a lightweight text decoder.",
            "ai_keywords": [
                "unified generation models",
                "autoregressive models",
                "non-autoregressive models",
                "discrete diffusion",
                "diffusion transformer",
                "pretrained backbones",
                "flexible generation",
                "multimodal generation",
                "text generation",
                "image generation",
                "vision-language reasoning",
                "quality",
                "efficiency",
                "visual priors",
                "scalable backbone"
            ]
        },
        "publishedAt": "2025-05-29T12:15:48.000Z",
        "title": "Muddit: Liberating Generation Beyond Text-to-Image with a Unified\n  Discrete Diffusion Model",
        "summary": "Unified generation models aim to handle diverse tasks across modalities --\nsuch as text generation, image generation, and vision-language reasoning --\nwithin a single architecture and decoding paradigm. Autoregressive unified\nmodels suffer from slow inference due to sequential decoding, and\nnon-autoregressive unified models suffer from weak generalization due to\nlimited pretrained backbones. We introduce Muddit, a unified discrete diffusion\ntransformer that enables fast and parallel generation across both text and\nimage modalities. Unlike prior unified diffusion models trained from scratch,\nMuddit integrates strong visual priors from a pretrained text-to-image backbone\nwith a lightweight text decoder, enabling flexible and high-quality multimodal\ngeneration under a unified architecture. Empirical results show that Muddit\nachieves competitive or superior performance compared to significantly larger\nautoregressive models in both quality and efficiency. The work highlights the\npotential of purely discrete diffusion, when equipped with strong visual\npriors, as a scalable and effective backbone for unified generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23606.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63fccdac93b993a4ebd7789a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "fullname": "Jinbin Bai",
            "name": "BryanW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22765",
            "authors": [
                {
                    "_id": "68394fb7f1e62a44f53ecf82",
                    "user": {
                        "_id": "658436f5c73f74776b19198a",
                        "avatarUrl": "/avatars/3f1d76af6fc0405d663c9294318fe83e.svg",
                        "isPro": false,
                        "fullname": "Iddo Yosha",
                        "user": "iyosha",
                        "type": "user"
                    },
                    "name": "Iddo Yosha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:52:57.738Z",
                    "hidden": false
                },
                {
                    "_id": "68394fb7f1e62a44f53ecf83",
                    "user": {
                        "_id": "66b9bc2dacdbc1d0b39c3b50",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
                        "isPro": false,
                        "fullname": "Gallil Maimon",
                        "user": "gallilmaimon",
                        "type": "user"
                    },
                    "name": "Gallil Maimon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:52:41.076Z",
                    "hidden": false
                },
                {
                    "_id": "68394fb7f1e62a44f53ecf84",
                    "name": "Yossi Adi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T18:32:56.000Z",
            "submittedOnDailyAt": "2025-05-30T05:20:21.736Z",
            "title": "StressTest: Can YOUR Speech LM Handle the Stress?",
            "submittedOnDailyBy": {
                "_id": "66b9bc2dacdbc1d0b39c3b50",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
                "isPro": false,
                "fullname": "Gallil Maimon",
                "user": "gallilmaimon",
                "type": "user"
            },
            "summary": "Sentence stress refers to emphasis, placed on specific words within a spoken\nutterance to highlight or contrast an idea, or to introduce new information. It\nis often used to imply an underlying intention that is not explicitly stated.\nRecent advances in speech-aware language models (SLMs) have enabled direct\nprocessing of audio, allowing models to bypass transcription and access the\nfull richness of the speech signal and perform audio reasoning tasks such as\nspoken question answering. Despite the crucial role of sentence stress in\nshaping meaning and speaker intent, it remains largely overlooked in evaluation\nand development of such models. In this work, we address this gap by\nintroducing StressTest, a benchmark specifically designed to evaluate a model's\nability to distinguish between interpretations of spoken sentences based on the\nstress pattern. We assess the performance of several leading SLMs and find\nthat, despite their overall capabilities, they perform poorly on such tasks. To\novercome this limitation, we propose a novel synthetic data generation\npipeline, and create Stress17k, a training set that simulates change of meaning\nimplied by stress variation. Then, we empirically show that optimizing models\nwith this synthetic dataset aligns well with real-world recordings and enables\neffective finetuning of SLMs. Results suggest, that our finetuned model,\nStresSLM, significantly outperforms existing models on both sentence stress\nreasoning and detection tasks. Code, models, data, and audio samples -\npages.cs.huji.ac.il/adiyoss-lab/stresstest.",
            "upvotes": 10,
            "discussionId": "68394fb8f1e62a44f53ecfa4",
            "projectPage": "https://pages.cs.huji.ac.il/adiyoss-lab/stresstest/",
            "githubRepo": "https://github.com/slp-rl/StressTest",
            "ai_summary": "A StressTest benchmark and synthetic Stress17k dataset are introduced to improve speech-aware language models' ability to interpret sentence stress in spoken language.",
            "ai_keywords": [
                "speech-aware language models",
                "spoken question answering",
                "sentence stress",
                "benchmark",
                "synthetic data generation pipeline",
                "audio reasoning",
                "sentence stress reasoning and detection tasks"
            ]
        },
        "publishedAt": "2025-05-28T14:32:56.000Z",
        "title": "StressTest: Can YOUR Speech LM Handle the Stress?",
        "summary": "Sentence stress refers to emphasis, placed on specific words within a spoken\nutterance to highlight or contrast an idea, or to introduce new information. It\nis often used to imply an underlying intention that is not explicitly stated.\nRecent advances in speech-aware language models (SLMs) have enabled direct\nprocessing of audio, allowing models to bypass transcription and access the\nfull richness of the speech signal and perform audio reasoning tasks such as\nspoken question answering. Despite the crucial role of sentence stress in\nshaping meaning and speaker intent, it remains largely overlooked in evaluation\nand development of such models. In this work, we address this gap by\nintroducing StressTest, a benchmark specifically designed to evaluate a model's\nability to distinguish between interpretations of spoken sentences based on the\nstress pattern. We assess the performance of several leading SLMs and find\nthat, despite their overall capabilities, they perform poorly on such tasks. To\novercome this limitation, we propose a novel synthetic data generation\npipeline, and create Stress17k, a training set that simulates change of meaning\nimplied by stress variation. Then, we empirically show that optimizing models\nwith this synthetic dataset aligns well with real-world recordings and enables\neffective finetuning of SLMs. Results suggest, that our finetuned model,\nStresSLM, significantly outperforms existing models on both sentence stress\nreasoning and detection tasks. Code, models, data, and audio samples -\npages.cs.huji.ac.il/adiyoss-lab/stresstest.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22765.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "fullname": "Gallil Maimon",
            "name": "gallilmaimon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22421",
            "authors": [
                {
                    "_id": "68391dbb0653b6a3441a7f7e",
                    "user": {
                        "_id": "6311d9ee04f842f79916158c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
                        "isPro": false,
                        "fullname": "chen",
                        "user": "antonio-c",
                        "type": "user"
                    },
                    "name": "Anthony Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:38.940Z",
                    "hidden": false
                },
                {
                    "_id": "68391dbb0653b6a3441a7f7f",
                    "name": "Wenzhao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68391dbb0653b6a3441a7f80",
                    "user": {
                        "_id": "647068944be5cf1f33491cb9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Aqudl2PTSKPAylBhlccWr.png",
                        "isPro": false,
                        "fullname": "Yida Wang",
                        "user": "wangyida",
                        "type": "user"
                    },
                    "name": "Yida Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:36.769Z",
                    "hidden": false
                },
                {
                    "_id": "68391dbb0653b6a3441a7f81",
                    "name": "Xueyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68391dbb0653b6a3441a7f82",
                    "name": "Kun Zhan",
                    "hidden": false
                },
                {
                    "_id": "68391dbb0653b6a3441a7f83",
                    "name": "Peng Jia",
                    "hidden": false
                },
                {
                    "_id": "68391dbb0653b6a3441a7f84",
                    "name": "Kurt Keutzer",
                    "hidden": false
                },
                {
                    "_id": "68391dbb0653b6a3441a7f85",
                    "name": "Shanghang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T14:46:51.000Z",
            "submittedOnDailyAt": "2025-05-30T01:26:00.575Z",
            "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action\n  Control",
            "submittedOnDailyBy": {
                "_id": "6311d9ee04f842f79916158c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
                "isPro": false,
                "fullname": "chen",
                "user": "antonio-c",
                "type": "user"
            },
            "summary": "Recent advancements in world models have revolutionized dynamic environment\nsimulation, allowing systems to foresee future states and assess potential\nactions. In autonomous driving, these capabilities help vehicles anticipate the\nbehavior of other road users, perform risk-aware planning, accelerate training\nin simulation, and adapt to novel scenarios, thereby enhancing safety and\nreliability. Current approaches exhibit deficiencies in maintaining robust 3D\ngeometric consistency or accumulating artifacts during occlusion handling, both\ncritical for reliable safety assessment in autonomous navigation tasks. To\naddress this, we introduce GeoDrive, which explicitly integrates robust 3D\ngeometry conditions into driving world models to enhance spatial understanding\nand action controllability. Specifically, we first extract a 3D representation\nfrom the input frame and then obtain its 2D rendering based on the\nuser-specified ego-car trajectory. To enable dynamic modeling, we propose a\ndynamic editing module during training to enhance the renderings by editing the\npositions of the vehicles. Extensive experiments demonstrate that our method\nsignificantly outperforms existing models in both action accuracy and 3D\nspatial awareness, leading to more realistic, adaptable, and reliable scene\nmodeling for safer autonomous driving. Additionally, our model can generalize\nto novel trajectories and offers interactive scene editing capabilities, such\nas object editing and object trajectory control.",
            "upvotes": 9,
            "discussionId": "68391dbc0653b6a3441a7fd1",
            "githubRepo": "https://github.com/antonioo-c/GeoDrive",
            "ai_summary": "GeoDrive integrates robust 3D geometry into driving world models to improve spatial understanding and action controllability in autonomous navigation, enhancing safety and reliability.",
            "ai_keywords": [
                "world models",
                "dynamic environment simulation",
                "autonomous driving",
                "3D geometric consistency",
                "occlusion handling",
                "3D representation",
                "2D rendering",
                "ego-car trajectory",
                "dynamic editing module",
                "action accuracy",
                "3D spatial awareness",
                "scene modeling",
                "object editing",
                "object trajectory control"
            ]
        },
        "publishedAt": "2025-05-28T10:46:51.000Z",
        "title": "GeoDrive: 3D Geometry-Informed Driving World Model with Precise Action\n  Control",
        "summary": "Recent advancements in world models have revolutionized dynamic environment\nsimulation, allowing systems to foresee future states and assess potential\nactions. In autonomous driving, these capabilities help vehicles anticipate the\nbehavior of other road users, perform risk-aware planning, accelerate training\nin simulation, and adapt to novel scenarios, thereby enhancing safety and\nreliability. Current approaches exhibit deficiencies in maintaining robust 3D\ngeometric consistency or accumulating artifacts during occlusion handling, both\ncritical for reliable safety assessment in autonomous navigation tasks. To\naddress this, we introduce GeoDrive, which explicitly integrates robust 3D\ngeometry conditions into driving world models to enhance spatial understanding\nand action controllability. Specifically, we first extract a 3D representation\nfrom the input frame and then obtain its 2D rendering based on the\nuser-specified ego-car trajectory. To enable dynamic modeling, we propose a\ndynamic editing module during training to enhance the renderings by editing the\npositions of the vehicles. Extensive experiments demonstrate that our method\nsignificantly outperforms existing models in both action accuracy and 3D\nspatial awareness, leading to more realistic, adaptable, and reliable scene\nmodeling for safer autonomous driving. Additionally, our model can generalize\nto novel trajectories and offers interactive scene editing capabilities, such\nas object editing and object trajectory control.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22421.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6311d9ee04f842f79916158c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/PvOYgDEpe0x5ExLyj1BK-.png",
            "fullname": "chen",
            "name": "antonio-c",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23754",
            "authors": [
                {
                    "_id": "68391c6dc4405ad056a9ff79",
                    "name": "Ziyin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff7a",
                    "name": "Jiahao Xu",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff7b",
                    "name": "Zhiwei He",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff7c",
                    "name": "Tian Liang",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff7d",
                    "name": "Qiuzhi Liu",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff7e",
                    "name": "Yansi Li",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff7f",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff80",
                    "name": "Zhengwen Liang",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff81",
                    "name": "Zhuosheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff82",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff83",
                    "name": "Zhaopeng Tu",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff84",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "68391c6dc4405ad056a9ff85",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:59:39.000Z",
            "submittedOnDailyAt": "2025-05-30T01:18:32.068Z",
            "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "660399710f1fc2f16de18072",
                "avatarUrl": "/avatars/c22a749cc45db693c2d9ea877c7cace4.svg",
                "isPro": false,
                "fullname": "Jiahao Xu",
                "user": "Jiahao004",
                "type": "user"
            },
            "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.",
            "upvotes": 8,
            "discussionId": "68391c6ec4405ad056a9ffb2",
            "ai_summary": "DeepTheorem enhances LLM theorem-proving through a large-scale natural language dataset and a tailored reinforcement learning strategy, achieving state-of-the-art results in informal theorem proving.",
            "ai_keywords": [
                "DeepTheorem",
                "automated theorem proving",
                "ATP",
                "natural language",
                "informal theorem-proving",
                "large-scale benchmark dataset",
                "IMO-level theorems",
                "reinforcement learning",
                "RL-Zero",
                "verified theorem variants",
                "proof correctness",
                "reasoning quality"
            ]
        },
        "publishedAt": "2025-05-29T13:59:39.000Z",
        "title": "DeepTheorem: Advancing LLM Reasoning for Theorem Proving Through Natural\n  Language and Reinforcement Learning",
        "summary": "Theorem proving serves as a major testbed for evaluating complex reasoning\nabilities in large language models (LLMs). However, traditional automated\ntheorem proving (ATP) approaches rely heavily on formal proof systems that\npoorly align with LLMs' strength derived from informal, natural language\nknowledge acquired during pre-training. In this work, we propose DeepTheorem, a\ncomprehensive informal theorem-proving framework exploiting natural language to\nenhance LLM mathematical reasoning. DeepTheorem includes a large-scale\nbenchmark dataset consisting of 121K high-quality IMO-level informal theorems\nand proofs spanning diverse mathematical domains, rigorously annotated for\ncorrectness, difficulty, and topic categories, accompanied by systematically\nconstructed verifiable theorem variants. We devise a novel reinforcement\nlearning strategy (RL-Zero) explicitly tailored to informal theorem proving,\nleveraging the verified theorem variants to incentivize robust mathematical\ninference. Additionally, we propose comprehensive outcome and process\nevaluation metrics examining proof correctness and the quality of reasoning\nsteps. Extensive experimental analyses demonstrate DeepTheorem significantly\nimproves LLM theorem-proving performance compared to existing datasets and\nsupervised fine-tuning protocols, achieving state-of-the-art accuracy and\nreasoning quality. Our findings highlight DeepTheorem's potential to\nfundamentally advance automated informal theorem proving and mathematical\nexploration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23754.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "660399710f1fc2f16de18072",
            "avatarUrl": "/avatars/c22a749cc45db693c2d9ea877c7cace4.svg",
            "fullname": "Jiahao Xu",
            "name": "Jiahao004",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23751",
            "authors": [
                {
                    "_id": "683907321bf7a7a94309d26b",
                    "user": {
                        "_id": "665bed48cd4115f092a0f750",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Muvzf256flNUAa8mBwkov.png",
                        "isPro": false,
                        "fullname": "Declan Kutscher",
                        "user": "d3tk",
                        "type": "user"
                    },
                    "name": "Declan Kutscher",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:54:30.099Z",
                    "hidden": false
                },
                {
                    "_id": "683907321bf7a7a94309d26c",
                    "name": "David M. Chan",
                    "hidden": false
                },
                {
                    "_id": "683907321bf7a7a94309d26d",
                    "name": "Yutong Bai",
                    "hidden": false
                },
                {
                    "_id": "683907321bf7a7a94309d26e",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "683907321bf7a7a94309d26f",
                    "user": {
                        "_id": "630489e5dae2eb7d083e78b1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630489e5dae2eb7d083e78b1/RKjwiquU1JzaHoV4op42F.jpeg",
                        "isPro": false,
                        "fullname": "Ritwik Gupta",
                        "user": "RitwikGupta",
                        "type": "user"
                    },
                    "name": "Ritwik Gupta",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:54:28.074Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:59:30.000Z",
            "submittedOnDailyAt": "2025-05-30T09:10:23.586Z",
            "title": "REOrdering Patches Improves Vision Models",
            "submittedOnDailyBy": {
                "_id": "665bed48cd4115f092a0f750",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Muvzf256flNUAa8mBwkov.png",
                "isPro": false,
                "fullname": "Declan Kutscher",
                "user": "d3tk",
                "type": "user"
            },
            "summary": "Sequence models such as transformers require inputs to be represented as\none-dimensional sequences. In vision, this typically involves flattening images\nusing a fixed row-major (raster-scan) order. While full self-attention is\npermutation-equivariant, modern long-sequence transformers increasingly rely on\narchitectural approximations that break this invariance and introduce\nsensitivity to patch ordering. We show that patch order significantly affects\nmodel performance in such settings, with simple alternatives like column-major\nor Hilbert curves yielding notable accuracy shifts. Motivated by this, we\npropose REOrder, a two-stage framework for discovering task-optimal patch\norderings. First, we derive an information-theoretic prior by evaluating the\ncompressibility of various patch sequences. Then, we learn a policy over\npermutations by optimizing a Plackett-Luce policy using REINFORCE. This\napproach enables efficient learning in a combinatorial permutation space.\nREOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to\n3.01% and Functional Map of the World by 13.35%.",
            "upvotes": 8,
            "discussionId": "683907341bf7a7a94309d2ce",
            "projectPage": "https://d3tk.github.io/REOrder/",
            "githubRepo": "https://github.com/d3tk/REOrder",
            "ai_summary": "REOrder discovers task-optimal patch orderings for long-sequence transformers, significantly improving accuracy over traditional ordering methods.",
            "ai_keywords": [
                "sequence models",
                "transformers",
                "self-attention",
                "permutation-equivariant",
                "patch ordering",
                "column-major",
                "Hilbert curves",
                "information-theoretic prior",
                "Plackett-Luce policy",
                "REINFORCE",
                "ImageNet-1K",
                "Functional Map of the World"
            ]
        },
        "publishedAt": "2025-05-29T13:59:30.000Z",
        "title": "REOrdering Patches Improves Vision Models",
        "summary": "Sequence models such as transformers require inputs to be represented as\none-dimensional sequences. In vision, this typically involves flattening images\nusing a fixed row-major (raster-scan) order. While full self-attention is\npermutation-equivariant, modern long-sequence transformers increasingly rely on\narchitectural approximations that break this invariance and introduce\nsensitivity to patch ordering. We show that patch order significantly affects\nmodel performance in such settings, with simple alternatives like column-major\nor Hilbert curves yielding notable accuracy shifts. Motivated by this, we\npropose REOrder, a two-stage framework for discovering task-optimal patch\norderings. First, we derive an information-theoretic prior by evaluating the\ncompressibility of various patch sequences. Then, we learn a policy over\npermutations by optimizing a Plackett-Luce policy using REINFORCE. This\napproach enables efficient learning in a combinatorial permutation space.\nREOrder improves top-1 accuracy over row-major ordering on ImageNet-1K by up to\n3.01% and Functional Map of the World by 13.35%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23751.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "665bed48cd4115f092a0f750",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Muvzf256flNUAa8mBwkov.png",
            "fullname": "Declan Kutscher",
            "name": "d3tk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23416",
            "authors": [
                {
                    "_id": "68393d075711daf8cc919c04",
                    "user": {
                        "_id": "62e21907eda17fc126a15210",
                        "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
                        "isPro": false,
                        "fullname": "Jang-Hyun Kim",
                        "user": "Jang-Hyun",
                        "type": "user"
                    },
                    "name": "Jang-Hyun Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:14.736Z",
                    "hidden": false
                },
                {
                    "_id": "68393d075711daf8cc919c05",
                    "user": {
                        "_id": "60eb9074cc720726777d22a2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60eb9074cc720726777d22a2/aI-pHJRmnYOfC5fH7fFzD.jpeg",
                        "isPro": false,
                        "fullname": "Jinuk Kim",
                        "user": "jusjinuk",
                        "type": "user"
                    },
                    "name": "Jinuk Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:17.350Z",
                    "hidden": false
                },
                {
                    "_id": "68393d075711daf8cc919c06",
                    "name": "Sangwoo Kwon",
                    "hidden": false
                },
                {
                    "_id": "68393d075711daf8cc919c07",
                    "name": "Jae W. Lee",
                    "hidden": false
                },
                {
                    "_id": "68393d075711daf8cc919c08",
                    "name": "Sangdoo Yun",
                    "hidden": false
                },
                {
                    "_id": "68393d075711daf8cc919c09",
                    "name": "Hyun Oh Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T13:05:47.000Z",
            "submittedOnDailyAt": "2025-05-30T03:37:32.573Z",
            "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
            "submittedOnDailyBy": {
                "_id": "62e21907eda17fc126a15210",
                "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
                "isPro": false,
                "fullname": "Jang-Hyun Kim",
                "user": "Jang-Hyun",
                "type": "user"
            },
            "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4times and FlashAttention decoding latency by approximately\n2times, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
            "upvotes": 8,
            "discussionId": "68393d075711daf8cc919c2b",
            "githubRepo": "https://github.com/snu-mllab/KVzip",
            "ai_summary": "KVzip, a query-agnostic KV cache eviction method for transformer-based LLMs, reduces KV cache size and decoding latency while maintaining performance across various tasks and models.",
            "ai_keywords": [
                "Transformer-based large language models (LLMs)",
                "KV cache",
                "context length",
                "query-agnostic",
                "KVzip",
                "query-aware",
                "KV eviction",
                "attention latency",
                "FlashAttention",
                "question-answering",
                "retrieval",
                "reasoning",
                "code comprehension",
                "LLaMA3.1-8B",
                "Qwen2.5-14B",
                "Gemma3-12B"
            ]
        },
        "publishedAt": "2025-05-29T09:05:47.000Z",
        "title": "KVzip: Query-Agnostic KV Cache Compression with Context Reconstruction",
        "summary": "Transformer-based large language models (LLMs) cache context as key-value\n(KV) pairs during inference. As context length grows, KV cache sizes expand,\nleading to substantial memory overhead and increased attention latency. This\npaper introduces KVzip, a query-agnostic KV cache eviction method enabling\neffective reuse of compressed KV caches across diverse queries. KVzip\nquantifies the importance of a KV pair using the underlying LLM to reconstruct\noriginal contexts from cached KV pairs, subsequently evicting pairs with lower\nimportance. Extensive empirical evaluations demonstrate that KVzip reduces KV\ncache size by 3-4times and FlashAttention decoding latency by approximately\n2times, with negligible performance loss in question-answering, retrieval,\nreasoning, and code comprehension tasks. Evaluations include various models\nsuch as LLaMA3.1-8B, Qwen2.5-14B, and Gemma3-12B, with context lengths reaching\nup to 170K tokens. KVzip significantly outperforms existing query-aware KV\neviction methods, which suffer from performance degradation even at a 90% cache\nbudget ratio under multi-query scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23416.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e21907eda17fc126a15210",
            "avatarUrl": "/avatars/097879cf1eafa5e77bde419c7172fac6.svg",
            "fullname": "Jang-Hyun Kim",
            "name": "Jang-Hyun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17818",
            "authors": [
                {
                    "_id": "6837b52b1233747046cfa5df",
                    "name": "Daeun Kyung",
                    "hidden": false
                },
                {
                    "_id": "6837b52b1233747046cfa5e0",
                    "name": "Hyunseung Chung",
                    "hidden": false
                },
                {
                    "_id": "6837b52b1233747046cfa5e1",
                    "name": "Seongsu Bae",
                    "hidden": false
                },
                {
                    "_id": "6837b52b1233747046cfa5e2",
                    "name": "Jiho Kim",
                    "hidden": false
                },
                {
                    "_id": "6837b52b1233747046cfa5e3",
                    "name": "Jae Ho Sohn",
                    "hidden": false
                },
                {
                    "_id": "6837b52b1233747046cfa5e4",
                    "name": "Taerim Kim",
                    "hidden": false
                },
                {
                    "_id": "6837b52b1233747046cfa5e5",
                    "name": "Soo Kyung Kim",
                    "hidden": false
                },
                {
                    "_id": "6837b52b1233747046cfa5e6",
                    "name": "Edward Choi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/645cd00f5ebf379fd6d7a4c1/LP4mF9N1AGEXPzwV9jmbE.png"
            ],
            "publishedAt": "2025-05-23T12:34:48.000Z",
            "submittedOnDailyAt": "2025-05-30T01:25:38.142Z",
            "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient\n  Interactions",
            "submittedOnDailyBy": {
                "_id": "645cd00f5ebf379fd6d7a4c1",
                "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
                "isPro": false,
                "fullname": "Daeun Kyung",
                "user": "dek924",
                "type": "user"
            },
            "summary": "Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluated eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3, was validated by four clinicians to confirm the robustness of\nour framework. As an open-source, customizable platform, PatientSim provides a\nreproducible and scalable solution that can be customized for specific training\nneeds. Offering a privacy-compliant environment, it serves as a robust testbed\nfor evaluating medical dialogue systems across diverse patient presentations\nand shows promise as an educational tool for healthcare.",
            "upvotes": 8,
            "discussionId": "6837b52c1233747046cfa614",
            "ai_summary": "PatientSim generates diverse and realistic patient personas using clinical data to evaluate LLMs in medical dialogue settings.",
            "ai_keywords": [
                "clinical profiles",
                "personas",
                "personality",
                "language proficiency",
                "medical history recall level",
                "cognitive confusion level",
                "patient simulator",
                "factual accuracy",
                "persona consistency",
                "privacy-compliant"
            ]
        },
        "publishedAt": "2025-05-23T08:34:48.000Z",
        "title": "PatientSim: A Persona-Driven Simulator for Realistic Doctor-Patient\n  Interactions",
        "summary": "Doctor-patient consultations require multi-turn, context-aware communication\ntailored to diverse patient personas. Training or evaluating doctor LLMs in\nsuch settings requires realistic patient interaction systems. However, existing\nsimulators often fail to reflect the full range of personas seen in clinical\npractice. To address this, we introduce PatientSim, a patient simulator that\ngenerates realistic and diverse patient personas for clinical scenarios,\ngrounded in medical expertise. PatientSim operates using: 1) clinical profiles,\nincluding symptoms and medical history, derived from real-world data in the\nMIMIC-ED and MIMIC-IV datasets, and 2) personas defined by four axes:\npersonality, language proficiency, medical history recall level, and cognitive\nconfusion level, resulting in 37 unique combinations. We evaluated eight LLMs\nfor factual accuracy and persona consistency. The top-performing open-source\nmodel, Llama 3.3, was validated by four clinicians to confirm the robustness of\nour framework. As an open-source, customizable platform, PatientSim provides a\nreproducible and scalable solution that can be customized for specific training\nneeds. Offering a privacy-compliant environment, it serves as a robust testbed\nfor evaluating medical dialogue systems across diverse patient presentations\nand shows promise as an educational tool for healthcare.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/645cd00f5ebf379fd6d7a4c1/LP4mF9N1AGEXPzwV9jmbE.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17818.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645cd00f5ebf379fd6d7a4c1",
            "avatarUrl": "/avatars/be8375ed32ac234919e26a2450bf9d38.svg",
            "fullname": "Daeun Kyung",
            "name": "dek924",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22810",
            "authors": [
                {
                    "_id": "68395c867f983113faf29005",
                    "name": "Zhoufaran Yang",
                    "hidden": false
                },
                {
                    "_id": "68395c867f983113faf29006",
                    "name": "Yan Shu",
                    "hidden": false
                },
                {
                    "_id": "68395c867f983113faf29007",
                    "name": "Zhifei Yang",
                    "hidden": false
                },
                {
                    "_id": "68395c867f983113faf29008",
                    "name": "Yan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68395c867f983113faf29009",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "68395c867f983113faf2900a",
                    "name": "Keyang Lu",
                    "hidden": false
                },
                {
                    "_id": "68395c867f983113faf2900b",
                    "name": "Gangyan Zeng",
                    "hidden": false
                },
                {
                    "_id": "68395c867f983113faf2900c",
                    "name": "Shaohui Liu",
                    "hidden": false
                },
                {
                    "_id": "68395c867f983113faf2900d",
                    "name": "Yu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68395c867f983113faf2900e",
                    "name": "Nicu Sebe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T19:39:35.000Z",
            "submittedOnDailyAt": "2025-05-30T05:52:32.917Z",
            "title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding",
            "submittedOnDailyBy": {
                "_id": "65c4f99b27736b5b86c2cbda",
                "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
                "isPro": false,
                "fullname": "Yan Shu",
                "user": "sy1998",
                "type": "user"
            },
            "summary": "Visual texts embedded in videos carry rich semantic information, which is\ncrucial for both holistic video understanding and fine-grained reasoning about\nlocal human actions. However, existing video understanding benchmarks largely\noverlook textual information, while OCR-specific benchmarks are constrained to\nstatic images, limiting their ability to capture the interaction between text\nand dynamic visual contexts. To address this gap, we propose VidText, a new\nbenchmark designed for comprehensive and in-depth evaluation of video text\nunderstanding. VidText offers the following key features: 1) It covers a wide\nrange of real-world scenarios and supports multilingual content, encompassing\ndiverse settings where video text naturally appears. 2) It introduces a\nhierarchical evaluation framework with video-level, clip-level, and\ninstance-level tasks, enabling assessment of both global summarization and\nlocal retrieval capabilities. 3) The benchmark also introduces a set of paired\nperception reasoning tasks, ranging from visual text perception to cross-modal\nreasoning between textual and visual information. Extensive experiments on 18\nstate-of-the-art Large Multimodal Models (LMMs) reveal that current models\nstruggle across most tasks, with significant room for improvement. Further\nanalysis highlights the impact of both model-intrinsic factors, such as input\nresolution and OCR capability, and external factors, including the use of\nauxiliary information and Chain-of-Thought reasoning strategies. We hope\nVidText will fill the current gap in video understanding benchmarks and serve\nas a foundation for future research on multimodal reasoning with video text in\ndynamic environments.",
            "upvotes": 7,
            "discussionId": "68395c897f983113faf290eb",
            "ai_summary": "VidText is a new benchmark that evaluates video text understanding across various tasks, covering global summarization and local retrieval, and highlights challenges for current multimodal models.",
            "ai_keywords": [
                "LMMs",
                "video text perception",
                "cross-modal reasoning",
                "Chain-of-Thought reasoning"
            ]
        },
        "publishedAt": "2025-05-28T15:39:35.000Z",
        "title": "VidText: Towards Comprehensive Evaluation for Video Text Understanding",
        "summary": "Visual texts embedded in videos carry rich semantic information, which is\ncrucial for both holistic video understanding and fine-grained reasoning about\nlocal human actions. However, existing video understanding benchmarks largely\noverlook textual information, while OCR-specific benchmarks are constrained to\nstatic images, limiting their ability to capture the interaction between text\nand dynamic visual contexts. To address this gap, we propose VidText, a new\nbenchmark designed for comprehensive and in-depth evaluation of video text\nunderstanding. VidText offers the following key features: 1) It covers a wide\nrange of real-world scenarios and supports multilingual content, encompassing\ndiverse settings where video text naturally appears. 2) It introduces a\nhierarchical evaluation framework with video-level, clip-level, and\ninstance-level tasks, enabling assessment of both global summarization and\nlocal retrieval capabilities. 3) The benchmark also introduces a set of paired\nperception reasoning tasks, ranging from visual text perception to cross-modal\nreasoning between textual and visual information. Extensive experiments on 18\nstate-of-the-art Large Multimodal Models (LMMs) reveal that current models\nstruggle across most tasks, with significant room for improvement. Further\nanalysis highlights the impact of both model-intrinsic factors, such as input\nresolution and OCR capability, and external factors, including the use of\nauxiliary information and Chain-of-Thought reasoning strategies. We hope\nVidText will fill the current gap in video understanding benchmarks and serve\nas a foundation for future research on multimodal reasoning with video text in\ndynamic environments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22810.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65c4f99b27736b5b86c2cbda",
            "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
            "fullname": "Yan Shu",
            "name": "sy1998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.14321",
            "authors": [
                {
                    "_id": "68396605ac00da416d094bbf",
                    "name": "Bo Feng",
                    "hidden": false
                },
                {
                    "_id": "68396605ac00da416d094bc0",
                    "name": "Zhengfeng Lai",
                    "hidden": false
                },
                {
                    "_id": "68396605ac00da416d094bc1",
                    "name": "Shiyu Li",
                    "hidden": false
                },
                {
                    "_id": "68396605ac00da416d094bc2",
                    "name": "Zizhen Wang",
                    "hidden": false
                },
                {
                    "_id": "68396605ac00da416d094bc3",
                    "name": "Simon Wang",
                    "hidden": false
                },
                {
                    "_id": "68396605ac00da416d094bc4",
                    "name": "Ping Huang",
                    "hidden": false
                },
                {
                    "_id": "68396605ac00da416d094bc5",
                    "name": "Meng Cao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T13:07:55.000Z",
            "submittedOnDailyAt": "2025-05-30T06:32:58.958Z",
            "title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or\n  True Temporal Understanding?",
            "submittedOnDailyBy": {
                "_id": "66b5295f83425904fa7a1a6a",
                "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
                "isPro": false,
                "fullname": "Zhengfeng Lai",
                "user": "jefflai",
                "type": "user"
            },
            "summary": "Existing video understanding benchmarks often conflate knowledge-based and\npurely image-based questions, rather than clearly isolating a model's temporal\nreasoning ability, which is the key aspect that distinguishes video\nunderstanding from other modalities. We identify two major limitations that\nobscure whether higher scores truly indicate stronger understanding of the\ndynamic content in videos: (1) strong language priors, where models can answer\nquestions without watching the video; and (2) shuffling invariance, where\nmodels maintain similar performance on certain questions even when video frames\nare temporally shuffled. To alleviate these issues, we propose VBenchComp, an\nautomated pipeline that categorizes questions into different domains:\nLLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions\ncan be answered without viewing the video; Semantic questions remain answerable\neven when the video frames are shuffled; and Temporal questions require\nunderstanding the correct temporal order of frames. The rest of the questions\nare labeled as Others. This can enable fine-grained evaluation of different\ncapabilities of a video LLM. Our analysis reveals nuanced model weaknesses that\nare hidden by traditional overall scores, and we offer insights and\nrecommendations for designing future benchmarks that more accurately assess\nvideo LLMs.",
            "upvotes": 7,
            "discussionId": "68396607ac00da416d094c6c",
            "ai_summary": "VBenchComp, an automated pipeline, categorizes video LLM questions into different domains to evaluate temporal reasoning and isolate model weaknesses beyond overall scores.",
            "ai_keywords": [
                "video understanding",
                "LLM-Answerable",
                "Semantic",
                "Temporal"
            ]
        },
        "publishedAt": "2025-05-20T09:07:55.000Z",
        "title": "Breaking Down Video LLM Benchmarks: Knowledge, Spatial Perception, or\n  True Temporal Understanding?",
        "summary": "Existing video understanding benchmarks often conflate knowledge-based and\npurely image-based questions, rather than clearly isolating a model's temporal\nreasoning ability, which is the key aspect that distinguishes video\nunderstanding from other modalities. We identify two major limitations that\nobscure whether higher scores truly indicate stronger understanding of the\ndynamic content in videos: (1) strong language priors, where models can answer\nquestions without watching the video; and (2) shuffling invariance, where\nmodels maintain similar performance on certain questions even when video frames\nare temporally shuffled. To alleviate these issues, we propose VBenchComp, an\nautomated pipeline that categorizes questions into different domains:\nLLM-Answerable, Semantic, and Temporal. Specifically, LLM-Answerable questions\ncan be answered without viewing the video; Semantic questions remain answerable\neven when the video frames are shuffled; and Temporal questions require\nunderstanding the correct temporal order of frames. The rest of the questions\nare labeled as Others. This can enable fine-grained evaluation of different\ncapabilities of a video LLM. Our analysis reveals nuanced model weaknesses that\nare hidden by traditional overall scores, and we offer insights and\nrecommendations for designing future benchmarks that more accurately assess\nvideo LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14321.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66b5295f83425904fa7a1a6a",
            "avatarUrl": "/avatars/a35568fb933ceef7451bd88fb3d5ab17.svg",
            "fullname": "Zhengfeng Lai",
            "name": "jefflai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23387",
            "authors": [
                {
                    "_id": "68397697a14020a996c30112",
                    "name": "Mingzhe Du",
                    "hidden": false
                },
                {
                    "_id": "68397697a14020a996c30113",
                    "name": "Luu Tuan Tuan",
                    "hidden": false
                },
                {
                    "_id": "68397697a14020a996c30114",
                    "name": "Yue Liu",
                    "hidden": false
                },
                {
                    "_id": "68397697a14020a996c30115",
                    "name": "Yuhao Qing",
                    "hidden": false
                },
                {
                    "_id": "68397697a14020a996c30116",
                    "name": "Dong Huang",
                    "hidden": false
                },
                {
                    "_id": "68397697a14020a996c30117",
                    "name": "Xinyi He",
                    "hidden": false
                },
                {
                    "_id": "68397697a14020a996c30118",
                    "name": "Qian Liu",
                    "hidden": false
                },
                {
                    "_id": "68397697a14020a996c30119",
                    "name": "Zejun Ma",
                    "hidden": false
                },
                {
                    "_id": "68397697a14020a996c3011a",
                    "name": "See-kiong Ng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T12:14:29.000Z",
            "submittedOnDailyAt": "2025-05-30T07:47:46.606Z",
            "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code\n  Efficiency Optimization",
            "submittedOnDailyBy": {
                "_id": "61711f02e0b1ddb56eb9b526",
                "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
                "isPro": true,
                "fullname": "Mingzhe Du",
                "user": "Elfsong",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) generate functionally correct solutions but\noften fall short in code efficiency, a critical bottleneck for real-world\ndeployment. In this paper, we introduce a novel test-time iterative\noptimization framework to address this, employing a closed-loop system where\nLLMs iteratively refine code based on empirical performance feedback from an\nexecution sandbox. We explore three training strategies: Supervised Fine-Tuning\n(SFT), Direct Preference Optimization (DPO), and Group Relative Policy\nOptimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark\nshow that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,\nusing reinforcement learning (RL) with execution feedback, continuously\noptimizes code performance, significantly boosting both pass@1 (from 47% to\n62%) and the likelihood of outperforming human submissions in efficiency (from\n31% to 45%). Our work demonstrates effective test-time code efficiency\nimprovement and critically reveals the power of RL in teaching LLMs to truly\nself-improve code efficiency.",
            "upvotes": 6,
            "discussionId": "68397698a14020a996c30176",
            "ai_summary": "A novel test-time iterative optimization framework using reinforcement learning continuously enhances code efficiency generated by large language models.",
            "ai_keywords": [
                "Large Language Models",
                "iterative optimization",
                "closed-loop system",
                "empirical performance feedback",
                "Supervised Fine-Tuning",
                "Direct Preference Optimization",
                "Group Relative Policy Optimization",
                "reinforcement learning",
                "pass@1",
                "Venus dataset",
                "APPS benchmark"
            ]
        },
        "publishedAt": "2025-05-29T08:14:29.000Z",
        "title": "Afterburner: Reinforcement Learning Facilitates Self-Improving Code\n  Efficiency Optimization",
        "summary": "Large Language Models (LLMs) generate functionally correct solutions but\noften fall short in code efficiency, a critical bottleneck for real-world\ndeployment. In this paper, we introduce a novel test-time iterative\noptimization framework to address this, employing a closed-loop system where\nLLMs iteratively refine code based on empirical performance feedback from an\nexecution sandbox. We explore three training strategies: Supervised Fine-Tuning\n(SFT), Direct Preference Optimization (DPO), and Group Relative Policy\nOptimization~(GRPO). Experiments on our Venus dataset and the APPS benchmark\nshow that SFT and DPO rapidly saturate in efficiency gains. In contrast, GRPO,\nusing reinforcement learning (RL) with execution feedback, continuously\noptimizes code performance, significantly boosting both pass@1 (from 47% to\n62%) and the likelihood of outperforming human submissions in efficiency (from\n31% to 45%). Our work demonstrates effective test-time code efficiency\nimprovement and critically reveals the power of RL in teaching LLMs to truly\nself-improve code efficiency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23387.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61711f02e0b1ddb56eb9b526",
            "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
            "fullname": "Mingzhe Du",
            "name": "Elfsong",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20755",
            "authors": [
                {
                    "_id": "6836a4401314d4ac39a526f3",
                    "user": {
                        "_id": "6838043c11b8b14a21f6ecd8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
                        "isPro": false,
                        "fullname": "Yifei Wang",
                        "user": "smallAI",
                        "type": "user"
                    },
                    "name": "Yifei Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-29T07:43:51.221Z",
                    "hidden": false
                },
                {
                    "_id": "6836a4401314d4ac39a526f4",
                    "name": "Weimin Bai",
                    "hidden": false
                },
                {
                    "_id": "6836a4401314d4ac39a526f5",
                    "name": "Colin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6836a4401314d4ac39a526f6",
                    "name": "Debing Zhang",
                    "hidden": false
                },
                {
                    "_id": "6836a4401314d4ac39a526f7",
                    "name": "Weijian Luo",
                    "hidden": false
                },
                {
                    "_id": "6836a4401314d4ac39a526f8",
                    "name": "He Sun",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6838043c11b8b14a21f6ecd8/y_4g5XJdwhTEjDjAMTQaS.png"
            ],
            "publishedAt": "2025-05-27T05:55:45.000Z",
            "submittedOnDailyAt": "2025-05-30T02:37:16.654Z",
            "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion\n  Divergence Instruction",
            "submittedOnDailyBy": {
                "_id": "6838043c11b8b14a21f6ecd8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
                "isPro": false,
                "fullname": "Yifei Wang",
                "user": "smallAI",
                "type": "user"
            },
            "summary": "In this paper, we unify more than 10 existing one-step diffusion distillation\napproaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a\ntheory-driven framework which we name the \\emph{Uni-Instruct}.\nUni-Instruct is motivated by our proposed diffusion expansion theory of the\nf-divergence family. Then we introduce key theories that overcome the\nintractability issue of the original expanded f-divergence, resulting in an\nequivalent yet tractable loss that effectively trains one-step diffusion models\nby minimizing the expanded f-divergence family. The novel unification\nintroduced by Uni-Instruct not only offers new theoretical contributions that\nhelp understand existing approaches from a high-level perspective but also\nleads to state-of-the-art one-step diffusion generation performances. On the\nCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet\nInception Distance (FID) values of \\emph{1.46} for unconditional\ngeneration and \\emph{1.38} for conditional generation. On the\nImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA\none-step generation FID of \\emph{1.02}, which outperforms its 79-step\nteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).\nWe also apply Uni-Instruct on broader tasks like text-to-3D generation. For\ntext-to-3D generation, Uni-Instruct gives decent results, which slightly\noutperforms previous methods, such as SDS and VSD, in terms of both generation\nquality and diversity. Both the solid theoretical and empirical contributions\nof Uni-Instruct will potentially help future studies on one-step diffusion\ndistillation and knowledge transferring of diffusion models.",
            "upvotes": 6,
            "discussionId": "6836a4441314d4ac39a52803",
            "ai_summary": "Uni-Instruct unifies and enhances one-step diffusion distillation methods through a novel diffusion expansion theory, achieving state-of-the-art performance in unconditional and conditional image generation and text-to-3D generation.",
            "ai_keywords": [
                "Uni-Instruct",
                "diffusion expansion theory",
                "f-divergence",
                "one-step diffusion models",
                "Frechet Inception Distance (FID)",
                "CIFAR10",
                "ImageNet-64x64",
                "text-to-3D generation",
                "SDS",
                "VSD"
            ]
        },
        "publishedAt": "2025-05-27T01:55:45.000Z",
        "title": "Uni-Instruct: One-step Diffusion Model through Unified Diffusion\n  Divergence Instruction",
        "summary": "In this paper, we unify more than 10 existing one-step diffusion distillation\napproaches, such as Diff-Instruct, DMD, SIM, SiD, f-distill, etc, inside a\ntheory-driven framework which we name the \\emph{Uni-Instruct}.\nUni-Instruct is motivated by our proposed diffusion expansion theory of the\nf-divergence family. Then we introduce key theories that overcome the\nintractability issue of the original expanded f-divergence, resulting in an\nequivalent yet tractable loss that effectively trains one-step diffusion models\nby minimizing the expanded f-divergence family. The novel unification\nintroduced by Uni-Instruct not only offers new theoretical contributions that\nhelp understand existing approaches from a high-level perspective but also\nleads to state-of-the-art one-step diffusion generation performances. On the\nCIFAR10 generation benchmark, Uni-Instruct achieves record-breaking Frechet\nInception Distance (FID) values of \\emph{1.46} for unconditional\ngeneration and \\emph{1.38} for conditional generation. On the\nImageNet-64times 64 generation benchmark, Uni-Instruct achieves a new SoTA\none-step generation FID of \\emph{1.02}, which outperforms its 79-step\nteacher diffusion with a significant improvement margin of 1.33 (1.02 vs 2.35).\nWe also apply Uni-Instruct on broader tasks like text-to-3D generation. For\ntext-to-3D generation, Uni-Instruct gives decent results, which slightly\noutperforms previous methods, such as SDS and VSD, in terms of both generation\nquality and diversity. Both the solid theoretical and empirical contributions\nof Uni-Instruct will potentially help future studies on one-step diffusion\ndistillation and knowledge transferring of diffusion models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6838043c11b8b14a21f6ecd8/y_4g5XJdwhTEjDjAMTQaS.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20755.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6838043c11b8b14a21f6ecd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kzuNcUqFpRbM4rlHEfbrx.png",
            "fullname": "Yifei Wang",
            "name": "smallAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23253",
            "authors": [
                {
                    "_id": "6839325ddbf608133c740556",
                    "name": "Yixun Liang",
                    "hidden": false
                },
                {
                    "_id": "6839325ddbf608133c740557",
                    "name": "Kunming Luo",
                    "hidden": false
                },
                {
                    "_id": "6839325ddbf608133c740558",
                    "name": "Xiao Chen",
                    "hidden": false
                },
                {
                    "_id": "6839325ddbf608133c740559",
                    "name": "Rui Chen",
                    "hidden": false
                },
                {
                    "_id": "6839325ddbf608133c74055a",
                    "name": "Hongyu Yan",
                    "hidden": false
                },
                {
                    "_id": "6839325ddbf608133c74055b",
                    "name": "Weiyu Li",
                    "hidden": false
                },
                {
                    "_id": "6839325ddbf608133c74055c",
                    "name": "Jiarui Liu",
                    "hidden": false
                },
                {
                    "_id": "6839325ddbf608133c74055d",
                    "name": "Ping Tan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647d8f65becb41a272907e7a/xUWTQ8gs16XOP8nrS-KNI.jpeg"
            ],
            "publishedAt": "2025-05-29T08:58:41.000Z",
            "submittedOnDailyAt": "2025-05-30T03:02:50.614Z",
            "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
            "submittedOnDailyBy": {
                "_id": "647d8f65becb41a272907e7a",
                "avatarUrl": "/avatars/f891a1986a9361b23b7c83c23031cf26.svg",
                "isPro": false,
                "fullname": "yixunliang",
                "user": "lyxun",
                "type": "user"
            },
            "summary": "We present UniTEX, a novel two-stage 3D texture generation framework to\ncreate high-quality, consistent textures for 3D assets. Existing approaches\npredominantly rely on UV-based inpainting to refine textures after reprojecting\nthe generated multi-view images onto the 3D shapes, which introduces challenges\nrelated to topological ambiguity. To address this, we propose to bypass the\nlimitations of UV mapping by operating directly in a unified 3D functional\nspace. Specifically, we first propose that lifts texture generation into 3D\nspace via Texture Functions (TFs)--a continuous, volumetric representation that\nmaps any 3D point to a texture value based solely on surface proximity,\nindependent of mesh topology. Then, we propose to predict these TFs directly\nfrom images and geometry inputs using a transformer-based Large Texturing Model\n(LTM). To further enhance texture quality and leverage powerful 2D priors, we\ndevelop an advanced LoRA-based strategy for efficiently adapting large-scale\nDiffusion Transformers (DiTs) for high-quality multi-view texture synthesis as\nour first stage. Extensive experiments demonstrate that UniTEX achieves\nsuperior visual quality and texture integrity compared to existing approaches,\noffering a generalizable and scalable solution for automated 3D texture\ngeneration. Code will available in: https://github.com/YixunLiang/UniTEX.",
            "upvotes": 4,
            "discussionId": "6839325fdbf608133c7405dc",
            "githubRepo": "https://github.com/YixunLiang/UniTEX",
            "ai_summary": "UniTEX generates high-quality, consistent 3D textures by using Texture Functions and adapting Diffusion Transformers directly from images and geometry without UV mapping.",
            "ai_keywords": [
                "Texture Functions",
                "transformer-based Large Texturing Model",
                "LoRA",
                "Diffusion Transformers"
            ]
        },
        "publishedAt": "2025-05-29T04:58:41.000Z",
        "title": "UniTEX: Universal High Fidelity Generative Texturing for 3D Shapes",
        "summary": "We present UniTEX, a novel two-stage 3D texture generation framework to\ncreate high-quality, consistent textures for 3D assets. Existing approaches\npredominantly rely on UV-based inpainting to refine textures after reprojecting\nthe generated multi-view images onto the 3D shapes, which introduces challenges\nrelated to topological ambiguity. To address this, we propose to bypass the\nlimitations of UV mapping by operating directly in a unified 3D functional\nspace. Specifically, we first propose that lifts texture generation into 3D\nspace via Texture Functions (TFs)--a continuous, volumetric representation that\nmaps any 3D point to a texture value based solely on surface proximity,\nindependent of mesh topology. Then, we propose to predict these TFs directly\nfrom images and geometry inputs using a transformer-based Large Texturing Model\n(LTM). To further enhance texture quality and leverage powerful 2D priors, we\ndevelop an advanced LoRA-based strategy for efficiently adapting large-scale\nDiffusion Transformers (DiTs) for high-quality multi-view texture synthesis as\nour first stage. Extensive experiments demonstrate that UniTEX achieves\nsuperior visual quality and texture integrity compared to existing approaches,\noffering a generalizable and scalable solution for automated 3D texture\ngeneration. Code will available in: https://github.com/YixunLiang/UniTEX.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647d8f65becb41a272907e7a/xUWTQ8gs16XOP8nrS-KNI.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23253.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647d8f65becb41a272907e7a",
            "avatarUrl": "/avatars/f891a1986a9361b23b7c83c23031cf26.svg",
            "fullname": "yixunliang",
            "name": "lyxun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22918",
            "authors": [
                {
                    "_id": "68393b9ef0458c8bcb652d78",
                    "user": {
                        "_id": "6814e98abbe5b8a5d92fc335",
                        "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
                        "isPro": false,
                        "fullname": "Ruichen Chen",
                        "user": "crc5577",
                        "type": "user"
                    },
                    "name": "Ruichen Chen",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-30T17:29:37.663Z",
                    "hidden": true
                },
                {
                    "_id": "68393b9ef0458c8bcb652d79",
                    "user": {
                        "_id": "661c391720b47b0daddfcc5a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/RwvDJRtlEEPch27xId9Cb.png",
                        "isPro": false,
                        "fullname": "Keith G. Mills",
                        "user": "kgmills",
                        "type": "user"
                    },
                    "name": "Keith G. Mills",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-30T06:54:23.854Z",
                    "hidden": false
                },
                {
                    "_id": "68393b9ef0458c8bcb652d7a",
                    "user": {
                        "_id": "64ac49ccb7d86b40fd60a8dd",
                        "avatarUrl": "/avatars/e9f5482cffdd1d5917523a496a3805f0.svg",
                        "isPro": false,
                        "fullname": "Liyao Jiang",
                        "user": "LiyaoJiang",
                        "type": "user"
                    },
                    "name": "Liyao Jiang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-30T05:01:59.348Z",
                    "hidden": false
                },
                {
                    "_id": "68393b9ef0458c8bcb652d7b",
                    "name": "Chao Gao",
                    "hidden": false
                },
                {
                    "_id": "68393b9ef0458c8bcb652d7c",
                    "name": "Di Niu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T22:39:12.000Z",
            "submittedOnDailyAt": "2025-05-30T06:23:07.403Z",
            "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape",
            "submittedOnDailyBy": {
                "_id": "6814e98abbe5b8a5d92fc335",
                "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
                "isPro": false,
                "fullname": "Ruichen Chen",
                "user": "crc5577",
                "type": "user"
            },
            "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\nhttps://github.com/cccrrrccc/Re-ttention{https://github.com/cccrrrccc/Re-ttention}",
            "upvotes": 4,
            "discussionId": "68393ba3f0458c8bcb652e60",
            "ai_summary": "Re-ttention uses temporal redundancy in diffusion models to enable high sparse attention in visual generation, maintaining quality with minimal computational overhead.",
            "ai_keywords": [
                "Diffusion Transformers",
                "sparse attention",
                "visual generation models",
                "probabilistic normalization shift",
                "Re-ttention",
                "attention scores",
                "softmax distribution",
                "latency reduction",
                "H100 GPU"
            ]
        },
        "publishedAt": "2025-05-28T18:39:12.000Z",
        "title": "Re-ttention: Ultra Sparse Visual Generation via Attention Statistical\n  Reshape",
        "summary": "Diffusion Transformers (DiT) have become the de-facto model for generating\nhigh-quality visual content like videos and images. A huge bottleneck is the\nattention mechanism where complexity scales quadratically with resolution and\nvideo length. One logical way to lessen this burden is sparse attention, where\nonly a subset of tokens or patches are included in the calculation. However,\nexisting techniques fail to preserve visual quality at extremely high sparsity\nlevels and might even incur non-negligible compute overheads. % To address this\nconcern, we propose Re-ttention, which implements very high sparse attention\nfor visual generation models by leveraging the temporal redundancy of Diffusion\nModels to overcome the probabilistic normalization shift within the attention\nmechanism. Specifically, Re-ttention reshapes attention scores based on the\nprior softmax distribution history in order to preserve the visual quality of\nthe full quadratic attention at very high sparsity levels. % Experimental\nresults on T2V/T2I models such as CogVideoX and the PixArt DiTs demonstrate\nthat Re-ttention requires as few as 3.1\\% of the tokens during inference,\noutperforming contemporary methods like FastDiTAttn, Sparse VideoGen and\nMInference. Further, we measure latency to show that our method can attain over\n45\\% end-to-end % and over 92\\% self-attention latency reduction on an H100 GPU\nat negligible overhead cost.\n  Code available online here:\nhttps://github.com/cccrrrccc/Re-ttention{https://github.com/cccrrrccc/Re-ttention}",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22918.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6814e98abbe5b8a5d92fc335",
            "avatarUrl": "/avatars/83c1963280bc94fdc7a2b21f6e0f1c59.svg",
            "fullname": "Ruichen Chen",
            "name": "crc5577",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21114",
            "authors": [
                {
                    "_id": "68366f4410fa22cd420ae295",
                    "user": {
                        "_id": "66615c855fd9d736e670e0a9",
                        "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
                        "isPro": false,
                        "fullname": "wangshuai",
                        "user": "wangsssssss",
                        "type": "user"
                    },
                    "name": "Shuai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:58:12.608Z",
                    "hidden": false
                },
                {
                    "_id": "68366f4410fa22cd420ae296",
                    "name": "Zexian Li",
                    "hidden": false
                },
                {
                    "_id": "68366f4410fa22cd420ae297",
                    "name": "Qipeng zhang",
                    "hidden": false
                },
                {
                    "_id": "68366f4410fa22cd420ae298",
                    "user": {
                        "_id": "649e7693a83143427691769c",
                        "avatarUrl": "/avatars/d04f7b3d417423abaa053375212da21f.svg",
                        "isPro": false,
                        "fullname": "Tianhui Song",
                        "user": "sthuihui",
                        "type": "user"
                    },
                    "name": "Tianhui Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:58:10.255Z",
                    "hidden": false
                },
                {
                    "_id": "68366f4410fa22cd420ae299",
                    "name": "Xubin Li",
                    "hidden": false
                },
                {
                    "_id": "68366f4410fa22cd420ae29a",
                    "name": "Tiezheng Ge",
                    "hidden": false
                },
                {
                    "_id": "68366f4410fa22cd420ae29b",
                    "name": "Bo Zheng",
                    "hidden": false
                },
                {
                    "_id": "68366f4410fa22cd420ae29c",
                    "name": "Limin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T12:33:43.000Z",
            "submittedOnDailyAt": "2025-05-30T05:34:57.709Z",
            "title": "Differentiable Solver Search for Fast Diffusion Sampling",
            "submittedOnDailyBy": {
                "_id": "66615c855fd9d736e670e0a9",
                "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
                "isPro": false,
                "fullname": "wangshuai",
                "user": "wangsssssss",
                "type": "user"
            },
            "summary": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes.",
            "upvotes": 4,
            "discussionId": "68366f4a10fa22cd420ae43f",
            "githubRepo": "https://github.com/MCG-NJU/NeuralSolver",
            "ai_summary": "Researchers propose a novel differentiable solver search algorithm that optimizes the computational efficiency and quality of diffusion models for image generation tasks.",
            "ai_keywords": [
                "diffusion models",
                "ODE-based solvers",
                "Adams-like multistep methods",
                "t-related Lagrange interpolation",
                "differentiable solver search algorithm",
                "rectified-flow models",
                "SiT-XL/2",
                "FlowDCN-XL/2",
                "DDPM",
                "DiT-XL/2",
                "FID scores",
                "ImageNet256",
                "computational efficiency",
                "generality"
            ]
        },
        "publishedAt": "2025-05-27T08:33:43.000Z",
        "title": "Differentiable Solver Search for Fast Diffusion Sampling",
        "summary": "Diffusion models have demonstrated remarkable generation quality but at the\ncost of numerous function evaluations. Recently, advanced ODE-based solvers\nhave been developed to mitigate the substantial computational demands of\nreverse-diffusion solving under limited sampling steps. However, these solvers,\nheavily inspired by Adams-like multistep methods, rely solely on t-related\nLagrange interpolation. We show that t-related Lagrange interpolation is\nsuboptimal for diffusion model and reveal a compact search space comprised of\ntime steps and solver coefficients. Building on our analysis, we propose a\nnovel differentiable solver search algorithm to identify more optimal solver.\nEquipped with the searched solver, rectified-flow models, e.g., SiT-XL/2 and\nFlowDCN-XL/2, achieve FID scores of 2.40 and 2.35, respectively, on ImageNet256\nwith only 10 steps. Meanwhile, DDPM model, DiT-XL/2, reaches a FID score of\n2.33 with only 10 steps. Notably, our searched solver outperforms traditional\nsolvers by a significant margin. Moreover, our searched solver demonstrates\ngenerality across various model architectures, resolutions, and model sizes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21114.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "fullname": "wangshuai",
            "name": "wangsssssss",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18962",
            "authors": [
                {
                    "_id": "6839ca4456bcc85d9fc05477",
                    "name": "Xiaoqiang Wang",
                    "hidden": false
                },
                {
                    "_id": "6839ca4456bcc85d9fc05478",
                    "name": "Suyuchen Wang",
                    "hidden": false
                },
                {
                    "_id": "6839ca4456bcc85d9fc05479",
                    "name": "Yun Zhu",
                    "hidden": false
                },
                {
                    "_id": "6839ca4456bcc85d9fc0547a",
                    "name": "Bang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T03:35:49.000Z",
            "submittedOnDailyAt": "2025-05-30T19:09:34.467Z",
            "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with\n  Dynamic Shortcuts",
            "submittedOnDailyBy": {
                "_id": "654a97282d2fcd6bf2851173",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
                "isPro": false,
                "fullname": "Bang Liu",
                "user": "Bang-UdeM-Mila",
                "type": "user"
            },
            "summary": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move\nbeyond fast System-1 responses and engage in deliberative System-2 reasoning.\nHowever, this comes at the cost of significant inefficiency due to verbose\nintermediate output. Recent latent-space reasoning methods improve efficiency\nby operating on hidden states without decoding into language, yet they treat\nall steps uniformly, failing to distinguish critical deductions from auxiliary\nsteps and resulting in suboptimal use of computational resources. In this\npaper, we propose System-1.5 Reasoning, an adaptive reasoning framework that\ndynamically allocates computation across reasoning steps through shortcut paths\nin latent space. Specifically, System-1.5 Reasoning introduces two types of\ndynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the\nvertical depth by early exiting non-critical tokens through lightweight adapter\nbranches, while allowing critical tokens to continue through deeper Transformer\nlayers. The step shortcut (SS) reuses hidden states across the decoding steps\nto skip trivial steps and reason horizontally in latent space. Training\nSystem-1.5 Reasoning involves a two-stage self-distillation process: first\ndistilling natural language CoT into latent-space continuous thought, and then\ndistilling full-path System-2 latent reasoning into adaptive shortcut paths\n(System-1.5 Reasoning). Experiments on reasoning tasks demonstrate the superior\nperformance of our method. For example, on GSM8K, System-1.5 Reasoning achieves\nreasoning performance comparable to traditional CoT fine-tuning methods while\naccelerating inference by over 20x and reducing token generation by 92.31% on\naverage.",
            "upvotes": 4,
            "discussionId": "6839ca4556bcc85d9fc054d5",
            "ai_summary": "System-1.5 Reasoning improves the efficiency and performance of large language models by dynamically allocating computation through adaptive shortcuts in latent space, leading to faster inference and reduced token generation.",
            "ai_keywords": [
                "Chain-of-thought (CoT) reasoning",
                "System-1 reasoning",
                "System-2 reasoning",
                "latent-space reasoning",
                "hidden states",
                "adaptive reasoning framework",
                "model depth shortcut (DS)",
                "step shortcut (SS)",
                "lightweight adapter branches",
                "Transformer layers",
                "self-distillation process",
                "continuous thought",
                "adaptive shortcut paths",
                "GSM8K",
                "CoT fine-tuning methods"
            ]
        },
        "publishedAt": "2025-05-24T23:35:49.000Z",
        "title": "System-1.5 Reasoning: Traversal in Language and Latent Spaces with\n  Dynamic Shortcuts",
        "summary": "Chain-of-thought (CoT) reasoning enables large language models (LLMs) to move\nbeyond fast System-1 responses and engage in deliberative System-2 reasoning.\nHowever, this comes at the cost of significant inefficiency due to verbose\nintermediate output. Recent latent-space reasoning methods improve efficiency\nby operating on hidden states without decoding into language, yet they treat\nall steps uniformly, failing to distinguish critical deductions from auxiliary\nsteps and resulting in suboptimal use of computational resources. In this\npaper, we propose System-1.5 Reasoning, an adaptive reasoning framework that\ndynamically allocates computation across reasoning steps through shortcut paths\nin latent space. Specifically, System-1.5 Reasoning introduces two types of\ndynamic shortcuts. The model depth shortcut (DS) adaptively reasons along the\nvertical depth by early exiting non-critical tokens through lightweight adapter\nbranches, while allowing critical tokens to continue through deeper Transformer\nlayers. The step shortcut (SS) reuses hidden states across the decoding steps\nto skip trivial steps and reason horizontally in latent space. Training\nSystem-1.5 Reasoning involves a two-stage self-distillation process: first\ndistilling natural language CoT into latent-space continuous thought, and then\ndistilling full-path System-2 latent reasoning into adaptive shortcut paths\n(System-1.5 Reasoning). Experiments on reasoning tasks demonstrate the superior\nperformance of our method. For example, on GSM8K, System-1.5 Reasoning achieves\nreasoning performance comparable to traditional CoT fine-tuning methods while\naccelerating inference by over 20x and reducing token generation by 92.31% on\naverage.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18962.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654a97282d2fcd6bf2851173",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a97282d2fcd6bf2851173/9zXf940gr4WNt4e-oOt4k.png",
            "fullname": "Bang Liu",
            "name": "Bang-UdeM-Mila",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23759",
            "authors": [
                {
                    "_id": "6839d69eb385aa5d7a37bc2b",
                    "name": "Heekyung Lee",
                    "hidden": false
                },
                {
                    "_id": "6839d69eb385aa5d7a37bc2c",
                    "name": "Jiaxin Ge",
                    "hidden": false
                },
                {
                    "_id": "6839d69eb385aa5d7a37bc2d",
                    "name": "Tsung-Han Wu",
                    "hidden": false
                },
                {
                    "_id": "6839d69eb385aa5d7a37bc2e",
                    "name": "Minwoo Kang",
                    "hidden": false
                },
                {
                    "_id": "6839d69eb385aa5d7a37bc2f",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "6839d69eb385aa5d7a37bc30",
                    "name": "David M. Chan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:59:47.000Z",
            "submittedOnDailyAt": "2025-05-30T14:32:49.955Z",
            "title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint",
            "submittedOnDailyBy": {
                "_id": "6388f68c43d8b0797a09ff84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
                "isPro": false,
                "fullname": "David Chan",
                "user": "davidchan",
                "type": "user"
            },
            "summary": "Rebus puzzles, visual riddles that encode language through imagery, spatial\narrangement, and symbolic substitution, pose a unique challenge to current\nvision-language models (VLMs). Unlike traditional image captioning or question\nanswering tasks, rebus solving requires multi-modal abstraction, symbolic\nreasoning, and a grasp of cultural, phonetic and linguistic puns. In this\npaper, we investigate the capacity of contemporary VLMs to interpret and solve\nrebus puzzles by constructing a hand-generated and annotated benchmark of\ndiverse English-language rebus puzzles, ranging from simple pictographic\nsubstitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how\ndifferent VLMs perform, and our findings reveal that while VLMs exhibit some\nsurprising capabilities in decoding simple visual clues, they struggle\nsignificantly with tasks requiring abstract reasoning, lateral thinking, and\nunderstanding visual metaphors.",
            "upvotes": 3,
            "discussionId": "6839d6a0b385aa5d7a37bcab",
            "githubRepo": "https://github.com/Kyunnilee/visual_puzzles",
            "ai_summary": "Vision-language models struggle with rebus puzzles, which require abstract reasoning and understanding of visual metaphors, despite performing well on simple visual cues.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "multi-modal abstraction",
                "symbolic reasoning",
                "cultural puns",
                "phonetic puns",
                "linguistic puns",
                "benchmark",
                "pictographic substitutions",
                "spatially-dependent cues",
                "visual metaphors",
                "abstract reasoning",
                "lateral thinking"
            ]
        },
        "publishedAt": "2025-05-29T13:59:47.000Z",
        "title": "Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint",
        "summary": "Rebus puzzles, visual riddles that encode language through imagery, spatial\narrangement, and symbolic substitution, pose a unique challenge to current\nvision-language models (VLMs). Unlike traditional image captioning or question\nanswering tasks, rebus solving requires multi-modal abstraction, symbolic\nreasoning, and a grasp of cultural, phonetic and linguistic puns. In this\npaper, we investigate the capacity of contemporary VLMs to interpret and solve\nrebus puzzles by constructing a hand-generated and annotated benchmark of\ndiverse English-language rebus puzzles, ranging from simple pictographic\nsubstitutions to spatially-dependent cues (\"head\" over \"heels\"). We analyze how\ndifferent VLMs perform, and our findings reveal that while VLMs exhibit some\nsurprising capabilities in decoding simple visual clues, they struggle\nsignificantly with tasks requiring abstract reasoning, lateral thinking, and\nunderstanding visual metaphors.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23759.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6388f68c43d8b0797a09ff84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669920369989-noauth.jpeg",
            "fullname": "David Chan",
            "name": "davidchan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23625",
            "authors": [
                {
                    "_id": "683924d1ac00da416df936dd",
                    "name": "Chao Huang",
                    "hidden": false
                },
                {
                    "_id": "683924d1ac00da416df936de",
                    "name": "Yuesheng Ma",
                    "hidden": false
                },
                {
                    "_id": "683924d1ac00da416df936df",
                    "name": "Junxuan Huang",
                    "hidden": false
                },
                {
                    "_id": "683924d1ac00da416df936e0",
                    "name": "Susan Liang",
                    "hidden": false
                },
                {
                    "_id": "683924d1ac00da416df936e1",
                    "name": "Yunlong Tang",
                    "hidden": false
                },
                {
                    "_id": "683924d1ac00da416df936e2",
                    "name": "Jing Bi",
                    "hidden": false
                },
                {
                    "_id": "683924d1ac00da416df936e3",
                    "name": "Wenqiang Liu",
                    "hidden": false
                },
                {
                    "_id": "683924d1ac00da416df936e4",
                    "name": "Nima Mesgarani",
                    "hidden": false
                },
                {
                    "_id": "683924d1ac00da416df936e5",
                    "name": "Chenliang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T16:31:45.000Z",
            "submittedOnDailyAt": "2025-05-30T01:54:58.337Z",
            "title": "ZeroSep: Separate Anything in Audio with Zero Training",
            "submittedOnDailyBy": {
                "_id": "67257ee0938e718957c9c100",
                "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
                "isPro": false,
                "fullname": "Chao Huang",
                "user": "ChaoHuangCS",
                "type": "user"
            },
            "summary": "Audio source separation is fundamental for machines to understand complex\nacoustic environments and underpins numerous audio applications. Current\nsupervised deep learning approaches, while powerful, are limited by the need\nfor extensive, task-specific labeled data and struggle to generalize to the\nimmense variability and open-set nature of real-world acoustic scenes. Inspired\nby the success of generative foundation models, we investigate whether\npre-trained text-guided audio diffusion models can overcome these limitations.\nWe make a surprising discovery: zero-shot source separation can be achieved\npurely through a pre-trained text-guided audio diffusion model under the right\nconfiguration. Our method, named ZeroSep, works by inverting the mixed audio\ninto the diffusion model's latent space and then using text conditioning to\nguide the denoising process to recover individual sources. Without any\ntask-specific training or fine-tuning, ZeroSep repurposes the generative\ndiffusion model for a discriminative separation task and inherently supports\nopen-set scenarios through its rich textual priors. ZeroSep is compatible with\na variety of pre-trained text-guided audio diffusion backbones and delivers\nstrong separation performance on multiple separation benchmarks, surpassing\neven supervised methods.",
            "upvotes": 3,
            "discussionId": "683924d6ac00da416df937f6",
            "ai_summary": "ZeroSep, a text-guided audio diffusion model, achieves zero-shot source separation through pre-trained models and text conditioning, outperforming supervised methods on various benchmarks.",
            "ai_keywords": [
                "audio source separation",
                "supervised deep learning",
                "generative foundation models",
                "text-guided audio diffusion models",
                "zero-shot source separation",
                "latent space",
                "denoising process",
                "discriminative separation task",
                "textual priors",
                "open-set scenarios"
            ]
        },
        "publishedAt": "2025-05-29T12:31:45.000Z",
        "title": "ZeroSep: Separate Anything in Audio with Zero Training",
        "summary": "Audio source separation is fundamental for machines to understand complex\nacoustic environments and underpins numerous audio applications. Current\nsupervised deep learning approaches, while powerful, are limited by the need\nfor extensive, task-specific labeled data and struggle to generalize to the\nimmense variability and open-set nature of real-world acoustic scenes. Inspired\nby the success of generative foundation models, we investigate whether\npre-trained text-guided audio diffusion models can overcome these limitations.\nWe make a surprising discovery: zero-shot source separation can be achieved\npurely through a pre-trained text-guided audio diffusion model under the right\nconfiguration. Our method, named ZeroSep, works by inverting the mixed audio\ninto the diffusion model's latent space and then using text conditioning to\nguide the denoising process to recover individual sources. Without any\ntask-specific training or fine-tuning, ZeroSep repurposes the generative\ndiffusion model for a discriminative separation task and inherently supports\nopen-set scenarios through its rich textual priors. ZeroSep is compatible with\na variety of pre-trained text-guided audio diffusion backbones and delivers\nstrong separation performance on multiple separation benchmarks, surpassing\neven supervised methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23625.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67257ee0938e718957c9c100",
            "avatarUrl": "/avatars/db1f792ee1a5d860ea4e98c11a016a1b.svg",
            "fullname": "Chao Huang",
            "name": "ChaoHuangCS",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20282",
            "authors": [
                {
                    "_id": "683889f78c8e3b72170f6412",
                    "user": {
                        "_id": "641ddac5be3bd3a5a06ed4a4",
                        "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
                        "isPro": false,
                        "fullname": "zitian gao",
                        "user": "zgao3186",
                        "type": "user"
                    },
                    "name": "Zitian Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:54:47.238Z",
                    "hidden": false
                },
                {
                    "_id": "683889f78c8e3b72170f6413",
                    "name": "Lynx Chen",
                    "hidden": false
                },
                {
                    "_id": "683889f78c8e3b72170f6414",
                    "name": "Joey Zhou",
                    "hidden": false
                },
                {
                    "_id": "683889f78c8e3b72170f6415",
                    "name": "Bryan Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:58:30.000Z",
            "submittedOnDailyAt": "2025-05-30T01:51:16.066Z",
            "title": "One-shot Entropy Minimization",
            "submittedOnDailyBy": {
                "_id": "641ddac5be3bd3a5a06ed4a4",
                "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
                "isPro": false,
                "fullname": "zitian gao",
                "user": "zgao3186",
                "type": "user"
            },
            "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em.",
            "upvotes": 3,
            "discussionId": "683889f78c8e3b72170f643d",
            "projectPage": "https://www.notion.so/One-shot-Entropy-Minimization-202606db813b80639773f850f39246a5",
            "githubRepo": "https://github.com/zitian-gao/one-shot-em",
            "ai_summary": "Entropy minimization with one sample and minimal optimization achieves significant performance improvements for large language models.",
            "ai_keywords": [
                "large language models",
                "entropy minimization",
                "unlabeled data",
                "optimization",
                "rule-based reinforcement learning",
                "post-training paradigms"
            ]
        },
        "publishedAt": "2025-05-26T13:58:30.000Z",
        "title": "One-shot Entropy Minimization",
        "summary": "We trained 13,440 large language models and found that entropy minimization\nrequires only a single unlabeled data and 10 steps optimization to achieve\nperformance improvements comparable to or even greater than those obtained\nusing thousands of data and carefully designed rewards in rule-based\nreinforcement learning. This striking result may prompt a rethinking of\npost-training paradigms for large language models. Our code is avaliable at\nhttps://github.com/zitian-gao/one-shot-em.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20282.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641ddac5be3bd3a5a06ed4a4",
            "avatarUrl": "/avatars/14969dff861d53b0a75305606495eca7.svg",
            "fullname": "zitian gao",
            "name": "zgao3186",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19716",
            "authors": [
                {
                    "_id": "6839ac8c17471fea48833b1c",
                    "name": "Yifan Wu",
                    "hidden": false
                },
                {
                    "_id": "6839ac8c17471fea48833b1d",
                    "name": "Jingze Shi",
                    "hidden": false
                },
                {
                    "_id": "6839ac8c17471fea48833b1e",
                    "name": "Bingheng Wu",
                    "hidden": false
                },
                {
                    "_id": "6839ac8c17471fea48833b1f",
                    "name": "Jiayi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6839ac8c17471fea48833b20",
                    "name": "Xiaotian Lin",
                    "hidden": false
                },
                {
                    "_id": "6839ac8c17471fea48833b21",
                    "name": "Nan Tang",
                    "hidden": false
                },
                {
                    "_id": "6839ac8c17471fea48833b22",
                    "name": "Yuyu Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T09:04:44.000Z",
            "submittedOnDailyAt": "2025-05-30T11:34:16.161Z",
            "title": "Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with\n  Difficulty-Aware Prompting",
            "submittedOnDailyBy": {
                "_id": "673ab3647afcea17eb4378fd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png",
                "isPro": false,
                "fullname": "Loser Cheems",
                "user": "JingzeShi",
                "type": "user"
            },
            "summary": "Existing chain-of-thought (CoT) distillation methods can effectively transfer\nreasoning abilities to base models but suffer from two major limitations:\nexcessive verbosity of reasoning traces and inadequate adaptability to problem\ndifficulty. Long reasoning traces significantly increase inference costs, and\nuniform-length solutions prevent base models from learning adaptive reasoning\nstrategies. To address these issues, we propose a difficulty-aware prompting\n(DAP) method to dynamically shorten reasoning traces without performance loss.\nIn our approach, a large teacher model first judges each problem's difficulty\nand then rewrites its reasoning traces to an appropriate shorter length,\nyielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we\ncurate a distilled dataset called LiteCoT consisting of 100K concise reasoning\nexamples, with solutions averaging only 720 tokens (an order of magnitude\nshorter than typical CoTs). Using LiteCoT, we distilled a new family of\nreasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5\narchitecture. Experiments show that a student model fine-tuned on just 100K of\nthese difficulty-pruned CoT samples outperforms a model distilled on 800K\noriginal Long CoT samples, while significantly reducing training and inference\ncosts. Our method also generalizes well: across 11 diverse benchmarks, the\nshorter difficulty-aware CoTs achieve equal or better accuracy than Long\nchains, using far fewer tokens. For example, on the challenging AIME24 exam,\nour approach reaches 74.2% Pass@1 using only about 5K inference tokens,\nsurpassing other methods that consume many more tokens. Our code and data are\navailable at https://github.com/Evanwu1125/LiteCoT.",
            "upvotes": 3,
            "discussionId": "6839ac8d17471fea48833b5e",
            "ai_summary": "The difficulty-aware prompting method shortens reasoning traces in a dataset, improving model performance and efficiency across various benchmarks.",
            "ai_keywords": [
                "chain-of-thought distillation",
                "reasoning traces",
                "inference costs",
                "adaptive reasoning strategies",
                "difficulty-aware prompting",
                "teacher model",
                "distilled dataset",
                "LiteCoT",
                "reasoning models",
                "Liter",
                "Qwen2.5 architecture",
                "fine-tuned",
                "student model",
                "training costs",
                "accuracy",
                "AIME24 exam",
                "Pass@1"
            ]
        },
        "publishedAt": "2025-05-26T05:04:44.000Z",
        "title": "Concise Reasoning, Big Gains: Pruning Long Reasoning Trace with\n  Difficulty-Aware Prompting",
        "summary": "Existing chain-of-thought (CoT) distillation methods can effectively transfer\nreasoning abilities to base models but suffer from two major limitations:\nexcessive verbosity of reasoning traces and inadequate adaptability to problem\ndifficulty. Long reasoning traces significantly increase inference costs, and\nuniform-length solutions prevent base models from learning adaptive reasoning\nstrategies. To address these issues, we propose a difficulty-aware prompting\n(DAP) method to dynamically shorten reasoning traces without performance loss.\nIn our approach, a large teacher model first judges each problem's difficulty\nand then rewrites its reasoning traces to an appropriate shorter length,\nyielding concise yet complete reasoning traces. Leveraging the DAP pipeline, we\ncurate a distilled dataset called LiteCoT consisting of 100K concise reasoning\nexamples, with solutions averaging only 720 tokens (an order of magnitude\nshorter than typical CoTs). Using LiteCoT, we distilled a new family of\nreasoning models called Liter (1.5B, 7B, and 32B) based on the Qwen2.5\narchitecture. Experiments show that a student model fine-tuned on just 100K of\nthese difficulty-pruned CoT samples outperforms a model distilled on 800K\noriginal Long CoT samples, while significantly reducing training and inference\ncosts. Our method also generalizes well: across 11 diverse benchmarks, the\nshorter difficulty-aware CoTs achieve equal or better accuracy than Long\nchains, using far fewer tokens. For example, on the challenging AIME24 exam,\nour approach reaches 74.2% Pass@1 using only about 5K inference tokens,\nsurpassing other methods that consume many more tokens. Our code and data are\navailable at https://github.com/Evanwu1125/LiteCoT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19716.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "673ab3647afcea17eb4378fd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png",
            "fullname": "Loser Cheems",
            "name": "JingzeShi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 35
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23761",
            "authors": [
                {
                    "_id": "6839107ed762b7c617b0731b",
                    "user": {
                        "_id": "67578bf874cf42cdedbf00df",
                        "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
                        "isPro": false,
                        "fullname": "Yunjae Won",
                        "user": "yunjae-won",
                        "type": "user"
                    },
                    "name": "Yunjae Won",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:54:04.071Z",
                    "hidden": false
                },
                {
                    "_id": "6839107ed762b7c617b0731c",
                    "name": "Hyunji Lee",
                    "hidden": false
                },
                {
                    "_id": "6839107ed762b7c617b0731d",
                    "name": "Hyeonbin Hwang",
                    "hidden": false
                },
                {
                    "_id": "6839107ed762b7c617b0731e",
                    "name": "Minjoon Seo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:59:50.000Z",
            "submittedOnDailyAt": "2025-05-30T05:38:24.883Z",
            "title": "Differential Information: An Information-Theoretic Perspective on\n  Preference Optimization",
            "submittedOnDailyBy": {
                "_id": "67578bf874cf42cdedbf00df",
                "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
                "isPro": false,
                "fullname": "Yunjae Won",
                "user": "yunjae-won",
                "type": "user"
            },
            "summary": "Direct Preference Optimization (DPO) has become a standard technique for\naligning language models with human preferences in a supervised manner. Despite\nits empirical success, the theoretical justification behind its log-ratio\nreward parameterization remains incomplete. In this work, we address this gap\nby utilizing the Differential Information Distribution (DID): a distribution\nover token sequences that captures the information gained during policy\nupdates. First, we show that when preference labels encode the differential\ninformation required to transform a reference policy into a target policy, the\nlog-ratio reward in DPO emerges as the uniquely optimal form for learning the\ntarget policy via preference optimization. This result naturally yields a\nclosed-form expression for the optimal sampling distribution over rejected\nresponses. Second, we find that the condition for preferences to encode\ndifferential information is fundamentally linked to an implicit assumption\nregarding log-margin ordered policies-an inductive bias widely used in\npreference optimization yet previously unrecognized. Finally, by analyzing the\nentropy of the DID, we characterize how learning low-entropy differential\ninformation reinforces the policy distribution, while high-entropy differential\ninformation induces a smoothing effect, which explains the log-likelihood\ndisplacement phenomenon. We validate our theoretical findings in synthetic\nexperiments and extend them to real-world instruction-following datasets. Our\nresults suggest that learning high-entropy differential information is crucial\nfor general instruction-following, while learning low-entropy differential\ninformation benefits knowledge-intensive question answering. Overall, our work\npresents a unifying perspective on the DPO objective, the structure of\npreference data, and resulting policy behaviors through the lens of\ndifferential information.",
            "upvotes": 2,
            "discussionId": "6839107fd762b7c617b07385",
            "ai_summary": "Theoretical analysis of Direct Preference Optimization (DPO) reveals that log-ratio reward parameterization is optimal for learning target policy via preference optimization, linked to log-margin ordered policies, and explains policy reinforcement and smoothing based on differential information entropy.",
            "ai_keywords": [
                "Direct Preference Optimization",
                "DPO",
                "log-ratio reward parameterization",
                "Differential Information Distribution",
                "DID",
                "token sequences",
                "policy updates",
                "preference labels",
                "target policy",
                "preference optimization",
                "log-margin ordered policies",
                "entropy",
                "differential information entropy",
                "log-likelihood displacement",
                "instruction-following datasets",
                "knowledge-intensive question answering"
            ]
        },
        "publishedAt": "2025-05-29T13:59:50.000Z",
        "title": "Differential Information: An Information-Theoretic Perspective on\n  Preference Optimization",
        "summary": "Direct Preference Optimization (DPO) has become a standard technique for\naligning language models with human preferences in a supervised manner. Despite\nits empirical success, the theoretical justification behind its log-ratio\nreward parameterization remains incomplete. In this work, we address this gap\nby utilizing the Differential Information Distribution (DID): a distribution\nover token sequences that captures the information gained during policy\nupdates. First, we show that when preference labels encode the differential\ninformation required to transform a reference policy into a target policy, the\nlog-ratio reward in DPO emerges as the uniquely optimal form for learning the\ntarget policy via preference optimization. This result naturally yields a\nclosed-form expression for the optimal sampling distribution over rejected\nresponses. Second, we find that the condition for preferences to encode\ndifferential information is fundamentally linked to an implicit assumption\nregarding log-margin ordered policies-an inductive bias widely used in\npreference optimization yet previously unrecognized. Finally, by analyzing the\nentropy of the DID, we characterize how learning low-entropy differential\ninformation reinforces the policy distribution, while high-entropy differential\ninformation induces a smoothing effect, which explains the log-likelihood\ndisplacement phenomenon. We validate our theoretical findings in synthetic\nexperiments and extend them to real-world instruction-following datasets. Our\nresults suggest that learning high-entropy differential information is crucial\nfor general instruction-following, while learning low-entropy differential\ninformation benefits knowledge-intensive question answering. Overall, our work\npresents a unifying perspective on the DPO objective, the structure of\npreference data, and resulting policy behaviors through the lens of\ndifferential information.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23761.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67578bf874cf42cdedbf00df",
            "avatarUrl": "/avatars/5981300ee0025c876973f94c81a85b19.svg",
            "fullname": "Yunjae Won",
            "name": "yunjae-won",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22944",
            "authors": [
                {
                    "_id": "6839f54680cc67601374b80c",
                    "name": "Angtian Wang",
                    "hidden": false
                },
                {
                    "_id": "6839f54680cc67601374b80d",
                    "name": "Haibin Huang",
                    "hidden": false
                },
                {
                    "_id": "6839f54680cc67601374b80e",
                    "name": "Jacob Zhiyuan Fang",
                    "hidden": false
                },
                {
                    "_id": "6839f54680cc67601374b80f",
                    "name": "Yiding Yang",
                    "hidden": false
                },
                {
                    "_id": "6839f54680cc67601374b810",
                    "name": "Chongyang Ma",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66a800a2c1454e2221e77473/n80Q1w9kaMEJlub_KPja2.mp4"
            ],
            "publishedAt": "2025-05-28T23:49:18.000Z",
            "submittedOnDailyAt": "2025-05-30T16:47:35.879Z",
            "title": "ATI: Any Trajectory Instruction for Controllable Video Generation",
            "submittedOnDailyBy": {
                "_id": "66a800a2c1454e2221e77473",
                "avatarUrl": "/avatars/03cd7a4294f82168b4ca0541aadcb67b.svg",
                "isPro": false,
                "fullname": "Angtian Wang",
                "user": "angtian",
                "type": "user"
            },
            "summary": "We propose a unified framework for motion control in video generation that\nseamlessly integrates camera movement, object-level translation, and\nfine-grained local motion using trajectory-based inputs. In contrast to prior\nmethods that address these motion types through separate modules or\ntask-specific designs, our approach offers a cohesive solution by projecting\nuser-defined trajectories into the latent space of pre-trained image-to-video\ngeneration models via a lightweight motion injector. Users can specify\nkeypoints and their motion paths to control localized deformations, entire\nobject motion, virtual camera dynamics, or combinations of these. The injected\ntrajectory signals guide the generative process to produce temporally\nconsistent and semantically aligned motion sequences. Our framework\ndemonstrates superior performance across multiple video motion control tasks,\nincluding stylized motion effects (e.g., motion brushes), dynamic viewpoint\nchanges, and precise local motion manipulation. Experiments show that our\nmethod provides significantly better controllability and visual quality\ncompared to prior approaches and commercial solutions, while remaining broadly\ncompatible with various state-of-the-art video generation backbones. Project\npage: https://anytraj.github.io/.",
            "upvotes": 2,
            "discussionId": "6839f54880cc67601374b920",
            "ai_summary": "A unified framework for video motion control integrates camera movement, object translation, and local motion via trajectory-based inputs, improving controllability and visual quality.",
            "ai_keywords": [
                "trajectory-based inputs",
                "latent space",
                "lightweight motion injector",
                "keypoints",
                "motion paths",
                "generative process",
                "temporally consistent",
                "semantically aligned",
                "motion sequences",
                "video motion control",
                "stylized motion effects",
                "dynamic viewpoint changes",
                "precise local motion manipulation"
            ]
        },
        "publishedAt": "2025-05-28T19:49:18.000Z",
        "title": "ATI: Any Trajectory Instruction for Controllable Video Generation",
        "summary": "We propose a unified framework for motion control in video generation that\nseamlessly integrates camera movement, object-level translation, and\nfine-grained local motion using trajectory-based inputs. In contrast to prior\nmethods that address these motion types through separate modules or\ntask-specific designs, our approach offers a cohesive solution by projecting\nuser-defined trajectories into the latent space of pre-trained image-to-video\ngeneration models via a lightweight motion injector. Users can specify\nkeypoints and their motion paths to control localized deformations, entire\nobject motion, virtual camera dynamics, or combinations of these. The injected\ntrajectory signals guide the generative process to produce temporally\nconsistent and semantically aligned motion sequences. Our framework\ndemonstrates superior performance across multiple video motion control tasks,\nincluding stylized motion effects (e.g., motion brushes), dynamic viewpoint\nchanges, and precise local motion manipulation. Experiments show that our\nmethod provides significantly better controllability and visual quality\ncompared to prior approaches and commercial solutions, while remaining broadly\ncompatible with various state-of-the-art video generation backbones. Project\npage: https://anytraj.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66a800a2c1454e2221e77473/n80Q1w9kaMEJlub_KPja2.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22944.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66a800a2c1454e2221e77473",
            "avatarUrl": "/avatars/03cd7a4294f82168b4ca0541aadcb67b.svg",
            "fullname": "Angtian Wang",
            "name": "angtian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22854",
            "authors": [
                {
                    "_id": "6839877749c61862e2ca1ca6",
                    "name": "Kornel Howil",
                    "hidden": false
                },
                {
                    "_id": "6839877749c61862e2ca1ca7",
                    "name": "Joanna Waczyńska",
                    "hidden": false
                },
                {
                    "_id": "6839877749c61862e2ca1ca8",
                    "name": "Piotr Borycki",
                    "hidden": false
                },
                {
                    "_id": "6839877749c61862e2ca1ca9",
                    "name": "Tadeusz Dziarmaga",
                    "hidden": false
                },
                {
                    "_id": "6839877749c61862e2ca1caa",
                    "name": "Marcin Mazur",
                    "hidden": false
                },
                {
                    "_id": "6839877749c61862e2ca1cab",
                    "name": "Przemysław Spurek",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65fc24b804769daf21a98fa6/rWmNqYRUuSUOVmLdPdlZ2.mp4"
            ],
            "publishedAt": "2025-05-28T20:41:24.000Z",
            "submittedOnDailyAt": "2025-05-30T08:58:00.492Z",
            "title": "CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian\n  Splatting",
            "submittedOnDailyBy": {
                "_id": "65fc24b804769daf21a98fa6",
                "avatarUrl": "/avatars/d7faf536940809823d56a442f55e9fb9.svg",
                "isPro": false,
                "fullname": "Kornel",
                "user": "kornelhowil",
                "type": "user"
            },
            "summary": "Gaussian Splatting (GS) has recently emerged as an efficient representation\nfor rendering 3D scenes from 2D images and has been extended to images, videos,\nand dynamic 4D content. However, applying style transfer to GS-based\nrepresentations, especially beyond simple color changes, remains challenging.\nIn this work, we introduce CLIPGaussians, the first unified style transfer\nframework that supports text- and image-guided stylization across multiple\nmodalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates\ndirectly on Gaussian primitives and integrates into existing GS pipelines as a\nplug-in module, without requiring large generative models or retraining from\nscratch. CLIPGaussians approach enables joint optimization of color and\ngeometry in 3D and 4D settings, and achieves temporal coherence in videos,\nwhile preserving a model size. We demonstrate superior style fidelity and\nconsistency across all tasks, validating CLIPGaussians as a universal and\nefficient solution for multimodal style transfer.",
            "upvotes": 2,
            "discussionId": "6839877d49c61862e2ca1e3a",
            "projectPage": "https://kornelhowil.github.io/CLIPGaussian/",
            "githubRepo": "https://github.com/kornelhowil/CLIPGaussian",
            "ai_summary": "CLIPGaussians is a style transfer framework that supports text- and image-guided stylization of 2D images, videos, 3D objects, and 4D scenes by optimizing color and geometry directly on Gaussian primitives.",
            "ai_keywords": [
                "Gaussian Splatting",
                "CLIPGaussians",
                "style transfer",
                "Gaussian primitives",
                "3D scenes",
                "4D scenes",
                "joint optimization",
                "color and geometry",
                "temporal coherence"
            ]
        },
        "publishedAt": "2025-05-28T16:41:24.000Z",
        "title": "CLIPGaussian: Universal and Multimodal Style Transfer Based on Gaussian\n  Splatting",
        "summary": "Gaussian Splatting (GS) has recently emerged as an efficient representation\nfor rendering 3D scenes from 2D images and has been extended to images, videos,\nand dynamic 4D content. However, applying style transfer to GS-based\nrepresentations, especially beyond simple color changes, remains challenging.\nIn this work, we introduce CLIPGaussians, the first unified style transfer\nframework that supports text- and image-guided stylization across multiple\nmodalities: 2D images, videos, 3D objects, and 4D scenes. Our method operates\ndirectly on Gaussian primitives and integrates into existing GS pipelines as a\nplug-in module, without requiring large generative models or retraining from\nscratch. CLIPGaussians approach enables joint optimization of color and\ngeometry in 3D and 4D settings, and achieves temporal coherence in videos,\nwhile preserving a model size. We demonstrate superior style fidelity and\nconsistency across all tasks, validating CLIPGaussians as a universal and\nefficient solution for multimodal style transfer.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65fc24b804769daf21a98fa6/rWmNqYRUuSUOVmLdPdlZ2.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22854.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fc24b804769daf21a98fa6",
            "avatarUrl": "/avatars/d7faf536940809823d56a442f55e9fb9.svg",
            "fullname": "Kornel",
            "name": "kornelhowil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22126",
            "authors": [
                {
                    "_id": "683944793e5dd928f04e8431",
                    "name": "Yifan Chang",
                    "hidden": false
                },
                {
                    "_id": "683944793e5dd928f04e8432",
                    "name": "Yukang Feng",
                    "hidden": false
                },
                {
                    "_id": "683944793e5dd928f04e8433",
                    "name": "Jianwen Sun",
                    "hidden": false
                },
                {
                    "_id": "683944793e5dd928f04e8434",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "683944793e5dd928f04e8435",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "683944793e5dd928f04e8436",
                    "name": "S. Kevin Zhou",
                    "hidden": false
                },
                {
                    "_id": "683944793e5dd928f04e8437",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T08:51:01.000Z",
            "submittedOnDailyAt": "2025-05-30T04:18:26.192Z",
            "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of\n  Image Generation Model",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities.",
            "upvotes": 2,
            "discussionId": "6839447c3e5dd928f04e84c1",
            "ai_summary": "The introduction of SridBench, a benchmark for scientific figure generation, reveals that current top-tier models, such as GPT-4o-image, fall short in semantic and structural accuracy compared to human performance, underscoring the need for more advanced multimodal reasoning-driven visual generation.",
            "ai_keywords": [
                "diffusion models",
                "multimodal models",
                "GPT-4o-image",
                "semantic understanding",
                "structural composition",
                "scientific illustration generation",
                "SridBench",
                "semantic fidelity",
                "structural accuracy"
            ]
        },
        "publishedAt": "2025-05-28T04:51:01.000Z",
        "title": "SridBench: Benchmark of Scientific Research Illustration Drawing of\n  Image Generation Model",
        "summary": "Recent years have seen rapid advances in AI-driven image generation. Early\ndiffusion models emphasized perceptual quality, while newer multimodal models\nlike GPT-4o-image integrate high-level reasoning, improving semantic\nunderstanding and structural composition. Scientific illustration generation\nexemplifies this evolution: unlike general image synthesis, it demands accurate\ninterpretation of technical content and transformation of abstract ideas into\nclear, standardized visuals. This task is significantly more\nknowledge-intensive and laborious, often requiring hours of manual work and\nspecialized tools. Automating it in a controllable, intelligent manner would\nprovide substantial practical value. Yet, no benchmark currently exists to\nevaluate AI on this front. To fill this gap, we introduce SridBench, the first\nbenchmark for scientific figure generation. It comprises 1,120 instances\ncurated from leading scientific papers across 13 natural and computer science\ndisciplines, collected via human experts and MLLMs. Each sample is evaluated\nalong six dimensions, including semantic fidelity and structural accuracy.\nExperimental results reveal that even top-tier models like GPT-4o-image lag\nbehind human performance, with common issues in text/visual clarity and\nscientific correctness. These findings highlight the need for more advanced\nreasoning-driven visual generation capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22126.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.19360",
            "authors": [
                {
                    "_id": "68392279896eb9ceb71fac39",
                    "name": "Manan Suri",
                    "hidden": false
                },
                {
                    "_id": "68392279896eb9ceb71fac3a",
                    "user": {
                        "_id": "65c16444d4c3b8dff2f0d78d",
                        "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
                        "isPro": false,
                        "fullname": "Puneet Mathur",
                        "user": "puneetm",
                        "type": "user"
                    },
                    "name": "Puneet Mathur",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-30T03:14:02.414Z",
                    "hidden": false
                },
                {
                    "_id": "68392279896eb9ceb71fac3b",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "68392279896eb9ceb71fac3c",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:28.558Z",
                    "hidden": false
                },
                {
                    "_id": "68392279896eb9ceb71fac3d",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "68392279896eb9ceb71fac3e",
                    "name": "Dinesh Manocha",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T23:17:32.000Z",
            "submittedOnDailyAt": "2025-05-30T01:44:05.816Z",
            "title": "ChartLens: Fine-grained Visual Attribution in Charts",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%.",
            "upvotes": 2,
            "discussionId": "6839227a896eb9ceb71fac99",
            "ai_summary": "ChartLens enhances multimodal language models with fine-grained visual attributions, improving the accuracy of chart understanding by 26-66%.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "chart understanding",
                "hallucinations",
                "Post-Hoc Visual Attribution",
                "ChartLens",
                "segmentation-based techniques",
                "set-of-marks prompting",
                "ChartVA-Eval",
                "synthetic charts",
                "real-world charts",
                "fine-grained attribution annotations"
            ]
        },
        "publishedAt": "2025-05-25T19:17:32.000Z",
        "title": "ChartLens: Fine-grained Visual Attribution in Charts",
        "summary": "The growing capabilities of multimodal large language models (MLLMs) have\nadvanced tasks like chart understanding. However, these models often suffer\nfrom hallucinations, where generated text sequences conflict with the provided\nvisual data. To address this, we introduce Post-Hoc Visual Attribution for\nCharts, which identifies fine-grained chart elements that validate a given\nchart-associated response. We propose ChartLens, a novel chart attribution\nalgorithm that uses segmentation-based techniques to identify chart objects and\nemploys set-of-marks prompting with MLLMs for fine-grained visual attribution.\nAdditionally, we present ChartVA-Eval, a benchmark with synthetic and\nreal-world charts from diverse domains like finance, policy, and economics,\nfeaturing fine-grained attribution annotations. Our evaluations show that\nChartLens improves fine-grained attributions by 26-66%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19360.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19286",
            "authors": [
                {
                    "_id": "6839220cf85de1fc56402b3d",
                    "name": "Utkarsh Sahu",
                    "hidden": false
                },
                {
                    "_id": "6839220cf85de1fc56402b3e",
                    "name": "Zhisheng Qi",
                    "hidden": false
                },
                {
                    "_id": "6839220cf85de1fc56402b3f",
                    "name": "Yongjia Lei",
                    "hidden": false
                },
                {
                    "_id": "6839220cf85de1fc56402b40",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "6839220cf85de1fc56402b41",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:31.804Z",
                    "hidden": false
                },
                {
                    "_id": "6839220cf85de1fc56402b42",
                    "name": "Nesreen K. Ahmed",
                    "hidden": false
                },
                {
                    "_id": "6839220cf85de1fc56402b43",
                    "user": {
                        "_id": "637c6d95a8716d642050b50f",
                        "avatarUrl": "/avatars/0955a10113807348f24db968c7bd7c7a.svg",
                        "isPro": false,
                        "fullname": "Mahantesh Halappanavar",
                        "user": "mhalappa",
                        "type": "user"
                    },
                    "name": "Mahantesh M Halappanavar",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-30T05:25:34.615Z",
                    "hidden": false
                },
                {
                    "_id": "6839220cf85de1fc56402b44",
                    "name": "Yao Ma",
                    "hidden": false
                },
                {
                    "_id": "6839220cf85de1fc56402b45",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T19:34:15.000Z",
            "submittedOnDailyAt": "2025-05-30T01:42:17.692Z",
            "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance.",
            "upvotes": 2,
            "discussionId": "6839220ef85de1fc56402ba7",
            "ai_summary": "The study explores the structural patterns of knowledge in large language models from a graph perspective, uncovering knowledge homophily and developing models for graph machine learning to estimate entity knowledge.",
            "ai_keywords": [
                "large language models",
                "graph perspective",
                "triplet",
                "entity",
                "node degree",
                "knowledge homophily",
                "graph machine learning",
                "knowledge checking",
                "fine-tuning"
            ]
        },
        "publishedAt": "2025-05-25T15:34:15.000Z",
        "title": "A Graph Perspective to Probe Structural Patterns of Knowledge in Large\n  Language Models",
        "summary": "Large language models have been extensively studied as neural knowledge bases\nfor their knowledge access, editability, reasoning, and explainability.\nHowever, few works focus on the structural patterns of their knowledge.\nMotivated by this gap, we investigate these structural patterns from a graph\nperspective. We quantify the knowledge of LLMs at both the triplet and entity\nlevels, and analyze how it relates to graph structural properties such as node\ndegree. Furthermore, we uncover the knowledge homophily, where topologically\nclose entities exhibit similar levels of knowledgeability, which further\nmotivates us to develop graph machine learning models to estimate entity\nknowledge based on its local neighbors. This model further enables valuable\nknowledge checking by selecting triplets less known to LLMs. Empirical results\nshow that using selected triplets for fine-tuning leads to superior\nperformance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19286.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.19236",
            "authors": [
                {
                    "_id": "68392fc4b6280677f75e6194",
                    "user": {
                        "_id": "5fbdf878485ef14d9a960f4d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
                        "isPro": false,
                        "fullname": "Qian Cao",
                        "user": "Aman",
                        "type": "user"
                    },
                    "name": "Qian Cao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:24.101Z",
                    "hidden": false
                },
                {
                    "_id": "68392fc4b6280677f75e6195",
                    "name": "Xiting Wang",
                    "hidden": false
                },
                {
                    "_id": "68392fc4b6280677f75e6196",
                    "name": "Yuzhuo Yuan",
                    "hidden": false
                },
                {
                    "_id": "68392fc4b6280677f75e6197",
                    "name": "Yahui Liu",
                    "hidden": false
                },
                {
                    "_id": "68392fc4b6280677f75e6198",
                    "name": "Fang Luo",
                    "hidden": false
                },
                {
                    "_id": "68392fc4b6280677f75e6199",
                    "name": "Ruihua Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-25T17:25:23.000Z",
            "submittedOnDailyAt": "2025-05-30T02:42:27.745Z",
            "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large\n  Language Model Evaluator",
            "submittedOnDailyBy": {
                "_id": "5fbdf878485ef14d9a960f4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
                "isPro": false,
                "fullname": "Qian Cao",
                "user": "Aman",
                "type": "user"
            },
            "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research.",
            "upvotes": 2,
            "discussionId": "68392fc5b6280677f75e61e6",
            "projectPage": "https://creval-creative-evaluation.github.io/",
            "ai_summary": "A novel pairwise-comparison framework using CreataSet dataset trains CrEval, an LLM-based evaluator that significantly improves the assessment of textual creativity aligned with human judgments.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "automated methods",
                "heuristic-based approaches",
                "prompting-based approaches",
                "pairwise-comparison framework",
                "shared contextual instructions",
                "CreataSet",
                "synthetic creative instruction-response pairs",
                "open-domain tasks",
                "human-level instructions",
                "CrEval",
                "human judgments",
                "robust evaluators",
                "creativity enhancement"
            ]
        },
        "publishedAt": "2025-05-25T13:25:23.000Z",
        "title": "Evaluating Text Creativity across Diverse Domains: A Dataset and Large\n  Language Model Evaluator",
        "summary": "Creativity evaluation remains a challenging frontier for large language\nmodels (LLMs). Current evaluations heavily rely on inefficient and costly human\njudgments, hindering progress in enhancing machine creativity. While automated\nmethods exist, ranging from psychological testing to heuristic- or\nprompting-based approaches, they often lack generalizability or alignment with\nhuman judgment. To address these issues, in this paper, we propose a novel\npairwise-comparison framework for assessing textual creativity, leveraging\nshared contextual instructions to improve evaluation consistency. We introduce\nCreataSet, a large-scale dataset with 100K+ human-level and 1M+ synthetic\ncreative instruction-response pairs spanning diverse open-domain tasks. Through\ntraining on CreataSet, we develop an LLM-based evaluator named CrEval. CrEval\ndemonstrates remarkable superiority over existing methods in alignment with\nhuman judgments. Experimental results underscore the indispensable significance\nof integrating both human-generated and synthetic data in training highly\nrobust evaluators, and showcase the practical utility of CrEval in boosting the\ncreativity of LLMs. We will release all data, code, and models publicly soon to\nsupport further research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19236.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5fbdf878485ef14d9a960f4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634724388998-5fbdf878485ef14d9a960f4d.jpeg",
            "fullname": "Qian Cao",
            "name": "Aman",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23764",
            "authors": [
                {
                    "_id": "6839f2b800f90dff0cff5f6c",
                    "name": "Sihan Yang",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f6d",
                    "name": "Runsen Xu",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f6e",
                    "name": "Yiman Xie",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f6f",
                    "name": "Sizhe Yang",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f70",
                    "name": "Mo Li",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f71",
                    "name": "Jingli Lin",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f72",
                    "name": "Chenming Zhu",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f73",
                    "name": "Xiaochen Chen",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f74",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f75",
                    "name": "Xiangyu Yue",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f76",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f77",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "6839f2b800f90dff0cff5f78",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:59:52.000Z",
            "submittedOnDailyAt": "2025-05-30T16:32:38.333Z",
            "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence",
            "submittedOnDailyBy": {
                "_id": "6458b1103b81018d6b93defb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6458b1103b81018d6b93defb/JFr5j-3UO0xxlEdUksikU.jpeg",
                "isPro": false,
                "fullname": "Runsen Xu",
                "user": "RunsenXu",
                "type": "user"
            },
            "summary": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .",
            "upvotes": 1,
            "discussionId": "6839f2bc00f90dff0cff602a",
            "ai_summary": "MMSI-Bench, a VQA benchmark for multi-image spatial intelligence, reveals significant gaps in multimodal large language models' performance compared to human accuracy.",
            "ai_keywords": [
                "VQA",
                "MMSI-Bench",
                "3D-vision",
                "multimodal large language models",
                "multiple-choice questions",
                "distractors",
                "reasoning processes",
                "grounding errors",
                "overlap-matching",
                "scene-reconstruction errors",
                "situation-transformation reasoning errors",
                "spatial-logic errors"
            ]
        },
        "publishedAt": "2025-05-29T13:59:52.000Z",
        "title": "MMSI-Bench: A Benchmark for Multi-Image Spatial Intelligence",
        "summary": "Spatial intelligence is essential for multimodal large language models\n(MLLMs) operating in the complex physical world. Existing benchmarks, however,\nprobe only single-image relations and thus fail to assess the multi-image\nspatial reasoning that real-world deployments demand. We introduce MMSI-Bench,\na VQA benchmark dedicated to multi-image spatial intelligence. Six 3D-vision\nresearchers spent more than 300 hours meticulously crafting 1,000 challenging,\nunambiguous multiple-choice questions from over 120,000 images, each paired\nwith carefully designed distractors and a step-by-step reasoning process. We\nconduct extensive experiments and thoroughly evaluate 34 open-source and\nproprietary MLLMs, observing a wide gap: the strongest open-source model\nattains roughly 30% accuracy and OpenAI's o3 reasoning model reaches 40%, while\nhumans score 97%. These results underscore the challenging nature of MMSI-Bench\nand the substantial headroom for future research. Leveraging the annotated\nreasoning processes, we also provide an automated error analysis pipeline that\ndiagnoses four dominant failure modes, including (1) grounding errors, (2)\noverlap-matching and scene-reconstruction errors, (3) situation-transformation\nreasoning errors, and (4) spatial-logic errors, offering valuable insights for\nadvancing multi-image spatial intelligence. Project page:\nhttps://runsenxu.com/projects/MMSI_Bench .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23764.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6458b1103b81018d6b93defb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6458b1103b81018d6b93defb/JFr5j-3UO0xxlEdUksikU.jpeg",
            "fullname": "Runsen Xu",
            "name": "RunsenXu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23734",
            "authors": [
                {
                    "_id": "683919ddf85de1fc563e0d1c",
                    "user": {
                        "_id": "66699aa8a33847217b5a49c7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                        "isPro": false,
                        "fullname": "Weijie Wang",
                        "user": "lhmd",
                        "type": "user"
                    },
                    "name": "Weijie Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:53:41.133Z",
                    "hidden": false
                },
                {
                    "_id": "683919ddf85de1fc563e0d1d",
                    "name": "Donny Y. Chen",
                    "hidden": false
                },
                {
                    "_id": "683919ddf85de1fc563e0d1e",
                    "name": "Zeyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "683919ddf85de1fc563e0d1f",
                    "name": "Duochao Shi",
                    "hidden": false
                },
                {
                    "_id": "683919ddf85de1fc563e0d20",
                    "name": "Akide Liu",
                    "hidden": false
                },
                {
                    "_id": "683919ddf85de1fc563e0d21",
                    "name": "Bohan Zhuang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66699aa8a33847217b5a49c7/ssgMI03qr8Cfr600xApGu.mp4"
            ],
            "publishedAt": "2025-05-29T17:57:04.000Z",
            "submittedOnDailyAt": "2025-05-30T15:57:51.264Z",
            "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
            "submittedOnDailyBy": {
                "_id": "66699aa8a33847217b5a49c7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
                "isPro": false,
                "fullname": "Weijie Wang",
                "user": "lhmd",
                "type": "user"
            },
            "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their encoders, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state Z that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state Z. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor.",
            "upvotes": 1,
            "discussionId": "683919def85de1fc563e0d82",
            "projectPage": "https://lhmd.top/zpressor",
            "githubRepo": "https://github.com/ziplab/ZPressor",
            "ai_summary": "ZPressor, a lightweight module, compresses multi-view inputs for feed-forward 3D Gaussian Splatting models, enhancing their scalability and performance under dense view settings.",
            "ai_keywords": [
                "Gaussian Splatting",
                "Information Bottleneck",
                "cross attention",
                "latent state",
                "multi-view inputs",
                "feed-forward models",
                "novel view synthesis",
                "DL3DV-10K",
                "RealEstate10K"
            ]
        },
        "publishedAt": "2025-05-29T13:57:04.000Z",
        "title": "ZPressor: Bottleneck-Aware Compression for Scalable Feed-Forward 3DGS",
        "summary": "Feed-forward 3D Gaussian Splatting (3DGS) models have recently emerged as a\npromising solution for novel view synthesis, enabling one-pass inference\nwithout the need for per-scene 3DGS optimization. However, their scalability is\nfundamentally constrained by the limited capacity of their encoders, leading to\ndegraded performance or excessive memory consumption as the number of input\nviews increases. In this work, we analyze feed-forward 3DGS frameworks through\nthe lens of the Information Bottleneck principle and introduce ZPressor, a\nlightweight architecture-agnostic module that enables efficient compression of\nmulti-view inputs into a compact latent state Z that retains essential scene\ninformation while discarding redundancy. Concretely, ZPressor enables existing\nfeed-forward 3DGS models to scale to over 100 input views at 480P resolution on\nan 80GB GPU, by partitioning the views into anchor and support sets and using\ncross attention to compress the information from the support views into anchor\nviews, forming the compressed latent state Z. We show that integrating\nZPressor into several state-of-the-art feed-forward 3DGS models consistently\nimproves performance under moderate input views and enhances robustness under\ndense view settings on two large-scale benchmarks DL3DV-10K and RealEstate10K.\nThe video results, code and trained models are available on our project page:\nhttps://lhmd.top/zpressor.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66699aa8a33847217b5a49c7/ssgMI03qr8Cfr600xApGu.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23734.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66699aa8a33847217b5a49c7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/u8Z-6U8U7ARXOpdBDI7Qm.png",
            "fullname": "Weijie Wang",
            "name": "lhmd",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23678",
            "authors": [
                {
                    "_id": "68392b8739589608ee01aa55",
                    "name": "Gabriel Sarch",
                    "hidden": false
                },
                {
                    "_id": "68392b8739589608ee01aa56",
                    "name": "Snigdha Saha",
                    "hidden": false
                },
                {
                    "_id": "68392b8739589608ee01aa57",
                    "name": "Naitik Khandelwal",
                    "hidden": false
                },
                {
                    "_id": "68392b8739589608ee01aa58",
                    "name": "Ayush Jain",
                    "hidden": false
                },
                {
                    "_id": "68392b8739589608ee01aa59",
                    "name": "Michael J. Tarr",
                    "hidden": false
                },
                {
                    "_id": "68392b8739589608ee01aa5a",
                    "name": "Aviral Kumar",
                    "hidden": false
                },
                {
                    "_id": "68392b8739589608ee01aa5b",
                    "name": "Katerina Fragkiadaki",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:20:26.000Z",
            "submittedOnDailyAt": "2025-05-30T14:07:44.023Z",
            "title": "Grounded Reinforcement Learning for Visual Reasoning",
            "submittedOnDailyBy": {
                "_id": "6666828b15ec3fd28b52a1bd",
                "avatarUrl": "/avatars/a80eeac5dd23aabce1eb299ebe7f245b.svg",
                "isPro": false,
                "fullname": "Gabriel H Sarch",
                "user": "gsarch",
                "type": "user"
            },
            "summary": "While reinforcement learning (RL) over chains of thought has significantly\nadvanced language models in tasks such as mathematics and coding, visual\nreasoning introduces added complexity by requiring models to direct visual\nattention, interpret perceptual inputs, and ground abstract reasoning in\nspatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement\nLearning), a vision-language model trained with RL to explicitly anchor each\nreasoning step to specific visual coordinates. Inspired by human visual\ndecision-making, ViGoRL learns to produce spatially grounded reasoning traces,\nguiding visual attention to task-relevant regions at each step. When\nfine-grained exploration is required, our novel multi-turn RL framework enables\nthe model to dynamically zoom into predicted coordinates as reasoning unfolds.\nAcross a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK\nfor spatial reasoning, V*bench for visual search, and ScreenSpot and\nVisualWebArena for web-based grounding--ViGoRL consistently outperforms both\nsupervised fine-tuning and conventional RL baselines that lack explicit\ngrounding mechanisms. Incorporating multi-turn RL with zoomed-in visual\nfeedback significantly improves ViGoRL's performance on localizing small GUI\nelements and visual search, achieving 86.4% on V*Bench. Additionally, we find\nthat grounding amplifies other visual behaviors such as region exploration,\ngrounded subgoal setting, and visual verification. Finally, human evaluations\nshow that the model's visual references are not only spatially accurate but\nalso helpful for understanding model reasoning steps. Our results show that\nvisually grounded RL is a strong paradigm for imbuing models with\ngeneral-purpose visual reasoning.",
            "upvotes": 1,
            "discussionId": "68392b9239589608ee01ab72",
            "projectPage": "https://visually-grounded-rl.github.io/",
            "githubRepo": "https://github.com/Gabesarch/grounded-rl",
            "ai_summary": "ViGoRL, a vision-language model enhanced with visually grounded reinforcement learning, achieves superior performance across various visual reasoning tasks by dynamically focusing visual attention and grounding reasoning in spatial evidence.",
            "ai_keywords": [
                "ViGoRL",
                "reinforcement learning",
                "vision-language model",
                "spatially grounded reasoning traces",
                "multi-turn RL",
                "fine-grained exploration",
                "SAT-2",
                "BLINK",
                "V*bench",
                "ScreenSpot",
                "VisualWebArena",
                "grounded subgoal setting",
                "visual verification"
            ]
        },
        "publishedAt": "2025-05-29T13:20:26.000Z",
        "title": "Grounded Reinforcement Learning for Visual Reasoning",
        "summary": "While reinforcement learning (RL) over chains of thought has significantly\nadvanced language models in tasks such as mathematics and coding, visual\nreasoning introduces added complexity by requiring models to direct visual\nattention, interpret perceptual inputs, and ground abstract reasoning in\nspatial evidence. We introduce ViGoRL (Visually Grounded Reinforcement\nLearning), a vision-language model trained with RL to explicitly anchor each\nreasoning step to specific visual coordinates. Inspired by human visual\ndecision-making, ViGoRL learns to produce spatially grounded reasoning traces,\nguiding visual attention to task-relevant regions at each step. When\nfine-grained exploration is required, our novel multi-turn RL framework enables\nthe model to dynamically zoom into predicted coordinates as reasoning unfolds.\nAcross a diverse set of visual reasoning benchmarks--including SAT-2 and BLINK\nfor spatial reasoning, V*bench for visual search, and ScreenSpot and\nVisualWebArena for web-based grounding--ViGoRL consistently outperforms both\nsupervised fine-tuning and conventional RL baselines that lack explicit\ngrounding mechanisms. Incorporating multi-turn RL with zoomed-in visual\nfeedback significantly improves ViGoRL's performance on localizing small GUI\nelements and visual search, achieving 86.4% on V*Bench. Additionally, we find\nthat grounding amplifies other visual behaviors such as region exploration,\ngrounded subgoal setting, and visual verification. Finally, human evaluations\nshow that the model's visual references are not only spatially accurate but\nalso helpful for understanding model reasoning steps. Our results show that\nvisually grounded RL is a strong paradigm for imbuing models with\ngeneral-purpose visual reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23678.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6666828b15ec3fd28b52a1bd",
            "avatarUrl": "/avatars/a80eeac5dd23aabce1eb299ebe7f245b.svg",
            "fullname": "Gabriel H Sarch",
            "name": "gsarch",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23671",
            "authors": [
                {
                    "_id": "6839ee8e011f06eb0e6ec2c8",
                    "name": "Manish Shetty",
                    "hidden": false
                },
                {
                    "_id": "6839ee8e011f06eb0e6ec2c9",
                    "name": "Naman Jain",
                    "hidden": false
                },
                {
                    "_id": "6839ee8e011f06eb0e6ec2ca",
                    "name": "Jinjian Liu",
                    "hidden": false
                },
                {
                    "_id": "6839ee8e011f06eb0e6ec2cb",
                    "name": "Vijay Kethanaboyina",
                    "hidden": false
                },
                {
                    "_id": "6839ee8e011f06eb0e6ec2cc",
                    "name": "Koushik Sen",
                    "hidden": false
                },
                {
                    "_id": "6839ee8e011f06eb0e6ec2cd",
                    "name": "Ion Stoica",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:14:55.000Z",
            "submittedOnDailyAt": "2025-05-30T23:27:57.318Z",
            "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents",
            "submittedOnDailyBy": {
                "_id": "6411ed451e42164b9f0f8b24",
                "avatarUrl": "/avatars/a5c023f407894d3d1dba7b343b847380.svg",
                "isPro": false,
                "fullname": "Naman Jain",
                "user": "StringChaos",
                "type": "user"
            },
            "summary": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research.",
            "upvotes": 1,
            "discussionId": "6839ee8f011f06eb0e6ec301",
            "ai_summary": "A benchmark evaluates high-performance software development capabilities of language models, identifying significant challenges and failure modes.",
            "ai_keywords": [
                "GSO",
                "SWE-Agents",
                "performance tests",
                "repository commit histories",
                "optimization tasks",
                "runtime efficiency",
                "expert developer optimization",
                "low-level languages",
                "lazy optimization strategies",
                "bottleneck localization"
            ]
        },
        "publishedAt": "2025-05-29T13:14:55.000Z",
        "title": "GSO: Challenging Software Optimization Tasks for Evaluating SWE-Agents",
        "summary": "Developing high-performance software is a complex task that requires\nspecialized expertise. We introduce GSO, a benchmark for evaluating language\nmodels' capabilities in developing high-performance software. We develop an\nautomated pipeline that generates and executes performance tests to analyze\nrepository commit histories to identify 102 challenging optimization tasks\nacross 10 codebases, spanning diverse domains and programming languages. An\nagent is provided with a codebase and performance test as a precise\nspecification, and tasked to improve the runtime efficiency, which is measured\nagainst the expert developer optimization. Our quantitative evaluation reveals\nthat leading SWE-Agents struggle significantly, achieving less than 5% success\nrate, with limited improvements even with inference-time scaling. Our\nqualitative analysis identifies key failure modes, including difficulties with\nlow-level languages, practicing lazy optimization strategies, and challenges in\naccurately localizing bottlenecks. We release the code and artifacts of our\nbenchmark along with agent trajectories to enable future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23671.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6411ed451e42164b9f0f8b24",
            "avatarUrl": "/avatars/a5c023f407894d3d1dba7b343b847380.svg",
            "fullname": "Naman Jain",
            "name": "StringChaos",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23183",
            "authors": [
                {
                    "_id": "683962e4cba8ce4f5ebced9c",
                    "user": {
                        "_id": "5e7749883d77a72421292d07",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
                        "isPro": false,
                        "fullname": "Gabriele Sarti",
                        "user": "gsarti",
                        "type": "user"
                    },
                    "name": "Gabriele Sarti",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-05-30T07:54:16.127Z",
                    "hidden": false
                },
                {
                    "_id": "683962e4cba8ce4f5ebced9d",
                    "name": "Vilém Zouhar",
                    "hidden": false
                },
                {
                    "_id": "683962e4cba8ce4f5ebced9e",
                    "name": "Malvina Nissim",
                    "hidden": false
                },
                {
                    "_id": "683962e4cba8ce4f5ebced9f",
                    "name": "Arianna Bisazza",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/1j5bJLYubeP6s5cGuTNzs.png",
                "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/I2PXFiD8jN4odhmi5vrG6.png"
            ],
            "publishedAt": "2025-05-29T07:20:36.000Z",
            "submittedOnDailyAt": "2025-05-30T06:23:03.812Z",
            "title": "Unsupervised Word-level Quality Estimation for Machine Translation\n  Through the Lens of Annotators (Dis)agreement",
            "submittedOnDailyBy": {
                "_id": "5e7749883d77a72421292d07",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
                "isPro": false,
                "fullname": "Gabriele Sarti",
                "user": "gsarti",
                "type": "user"
            },
            "summary": "Word-level quality estimation (WQE) aims to automatically identify\nfine-grained error spans in machine-translated outputs and has found many uses,\nincluding assisting translators during post-editing. Modern WQE techniques are\noften expensive, involving prompting of large language models or ad-hoc\ntraining on large amounts of human-labeled data. In this work, we investigate\nefficient alternatives exploiting recent advances in language model\ninterpretability and uncertainty quantification to identify translation errors\nfrom the inner workings of translation models. In our evaluation spanning 14\nmetrics across 12 translation directions, we quantify the impact of human label\nvariation on metric performance by using multiple sets of human labels. Our\nresults highlight the untapped potential of unsupervised metrics, the\nshortcomings of supervised methods when faced with label uncertainty, and the\nbrittleness of single-annotator evaluation practices.",
            "upvotes": 1,
            "discussionId": "683962e6cba8ce4f5ebcedfc",
            "githubRepo": "https://github.com/gsarti/labl",
            "ai_summary": "Evaluation of word-level quality estimation techniques leverages model interpretability and uncertainty quantification to identify translation errors with a focus on the impact of label variation and the performance of supervised versus unsupervised metrics.",
            "ai_keywords": [
                "language model interpretability",
                "uncertainty quantification",
                "word-level quality estimation",
                "translation errors",
                "human label variation",
                "unsupervised metrics",
                "supervised methods",
                "single-annotator evaluation practices"
            ]
        },
        "publishedAt": "2025-05-29T03:20:36.000Z",
        "title": "Unsupervised Word-level Quality Estimation for Machine Translation\n  Through the Lens of Annotators (Dis)agreement",
        "summary": "Word-level quality estimation (WQE) aims to automatically identify\nfine-grained error spans in machine-translated outputs and has found many uses,\nincluding assisting translators during post-editing. Modern WQE techniques are\noften expensive, involving prompting of large language models or ad-hoc\ntraining on large amounts of human-labeled data. In this work, we investigate\nefficient alternatives exploiting recent advances in language model\ninterpretability and uncertainty quantification to identify translation errors\nfrom the inner workings of translation models. In our evaluation spanning 14\nmetrics across 12 translation directions, we quantify the impact of human label\nvariation on metric performance by using multiple sets of human labels. Our\nresults highlight the untapped potential of unsupervised metrics, the\nshortcomings of supervised methods when faced with label uncertainty, and the\nbrittleness of single-annotator evaluation practices.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/1j5bJLYubeP6s5cGuTNzs.png",
            "https://cdn-uploads.huggingface.co/production/uploads/5e7749883d77a72421292d07/I2PXFiD8jN4odhmi5vrG6.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23183.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e7749883d77a72421292d07",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5e7749883d77a72421292d07/M4AmBReZk_otxCIG3o0bL.jpeg",
            "fullname": "Gabriele Sarti",
            "name": "gsarti",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 225
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22888",
            "authors": [
                {
                    "_id": "683987754a3a71a917cc50ef",
                    "name": "Jirui Qi",
                    "hidden": false
                },
                {
                    "_id": "683987754a3a71a917cc50f0",
                    "name": "Shan Chen",
                    "hidden": false
                },
                {
                    "_id": "683987754a3a71a917cc50f1",
                    "name": "Zidi Xiong",
                    "hidden": false
                },
                {
                    "_id": "683987754a3a71a917cc50f2",
                    "name": "Raquel Fernández",
                    "hidden": false
                },
                {
                    "_id": "683987754a3a71a917cc50f3",
                    "name": "Danielle S. Bitterman",
                    "hidden": false
                },
                {
                    "_id": "683987754a3a71a917cc50f4",
                    "name": "Arianna Bisazza",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T21:44:12.000Z",
            "submittedOnDailyAt": "2025-05-30T08:57:39.694Z",
            "title": "When Models Reason in Your Language: Controlling Thinking Trace Language\n  Comes at the Cost of Accuracy",
            "submittedOnDailyBy": {
                "_id": "645a415077d040cfd64e036e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645a415077d040cfd64e036e/2k6QsstgWC1H4oRMlzcc5.png",
                "isPro": false,
                "fullname": "Jirui Qi",
                "user": "JRQi",
                "type": "user"
            },
            "summary": "Recent Large Reasoning Models (LRMs) with thinking traces have shown strong\nperformance on English reasoning tasks. However, their ability to think in\nother languages is less studied. This capability is as important as answer\naccuracy for real world applications because users may find the reasoning trace\nuseful for oversight only when it is expressed in their own language. We\ncomprehensively evaluate two leading families of LRMs on our XReasoning\nbenchmark and find that even the most advanced models often revert to English\nor produce fragmented reasoning in other languages, revealing a substantial gap\nin multilingual reasoning. Prompt based interventions that force models to\nreason in the users language improve readability and oversight but reduce\nanswer accuracy, exposing an important trade off. We further show that targeted\npost training on just 100 examples mitigates this mismatch, though some\naccuracy loss remains. Our results highlight the limited multilingual reasoning\ncapabilities of current LRMs and outline directions for future work. Code and\ndata are available at https://github.com/Betswish/mCoT-XReasoning.",
            "upvotes": 1,
            "discussionId": "683987764a3a71a917cc5122",
            "githubRepo": "https://github.com/Betswish/mCoT-XReasoning",
            "ai_summary": "Evaluation of Large Reasoning Models in multilingual reasoning shows limited capability, with interventions improving readability but reducing accuracy.",
            "ai_keywords": [
                "Large Reasoning Models",
                "XReasoning benchmark",
                "multilingual reasoning",
                "prompt-based interventions",
                "post-training"
            ]
        },
        "publishedAt": "2025-05-28T17:44:12.000Z",
        "title": "When Models Reason in Your Language: Controlling Thinking Trace Language\n  Comes at the Cost of Accuracy",
        "summary": "Recent Large Reasoning Models (LRMs) with thinking traces have shown strong\nperformance on English reasoning tasks. However, their ability to think in\nother languages is less studied. This capability is as important as answer\naccuracy for real world applications because users may find the reasoning trace\nuseful for oversight only when it is expressed in their own language. We\ncomprehensively evaluate two leading families of LRMs on our XReasoning\nbenchmark and find that even the most advanced models often revert to English\nor produce fragmented reasoning in other languages, revealing a substantial gap\nin multilingual reasoning. Prompt based interventions that force models to\nreason in the users language improve readability and oversight but reduce\nanswer accuracy, exposing an important trade off. We further show that targeted\npost training on just 100 examples mitigates this mismatch, though some\naccuracy loss remains. Our results highlight the limited multilingual reasoning\ncapabilities of current LRMs and outline directions for future work. Code and\ndata are available at https://github.com/Betswish/mCoT-XReasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22888.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645a415077d040cfd64e036e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645a415077d040cfd64e036e/2k6QsstgWC1H4oRMlzcc5.png",
            "fullname": "Jirui Qi",
            "name": "JRQi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21190",
            "authors": [
                {
                    "_id": "6838be453538dad22f9bc2a4",
                    "user": {
                        "_id": "63e50836c9b2b7e1ea3413fd",
                        "avatarUrl": "/avatars/6c43fbce3ac2564e7860ff33fe8c9e82.svg",
                        "isPro": false,
                        "fullname": "Jonghak Moon",
                        "user": "SuperSupermoon",
                        "type": "user"
                    },
                    "name": "Jong Hak Moon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-30T06:58:33.487Z",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2a5",
                    "name": "Geon Choi",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2a6",
                    "name": "Paloma Rabaey",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2a7",
                    "name": "Min Gwan Kim",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2a8",
                    "name": "Hyuk Gi Hong",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2a9",
                    "name": "Jung-Oh Lee",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2aa",
                    "name": "Hangyul Yoon",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2ab",
                    "name": "Eun Woo Doe",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2ac",
                    "name": "Jiyoun Kim",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2ad",
                    "name": "Harshita Sharma",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2ae",
                    "name": "Daniel C. Castro",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2af",
                    "name": "Javier Alvarez-Valle",
                    "hidden": false
                },
                {
                    "_id": "6838be453538dad22f9bc2b0",
                    "name": "Edward Choi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63e50836c9b2b7e1ea3413fd/EQUDD6-a3CUpGyEtTM04o.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63e50836c9b2b7e1ea3413fd/72LzF1yLNJ28AE7A6Nk1P.png"
            ],
            "publishedAt": "2025-05-27T13:40:00.000Z",
            "submittedOnDailyAt": "2025-05-30T13:28:16.056Z",
            "title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray\n  Interpretation",
            "submittedOnDailyBy": {
                "_id": "63e50836c9b2b7e1ea3413fd",
                "avatarUrl": "/avatars/6c43fbce3ac2564e7860ff33fe8c9e82.svg",
                "isPro": false,
                "fullname": "Jonghak Moon",
                "user": "SuperSupermoon",
                "type": "user"
            },
            "summary": "Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage",
            "upvotes": 1,
            "discussionId": "6838be473538dad22f9bc322",
            "ai_summary": "The paper introduces LUNGUAGE, a benchmark for structured radiology report generation, and LUNGUAGESCORE, an evaluation metric, enabling fine-grained structured report evaluation and longitudinal interpretation.",
            "ai_keywords": [
                "LUNGUAGE",
                "LUNGUAGESCORE",
                "structured radiology report generation",
                "two-stage framework",
                "schema-aligned structured representations",
                "evaluative metric",
                "entity",
                "relation",
                "attribute",
                "temporal consistency",
                "longitudinal interpretation"
            ]
        },
        "publishedAt": "2025-05-27T09:40:00.000Z",
        "title": "Lunguage: A Benchmark for Structured and Sequential Chest X-ray\n  Interpretation",
        "summary": "Radiology reports convey detailed clinical observations and capture\ndiagnostic reasoning that evolves over time. However, existing evaluation\nmethods are limited to single-report settings and rely on coarse metrics that\nfail to capture fine-grained clinical semantics and temporal dependencies. We\nintroduce LUNGUAGE,a benchmark dataset for structured radiology report\ngeneration that supports both single-report evaluation and longitudinal\npatient-level assessment across multiple studies. It contains 1,473 annotated\nchest X-ray reports, each reviewed by experts, and 80 of them contain\nlongitudinal annotations to capture disease progression and inter-study\nintervals, also reviewed by experts. Using this benchmark, we develop a\ntwo-stage framework that transforms generated reports into fine-grained,\nschema-aligned structured representations, enabling longitudinal\ninterpretation. We also propose LUNGUAGESCORE, an interpretable metric that\ncompares structured outputs at the entity, relation, and attribute level while\nmodeling temporal consistency across patient timelines. These contributions\nestablish the first benchmark dataset, structuring framework, and evaluation\nmetric for sequential radiology reporting, with empirical results demonstrating\nthat LUNGUAGESCORE effectively supports structured report evaluation. The code\nis available at: https://github.com/SuperSupermoon/Lunguage",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63e50836c9b2b7e1ea3413fd/EQUDD6-a3CUpGyEtTM04o.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63e50836c9b2b7e1ea3413fd/72LzF1yLNJ28AE7A6Nk1P.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21190.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e50836c9b2b7e1ea3413fd",
            "avatarUrl": "/avatars/6c43fbce3ac2564e7860ff33fe8c9e82.svg",
            "fullname": "Jonghak Moon",
            "name": "SuperSupermoon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.20199",
            "authors": [
                {
                    "_id": "683919611186f2cbf3ed2267",
                    "name": "Pengxiang Li",
                    "hidden": false
                },
                {
                    "_id": "683919611186f2cbf3ed2268",
                    "name": "Shilin Yan",
                    "hidden": false
                },
                {
                    "_id": "683919611186f2cbf3ed2269",
                    "name": "Joey Tsai",
                    "hidden": false
                },
                {
                    "_id": "683919611186f2cbf3ed226a",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "683919611186f2cbf3ed226b",
                    "name": "Ruichuan An",
                    "hidden": false
                },
                {
                    "_id": "683919611186f2cbf3ed226c",
                    "name": "Ziyu Guo",
                    "hidden": false
                },
                {
                    "_id": "683919611186f2cbf3ed226d",
                    "name": "Xiaowei Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T16:40:22.000Z",
            "submittedOnDailyAt": "2025-05-30T01:05:23.884Z",
            "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
            "submittedOnDailyBy": {
                "_id": "64245f2c089d5fae56b4549a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
                "isPro": false,
                "fullname": "Pengxiang Li",
                "user": "pengxiang",
                "type": "user"
            },
            "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.",
            "upvotes": 1,
            "discussionId": "683919611186f2cbf3ed2292",
            "ai_summary": "Adaptive Classifier-Free Guidance (A-CFG) dynamically adjusts the guidance in masked diffusion language models by focusing on areas of low model confidence, leading to significant improvements in language generation performance.",
            "ai_keywords": [
                "Classifier-Free Guidance (CFG)",
                "Adaptive Classifier-Free Guidance (A-CFG)",
                "masked diffusion language model",
                "predictive confidence",
                "GPQA"
            ]
        },
        "publishedAt": "2025-05-26T12:40:22.000Z",
        "title": "Adaptive Classifier-Free Guidance via Dynamic Low-Confidence Masking",
        "summary": "Classifier-Free Guidance (CFG) significantly enhances controllability in\ngenerative models by interpolating conditional and unconditional predictions.\nHowever, standard CFG often employs a static unconditional input, which can be\nsuboptimal for iterative generation processes where model uncertainty varies\ndynamically. We introduce Adaptive Classifier-Free Guidance (A-CFG), a novel\nmethod that tailors the unconditional input by leveraging the model's\ninstantaneous predictive confidence. At each step of an iterative (masked)\ndiffusion language model, A-CFG identifies tokens in the currently generated\nsequence for which the model exhibits low confidence. These tokens are\ntemporarily re-masked to create a dynamic, localized unconditional input. This\nfocuses CFG's corrective influence precisely on areas of ambiguity, leading to\nmore effective guidance. We integrate A-CFG into a state-of-the-art masked\ndiffusion language model and demonstrate its efficacy. Experiments on diverse\nlanguage generation benchmarks show that A-CFG yields substantial improvements\nover standard CFG, achieving, for instance, a 3.9 point gain on GPQA. Our work\nhighlights the benefit of dynamically adapting guidance mechanisms to model\nuncertainty in iterative generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20199.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "fullname": "Pengxiang Li",
            "name": "pengxiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18142",
            "authors": [
                {
                    "_id": "6839d7b3ede0c6c6e33e9363",
                    "name": "Junfeng Wu",
                    "hidden": false
                },
                {
                    "_id": "6839d7b3ede0c6c6e33e9364",
                    "name": "Dongliang Luo",
                    "hidden": false
                },
                {
                    "_id": "6839d7b3ede0c6c6e33e9365",
                    "name": "Weizhi Zhao",
                    "hidden": false
                },
                {
                    "_id": "6839d7b3ede0c6c6e33e9366",
                    "name": "Zhihao Xie",
                    "hidden": false
                },
                {
                    "_id": "6839d7b3ede0c6c6e33e9367",
                    "name": "Yuanhao Wang",
                    "hidden": false
                },
                {
                    "_id": "6839d7b3ede0c6c6e33e9368",
                    "name": "Junyi Li",
                    "hidden": false
                },
                {
                    "_id": "6839d7b3ede0c6c6e33e9369",
                    "name": "Xudong Xie",
                    "hidden": false
                },
                {
                    "_id": "6839d7b3ede0c6c6e33e936a",
                    "name": "Yuliang Liu",
                    "hidden": false
                },
                {
                    "_id": "6839d7b3ede0c6c6e33e936b",
                    "name": "Xiang Bai",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6569de56046899997ba5ac4e/MO1xgdbEZswzcywIaSOyD.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/6569de56046899997ba5ac4e/3QZC_QFmiD9o6ZH6c-Iaq.jpeg"
            ],
            "publishedAt": "2025-05-23T17:52:16.000Z",
            "submittedOnDailyAt": "2025-05-30T14:44:33.396Z",
            "title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation",
            "submittedOnDailyBy": {
                "_id": "6569de56046899997ba5ac4e",
                "avatarUrl": "/avatars/287ee4c36a9edf2540eed779fd7d41e2.svg",
                "isPro": true,
                "fullname": "Junfeng Wu",
                "user": "Junfeng5",
                "type": "user"
            },
            "summary": "In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nVisual tokenizers and VAEs have significantly advanced visual generation and\nmultimodal modeling by providing more efficient compressed or quantized image\nrepresentations. However, while helping production models reduce computational\nburdens, the information loss from image compression fundamentally limits the\nupper bound of visual generation quality. To evaluate this upper bound, we\nfocus on assessing reconstructed text and facial features since they typically:\n1) exist at smaller scales, 2) contain dense and rich textures, 3) are prone to\ncollapse, and 4) are highly sensitive to human vision. We first collect and\ncurate a diverse set of clear text and face images from existing datasets.\nUnlike approaches using VLM models, we employ established OCR and face\nrecognition models for evaluation, ensuring accuracy while maintaining an\nexceptionally lightweight assessment process <span style=\"font-weight: bold;\ncolor: rgb(214, 21, 21);\">requiring just 2GB memory and 4 minutes</span> to\ncomplete. Using our benchmark, we analyze text and face reconstruction quality\nacross various scales for different image tokenizers and VAEs. Our results show\nmodern visual tokenizers still struggle to preserve fine-grained features,\nespecially at smaller scales. We further extend this evaluation framework to\nvideo, conducting comprehensive analysis of video tokenizers. Additionally, we\ndemonstrate that traditional metrics fail to accurately reflect reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.",
            "upvotes": 1,
            "discussionId": "6839d7b7ede0c6c6e33e94a3",
            "projectPage": "https://wjf5203.github.io/TokBench/",
            "githubRepo": "https://github.com/wjf5203/TokBench",
            "ai_summary": "Evaluation of visual tokenizers and VAEs on fine-grained feature reconstruction, focusing on text and face, reveals limitations in preserving detailed visual content and highlights the need for specialized metrics.",
            "ai_keywords": [
                "visual tokenizers",
                "VAEs",
                "fine-grained features",
                "benchmarking",
                "text reconstruction",
                "facial feature reconstruction",
                "image compression",
                "information loss",
                "visual generation quality",
                "multimodal modeling",
                "OCR",
                "face recognition",
                "video tokenizers",
                "reconstruction metrics"
            ]
        },
        "publishedAt": "2025-05-23T13:52:16.000Z",
        "title": "TokBench: Evaluating Your Visual Tokenizer before Visual Generation",
        "summary": "In this work, we reveal the limitations of visual tokenizers and VAEs in\npreserving fine-grained features, and propose a benchmark to evaluate\nreconstruction performance for two challenging visual contents: text and face.\nVisual tokenizers and VAEs have significantly advanced visual generation and\nmultimodal modeling by providing more efficient compressed or quantized image\nrepresentations. However, while helping production models reduce computational\nburdens, the information loss from image compression fundamentally limits the\nupper bound of visual generation quality. To evaluate this upper bound, we\nfocus on assessing reconstructed text and facial features since they typically:\n1) exist at smaller scales, 2) contain dense and rich textures, 3) are prone to\ncollapse, and 4) are highly sensitive to human vision. We first collect and\ncurate a diverse set of clear text and face images from existing datasets.\nUnlike approaches using VLM models, we employ established OCR and face\nrecognition models for evaluation, ensuring accuracy while maintaining an\nexceptionally lightweight assessment process <span style=\"font-weight: bold;\ncolor: rgb(214, 21, 21);\">requiring just 2GB memory and 4 minutes</span> to\ncomplete. Using our benchmark, we analyze text and face reconstruction quality\nacross various scales for different image tokenizers and VAEs. Our results show\nmodern visual tokenizers still struggle to preserve fine-grained features,\nespecially at smaller scales. We further extend this evaluation framework to\nvideo, conducting comprehensive analysis of video tokenizers. Additionally, we\ndemonstrate that traditional metrics fail to accurately reflect reconstruction\nperformance for faces and text, while our proposed metrics serve as an\neffective complement.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6569de56046899997ba5ac4e/MO1xgdbEZswzcywIaSOyD.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/6569de56046899997ba5ac4e/3QZC_QFmiD9o6ZH6c-Iaq.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18142.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6569de56046899997ba5ac4e",
            "avatarUrl": "/avatars/287ee4c36a9edf2540eed779fd7d41e2.svg",
            "fullname": "Junfeng Wu",
            "name": "Junfeng5",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23738",
            "authors": [
                {
                    "_id": "6839e1b656bcc85d9fc73669",
                    "name": "Xiaojuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6839e1b656bcc85d9fc7366a",
                    "name": "Aleksander Holynski",
                    "hidden": false
                },
                {
                    "_id": "6839e1b656bcc85d9fc7366b",
                    "name": "Brian Curless",
                    "hidden": false
                },
                {
                    "_id": "6839e1b656bcc85d9fc7366c",
                    "name": "Ira Kemelmacher",
                    "hidden": false
                },
                {
                    "_id": "6839e1b656bcc85d9fc7366d",
                    "name": "Steve Seitz",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643ee9a6022a0350efd745d1/c-Ibje2ajzK3Ymxm2QJNT.mp4"
            ],
            "publishedAt": "2025-05-29T17:58:02.000Z",
            "submittedOnDailyAt": "2025-05-30T15:21:32.690Z",
            "title": "How Animals Dance (When You're Not Looking)",
            "submittedOnDailyBy": {
                "_id": "643ee9a6022a0350efd745d1",
                "avatarUrl": "/avatars/c8c17689ed88b1966d2a36361e3f83cc.svg",
                "isPro": false,
                "fullname": "Xiaojuan Wang",
                "user": "xiaojwan",
                "type": "user"
            },
            "summary": "We present a keyframe-based framework for generating music-synchronized,\nchoreography aware animal dance videos. Starting from a few keyframes\nrepresenting distinct animal poses -- generated via text-to-image prompting or\nGPT-4o -- we formulate dance synthesis as a graph optimization problem: find\nthe optimal keyframe structure that satisfies a specified choreography pattern\nof beats, which can be automatically estimated from a reference dance video. We\nalso introduce an approach for mirrored pose image generation, essential for\ncapturing symmetry in dance. In-between frames are synthesized using an video\ndiffusion model. With as few as six input keyframes, our method can produce up\nto 30 second dance videos across a wide range of animals and music tracks.",
            "upvotes": 0,
            "discussionId": "6839e1ba56bcc85d9fc73807",
            "projectPage": "https://how-animals-dance.github.io/",
            "ai_summary": "A framework synthesizes music-synchronized animal dance videos from keyframes using graph optimization and video diffusion models, supporting symmetry and wide animal and music variation.",
            "ai_keywords": [
                "graph optimization",
                "video diffusion model",
                "mirrored pose image generation"
            ]
        },
        "publishedAt": "2025-05-29T13:58:02.000Z",
        "title": "How Animals Dance (When You're Not Looking)",
        "summary": "We present a keyframe-based framework for generating music-synchronized,\nchoreography aware animal dance videos. Starting from a few keyframes\nrepresenting distinct animal poses -- generated via text-to-image prompting or\nGPT-4o -- we formulate dance synthesis as a graph optimization problem: find\nthe optimal keyframe structure that satisfies a specified choreography pattern\nof beats, which can be automatically estimated from a reference dance video. We\nalso introduce an approach for mirrored pose image generation, essential for\ncapturing symmetry in dance. In-between frames are synthesized using an video\ndiffusion model. With as few as six input keyframes, our method can produce up\nto 30 second dance videos across a wide range of animals and music tracks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643ee9a6022a0350efd745d1/c-Ibje2ajzK3Ymxm2QJNT.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23738.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643ee9a6022a0350efd745d1",
            "avatarUrl": "/avatars/c8c17689ed88b1966d2a36361e3f83cc.svg",
            "fullname": "Xiaojuan Wang",
            "name": "xiaojwan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22988",
            "authors": [
                {
                    "_id": "683a57be57009ee6f3c7a0d2",
                    "name": "Albert Tseng",
                    "hidden": false
                },
                {
                    "_id": "683a57be57009ee6f3c7a0d3",
                    "user": {
                        "_id": "67aee52fe680b6d64f1d1646",
                        "avatarUrl": "/avatars/334b73779ce2e62c41071d91cc1ffc5e.svg",
                        "isPro": false,
                        "fullname": "Zhaofeng Sun",
                        "user": "thuEcstasy1",
                        "type": "user"
                    },
                    "name": "Zhaofeng Sun",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-05-31T01:13:35.081Z",
                    "hidden": false
                },
                {
                    "_id": "683a57be57009ee6f3c7a0d4",
                    "name": "Christopher De Sa",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T01:53:00.000Z",
            "submittedOnDailyAt": "2025-05-30T23:45:36.403Z",
            "title": "Model-Preserving Adaptive Rounding",
            "submittedOnDailyBy": {
                "_id": "649e3f263914db6cf8e8ab1f",
                "avatarUrl": "/avatars/35298884b7633c67a9bcff0a32d31211.svg",
                "isPro": false,
                "fullname": "Albert Tseng",
                "user": "at676",
                "type": "user"
            },
            "summary": "The main goal of post-training quantization (PTQ) is to produced a compressed\nmodel whose output distribution is as close to the original model's as\npossible. To do this tractably, almost all LLM PTQ algorithms quantize linear\nlayers by independently minimizing the immediate activation error. However,\nthis localized objective ignores the effect of subsequent layers, so reducing\nit does not necessarily give a closer model. In this work, we introduce Yet\nAnother Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses\nKronecker-factored approximations of each linear layer's Hessian with respect\nto the full model KL divergence. YAQA consists of two components:\nKronecker-factored sketches of the full layerwise Hessian that can be tractably\ncomputed for hundred-billion parameter LLMs, and a quantizer-independent\nrounding algorithm that uses these sketches and comes with theoretical\nguarantees. Across a wide range of models and quantizers, YAQA empirically\nreduces the KL divergence to the original model by approx 30% while\nachieving state of the art performance on downstream tasks.",
            "upvotes": 0,
            "discussionId": "683a57bf57009ee6f3c7a109"
        },
        "publishedAt": "2025-05-28T21:53:00.000Z",
        "title": "Model-Preserving Adaptive Rounding",
        "summary": "The main goal of post-training quantization (PTQ) is to produced a compressed\nmodel whose output distribution is as close to the original model's as\npossible. To do this tractably, almost all LLM PTQ algorithms quantize linear\nlayers by independently minimizing the immediate activation error. However,\nthis localized objective ignores the effect of subsequent layers, so reducing\nit does not necessarily give a closer model. In this work, we introduce Yet\nAnother Quantization Algorithm (YAQA), an adaptive rounding algorithm that uses\nKronecker-factored approximations of each linear layer's Hessian with respect\nto the full model KL divergence. YAQA consists of two components:\nKronecker-factored sketches of the full layerwise Hessian that can be tractably\ncomputed for hundred-billion parameter LLMs, and a quantizer-independent\nrounding algorithm that uses these sketches and comes with theoretical\nguarantees. Across a wide range of models and quantizers, YAQA empirically\nreduces the KL divergence to the original model by approx 30% while\nachieving state of the art performance on downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22988.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649e3f263914db6cf8e8ab1f",
            "avatarUrl": "/avatars/35298884b7633c67a9bcff0a32d31211.svg",
            "fullname": "Albert Tseng",
            "name": "at676",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20099",
            "authors": [
                {
                    "_id": "6839b771cdef5cabcc651a24",
                    "name": "Chuangtao Ma",
                    "hidden": false
                },
                {
                    "_id": "6839b771cdef5cabcc651a25",
                    "name": "Yongrui Chen",
                    "hidden": false
                },
                {
                    "_id": "6839b771cdef5cabcc651a26",
                    "name": "Tianxing Wu",
                    "hidden": false
                },
                {
                    "_id": "6839b771cdef5cabcc651a27",
                    "name": "Arijit Khan",
                    "hidden": false
                },
                {
                    "_id": "6839b771cdef5cabcc651a28",
                    "name": "Haofen Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T15:08:23.000Z",
            "submittedOnDailyAt": "2025-05-30T12:23:33.059Z",
            "title": "Large Language Models Meet Knowledge Graphs for Question Answering:\n  Synthesis and Opportunities",
            "submittedOnDailyBy": {
                "_id": "66c8b6e42400073af3219888",
                "avatarUrl": "/avatars/9b37a5a6e20d5389d53686663b9ef7a1.svg",
                "isPro": false,
                "fullname": "Chuangtao Ma",
                "user": "ctma",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.",
            "upvotes": 0,
            "discussionId": "6839b772cdef5cabcc651a5a",
            "githubRepo": "https://github.com/machuangtao/LLM-KG4QA",
            "ai_summary": "This survey categorizes and analyzes methods for integrating large language models with knowledge graphs to improve their performance on complex question-answering tasks.",
            "ai_keywords": [
                "large language models",
                "knowledge graphs",
                "natural language understanding",
                "generation",
                "reasoning capacity",
                "hallucinations",
                "systematization",
                "state-of-the-art",
                "evaluation metrics",
                "benchmark datasets"
            ]
        },
        "publishedAt": "2025-05-26T11:08:23.000Z",
        "title": "Large Language Models Meet Knowledge Graphs for Question Answering:\n  Synthesis and Opportunities",
        "summary": "Large language models (LLMs) have demonstrated remarkable performance on\nquestion-answering (QA) tasks because of their superior capabilities in natural\nlanguage understanding and generation. However, LLM-based QA struggles with\ncomplex QA tasks due to poor reasoning capacity, outdated knowledge, and\nhallucinations. Several recent works synthesize LLMs and knowledge graphs (KGs)\nfor QA to address the above challenges. In this survey, we propose a new\nstructured taxonomy that categorizes the methodology of synthesizing LLMs and\nKGs for QA according to the categories of QA and the KG's role when integrating\nwith LLMs. We systematically survey state-of-the-art advances in synthesizing\nLLMs and KGs for QA and compare and analyze these approaches in terms of\nstrength, limitations, and KG requirements. We then align the approaches with\nQA and discuss how these approaches address the main challenges of different\ncomplex QA. Finally, we summarize the advancements, evaluation metrics, and\nbenchmark datasets and highlight open challenges and opportunities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20099.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66c8b6e42400073af3219888",
            "avatarUrl": "/avatars/9b37a5a6e20d5389d53686663b9ef7a1.svg",
            "fullname": "Chuangtao Ma",
            "name": "ctma",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.14599",
            "authors": [
                {
                    "_id": "683a4c9070c36271751e3fbc",
                    "name": "Guangzhi Xiong",
                    "hidden": false
                },
                {
                    "_id": "683a4c9070c36271751e3fbd",
                    "name": "Eric Xie",
                    "hidden": false
                },
                {
                    "_id": "683a4c9070c36271751e3fbe",
                    "name": "Corey Williams",
                    "hidden": false
                },
                {
                    "_id": "683a4c9070c36271751e3fbf",
                    "name": "Myles Kim",
                    "hidden": false
                },
                {
                    "_id": "683a4c9070c36271751e3fc0",
                    "name": "Amir Hassan Shariatmadari",
                    "hidden": false
                },
                {
                    "_id": "683a4c9070c36271751e3fc1",
                    "name": "Sikun Guo",
                    "hidden": false
                },
                {
                    "_id": "683a4c9070c36271751e3fc2",
                    "name": "Stefan Bekiranov",
                    "hidden": false
                },
                {
                    "_id": "683a4c9070c36271751e3fc3",
                    "name": "Aidong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-20T16:49:40.000Z",
            "submittedOnDailyAt": "2025-05-30T22:56:53.100Z",
            "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating\n  Truthfulness and Hallucination in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "657e56d11e3e9c41a4a57d2c",
                "avatarUrl": "/avatars/1a6e7cff2693e1523f87ad24f4529872.svg",
                "isPro": false,
                "fullname": "Guangzhi Xiong",
                "user": "TeddyXGZ",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.",
            "upvotes": 0,
            "discussionId": "683a4c9170c36271751e3ffa"
        },
        "publishedAt": "2025-05-20T12:49:40.000Z",
        "title": "Toward Reliable Biomedical Hypothesis Generation: Evaluating\n  Truthfulness and Hallucination in Large Language Models",
        "summary": "Large language models (LLMs) have shown significant potential in scientific\ndisciplines such as biomedicine, particularly in hypothesis generation, where\nthey can analyze vast literature, identify patterns, and suggest research\ndirections. However, a key challenge lies in evaluating the truthfulness of\ngenerated hypotheses, as verifying their accuracy often requires substantial\ntime and resources. Additionally, the hallucination problem in LLMs can lead to\nthe generation of hypotheses that appear plausible but are ultimately\nincorrect, undermining their reliability. To facilitate the systematic study of\nthese challenges, we introduce TruthHypo, a benchmark for assessing the\ncapabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD,\na knowledge-based hallucination detector to evaluate how well hypotheses are\ngrounded in existing knowledge. Our results show that LLMs struggle to generate\ntruthful hypotheses. By analyzing hallucinations in reasoning steps, we\ndemonstrate that the groundedness scores provided by KnowHD serve as an\neffective metric for filtering truthful hypotheses from the diverse outputs of\nLLMs. Human evaluations further validate the utility of KnowHD in identifying\ntruthful hypotheses and accelerating scientific discovery. Our data and source\ncode are available at https://github.com/Teddy-XiongGZ/TruthHypo.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14599.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "657e56d11e3e9c41a4a57d2c",
            "avatarUrl": "/avatars/1a6e7cff2693e1523f87ad24f4529872.svg",
            "fullname": "Guangzhi Xiong",
            "name": "TeddyXGZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]