[
    {
        "paper": {
            "id": "2505.24864",
            "authors": [
                {
                    "_id": "683d2d05ae87a04bca311b22",
                    "name": "Mingjie Liu",
                    "hidden": false
                },
                {
                    "_id": "683d2d05ae87a04bca311b23",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "683d2d05ae87a04bca311b24",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "683d2d05ae87a04bca311b25",
                    "name": "Jian Hu",
                    "hidden": false
                },
                {
                    "_id": "683d2d05ae87a04bca311b26",
                    "name": "Xin Dong",
                    "hidden": false
                },
                {
                    "_id": "683d2d05ae87a04bca311b27",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "683d2d05ae87a04bca311b28",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "683d2d05ae87a04bca311b29",
                    "name": "Yi Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T17:59:01.000Z",
            "submittedOnDailyAt": "2025-06-02T03:18:21.654Z",
            "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "633bd54b00732349209a18fe",
                "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
                "isPro": false,
                "fullname": "Shizhe Diao",
                "user": "shizhediao",
                "type": "user"
            },
            "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
            "upvotes": 81,
            "discussionId": "683d2d08ae87a04bca311bd4",
            "ai_summary": "Prolonged reinforcement learning training (ProRL) uncovers novel reasoning strategies in language models, outperforming base models and suggesting meaningful expansion of reasoning capabilities.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "ProRL",
                "KL divergence control",
                "reference policy resetting",
                "pass@k evaluations",
                "reasoning boundary improvements",
                "task competence",
                "long-horizon RL"
            ]
        },
        "publishedAt": "2025-05-30T13:59:01.000Z",
        "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in\n  Large Language Models",
        "summary": "Recent advances in reasoning-centric language models have highlighted\nreinforcement learning (RL) as a promising method for aligning models with\nverifiable rewards. However, it remains contentious whether RL truly expands a\nmodel's reasoning capabilities or merely amplifies high-reward outputs already\nlatent in the base model's distribution, and whether continually scaling up RL\ncompute reliably leads to improved reasoning performance. In this work, we\nchallenge prevailing assumptions by demonstrating that prolonged RL (ProRL)\ntraining can uncover novel reasoning strategies that are inaccessible to base\nmodels, even under extensive sampling. We introduce ProRL, a novel training\nmethodology that incorporates KL divergence control, reference policy\nresetting, and a diverse suite of tasks. Our empirical analysis reveals that\nRL-trained models consistently outperform base models across a wide range of\npass@k evaluations, including scenarios where base models fail entirely\nregardless of the number of attempts. We further show that reasoning boundary\nimprovements correlates strongly with task competence of base model and\ntraining duration, suggesting that RL can explore and populate new regions of\nsolution space over time. These findings offer new insights into the conditions\nunder which RL meaningfully expands reasoning boundaries in language models and\nestablish a foundation for future work on long-horizon RL for reasoning. We\nrelease model weights to support further research:\nhttps://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24864.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "633bd54b00732349209a18fe",
            "avatarUrl": "/avatars/150aa5b8d9bcca24366402932bf2d049.svg",
            "fullname": "Shizhe Diao",
            "name": "shizhediao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24863",
            "authors": [
                {
                    "_id": "683d0b3de2a7d8d9778bd141",
                    "user": {
                        "_id": "6719bfd07c6e6c83a388aeae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6719bfd07c6e6c83a388aeae/jHxryk04dzHo23TX5F5sz.png",
                        "isPro": false,
                        "fullname": "Junyu Zhang",
                        "user": "jyzhang1208",
                        "type": "user"
                    },
                    "name": "Junyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:40:59.716Z",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd142",
                    "user": {
                        "_id": "6201fc5d91d53938a6432fbf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
                        "isPro": false,
                        "fullname": "Runpei Dong",
                        "user": "RunpeiDong",
                        "type": "user"
                    },
                    "name": "Runpei Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:41:03.079Z",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd143",
                    "name": "Han Wang",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd144",
                    "name": "Xuying Ning",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd145",
                    "name": "Haoran Geng",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd146",
                    "name": "Peihao Li",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd147",
                    "name": "Xialin He",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd148",
                    "name": "Yutong Bai",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd149",
                    "name": "Jitendra Malik",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd14a",
                    "name": "Saurabh Gupta",
                    "hidden": false
                },
                {
                    "_id": "683d0b3de2a7d8d9778bd14b",
                    "name": "Huan Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
            ],
            "publishedAt": "2025-05-30T17:58:36.000Z",
            "submittedOnDailyAt": "2025-06-02T00:55:04.615Z",
            "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
            "submittedOnDailyBy": {
                "_id": "6201fc5d91d53938a6432fbf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
                "isPro": false,
                "fullname": "Runpei Dong",
                "user": "RunpeiDong",
                "type": "user"
            },
            "summary": "This paper presents AlphaOne (alpha1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\nalpha1 first introduces alpha moment, which represents the scaled\nthinking phase with a universal parameter alpha. Within this scaled\npre-alpha moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the alpha moment, alpha1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate alpha1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/",
            "upvotes": 61,
            "discussionId": "683d0b3ee2a7d8d9778bd1ce",
            "projectPage": "https://alphaone-project.github.io/",
            "githubRepo": "https://github.com/ASTRAL-Group/AlphaOne",
            "ai_summary": "AlphaOne dynamically modulates reasoning in large models by introducing $\\alpha$ moment and Bernoulli process for slow thinking, improving efficiency and capability across diverse domains.",
            "ai_keywords": [
                "AlphaOne",
                "$\\alpha$ moment",
                "Bernoulli stochastic process",
                "large reasoning models",
                "reasoning transition tokens",
                "end-of-thinking token",
                "monotonic scaling methods",
                "fast reasoning",
                "efficient answer generation"
            ]
        },
        "publishedAt": "2025-05-30T13:58:36.000Z",
        "title": "AlphaOne: Reasoning Models Thinking Slow and Fast at Test Time",
        "summary": "This paper presents AlphaOne (alpha1), a universal framework for\nmodulating reasoning progress in large reasoning models (LRMs) at test time.\nalpha1 first introduces alpha moment, which represents the scaled\nthinking phase with a universal parameter alpha. Within this scaled\npre-alpha moment phase, it dynamically schedules slow thinking transitions\nby modeling the insertion of reasoning transition tokens as a Bernoulli\nstochastic process. After the alpha moment, alpha1 deterministically\nterminates slow thinking with the end-of-thinking token, thereby fostering fast\nreasoning and efficient answer generation. This approach unifies and\ngeneralizes existing monotonic scaling methods by enabling flexible and dense\nslow-to-fast reasoning modulation. Extensive empirical studies on various\nchallenging benchmarks across mathematical, coding, and scientific domains\ndemonstrate alpha1's superior reasoning capability and efficiency. Project\npage: https://alphaone-project.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6201fc5d91d53938a6432fbf/dBNLCtnWBtBclw0ZZsYBU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24863.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6201fc5d91d53938a6432fbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6201fc5d91d53938a6432fbf/VLs8ZYaZrop4KBpZn53fH.jpeg",
            "fullname": "Runpei Dong",
            "name": "RunpeiDong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24867",
            "authors": [
                {
                    "_id": "683d3d6f3f97feb881155aef",
                    "user": {
                        "_id": "5df7ca7cda6d0311fd3d53f2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5df7ca7cda6d0311fd3d53f2/dtAoDSqgNxeO9AYg9V3na.jpeg",
                        "isPro": false,
                        "fullname": "Ujjwal Upadhyay",
                        "user": "ujjwal9",
                        "type": "user"
                    },
                    "name": "Ujjwal Upadhyay",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-02T05:58:12.617Z",
                    "hidden": false
                },
                {
                    "_id": "683d3d6f3f97feb881155af0",
                    "user": {
                        "_id": "65262a396b41932089fd7bae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
                        "isPro": true,
                        "fullname": "Mukul Ranjan",
                        "user": "mukul54",
                        "type": "user"
                    },
                    "name": "Mukul Ranjan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:40:23.895Z",
                    "hidden": false
                },
                {
                    "_id": "683d3d6f3f97feb881155af1",
                    "name": "Zhiqiang Shen",
                    "hidden": false
                },
                {
                    "_id": "683d3d6f3f97feb881155af2",
                    "name": "Mohamed Elhoseiny",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T17:59:12.000Z",
            "submittedOnDailyAt": "2025-06-02T04:31:40.253Z",
            "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
            "submittedOnDailyBy": {
                "_id": "65262a396b41932089fd7bae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
                "isPro": true,
                "fullname": "Mukul Ranjan",
                "user": "mukul54",
                "type": "user"
            },
            "summary": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce SpookyBench, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.",
            "upvotes": 58,
            "discussionId": "683d3d743f97feb881155c56",
            "projectPage": "https://timeblindness.github.io",
            "githubRepo": "https://github.com/TimeBlindness/time-blindness",
            "ai_summary": "SpookyBench is a benchmark for temporal pattern recognition in videos that highlights the limitations of vision-language models in processing noise-like frames without spatial information.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "spatio-temporal relationships",
                "temporal sequences",
                "noise-like frames",
                "biological signaling",
                "covert communication",
                "frame-level spatial features",
                "temporal understanding",
                "data sets",
                "low spatial signal-to-noise ratios",
                "SNR",
                "temporal reasoning",
                "novel architectures",
                "training paradigms",
                "systematic analysis"
            ]
        },
        "publishedAt": "2025-05-30T13:59:12.000Z",
        "title": "Time Blindness: Why Video-Language Models Can't See What Humans Can?",
        "summary": "Recent advances in vision-language models (VLMs) have made impressive strides\nin understanding spatio-temporal relationships in videos. However, when spatial\ninformation is obscured, these models struggle to capture purely temporal\npatterns. We introduce SpookyBench, a benchmark where information is\nencoded solely in temporal sequences of noise-like frames, mirroring natural\nphenomena from biological signaling to covert communication. Interestingly,\nwhile humans can recognize shapes, text, and patterns in these sequences with\nover 98% accuracy, state-of-the-art VLMs achieve 0% accuracy. This performance\ngap highlights a critical limitation: an over-reliance on frame-level spatial\nfeatures and an inability to extract meaning from temporal cues. Furthermore,\nwhen trained in data sets with low spatial signal-to-noise ratios (SNR),\ntemporal understanding of models degrades more rapidly than human perception,\nespecially in tasks requiring fine-grained temporal reasoning. Overcoming this\nlimitation will require novel architectures or training paradigms that decouple\nspatial dependencies from temporal processing. Our systematic analysis shows\nthat this issue persists across model scales and architectures. We release\nSpookyBench to catalyze research in temporal pattern recognition and bridge the\ngap between human and machine video understanding. Dataset and code has been\nmade available on our project website: https://timeblindness.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24867.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65262a396b41932089fd7bae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65262a396b41932089fd7bae/6YIEoAfJojuTW1UOKlwZT.png",
            "fullname": "Mukul Ranjan",
            "name": "mukul54",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24098",
            "authors": [
                {
                    "_id": "683d2cee5bdbb3803e42bc8a",
                    "name": "Zhongmou He",
                    "hidden": false
                },
                {
                    "_id": "683d2cee5bdbb3803e42bc8b",
                    "name": "Yee Man Choi",
                    "hidden": false
                },
                {
                    "_id": "683d2cee5bdbb3803e42bc8c",
                    "name": "Kexun Zhang",
                    "hidden": false
                },
                {
                    "_id": "683d2cee5bdbb3803e42bc8d",
                    "name": "Jiabao Ji",
                    "hidden": false
                },
                {
                    "_id": "683d2cee5bdbb3803e42bc8e",
                    "user": {
                        "_id": "65a374a59acab1998092a9bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65a374a59acab1998092a9bc/M3s_7bSf9G-6b9nLg7N3Z.jpeg",
                        "isPro": false,
                        "fullname": "Antonio",
                        "user": "JuntingZhou",
                        "type": "user"
                    },
                    "name": "Junting Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:40:34.926Z",
                    "hidden": false
                },
                {
                    "_id": "683d2cee5bdbb3803e42bc8f",
                    "name": "Dejia Xu",
                    "hidden": false
                },
                {
                    "_id": "683d2cee5bdbb3803e42bc90",
                    "name": "Ivan Bercovich",
                    "hidden": false
                },
                {
                    "_id": "683d2cee5bdbb3803e42bc91",
                    "name": "Aidan Zhang",
                    "hidden": false
                },
                {
                    "_id": "683d2cee5bdbb3803e42bc92",
                    "name": "Lei Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
            ],
            "publishedAt": "2025-05-30T01:00:34.000Z",
            "submittedOnDailyAt": "2025-06-02T03:20:27.903Z",
            "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding",
            "submittedOnDailyBy": {
                "_id": "62ee423b4bebb4ab55c674b1",
                "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
                "isPro": false,
                "fullname": "Kexun Zhang",
                "user": "k1z",
                "type": "user"
            },
            "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/.",
            "upvotes": 37,
            "discussionId": "683d2cef5bdbb3803e42bccc",
            "projectPage": "https://leililab.github.io/HardTests/",
            "ai_summary": "HARDTESTGEN creates a large, high-quality competitive programming dataset to enhance the precision and recall of verifiers in evaluating LLM-generated code.",
            "ai_keywords": [
                "LLM reasoning",
                "reinforcement learning",
                "verifiers",
                "test synthesis",
                "LLMs",
                "competitive programming",
                "synthetic tests",
                "precision",
                "recall",
                "code generation performance"
            ]
        },
        "publishedAt": "2025-05-29T21:00:34.000Z",
        "title": "HardTests: Synthesizing High-Quality Test Cases for LLM Coding",
        "summary": "Verifiers play a crucial role in large language model (LLM) reasoning, needed\nby post-training techniques such as reinforcement learning. However, reliable\nverifiers are hard to get for difficult coding problems, because a\nwell-disguised wrong solution may only be detected by carefully human-written\nedge cases that are difficult to synthesize. To address this issue, we propose\nHARDTESTGEN, a pipeline for high-quality test synthesis using LLMs. With this\npipeline, we curate a comprehensive competitive programming dataset HARDTESTS\nwith 47k problems and synthetic high-quality tests. Compared with existing\ntests, HARDTESTGEN tests demonstrate precision that is 11.3 percentage points\nhigher and recall that is 17.5 percentage points higher when evaluating\nLLM-generated code. For harder problems, the improvement in precision can be as\nlarge as 40 points. HARDTESTS also proves to be more effective for model\ntraining, measured by downstream code generation performance. We will\nopen-source our dataset and synthesis pipeline at\nhttps://leililab.github.io/HardTests/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62ee423b4bebb4ab55c674b1/yE3pB5JGaOf-sdyDjpCk6.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24098.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ee423b4bebb4ab55c674b1",
            "avatarUrl": "/avatars/ce2797937e8225937fc84d6847d50077.svg",
            "fullname": "Kexun Zhang",
            "name": "k1z",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.14752",
            "authors": [
                {
                    "_id": "6832c2c8ba29b909f4013a6d",
                    "user": {
                        "_id": "67569b1860146dd8c9c8008f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
                        "isPro": false,
                        "fullname": "Yihong Tang",
                        "user": "HYTYH",
                        "type": "user"
                    },
                    "name": "Yihong Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-26T08:12:00.941Z",
                    "hidden": false
                },
                {
                    "_id": "6832c2c8ba29b909f4013a6e",
                    "name": "Menglin Kong",
                    "hidden": false
                },
                {
                    "_id": "6832c2c8ba29b909f4013a6f",
                    "name": "Lijun Sun",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
            ],
            "publishedAt": "2025-05-20T13:35:38.000Z",
            "submittedOnDailyAt": "2025-06-02T02:10:16.659Z",
            "title": "Large Language Models for Data Synthesis",
            "submittedOnDailyBy": {
                "_id": "67569b1860146dd8c9c8008f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
                "isPro": false,
                "fullname": "Yihong Tang",
                "user": "HYTYH",
                "type": "user"
            },
            "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond.",
            "upvotes": 35,
            "discussionId": "6832c2c9ba29b909f4013aea",
            "projectPage": "https://yihongt.github.io/llmsynthor_web/",
            "githubRepo": "https://github.com/YihongT/LLMSynthor",
            "ai_summary": "LLMSynthor enhances LLMs for efficient and statistically accurate data synthesis through distributional feedback and proposal sampling.",
            "ai_keywords": [
                "Large Language Models",
                "LLMSynthor",
                "nonparametric copula simulator",
                "LLM Proposal Sampling",
                "summary statistics space",
                "synthetic data",
                "statistical fidelity",
                "practical utility",
                "cross-data adaptability"
            ]
        },
        "publishedAt": "2025-05-20T09:35:38.000Z",
        "title": "Large Language Models for Data Synthesis",
        "summary": "Generating synthetic data that faithfully captures the statistical structure\nof real-world distributions is a fundamental challenge in data modeling.\nClassical approaches often depend on strong parametric assumptions or manual\nstructural design and struggle in high-dimensional or heterogeneous domains.\nRecent progress in Large Language Models (LLMs) reveals their potential as\nflexible, high-dimensional priors over real-world distributions. However, when\napplied to data synthesis, standard LLM-based sampling is inefficient,\nconstrained by fixed context limits, and fails to ensure statistical alignment.\nGiven this, we introduce LLMSynthor, a general framework for data synthesis\nthat transforms LLMs into structure-aware simulators guided by distributional\nfeedback. LLMSynthor treats the LLM as a nonparametric copula simulator for\nmodeling high-order dependencies and introduces LLM Proposal Sampling to\ngenerate grounded proposal distributions that improve sampling efficiency\nwithout requiring rejection. By minimizing discrepancies in the summary\nstatistics space, the iterative synthesis loop aligns real and synthetic data\nwhile gradually uncovering and refining the latent generative structure. We\nevaluate LLMSynthor in both controlled and real-world settings using\nheterogeneous datasets in privacy-sensitive domains (e.g., e-commerce,\npopulation, and mobility) that encompass both structured and unstructured\nformats. The synthetic data produced by LLMSynthor shows high statistical\nfidelity, practical utility, and cross-data adaptability, positioning it as a\nvaluable tool across economics, social science, urban studies, and beyond.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/VaA9NxCa0ncxzh03aZE8c.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67569b1860146dd8c9c8008f/oEgXPOzTQVTDfuTp5Z575.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.14752.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67569b1860146dd8c9c8008f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67569b1860146dd8c9c8008f/f5Tz2yVTry4LGyQE2VC6-.jpeg",
            "fullname": "Yihong Tang",
            "name": "HYTYH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18842",
            "authors": [
                {
                    "_id": "6839543d6451d371f9e834ec",
                    "name": "Jiwan Chung",
                    "hidden": false
                },
                {
                    "_id": "6839543d6451d371f9e834ed",
                    "user": {
                        "_id": "646aecb04c1cd18b497a50ee",
                        "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
                        "isPro": false,
                        "fullname": "Junhyeok Kim",
                        "user": "kjunh",
                        "type": "user"
                    },
                    "name": "Junhyeok Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:46:37.442Z",
                    "hidden": false
                },
                {
                    "_id": "6839543d6451d371f9e834ee",
                    "user": {
                        "_id": "67021743e4d49b157afd8260",
                        "avatarUrl": "/avatars/2a22a18cd45f6d115e8a3a5d1e477dcb.svg",
                        "isPro": false,
                        "fullname": "Siyeol Kim",
                        "user": "siyeolkim",
                        "type": "user"
                    },
                    "name": "Siyeol Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:46:34.334Z",
                    "hidden": false
                },
                {
                    "_id": "6839543d6451d371f9e834ef",
                    "user": {
                        "_id": "64ca095f29d2f65419015fcb",
                        "avatarUrl": "/avatars/722df32d9d99382fdc038ca8f1c15c65.svg",
                        "isPro": false,
                        "fullname": "Jaeyoung Lee",
                        "user": "given131",
                        "type": "user"
                    },
                    "name": "Jaeyoung Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T13:39:57.765Z",
                    "hidden": false
                },
                {
                    "_id": "6839543d6451d371f9e834f0",
                    "name": "Min Soo Kim",
                    "hidden": false
                },
                {
                    "_id": "6839543d6451d371f9e834f1",
                    "name": "Youngjae Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T19:30:47.000Z",
            "submittedOnDailyAt": "2025-06-02T02:58:04.513Z",
            "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation",
            "submittedOnDailyBy": {
                "_id": "646aecb04c1cd18b497a50ee",
                "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
                "isPro": false,
                "fullname": "Junhyeok Kim",
                "user": "kjunh",
                "type": "user"
            },
            "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.",
            "upvotes": 29,
            "discussionId": "6839543f6451d371f9e83544",
            "githubRepo": "https://github.com/jun297/v1",
            "ai_summary": "v1 enhances Multimodal Large Language Models by enabling selective and dynamic visual region retrieval during inference, improving performance on multimodal reasoning tasks.",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "point-and-copy mechanism",
                "visual tokens",
                "multimodal reasoning traces",
                "visual grounding annotations",
                "MathVista",
                "MathVision",
                "MathVerse",
                "grounded multimodal reasoning"
            ]
        },
        "publishedAt": "2025-05-24T15:30:47.000Z",
        "title": "Don't Look Only Once: Towards Multimodal Interactive Reasoning with\n  Selective Visual Revisitation",
        "summary": "We present v1, a lightweight extension to Multimodal Large Language Models\n(MLLMs) that enables selective visual revisitation during inference. While\ncurrent MLLMs typically consume visual input only once and reason purely over\ninternal memory, v1 introduces a simple point-and-copy mechanism that allows\nthe model to dynamically retrieve relevant image regions throughout the\nreasoning process. This mechanism augments existing architectures with minimal\nmodifications, enabling contextual access to visual tokens based on the model's\nevolving hypotheses. To train this capability, we construct v1g, a dataset of\n300K multimodal reasoning traces with interleaved visual grounding annotations.\nExperiments on three multimodal mathematical reasoning benchmarks -- MathVista,\nMathVision, and MathVerse -- demonstrate that v1 consistently improves\nperformance over comparable baselines, particularly on tasks requiring\nfine-grained visual reference and multi-step reasoning. Our results suggest\nthat dynamic visual access is a promising direction for enhancing grounded\nmultimodal reasoning. Code, models, and data will be released to support future\nresearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18842.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646aecb04c1cd18b497a50ee",
            "avatarUrl": "/avatars/de15c724056f36a41cb4f375d05ed836.svg",
            "fullname": "Junhyeok Kim",
            "name": "kjunh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24862",
            "authors": [
                {
                    "_id": "683d54f364b44c0ccabb9e65",
                    "name": "Cailin Zhuang",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e66",
                    "name": "Ailin Huang",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e67",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:40:49.393Z",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e68",
                    "name": "Jingwei Wu",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e69",
                    "name": "Yaoqi Hu",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e6a",
                    "name": "Jiaqi Liao",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e6b",
                    "name": "Zhewei Huang",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e6c",
                    "name": "Hongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e6d",
                    "name": "Xinyao Liao",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e6e",
                    "name": "Weiwei Cai",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e6f",
                    "name": "Hengyuan Xu",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e70",
                    "name": "Xuanyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e71",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e72",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "683d54f364b44c0ccabb9e73",
                    "name": "Chi Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
            ],
            "publishedAt": "2025-05-30T17:58:21.000Z",
            "submittedOnDailyAt": "2025-06-02T06:09:52.296Z",
            "title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization",
            "submittedOnDailyBy": {
                "_id": "64b914c8ace99c0723ad83a9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
                "isPro": false,
                "fullname": "Wei Cheng",
                "user": "wchengad",
                "type": "user"
            },
            "summary": "Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements.",
            "upvotes": 23,
            "discussionId": "683d54f764b44c0ccabb9f60",
            "projectPage": "https://vistorybench.github.io/",
            "githubRepo": "https://github.com/vistorybench/vistorybench",
            "ai_summary": "ViStoryBench is a comprehensive evaluation benchmark for story visualization frameworks, featuring diverse datasets and metrics to assess model performance across narrative and visual dimensions.",
            "ai_keywords": [
                "generative models",
                "story visualization",
                "evaluation benchmark",
                "diverse dataset",
                "plots",
                "visual aesthetics",
                "narrative structures",
                "visual elements",
                "character consistency",
                "complex plots",
                "world-building",
                "evaluation metrics"
            ]
        },
        "publishedAt": "2025-05-30T13:58:21.000Z",
        "title": "ViStoryBench: Comprehensive Benchmark Suite for Story Visualization",
        "summary": "Story visualization, which aims to generate a sequence of visually coherent\nimages aligning with a given narrative and reference images, has seen\nsignificant progress with recent advancements in generative models. To further\nenhance the performance of story visualization frameworks in real-world\nscenarios, we introduce a comprehensive evaluation benchmark, ViStoryBench. We\ncollect a diverse dataset encompassing various story types and artistic styles,\nensuring models are evaluated across multiple dimensions such as different\nplots (e.g., comedy, horror) and visual aesthetics (e.g., anime, 3D\nrenderings). ViStoryBench is carefully curated to balance narrative structures\nand visual elements, featuring stories with single and multiple protagonists to\ntest models' ability to maintain character consistency. Additionally, it\nincludes complex plots and intricate world-building to challenge models in\ngenerating accurate visuals. To ensure comprehensive comparisons, our benchmark\nincorporates a wide range of evaluation metrics assessing critical aspects.\nThis structured and multifaceted framework enables researchers to thoroughly\nidentify both the strengths and weaknesses of different models, fostering\ntargeted improvements.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/ZHmu_F8c4mbhiPtomY6H7.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24862.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "fullname": "Wei Cheng",
            "name": "wchengad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24025",
            "authors": [
                {
                    "_id": "683dee0d6416d41bef4739d4",
                    "name": "Chenbin Pan",
                    "hidden": false
                },
                {
                    "_id": "683dee0d6416d41bef4739d5",
                    "name": "Wenbin He",
                    "hidden": false
                },
                {
                    "_id": "683dee0d6416d41bef4739d6",
                    "name": "Zhengzhong Tu",
                    "hidden": false
                },
                {
                    "_id": "683dee0d6416d41bef4739d7",
                    "name": "Liu Ren",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T21:58:06.000Z",
            "submittedOnDailyAt": "2025-06-02T17:09:27.748Z",
            "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models",
            "submittedOnDailyBy": {
                "_id": "62548d5fef3debb2ddf91217",
                "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
                "isPro": false,
                "fullname": "Zhengzhong Tu",
                "user": "vztu",
                "type": "user"
            },
            "summary": "The recent explosive interest in the reasoning capabilities of large language\nmodels, such as DeepSeek-R1, has demonstrated remarkable success through\nreinforcement learning-based fine-tuning frameworks, exemplified by methods\nlike Group Relative Policy Optimization (GRPO). However, such reasoning\nabilities remain underexplored and notably absent in vision foundation models,\nincluding representation models like the DINO series. In this work, we propose\nDINO-R1, the first such attempt to incentivize visual in-context\nreasoning capabilities of vision foundation models using reinforcement\nlearning. Specifically, DINO-R1 introduces Group Relative Query\nOptimization (GRQO), a novel reinforcement-style training strategy explicitly\ndesigned for query-based representation models, which computes query-level\nrewards based on group-normalized alignment quality. We also apply\nKL-regularization to stabilize the objectness distribution to reduce the\ntraining instability. This joint optimization enables dense and expressive\nsupervision across queries while mitigating overfitting and distributional\ndrift. Building upon Grounding-DINO, we train a series of DINO-R1 family models\nthat integrate a visual prompt encoder and a visual-guided query selection\nmechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that\nDINO-R1 significantly outperforms supervised fine-tuning baselines, achieving\nstrong generalization in both open-vocabulary and closed-set visual prompting\nscenarios.",
            "upvotes": 20,
            "discussionId": "683dee0e6416d41bef473a1e",
            "ai_summary": "DINO-R1 incorporates reinforcement learning to enhance visual in-context reasoning capabilities in vision foundation models, achieving better performance than supervised fine-tuning across various visual prompting scenarios.",
            "ai_keywords": [
                "DeepSeek-R1",
                "reinforcement learning-based fine-tuning",
                "Group Relative Policy Optimization (GRPO)",
                "visual in-context reasoning",
                "vision foundation models",
                "DINO series",
                "DINO-R1",
                "Group Relative Query Optimization (GRQO)",
                "KL-regularization",
                "objectness distribution",
                "Grounding-DINO",
                "visual prompt encoder",
                "visual-guided query selection mechanism",
                "COCO",
                "LVIS",
                "ODinW",
                "open-vocabulary",
                "closed-set visual prompting"
            ]
        },
        "publishedAt": "2025-05-29T17:58:06.000Z",
        "title": "DINO-R1: Incentivizing Reasoning Capability in Vision Foundation Models",
        "summary": "The recent explosive interest in the reasoning capabilities of large language\nmodels, such as DeepSeek-R1, has demonstrated remarkable success through\nreinforcement learning-based fine-tuning frameworks, exemplified by methods\nlike Group Relative Policy Optimization (GRPO). However, such reasoning\nabilities remain underexplored and notably absent in vision foundation models,\nincluding representation models like the DINO series. In this work, we propose\nDINO-R1, the first such attempt to incentivize visual in-context\nreasoning capabilities of vision foundation models using reinforcement\nlearning. Specifically, DINO-R1 introduces Group Relative Query\nOptimization (GRQO), a novel reinforcement-style training strategy explicitly\ndesigned for query-based representation models, which computes query-level\nrewards based on group-normalized alignment quality. We also apply\nKL-regularization to stabilize the objectness distribution to reduce the\ntraining instability. This joint optimization enables dense and expressive\nsupervision across queries while mitigating overfitting and distributional\ndrift. Building upon Grounding-DINO, we train a series of DINO-R1 family models\nthat integrate a visual prompt encoder and a visual-guided query selection\nmechanism. Extensive experiments on COCO, LVIS, and ODinW demonstrate that\nDINO-R1 significantly outperforms supervised fine-tuning baselines, achieving\nstrong generalization in both open-vocabulary and closed-set visual prompting\nscenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24025.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62548d5fef3debb2ddf91217",
            "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
            "fullname": "Zhengzhong Tu",
            "name": "vztu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24878",
            "authors": [
                {
                    "_id": "683d160e51706d12b2c6f79f",
                    "name": "Yaxin Luo",
                    "hidden": false
                },
                {
                    "_id": "683d160e51706d12b2c6f7a0",
                    "name": "Zhaoyi Li",
                    "hidden": false
                },
                {
                    "_id": "683d160e51706d12b2c6f7a1",
                    "name": "Jiacheng Liu",
                    "hidden": false
                },
                {
                    "_id": "683d160e51706d12b2c6f7a2",
                    "user": {
                        "_id": "683d2ac900c71614bab8ea02",
                        "avatarUrl": "/avatars/7cb1a5c2c778774262a7d7cb6d309abe.svg",
                        "isPro": false,
                        "fullname": "Jiacheng Cui",
                        "user": "jiachengcui888",
                        "type": "user"
                    },
                    "name": "Jiacheng Cui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:40:52.498Z",
                    "hidden": false
                },
                {
                    "_id": "683d160e51706d12b2c6f7a3",
                    "name": "Xiaohan Zhao",
                    "hidden": false
                },
                {
                    "_id": "683d160e51706d12b2c6f7a4",
                    "name": "Zhiqiang Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T17:59:55.000Z",
            "submittedOnDailyAt": "2025-06-02T01:40:24.093Z",
            "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
            "submittedOnDailyBy": {
                "_id": "653cb809b424289c5f384a02",
                "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
                "isPro": true,
                "fullname": "YaxinLuo",
                "user": "YaxinLuo",
                "type": "user"
            },
            "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
            "upvotes": 16,
            "discussionId": "683d160f51706d12b2c6f7f4",
            "githubRepo": "https://github.com/MetaAgentX/OpenCaptchaWorld",
            "ai_summary": "Open CaptchaWorld benchmark evaluates MLLM-powered agents on diverse CAPTCHA puzzles, revealing significant performance gaps compared to humans.",
            "ai_keywords": [
                "multimodal LLM",
                "CAPTCHA",
                "visual reasoning",
                "interaction capabilities",
                "CAPTCHA Reasoning Depth",
                "Browser-Use Openai-o3"
            ]
        },
        "publishedAt": "2025-05-30T13:59:55.000Z",
        "title": "Open CaptchaWorld: A Comprehensive Web-based Platform for Testing and\n  Benchmarking Multimodal LLM Agents",
        "summary": "CAPTCHAs have been a critical bottleneck for deploying web agents in\nreal-world applications, often blocking them from completing end-to-end\nautomation tasks. While modern multimodal LLM agents have demonstrated\nimpressive performance in static perception tasks, their ability to handle\ninteractive, multi-step reasoning challenges like CAPTCHAs is largely untested.\nTo address this gap, we introduce Open CaptchaWorld, the first web-based\nbenchmark and platform specifically designed to evaluate the visual reasoning\nand interaction capabilities of MLLM-powered agents through diverse and dynamic\nCAPTCHA puzzles. Our benchmark spans 20 modern CAPTCHA types, totaling 225\nCAPTCHAs, annotated with a new metric we propose: CAPTCHA Reasoning Depth,\nwhich quantifies the number of cognitive and motor steps required to solve each\npuzzle. Experimental results show that humans consistently achieve near-perfect\nscores, state-of-the-art MLLM agents struggle significantly, with success rates\nat most 40.0% by Browser-Use Openai-o3, far below human-level performance,\n93.3%. This highlights Open CaptchaWorld as a vital benchmark for diagnosing\nthe limits of current multimodal agents and guiding the development of more\nrobust multimodal reasoning systems. Code and Data are available at this https\nURL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24878.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653cb809b424289c5f384a02",
            "avatarUrl": "/avatars/a1565ab5ae51075c75d6857d64c426a8.svg",
            "fullname": "YaxinLuo",
            "name": "YaxinLuo",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24871",
            "authors": [
                {
                    "_id": "683dcb779187578a8ae24286",
                    "name": "Yiqing Liang",
                    "hidden": false
                },
                {
                    "_id": "683dcb779187578a8ae24287",
                    "name": "Jielin Qiu",
                    "hidden": false
                },
                {
                    "_id": "683dcb779187578a8ae24288",
                    "name": "Wenhao Ding",
                    "hidden": false
                },
                {
                    "_id": "683dcb779187578a8ae24289",
                    "name": "Zuxin Liu",
                    "hidden": false
                },
                {
                    "_id": "683dcb779187578a8ae2428a",
                    "name": "James Tompkin",
                    "hidden": false
                },
                {
                    "_id": "683dcb779187578a8ae2428b",
                    "name": "Mengdi Xu",
                    "hidden": false
                },
                {
                    "_id": "683dcb779187578a8ae2428c",
                    "name": "Mengzhou Xia",
                    "hidden": false
                },
                {
                    "_id": "683dcb779187578a8ae2428d",
                    "name": "Zhengzhong Tu",
                    "hidden": false
                },
                {
                    "_id": "683dcb779187578a8ae2428e",
                    "name": "Laixi Shi",
                    "hidden": false
                },
                {
                    "_id": "683dcb779187578a8ae2428f",
                    "name": "Jiacheng Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T17:59:38.000Z",
            "submittedOnDailyAt": "2025-06-02T16:56:56.089Z",
            "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6581d57b283fdb0c8dcb9851",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/elZhAKdRizAiAKtiAWBr3.png",
                "isPro": true,
                "fullname": "Yiqing Liang",
                "user": "yiqingliang",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.",
            "upvotes": 16,
            "discussionId": "683dcb789187578a8ae242be",
            "ai_summary": "A framework for post-training multimodal large language models using reinforcement learning with verifiable rewards introduces a data mixture strategy to enhance general reasoning abilities and benchmark performance.",
            "ai_keywords": [
                "reinforcement learning with verifiable rewards",
                "multimodal large language models",
                "multi-dataset post-training",
                "data mixture problem",
                "online RL learning",
                "RL fine-tuning outcome",
                "mixture prediction strategies"
            ]
        },
        "publishedAt": "2025-05-30T13:59:38.000Z",
        "title": "MoDoMoDo: Multi-Domain Data Mixtures for Multimodal LLM Reinforcement\n  Learning",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na powerful paradigm for post-training large language models (LLMs), achieving\nstate-of-the-art performance on tasks with structured, verifiable answers.\nApplying RLVR to Multimodal LLMs (MLLMs) presents significant opportunities but\nis complicated by the broader, heterogeneous nature of vision-language tasks\nthat demand nuanced visual, logical, and spatial capabilities. As such,\ntraining MLLMs using RLVR on multiple datasets could be beneficial but creates\nchallenges with conflicting objectives from interaction among diverse datasets,\nhighlighting the need for optimal dataset mixture strategies to improve\ngeneralization and reasoning. We introduce a systematic post-training framework\nfor Multimodal LLM RLVR, featuring a rigorous data mixture problem formulation\nand benchmark implementation. Specifically, (1) We developed a multimodal RLVR\nframework for multi-dataset post-training by curating a dataset that contains\ndifferent verifiable vision-language problems and enabling multi-domain online\nRL learning with different verifiable rewards; (2) We proposed a data mixture\nstrategy that learns to predict the RL fine-tuning outcome from the data\nmixture distribution, and consequently optimizes the best mixture.\nComprehensive experiments showcase that multi-domain RLVR training, when\ncombined with mixture prediction strategies, can significantly boost MLLM\ngeneral reasoning capacities. Our best mixture improves the post-trained\nmodel's accuracy on out-of-distribution benchmarks by an average of 5.24%\ncompared to the same model post-trained with uniform data mixture, and by a\ntotal of 20.74% compared to the pre-finetuning baseline.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24871.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6581d57b283fdb0c8dcb9851",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/elZhAKdRizAiAKtiAWBr3.png",
            "fullname": "Yiqing Liang",
            "name": "yiqingliang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23941",
            "authors": [
                {
                    "_id": "683cf4405810d395f0a3788b",
                    "user": {
                        "_id": "6631fd5961a4305e5610d403",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6631fd5961a4305e5610d403/P1Dtxzn-KIbYDDsiw60nr.jpeg",
                        "isPro": true,
                        "fullname": "An Vo",
                        "user": "anvo25",
                        "type": "user"
                    },
                    "name": "An Vo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T13:39:54.864Z",
                    "hidden": false
                },
                {
                    "_id": "683cf4405810d395f0a3788c",
                    "name": "Khai-Nguyen Nguyen",
                    "hidden": false
                },
                {
                    "_id": "683cf4405810d395f0a3788d",
                    "user": {
                        "_id": "6039478ab3ecf716b1a5fd4d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                        "isPro": true,
                        "fullname": "taesiri",
                        "user": "taesiri",
                        "type": "user"
                    },
                    "name": "Mohammad Reza Taesiri",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-02T00:52:37.933Z",
                    "hidden": false
                },
                {
                    "_id": "683cf4405810d395f0a3788e",
                    "user": {
                        "_id": "67a87266e113700e5ee1c3fc",
                        "avatarUrl": "/avatars/ea21f5d239244b810c892333663b5390.svg",
                        "isPro": false,
                        "fullname": "Tường Vy",
                        "user": "tuongvy2603",
                        "type": "user"
                    },
                    "name": "Vy Tuong Dang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T13:39:52.070Z",
                    "hidden": false
                },
                {
                    "_id": "683cf4405810d395f0a3788f",
                    "user": {
                        "_id": "653194a4c8da3465f4701ad1",
                        "avatarUrl": "/avatars/6682164fcaf1d339ce9ac82ba131af5e.svg",
                        "isPro": true,
                        "fullname": "Khai-Nguyen Nguyen",
                        "user": "knguyennguyen",
                        "type": "user"
                    },
                    "name": "Anh Totti Nguyen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-02T00:45:56.803Z",
                    "hidden": false
                },
                {
                    "_id": "683cf4405810d395f0a37890",
                    "name": "Daeyoung Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T18:47:58.000Z",
            "submittedOnDailyAt": "2025-06-02T03:28:19.444Z",
            "title": "Vision Language Models are Biased",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io.",
            "upvotes": 16,
            "discussionId": "683cf4445810d395f0a37983",
            "projectPage": "https://vlmsarebiased.github.io/",
            "githubRepo": "https://github.com/anvo25/vlms-are-biased",
            "ai_summary": "Vision language models exhibit strong biases in counting and identification tasks, demonstrating a failure mode that persist even with additional instructions or context.",
            "ai_keywords": [
                "large language models",
                "vision language models",
                "downstream tasks",
                "popular subjects",
                "accuracy",
                "visual tasks",
                "counting",
                "identification",
                "biases",
                "counterfactual image",
                "automated framework"
            ]
        },
        "publishedAt": "2025-05-29T14:47:58.000Z",
        "title": "Vision Language Models are Biased",
        "summary": "Large language models (LLMs) memorize a vast amount of prior knowledge from\nthe Internet that help them on downstream tasks but also may notoriously sway\ntheir outputs towards wrong or biased answers. In this work, we test how the\nknowledge about popular subjects hurt the accuracy of vision language models\n(VLMs) on standard, objective visual tasks of counting and identification. We\nfind that state-of-the-art VLMs are strongly biased (e.g, unable to recognize a\nfourth stripe has been added to a 3-stripe Adidas logo) scoring an average of\n17.05% accuracy in counting (e.g., counting stripes in an Adidas-like logo)\nacross 7 diverse domains from animals, logos, chess, board games, optical\nillusions, to patterned grids. Insert text (e.g., \"Adidas\") describing the\nsubject name into the counterfactual image further decreases VLM accuracy. The\nbiases in VLMs are so strong that instructing them to double-check their\nresults or rely exclusively on image details to answer improves counting\naccuracy by only +2 points, on average. Our work presents an interesting\nfailure mode in VLMs and an automated framework for testing VLM biases. Code\nand data are available at: vlmsarebiased.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23941.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 85
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21437",
            "authors": [
                {
                    "_id": "683d79b5b84018f0604674a7",
                    "user": {
                        "_id": "6836adaf8ec432fdc7e1c550",
                        "avatarUrl": "/avatars/ca21e0022e43fae7eddcd4b58aa7406f.svg",
                        "isPro": false,
                        "fullname": "Huaijin Pi",
                        "user": "huaijinpi",
                        "type": "user"
                    },
                    "name": "Huaijin Pi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T13:39:28.295Z",
                    "hidden": false
                },
                {
                    "_id": "683d79b5b84018f0604674a8",
                    "user": {
                        "_id": "67e22bdb84513315a91ad112",
                        "avatarUrl": "/avatars/f82ed1b814411f4ac0527cdd2603806e.svg",
                        "isPro": false,
                        "fullname": "Zhi Cen",
                        "user": "anitacen",
                        "type": "user"
                    },
                    "name": "Zhi Cen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T13:39:24.089Z",
                    "hidden": false
                },
                {
                    "_id": "683d79b5b84018f0604674a9",
                    "user": {
                        "_id": "645223fb01d7bd9555ea399a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
                        "isPro": false,
                        "fullname": "Zhiyang Dou",
                        "user": "frankzydou",
                        "type": "user"
                    },
                    "name": "Zhiyang Dou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T13:39:26.083Z",
                    "hidden": false
                },
                {
                    "_id": "683d79b5b84018f0604674aa",
                    "name": "Taku Komura",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6836adaf8ec432fdc7e1c550/dJTT2J8mzVgnRjITHmGmX.mp4"
            ],
            "publishedAt": "2025-05-27T17:11:50.000Z",
            "submittedOnDailyAt": "2025-06-02T08:57:36.397Z",
            "title": "CoDA: Coordinated Diffusion Noise Optimization for Whole-Body\n  Manipulation of Articulated Objects",
            "submittedOnDailyBy": {
                "_id": "6836adaf8ec432fdc7e1c550",
                "avatarUrl": "/avatars/ca21e0022e43fae7eddcd4b58aa7406f.svg",
                "isPro": false,
                "fullname": "Huaijin Pi",
                "user": "huaijinpi",
                "type": "user"
            },
            "summary": "Synthesizing whole-body manipulation of articulated objects, including body\nmotion, hand motion, and object motion, is a critical yet challenging task with\nbroad applications in virtual humans and robotics. The core challenges are\ntwofold. First, achieving realistic whole-body motion requires tight\ncoordination between the hands and the rest of the body, as their movements are\ninterdependent during manipulation. Second, articulated object manipulation\ntypically involves high degrees of freedom and demands higher precision, often\nrequiring the fingers to be placed at specific regions to actuate movable\nparts. To address these challenges, we propose a novel coordinated diffusion\nnoise optimization framework. Specifically, we perform noise-space optimization\nover three specialized diffusion models for the body, left hand, and right\nhand, each trained on its own motion dataset to improve generalization.\nCoordination naturally emerges through gradient flow along the human kinematic\nchain, allowing the global body posture to adapt in response to hand motion\nobjectives with high fidelity. To further enhance precision in hand-object\ninteraction, we adopt a unified representation based on basis point sets (BPS),\nwhere end-effector positions are encoded as distances to the same BPS used for\nobject geometry. This unified representation captures fine-grained spatial\nrelationships between the hand and articulated object parts, and the resulting\ntrajectories serve as targets to guide the optimization of diffusion noise,\nproducing highly accurate interaction motion. We conduct extensive experiments\ndemonstrating that our method outperforms existing approaches in motion quality\nand physical plausibility, and enables various capabilities such as object pose\ncontrol, simultaneous walking and manipulation, and whole-body generation from\nhand-only data.",
            "upvotes": 16,
            "discussionId": "683d79bab84018f060467664",
            "projectPage": "https://phj128.github.io/page/CoDA/index.html",
            "githubRepo": "https://github.com/phj128/CoDA",
            "ai_summary": "A coordinated diffusion noise optimization framework improves whole-body manipulation of articulated objects by leveraging specialized diffusion models for body and hand motions and a unified basis point set representation for precise hand-object interaction.",
            "ai_keywords": [
                "diffusion noise optimization",
                "diffusion models",
                "noise-space optimization",
                "human kinematic chain",
                "basis point sets",
                "motion quality",
                "physical plausibility",
                "object pose control",
                "simultaneous walking and manipulation",
                "whole-body generation"
            ]
        },
        "publishedAt": "2025-05-27T13:11:50.000Z",
        "title": "CoDA: Coordinated Diffusion Noise Optimization for Whole-Body\n  Manipulation of Articulated Objects",
        "summary": "Synthesizing whole-body manipulation of articulated objects, including body\nmotion, hand motion, and object motion, is a critical yet challenging task with\nbroad applications in virtual humans and robotics. The core challenges are\ntwofold. First, achieving realistic whole-body motion requires tight\ncoordination between the hands and the rest of the body, as their movements are\ninterdependent during manipulation. Second, articulated object manipulation\ntypically involves high degrees of freedom and demands higher precision, often\nrequiring the fingers to be placed at specific regions to actuate movable\nparts. To address these challenges, we propose a novel coordinated diffusion\nnoise optimization framework. Specifically, we perform noise-space optimization\nover three specialized diffusion models for the body, left hand, and right\nhand, each trained on its own motion dataset to improve generalization.\nCoordination naturally emerges through gradient flow along the human kinematic\nchain, allowing the global body posture to adapt in response to hand motion\nobjectives with high fidelity. To further enhance precision in hand-object\ninteraction, we adopt a unified representation based on basis point sets (BPS),\nwhere end-effector positions are encoded as distances to the same BPS used for\nobject geometry. This unified representation captures fine-grained spatial\nrelationships between the hand and articulated object parts, and the resulting\ntrajectories serve as targets to guide the optimization of diffusion noise,\nproducing highly accurate interaction motion. We conduct extensive experiments\ndemonstrating that our method outperforms existing approaches in motion quality\nand physical plausibility, and enables various capabilities such as object pose\ncontrol, simultaneous walking and manipulation, and whole-body generation from\nhand-only data.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6836adaf8ec432fdc7e1c550/dJTT2J8mzVgnRjITHmGmX.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21437.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6836adaf8ec432fdc7e1c550",
            "avatarUrl": "/avatars/ca21e0022e43fae7eddcd4b58aa7406f.svg",
            "fullname": "Huaijin Pi",
            "name": "huaijinpi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23009",
            "authors": [
                {
                    "_id": "683916c60df60182c0dee89d",
                    "user": {
                        "_id": "66958c29d4ca2767b9c41005",
                        "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
                        "isPro": true,
                        "fullname": "Ruskin Raj Manku",
                        "user": "ruskinmanku",
                        "type": "user"
                    },
                    "name": "Ruskin Raj Manku",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:47:12.390Z",
                    "hidden": false
                },
                {
                    "_id": "683916c60df60182c0dee89e",
                    "name": "Yuzhi Tang",
                    "hidden": false
                },
                {
                    "_id": "683916c60df60182c0dee89f",
                    "name": "Xingjian Shi",
                    "hidden": false
                },
                {
                    "_id": "683916c60df60182c0dee8a0",
                    "name": "Mu Li",
                    "hidden": false
                },
                {
                    "_id": "683916c60df60182c0dee8a1",
                    "name": "Alex Smola",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T02:36:24.000Z",
            "submittedOnDailyAt": "2025-06-02T01:24:21.995Z",
            "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge",
            "submittedOnDailyBy": {
                "_id": "66958c29d4ca2767b9c41005",
                "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
                "isPro": true,
                "fullname": "Ruskin Raj Manku",
                "user": "ruskinmanku",
                "type": "user"
            },
            "summary": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on EmergentTTS, we\nintroduce EmergentTTS-Eval, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\nhttps://github.com/boson-ai/EmergentTTS-Eval-public{code} and the\nhttps://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.",
            "upvotes": 14,
            "discussionId": "683916c70df60182c0dee8dc",
            "ai_summary": "A comprehensive TTS benchmark, EmergentTTS-Eval, automates test-case generation and evaluation using LLMs and LALM to assess nuanced and semantically complex text in speech outputs.",
            "ai_keywords": [
                "EmergentTTS-Eval",
                "LLMs",
                "Large Audio Language Model (LALM)",
                "expressed emotion",
                "prosodic",
                "intonational",
                "pronunciation accuracy",
                "TTS systems",
                "model-as-a-judge"
            ]
        },
        "publishedAt": "2025-05-28T22:36:24.000Z",
        "title": "EmergentTTS-Eval: Evaluating TTS Models on Complex Prosodic,\n  Expressiveness, and Linguistic Challenges Using Model-as-a-Judge",
        "summary": "Text-to-Speech (TTS) benchmarks often fail to capture how well models handle\nnuanced and semantically complex text. Building on EmergentTTS, we\nintroduce EmergentTTS-Eval, a comprehensive benchmark covering six\nchallenging TTS scenarios: emotions, paralinguistics, foreign words, syntactic\ncomplexity, complex pronunciation (e.g. URLs, formulas), and questions.\nCrucially, our framework automates both test-case generation and evaluation,\nmaking the benchmark easily extensible. Starting from a small set of\nhuman-written seed prompts, we iteratively extend them using LLMs to target\nspecific structural, phonetic and prosodic challenges, resulting in 1,645\ndiverse test cases. Moreover, we employ a model-as-a-judge approach, using a\nLarge Audio Language Model (LALM) to assess the speech across multiple\ndimensions such as expressed emotion, prosodic, intonational, and pronunciation\naccuracy. We evaluate state-of-the-art open-source and proprietary TTS systems,\nsuch as 11Labs, Deepgram, and OpenAI's 4o-mini-TTS, on EmergentTTS-Eval,\ndemonstrating its ability to reveal fine-grained performance differences.\nResults show that the model-as-a-judge approach offers robust TTS assessment\nand a high correlation with human preferences. We open source the evaluation\nhttps://github.com/boson-ai/EmergentTTS-Eval-public{code} and the\nhttps://huggingface.co/datasets/bosonai/EmergentTTS-Eval{dataset}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23009.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66958c29d4ca2767b9c41005",
            "avatarUrl": "/avatars/c81b65c1ad345c48c252773ea78b7607.svg",
            "fullname": "Ruskin Raj Manku",
            "name": "ruskinmanku",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24196",
            "authors": [
                {
                    "_id": "683d29da83edd521f116444c",
                    "name": "Longze Chen",
                    "hidden": false
                },
                {
                    "_id": "683d29da83edd521f116444d",
                    "name": "Renke Shan",
                    "hidden": false
                },
                {
                    "_id": "683d29da83edd521f116444e",
                    "name": "Huiming Wang",
                    "hidden": false
                },
                {
                    "_id": "683d29da83edd521f116444f",
                    "user": {
                        "_id": "67ffb89774193e65a77cd914",
                        "avatarUrl": "/avatars/4302f06bcb1fa4c372b4d675459c4068.svg",
                        "isPro": false,
                        "fullname": "Wang Lu",
                        "user": "lwzzfzl",
                        "type": "user"
                    },
                    "name": "Lu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T13:39:48.296Z",
                    "hidden": false
                },
                {
                    "_id": "683d29da83edd521f1164450",
                    "name": "Ziqiang Liu",
                    "hidden": false
                },
                {
                    "_id": "683d29da83edd521f1164451",
                    "name": "Run Luo",
                    "hidden": false
                },
                {
                    "_id": "683d29da83edd521f1164452",
                    "name": "Jiawei Wang",
                    "hidden": false
                },
                {
                    "_id": "683d29da83edd521f1164453",
                    "name": "Hamid Alinejad-Rokny",
                    "hidden": false
                },
                {
                    "_id": "683d29da83edd521f1164454",
                    "name": "Min Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T04:15:06.000Z",
            "submittedOnDailyAt": "2025-06-02T03:23:19.536Z",
            "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding",
            "submittedOnDailyBy": {
                "_id": "64c7b4d1c547ed5243c07b6c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
                "isPro": false,
                "fullname": "Longze Chen",
                "user": "lzchen2001",
                "type": "user"
            },
            "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text.",
            "upvotes": 13,
            "discussionId": "683d29db83edd521f1164482",
            "ai_summary": "CLaSp, an in-context layer-skipping strategy for self-speculative decoding, accelerates Large Language Model decoding without additional modules or training, achieving a 1.3x to 1.7x speedup on LLaMA3 models.",
            "ai_keywords": [
                "speculative decoding",
                "large language models",
                "draft model",
                "verify model",
                "in-context layer-skipping",
                "dynamic programming algorithm",
                "hidden states",
                "verification stage"
            ]
        },
        "publishedAt": "2025-05-30T00:15:06.000Z",
        "title": "CLaSp: In-Context Layer Skip for Self-Speculative Decoding",
        "summary": "Speculative decoding (SD) is a promising method for accelerating the decoding\nprocess of Large Language Models (LLMs). The efficiency of SD primarily hinges\non the consistency between the draft model and the verify model. However,\nexisting drafting approaches typically require additional modules to be\ntrained, which can be challenging to implement and ensure compatibility across\nvarious LLMs. In this paper, we propose CLaSp, an in-context layer-skipping\nstrategy for self-speculative decoding. Unlike prior methods, CLaSp does not\nrequire additional drafting modules or extra training. Instead, it employs a\nplug-and-play mechanism by skipping intermediate layers of the verify model to\nconstruct a compressed draft model. Specifically, we develop a dynamic\nprogramming algorithm that optimizes the layer-skipping process by leveraging\nthe complete hidden states from the last verification stage as an objective.\nThis enables CLaSp to dynamically adjust its layer-skipping strategy after each\nverification stage, without relying on pre-optimized sets of skipped layers.\nExperimental results across diverse downstream tasks demonstrate that CLaSp\nachieves a speedup of 1.3x ~ 1.7x on LLaMA3 series models without altering the\noriginal distribution of the generated text.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24196.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "64c7b4d1c547ed5243c07b6c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c7b4d1c547ed5243c07b6c/h96CLBj6dcm01soK2UJzr.jpeg",
            "fullname": "Longze Chen",
            "name": "lzchen2001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24858",
            "authors": [
                {
                    "_id": "683d2a3651706d12b2cc8ace",
                    "name": "Gabrielle Kaili-May Liu",
                    "hidden": false
                },
                {
                    "_id": "683d2a3651706d12b2cc8acf",
                    "name": "Gal Yona",
                    "hidden": false
                },
                {
                    "_id": "683d2a3651706d12b2cc8ad0",
                    "name": "Avi Caciularu",
                    "hidden": false
                },
                {
                    "_id": "683d2a3651706d12b2cc8ad1",
                    "name": "Idan Szpektor",
                    "hidden": false
                },
                {
                    "_id": "683d2a3651706d12b2cc8ad2",
                    "name": "Tim G. J. Rudner",
                    "hidden": false
                },
                {
                    "_id": "683d2a3651706d12b2cc8ad3",
                    "name": "Arman Cohan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T17:54:08.000Z",
            "submittedOnDailyAt": "2025-06-02T03:13:37.735Z",
            "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
            "submittedOnDailyBy": {
                "_id": "64f1ca1d5b8a6a5d39d75771",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
                "isPro": false,
                "fullname": "John Chih Liu",
                "user": "johncliu",
                "type": "user"
            },
            "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of faithful confidence calibration of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\nfaithfully reflect their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
            "upvotes": 12,
            "discussionId": "683d2a3751706d12b2cc8b0a",
            "ai_summary": "A study reveals that Large Language Models (LLMs) struggle with expressing uncertainty accurately and introduces MetaFaith, a prompt-based method that enhances their calibration significantly.",
            "ai_keywords": [
                "faithful confidence calibration",
                "linguistic expressions of uncertainty",
                "intrinsic uncertainty",
                "prompting strategies",
                "metacognition"
            ]
        },
        "publishedAt": "2025-05-30T13:54:08.000Z",
        "title": "MetaFaith: Faithful Natural Language Uncertainty Expression in LLMs",
        "summary": "A critical component in the trustworthiness of LLMs is reliable uncertainty\ncommunication, yet LLMs often use assertive language when conveying false\nclaims, leading to over-reliance and eroded trust. We present the first\nsystematic study of faithful confidence calibration of LLMs,\nbenchmarking models' ability to use linguistic expressions of uncertainty that\nfaithfully reflect their intrinsic uncertainty, across a\ncomprehensive array of models, datasets, and prompting strategies. Our results\ndemonstrate that LLMs largely fail at this task, and that existing\ninterventions are insufficient: standard prompt approaches provide only\nmarginal gains, and existing, factuality-based calibration techniques can even\nharm faithful calibration. To address this critical gap, we introduce\nMetaFaith, a novel prompt-based calibration approach inspired by human\nmetacognition. We show that MetaFaith robustly improves faithful calibration\nacross diverse models and task domains, enabling up to 61% improvement in\nfaithfulness and achieving an 83% win rate over original generations as judged\nby humans.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24858.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f1ca1d5b8a6a5d39d75771",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64f1ca1d5b8a6a5d39d75771/Caq_Ahp7Qkm1nHyhBcztE.jpeg",
            "fullname": "John Chih Liu",
            "name": "johncliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24785",
            "authors": [
                {
                    "_id": "683dfe8b4acfa22520c6a9ed",
                    "name": "Patrick Tser Jern Kon",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9ee",
                    "name": "Jiachen Liu",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9ef",
                    "name": "Xinyi Zhu",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f0",
                    "name": "Qiuyi Ding",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f1",
                    "name": "Jingjia Peng",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f2",
                    "name": "Jiarong Xing",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f3",
                    "name": "Yibo Huang",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f4",
                    "name": "Yiming Qiu",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f5",
                    "name": "Jayanth Srinivasa",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f6",
                    "name": "Myungjin Lee",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f7",
                    "name": "Mosharaf Chowdhury",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f8",
                    "name": "Matei Zaharia",
                    "hidden": false
                },
                {
                    "_id": "683dfe8b4acfa22520c6a9f9",
                    "name": "Ang Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/648fc22019e7511674b31f12/NQsfxORQkdLT7SJ0OwTDp.png"
            ],
            "publishedAt": "2025-05-30T16:46:29.000Z",
            "submittedOnDailyAt": "2025-06-02T18:14:37.764Z",
            "title": "EXP-Bench: Can AI Conduct AI Research Experiments?",
            "submittedOnDailyBy": {
                "_id": "648fc22019e7511674b31f12",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648fc22019e7511674b31f12/9kRR00GMFYcuj6zR0BVfx.jpeg",
                "isPro": false,
                "fullname": "Amber",
                "user": "AmberLJC",
                "type": "user"
            },
            "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.",
            "upvotes": 12,
            "discussionId": "683dfe8d4acfa22520c6aa4f",
            "githubRepo": "https://github.com/Just-Curieous/Curie",
            "ai_summary": "EXP-Bench evaluates AI agents' end-to-end research experiment capabilities through curated tasks from top AI papers, highlighting current limitations.",
            "ai_keywords": [
                "LLM-based agents",
                "OpenHands",
                "IterativeAgent",
                "end-to-end experimentation"
            ]
        },
        "publishedAt": "2025-05-30T12:46:29.000Z",
        "title": "EXP-Bench: Can AI Conduct AI Research Experiments?",
        "summary": "Automating AI research holds immense potential for accelerating scientific\nprogress, yet current AI agents struggle with the complexities of rigorous,\nend-to-end experimentation. We introduce EXP-Bench, a novel benchmark designed\nto systematically evaluate AI agents on complete research experiments sourced\nfrom influential AI publications. Given a research question and incomplete\nstarter code, EXP-Bench challenges AI agents to formulate hypotheses, design\nand implement experimental procedures, execute them, and analyze results. To\nenable the creation of such intricate and authentic tasks with high-fidelity,\nwe design a semi-autonomous pipeline to extract and structure crucial\nexperimental details from these research papers and their associated\nopen-source code. With the pipeline, EXP-Bench curated 461 AI research tasks\nfrom 51 top-tier AI research papers. Evaluations of leading LLM-based agents,\nsuch as OpenHands and IterativeAgent on EXP-Bench demonstrate partial\ncapabilities: while scores on individual experimental aspects such as design or\nimplementation correctness occasionally reach 20-35%, the success rate for\ncomplete, executable experiments was a mere 0.5%. By identifying these\nbottlenecks and providing realistic step-by-step experiment procedures,\nEXP-Bench serves as a vital tool for future AI agents to improve their ability\nto conduct AI research experiments. EXP-Bench is open-sourced at\nhttps://github.com/Just-Curieous/Curie/tree/main/benchmark/exp_bench.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648fc22019e7511674b31f12/NQsfxORQkdLT7SJ0OwTDp.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24785.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "648fc22019e7511674b31f12",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648fc22019e7511674b31f12/9kRR00GMFYcuj6zR0BVfx.jpeg",
            "fullname": "Amber",
            "name": "AmberLJC",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24521",
            "authors": [
                {
                    "_id": "683d11d1495f0b58f2fd49a9",
                    "name": "Yang-Tian Sun",
                    "hidden": false
                },
                {
                    "_id": "683d11d1495f0b58f2fd49aa",
                    "name": "Xin Yu",
                    "hidden": false
                },
                {
                    "_id": "683d11d1495f0b58f2fd49ab",
                    "name": "Zehuan Huang",
                    "hidden": false
                },
                {
                    "_id": "683d11d1495f0b58f2fd49ac",
                    "name": "Yi-Hua Huang",
                    "hidden": false
                },
                {
                    "_id": "683d11d1495f0b58f2fd49ad",
                    "name": "Yuan-Chen Guo",
                    "hidden": false
                },
                {
                    "_id": "683d11d1495f0b58f2fd49ae",
                    "name": "Ziyi Yang",
                    "hidden": false
                },
                {
                    "_id": "683d11d1495f0b58f2fd49af",
                    "name": "Yan-Pei Cao",
                    "hidden": false
                },
                {
                    "_id": "683d11d1495f0b58f2fd49b0",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T12:31:59.000Z",
            "submittedOnDailyAt": "2025-06-02T01:25:45.570Z",
            "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation",
            "submittedOnDailyBy": {
                "_id": "6375d136dee28348a9c63cbf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
                "isPro": false,
                "fullname": "zehuan-huang",
                "user": "huanngzh",
                "type": "user"
            },
            "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.",
            "upvotes": 12,
            "discussionId": "683d11d3495f0b58f2fd4a95",
            "projectPage": "https://sunyangtian.github.io/UniGeo-web/",
            "githubRepo": "https://github.com/SunYangtian/UniGeo",
            "ai_summary": "Video generation models leveraging diffusion priors achieve superior global geometric attribute estimation and reconstructions, benefiting from inter-frame consistency and joint training on shared attributes.",
            "ai_keywords": [
                "diffusion models",
                "monocular geometric estimation",
                "depth",
                "normal",
                "camera coordinate system",
                "intrinsic consistency",
                "video generation models",
                "global coordinate system",
                "positional encodings",
                "joint training",
                "static video data",
                "dynamic video scenes"
            ]
        },
        "publishedAt": "2025-05-30T08:31:59.000Z",
        "title": "UniGeo: Taming Video Diffusion for Unified Consistent Geometry\n  Estimation",
        "summary": "Recently, methods leveraging diffusion model priors to assist monocular\ngeometric estimation (e.g., depth and normal) have gained significant attention\ndue to their strong generalization ability. However, most existing works focus\non estimating geometric properties within the camera coordinate system of\nindividual video frames, neglecting the inherent ability of diffusion models to\ndetermine inter-frame correspondence. In this work, we demonstrate that,\nthrough appropriate design and fine-tuning, the intrinsic consistency of video\ngeneration models can be effectively harnessed for consistent geometric\nestimation. Specifically, we 1) select geometric attributes in the global\ncoordinate system that share the same correspondence with video frames as the\nprediction targets, 2) introduce a novel and efficient conditioning method by\nreusing positional encodings, and 3) enhance performance through joint training\non multiple geometric attributes that share the same correspondence. Our\nresults achieve superior performance in predicting global geometric attributes\nin videos and can be directly applied to reconstruction tasks. Even when\ntrained solely on static video data, our approach exhibits the potential to\ngeneralize to dynamic video scenes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24521.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6375d136dee28348a9c63cbf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
            "fullname": "zehuan-huang",
            "name": "huanngzh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 32
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24850",
            "authors": [
                {
                    "_id": "683d0ffbe41c42faceda19b2",
                    "user": {
                        "_id": "6587e5a4b2177de3967ff434",
                        "avatarUrl": "/avatars/f2dfbc44eb2bff8d8d66d26db8539708.svg",
                        "isPro": false,
                        "fullname": "Shuyao Xu",
                        "user": "Tim-Xu",
                        "type": "user"
                    },
                    "name": "Shuyao Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:40:56.229Z",
                    "hidden": false
                },
                {
                    "_id": "683d0ffbe41c42faceda19b3",
                    "name": "Cheng Peng",
                    "hidden": false
                },
                {
                    "_id": "683d0ffbe41c42faceda19b4",
                    "name": "Jiangxuan Long",
                    "hidden": false
                },
                {
                    "_id": "683d0ffbe41c42faceda19b5",
                    "name": "Weidi Xu",
                    "hidden": false
                },
                {
                    "_id": "683d0ffbe41c42faceda19b6",
                    "name": "Wei Chu",
                    "hidden": false
                },
                {
                    "_id": "683d0ffbe41c42faceda19b7",
                    "name": "Yuan Qi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T17:47:17.000Z",
            "submittedOnDailyAt": "2025-06-02T02:07:38.924Z",
            "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "66e83ec5deb449d8d856e78d",
                "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
                "isPro": false,
                "fullname": "Tongyan Hu",
                "user": "entropyhu",
                "type": "user"
            },
            "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
            "upvotes": 9,
            "discussionId": "683d0ffce41c42faceda19da",
            "githubRepo": "https://github.com/Tim-Siu/reinforcement-distillation",
            "ai_summary": "Reinforcement Distillation (REDI) leverages both positive and negative traces to enhance large language model reasoning performance offline, outperforming traditional methods and achieving state-of-the-art results with limited open data.",
            "ai_keywords": [
                "model distillation",
                "DeepSeek-R1",
                "OpenAI's o1",
                "Reinforcement Distillation (REDI)",
                "Supervised Fine-Tuning (SFT)",
                "REDI objective",
                "DPO",
                "SimPO",
                "mathematical reasoning tasks",
                "MATH-500",
                "Qwen-REDI-1.5B",
                "DeepSeek-R1-Distill-Qwen-1.5B"
            ]
        },
        "publishedAt": "2025-05-30T13:47:17.000Z",
        "title": "Harnessing Negative Signals: Reinforcement Distillation from Teacher\n  Data for LLM Reasoning",
        "summary": "Recent advances in model distillation demonstrate that data from advanced\nreasoning models (e.g., DeepSeek-R1, OpenAI's o1) can effectively transfer\ncomplex reasoning abilities to smaller, efficient student models. However,\nstandard practices employ rejection sampling, discarding incorrect reasoning\nexamples -- valuable, yet often underutilized data. This paper addresses the\ncritical question: How can both positive and negative distilled reasoning\ntraces be effectively leveraged to maximize LLM reasoning performance in an\noffline setting? To this end, We propose Reinforcement Distillation (REDI), a\ntwo-stage framework. Stage 1 learns from positive traces via Supervised\nFine-Tuning (SFT). Stage 2 further refines the model using both positive and\nnegative traces through our proposed REDI objective. This novel objective is a\nsimple, reference-free loss function that outperforms established methods like\nDPO and SimPO in this distillation context. Our empirical evaluations\ndemonstrate REDI's superiority over baseline Rejection Sampling SFT or SFT\ncombined with DPO/SimPO on mathematical reasoning tasks. Notably, the\nQwen-REDI-1.5B model, post-trained on just 131k positive and negative examples\nfrom the open Open-R1 dataset, achieves an 83.1% score on MATH-500 (pass@1).\nIts performance matches or surpasses that of DeepSeek-R1-Distill-Qwen-1.5B (a\nmodel post-trained on 800k proprietary data) across various mathematical\nreasoning benchmarks, establishing a new state-of-the-art for 1.5B models\npost-trained offline with openly available data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24850.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66e83ec5deb449d8d856e78d",
            "avatarUrl": "/avatars/c5e56be65fcacb3192ce10ba6d8f48e2.svg",
            "fullname": "Tongyan Hu",
            "name": "entropyhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24417",
            "authors": [
                {
                    "_id": "683d0b6c5810d395f0a9a49e",
                    "name": "Runnan Lu",
                    "hidden": false
                },
                {
                    "_id": "683d0b6c5810d395f0a9a49f",
                    "name": "Yuxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "683d0b6c5810d395f0a9a4a0",
                    "name": "Jailing Liu",
                    "hidden": false
                },
                {
                    "_id": "683d0b6c5810d395f0a9a4a1",
                    "name": "Haifa Wang",
                    "hidden": false
                },
                {
                    "_id": "683d0b6c5810d395f0a9a4a2",
                    "name": "Yiren Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T09:55:39.000Z",
            "submittedOnDailyAt": "2025-06-02T00:55:24.254Z",
            "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering",
            "submittedOnDailyBy": {
                "_id": "64311a95034ecbefddd141ef",
                "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                "isPro": true,
                "fullname": "Yiren Song",
                "user": "yiren98",
                "type": "user"
            },
            "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.",
            "upvotes": 9,
            "discussionId": "683d0b6f5810d395f0a9a57b",
            "ai_summary": "The paper presents EasyText, a multilingual text rendering framework using DiT that enhances rendering precision and visual quality with large datasets.",
            "ai_keywords": [
                "DiT (Diffusion Transformer)",
                "denoising latents",
                "multilingual character tokens",
                "character positioning encoding",
                "position encoding interpolation",
                "synthetic text image dataset",
                "pretraining",
                "fine-tuning",
                "multilingual text rendering",
                "visual quality",
                "layout-aware text integration"
            ]
        },
        "publishedAt": "2025-05-30T05:55:39.000Z",
        "title": "EasyText: Controllable Diffusion Transformer for Multilingual Text\n  Rendering",
        "summary": "Generating accurate multilingual text with diffusion models has long been\ndesired but remains challenging. Recent methods have made progress in rendering\ntext in a single language, but rendering arbitrary languages is still an\nunexplored area. This paper introduces EasyText, a text rendering framework\nbased on DiT (Diffusion Transformer), which connects denoising latents with\nmultilingual character tokens encoded as character tokens. We propose character\npositioning encoding and position encoding interpolation techniques to achieve\ncontrollable and precise text rendering. Additionally, we construct a\nlarge-scale synthetic text image dataset with 1 million multilingual image-text\nannotations as well as a high-quality dataset of 20K annotated images, which\nare used for pretraining and fine-tuning respectively. Extensive experiments\nand evaluations demonstrate the effectiveness and advancement of our approach\nin multilingual text rendering, visual quality, and layout-aware text\nintegration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24417.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64311a95034ecbefddd141ef",
            "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
            "fullname": "Yiren Song",
            "name": "yiren98",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20873",
            "authors": [
                {
                    "_id": "68395a548ead63ba096bba45",
                    "user": {
                        "_id": "6770efb5b673f241332fc4a7",
                        "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
                        "isPro": false,
                        "fullname": "Chaeyoung Jung",
                        "user": "Chae0",
                        "type": "user"
                    },
                    "name": "Chaeyoung Jung",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:46:20.597Z",
                    "hidden": false
                },
                {
                    "_id": "68395a548ead63ba096bba46",
                    "name": "Youngjoon Jang",
                    "hidden": false
                },
                {
                    "_id": "68395a548ead63ba096bba47",
                    "name": "Jongmin Choi",
                    "hidden": false
                },
                {
                    "_id": "68395a548ead63ba096bba48",
                    "name": "Joon Son Chung",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T08:22:56.000Z",
            "submittedOnDailyAt": "2025-06-02T06:33:06.810Z",
            "title": "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6770efb5b673f241332fc4a7",
                "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
                "isPro": false,
                "fullname": "Chaeyoung Jung",
                "user": "Chae0",
                "type": "user"
            },
            "summary": "The goal of this work is to enhance balanced multimodal understanding in\naudio-visual large language models (AV-LLMs) by addressing modality bias\nwithout requiring additional training. In current AV-LLMs, audio and video\nfeatures are typically processed jointly in the decoder. While this strategy\nfacilitates unified multimodal understanding, it may introduce modality bias,\nwhere the model tends to over-rely on one modality due to imbalanced training\nsignals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet\neffective inference-time strategy that requires no additional training or\narchitectural modifications. FMD first performs modality-specific reasoning by\nprocessing audio-only and video-only inputs through the early decoder layers (a\nfork phase), and then merges the resulting hidden states for joint reasoning in\nthe remaining layers (a merge phase). This approach promotes balanced modality\ncontributions and leverages complementary information across modalities. We\nevaluate our method on two representative AV-LLMs, VideoLLaMA2 and\nvideo-SALMONN, using three benchmark datasets. Experimental results demonstrate\nconsistent performance improvements on tasks focused on audio, video, and\ncombined audio-visual reasoning, demonstrating the effectiveness of\ninference-time interventions for robust multimodal understanding.",
            "upvotes": 9,
            "discussionId": "68395a558ead63ba096bba7b",
            "ai_summary": "The Fork-Merge Decoding strategy improves balanced multimodal understanding in audio-visual large language models by separating and then combining modality-specific reasoning.",
            "ai_keywords": [
                "fork-merge decoding",
                "AU-LLMs",
                "modality bias",
                "audio-visual large language models",
                "VideoLLaMA2",
                "video-SALMONN",
                "benchmark datasets",
                "audio-visual reasoning"
            ]
        },
        "publishedAt": "2025-05-27T04:22:56.000Z",
        "title": "Fork-Merge Decoding: Enhancing Multimodal Understanding in Audio-Visual\n  Large Language Models",
        "summary": "The goal of this work is to enhance balanced multimodal understanding in\naudio-visual large language models (AV-LLMs) by addressing modality bias\nwithout requiring additional training. In current AV-LLMs, audio and video\nfeatures are typically processed jointly in the decoder. While this strategy\nfacilitates unified multimodal understanding, it may introduce modality bias,\nwhere the model tends to over-rely on one modality due to imbalanced training\nsignals. To mitigate this, we propose Fork-Merge Decoding (FMD), a simple yet\neffective inference-time strategy that requires no additional training or\narchitectural modifications. FMD first performs modality-specific reasoning by\nprocessing audio-only and video-only inputs through the early decoder layers (a\nfork phase), and then merges the resulting hidden states for joint reasoning in\nthe remaining layers (a merge phase). This approach promotes balanced modality\ncontributions and leverages complementary information across modalities. We\nevaluate our method on two representative AV-LLMs, VideoLLaMA2 and\nvideo-SALMONN, using three benchmark datasets. Experimental results demonstrate\nconsistent performance improvements on tasks focused on audio, video, and\ncombined audio-visual reasoning, demonstrating the effectiveness of\ninference-time interventions for robust multimodal understanding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20873.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6770efb5b673f241332fc4a7",
            "avatarUrl": "/avatars/7aca599492233e6a51c2d6a8f52c644e.svg",
            "fullname": "Chaeyoung Jung",
            "name": "Chae0",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21523",
            "authors": [
                {
                    "_id": "683d8d3ef63376ea8e7bf358",
                    "user": {
                        "_id": "65e71f6bcd3df9b0f6b2678b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e71f6bcd3df9b0f6b2678b/8mdex6eI80TGezot_GGI3.jpeg",
                        "isPro": false,
                        "fullname": "Chengzhi Liu",
                        "user": "LCZZZZ",
                        "type": "user"
                    },
                    "name": "Chengzhi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T13:39:21.988Z",
                    "hidden": false
                },
                {
                    "_id": "683d8d3ef63376ea8e7bf359",
                    "name": "Zhongxing Xu",
                    "hidden": false
                },
                {
                    "_id": "683d8d3ef63376ea8e7bf35a",
                    "name": "Qingyue Wei",
                    "hidden": false
                },
                {
                    "_id": "683d8d3ef63376ea8e7bf35b",
                    "name": "Juncheng Wu",
                    "hidden": false
                },
                {
                    "_id": "683d8d3ef63376ea8e7bf35c",
                    "name": "James Zou",
                    "hidden": false
                },
                {
                    "_id": "683d8d3ef63376ea8e7bf35d",
                    "name": "Xin Eric Wang",
                    "hidden": false
                },
                {
                    "_id": "683d8d3ef63376ea8e7bf35e",
                    "name": "Yuyin Zhou",
                    "hidden": false
                },
                {
                    "_id": "683d8d3ef63376ea8e7bf35f",
                    "name": "Sheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T05:08:40.000Z",
            "submittedOnDailyAt": "2025-06-02T10:10:28.000Z",
            "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in\n  Multimodal Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "65e71f6bcd3df9b0f6b2678b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e71f6bcd3df9b0f6b2678b/8mdex6eI80TGezot_GGI3.jpeg",
                "isPro": false,
                "fullname": "Chengzhi Liu",
                "user": "LCZZZZ",
                "type": "user"
            },
            "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity.",
            "upvotes": 9,
            "discussionId": "683d8d3ff63376ea8e7bf3c2",
            "projectPage": "https://mlrm-halu.github.io/",
            "githubRepo": "https://github.com/MLRM-Halu/MLRM-Halu",
            "ai_summary": "A new metric and benchmark are introduced to evaluate multimodal large language models' ability to maintain visual grounding while performing extended reasoning, revealing that larger models and specific training data types improve this balance.",
            "ai_keywords": [
                "multimodal large language models",
                "extended reasoning chains",
                "hallucination",
                "attention analysis",
                "RH-AUC",
                "perception accuracy",
                "RH-Bench",
                "multimodal tasks",
                "perceptual fidelity"
            ]
        },
        "publishedAt": "2025-05-23T01:08:40.000Z",
        "title": "More Thinking, Less Seeing? Assessing Amplified Hallucination in\n  Multimodal Reasoning Models",
        "summary": "Test-time compute has empowered multimodal large language models to generate\nextended reasoning chains, yielding strong performance on tasks such as\nmultimodal math reasoning. However, this improved reasoning ability often comes\nwith increased hallucination: as generations become longer, models tend to\ndrift away from image-grounded content and rely more heavily on language\npriors. Attention analysis shows that longer reasoning chains lead to reduced\nfocus on visual inputs, which contributes to hallucination. To systematically\nstudy this phenomenon, we introduce RH-AUC, a metric that quantifies how a\nmodel's perception accuracy changes with reasoning length, allowing us to\nevaluate whether the model preserves visual grounding during reasoning. We also\nrelease RH-Bench, a diagnostic benchmark that spans a variety of multimodal\ntasks, designed to assess the trade-off between reasoning ability and\nhallucination. Our analysis reveals that (i) larger models typically achieve a\nbetter balance between reasoning and perception, and (ii) this balance is\ninfluenced more by the types and domains of training data than by its overall\nvolume. These findings underscore the importance of evaluation frameworks that\njointly consider both reasoning quality and perceptual fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21523.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e71f6bcd3df9b0f6b2678b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65e71f6bcd3df9b0f6b2678b/8mdex6eI80TGezot_GGI3.jpeg",
            "fullname": "Chengzhi Liu",
            "name": "LCZZZZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24293",
            "authors": [
                {
                    "_id": "683d01ec446fd0c8ff323010",
                    "user": {
                        "_id": "6658f863ce1b283888625af3",
                        "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
                        "isPro": false,
                        "fullname": "James Golden",
                        "user": "jamesgolden1",
                        "type": "user"
                    },
                    "name": "James R. Golden",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-02T02:38:10.635Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
            ],
            "publishedAt": "2025-05-30T07:08:33.000Z",
            "submittedOnDailyAt": "2025-06-02T01:02:58.980Z",
            "title": "Large Language Models are Locally Linear Mappings",
            "submittedOnDailyBy": {
                "_id": "6658f863ce1b283888625af3",
                "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
                "isPro": false,
                "fullname": "James Golden",
                "user": "jamesgolden1",
                "type": "user"
            },
            "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process.",
            "upvotes": 8,
            "discussionId": "683d01ee446fd0c8ff323087",
            "githubRepo": "https://github.com/jamesgolden1/llms-are-llms/",
            "ai_summary": "LLMs can be approximated as linear systems for inference, offering insights into their internal representations and semantic structures without altering the models or their predictions.",
            "ai_keywords": [
                "large language models (LLMs)",
                "inference operations",
                "linear system",
                "gradient computation",
                "Jacobian",
                "singular value decomposition",
                "low-dimensional subspaces",
                "semantic concepts",
                "attention components",
                "MLP components",
                "locally linear decompositions"
            ]
        },
        "publishedAt": "2025-05-30T03:08:33.000Z",
        "title": "Large Language Models are Locally Linear Mappings",
        "summary": "We demonstrate that the inference operations of several open-weight large\nlanguage models (LLMs) can be mapped to an exactly equivalent linear system for\nan input sequence without modifying the model weights or altering output\npredictions. Extending techniques from image diffusion models that exhibit\nlocal or piecewise linearity, we strategically alter the gradient computation\nwith respect to a given input sequence for a next-token prediction such that\nthe Jacobian of the model nearly exactly reproduces the forward prediction with\na linear system. We demonstrate this approach across models (Llama 3, Gemma 3,\nQwen 3, Phi 4, Mistral Ministral and OLMo 2, up to Llama 3.3 70B Q4) and show\nthrough the singular value decomposition of the detached Jacobian that these\nLLMs operate in extremely low-dimensional subspaces where many of the largest\nsingular vectors decode to concepts related to the most-likely output token.\nThis approach also allows us to examine the operation of each successive layer\n(and its attention and MLP components) as nearly-exact linear systems and\nobserve the emergence of semantic concepts. Despite their expressive power and\nglobal nonlinearity, modern LLMs can be interpreted through nearly-exact\nlocally linear decompositions that provide insights into their internal\nrepresentations and reveal interpretable semantic structures in the next-token\nprediction process.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6658f863ce1b283888625af3/uulwAnV1EYXXSdsC-eDMW.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24293.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6658f863ce1b283888625af3",
            "avatarUrl": "/avatars/e4b2c7df0f398eb68c0566031ceac99e.svg",
            "fullname": "James Golden",
            "name": "jamesgolden1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.13157",
            "authors": [
                {
                    "_id": "683b58eb84fbd4b28d8d891e",
                    "user": {
                        "_id": "6469408ab2321e47d3294414",
                        "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
                        "isPro": false,
                        "fullname": "Yassine El Boudouri",
                        "user": "yelboudouri",
                        "type": "user"
                    },
                    "name": "Yassine El Boudouri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:42:17.658Z",
                    "hidden": false
                },
                {
                    "_id": "683b58eb84fbd4b28d8d891f",
                    "name": "Walter Nuninger",
                    "hidden": false
                },
                {
                    "_id": "683b58eb84fbd4b28d8d8920",
                    "name": "Julian Alvarez",
                    "hidden": false
                },
                {
                    "_id": "683b58eb84fbd4b28d8d8921",
                    "name": "Yvan Peter",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-19T14:18:16.000Z",
            "submittedOnDailyAt": "2025-06-02T07:13:19.898Z",
            "title": "Role-Playing Evaluation for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6469408ab2321e47d3294414",
                "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
                "isPro": false,
                "fullname": "Yassine El Boudouri",
                "user": "yelboudouri",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval",
            "upvotes": 7,
            "discussionId": "683b58ec84fbd4b28d8d8935",
            "githubRepo": "https://github.com/yelboudouri/RPEval",
            "ai_summary": "A benchmark called Role-Playing Eval assesses Large Language Models in role-playing across emotional understanding, decision-making, moral alignment, and in-character consistency.",
            "ai_keywords": [
                "Large Language Models",
                "Role-Playing Eval",
                "emotional understanding",
                "decision-making",
                "moral alignment",
                "in-character consistency"
            ]
        },
        "publishedAt": "2025-05-19T10:18:16.000Z",
        "title": "Role-Playing Evaluation for Large Language Models",
        "summary": "Large Language Models (LLMs) demonstrate a notable capacity for adopting\npersonas and engaging in role-playing. However, evaluating this ability\npresents significant challenges, as human assessments are resource-intensive\nand automated evaluations can be biased. To address this, we introduce\nRole-Playing Eval (RPEval), a novel benchmark designed to assess LLM\nrole-playing capabilities across four key dimensions: emotional understanding,\ndecision-making, moral alignment, and in-character consistency. This article\ndetails the construction of RPEval and presents baseline evaluations. Our code\nand dataset are available at https://github.com/yelboudouri/RPEval",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.13157.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6469408ab2321e47d3294414",
            "avatarUrl": "/avatars/05621d33f9f6337bc66e59d9e81d05ef.svg",
            "fullname": "Yassine El Boudouri",
            "name": "yelboudouri",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24875",
            "authors": [
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6bc",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6bd",
                    "name": "Yunqi Li",
                    "hidden": false
                },
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6be",
                    "user": {
                        "_id": "62d18eb81e36881a57f29bf4",
                        "avatarUrl": "/avatars/104851421b4ee9641daaf15942fa7ea1.svg",
                        "isPro": false,
                        "fullname": "Yif Yang",
                        "user": "Yif29",
                        "type": "user"
                    },
                    "name": "Yifan Yang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-02T15:48:59.450Z",
                    "hidden": false
                },
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6bf",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6c0",
                    "name": "Yuqing Yang",
                    "hidden": false
                },
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6c1",
                    "name": "Dai Qi",
                    "hidden": false
                },
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6c2",
                    "name": "Jianmin Bao",
                    "hidden": false
                },
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6c3",
                    "name": "Dongdong Chen",
                    "hidden": false
                },
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6c4",
                    "name": "Chong Luo",
                    "hidden": false
                },
                {
                    "_id": "683dc7e8dd3bccdc9a2ec6c5",
                    "name": "Lili Qiu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62d18eb81e36881a57f29bf4/UVxJVg0kK6Vu5NcXfrmo2.png",
                "https://cdn-uploads.huggingface.co/production/uploads/62d18eb81e36881a57f29bf4/E_En-2ktpBXWc1ALe9xqK.png"
            ],
            "publishedAt": "2025-05-30T17:59:48.000Z",
            "submittedOnDailyAt": "2025-06-02T14:27:16.632Z",
            "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT\n  and RL",
            "submittedOnDailyBy": {
                "_id": "62d18eb81e36881a57f29bf4",
                "avatarUrl": "/avatars/104851421b4ee9641daaf15942fa7ea1.svg",
                "isPro": false,
                "fullname": "Yif Yang",
                "user": "Yif29",
                "type": "user"
            },
            "summary": "Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen.",
            "upvotes": 5,
            "discussionId": "683dc7ebdd3bccdc9a2ec77a",
            "ai_summary": "A two-stage framework ReasonGen-R1 integrates chain-of-thought reasoning and reinforcement learning to enhance image generation by imbuing models with text-based thinking skills and refining outputs through reward optimization.",
            "ai_keywords": [
                "autoregressive image generator",
                "supervised fine-tuning",
                "reasoning dataset",
                "Group Relative Policy Optimization",
                "model crafted rationales",
                "pretrained vision language model",
                "reward signals",
                "GenEval",
                "DPG",
                "T2I benchmark"
            ]
        },
        "publishedAt": "2025-05-30T13:59:48.000Z",
        "title": "ReasonGen-R1: CoT for Autoregressive Image generation models through SFT\n  and RL",
        "summary": "Although chain-of-thought reasoning and reinforcement learning (RL) have\ndriven breakthroughs in NLP, their integration into generative vision models\nremains underexplored. We introduce ReasonGen-R1, a two-stage framework that\nfirst imbues an autoregressive image generator with explicit text-based\n\"thinking\" skills via supervised fine-tuning on a newly generated reasoning\ndataset of written rationales, and then refines its outputs using Group\nRelative Policy Optimization. To enable the model to reason through text before\ngenerating images, We automatically generate and release a corpus of model\ncrafted rationales paired with visual prompts, enabling controlled planning of\nobject layouts, styles, and scene compositions. Our GRPO algorithm uses reward\nsignals from a pretrained vision language model to assess overall visual\nquality, optimizing the policy in each update. Evaluations on GenEval, DPG, and\nthe T2I benchmark demonstrate that ReasonGen-R1 consistently outperforms strong\nbaselines and prior state-of-the-art models. More: aka.ms/reasongen.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62d18eb81e36881a57f29bf4/UVxJVg0kK6Vu5NcXfrmo2.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62d18eb81e36881a57f29bf4/E_En-2ktpBXWc1ALe9xqK.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24875.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d18eb81e36881a57f29bf4",
            "avatarUrl": "/avatars/104851421b4ee9641daaf15942fa7ea1.svg",
            "fullname": "Yif Yang",
            "name": "Yif29",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24615",
            "authors": [
                {
                    "_id": "683d4295c31058e5bf2e2b0b",
                    "name": "Yan Liu",
                    "hidden": false
                },
                {
                    "_id": "683d4295c31058e5bf2e2b0c",
                    "user": {
                        "_id": "646a11791556443f24b582e9",
                        "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
                        "isPro": false,
                        "fullname": "Zonglin Yang",
                        "user": "ZonglinY",
                        "type": "user"
                    },
                    "name": "Zonglin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:40:15.714Z",
                    "hidden": false
                },
                {
                    "_id": "683d4295c31058e5bf2e2b0d",
                    "name": "Soujanya Poria",
                    "hidden": false
                },
                {
                    "_id": "683d4295c31058e5bf2e2b0e",
                    "name": "Thanh-Son Nguyen",
                    "hidden": false
                },
                {
                    "_id": "683d4295c31058e5bf2e2b0f",
                    "name": "Erik Cambria",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T14:08:13.000Z",
            "submittedOnDailyAt": "2025-06-02T04:51:23.329Z",
            "title": "Harnessing Large Language Models for Scientific Novelty Detection",
            "submittedOnDailyBy": {
                "_id": "646a11791556443f24b582e9",
                "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
                "isPro": false,
                "fullname": "Zonglin Yang",
                "user": "ZonglinY",
                "type": "user"
            },
            "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
            "upvotes": 5,
            "discussionId": "683d4296c31058e5bf2e2b63",
            "ai_summary": "A method utilizing large language models to detect scientific novelty by distilling idea-level knowledge and constructing specialized datasets in marketing and NLP domains.",
            "ai_keywords": [
                "large language models",
                "scientific novelty detection",
                "closure sets",
                "idea retrieval",
                "idea conception",
                "lightweight retriever",
                "knowledge distillation"
            ]
        },
        "publishedAt": "2025-05-30T10:08:13.000Z",
        "title": "Harnessing Large Language Models for Scientific Novelty Detection",
        "summary": "In an era of exponential scientific growth, identifying novel research ideas\nis crucial and challenging in academia. Despite potential, the lack of an\nappropriate benchmark dataset hinders the research of novelty detection. More\nimportantly, simply adopting existing NLP technologies, e.g., retrieving and\nthen cross-checking, is not a one-size-fits-all solution due to the gap between\ntextual similarity and idea conception. In this paper, we propose to harness\nlarge language models (LLMs) for scientific novelty detection (ND), associated\nwith two new datasets in marketing and NLP domains. To construct the\nconsiderate datasets for ND, we propose to extract closure sets of papers based\non their relationship, and then summarize their main ideas based on LLMs. To\ncapture idea conception, we propose to train a lightweight retriever by\ndistilling the idea-level knowledge from LLMs to align ideas with similar\nconception, enabling efficient and accurate idea retrieval for LLM novelty\ndetection. Experiments show our method consistently outperforms others on the\nproposed benchmark datasets for idea retrieval and ND tasks. Codes and data are\navailable at https://anonymous.4open.science/r/NoveltyDetection-10FB/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24615.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646a11791556443f24b582e9",
            "avatarUrl": "/avatars/b54d416ddca2f124492ba51bc4b95fd2.svg",
            "fullname": "Zonglin Yang",
            "name": "ZonglinY",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23844",
            "authors": [
                {
                    "_id": "683d0ac47852d920b7dc3dc5",
                    "name": "Zhenglun Kong",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dc6",
                    "name": "Zheng Zhan",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dc7",
                    "name": "Shiyue Hou",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dc8",
                    "name": "Yifan Gong",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dc9",
                    "name": "Xin Meng",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dca",
                    "name": "Pengwei Sui",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dcb",
                    "name": "Peiyan Dong",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dcc",
                    "name": "Xuan Shen",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dcd",
                    "name": "Zifeng Wang",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dce",
                    "name": "Pu Zhao",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dcf",
                    "name": "Hao Tang",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dd0",
                    "name": "Stratis Ioannidis",
                    "hidden": false
                },
                {
                    "_id": "683d0ac47852d920b7dc3dd1",
                    "name": "Yanzhi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T16:24:50.000Z",
            "submittedOnDailyAt": "2025-06-02T00:52:55.266Z",
            "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation",
            "submittedOnDailyBy": {
                "_id": "5f2c36551ebc8c6ede2f0e53",
                "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
                "isPro": false,
                "fullname": "Tony Kong",
                "user": "TonyK",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration",
            "upvotes": 5,
            "discussionId": "683d0ac57852d920b7dc3e20",
            "projectPage": "https://github.com/ZLKong/LLM_Integration/tree/main",
            "githubRepo": "https://github.com/ZLKong/LLM_Integration/tree/main",
            "ai_summary": "A framework for adaptive selection and dynamic weighted fusion of knowledge from multiple LLMs reduces interference and improves scalability in knowledge aggregation.",
            "ai_keywords": [
                "large language models",
                "fine-tuning",
                "ensemble",
                "weight merging",
                "adaptive selection network",
                "dynamic weighted fusion",
                "feedback-driven loss function",
                "knowledge interference"
            ]
        },
        "publishedAt": "2025-05-28T12:24:50.000Z",
        "title": "Enabling Flexible Multi-LLM Integration for Scalable Knowledge\n  Aggregation",
        "summary": "Large language models (LLMs) have shown remarkable promise but remain\nchallenging to continually improve through traditional finetuning, particularly\nwhen integrating capabilities from other specialized LLMs. Popular methods like\nensemble and weight merging require substantial memory and struggle to adapt to\nchanging data environments. Recent efforts have transferred knowledge from\nmultiple LLMs into a single target model; however, they suffer from\ninterference and degraded performance among tasks, largely due to limited\nflexibility in candidate selection and training pipelines. To address these\nissues, we propose a framework that adaptively selects and aggregates knowledge\nfrom diverse LLMs to build a single, stronger model, avoiding the high memory\noverhead of ensemble and inflexible weight merging. Specifically, we design an\nadaptive selection network that identifies the most relevant source LLMs based\non their scores, thereby reducing knowledge interference. We further propose a\ndynamic weighted fusion strategy that accounts for the inherent strengths of\ncandidate LLMs, along with a feedback-driven loss function that prevents the\nselector from converging on a single subset of sources. Experimental results\ndemonstrate that our method can enable a more stable and scalable knowledge\naggregation process while reducing knowledge interference by up to 50% compared\nto existing approaches. Code is avaliable at\nhttps://github.com/ZLKong/LLM_Integration",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23844.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f2c36551ebc8c6ede2f0e53",
            "avatarUrl": "/avatars/e3ddbd15f50b86958377b5fc2460a57e.svg",
            "fullname": "Tony Kong",
            "name": "TonyK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21864",
            "authors": [
                {
                    "_id": "683b8af5091615f46fabadde",
                    "user": {
                        "_id": "655a50a850b9a14799165d53",
                        "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
                        "isPro": false,
                        "fullname": "Mengda Xu",
                        "user": "mengdaxu",
                        "type": "user"
                    },
                    "name": "Mengda Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:42:10.223Z",
                    "hidden": false
                },
                {
                    "_id": "683b8af5091615f46fabaddf",
                    "name": "Han Zhang",
                    "hidden": false
                },
                {
                    "_id": "683b8af5091615f46fabade0",
                    "name": "Yifan Hou",
                    "hidden": false
                },
                {
                    "_id": "683b8af5091615f46fabade1",
                    "name": "Zhenjia Xu",
                    "hidden": false
                },
                {
                    "_id": "683b8af5091615f46fabade2",
                    "name": "Linxi Fan",
                    "hidden": false
                },
                {
                    "_id": "683b8af5091615f46fabade3",
                    "name": "Manuela Veloso",
                    "hidden": false
                },
                {
                    "_id": "683b8af5091615f46fabade4",
                    "name": "Shuran Song",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655a50a850b9a14799165d53/IaKGx3B7I2pvk3nOzX9s2.mp4"
            ],
            "publishedAt": "2025-05-28T01:25:27.000Z",
            "submittedOnDailyAt": "2025-06-02T06:21:16.447Z",
            "title": "DexUMI: Using Human Hand as the Universal Manipulation Interface for\n  Dexterous Manipulation",
            "submittedOnDailyBy": {
                "_id": "655a50a850b9a14799165d53",
                "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
                "isPro": false,
                "fullname": "Mengda Xu",
                "user": "mengdaxu",
                "type": "user"
            },
            "summary": "We present DexUMI - a data collection and policy learning framework that uses\nthe human hand as the natural interface to transfer dexterous manipulation\nskills to various robot hands. DexUMI includes hardware and software\nadaptations to minimize the embodiment gap between the human hand and various\nrobot hands. The hardware adaptation bridges the kinematics gap using a\nwearable hand exoskeleton. It allows direct haptic feedback in manipulation\ndata collection and adapts human motion to feasible robot hand motion. The\nsoftware adaptation bridges the visual gap by replacing the human hand in video\ndata with high-fidelity robot hand inpainting. We demonstrate DexUMI's\ncapabilities through comprehensive real-world experiments on two different\ndexterous robot hand hardware platforms, achieving an average task success rate\nof 86%.",
            "upvotes": 5,
            "discussionId": "683b8af8091615f46fabaf00",
            "projectPage": "https://dex-umi.github.io/",
            "githubRepo": "https://github.com/real-stanford/DexUMI",
            "ai_summary": "DexUMI framework utilizes a wearable hand exoskeleton and high-fidelity robot hand inpainting to transfer dexterous manipulation skills from human hands to robot hands, achieving high task success rates.",
            "ai_keywords": [
                "wearable hand exoskeleton",
                "haptic feedback",
                "robot hand inpainting",
                "dexterous manipulation",
                "kinematics",
                "visual gap"
            ]
        },
        "publishedAt": "2025-05-27T21:25:27.000Z",
        "title": "DexUMI: Using Human Hand as the Universal Manipulation Interface for\n  Dexterous Manipulation",
        "summary": "We present DexUMI - a data collection and policy learning framework that uses\nthe human hand as the natural interface to transfer dexterous manipulation\nskills to various robot hands. DexUMI includes hardware and software\nadaptations to minimize the embodiment gap between the human hand and various\nrobot hands. The hardware adaptation bridges the kinematics gap using a\nwearable hand exoskeleton. It allows direct haptic feedback in manipulation\ndata collection and adapts human motion to feasible robot hand motion. The\nsoftware adaptation bridges the visual gap by replacing the human hand in video\ndata with high-fidelity robot hand inpainting. We demonstrate DexUMI's\ncapabilities through comprehensive real-world experiments on two different\ndexterous robot hand hardware platforms, achieving an average task success rate\nof 86%.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/655a50a850b9a14799165d53/IaKGx3B7I2pvk3nOzX9s2.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21864.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655a50a850b9a14799165d53",
            "avatarUrl": "/avatars/6bb37ac0b6840771ff7a6a6da4192ace.svg",
            "fullname": "Mengda Xu",
            "name": "mengdaxu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24517",
            "authors": [
                {
                    "_id": "683d3f3100c71614babecb8c",
                    "user": {
                        "_id": "64395702bb7ded0a0fee8889",
                        "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
                        "isPro": false,
                        "fullname": "Yinqi Li",
                        "user": "yinqi",
                        "type": "user"
                    },
                    "name": "Yinqi Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:40:19.037Z",
                    "hidden": false
                },
                {
                    "_id": "683d3f3100c71614babecb8d",
                    "name": "Jiahe Zhao",
                    "hidden": false
                },
                {
                    "_id": "683d3f3100c71614babecb8e",
                    "name": "Hong Chang",
                    "hidden": false
                },
                {
                    "_id": "683d3f3100c71614babecb8f",
                    "name": "Ruibing Hou",
                    "hidden": false
                },
                {
                    "_id": "683d3f3100c71614babecb90",
                    "name": "Shiguang Shan",
                    "hidden": false
                },
                {
                    "_id": "683d3f3100c71614babecb91",
                    "name": "Xilin Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T12:29:38.000Z",
            "submittedOnDailyAt": "2025-06-02T04:55:28.985Z",
            "title": "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP",
            "submittedOnDailyBy": {
                "_id": "64395702bb7ded0a0fee8889",
                "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
                "isPro": false,
                "fullname": "Yinqi Li",
                "user": "yinqi",
                "type": "user"
            },
            "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un^2CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP.",
            "upvotes": 4,
            "discussionId": "683d3f3200c71614babecbe3",
            "githubRepo": "https://github.com/LiYinqi/un2CLIP",
            "ai_summary": "A generative model framework, unCLIP, is inverted to improve CLIP's ability to capture detailed visual information while maintaining text alignment.",
            "ai_keywords": [
                "Contrastive Language-Image Pre-training",
                "CLIP",
                "unCLIP",
                "image generator",
                "image encoding",
                "data distribution",
                "dense-prediction",
                "vision-centric",
                "multimodal",
                "open-vocabulary segmentation",
                "multimodal large language model",
                "MMVP-VLM benchmark"
            ]
        },
        "publishedAt": "2025-05-30T08:29:38.000Z",
        "title": "un^2CLIP: Improving CLIP's Visual Detail Capturing Ability via\n  Inverting unCLIP",
        "summary": "Contrastive Language-Image Pre-training (CLIP) has become a foundation model\nand has been applied to various vision and multimodal tasks. However, recent\nworks indicate that CLIP falls short in distinguishing detailed differences in\nimages and shows suboptimal performance on dense-prediction and vision-centric\nmultimodal tasks. Therefore, this work focuses on improving existing CLIP\nmodels, aiming to capture as many visual details in images as possible. We find\nthat a specific type of generative models, unCLIP, provides a suitable\nframework for achieving our goal. Specifically, unCLIP trains an image\ngenerator conditioned on the CLIP image embedding. In other words, it inverts\nthe CLIP image encoder. Compared to discriminative models like CLIP, generative\nmodels are better at capturing image details because they are trained to learn\nthe data distribution of images. Additionally, the conditional input space of\nunCLIP aligns with CLIP's original image-text embedding space. Therefore, we\npropose to invert unCLIP (dubbed un^2CLIP) to improve the CLIP model. In this\nway, the improved image encoder can gain unCLIP's visual detail capturing\nability while preserving its alignment with the original text encoder\nsimultaneously. We evaluate our improved CLIP across various tasks to which\nCLIP has been applied, including the challenging MMVP-VLM benchmark, the\ndense-prediction open-vocabulary segmentation task, and multimodal large\nlanguage model tasks. Experiments show that un^2CLIP significantly improves\nthe original CLIP and previous CLIP improvement methods. Code and models will\nbe available at https://github.com/LiYinqi/un2CLIP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24517.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64395702bb7ded0a0fee8889",
            "avatarUrl": "/avatars/afe8f9b6de358497b0db8a03f8a3a704.svg",
            "fullname": "Yinqi Li",
            "name": "yinqi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23926",
            "authors": [
                {
                    "_id": "683d33be277ad05e5a672f79",
                    "name": "Xuweiyi Chen",
                    "hidden": false
                },
                {
                    "_id": "683d33be277ad05e5a672f7a",
                    "name": "Wentao Zhou",
                    "hidden": false
                },
                {
                    "_id": "683d33be277ad05e5a672f7b",
                    "name": "Aruni RoyChowdhury",
                    "hidden": false
                },
                {
                    "_id": "683d33be277ad05e5a672f7c",
                    "name": "Zezhou Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T18:21:47.000Z",
            "submittedOnDailyAt": "2025-06-02T03:49:08.677Z",
            "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts",
            "submittedOnDailyBy": {
                "_id": "634632aaac1cb29fb2ac9f14",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
                "isPro": false,
                "fullname": "Xuweiyi Chen",
                "user": "Xuweiyi",
                "type": "user"
            },
            "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.",
            "upvotes": 4,
            "discussionId": "683d33c4277ad05e5a67310e",
            "ai_summary": "Point-MoE, a Mixture-of-Experts architecture, enables large-scale, cross-domain generalization in 3D perception by automatically specializing experts without domain labels.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "Point-MoE",
                "point cloud backbones",
                "3D perception",
                "domain heterogeneity",
                "domain labels",
                "top-k routing",
                "multi-domain baselines"
            ]
        },
        "publishedAt": "2025-05-29T14:21:47.000Z",
        "title": "Point-MoE: Towards Cross-Domain Generalization in 3D Semantic\n  Segmentation via Mixture-of-Experts",
        "summary": "While scaling laws have transformed natural language processing and computer\nvision, 3D point cloud understanding has yet to reach that stage. This can be\nattributed to both the comparatively smaller scale of 3D datasets, as well as\nthe disparate sources of the data itself. Point clouds are captured by diverse\nsensors (e.g., depth cameras, LiDAR) across varied domains (e.g., indoor,\noutdoor), each introducing unique scanning patterns, sampling densities, and\nsemantic biases. Such domain heterogeneity poses a major barrier towards\ntraining unified models at scale, especially under the realistic constraint\nthat domain labels are typically inaccessible at inference time. In this work,\nwe propose Point-MoE, a Mixture-of-Experts architecture designed to enable\nlarge-scale, cross-domain generalization in 3D perception. We show that\nstandard point cloud backbones degrade significantly in performance when\ntrained on mixed-domain data, whereas Point-MoE with a simple top-k routing\nstrategy can automatically specialize experts, even without access to domain\nlabels. Our experiments demonstrate that Point-MoE not only outperforms strong\nmulti-domain baselines but also generalizes better to unseen domains. This work\nhighlights a scalable path forward for 3D understanding: letting the model\ndiscover structure in diverse 3D data, rather than imposing it via manual\ncuration or domain supervision.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23926.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634632aaac1cb29fb2ac9f14",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634632aaac1cb29fb2ac9f14/nGZ2TzKOOcKMAR_NFYKkR.jpeg",
            "fullname": "Xuweiyi Chen",
            "name": "Xuweiyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24869",
            "authors": [
                {
                    "_id": "683dfea7a6815c77ac987005",
                    "name": "Ce Zhang",
                    "hidden": false
                },
                {
                    "_id": "683dfea7a6815c77ac987006",
                    "name": "Yan-Bo Lin",
                    "hidden": false
                },
                {
                    "_id": "683dfea7a6815c77ac987007",
                    "name": "Ziyang Wang",
                    "hidden": false
                },
                {
                    "_id": "683dfea7a6815c77ac987008",
                    "name": "Mohit Bansal",
                    "hidden": false
                },
                {
                    "_id": "683dfea7a6815c77ac987009",
                    "name": "Gedas Bertasius",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T17:59:19.000Z",
            "submittedOnDailyAt": "2025-06-02T18:14:19.466Z",
            "title": "SiLVR: A Simple Language-based Video Reasoning Framework",
            "submittedOnDailyBy": {
                "_id": "6519fd9416f0c073c5c551c3",
                "avatarUrl": "/avatars/597ae2f69e1fa2b4bd32a69d1d999b55.svg",
                "isPro": true,
                "fullname": "Ce Zhang",
                "user": "ceezh",
                "type": "user"
            },
            "summary": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.",
            "upvotes": 3,
            "discussionId": "683dfea7a6815c77ac987029",
            "ai_summary": "SiLVR, a language-based framework, enhances multimodal LLMs' video reasoning by leveraging adaptive token reduction, achieving top results on several benchmarks.",
            "ai_keywords": [
                "Large Language Models",
                "multimodal LLMs",
                "video-language tasks",
                "Language-based Video Reasoning",
                "multisensory inputs",
                "adaptive token reduction",
                "temporal granularity",
                "Video-MME",
                "Video-MMMU",
                "Video-MMLU",
                "CGBench",
                "EgoLife"
            ]
        },
        "publishedAt": "2025-05-30T13:59:19.000Z",
        "title": "SiLVR: A Simple Language-based Video Reasoning Framework",
        "summary": "Recent advances in test-time optimization have led to remarkable reasoning\ncapabilities in Large Language Models (LLMs), enabling them to solve highly\ncomplex problems in math and coding. However, the reasoning capabilities of\nmultimodal LLMs (MLLMs) still significantly lag, especially for complex\nvideo-language tasks. To address this issue, we present SiLVR, a Simple\nLanguage-based Video Reasoning framework that decomposes complex video\nunderstanding into two stages. In the first stage, SiLVR transforms raw video\ninto language-based representations using multisensory inputs, such as short\nclip captions and audio/speech subtitles. In the second stage, language\ndescriptions are fed into a powerful reasoning LLM to solve complex\nvideo-language understanding tasks. To handle long-context multisensory inputs,\nwe use an adaptive token reduction scheme, which dynamically determines the\ntemporal granularity with which to sample the tokens. Our simple, modular, and\ntraining-free video reasoning framework achieves the best-reported results on\nVideo-MME (long), Video-MMMU (comprehension), Video-MMLU, CGBench, and EgoLife.\nFurthermore, our empirical study focused on video reasoning capabilities shows\nthat, despite not being explicitly trained on video, strong reasoning LLMs can\neffectively aggregate multisensory input information from video, speech, and\naudio for complex temporal, causal, long-context, and knowledge acquisition\nreasoning tasks in video. Code is available at https://github.com/CeeZh/SILVR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24869.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6519fd9416f0c073c5c551c3",
            "avatarUrl": "/avatars/597ae2f69e1fa2b4bd32a69d1d999b55.svg",
            "fullname": "Ce Zhang",
            "name": "ceezh",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24189",
            "authors": [
                {
                    "_id": "683da3efae07a09d565d638e",
                    "name": "Orlando Marquez Ayala",
                    "hidden": false
                },
                {
                    "_id": "683da3efae07a09d565d638f",
                    "name": "Patrice Bechard",
                    "hidden": false
                },
                {
                    "_id": "683da3efae07a09d565d6390",
                    "name": "Emily Chen",
                    "hidden": false
                },
                {
                    "_id": "683da3efae07a09d565d6391",
                    "name": "Maggie Baird",
                    "hidden": false
                },
                {
                    "_id": "683da3efae07a09d565d6392",
                    "name": "Jingfei Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T03:59:35.000Z",
            "submittedOnDailyAt": "2025-06-02T11:45:56.958Z",
            "title": "Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code\n  Workflows",
            "submittedOnDailyBy": {
                "_id": "607f060442beb4da0f990182",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f060442beb4da0f990182/j5W2tLyU6JqkaTf3kv66s.jpeg",
                "isPro": false,
                "fullname": "Patrice Bechard",
                "user": "patricebechard",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) such as GPT-4o can handle a wide range of\ncomplex tasks with the right prompt. As per token costs are reduced, the\nadvantages of fine-tuning Small Language Models (SLMs) for real-world\napplications -- faster inference, lower costs -- may no longer be clear. In\nthis work, we present evidence that, for domain-specific tasks that require\nstructured outputs, SLMs still have a quality advantage. We compare fine-tuning\nan SLM against prompting LLMs on the task of generating low-code workflows in\nJSON form. We observe that while a good prompt can yield reasonable results,\nfine-tuning improves quality by 10% on average. We also perform systematic\nerror analysis to reveal model limitations.",
            "upvotes": 3,
            "discussionId": "683da3f0ae07a09d565d63c0",
            "ai_summary": "Fine-tuning Small Language Models yields higher quality for structured output tasks compared to prompting Large Language Models, despite reduced token costs.",
            "ai_keywords": [
                "Large Language Models",
                "GPT-4o",
                "Small Language Models",
                "fine-tuning",
                "low-code workflows",
                "JSON",
                "error analysis"
            ]
        },
        "publishedAt": "2025-05-29T23:59:35.000Z",
        "title": "Fine-Tune an SLM or Prompt an LLM? The Case of Generating Low-Code\n  Workflows",
        "summary": "Large Language Models (LLMs) such as GPT-4o can handle a wide range of\ncomplex tasks with the right prompt. As per token costs are reduced, the\nadvantages of fine-tuning Small Language Models (SLMs) for real-world\napplications -- faster inference, lower costs -- may no longer be clear. In\nthis work, we present evidence that, for domain-specific tasks that require\nstructured outputs, SLMs still have a quality advantage. We compare fine-tuning\nan SLM against prompting LLMs on the task of generating low-code workflows in\nJSON form. We observe that while a good prompt can yield reasonable results,\nfine-tuning improves quality by 10% on average. We also perform systematic\nerror analysis to reveal model limitations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24189.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "607f060442beb4da0f990182",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/607f060442beb4da0f990182/j5W2tLyU6JqkaTf3kv66s.jpeg",
            "fullname": "Patrice Bechard",
            "name": "patricebechard",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20047",
            "authors": [
                {
                    "_id": "683596d32bb7489b50c2831c",
                    "user": {
                        "_id": "61deb0a302496c6d78da4ade",
                        "avatarUrl": "/avatars/1d31be74c0e6c983860d94846b4d3770.svg",
                        "isPro": false,
                        "fullname": "Debargha Ganguly",
                        "user": "Debargha",
                        "type": "user"
                    },
                    "name": "Debargha Ganguly",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:59:32.141Z",
                    "hidden": false
                },
                {
                    "_id": "683596d32bb7489b50c2831d",
                    "name": "Vikash Singh",
                    "hidden": false
                },
                {
                    "_id": "683596d32bb7489b50c2831e",
                    "name": "Sreehari Sankar",
                    "hidden": false
                },
                {
                    "_id": "683596d32bb7489b50c2831f",
                    "name": "Biyao Zhang",
                    "hidden": false
                },
                {
                    "_id": "683596d32bb7489b50c28320",
                    "name": "Xuecen Zhang",
                    "hidden": false
                },
                {
                    "_id": "683596d32bb7489b50c28321",
                    "name": "Srinivasan Iyengar",
                    "hidden": false
                },
                {
                    "_id": "683596d32bb7489b50c28322",
                    "name": "Xiaotian Han",
                    "hidden": false
                },
                {
                    "_id": "683596d32bb7489b50c28323",
                    "name": "Amit Sharma",
                    "hidden": false
                },
                {
                    "_id": "683596d32bb7489b50c28324",
                    "name": "Shivkumar Kalyanaraman",
                    "hidden": false
                },
                {
                    "_id": "683596d32bb7489b50c28325",
                    "name": "Vipin Chaudhary",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/61deb0a302496c6d78da4ade/8DI7DzZsaWROgXPg-f9YV.png"
            ],
            "publishedAt": "2025-05-26T14:34:04.000Z",
            "submittedOnDailyAt": "2025-06-02T11:03:54.493Z",
            "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated\n  Reasoning Tasks",
            "submittedOnDailyBy": {
                "_id": "61deb0a302496c6d78da4ade",
                "avatarUrl": "/avatars/1d31be74c0e6c983860d94846b4d3770.svg",
                "isPro": false,
                "fullname": "Debargha Ganguly",
                "user": "Debargha",
                "type": "user"
            },
            "summary": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline.",
            "upvotes": 3,
            "discussionId": "683596d42bb7489b50c28360",
            "ai_summary": "This research explores uncertainty quantification in large language models for generating formal specifications, introducing a PCFG framework to improve error detection and selective verification.",
            "ai_keywords": [
                "large language models (LLMs)",
                "formal verification",
                "satisfiability modulo theories (SMT)",
                "autoformalization",
                "uncertainty quantification (UQ)",
                "entropy",
                "probabilistic context-free grammar (PCFG)",
                "grammar entropy",
                "AUROC"
            ]
        },
        "publishedAt": "2025-05-26T10:34:04.000Z",
        "title": "Grammars of Formal Uncertainty: When to Trust LLMs in Automated\n  Reasoning Tasks",
        "summary": "Large language models (LLMs) show remarkable promise for democratizing\nautomated reasoning by generating formal specifications. However, a fundamental\ntension exists: LLMs are probabilistic, while formal verification demands\ndeterministic guarantees. This paper addresses this epistemological gap by\ncomprehensively investigating failure modes and uncertainty quantification (UQ)\nin LLM-generated formal artifacts. Our systematic evaluation of five frontier\nLLMs reveals Satisfiability Modulo Theories (SMT) based autoformalization's\ndomain-specific impact on accuracy (from +34.8% on logical tasks to -44.5% on\nfactual ones), with known UQ techniques like the entropy of token probabilities\nfailing to identify these errors. We introduce a probabilistic context-free\ngrammar (PCFG) framework to model LLM outputs, yielding a refined uncertainty\ntaxonomy. We find uncertainty signals are task-dependent (e.g., grammar entropy\nfor logic, AUROC>0.93). Finally, a lightweight fusion of these signals enables\nselective verification, drastically reducing errors (14-100%) with minimal\nabstention, transforming LLM-driven formalization into a reliable engineering\ndiscipline.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61deb0a302496c6d78da4ade/8DI7DzZsaWROgXPg-f9YV.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20047.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61deb0a302496c6d78da4ade",
            "avatarUrl": "/avatars/1d31be74c0e6c983860d94846b4d3770.svg",
            "fullname": "Debargha Ganguly",
            "name": "Debargha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24581",
            "authors": [
                {
                    "_id": "683d7ba30ba4502a904432b3",
                    "name": "Omer Nacar",
                    "hidden": false
                },
                {
                    "_id": "683d7ba30ba4502a904432b4",
                    "name": "Anis Koubaa",
                    "hidden": false
                },
                {
                    "_id": "683d7ba30ba4502a904432b5",
                    "name": "Serry Sibaee",
                    "hidden": false
                },
                {
                    "_id": "683d7ba30ba4502a904432b6",
                    "name": "Yasser Al-Habashi",
                    "hidden": false
                },
                {
                    "_id": "683d7ba30ba4502a904432b7",
                    "name": "Adel Ammar",
                    "hidden": false
                },
                {
                    "_id": "683d7ba30ba4502a904432b8",
                    "name": "Wadii Boulila",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T13:29:03.000Z",
            "submittedOnDailyAt": "2025-06-02T08:55:53.974Z",
            "title": "GATE: General Arabic Text Embedding for Enhanced Semantic Textual\n  Similarity with Matryoshka Representation Learning and Hybrid Loss Training",
            "submittedOnDailyBy": {
                "_id": "628f7a71dd993507cfcbe587",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                "isPro": true,
                "fullname": "Omartificial Intelligence Space",
                "user": "Omartificial-Intelligence-Space",
                "type": "user"
            },
            "summary": "Semantic textual similarity (STS) is a critical task in natural language\nprocessing (NLP), enabling applications in retrieval, clustering, and\nunderstanding semantic relationships between texts. However, research in this\narea for the Arabic language remains limited due to the lack of high-quality\ndatasets and pre-trained models. This scarcity of resources has restricted the\naccurate evaluation and advance of semantic similarity in Arabic text. This\npaper introduces General Arabic Text Embedding (GATE) models that achieve\nstate-of-the-art performance on the Semantic Textual Similarity task within the\nMTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid\nloss training approach with Arabic triplet datasets for Natural Language\nInference, which are essential for enhancing model performance in tasks that\ndemand fine-grained semantic understanding. GATE outperforms larger models,\nincluding OpenAI, with a 20-25% performance improvement on STS benchmarks,\neffectively capturing the unique semantic nuances of Arabic.",
            "upvotes": 2,
            "discussionId": "683d7ba40ba4502a904432dc",
            "ai_summary": "GATE models using Matryoshka Representation Learning and a hybrid loss approach achieve state-of-the-art performance on Arabic Semantic Textual Similarity benchmarks.",
            "ai_keywords": [
                "Matryoshka Representation Learning",
                "hybrid loss training",
                "triplet datasets",
                "Natural Language Inference",
                "Semantic Textual Similarity",
                "STS benchmarks"
            ]
        },
        "publishedAt": "2025-05-30T09:29:03.000Z",
        "title": "GATE: General Arabic Text Embedding for Enhanced Semantic Textual\n  Similarity with Matryoshka Representation Learning and Hybrid Loss Training",
        "summary": "Semantic textual similarity (STS) is a critical task in natural language\nprocessing (NLP), enabling applications in retrieval, clustering, and\nunderstanding semantic relationships between texts. However, research in this\narea for the Arabic language remains limited due to the lack of high-quality\ndatasets and pre-trained models. This scarcity of resources has restricted the\naccurate evaluation and advance of semantic similarity in Arabic text. This\npaper introduces General Arabic Text Embedding (GATE) models that achieve\nstate-of-the-art performance on the Semantic Textual Similarity task within the\nMTEB benchmark. GATE leverages Matryoshka Representation Learning and a hybrid\nloss training approach with Arabic triplet datasets for Natural Language\nInference, which are essential for enhancing model performance in tasks that\ndemand fine-grained semantic understanding. GATE outperforms larger models,\nincluding OpenAI, with a 20-25% performance improvement on STS benchmarks,\neffectively capturing the unique semantic nuances of Arabic.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24581.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628f7a71dd993507cfcbe587",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
            "fullname": "Omartificial Intelligence Space",
            "name": "Omartificial-Intelligence-Space",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 100
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.00073",
            "authors": [
                {
                    "_id": "683e3eb44f5a8c2890e122bf",
                    "name": "Shenzhe Zhu",
                    "hidden": false
                },
                {
                    "_id": "683e3eb44f5a8c2890e122c0",
                    "name": "Jiao Sun",
                    "hidden": false
                },
                {
                    "_id": "683e3eb44f5a8c2890e122c1",
                    "name": "Yi Nian",
                    "hidden": false
                },
                {
                    "_id": "683e3eb44f5a8c2890e122c2",
                    "name": "Tobin South",
                    "hidden": false
                },
                {
                    "_id": "683e3eb44f5a8c2890e122c3",
                    "name": "Alex Pentland",
                    "hidden": false
                },
                {
                    "_id": "683e3eb44f5a8c2890e122c4",
                    "name": "Jiaxin Pei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T17:41:39.000Z",
            "submittedOnDailyAt": "2025-06-02T22:47:02.090Z",
            "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and\n  Transactions in Consumer Markets",
            "submittedOnDailyBy": {
                "_id": "6614243f67d7bfc73afc6b77",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
                "isPro": false,
                "fullname": "Shenzhe Zhu",
                "user": "Chouoftears",
                "type": "user"
            },
            "summary": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents.",
            "upvotes": 2,
            "discussionId": "683e3eb54f5a8c2890e1231d",
            "projectPage": "https://shenzhezhu.github.io/A2A-NT/",
            "githubRepo": "https://github.com/ShenzheZhu/A2A-NT",
            "ai_summary": "LLM agents exhibit varying performance and behavioral anomalies in automated negotiations, highlighting both potential benefits and risks in consumer markets.",
            "ai_keywords": [
                "LLM",
                "AI agents",
                "negotiations",
                "transactions",
                "favorable deals",
                "automation",
                "behavioral anomalies",
                "financial losses"
            ]
        },
        "publishedAt": "2025-05-29T13:41:39.000Z",
        "title": "The Automated but Risky Game: Modeling Agent-to-Agent Negotiations and\n  Transactions in Consumer Markets",
        "summary": "AI agents are increasingly used in consumer-facing applications to assist\nwith tasks such as product search, negotiation, and transaction execution. In\nthis paper, we explore a future scenario where both consumers and merchants\nauthorize AI agents to fully automate negotiations and transactions. We aim to\nanswer two key questions: (1) Do different LLM agents vary in their ability to\nsecure favorable deals for users? (2) What risks arise from fully automating\ndeal-making with AI agents in consumer markets? To address these questions, we\ndevelop an experimental framework that evaluates the performance of various LLM\nagents in real-world negotiation and transaction settings. Our findings reveal\nthat AI-mediated deal-making is an inherently imbalanced game -- different\nagents achieve significantly different outcomes for their users. Moreover,\nbehavioral anomalies in LLMs can result in financial losses for both consumers\nand merchants, such as overspending or accepting unreasonable deals. These\nresults underscore that while automation can improve efficiency, it also\nintroduces substantial risks. Users should exercise caution when delegating\nbusiness decisions to AI agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00073.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6614243f67d7bfc73afc6b77",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6614243f67d7bfc73afc6b77/ifdjAb_BPraEJ2eIySk_K.jpeg",
            "fullname": "Shenzhe Zhu",
            "name": "Chouoftears",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23832",
            "authors": [
                {
                    "_id": "683d67343f97feb881211cf8",
                    "name": "Chaeeun Kim",
                    "hidden": false
                },
                {
                    "_id": "683d67343f97feb881211cf9",
                    "name": "Jinu Lee",
                    "hidden": false
                },
                {
                    "_id": "683d67343f97feb881211cfa",
                    "name": "Wonseok Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T09:02:41.000Z",
            "submittedOnDailyAt": "2025-06-02T07:26:28.412Z",
            "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements\n  Generation",
            "submittedOnDailyBy": {
                "_id": "614c9487cbb5e52274a4024d",
                "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
                "isPro": false,
                "fullname": "Chaeeun Kim",
                "user": "Chaeeun-Kim",
                "type": "user"
            },
            "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%.",
            "upvotes": 2,
            "discussionId": "683d67353f97feb881211d6a",
            "ai_summary": "LegalSearchLM outperforms existing models in retrieving relevant legal cases by incorporating comprehensive reasoning and content generation, demonstrated on LEGAR BENCH, a large-scale Korean LCR benchmark.",
            "ai_keywords": [
                "embedding-based",
                "lexical matching",
                "LEGAR BENCH",
                "LegalSearchLM",
                "legal element reasoning",
                "constrained decoding"
            ]
        },
        "publishedAt": "2025-05-28T05:02:41.000Z",
        "title": "LegalSearchLM: Rethinking Legal Case Retrieval as Legal Elements\n  Generation",
        "summary": "Legal Case Retrieval (LCR), which retrieves relevant cases from a query case,\nis a fundamental task for legal professionals in research and decision-making.\nHowever, existing studies on LCR face two major limitations. First, they are\nevaluated on relatively small-scale retrieval corpora (e.g., 100-55K cases) and\nuse a narrow range of criminal query types, which cannot sufficiently reflect\nthe complexity of real-world legal retrieval scenarios. Second, their reliance\non embedding-based or lexical matching methods often results in limited\nrepresentations and legally irrelevant matches. To address these issues, we\npresent: (1) LEGAR BENCH, the first large-scale Korean LCR benchmark, covering\n411 diverse crime types in queries over 1.2M legal cases; and (2)\nLegalSearchLM, a retrieval model that performs legal element reasoning over the\nquery case and directly generates content grounded in the target cases through\nconstrained decoding. Experimental results show that LegalSearchLM outperforms\nbaselines by 6-20% on LEGAR BENCH, achieving state-of-the-art performance. It\nalso demonstrates strong generalization to out-of-domain cases, outperforming\nnaive generative models trained on in-domain data by 15%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23832.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "614c9487cbb5e52274a4024d",
            "avatarUrl": "/avatars/a923db5ea27c4184ed2ce84738860203.svg",
            "fullname": "Chaeeun Kim",
            "name": "Chaeeun-Kim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20977",
            "authors": [
                {
                    "_id": "683da8dc28c2a6768816306f",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "683da8dc28c2a67688163070",
                    "name": "Jinlong Ma",
                    "hidden": false
                },
                {
                    "_id": "683da8dc28c2a67688163071",
                    "name": "Yongshuai Hou",
                    "hidden": false
                },
                {
                    "_id": "683da8dc28c2a67688163072",
                    "name": "Xuefeng Bai",
                    "hidden": false
                },
                {
                    "_id": "683da8dc28c2a67688163073",
                    "name": "Kehai Chen",
                    "hidden": false
                },
                {
                    "_id": "683da8dc28c2a67688163074",
                    "name": "Yang Xiang",
                    "hidden": false
                },
                {
                    "_id": "683da8dc28c2a67688163075",
                    "name": "Jun Yu",
                    "hidden": false
                },
                {
                    "_id": "683da8dc28c2a67688163076",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T10:07:59.000Z",
            "submittedOnDailyAt": "2025-06-02T12:07:10.160Z",
            "title": "Evaluating and Steering Modality Preferences in Multimodal Large\n  Language Model",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
                "isPro": false,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\non complex tasks with multimodal context. However, it is still understudied\nwhether they exhibit modality preference when processing multimodal contexts.\nTo study this question, we first build a MC\\textsuperscript{2}\nbenchmark under controlled evidence conflict scenarios to systematically\nevaluate modality preference, which is the tendency to favor one modality over\nanother when making decisions based on multimodal conflicting evidence. Our\nextensive evaluation reveals that all 18 tested MLLMs generally demonstrate\nclear modality bias, and modality preference can be influenced by external\ninterventions. An in-depth analysis reveals that the preference direction can\nbe captured within the latent representations of MLLMs. Built on this, we\npropose a probing and steering method based on representation engineering to\nexplicitly control modality preference without additional fine-tuning or\ncarefully crafted prompts. Our method effectively amplifies modality preference\ntoward a desired direction and applies to downstream tasks such as\nhallucination mitigation and multimodal machine translation, yielding promising\nimprovements.",
            "upvotes": 2,
            "discussionId": "683da8dd28c2a676881630c5",
            "ai_summary": "MLLMs exhibit modality bias in multimodal processing, which can be controlled using a representation engineering method to improve tasks like hallucination mitigation and multimodal machine translation.",
            "ai_keywords": [
                "Multimodal large language models",
                "MLLMs",
                "MC\\textsuperscript{2} benchmark",
                "modality preference",
                "modality bias",
                "latent representations",
                "representation engineering",
                "probing",
                "steering method",
                "hallucination mitigation",
                "multimodal machine translation"
            ]
        },
        "publishedAt": "2025-05-27T06:07:59.000Z",
        "title": "Evaluating and Steering Modality Preferences in Multimodal Large\n  Language Model",
        "summary": "Multimodal large language models (MLLMs) have achieved remarkable performance\non complex tasks with multimodal context. However, it is still understudied\nwhether they exhibit modality preference when processing multimodal contexts.\nTo study this question, we first build a MC\\textsuperscript{2}\nbenchmark under controlled evidence conflict scenarios to systematically\nevaluate modality preference, which is the tendency to favor one modality over\nanother when making decisions based on multimodal conflicting evidence. Our\nextensive evaluation reveals that all 18 tested MLLMs generally demonstrate\nclear modality bias, and modality preference can be influenced by external\ninterventions. An in-depth analysis reveals that the preference direction can\nbe captured within the latent representations of MLLMs. Built on this, we\npropose a probing and steering method based on representation engineering to\nexplicitly control modality preference without additional fine-tuning or\ncarefully crafted prompts. Our method effectively amplifies modality preference\ntoward a desired direction and applies to downstream tasks such as\nhallucination mitigation and multimodal machine translation, yielding promising\nimprovements.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20977.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/6OUJ7Hc9T1jXynYH3FGaf.png",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 716
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24782",
            "authors": [
                {
                    "_id": "683d53ace2a7d8d977a00dab",
                    "name": "Max Conti",
                    "hidden": false
                },
                {
                    "_id": "683d53ace2a7d8d977a00dac",
                    "user": {
                        "_id": "60f2e021adf471cbdf8bb660",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
                        "isPro": false,
                        "fullname": "Manuel Faysse",
                        "user": "manu",
                        "type": "user"
                    },
                    "name": "Manuel Faysse",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-02T15:07:26.977Z",
                    "hidden": false
                },
                {
                    "_id": "683d53ace2a7d8d977a00dad",
                    "name": "Gautier Viaud",
                    "hidden": false
                },
                {
                    "_id": "683d53ace2a7d8d977a00dae",
                    "name": "Antoine Bosselut",
                    "hidden": false
                },
                {
                    "_id": "683d53ace2a7d8d977a00daf",
                    "name": "Céline Hudelot",
                    "hidden": false
                },
                {
                    "_id": "683d53ace2a7d8d977a00db0",
                    "name": "Pierre Colombo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/60f2e021adf471cbdf8bb660/ZZ-RlVZ4_0DG50pF008ju.png"
            ],
            "publishedAt": "2025-05-30T16:43:28.000Z",
            "submittedOnDailyAt": "2025-06-02T13:36:39.790Z",
            "title": "Context is Gold to find the Gold Passage: Evaluating and Training\n  Contextual Document Embeddings",
            "submittedOnDailyBy": {
                "_id": "60f2e021adf471cbdf8bb660",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
                "isPro": false,
                "fullname": "Manuel Faysse",
                "user": "manu",
                "type": "user"
            },
            "summary": "A limitation of modern document retrieval embedding methods is that they\ntypically encode passages (chunks) from the same documents independently, often\noverlooking crucial contextual information from the rest of the document that\ncould greatly improve individual chunk representations.\n  In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a\nbenchmark designed to evaluate retrieval models on their ability to leverage\ndocument-wide context. Our results show that state-of-the-art embedding models\nstruggle in retrieval scenarios where context is required. To address this\nlimitation, we propose InSeNT (In-sequence Negative Training), a novel\ncontrastive post-training approach which combined with late chunking pooling\nenhances contextual representation learning while preserving computational\nefficiency. Our method significantly improves retrieval quality on ConTEB\nwithout sacrificing base model performance. We further find chunks embedded\nwith our method are more robust to suboptimal chunking strategies and larger\nretrieval corpus sizes. We open-source all artifacts at\nhttps://github.com/illuin-tech/contextual-embeddings.",
            "upvotes": 1,
            "discussionId": "683d53ace2a7d8d977a00dd0",
            "ai_summary": "A context-aware benchmark and contrastive training method improve document retrieval quality by leveraging full-document context and maintaining computational efficiency.",
            "ai_keywords": [
                "ConTEB",
                "Context-aware Text Embedding Benchmark",
                "InSeNT",
                "In-sequence Negative Training",
                "contrastive post-training",
                "contextual representation learning",
                "chunking strategies",
                "retrieval corpus size"
            ]
        },
        "publishedAt": "2025-05-30T12:43:28.000Z",
        "title": "Context is Gold to find the Gold Passage: Evaluating and Training\n  Contextual Document Embeddings",
        "summary": "A limitation of modern document retrieval embedding methods is that they\ntypically encode passages (chunks) from the same documents independently, often\noverlooking crucial contextual information from the rest of the document that\ncould greatly improve individual chunk representations.\n  In this work, we introduce ConTEB (Context-aware Text Embedding Benchmark), a\nbenchmark designed to evaluate retrieval models on their ability to leverage\ndocument-wide context. Our results show that state-of-the-art embedding models\nstruggle in retrieval scenarios where context is required. To address this\nlimitation, we propose InSeNT (In-sequence Negative Training), a novel\ncontrastive post-training approach which combined with late chunking pooling\nenhances contextual representation learning while preserving computational\nefficiency. Our method significantly improves retrieval quality on ConTEB\nwithout sacrificing base model performance. We further find chunks embedded\nwith our method are more robust to suboptimal chunking strategies and larger\nretrieval corpus sizes. We open-source all artifacts at\nhttps://github.com/illuin-tech/contextual-embeddings.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60f2e021adf471cbdf8bb660/ZZ-RlVZ4_0DG50pF008ju.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24782.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f2e021adf471cbdf8bb660",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654090481550-60f2e021adf471cbdf8bb660.jpeg",
            "fullname": "Manuel Faysse",
            "name": "manu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 143
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24672",
            "authors": [
                {
                    "_id": "683e45b2fce31842c602ec84",
                    "name": "Xiaorui Wu",
                    "hidden": false
                },
                {
                    "_id": "683e45b2fce31842c602ec85",
                    "name": "Xiaofeng Mao",
                    "hidden": false
                },
                {
                    "_id": "683e45b2fce31842c602ec86",
                    "name": "Fei Li",
                    "hidden": false
                },
                {
                    "_id": "683e45b2fce31842c602ec87",
                    "name": "Xin Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e45b2fce31842c602ec88",
                    "name": "Xuanhong Li",
                    "hidden": false
                },
                {
                    "_id": "683e45b2fce31842c602ec89",
                    "name": "Chong Teng",
                    "hidden": false
                },
                {
                    "_id": "683e45b2fce31842c602ec8a",
                    "name": "Donghong Ji",
                    "hidden": false
                },
                {
                    "_id": "683e45b2fce31842c602ec8b",
                    "name": "Zhuang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T15:02:21.000Z",
            "submittedOnDailyAt": "2025-06-02T23:22:40.172Z",
            "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional\n  Diversified Red-Teaming Data Synthesis",
            "submittedOnDailyBy": {
                "_id": "63d159132036e44c44f87a91",
                "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
                "isPro": false,
                "fullname": "Zhuang Li",
                "user": "lizhuang144",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset.",
            "upvotes": 1,
            "discussionId": "683e45b4fce31842c602ecd4"
        },
        "publishedAt": "2025-05-30T11:02:21.000Z",
        "title": "TRIDENT: Enhancing Large Language Model Safety with Tri-Dimensional\n  Diversified Red-Teaming Data Synthesis",
        "summary": "Large Language Models (LLMs) excel in various natural language processing\ntasks but remain vulnerable to generating harmful content or being exploited\nfor malicious purposes. Although safety alignment datasets have been introduced\nto mitigate such risks through supervised fine-tuning (SFT), these datasets\noften lack comprehensive risk coverage. Most existing datasets focus primarily\non lexical diversity while neglecting other critical dimensions. To address\nthis limitation, we propose a novel analysis framework to systematically\nmeasure the risk coverage of alignment datasets across three essential\ndimensions: Lexical Diversity, Malicious Intent, and Jailbreak Tactics. We\nfurther introduce TRIDENT, an automated pipeline that leverages persona-based,\nzero-shot LLM generation to produce diverse and comprehensive instructions\nspanning these dimensions. Each harmful instruction is paired with an ethically\naligned response, resulting in two datasets: TRIDENT-Core, comprising 26,311\nexamples, and TRIDENT-Edge, with 18,773 examples. Fine-tuning Llama 3.1-8B on\nTRIDENT-Edge demonstrates substantial improvements, achieving an average 14.29%\nreduction in Harm Score, and a 20% decrease in Attack Success Rate compared to\nthe best-performing baseline model fine-tuned on the WildBreak dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24672.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d159132036e44c44f87a91",
            "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
            "fullname": "Zhuang Li",
            "name": "lizhuang144",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24119",
            "authors": [
                {
                    "_id": "683daea0f3b72ddf735af295",
                    "name": "Zheng-Xin Yong",
                    "hidden": false
                },
                {
                    "_id": "683daea0f3b72ddf735af296",
                    "name": "Beyza Ermis",
                    "hidden": false
                },
                {
                    "_id": "683daea0f3b72ddf735af297",
                    "name": "Marzieh Fadaee",
                    "hidden": false
                },
                {
                    "_id": "683daea0f3b72ddf735af298",
                    "name": "Stephen H. Bach",
                    "hidden": false
                },
                {
                    "_id": "683daea0f3b72ddf735af299",
                    "user": {
                        "_id": "6544e43b12da508864c38f96",
                        "avatarUrl": "/avatars/76f0cd55b4bf9c03d2686e146c6f795f.svg",
                        "isPro": false,
                        "fullname": "Julia Kreutzer",
                        "user": "JuliaKreutzerCohere",
                        "type": "user"
                    },
                    "name": "Julia Kreutzer",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-02T14:01:05.682Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T01:32:44.000Z",
            "submittedOnDailyAt": "2025-06-02T12:41:20.102Z",
            "title": "The State of Multilingual LLM Safety Research: From Measuring the\n  Language Gap to Mitigating It",
            "submittedOnDailyBy": {
                "_id": "61424bf4f0d914a5f606a823",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
                "isPro": false,
                "fullname": "Yong Zheng-Xin",
                "user": "yongzx",
                "type": "user"
            },
            "summary": "This paper presents a comprehensive analysis of the linguistic diversity of\nLLM safety research, highlighting the English-centric nature of the field.\nThrough a systematic review of nearly 300 publications from 2020--2024 across\nmajor NLP conferences and workshops at *ACL, we identify a significant and\ngrowing language gap in LLM safety research, with even high-resource\nnon-English languages receiving minimal attention. We further observe that\nnon-English languages are rarely studied as a standalone language and that\nEnglish safety research exhibits poor language documentation practice. To\nmotivate future research into multilingual safety, we make several\nrecommendations based on our survey, and we then pose three concrete future\ndirections on safety evaluation, training data generation, and crosslingual\nsafety generalization. Based on our survey and proposed directions, the field\ncan develop more robust, inclusive AI safety practices for diverse global\npopulations.",
            "upvotes": 1,
            "discussionId": "683daea1f3b72ddf735af2d4",
            "ai_summary": "The analysis reveals a significant language gap in LLM safety research, focusing mainly on English, and suggests recommendations for multilingual safety evaluation and crosslingual generalization to enhance global AI safety.",
            "ai_keywords": [
                "linguistic diversity",
                "LLM safety",
                "English-centric",
                "NLP conferences",
                "ACL",
                "language gap",
                "multilingual safety",
                "safety evaluation",
                "training data generation",
                "crosslingual safety generalization"
            ]
        },
        "publishedAt": "2025-05-29T21:32:44.000Z",
        "title": "The State of Multilingual LLM Safety Research: From Measuring the\n  Language Gap to Mitigating It",
        "summary": "This paper presents a comprehensive analysis of the linguistic diversity of\nLLM safety research, highlighting the English-centric nature of the field.\nThrough a systematic review of nearly 300 publications from 2020--2024 across\nmajor NLP conferences and workshops at *ACL, we identify a significant and\ngrowing language gap in LLM safety research, with even high-resource\nnon-English languages receiving minimal attention. We further observe that\nnon-English languages are rarely studied as a standalone language and that\nEnglish safety research exhibits poor language documentation practice. To\nmotivate future research into multilingual safety, we make several\nrecommendations based on our survey, and we then pose three concrete future\ndirections on safety evaluation, training data generation, and crosslingual\nsafety generalization. Based on our survey and proposed directions, the field\ncan develop more robust, inclusive AI safety practices for diverse global\npopulations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24119.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61424bf4f0d914a5f606a823",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61424bf4f0d914a5f606a823/0td8lR4elBaVvJUD9Pojh.png",
            "fullname": "Yong Zheng-Xin",
            "name": "yongzx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23923",
            "authors": [
                {
                    "_id": "683dd20fb1080514bd945374",
                    "name": "Feiteng Fang",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd945375",
                    "name": "Ting-En Lin",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd945376",
                    "name": "Yuchuan Wu",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd945377",
                    "name": "Xiong Liu",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd945378",
                    "name": "Xiang Huang",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd945379",
                    "name": "Dingwei Chen",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd94537a",
                    "name": "Jing Ye",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd94537b",
                    "name": "Haonan Zhang",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd94537c",
                    "name": "Liang Zhu",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd94537d",
                    "name": "Hamid Alinejad-Rokny",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd94537e",
                    "name": "Min Yang",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd94537f",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "683dd20fb1080514bd945380",
                    "name": "Yongbin Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T18:15:18.000Z",
            "submittedOnDailyAt": "2025-06-02T15:04:55.736Z",
            "title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced\n  Role-Playing Language Agents",
            "submittedOnDailyBy": {
                "_id": "634d4284f0a69955f66489f3",
                "avatarUrl": "/avatars/9fac134537e6ef802f26d6d54c0e0bca.svg",
                "isPro": false,
                "fullname": "feitengfang",
                "user": "feltoner",
                "type": "user"
            },
            "summary": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic\nand engaging human-computer interactions. However, traditional reward models\noften struggle with scalability and adapting to subjective conversational\npreferences. We propose ChARM, a Character-based Act-adaptive Reward Model,\naddressing these challenges through two innovations: (1) an act-adaptive margin\nthat significantly enhances learning efficiency and generalizability, and (2) a\nself-evolution mechanism leveraging large-scale unlabeled data to improve\ntraining coverage. Additionally, we introduce RoleplayPref, the first\nlarge-scale preference dataset specifically for RPLAs, featuring 1,108\ncharacters, 13 subcategories, and 16,888 bilingual dialogues, alongside\nRoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%\nimprovement over the conventional Bradley-Terry model in preference rankings.\nFurthermore, applying ChARM-generated rewards to preference learning techniques\n(e.g., direct preference optimization) achieves state-of-the-art results on\nCharacterEval and RoleplayEval. Code and dataset are available at\nhttps://github.com/calubkk/ChARM.",
            "upvotes": 1,
            "discussionId": "683dd210b1080514bd9453b7",
            "ai_summary": "ChARM, a character-focused adaptive reward model, improves preference learning for role-playing language agents by using an act-adaptive margin and self-evolution with unlabeled data, achieving superior results on dedicated benchmarks.",
            "ai_keywords": [
                "Role-Playing Language Agents",
                "ChARM",
                "act-adaptive margin",
                "self-evolution mechanism",
                "RoleplayPref",
                "RoleplayEval",
                "preference learning",
                "direct preference optimization",
                "CharacterEval"
            ]
        },
        "publishedAt": "2025-05-29T14:15:18.000Z",
        "title": "ChARM: Character-based Act-adaptive Reward Modeling for Advanced\n  Role-Playing Language Agents",
        "summary": "Role-Playing Language Agents (RPLAs) aim to simulate characters for realistic\nand engaging human-computer interactions. However, traditional reward models\noften struggle with scalability and adapting to subjective conversational\npreferences. We propose ChARM, a Character-based Act-adaptive Reward Model,\naddressing these challenges through two innovations: (1) an act-adaptive margin\nthat significantly enhances learning efficiency and generalizability, and (2) a\nself-evolution mechanism leveraging large-scale unlabeled data to improve\ntraining coverage. Additionally, we introduce RoleplayPref, the first\nlarge-scale preference dataset specifically for RPLAs, featuring 1,108\ncharacters, 13 subcategories, and 16,888 bilingual dialogues, alongside\nRoleplayEval, a dedicated evaluation benchmark. Experimental results show a 13%\nimprovement over the conventional Bradley-Terry model in preference rankings.\nFurthermore, applying ChARM-generated rewards to preference learning techniques\n(e.g., direct preference optimization) achieves state-of-the-art results on\nCharacterEval and RoleplayEval. Code and dataset are available at\nhttps://github.com/calubkk/ChARM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23923.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634d4284f0a69955f66489f3",
            "avatarUrl": "/avatars/9fac134537e6ef802f26d6d54c0e0bca.svg",
            "fullname": "feitengfang",
            "name": "feltoner",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23856",
            "authors": [
                {
                    "_id": "683df77d4c5b9f381d2f54b9",
                    "name": "Sahil Verma",
                    "hidden": false
                },
                {
                    "_id": "683df77d4c5b9f381d2f54ba",
                    "name": "Keegan Hines",
                    "hidden": false
                },
                {
                    "_id": "683df77d4c5b9f381d2f54bb",
                    "name": "Jeff Bilmes",
                    "hidden": false
                },
                {
                    "_id": "683df77d4c5b9f381d2f54bc",
                    "name": "Charlotte Siska",
                    "hidden": false
                },
                {
                    "_id": "683df77d4c5b9f381d2f54bd",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                },
                {
                    "_id": "683df77d4c5b9f381d2f54be",
                    "name": "Hila Gonen",
                    "hidden": false
                },
                {
                    "_id": "683df77d4c5b9f381d2f54bf",
                    "name": "Chandan Singh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63ba371eefe99543b35701ab/Eez22C_CEXs6XY-pehelK.png"
            ],
            "publishedAt": "2025-05-29T05:25:27.000Z",
            "submittedOnDailyAt": "2025-06-02T17:42:57.389Z",
            "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across\n  Modalities",
            "submittedOnDailyBy": {
                "_id": "63ba371eefe99543b35701ab",
                "avatarUrl": "/avatars/87e700af5b31caa4a2680701a897fbd9.svg",
                "isPro": false,
                "fullname": "Sahil Verma",
                "user": "vsahil",
                "type": "user"
            },
            "summary": "The emerging capabilities of large language models (LLMs) have sparked\nconcerns about their immediate potential for harmful misuse. The core approach\nto mitigate these concerns is the detection of harmful queries to the model.\nCurrent detection approaches are fallible, and are particularly susceptible to\nattacks that exploit mismatched generalization of model capabilities (e.g.,\nprompts in low-resource languages or prompts provided in non-text modalities\nsuch as image and audio). To tackle this challenge, we propose OMNIGUARD, an\napproach for detecting harmful prompts across languages and modalities. Our\napproach (i) identifies internal representations of an LLM/MLLM that are\naligned across languages or modalities and then (ii) uses them to build a\nlanguage-agnostic or modality-agnostic classifier for detecting harmful\nprompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\%\nover the strongest baseline in a multilingual setting, by 20.44\\% for\nimage-based prompts, and sets a new SOTA for audio-based prompts. By\nrepurposing embeddings computed during generation, OMNIGUARD is also very\nefficient (approx 120 times faster than the next fastest baseline). Code\nand data are available at: https://github.com/vsahil/OmniGuard.",
            "upvotes": 1,
            "discussionId": "683df77e4c5b9f381d2f54f4",
            "ai_summary": "OMNIGUARD detects harmful prompts across languages and modalities by identifying aligned internal representations in large language models, achieving high accuracy and efficiency.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "MLLMs",
                "harmful queries",
                "OMNIGUARD",
                "language-agnostic classifier",
                "modality-agnostic classifier",
                "harmful prompts",
                "multilingual setting",
                "image-based prompts",
                "audio-based prompts",
                "embeddings",
                "generation"
            ]
        },
        "publishedAt": "2025-05-29T01:25:27.000Z",
        "title": "OMNIGUARD: An Efficient Approach for AI Safety Moderation Across\n  Modalities",
        "summary": "The emerging capabilities of large language models (LLMs) have sparked\nconcerns about their immediate potential for harmful misuse. The core approach\nto mitigate these concerns is the detection of harmful queries to the model.\nCurrent detection approaches are fallible, and are particularly susceptible to\nattacks that exploit mismatched generalization of model capabilities (e.g.,\nprompts in low-resource languages or prompts provided in non-text modalities\nsuch as image and audio). To tackle this challenge, we propose OMNIGUARD, an\napproach for detecting harmful prompts across languages and modalities. Our\napproach (i) identifies internal representations of an LLM/MLLM that are\naligned across languages or modalities and then (ii) uses them to build a\nlanguage-agnostic or modality-agnostic classifier for detecting harmful\nprompts. OMNIGUARD improves harmful prompt classification accuracy by 11.57\\%\nover the strongest baseline in a multilingual setting, by 20.44\\% for\nimage-based prompts, and sets a new SOTA for audio-based prompts. By\nrepurposing embeddings computed during generation, OMNIGUARD is also very\nefficient (approx 120 times faster than the next fastest baseline). Code\nand data are available at: https://github.com/vsahil/OmniGuard.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ba371eefe99543b35701ab/Eez22C_CEXs6XY-pehelK.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23856.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ba371eefe99543b35701ab",
            "avatarUrl": "/avatars/87e700af5b31caa4a2680701a897fbd9.svg",
            "fullname": "Sahil Verma",
            "name": "vsahil",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21749",
            "authors": [
                {
                    "_id": "683db6ff97f03081fefa4e9f",
                    "name": "M. Reza Ebrahimi",
                    "hidden": false
                },
                {
                    "_id": "683db6ff97f03081fefa4ea0",
                    "name": "Roland Memisevic",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/648a1075e8bee5332919e0ab/WigPmWt705l6ELIwoM-_G.png"
            ],
            "publishedAt": "2025-05-27T20:38:19.000Z",
            "submittedOnDailyAt": "2025-06-02T13:22:57.288Z",
            "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks",
            "submittedOnDailyBy": {
                "_id": "648a1075e8bee5332919e0ab",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648a1075e8bee5332919e0ab/KDzi904zGPuy_PL3gbUYE.jpeg",
                "isPro": false,
                "fullname": "M.Reza Ebrahimi",
                "user": "mamaj92",
                "type": "user"
            },
            "summary": "The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bi-linear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of task that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbi-linear state updates form a natural hierarchy corresponding to state\ntracking tasks of increasing complexity, with popular linear recurrent networks\nsuch as Mamba residing at the lowest-complexity center of that hierarchy.",
            "upvotes": 1,
            "discussionId": "683db70097f03081fefa4ece",
            "ai_summary": "Bilinear operations in recurrent neural networks are shown to be a natural bias for state tracking tasks, forming a hierarchical structure where linear recurrent networks are the simplest form.",
            "ai_keywords": [
                "hidden units",
                "recurrent neural networks",
                "gating mechanisms",
                "bi-linear operations",
                "input embeddings",
                "state tracking tasks",
                "bi-linear state updates",
                "Mamba"
            ]
        },
        "publishedAt": "2025-05-27T16:38:19.000Z",
        "title": "Revisiting Bi-Linear State Transitions in Recurrent Neural Networks",
        "summary": "The role of hidden units in recurrent neural networks is typically seen as\nmodeling memory, with research focusing on enhancing information retention\nthrough gating mechanisms. A less explored perspective views hidden units as\nactive participants in the computation performed by the network, rather than\npassive memory stores. In this work, we revisit bi-linear operations, which\ninvolve multiplicative interactions between hidden units and input embeddings.\nWe demonstrate theoretically and empirically that they constitute a natural\ninductive bias for representing the evolution of hidden states in state\ntracking tasks. These are the simplest type of task that require hidden units\nto actively contribute to the behavior of the network. We also show that\nbi-linear state updates form a natural hierarchy corresponding to state\ntracking tasks of increasing complexity, with popular linear recurrent networks\nsuch as Mamba residing at the lowest-complexity center of that hierarchy.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648a1075e8bee5332919e0ab/WigPmWt705l6ELIwoM-_G.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21749.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648a1075e8bee5332919e0ab",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648a1075e8bee5332919e0ab/KDzi904zGPuy_PL3gbUYE.jpeg",
            "fullname": "M.Reza Ebrahimi",
            "name": "mamaj92",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]