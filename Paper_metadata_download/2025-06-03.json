[
    {
        "paper": {
            "id": "2506.01939",
            "authors": [
                {
                    "_id": "683e7a6d97fd742a8edee1ba",
                    "user": {
                        "_id": "6486dde1f74857df3f1a5828",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
                        "isPro": false,
                        "fullname": "Shenzhi Wang",
                        "user": "shenzhi-wang",
                        "type": "user"
                    },
                    "name": "Shenzhi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:41:16.481Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1bb",
                    "name": "Le Yu",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1bc",
                    "name": "Chang Gao",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1bd",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:41:13.605Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1be",
                    "name": "Shixuan Liu",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1bf",
                    "name": "Rui Lu",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c0",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c1",
                    "user": {
                        "_id": "63f30b870a16587ea970edfe",
                        "avatarUrl": "/avatars/059491b33fecec69032e6d481229ee31.svg",
                        "isPro": false,
                        "fullname": "Xiong-Hui Chen",
                        "user": "xionghuichen",
                        "type": "user"
                    },
                    "name": "Xionghui Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:41:10.358Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c2",
                    "name": "Jianxin Yang",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c3",
                    "user": {
                        "_id": "64704e973601bb7b06643e98",
                        "avatarUrl": "/avatars/52e51f4d1be6769e4397b8be2799cf32.svg",
                        "isPro": false,
                        "fullname": "Zhenru Zhang",
                        "user": "Zhenru",
                        "type": "user"
                    },
                    "name": "Zhenru Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:40:21.027Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c4",
                    "user": {
                        "_id": "666aacfb918ba11c7c598194",
                        "avatarUrl": "/avatars/45bee8f1fdbdd256ee47d25e4bf01a7a.svg",
                        "isPro": false,
                        "fullname": "Yuqiong Liu",
                        "user": "lyq333",
                        "type": "user"
                    },
                    "name": "Yuqiong Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:40:30.240Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c5",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c6",
                    "name": "Andrew Zhao",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c7",
                    "name": "Yang Yue",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c8",
                    "name": "Shiji Song",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1c9",
                    "user": {
                        "_id": "63d9d68c1cae35c27bf7a6a7",
                        "avatarUrl": "/avatars/b5ad98cf269ae5f1fe90861fb4170fae.svg",
                        "isPro": false,
                        "fullname": "Bowen Yu",
                        "user": "Tigerph",
                        "type": "user"
                    },
                    "name": "Bowen Yu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-03T04:30:38.648Z",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1ca",
                    "name": "Gao Huang",
                    "hidden": false
                },
                {
                    "_id": "683e7a6d97fd742a8edee1cb",
                    "user": {
                        "_id": "620760a26e3b7210c2ff1943",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620760a26e3b7210c2ff1943/VC-rKqimF6yxGESNVlPoR.jpeg",
                        "isPro": false,
                        "fullname": "Junyang Lin",
                        "user": "JustinLin610",
                        "type": "user"
                    },
                    "name": "Junyang Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:39:54.278Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T17:54:39.000Z",
            "submittedOnDailyAt": "2025-06-03T03:09:30.655Z",
            "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "6486dde1f74857df3f1a5828",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
                "isPro": false,
                "fullname": "Shenzhi Wang",
                "user": "shenzhi-wang",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.",
            "upvotes": 98,
            "discussionId": "683e7a6e97fd742a8edee227",
            "projectPage": "https://shenzhi-wang.github.io/high-entropy-minority-tokens-rlvr/",
            "ai_summary": "Token entropy patterns are crucial in RLVR, with high-entropy tokens significantly impacting reasoning performance and model optimization.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "Large Language Models",
                "LLMs",
                "token entropy patterns",
                "Chain-of-Thought",
                "CoT reasoning",
                "high-entropy tokens",
                "policy gradient updates",
                "Qwen3-8B",
                "Qwen3-32B",
                "Qwen3-14B",
                "AIME"
            ]
        },
        "publishedAt": "2025-06-02T13:54:39.000Z",
        "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective\n  Reinforcement Learning for LLM Reasoning",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful approach to enhancing the reasoning capabilities of Large Language\nModels (LLMs), while its mechanisms are not yet well understood. In this work,\nwe undertake a pioneering exploration of RLVR through the novel perspective of\ntoken entropy patterns, comprehensively analyzing how different tokens\ninfluence reasoning performance. By examining token entropy patterns in\nChain-of-Thought (CoT) reasoning, we observe that only a small fraction of\ntokens exhibit high entropy, and these tokens act as critical forks that steer\nthe model toward diverse reasoning pathways. Furthermore, studying how entropy\npatterns evolve during RLVR training reveals that RLVR largely adheres to the\nbase model's entropy patterns, primarily adjusting the entropy of high-entropy\ntokens. These findings highlight the significance of high-entropy tokens (i.e.,\nforking tokens) to RLVR. We ultimately improve RLVR by restricting policy\ngradient updates to forking tokens and uncover a finding even beyond the 80/20\nrule: utilizing only 20% of the tokens while maintaining performance comparable\nto full-gradient updates on the Qwen3-8B base model and significantly\nsurpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71\non AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models,\nhighlighting a strong scaling trend. In contrast, training exclusively on the\n80% lowest-entropy tokens leads to a marked decline in performance. These\nfindings indicate that the efficacy of RLVR primarily arises from optimizing\nthe high-entropy tokens that decide reasoning directions. Collectively, our\nresults highlight the potential to understand RLVR through a token-entropy\nperspective and optimize RLVR by leveraging high-entropy minority tokens to\nfurther improve LLM reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01939.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6486dde1f74857df3f1a5828",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6486dde1f74857df3f1a5828/FgE80CpalBO5qqArdfwxA.jpeg",
            "fullname": "Shenzhi Wang",
            "name": "shenzhi-wang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 327
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24760",
            "authors": [
                {
                    "_id": "683e6af92139ea008faa74ba",
                    "user": {
                        "_id": "65144e46004a986ccc9d21d6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
                        "isPro": false,
                        "fullname": "Zafir Stojanovski",
                        "user": "zafstojano",
                        "type": "user"
                    },
                    "name": "Zafir Stojanovski",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-03T06:28:46.378Z",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74bb",
                    "user": {
                        "_id": "6303f5f37b50dd9d0a371b28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6303f5f37b50dd9d0a371b28/H25eCzAYVwBtpSpD8tnUV.jpeg",
                        "isPro": false,
                        "fullname": "Oliver Stanley",
                        "user": "OllieStanley",
                        "type": "user"
                    },
                    "name": "Oliver Stanley",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:45:56.277Z",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74bc",
                    "name": "Joe Sharratt",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74bd",
                    "name": "Richard Jones",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74be",
                    "name": "Abdulhakeem Adefioye",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74bf",
                    "user": {
                        "_id": "6304061c0547362a22a76a17",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661339692442-6304061c0547362a22a76a17.jpeg",
                        "isPro": false,
                        "fullname": "Jean Kaddour",
                        "user": "JeanKaddour",
                        "type": "user"
                    },
                    "name": "Jean Kaddour",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T09:13:54.739Z",
                    "hidden": false
                },
                {
                    "_id": "683e6af92139ea008faa74c0",
                    "name": "Andreas Köpf",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T16:20:18.000Z",
            "submittedOnDailyAt": "2025-06-03T02:02:10.153Z",
            "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards",
            "submittedOnDailyBy": {
                "_id": "65144e46004a986ccc9d21d6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
                "isPro": false,
                "fullname": "Zafir Stojanovski",
                "user": "zafstojano",
                "type": "user"
            },
            "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models.",
            "upvotes": 50,
            "discussionId": "683e6afa2139ea008faa7531",
            "githubRepo": "https://github.com/open-thought/reasoning-gym",
            "ai_summary": "Reasoning Gym provides a library of reasoning environments with verifiable rewards and procedural data generation for reinforcement learning, enabling the evaluation and training of reasoning models at varying difficulty levels.",
            "ai_keywords": [
                "reinforcement learning",
                "verifiable rewards",
                "data generators",
                "verifiers",
                "procedural generation",
                "reasoning models"
            ]
        },
        "publishedAt": "2025-05-30T12:20:18.000Z",
        "title": "REASONING GYM: Reasoning Environments for Reinforcement Learning with\n  Verifiable Rewards",
        "summary": "We introduce Reasoning Gym (RG), a library of reasoning environments for\nreinforcement learning with verifiable rewards. It provides over 100 data\ngenerators and verifiers spanning multiple domains including algebra,\narithmetic, computation, cognition, geometry, graph theory, logic, and various\ncommon games. Its key innovation is the ability to generate virtually infinite\ntraining data with adjustable complexity, unlike most previous reasoning\ndatasets, which are typically fixed. This procedural generation approach allows\nfor continuous evaluation across varying difficulty levels. Our experimental\nresults demonstrate the efficacy of RG in both evaluating and reinforcement\nlearning of reasoning models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24760.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "65144e46004a986ccc9d21d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65144e46004a986ccc9d21d6/oImRY64V-UeYGdrbd_ozU.jpeg",
            "fullname": "Zafir Stojanovski",
            "name": "zafstojano",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01844",
            "authors": [
                {
                    "_id": "683eb85825fcc99d2a7fc26d",
                    "user": {
                        "_id": "62bdeedd01dc22b4d22a371e",
                        "avatarUrl": "/avatars/6adc904fb1e08661d293a966270afabb.svg",
                        "isPro": false,
                        "fullname": "Mustafa Shukor",
                        "user": "mshukor",
                        "type": "user"
                    },
                    "name": "Mustafa Shukor",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:11:09.058Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc26e",
                    "user": {
                        "_id": "640e21ef3c82bd463ee5a76d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640e21ef3c82bd463ee5a76d/nVR1DFPAsiLw6Boys28Rb.jpeg",
                        "isPro": false,
                        "fullname": "Dana Aubakirova",
                        "user": "danaaubakirova",
                        "type": "user"
                    },
                    "name": "Dana Aubakirova",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:11:18.586Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc26f",
                    "user": {
                        "_id": "63d67eac6f49aa8230601996",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d67eac6f49aa8230601996/djvtWdy718whUgh7tu1Ko.jpeg",
                        "isPro": false,
                        "fullname": "Francesco Capuano",
                        "user": "fracapuano",
                        "type": "user"
                    },
                    "name": "Francesco Capuano",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:11:29.639Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc270",
                    "user": {
                        "_id": "65f9d37113336392bad1e49c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f9d37113336392bad1e49c/B0Fxwconnu7lvtjBz4Ruq.jpeg",
                        "isPro": false,
                        "fullname": "Pepijn Kooijmans",
                        "user": "pepijn223",
                        "type": "user"
                    },
                    "name": "Pepijn Kooijmans",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:11:38.693Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc271",
                    "user": {
                        "_id": "67b124b081d4eae18b957606",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/CXvSv2l15uPkMQL_HBRDF.png",
                        "isPro": false,
                        "fullname": "Steven Palma",
                        "user": "imstevenpmwork",
                        "type": "user"
                    },
                    "name": "Steven Palma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:11:47.379Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc272",
                    "user": {
                        "_id": "64c255b2254239173af0570a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c255b2254239173af0570a/JvYNX4gpk0hVQJeJif8Mo.jpeg",
                        "isPro": false,
                        "fullname": "Adil Zouitine",
                        "user": "AdilZtn",
                        "type": "user"
                    },
                    "name": "Adil Zouitine",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:11:56.957Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc273",
                    "user": {
                        "_id": "668bd06dd58b51a628566d80",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/668bd06dd58b51a628566d80/II7Yr5dT5ItMrpoMkQEy3.jpeg",
                        "isPro": false,
                        "fullname": "Michel Aractingi",
                        "user": "aractingi",
                        "type": "user"
                    },
                    "name": "Michel Aractingi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:12:05.798Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc274",
                    "user": {
                        "_id": "67d7dea1786ddcb3af5a44b3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d7dea1786ddcb3af5a44b3/gEgXTH4oO91GIzjHR-yrb.png",
                        "isPro": false,
                        "fullname": "Caroline Pascal",
                        "user": "CarolinePascal",
                        "type": "user"
                    },
                    "name": "Caroline Pascal",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:12:15.340Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc275",
                    "user": {
                        "_id": "631365ad289cf15634c6f600",
                        "avatarUrl": "/avatars/a464d228328719274a20121e2a82f703.svg",
                        "isPro": false,
                        "fullname": "Martino Russi",
                        "user": "nepyope",
                        "type": "user"
                    },
                    "name": "Martino Russi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:12:23.474Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc276",
                    "user": {
                        "_id": "65d66b494bbd0d92b641cdbb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
                        "isPro": false,
                        "fullname": "Andres Marafioti",
                        "user": "andito",
                        "type": "user"
                    },
                    "name": "Andres Marafioti",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:12:31.740Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc277",
                    "user": {
                        "_id": "65fcb7f133a3d6f126772121",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65fcb7f133a3d6f126772121/BvVbNqnlQgDr2f_9dm5Es.jpeg",
                        "isPro": false,
                        "fullname": "Simon  Alibert",
                        "user": "aliberts",
                        "type": "user"
                    },
                    "name": "Simon Alibert",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:12:39.999Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc278",
                    "name": "Matthieu Cord",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc279",
                    "user": {
                        "_id": "5df7e9e5da6d0311fd3d53f9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg",
                        "isPro": false,
                        "fullname": "Thomas Wolf",
                        "user": "thomwolf",
                        "type": "user"
                    },
                    "name": "Thomas Wolf",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:13:02.139Z",
                    "hidden": false
                },
                {
                    "_id": "683eb85825fcc99d2a7fc27a",
                    "user": {
                        "_id": "62f857fbb9fda55613ce80d9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f857fbb9fda55613ce80d9/d7bRniKLmOt-iFN07k1Su.png",
                        "isPro": false,
                        "fullname": "Remi Cadene",
                        "user": "cadene",
                        "type": "user"
                    },
                    "name": "Remi Cadene",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T09:13:11.882Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T16:30:19.000Z",
            "submittedOnDailyAt": "2025-06-03T07:31:41.152Z",
            "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics",
            "submittedOnDailyBy": {
                "_id": "65d66b494bbd0d92b641cdbb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
                "isPro": false,
                "fullname": "Andres Marafioti",
                "user": "andito",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) pretrained on large-scale multimodal datasets\nencode rich visual and linguistic knowledge, making them a strong foundation\nfor robotics. Rather than training robotic policies from scratch, recent\napproaches adapt VLMs into vision-language-action (VLA) models that enable\nnatural language-driven perception and control. However, existing VLAs are\ntypically massive--often with billions of parameters--leading to high training\ncosts and limited real-world deployability. Moreover, they rely on academic and\nindustrial datasets, overlooking the growing availability of\ncommunity-collected data from affordable robotic platforms. In this work, we\npresent SmolVLA, a small, efficient, and community-driven VLA that drastically\nreduces both training and inference costs, while retaining competitive\nperformance. SmolVLA is designed to be trained on a single GPU and deployed on\nconsumer-grade GPUs or even CPUs. To further improve responsiveness, we\nintroduce an asynchronous inference stack decoupling perception and action\nprediction from action execution, allowing higher control rates with chunked\naction generation. Despite its compact size, SmolVLA achieves performance\ncomparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both\nsimulated as well as real-world robotic benchmarks and release all code,\npretrained models, and training data.",
            "upvotes": 45,
            "discussionId": "683eb85925fcc99d2a7fc2dc",
            "ai_summary": "SmolVLA is a compact, efficient vision-language-action model that achieves competitive performance at reduced computational costs and can be deployed on consumer-grade hardware.",
            "ai_keywords": [
                "vision-language models",
                "multimodal datasets",
                "robotic policies",
                "vision-language-action models",
                "natural language-driven perception",
                "asynchronous inference",
                "action prediction",
                "action execution",
                "chunked action generation",
                "performance benchmarks"
            ]
        },
        "publishedAt": "2025-06-02T12:30:19.000Z",
        "title": "SmolVLA: A Vision-Language-Action Model for Affordable and Efficient\n  Robotics",
        "summary": "Vision-language models (VLMs) pretrained on large-scale multimodal datasets\nencode rich visual and linguistic knowledge, making them a strong foundation\nfor robotics. Rather than training robotic policies from scratch, recent\napproaches adapt VLMs into vision-language-action (VLA) models that enable\nnatural language-driven perception and control. However, existing VLAs are\ntypically massive--often with billions of parameters--leading to high training\ncosts and limited real-world deployability. Moreover, they rely on academic and\nindustrial datasets, overlooking the growing availability of\ncommunity-collected data from affordable robotic platforms. In this work, we\npresent SmolVLA, a small, efficient, and community-driven VLA that drastically\nreduces both training and inference costs, while retaining competitive\nperformance. SmolVLA is designed to be trained on a single GPU and deployed on\nconsumer-grade GPUs or even CPUs. To further improve responsiveness, we\nintroduce an asynchronous inference stack decoupling perception and action\nprediction from action execution, allowing higher control rates with chunked\naction generation. Despite its compact size, SmolVLA achieves performance\ncomparable to VLAs that are 10x larger. We evaluate SmolVLA on a range of both\nsimulated as well as real-world robotic benchmarks and release all code,\npretrained models, and training data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01844.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "65d66b494bbd0d92b641cdbb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d66b494bbd0d92b641cdbb/6-7dm7B-JxcoS1QlCPdMN.jpeg",
            "fullname": "Andres Marafioti",
            "name": "andito",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 185
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00539",
            "authors": [
                {
                    "_id": "683e76c17a0996f979e72700",
                    "user": {
                        "_id": "671a4abbef737c0abe21b3f8",
                        "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
                        "isPro": false,
                        "fullname": "Ruihan Yang",
                        "user": "rhyang2021",
                        "type": "user"
                    },
                    "name": "Ruihan Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:41:32.053Z",
                    "hidden": false
                },
                {
                    "_id": "683e76c17a0996f979e72701",
                    "name": "Yikai Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e76c17a0996f979e72702",
                    "user": {
                        "_id": "63f86b099f87cc3e645b51d9",
                        "avatarUrl": "/avatars/27ca5ba425640bf67474cee871e8e53a.svg",
                        "isPro": false,
                        "fullname": "Ellie Chen",
                        "user": "sheep33333",
                        "type": "user"
                    },
                    "name": "Aili Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:41:28.422Z",
                    "hidden": false
                },
                {
                    "_id": "683e76c17a0996f979e72703",
                    "user": {
                        "_id": "60e272ca6c78a8c122b12127",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60e272ca6c78a8c122b12127/xldEGBzGrU-bX6IwAw0Ie.jpeg",
                        "isPro": false,
                        "fullname": "Xintao Wang",
                        "user": "Xintao",
                        "type": "user"
                    },
                    "name": "Xintao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T10:44:59.705Z",
                    "hidden": false
                },
                {
                    "_id": "683e76c17a0996f979e72704",
                    "name": "Siyu Yuan",
                    "hidden": false
                },
                {
                    "_id": "683e76c17a0996f979e72705",
                    "name": "Jiangjie Chen",
                    "hidden": false
                },
                {
                    "_id": "683e76c17a0996f979e72706",
                    "name": "Deqing Yang",
                    "hidden": false
                },
                {
                    "_id": "683e76c17a0996f979e72707",
                    "name": "Yanghua Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T12:54:49.000Z",
            "submittedOnDailyAt": "2025-06-03T02:50:22.439Z",
            "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation",
            "submittedOnDailyBy": {
                "_id": "671a4abbef737c0abe21b3f8",
                "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
                "isPro": false,
                "fullname": "Ruihan Yang",
                "user": "rhyang2021",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.",
            "upvotes": 25,
            "discussionId": "683e76ca7a0996f979e728e0",
            "projectPage": "https://aria-agent.github.io/",
            "githubRepo": "https://github.com/rhyang2021/ARIA",
            "ai_summary": "ARIA aggregates rewards in an intention space to mitigate reward sparsity and improve policy optimization in language-based reinforcement learning tasks.",
            "ai_keywords": [
                "large language models",
                "reinforcement learning",
                "action space",
                "token distribution",
                "extreme reward sparsity",
                "reward variance",
                "policy optimization",
                "intention space",
                "semantically similar actions",
                "shared rewards",
                "policy gradient variance",
                "performance gains",
                "offline RL",
                "online RL"
            ]
        },
        "publishedAt": "2025-05-31T08:54:49.000Z",
        "title": "ARIA: Training Language Agents with Intention-Driven Reward Aggregation",
        "summary": "Large language models (LLMs) have enabled agents to perform complex reasoning\nand decision-making through free-form language interactions. However, in\nopen-ended language action environments (e.g., negotiation or question-asking\ngames), the action space can be formulated as a joint distribution over tokens,\nresulting in an exponentially large action space. Sampling actions in such a\nspace can lead to extreme reward sparsity, which brings large reward variance,\nhindering effective reinforcement learning (RL). To address this, we propose\nARIA, a method that Aggregates Rewards in Intention space to enable efficient\nand effective language Agents training. ARIA aims to project natural language\nactions from the high-dimensional joint token distribution space into a\nlow-dimensional intention space, where semantically similar actions are\nclustered and assigned shared rewards. This intention-aware reward aggregation\nreduces reward variance by densifying reward signals, fostering better policy\noptimization. Extensive experiments demonstrate that ARIA not only\nsignificantly reduces policy gradient variance, but also delivers substantial\nperformance gains of an average of 9.95% across four downstream tasks,\nconsistently outperforming offline and online RL baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00539.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "671a4abbef737c0abe21b3f8",
            "avatarUrl": "/avatars/da826af5472a3b9f1969f0c766672731.svg",
            "fullname": "Ruihan Yang",
            "name": "rhyang2021",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00996",
            "authors": [
                {
                    "_id": "683e7cbf402acb186580d5ec",
                    "user": {
                        "_id": "64797735a68454566356b708",
                        "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
                        "isPro": false,
                        "fullname": "Kinam Kim",
                        "user": "kinam0252",
                        "type": "user"
                    },
                    "name": "Kinam Kim",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:32:41.547Z",
                    "hidden": false
                },
                {
                    "_id": "683e7cbf402acb186580d5ed",
                    "user": {
                        "_id": "6331ed964db0a767bbee4706",
                        "avatarUrl": "/avatars/2ef3fa6083ca5208f4b87c91345267d9.svg",
                        "isPro": false,
                        "fullname": "Junha Hyung",
                        "user": "JunhaH",
                        "type": "user"
                    },
                    "name": "Junha Hyung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:32:50.417Z",
                    "hidden": false
                },
                {
                    "_id": "683e7cbf402acb186580d5ee",
                    "user": {
                        "_id": "64be3127805e5b64572da65c",
                        "avatarUrl": "/avatars/edd3e94ba9e375827cc75b164602bcac.svg",
                        "isPro": false,
                        "fullname": "Jaegul Choo",
                        "user": "joyfull78",
                        "type": "user"
                    },
                    "name": "Jaegul Choo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:33:03.788Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64797735a68454566356b708/a-8jl8yq3KdDLnuZEaB3K.mp4"
            ],
            "publishedAt": "2025-06-01T12:57:43.000Z",
            "submittedOnDailyAt": "2025-06-03T03:25:10.796Z",
            "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models",
            "submittedOnDailyBy": {
                "_id": "64797735a68454566356b708",
                "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
                "isPro": false,
                "fullname": "Kinam Kim",
                "user": "kinam0252",
                "type": "user"
            },
            "summary": "Recent advances in text-to-video diffusion models have enabled high-quality\nvideo synthesis, but controllable generation remains challenging, particularly\nunder limited data and compute. Existing fine-tuning methods for conditional\ngeneration often rely on external encoders or architectural modifications,\nwhich demand large datasets and are typically restricted to spatially aligned\nconditioning, limiting flexibility and scalability. In this work, we introduce\nTemporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach\nfor adapting pretrained video diffusion models to diverse conditional\ngeneration tasks. Our key idea is to concatenate condition and target frames\nalong the temporal axis and insert intermediate buffer frames with\nprogressively increasing noise levels. These buffer frames enable smooth\ntransitions, aligning the fine-tuning process with the pretrained model's\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10-30 training samples. We validate our method\nacross a range of tasks, including image-to-video and video-to-video\ngeneration, using large-scale base models such as CogVideoX-5B and Wan-14B.\nExtensive experiments show that TIC-FT outperforms existing baselines in both\ncondition fidelity and visual quality, while remaining highly efficient in both\ntraining and inference. For additional results, visit\nhttps://kinam0252.github.io/TIC-FT/",
            "upvotes": 24,
            "discussionId": "683e7cc1402acb186580d663",
            "ai_summary": "Temporal In-Context Fine-Tuning (TIC-FT) enhances pretrained video diffusion models for diverse conditional generation tasks with minimal data and without architectural changes.",
            "ai_keywords": [
                "text-to-video diffusion models",
                "fine-tuning",
                "external encoders",
                "architectural modifications",
                "Temporal In-Context Fine-Tuning",
                "condition and target frames",
                "buffer frames",
                "noise levels",
                "smooth transitions",
                "pretrained video diffusion models",
                "image-to-video generation",
                "video-to-video generation",
                "CogVideoX-5B",
                "Wan-14B"
            ]
        },
        "publishedAt": "2025-06-01T08:57:43.000Z",
        "title": "Temporal In-Context Fine-Tuning for Versatile Control of Video Diffusion\n  Models",
        "summary": "Recent advances in text-to-video diffusion models have enabled high-quality\nvideo synthesis, but controllable generation remains challenging, particularly\nunder limited data and compute. Existing fine-tuning methods for conditional\ngeneration often rely on external encoders or architectural modifications,\nwhich demand large datasets and are typically restricted to spatially aligned\nconditioning, limiting flexibility and scalability. In this work, we introduce\nTemporal In-Context Fine-Tuning (TIC-FT), an efficient and versatile approach\nfor adapting pretrained video diffusion models to diverse conditional\ngeneration tasks. Our key idea is to concatenate condition and target frames\nalong the temporal axis and insert intermediate buffer frames with\nprogressively increasing noise levels. These buffer frames enable smooth\ntransitions, aligning the fine-tuning process with the pretrained model's\ntemporal dynamics. TIC-FT requires no architectural changes and achieves strong\nperformance with as few as 10-30 training samples. We validate our method\nacross a range of tasks, including image-to-video and video-to-video\ngeneration, using large-scale base models such as CogVideoX-5B and Wan-14B.\nExtensive experiments show that TIC-FT outperforms existing baselines in both\ncondition fidelity and visual quality, while remaining highly efficient in both\ntraining and inference. For additional results, visit\nhttps://kinam0252.github.io/TIC-FT/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64797735a68454566356b708/a-8jl8yq3KdDLnuZEaB3K.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00996.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64797735a68454566356b708",
            "avatarUrl": "/avatars/3424d022dd8ad29b56eb41814c5c3dee.svg",
            "fullname": "Kinam Kim",
            "name": "kinam0252",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01853",
            "authors": [
                {
                    "_id": "683e671483a130f817c4937a",
                    "user": {
                        "_id": "65a420cd90e65dc39a6abe9e",
                        "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
                        "isPro": false,
                        "fullname": "yejunliang",
                        "user": "yejunliang23",
                        "type": "user"
                    },
                    "name": "Junliang Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:36:53.278Z",
                    "hidden": false
                },
                {
                    "_id": "683e671483a130f817c4937b",
                    "user": {
                        "_id": "634e15aec1ce28f1de91c470",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634e15aec1ce28f1de91c470/MdtJKgE_fbHydhqESr-DH.png",
                        "isPro": false,
                        "fullname": "Zhengyi Wang",
                        "user": "Zhengyi",
                        "type": "user"
                    },
                    "name": "Zhengyi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:37:23.401Z",
                    "hidden": false
                },
                {
                    "_id": "683e671483a130f817c4937c",
                    "user": {
                        "_id": "6522e4fbd89bc7773ddc4b58",
                        "avatarUrl": "/avatars/3e9b158af52c5f738a3eae72dcbb3824.svg",
                        "isPro": false,
                        "fullname": "Ruowen Zhao",
                        "user": "zzzrw",
                        "type": "user"
                    },
                    "name": "Ruowen Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:46:03.565Z",
                    "hidden": false
                },
                {
                    "_id": "683e671483a130f817c4937d",
                    "name": "Shenghao Xie",
                    "hidden": false
                },
                {
                    "_id": "683e671483a130f817c4937e",
                    "name": "Jun Zhu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/84A40qBWJeZaBv2qYO6Pj.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/Xl6IajTG3VravdQABgC6l.mp4"
            ],
            "publishedAt": "2025-06-02T16:40:50.000Z",
            "submittedOnDailyAt": "2025-06-03T03:57:42.122Z",
            "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "65a420cd90e65dc39a6abe9e",
                "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
                "isPro": false,
                "fullname": "yejunliang",
                "user": "yejunliang23",
                "type": "user"
            },
            "summary": "Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to\ngrowing appreciation for native multimodal large language models. However, its\nmultimodal capabilities remain confined to images and text. Yet beyond images,\nthe ability to understand and generate 3D content is equally crucial. To\naddress this gap, we propose ShapeLLM-Omni-a native 3D large language model\ncapable of understanding and generating 3D assets and text in any sequence.\nFirst, we train a 3D vector-quantized variational autoencoder (VQVAE), which\nmaps 3D objects into a discrete latent space to achieve efficient and accurate\nshape representation and reconstruction. Building upon the 3D-aware discrete\ntokens, we innovatively construct a large-scale continuous training dataset\nnamed 3D-Alpaca, encompassing generation, comprehension, and editing, thus\nproviding rich resources for future research and training. Finally, by\nperforming instruction-based training of the Qwen-2.5-vl-7B-Instruct model on\nthe 3D-Alpaca dataset. Our work provides an effective attempt at extending\nmultimodal models with basic 3D capabilities, which contributes to future\nresearch in 3D-native AI. Project page:\nhttps://github.com/JAMESYJL/ShapeLLM-Omni",
            "upvotes": 23,
            "discussionId": "683e671683a130f817c493cd",
            "ai_summary": "A native 3D large language model, ShapeLLM-Omni, is proposed to understand and generate 3D assets and text, trained using a 3D vector-quantized variational autoencoder and a new 3D-Alpaca dataset.",
            "ai_keywords": [
                "3D vector-quantized variational autoencoder (VQVAE)",
                "discrete latent space",
                "instruction-based training",
                "3D-Alpaca dataset",
                "3D-native AI"
            ]
        },
        "publishedAt": "2025-06-02T12:40:50.000Z",
        "title": "ShapeLLM-Omni: A Native Multimodal LLM for 3D Generation and\n  Understanding",
        "summary": "Recently, the powerful text-to-image capabilities of ChatGPT-4o have led to\ngrowing appreciation for native multimodal large language models. However, its\nmultimodal capabilities remain confined to images and text. Yet beyond images,\nthe ability to understand and generate 3D content is equally crucial. To\naddress this gap, we propose ShapeLLM-Omni-a native 3D large language model\ncapable of understanding and generating 3D assets and text in any sequence.\nFirst, we train a 3D vector-quantized variational autoencoder (VQVAE), which\nmaps 3D objects into a discrete latent space to achieve efficient and accurate\nshape representation and reconstruction. Building upon the 3D-aware discrete\ntokens, we innovatively construct a large-scale continuous training dataset\nnamed 3D-Alpaca, encompassing generation, comprehension, and editing, thus\nproviding rich resources for future research and training. Finally, by\nperforming instruction-based training of the Qwen-2.5-vl-7B-Instruct model on\nthe 3D-Alpaca dataset. Our work provides an effective attempt at extending\nmultimodal models with basic 3D capabilities, which contributes to future\nresearch in 3D-native AI. Project page:\nhttps://github.com/JAMESYJL/ShapeLLM-Omni",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/84A40qBWJeZaBv2qYO6Pj.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/65a420cd90e65dc39a6abe9e/Xl6IajTG3VravdQABgC6l.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01853.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65a420cd90e65dc39a6abe9e",
            "avatarUrl": "/avatars/81ac5b749043e899f5017782409f9e28.svg",
            "fullname": "yejunliang",
            "name": "yejunliang23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00411",
            "authors": [
                {
                    "_id": "683e86e31bca54bb6d169fc4",
                    "user": {
                        "_id": "6707eaaf5f50b17754ff9cbc",
                        "avatarUrl": "/avatars/f08ca6228b124a8955787d0662a52bbd.svg",
                        "isPro": false,
                        "fullname": "Yang",
                        "user": "Yysrc",
                        "type": "user"
                    },
                    "name": "Yi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:40:09.713Z",
                    "hidden": false
                },
                {
                    "_id": "683e86e31bca54bb6d169fc5",
                    "name": "Jiaxuan Sun",
                    "hidden": false
                },
                {
                    "_id": "683e86e31bca54bb6d169fc6",
                    "user": {
                        "_id": "654e330f350abceb30a1390b",
                        "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
                        "isPro": false,
                        "fullname": "KouSiqi",
                        "user": "karrykkk",
                        "type": "user"
                    },
                    "name": "Siqi Kou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:35:37.198Z",
                    "hidden": false
                },
                {
                    "_id": "683e86e31bca54bb6d169fc7",
                    "name": "Yihan Wang",
                    "hidden": false
                },
                {
                    "_id": "683e86e31bca54bb6d169fc8",
                    "user": {
                        "_id": "673d5f411b0fe168ad4896b2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYgQx6XNi_P5GxlUKbH5G.png",
                        "isPro": false,
                        "fullname": "Zhijie Deng",
                        "user": "thudzj",
                        "type": "user"
                    },
                    "name": "Zhijie Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:35:13.092Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T06:01:03.000Z",
            "submittedOnDailyAt": "2025-06-03T03:54:29.259Z",
            "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks",
            "submittedOnDailyBy": {
                "_id": "654e330f350abceb30a1390b",
                "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
                "isPro": false,
                "fullname": "KouSiqi",
                "user": "karrykkk",
                "type": "user"
            },
            "summary": "Real-world embodied agents face long-horizon tasks, characterized by\nhigh-level goals demanding multi-step solutions beyond single actions.\nSuccessfully navigating these requires both high-level task planning (i.e.,\ndecomposing goals into sub-tasks) and low-level motion control (i.e.,\ngenerating precise robot actions). While existing vision language action (VLA)\nmodels and hierarchical architectures offer potential in embodied tasks, the\nformer often falter in planning, and the latter can suffer from coordination\nissues, both hampering performance. We introduce a new unified VLA framework\nfor long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA\nleverages a large pretrained vision language model (VLM) as the backbone to\njointly generate language and action tokens for sub-task generation and robot\naction prediction, respectively. This shared representation promotes better\ngeneralization across tasks. Additionally, LoHoVLA embraces a hierarchical\nclosed-loop control mechanism to mitigate errors originating from both\nhigh-level planning and low-level control. To train LoHoVLA, we introduce\nLoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon\ntasks, each with 1,000 expert demonstrations composed of visual observations,\nlinguistic goals, sub-tasks, and robot actions. Experimental results show that\nLoHoVLA significantly surpasses both hierarchical and standard VLA approaches\non long-horizon embodied tasks in the Ravens simulator. These findings\nunderscore the promise of unified architectures for advancing generalizable\nembodied intelligence.",
            "upvotes": 23,
            "discussionId": "683e86e31bca54bb6d169ff5",
            "ai_summary": "A unified vision language action framework, LoHoVLA, combines a large pretrained vision language model with hierarchical closed-loop control to improve performance on long-horizon embodied tasks.",
            "ai_keywords": [
                "vision language action models",
                "hierarchical architectures",
                "high-level task planning",
                "low-level motion control",
                "LoHoVLA",
                "large pretrained vision language model",
                "shared representation",
                "hierarchical closed-loop control",
                "LoHoSet",
                "Ravens simulator",
                "long-horizon tasks",
                "sub-task generation",
                "robot action prediction",
                "embodied intelligence"
            ]
        },
        "publishedAt": "2025-05-31T02:01:03.000Z",
        "title": "LoHoVLA: A Unified Vision-Language-Action Model for Long-Horizon\n  Embodied Tasks",
        "summary": "Real-world embodied agents face long-horizon tasks, characterized by\nhigh-level goals demanding multi-step solutions beyond single actions.\nSuccessfully navigating these requires both high-level task planning (i.e.,\ndecomposing goals into sub-tasks) and low-level motion control (i.e.,\ngenerating precise robot actions). While existing vision language action (VLA)\nmodels and hierarchical architectures offer potential in embodied tasks, the\nformer often falter in planning, and the latter can suffer from coordination\nissues, both hampering performance. We introduce a new unified VLA framework\nfor long-horizon tasks, dubbed LoHoVLA, to overcome these limitations. LoHoVLA\nleverages a large pretrained vision language model (VLM) as the backbone to\njointly generate language and action tokens for sub-task generation and robot\naction prediction, respectively. This shared representation promotes better\ngeneralization across tasks. Additionally, LoHoVLA embraces a hierarchical\nclosed-loop control mechanism to mitigate errors originating from both\nhigh-level planning and low-level control. To train LoHoVLA, we introduce\nLoHoSet, a dataset built on the Ravens simulator, containing 20 long-horizon\ntasks, each with 1,000 expert demonstrations composed of visual observations,\nlinguistic goals, sub-tasks, and robot actions. Experimental results show that\nLoHoVLA significantly surpasses both hierarchical and standard VLA approaches\non long-horizon embodied tasks in the Ravens simulator. These findings\nunderscore the promise of unified architectures for advancing generalizable\nembodied intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00411.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654e330f350abceb30a1390b",
            "avatarUrl": "/avatars/e54a8be788fa1bdc7acefecc208215bb.svg",
            "fullname": "KouSiqi",
            "name": "karrykkk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01713",
            "authors": [
                {
                    "_id": "683ec81753981b08324ce57b",
                    "name": "Zhongwei Wan",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce57c",
                    "name": "Zhihao Dou",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce57d",
                    "user": {
                        "_id": "631b9ff5824f2502e3557c7e",
                        "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
                        "isPro": true,
                        "fullname": "liu",
                        "user": "che111",
                        "type": "user"
                    },
                    "name": "Che Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:38:05.701Z",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce57e",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce57f",
                    "user": {
                        "_id": "64bd14f65b8d826146cd8512",
                        "avatarUrl": "/avatars/2b157e555495bac18ff332a309d0ce23.svg",
                        "isPro": false,
                        "fullname": "Dongfei Cui",
                        "user": "vencen",
                        "type": "user"
                    },
                    "name": "Dongfei Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:38:16.192Z",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce580",
                    "name": "Qinjian Zhao",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce581",
                    "name": "Hui Shen",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce582",
                    "name": "Jing Xiong",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce583",
                    "name": "Yi Xin",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce584",
                    "name": "Yifan Jiang",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce585",
                    "name": "Yangfan He",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce586",
                    "name": "Mi Zhang",
                    "hidden": false
                },
                {
                    "_id": "683ec81753981b08324ce587",
                    "name": "Shen Yan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T14:21:44.000Z",
            "submittedOnDailyAt": "2025-06-03T08:34:07.706Z",
            "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "631b9ff5824f2502e3557c7e",
                "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
                "isPro": true,
                "fullname": "liu",
                "user": "che111",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality.",
            "upvotes": 20,
            "discussionId": "683ec81853981b08324ce5f1",
            "projectPage": "https://srpo.pages.dev/",
            "ai_summary": "A two-stage reinforcement learning framework enhances reasoning and self-reflection in multimodal large language models through reflection-focused datasets and a novel reward mechanism.",
            "ai_keywords": [
                "multimodal large language models",
                "reasoning tasks",
                "reflection methods",
                "reinforcement learning",
                "reflection-aware reinforcement learning",
                "GRPO framework",
                "reward mechanism",
                "MathVista",
                "MathVision",
                "MathVerse",
                "MMMU-Pro",
                "Qwen-2.5-VL-7B",
                "Qwen-2.5-VL-32B",
                "reasoning accuracy",
                "reflection quality"
            ]
        },
        "publishedAt": "2025-06-02T10:21:44.000Z",
        "title": "SRPO: Enhancing Multimodal LLM Reasoning via Reflection-Aware\n  Reinforcement Learning",
        "summary": "Multimodal large language models (MLLMs) have shown promising capabilities in\nreasoning tasks, yet still struggle with complex problems requiring explicit\nself-reflection and self-correction, especially compared to their unimodal\ntext-based counterparts. Existing reflection methods are simplistic and\nstruggle to generate meaningful and instructive feedback, as the reasoning\nability and knowledge limits of pre-trained models are largely fixed during\ninitial training. To overcome these challenges, we propose Multimodal\nSelf-Reflection enhanced reasoning with Group Relative Policy Optimization\n(SRPO), a two-stage reflection-aware reinforcement learning (RL) framework\nexplicitly designed to enhance multimodal LLM reasoning. In the first stage, we\nconstruct a high-quality, reflection-focused dataset under the guidance of an\nadvanced MLLM, which generates reflections based on initial responses to help\nthe policy model learn both reasoning and self-reflection. In the second stage,\nwe introduce a novel reward mechanism within the GRPO framework that encourages\nconcise and cognitively meaningful reflection while avoiding redundancy.\nExtensive experiments across multiple multimodal reasoning benchmarks,\nincluding MathVista, MathVision, MathVerse, and MMMU-Pro, using Qwen-2.5-VL-7B\nand Qwen-2.5-VL-32B demonstrate that SRPO significantly outperforms\nstate-of-the-art models, achieving notable improvements in both reasoning\naccuracy and reflection quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01713.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "fullname": "liu",
            "name": "che111",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01667",
            "authors": [
                {
                    "_id": "683eaa8ed8d42fc832445ebd",
                    "user": {
                        "_id": "65c4f99b27736b5b86c2cbda",
                        "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
                        "isPro": false,
                        "fullname": "Yan Shu",
                        "user": "sy1998",
                        "type": "user"
                    },
                    "name": "Yan Shu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T12:33:48.379Z",
                    "hidden": false
                },
                {
                    "_id": "683eaa8ed8d42fc832445ebe",
                    "name": "Bin Ren",
                    "hidden": false
                },
                {
                    "_id": "683eaa8ed8d42fc832445ebf",
                    "name": "Zhitong Xiong",
                    "hidden": false
                },
                {
                    "_id": "683eaa8ed8d42fc832445ec0",
                    "name": "Danda Pani Paudel",
                    "hidden": false
                },
                {
                    "_id": "683eaa8ed8d42fc832445ec1",
                    "name": "Luc Van Gool",
                    "hidden": false
                },
                {
                    "_id": "683eaa8ed8d42fc832445ec2",
                    "name": "Begum Demir",
                    "hidden": false
                },
                {
                    "_id": "683eaa8ed8d42fc832445ec3",
                    "name": "Nicu Sebe",
                    "hidden": false
                },
                {
                    "_id": "683eaa8ed8d42fc832445ec4",
                    "user": {
                        "_id": "62f3b1ea81861bd9bc5c5538",
                        "avatarUrl": "/avatars/0aef9ac5bfa91b9894166fe3c29925da.svg",
                        "isPro": false,
                        "fullname": "Paolo Rota",
                        "user": "paolorota",
                        "type": "user"
                    },
                    "name": "Paolo Rota",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:40:42.097Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T13:36:05.000Z",
            "submittedOnDailyAt": "2025-06-03T06:26:41.885Z",
            "title": "EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "65c4f99b27736b5b86c2cbda",
                "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
                "isPro": false,
                "fullname": "Yan Shu",
                "user": "sy1998",
                "type": "user"
            },
            "summary": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nvarious vision-language tasks. However, they often struggle to comprehensively\nunderstand Earth Observation (EO) data, which is critical for monitoring the\nenvironment and the effects of human activity on it. In this work, we present\nEarthMind, a novel vision-language framework for multi-granular and\nmulti-sensor EO data understanding. EarthMind features two core components: (1)\nSpatial Attention Prompting (SAP), which reallocates attention within the LLM\nto enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns\nheterogeneous modalities into a shared space and adaptively reweighs tokens\nbased on their information density for effective fusion. To facilitate\nmulti-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive\nbenchmark with over 2,000 human-annotated multi-sensor image-question pairs,\ncovering a wide range of perception and reasoning tasks. Extensive experiments\ndemonstrate the effectiveness of EarthMind. It achieves state-of-the-art\nperformance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in\nscale. Moreover, EarthMind outperforms existing methods on multiple public EO\nbenchmarks, showcasing its potential to handle both multi-granular and\nmulti-sensor challenges in a unified framework.",
            "upvotes": 18,
            "discussionId": "683eaa94d8d42fc832446013",
            "githubRepo": "https://github.com/shuyansy/EarthMind",
            "ai_summary": "EarthMind is a vision-language framework that uses spatial attention prompting and cross-modal fusion for efficient multi-granular and multi-sensor Earth Observation data understanding, outperforming larger models on specialized benchmarks.",
            "ai_keywords": [
                "Spatial Attention Prompting",
                "Cross-modal Fusion",
                "Earth Observation",
                "multi-granular",
                "multi-sensor",
                "EarthMind-Bench"
            ]
        },
        "publishedAt": "2025-06-02T09:36:05.000Z",
        "title": "EarthMind: Towards Multi-Granular and Multi-Sensor Earth Observation\n  with Large Multimodal Models",
        "summary": "Large Multimodal Models (LMMs) have demonstrated strong performance in\nvarious vision-language tasks. However, they often struggle to comprehensively\nunderstand Earth Observation (EO) data, which is critical for monitoring the\nenvironment and the effects of human activity on it. In this work, we present\nEarthMind, a novel vision-language framework for multi-granular and\nmulti-sensor EO data understanding. EarthMind features two core components: (1)\nSpatial Attention Prompting (SAP), which reallocates attention within the LLM\nto enhance pixel-level understanding; and (2) Cross-modal Fusion, which aligns\nheterogeneous modalities into a shared space and adaptively reweighs tokens\nbased on their information density for effective fusion. To facilitate\nmulti-sensor fusion evaluation, we propose EarthMind-Bench, a comprehensive\nbenchmark with over 2,000 human-annotated multi-sensor image-question pairs,\ncovering a wide range of perception and reasoning tasks. Extensive experiments\ndemonstrate the effectiveness of EarthMind. It achieves state-of-the-art\nperformance on EarthMind-Bench, surpassing GPT-4o despite being only 4B in\nscale. Moreover, EarthMind outperforms existing methods on multiple public EO\nbenchmarks, showcasing its potential to handle both multi-granular and\nmulti-sensor challenges in a unified framework.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01667.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65c4f99b27736b5b86c2cbda",
            "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
            "fullname": "Yan Shu",
            "name": "sy1998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24298",
            "authors": [
                {
                    "_id": "683d12963aa5ac98190e1eda",
                    "name": "Wei Fu",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1edb",
                    "user": {
                        "_id": "65338f12392f3bcb323d78b2",
                        "avatarUrl": "/avatars/960b9338298ff6c663a6706aa6189e72.svg",
                        "isPro": false,
                        "fullname": "Jiaxuan Gao",
                        "user": "samjia2000",
                        "type": "user"
                    },
                    "name": "Jiaxuan Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:39:56.082Z",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1edc",
                    "name": "Xujie Shen",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1edd",
                    "name": "Chen Zhu",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1ede",
                    "user": {
                        "_id": "66d782669b7da501cd079628",
                        "avatarUrl": "/avatars/5fc76cc6d11588c4d2404989ca400809.svg",
                        "isPro": false,
                        "fullname": "Zhi Yu mei",
                        "user": "Souleaf",
                        "type": "user"
                    },
                    "name": "Zhiyu Mei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:40:20.790Z",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1edf",
                    "name": "Chuyi He",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1ee0",
                    "name": "Shusheng Xu",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1ee1",
                    "name": "Guo Wei",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1ee2",
                    "name": "Jun Mei",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1ee3",
                    "name": "Jiashu Wang",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1ee4",
                    "name": "Tongkai Yang",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1ee5",
                    "name": "Binhang Yuan",
                    "hidden": false
                },
                {
                    "_id": "683d12963aa5ac98190e1ee6",
                    "name": "Yi Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T07:18:25.000Z",
            "submittedOnDailyAt": "2025-06-03T05:39:21.066Z",
            "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning",
            "submittedOnDailyBy": {
                "_id": "63159678915d0b80682fe9f9",
                "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
                "isPro": false,
                "fullname": "Shusheng Xu",
                "user": "xssstory",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has become a trending paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are synchronous\nby alternating generation and training in a batch setting, where the rollouts\nin each training batch are generated by the same (or latest) model. This\nstabilizes RL training but suffers from severe system-level inefficiency.\nGeneration must wait until the longest output in the batch is completed before\nmodel update, resulting in GPU underutilization. We present AReaL, a\nfully asynchronous RL system that completely decouples generation from\ntraining. Rollout workers in AReaL continuously generate new outputs without\nwaiting, while training workers update the model whenever a batch of data is\ncollected. AReaL also incorporates a collection of system-level optimizations,\nleading to substantially higher GPU utilization. To stabilize RL training,\nAReaL balances the workload of rollout and training workers to control data\nstaleness, and adopts a staleness-enhanced PPO variant to better handle\noutdated training samples. Extensive experiments on math and code reasoning\nbenchmarks show that AReaL achieves up to 2.57times training\nspeedup compared to the best synchronous systems with the same number of GPUs\nand matched or even improved final performance. The code of AReaL is available\nat https://github.com/inclusionAI/AReaL/.",
            "upvotes": 16,
            "discussionId": "683d12973aa5ac98190e1f19",
            "ai_summary": "AReaL, a fully asynchronous reinforcement learning system, decouples generation and training to achieve higher GPU utilization and up to 2.57x training speedup for large language models on reasoning tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "asynchronous system",
                "rollouts",
                "model update",
                "GPU utilization",
                "PPO",
                "data staleness",
                "training speedup"
            ]
        },
        "publishedAt": "2025-05-30T03:18:25.000Z",
        "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for\n  Language Reasoning",
        "summary": "Reinforcement learning (RL) has become a trending paradigm for training large\nlanguage models (LLMs), particularly for reasoning tasks. Effective RL for LLMs\nrequires massive parallelization and poses an urgent need for efficient\ntraining systems. Most existing large-scale RL systems for LLMs are synchronous\nby alternating generation and training in a batch setting, where the rollouts\nin each training batch are generated by the same (or latest) model. This\nstabilizes RL training but suffers from severe system-level inefficiency.\nGeneration must wait until the longest output in the batch is completed before\nmodel update, resulting in GPU underutilization. We present AReaL, a\nfully asynchronous RL system that completely decouples generation from\ntraining. Rollout workers in AReaL continuously generate new outputs without\nwaiting, while training workers update the model whenever a batch of data is\ncollected. AReaL also incorporates a collection of system-level optimizations,\nleading to substantially higher GPU utilization. To stabilize RL training,\nAReaL balances the workload of rollout and training workers to control data\nstaleness, and adopts a staleness-enhanced PPO variant to better handle\noutdated training samples. Extensive experiments on math and code reasoning\nbenchmarks show that AReaL achieves up to 2.57times training\nspeedup compared to the best synchronous systems with the same number of GPUs\nand matched or even improved final performance. The code of AReaL is available\nat https://github.com/inclusionAI/AReaL/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24298.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63159678915d0b80682fe9f9",
            "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
            "fullname": "Shusheng Xu",
            "name": "xssstory",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.01863",
            "authors": [
                {
                    "_id": "683ee1a6e65df2618201e81b",
                    "name": "Andrei Panferov",
                    "hidden": false
                },
                {
                    "_id": "683ee1a6e65df2618201e81c",
                    "user": {
                        "_id": "65f08bd773f2e1e291244d86",
                        "avatarUrl": "/avatars/4d66171163094024ebc82bb59859d9c5.svg",
                        "isPro": false,
                        "fullname": "Alexandra",
                        "user": "alexandraww",
                        "type": "user"
                    },
                    "name": "Alexandra Volkova",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T12:33:09.324Z",
                    "hidden": false
                },
                {
                    "_id": "683ee1a6e65df2618201e81d",
                    "name": "Ionut-Vlad Modoranu",
                    "hidden": false
                },
                {
                    "_id": "683ee1a6e65df2618201e81e",
                    "name": "Vage Egiazarian",
                    "hidden": false
                },
                {
                    "_id": "683ee1a6e65df2618201e81f",
                    "name": "Mher Safaryan",
                    "hidden": false
                },
                {
                    "_id": "683ee1a6e65df2618201e820",
                    "name": "Dan Alistarh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T16:52:51.000Z",
            "submittedOnDailyAt": "2025-06-03T11:07:41.721Z",
            "title": "Unified Scaling Laws for Compressed Representations",
            "submittedOnDailyBy": {
                "_id": "65f08bd773f2e1e291244d86",
                "avatarUrl": "/avatars/4d66171163094024ebc82bb59859d9c5.svg",
                "isPro": false,
                "fullname": "Alexandra",
                "user": "alexandraww",
                "type": "user"
            },
            "summary": "Scaling laws have shaped recent advances in machine learning by enabling\npredictable scaling of model performance based on model size, computation, and\ndata volume. Concurrently, the rise in computational cost for AI has motivated\nmodel compression techniques, notably quantization and sparsification, which\nhave emerged to mitigate the steep computational demands associated with\nlarge-scale training and inference. This paper investigates the interplay\nbetween scaling laws and compression formats, exploring whether a unified\nscaling framework can accurately predict model performance when training occurs\nover various compressed representations, such as sparse, scalar-quantized,\nsparse-quantized or even vector-quantized formats. Our key contributions\ninclude validating a general scaling law formulation and showing that it is\napplicable both individually but also composably across compression types.\nBased on this, our main finding is demonstrating both theoretically and\nempirically that there exists a simple \"capacity\" metric -- based on the\nrepresentation's ability to fit random Gaussian data -- which can robustly\npredict parameter efficiency across multiple compressed representations. On the\npractical side, we extend our formulation to directly compare the accuracy\npotential of different compressed formats, and to derive better algorithms for\ntraining over sparse-quantized formats.",
            "upvotes": 14,
            "discussionId": "683ee1a6e65df2618201e83b",
            "ai_summary": "A study on scaling laws and compression techniques shows that a unified capacity metric can predict model performance across different compressed formats, including scalar-quantized, sparse-quantized, and vector-quantized representations.",
            "ai_keywords": [
                "scaling laws",
                "model compression",
                "quantization",
                "sparsification",
                "sparse representations",
                "scalar-quantized",
                "sparse-quantized",
                "vector-quantized",
                "parameter efficiency",
                "Gaussian data",
                "capacity metric"
            ]
        },
        "publishedAt": "2025-06-02T12:52:51.000Z",
        "title": "Unified Scaling Laws for Compressed Representations",
        "summary": "Scaling laws have shaped recent advances in machine learning by enabling\npredictable scaling of model performance based on model size, computation, and\ndata volume. Concurrently, the rise in computational cost for AI has motivated\nmodel compression techniques, notably quantization and sparsification, which\nhave emerged to mitigate the steep computational demands associated with\nlarge-scale training and inference. This paper investigates the interplay\nbetween scaling laws and compression formats, exploring whether a unified\nscaling framework can accurately predict model performance when training occurs\nover various compressed representations, such as sparse, scalar-quantized,\nsparse-quantized or even vector-quantized formats. Our key contributions\ninclude validating a general scaling law formulation and showing that it is\napplicable both individually but also composably across compression types.\nBased on this, our main finding is demonstrating both theoretically and\nempirically that there exists a simple \"capacity\" metric -- based on the\nrepresentation's ability to fit random Gaussian data -- which can robustly\npredict parameter efficiency across multiple compressed representations. On the\npractical side, we extend our formulation to directly compare the accuracy\npotential of different compressed formats, and to derive better algorithms for\ntraining over sparse-quantized formats.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01863.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65f08bd773f2e1e291244d86",
            "avatarUrl": "/avatars/4d66171163094024ebc82bb59859d9c5.svg",
            "fullname": "Alexandra",
            "name": "alexandraww",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24846",
            "authors": [
                {
                    "_id": "683e82f2fa7ede4842f95214",
                    "name": "Jingyan Shen",
                    "hidden": false
                },
                {
                    "_id": "683e82f2fa7ede4842f95215",
                    "user": {
                        "_id": "66f8689725464a7989b75845",
                        "avatarUrl": "/avatars/43a61a528c5779103eaf5687ba44ee14.svg",
                        "isPro": false,
                        "fullname": "Jiarui Yao",
                        "user": "FlippyDora",
                        "type": "user"
                    },
                    "name": "Jiarui Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:40:58.100Z",
                    "hidden": false
                },
                {
                    "_id": "683e82f2fa7ede4842f95216",
                    "user": {
                        "_id": "64d45451c34a346181b130dd",
                        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
                        "isPro": false,
                        "fullname": "Rui Yang",
                        "user": "Ray2333",
                        "type": "user"
                    },
                    "name": "Rui Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:40:16.572Z",
                    "hidden": true
                },
                {
                    "_id": "683e82f2fa7ede4842f95217",
                    "name": "Yifan Sun",
                    "hidden": false
                },
                {
                    "_id": "683e82f2fa7ede4842f95218",
                    "name": "Feng Luo",
                    "hidden": false
                },
                {
                    "_id": "683e82f2fa7ede4842f95219",
                    "name": "Rui Pan",
                    "hidden": false
                },
                {
                    "_id": "683e82f2fa7ede4842f9521a",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e82f2fa7ede4842f9521b",
                    "name": "Han Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T17:44:28.000Z",
            "submittedOnDailyAt": "2025-06-03T03:51:59.115Z",
            "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning",
            "submittedOnDailyBy": {
                "_id": "64d45451c34a346181b130dd",
                "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
                "isPro": false,
                "fullname": "Rui Yang",
                "user": "Ray2333",
                "type": "user"
            },
            "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.",
            "upvotes": 14,
            "discussionId": "683e82f3fa7ede4842f95246",
            "ai_summary": "MiCRo, a two-stage framework, improves personalized preference learning for large language models by leveraging binary preference datasets and dynamically adapting mixture weights based on context, effectively capturing diverse human preferences.",
            "ai_keywords": [
                "Reward modeling",
                "reinforcement learning from human feedback (RLHF)",
                "Large Language Models (LLMs)",
                "Bradley-Terry (BT) model",
                "mixture distribution",
                "personalization",
                "pluralistic alignment",
                "multi-objective learning",
                "context-aware mixture modeling",
                "online routing strategy"
            ]
        },
        "publishedAt": "2025-05-30T13:44:28.000Z",
        "title": "MiCRo: Mixture Modeling and Context-aware Routing for Personalized\n  Preference Learning",
        "summary": "Reward modeling is a key step in building safe foundation models when\napplying reinforcement learning from human feedback (RLHF) to align Large\nLanguage Models (LLMs). However, reward modeling based on the Bradley-Terry\n(BT) model assumes a global reward function, failing to capture the inherently\ndiverse and heterogeneous human preferences. Hence, such oversimplification\nlimits LLMs from supporting personalization and pluralistic alignment.\nTheoretically, we show that when human preferences follow a mixture\ndistribution of diverse subgroups, a single BT model has an irreducible error.\nWhile existing solutions, such as multi-objective learning with fine-grained\nannotations, help address this issue, they are costly and constrained by\npredefined attributes, failing to fully capture the richness of human values.\nIn this work, we introduce MiCRo, a two-stage framework that enhances\npersonalized preference learning by leveraging large-scale binary preference\ndatasets without requiring explicit fine-grained annotations. In the first\nstage, MiCRo introduces context-aware mixture modeling approach to capture\ndiverse human preferences. In the second stage, MiCRo integrates an online\nrouting strategy that dynamically adapts mixture weights based on specific\ncontext to resolve ambiguity, allowing for efficient and scalable preference\nadaptation with minimal additional supervision. Experiments on multiple\npreference datasets demonstrate that MiCRo effectively captures diverse human\npreferences and significantly improves downstream personalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24846.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d45451c34a346181b130dd",
            "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
            "fullname": "Rui Yang",
            "name": "Ray2333",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01413",
            "authors": [
                {
                    "_id": "683e6aa057738c5cc3616d70",
                    "user": {
                        "_id": "6390525c00fb8ec4a424e0ff",
                        "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
                        "isPro": false,
                        "fullname": "Yulei Qin",
                        "user": "yolay",
                        "type": "user"
                    },
                    "name": "Yulei Qin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-03T03:23:18.288Z",
                    "hidden": false
                },
                {
                    "_id": "683e6aa057738c5cc3616d71",
                    "user": {
                        "_id": "648c17fc17c7ceb9b4200d31",
                        "avatarUrl": "/avatars/abc5b025dd682935056c33a9e1e362f0.svg",
                        "isPro": false,
                        "fullname": "Gang Li",
                        "user": "ligang-cs",
                        "type": "user"
                    },
                    "name": "Gang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T12:33:53.264Z",
                    "hidden": false
                },
                {
                    "_id": "683e6aa057738c5cc3616d72",
                    "name": "Zongyi Li",
                    "hidden": false
                },
                {
                    "_id": "683e6aa057738c5cc3616d73",
                    "name": "Zihan Xu",
                    "hidden": false
                },
                {
                    "_id": "683e6aa057738c5cc3616d74",
                    "name": "Yuchen Shi",
                    "hidden": false
                },
                {
                    "_id": "683e6aa057738c5cc3616d75",
                    "name": "Zhekai Lin",
                    "hidden": false
                },
                {
                    "_id": "683e6aa057738c5cc3616d76",
                    "name": "Xiao Cui",
                    "hidden": false
                },
                {
                    "_id": "683e6aa057738c5cc3616d77",
                    "name": "Ke Li",
                    "hidden": false
                },
                {
                    "_id": "683e6aa057738c5cc3616d78",
                    "name": "Xing Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T08:11:44.000Z",
            "submittedOnDailyAt": "2025-06-03T02:06:35.145Z",
            "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "6390525c00fb8ec4a424e0ff",
                "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
                "isPro": false,
                "fullname": "Yulei Qin",
                "user": "yolay",
                "type": "user"
            },
            "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.",
            "upvotes": 11,
            "discussionId": "683e6aa657738c5cc3616ecc",
            "projectPage": "https://huggingface.co/collections/yolay/raif-arxivorg-pdf-250601413-682b16e5c0c2fa9b73811369",
            "githubRepo": "https://github.com/yuleiqin/RAIF",
            "ai_summary": "A method is proposed to enhance large language models in handling complex instructions through incentivized reasoning and reinforcement learning, improving performance and reducing computational load.",
            "ai_keywords": [
                "chain-of-thought (CoT)",
                "reinforcement learning (RL)",
                "rule-centric reward signals",
                "sample-wise contrast",
                "behavior cloning",
                "instruction following",
                "decomposition of complex instructions"
            ]
        },
        "publishedAt": "2025-06-02T04:11:44.000Z",
        "title": "Incentivizing Reasoning for Advanced Instruction-Following of Large\n  Language Models",
        "summary": "Existing large language models (LLMs) face challenges of following complex\ninstructions, especially when multiple constraints are present and organized in\nparalleling, chaining, and branching structures. One intuitive solution, namely\nchain-of-thought (CoT), is expected to universally improve capabilities of\nLLMs. However, we find that the vanilla CoT exerts a negative impact on\nperformance due to its superficial reasoning pattern of simply paraphrasing the\ninstructions. It fails to peel back the compositions of constraints for\nidentifying their relationship across hierarchies of types and dimensions. To\nthis end, we propose a systematic method to boost LLMs in dealing with complex\ninstructions via incentivizing reasoning for test-time compute scaling. First,\nwe stem from the decomposition of complex instructions under existing\ntaxonomies and propose a reproducible data acquisition method. Second, we\nexploit reinforcement learning (RL) with verifiable rule-centric reward signals\nto cultivate reasoning specifically for instruction following. We address the\nshallow, non-essential nature of reasoning under complex instructions via\nsample-wise contrast for superior CoT enforcement. We also exploit behavior\ncloning of experts to facilitate steady distribution shift from fast-thinking\nLLMs to skillful reasoners. Extensive evaluations on seven comprehensive\nbenchmarks confirm the validity of the proposed method, where a 1.5B LLM\nachieves 11.74% gains with performance comparable to a 8B LLM. Codes and data\nare available at https://github.com/yuleiqin/RAIF.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01413.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6390525c00fb8ec4a424e0ff",
            "avatarUrl": "/avatars/4302571e2ef4a9875491221aa630a329.svg",
            "fullname": "Yulei Qin",
            "name": "yolay",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01952",
            "authors": [
                {
                    "_id": "683ebebfb5052f5f8741c7f5",
                    "user": {
                        "_id": "6527b37c0ae663e384eb1b85",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
                        "isPro": true,
                        "fullname": "Atsuyuki Miyai",
                        "user": "AtsuMiyai",
                        "type": "user"
                    },
                    "name": "Atsuyuki Miyai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T09:39:31.731Z",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7f6",
                    "name": "Zaiying Zhao",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7f7",
                    "name": "Kazuki Egashira",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7f8",
                    "name": "Atsuki Sato",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7f9",
                    "name": "Tatsumi Sunada",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7fa",
                    "name": "Shota Onohara",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7fb",
                    "name": "Hiromasa Yamanishi",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7fc",
                    "name": "Mashiro Toyooka",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7fd",
                    "name": "Kunato Nishina",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7fe",
                    "name": "Ryoma Maeda",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c7ff",
                    "name": "Kiyoharu Aizawa",
                    "hidden": false
                },
                {
                    "_id": "683ebebfb5052f5f8741c800",
                    "name": "Toshihiko Yamasaki",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T17:59:45.000Z",
            "submittedOnDailyAt": "2025-06-03T07:52:49.121Z",
            "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks",
            "submittedOnDailyBy": {
                "_id": "6527b37c0ae663e384eb1b85",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
                "isPro": true,
                "fullname": "Atsuyuki Miyai",
                "user": "AtsuMiyai",
                "type": "user"
            },
            "summary": "Powered by a large language model (LLM), a web browsing agent operates web\nbrowsers in a human-like manner and offers a highly transparent path toward\nautomating a wide range of everyday tasks. As web agents become increasingly\ncapable and demonstrate proficiency in general browsing tasks, a critical\nquestion emerges: Can they go beyond general browsing to robustly handle tasks\nthat are tedious and complex, or chores that humans often avoid doing\nthemselves? In this paper, we introduce WebChoreArena, a new fully reproducible\nbenchmark comprising 532 carefully curated tasks designed to extend the scope\nof WebArena beyond general browsing to more labor-intensive and tedious tasks.\nWebChoreArena systematically integrates three key challenges: (i) Massive\nMemory tasks requiring accurate retrieval of large amounts of information in\nthe observations, (ii) Calculation tasks demanding precise mathematical\nreasoning, and (iii) Long-Term Memory tasks necessitating long-term memory\nacross multiple webpages. Built on top of the fully reproducible and widely\nadopted four WebArena simulation environments, WebChoreArena ensures strict\nreproducibility and enables fair, direct comparisons with the established\nWebArena benchmark, offering key insights into agent progress. Our experimental\nresults demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7\nSonnet, and Gemini 2.5 Pro, significant improvements in performance are\nobserved on WebChoreArena. These findings suggest that WebChoreArena is\nwell-suited to measure the advancement of state-of-the-art LLMs with greater\nclarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,\nthere remains substantial room for improvement compared to WebArena,\nhighlighting the increased challenges posed by WebChoreArena.",
            "upvotes": 10,
            "discussionId": "683ebec0b5052f5f8741c847",
            "ai_summary": "WebChoreArena, a new benchmark comprising 532 tasks, extends the scope of WebArena to more complex and tedious web browsing tasks, measuring advancements in LLM capabilities.",
            "ai_keywords": [
                "LLM",
                "Web browsing agent",
                "WebChoreArena",
                "benchmark",
                "general browsing",
                "Massive Memory tasks",
                "Calculation tasks",
                "Long-Term Memory tasks",
                "WebArena simulation environments",
                "GPT-4o",
                "Claude 3.7 Sonnet",
                "Gemini 2.5 Pro"
            ]
        },
        "publishedAt": "2025-06-02T13:59:45.000Z",
        "title": "WebChoreArena: Evaluating Web Browsing Agents on Realistic Tedious Web\n  Tasks",
        "summary": "Powered by a large language model (LLM), a web browsing agent operates web\nbrowsers in a human-like manner and offers a highly transparent path toward\nautomating a wide range of everyday tasks. As web agents become increasingly\ncapable and demonstrate proficiency in general browsing tasks, a critical\nquestion emerges: Can they go beyond general browsing to robustly handle tasks\nthat are tedious and complex, or chores that humans often avoid doing\nthemselves? In this paper, we introduce WebChoreArena, a new fully reproducible\nbenchmark comprising 532 carefully curated tasks designed to extend the scope\nof WebArena beyond general browsing to more labor-intensive and tedious tasks.\nWebChoreArena systematically integrates three key challenges: (i) Massive\nMemory tasks requiring accurate retrieval of large amounts of information in\nthe observations, (ii) Calculation tasks demanding precise mathematical\nreasoning, and (iii) Long-Term Memory tasks necessitating long-term memory\nacross multiple webpages. Built on top of the fully reproducible and widely\nadopted four WebArena simulation environments, WebChoreArena ensures strict\nreproducibility and enables fair, direct comparisons with the established\nWebArena benchmark, offering key insights into agent progress. Our experimental\nresults demonstrate that as LLMs evolve, represented by GPT-4o, Claude 3.7\nSonnet, and Gemini 2.5 Pro, significant improvements in performance are\nobserved on WebChoreArena. These findings suggest that WebChoreArena is\nwell-suited to measure the advancement of state-of-the-art LLMs with greater\nclarity. Nevertheless, the results also indicate that even with Gemini 2.5 Pro,\nthere remains substantial room for improvement compared to WebArena,\nhighlighting the increased challenges posed by WebChoreArena.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01952.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6527b37c0ae663e384eb1b85",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6527b37c0ae663e384eb1b85/zKWa8h6YU4BWfcitpM5Pl.png",
            "fullname": "Atsuyuki Miyai",
            "name": "AtsuMiyai",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24523",
            "authors": [
                {
                    "_id": "683eae20ff848ec97ef4d377",
                    "user": {
                        "_id": "616a9f40ebe369e3e0209c00",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/616a9f40ebe369e3e0209c00/lBzeeugOSWz-VkhGMr25Z.png",
                        "isPro": false,
                        "fullname": "Andrea Pedrotti",
                        "user": "andreapdr",
                        "type": "user"
                    },
                    "name": "Andrea Pedrotti",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:36:50.501Z",
                    "hidden": false
                },
                {
                    "_id": "683eae20ff848ec97ef4d378",
                    "user": {
                        "_id": "630f299da119d49bc1df7633",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671839656656-630f299da119d49bc1df7633.jpeg",
                        "isPro": false,
                        "fullname": "Michele Papucci",
                        "user": "mpapucci",
                        "type": "user"
                    },
                    "name": "Michele Papucci",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T12:33:42.976Z",
                    "hidden": false
                },
                {
                    "_id": "683eae20ff848ec97ef4d379",
                    "name": "Cristiano Ciaccio",
                    "hidden": false
                },
                {
                    "_id": "683eae20ff848ec97ef4d37a",
                    "user": {
                        "_id": "62e3fe792a8df5b22feed15e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e3fe792a8df5b22feed15e/iy8J4oGhW8Nr9QAgoH8ES.jpeg",
                        "isPro": false,
                        "fullname": "Alessio Miaschi",
                        "user": "alemiaschi",
                        "type": "user"
                    },
                    "name": "Alessio Miaschi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T12:33:45.653Z",
                    "hidden": false
                },
                {
                    "_id": "683eae20ff848ec97ef4d37b",
                    "name": "Giovanni Puccetti",
                    "hidden": false
                },
                {
                    "_id": "683eae20ff848ec97ef4d37c",
                    "name": "Felice Dell'Orletta",
                    "hidden": false
                },
                {
                    "_id": "683eae20ff848ec97ef4d37d",
                    "name": "Andrea Esuli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T12:33:30.000Z",
            "submittedOnDailyAt": "2025-06-03T09:40:00.949Z",
            "title": "Stress-testing Machine Generated Text Detection: Shifting Language\n  Models Writing Style to Fool Detectors",
            "submittedOnDailyBy": {
                "_id": "62e3fe792a8df5b22feed15e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e3fe792a8df5b22feed15e/iy8J4oGhW8Nr9QAgoH8ES.jpeg",
                "isPro": false,
                "fullname": "Alessio Miaschi",
                "user": "alemiaschi",
                "type": "user"
            },
            "summary": "Recent advancements in Generative AI and Large Language Models (LLMs) have\nenabled the creation of highly realistic synthetic content, raising concerns\nabout the potential for malicious use, such as misinformation and manipulation.\nMoreover, detecting Machine-Generated Text (MGT) remains challenging due to the\nlack of robust benchmarks that assess generalization to real-world scenarios.\nIn this work, we present a pipeline to test the resilience of state-of-the-art\nMGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed\nadversarial attacks. To challenge the detectors, we fine-tune language models\nusing Direct Preference Optimization (DPO) to shift the MGT style toward\nhuman-written text (HWT). This exploits the detectors' reliance on stylistic\nclues, making new generations more challenging to detect. Additionally, we\nanalyze the linguistic shifts induced by the alignment and which features are\nused by detectors to detect MGT texts. Our results show that detectors can be\neasily fooled with relatively few examples, resulting in a significant drop in\ndetection performance. This highlights the importance of improving detection\nmethods and making them robust to unseen in-domain texts.",
            "upvotes": 8,
            "discussionId": "683eae20ff848ec97ef4d3a3",
            "githubRepo": "https://github.com/gpucce/control_mgt/tree/devel-dpo?tab=readme-ov-file",
            "ai_summary": "Adversarial attacks using Direct Preference Optimization fine-tune language models to evade detection, leading to a significant drop in the performance of existing MGT detectors.",
            "ai_keywords": [
                "Generative AI",
                "Large Language Models",
                "Machine-Generated Text",
                "MGT detectors",
                "Mage",
                "Radar",
                "LLM-DetectAIve",
                "Direct Preference Optimization",
                "linguistic shifts",
                "stylistic clues"
            ]
        },
        "publishedAt": "2025-05-30T08:33:30.000Z",
        "title": "Stress-testing Machine Generated Text Detection: Shifting Language\n  Models Writing Style to Fool Detectors",
        "summary": "Recent advancements in Generative AI and Large Language Models (LLMs) have\nenabled the creation of highly realistic synthetic content, raising concerns\nabout the potential for malicious use, such as misinformation and manipulation.\nMoreover, detecting Machine-Generated Text (MGT) remains challenging due to the\nlack of robust benchmarks that assess generalization to real-world scenarios.\nIn this work, we present a pipeline to test the resilience of state-of-the-art\nMGT detectors (e.g., Mage, Radar, LLM-DetectAIve) to linguistically informed\nadversarial attacks. To challenge the detectors, we fine-tune language models\nusing Direct Preference Optimization (DPO) to shift the MGT style toward\nhuman-written text (HWT). This exploits the detectors' reliance on stylistic\nclues, making new generations more challenging to detect. Additionally, we\nanalyze the linguistic shifts induced by the alignment and which features are\nused by detectors to detect MGT texts. Our results show that detectors can be\neasily fooled with relatively few examples, resulting in a significant drop in\ndetection performance. This highlights the importance of improving detection\nmethods and making them robust to unseen in-domain texts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24523.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e3fe792a8df5b22feed15e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e3fe792a8df5b22feed15e/iy8J4oGhW8Nr9QAgoH8ES.jpeg",
            "fullname": "Alessio Miaschi",
            "name": "alemiaschi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24183",
            "authors": [
                {
                    "_id": "683e9a174de2ca71b8bc915b",
                    "user": {
                        "_id": "67de68f4f38795c545310088",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8604Li6_OlATOsTdY9oHL.png",
                        "isPro": false,
                        "fullname": "Yaoyu Zhu",
                        "user": "zhuyaoyu",
                        "type": "user"
                    },
                    "name": "Yaoyu Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:39:09.237Z",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc915c",
                    "user": {
                        "_id": "62c581177b48ba0bb8cdb737",
                        "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
                        "isPro": false,
                        "fullname": "di huang",
                        "user": "dihuang",
                        "type": "user"
                    },
                    "name": "Di Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:37:07.655Z",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc915d",
                    "name": "Hanqi Lyu",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc915e",
                    "name": "Xiaoyun Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc915f",
                    "name": "Chongxiao Li",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9160",
                    "name": "Wenxuan Shi",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9161",
                    "name": "Yutong Wu",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9162",
                    "name": "Jianan Mu",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9163",
                    "name": "Jinghua Wang",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9164",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9165",
                    "name": "Pengwei Jin",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9166",
                    "name": "Shuyao Cheng",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9167",
                    "name": "Shengwen Liang",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9168",
                    "name": "Xishan Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc9169",
                    "name": "Rui Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc916a",
                    "name": "Zidong Du",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc916b",
                    "name": "Qi Guo",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc916c",
                    "name": "Xing Hu",
                    "hidden": false
                },
                {
                    "_id": "683e9a174de2ca71b8bc916d",
                    "name": "Yunji Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T03:51:06.000Z",
            "submittedOnDailyAt": "2025-06-03T05:58:04.258Z",
            "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
            "submittedOnDailyBy": {
                "_id": "62c581177b48ba0bb8cdb737",
                "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
                "isPro": false,
                "fullname": "di huang",
                "user": "dihuang",
                "type": "user"
            },
            "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
            "upvotes": 7,
            "discussionId": "683e9a184de2ca71b8bc91b3",
            "projectPage": "https://iprc-dip.github.io/CodeV-R1",
            "ai_summary": "CodeV-R1, an RLVR framework for Verilog generation, achieves state-of-the-art performance in EDA using a rule-based testbench, round-trip data synthesis, and adaptive RLVR training.",
            "ai_keywords": [
                "reinforcement learning with verifiable reward",
                "RLVR",
                "electronic design automation",
                "EDA",
                "hardware description languages",
                "HDLs",
                "Verilog",
                "natural-language",
                "NL",
                "testbench generator",
                "equivalence checking",
                "round-trip data synthesis",
                "dataset",
                "two-stage training pipeline",
                "distillation",
                "adaptive DAPO",
                "VerilogEval",
                "RTLLM"
            ]
        },
        "publishedAt": "2025-05-29T23:51:06.000Z",
        "title": "CodeV-R1: Reasoning-Enhanced Verilog Generation",
        "summary": "Large language models (LLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have achieved breakthroughs on tasks with explicit,\nautomatable verification, such as software programming and mathematical\nproblems. Extending RLVR to electronic design automation (EDA), especially\nautomatically generating hardware description languages (HDLs) like Verilog\nfrom natural-language (NL) specifications, however, poses three key challenges:\nthe lack of automated and accurate verification environments, the scarcity of\nhigh-quality NL-code pairs, and the prohibitive computation cost of RLVR. To\nthis end, we introduce CodeV-R1, an RLVR framework for training Verilog\ngeneration LLMs. First, we develop a rule-based testbench generator that\nperforms robust equivalence checking against golden references. Second, we\npropose a round-trip data synthesis method that pairs open-source Verilog\nsnippets with LLM-generated NL descriptions, verifies code-NL-code consistency\nvia the generated testbench, and filters out inequivalent examples to yield a\nhigh-quality dataset. Third, we employ a two-stage \"distill-then-RL\" training\npipeline: distillation for the cold start of reasoning abilities, followed by\nadaptive DAPO, our novel RLVR algorithm that can reduce training cost by\nadaptively adjusting sampling rate. The resulting model, CodeV-R1-7B, achieves\n68.6% and 72.9% pass@1 on VerilogEval v2 and RTLLM v1.1, respectively,\nsurpassing prior state-of-the-art by 12~20%, while matching or even exceeding\nthe performance of 671B DeepSeek-R1. We will release our model, training\npipeline, and dataset to facilitate research in EDA and LLM communities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24183.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c581177b48ba0bb8cdb737",
            "avatarUrl": "/avatars/d1a85c28f13bb86481b6be80824eb1fa.svg",
            "fullname": "di huang",
            "name": "dihuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21179",
            "authors": [
                {
                    "_id": "6838c66dc60fb2fc462cec9f",
                    "user": {
                        "_id": "64d0eb731ed6649d70afb136",
                        "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
                        "isPro": true,
                        "fullname": "Chen Dar-Yen",
                        "user": "ChenDY",
                        "type": "user"
                    },
                    "name": "Dar-Yen Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:47:40.163Z",
                    "hidden": false
                },
                {
                    "_id": "6838c66dc60fb2fc462ceca0",
                    "user": {
                        "_id": "638c81fa61eb51017518fa31",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f0eCrzBrxz7Y9n25WkZ2v.png",
                        "isPro": false,
                        "fullname": "Hmrishav Bandyopadhyay",
                        "user": "Hmrishav",
                        "type": "user"
                    },
                    "name": "Hmrishav Bandyopadhyay",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T13:40:02.354Z",
                    "hidden": false
                },
                {
                    "_id": "6838c66dc60fb2fc462ceca1",
                    "name": "Kai Zou",
                    "hidden": false
                },
                {
                    "_id": "6838c66dc60fb2fc462ceca2",
                    "name": "Yi-Zhe Song",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64d0eb731ed6649d70afb136/BtItSeWg7RJJDSWqnZWIB.mp4"
            ],
            "publishedAt": "2025-05-27T13:30:46.000Z",
            "submittedOnDailyAt": "2025-06-03T07:58:27.904Z",
            "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model",
            "submittedOnDailyBy": {
                "_id": "64d0eb731ed6649d70afb136",
                "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
                "isPro": true,
                "fullname": "Chen Dar-Yen",
                "user": "ChenDY",
                "type": "user"
            },
            "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a universal plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!",
            "upvotes": 7,
            "discussionId": "6838c673c60fb2fc462cee10",
            "projectPage": "https://chendaryen.github.io/NAG.github.io/",
            "githubRepo": "https://github.com/ChenDarYen/Normalized-Attention-Guidance",
            "ai_summary": "Normalized Attention Guidance (NAG) enhances diffusion models by providing effective negative guidance across regimes and modalities without retraining.",
            "ai_keywords": [
                "negative guidance",
                "diffusion models",
                "few-step sampling",
                "Classifier-Free Guidance (CFG)",
                "Normalized Attention Guidance (NAG)",
                "attention space",
                "L1-based normalization",
                "extrapolation",
                "fidelity",
                "CLIP Score",
                "FID",
                "PFID",
                "ImageReward",
                "UNet",
                "DiT",
                "image",
                "video",
                "model-agnostic inference-time approach"
            ]
        },
        "publishedAt": "2025-05-27T09:30:46.000Z",
        "title": "Normalized Attention Guidance: Universal Negative Guidance for Diffusion\n  Model",
        "summary": "Negative guidance -- explicitly suppressing unwanted attributes -- remains a\nfundamental challenge in diffusion models, particularly in few-step sampling\nregimes. While Classifier-Free Guidance (CFG) works well in standard settings,\nit fails under aggressive sampling step compression due to divergent\npredictions between positive and negative branches. We present Normalized\nAttention Guidance (NAG), an efficient, training-free mechanism that applies\nextrapolation in attention space with L1-based normalization and refinement.\nNAG restores effective negative guidance where CFG collapses while maintaining\nfidelity. Unlike existing approaches, NAG generalizes across architectures\n(UNet, DiT), sampling regimes (few-step, multi-step), and modalities (image,\nvideo), functioning as a universal plug-in with minimal computational\noverhead. Through extensive experimentation, we demonstrate consistent\nimprovements in text alignment (CLIP Score), fidelity (FID, PFID), and\nhuman-perceived quality (ImageReward). Our ablation studies validate each\ndesign component, while user studies confirm significant preference for\nNAG-guided outputs. As a model-agnostic inference-time approach requiring no\nretraining, NAG provides effortless negative guidance for all modern diffusion\nframeworks -- pseudocode in the Appendix!",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64d0eb731ed6649d70afb136/BtItSeWg7RJJDSWqnZWIB.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21179.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64d0eb731ed6649d70afb136",
            "avatarUrl": "/avatars/4c591c86c575c82760126c39af5a02b4.svg",
            "fullname": "Chen Dar-Yen",
            "name": "ChenDY",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01928",
            "authors": [
                {
                    "_id": "683f0717c885f4668d274e08",
                    "name": "Subham Sekhar Sahoo",
                    "hidden": false
                },
                {
                    "_id": "683f0717c885f4668d274e09",
                    "name": "Zhihan Yang",
                    "hidden": false
                },
                {
                    "_id": "683f0717c885f4668d274e0a",
                    "name": "Yash Akhauri",
                    "hidden": false
                },
                {
                    "_id": "683f0717c885f4668d274e0b",
                    "name": "Johnna Liu",
                    "hidden": false
                },
                {
                    "_id": "683f0717c885f4668d274e0c",
                    "name": "Deepansha Singh",
                    "hidden": false
                },
                {
                    "_id": "683f0717c885f4668d274e0d",
                    "name": "Zhoujun Cheng",
                    "hidden": false
                },
                {
                    "_id": "683f0717c885f4668d274e0e",
                    "name": "Zhengzhong Liu",
                    "hidden": false
                },
                {
                    "_id": "683f0717c885f4668d274e0f",
                    "name": "Eric Xing",
                    "hidden": false
                },
                {
                    "_id": "683f0717c885f4668d274e10",
                    "name": "John Thickstun",
                    "hidden": false
                },
                {
                    "_id": "683f0717c885f4668d274e11",
                    "name": "Arash Vahdat",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/rzicfdPdBulGzJFE7soIU.png"
            ],
            "publishedAt": "2025-06-02T17:47:27.000Z",
            "submittedOnDailyAt": "2025-06-03T13:18:35.839Z",
            "title": "Esoteric Language Models",
            "submittedOnDailyBy": {
                "_id": "661839d73b412cdc851299c1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661839d73b412cdc851299c1/xicwANPQPTFdWfblisL2-.png",
                "isPro": false,
                "fullname": "Subham Sekhar Sahoo",
                "user": "s-sahoo",
                "type": "user"
            },
            "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)",
            "upvotes": 6,
            "discussionId": "683f0718c885f4668d274e45",
            "projectPage": "https://s-sahoo.com/Eso-LMs/",
            "githubRepo": "https://github.com/s-sahoo/Eso-LMs",
            "ai_summary": "Eso-LMs, a novel fusion of autoregressive and masked diffusion models, introduce KV caching to MDMs, achieving faster inference and superior performance on language modeling benchmarks.",
            "ai_keywords": [
                "diffusion-based language models",
                "autoregressive models",
                "Masked Diffusion Models (MDMs)",
                "Eso-LMs",
                "KV caching",
                "parallel generation",
                "inference-time efficiency",
                "sampling schedule"
            ]
        },
        "publishedAt": "2025-06-02T13:47:27.000Z",
        "title": "Esoteric Language Models",
        "summary": "Diffusion-based language models offer a compelling alternative to\nautoregressive (AR) models by enabling parallel and controllable generation.\nAmong this family of models, Masked Diffusion Models (MDMs) achieve the\nstrongest performance but still underperform AR models in perplexity and lack\nkey inference-time efficiency features--most notably, KV caching. In this work,\nwe introduce Eso-LMs, a new family of models that fuses AR and MDM paradigms,\nenabling smooth interpolation between their perplexities while overcoming their\nrespective limitations. Eso-LMs set a new state of the art on standard language\nmodeling benchmarks. Crucially, we are the **first to introduce KV caching for\nMDMs** while preserving parallel generation, significantly improving inference\nefficiency. Combined with an optimized sampling schedule, our method achieves\nup to **65x** faster inference than standard MDMs and **4x** faster inference\nthan prior semi-autoregressive approaches. We provide the code and model\ncheckpoints on the project page:\n[http://s-sahoo.github.io/Eso-LMs](http://s-sahoo.github.io/Eso-LMs)",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/rzicfdPdBulGzJFE7soIU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01928.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "661839d73b412cdc851299c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661839d73b412cdc851299c1/xicwANPQPTFdWfblisL2-.png",
            "fullname": "Subham Sekhar Sahoo",
            "name": "s-sahoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.00979",
            "authors": [
                {
                    "_id": "683e5e514c5b9f381d47c546",
                    "name": "Wayne Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e5e514c5b9f381d47c547",
                    "user": {
                        "_id": "652fc2605615e57807e3db19",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc2605615e57807e3db19/kbRcpR0YFQnU3IlziqCHf.png",
                        "isPro": false,
                        "fullname": "Changjiang Jiang",
                        "user": "arnodjiang",
                        "type": "user"
                    },
                    "name": "Changjiang Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:46:29.222Z",
                    "hidden": false
                },
                {
                    "_id": "683e5e514c5b9f381d47c548",
                    "name": "Zhonghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e5e514c5b9f381d47c549",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "683e5e514c5b9f381d47c54a",
                    "name": "Fengchang Yu",
                    "hidden": false
                },
                {
                    "_id": "683e5e514c5b9f381d47c54b",
                    "name": "Wei Peng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/652fc2605615e57807e3db19/-U07HT0V3A6bY3bqdqlXE.png",
                "https://cdn-uploads.huggingface.co/production/uploads/652fc2605615e57807e3db19/d1PA9x2WdjDLFB0aG6tGi.png",
                "https://cdn-uploads.huggingface.co/production/uploads/652fc2605615e57807e3db19/jdjxYj4Y-NZo_qwl5ChCP.png",
                "https://cdn-uploads.huggingface.co/production/uploads/652fc2605615e57807e3db19/0LMotcdT5xMWQ-Hhw1LBe.png"
            ],
            "publishedAt": "2025-06-01T12:20:22.000Z",
            "submittedOnDailyAt": "2025-06-03T14:41:26.872Z",
            "title": "IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and\n  Video AIGC Detection",
            "submittedOnDailyBy": {
                "_id": "652fc2605615e57807e3db19",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc2605615e57807e3db19/kbRcpR0YFQnU3IlziqCHf.png",
                "isPro": false,
                "fullname": "Changjiang Jiang",
                "user": "arnodjiang",
                "type": "user"
            },
            "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC) in\nvisual domains has resulted in highly realistic synthetic images and videos,\ndriven by sophisticated generative frameworks such as diffusion-based\narchitectures. While these breakthroughs open substantial opportunities, they\nsimultaneously raise critical concerns about content authenticity and\nintegrity. Many current AIGC detection methods operate as black-box binary\nclassifiers, which offer limited interpretability, and no approach supports\ndetecting both images and videos in a unified framework. This dual limitation\ncompromises model transparency, reduces trustworthiness, and hinders practical\ndeployment. To address these challenges, we introduce IVY-FAKE , a novel,\nunified, and large-scale dataset specifically designed for explainable\nmultimodal AIGC detection. Unlike prior benchmarks, which suffer from\nfragmented modality coverage and sparse annotations, IVY-FAKE contains over\n150,000 richly annotated training samples (images and videos) and 18,700\nevaluation examples, each accompanied by detailed natural-language reasoning\nbeyond simple binary labels. Building on this, we propose Ivy Explainable\nDetector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture\nthat jointly performs explainable detection for both image and video content.\nOur unified vision-language model achieves state-of-the-art performance across\nmultiple image and video detection benchmarks, highlighting the significant\nadvancements enabled by our dataset and modeling framework. Our data is\npublicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.",
            "upvotes": 6,
            "discussionId": "683e5e544c5b9f381d47c5d1",
            "projectPage": "https://pi3ai.github.io/IvyFake",
            "ai_summary": "IVY-FAKE dataset and Ivy Explainable Detector (IVY-XDETECTOR) architecture address the limitations of current AIGC detection by providing a unified, explainable framework for images and videos.",
            "ai_keywords": [
                "diffusion-based architectures",
                "IVY-FAKE",
                "Ivy Explainable Detector (IVY-XDETECTOR)",
                "unified AIGC detection",
                "vision-language model"
            ]
        },
        "publishedAt": "2025-06-01T08:20:22.000Z",
        "title": "IVY-FAKE: A Unified Explainable Framework and Benchmark for Image and\n  Video AIGC Detection",
        "summary": "The rapid advancement of Artificial Intelligence Generated Content (AIGC) in\nvisual domains has resulted in highly realistic synthetic images and videos,\ndriven by sophisticated generative frameworks such as diffusion-based\narchitectures. While these breakthroughs open substantial opportunities, they\nsimultaneously raise critical concerns about content authenticity and\nintegrity. Many current AIGC detection methods operate as black-box binary\nclassifiers, which offer limited interpretability, and no approach supports\ndetecting both images and videos in a unified framework. This dual limitation\ncompromises model transparency, reduces trustworthiness, and hinders practical\ndeployment. To address these challenges, we introduce IVY-FAKE , a novel,\nunified, and large-scale dataset specifically designed for explainable\nmultimodal AIGC detection. Unlike prior benchmarks, which suffer from\nfragmented modality coverage and sparse annotations, IVY-FAKE contains over\n150,000 richly annotated training samples (images and videos) and 18,700\nevaluation examples, each accompanied by detailed natural-language reasoning\nbeyond simple binary labels. Building on this, we propose Ivy Explainable\nDetector (IVY-XDETECTOR), a unified AIGC detection and explainable architecture\nthat jointly performs explainable detection for both image and video content.\nOur unified vision-language model achieves state-of-the-art performance across\nmultiple image and video detection benchmarks, highlighting the significant\nadvancements enabled by our dataset and modeling framework. Our data is\npublicly available at https://huggingface.co/datasets/AI-Safeguard/Ivy-Fake.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/652fc2605615e57807e3db19/-U07HT0V3A6bY3bqdqlXE.png",
            "https://cdn-uploads.huggingface.co/production/uploads/652fc2605615e57807e3db19/d1PA9x2WdjDLFB0aG6tGi.png",
            "https://cdn-uploads.huggingface.co/production/uploads/652fc2605615e57807e3db19/jdjxYj4Y-NZo_qwl5ChCP.png",
            "https://cdn-uploads.huggingface.co/production/uploads/652fc2605615e57807e3db19/0LMotcdT5xMWQ-Hhw1LBe.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00979.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652fc2605615e57807e3db19",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fc2605615e57807e3db19/kbRcpR0YFQnU3IlziqCHf.png",
            "fullname": "Changjiang Jiang",
            "name": "arnodjiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24842",
            "authors": [
                {
                    "_id": "683e9da357738c5cc36d5175",
                    "name": "Harsh Chaudhari",
                    "hidden": false
                },
                {
                    "_id": "683e9da357738c5cc36d5176",
                    "name": "Jamie Hayes",
                    "hidden": false
                },
                {
                    "_id": "683e9da357738c5cc36d5177",
                    "name": "Matthew Jagielski",
                    "hidden": false
                },
                {
                    "_id": "683e9da357738c5cc36d5178",
                    "name": "Ilia Shumailov",
                    "hidden": false
                },
                {
                    "_id": "683e9da357738c5cc36d5179",
                    "name": "Milad Nasr",
                    "hidden": false
                },
                {
                    "_id": "683e9da357738c5cc36d517a",
                    "name": "Alina Oprea",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T17:41:58.000Z",
            "submittedOnDailyAt": "2025-06-03T05:35:48.034Z",
            "title": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "6475c2794766357252e69e9f",
                "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
                "isPro": false,
                "fullname": "i",
                "user": "iliashum",
                "type": "user"
            },
            "summary": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies.",
            "upvotes": 6,
            "discussionId": "683e9da457738c5cc36d51b2",
            "ai_summary": "Adversarial injection of biased content can significantly propagate from teacher to student models during distillation, leading to frequent biased responses in both targeted and untargeted scenarios across various bias types and modalities.",
            "ai_keywords": [
                "model distillation",
                "language models",
                "adversarial manipulation",
                "data poisoning",
                "bias injection",
                "Untargeted Propagation",
                "Targeted Propagation",
                "perplexity filtering",
                "bias detection systems",
                "LLM-based autorater frameworks"
            ]
        },
        "publishedAt": "2025-05-30T13:41:58.000Z",
        "title": "Cascading Adversarial Bias from Injection to Distillation in Language\n  Models",
        "summary": "Model distillation has become essential for creating smaller, deployable\nlanguage models that retain larger system capabilities. However, widespread\ndeployment raises concerns about resilience to adversarial manipulation. This\npaper investigates vulnerability of distilled models to adversarial injection\nof biased content during training. We demonstrate that adversaries can inject\nsubtle biases into teacher models through minimal data poisoning, which\npropagates to student models and becomes significantly amplified. We propose\ntwo propagation modes: Untargeted Propagation, where bias affects multiple\ntasks, and Targeted Propagation, focusing on specific tasks while maintaining\nnormal behavior elsewhere. With only 25 poisoned samples (0.25% poisoning\nrate), student models generate biased responses 76.9% of the time in targeted\nscenarios - higher than 69.4% in teacher models. For untargeted propagation,\nadversarial bias appears 6x-29x more frequently in student models on unseen\ntasks. We validate findings across six bias types (targeted advertisements,\nphishing links, narrative manipulations, insecure coding practices), various\ndistillation methods, and different modalities spanning text and code\ngeneration. Our evaluation reveals shortcomings in current defenses -\nperplexity filtering, bias detection systems, and LLM-based autorater\nframeworks - against these attacks. Results expose significant security\nvulnerabilities in distilled models, highlighting need for specialized\nsafeguards. We propose practical design principles for building effective\nadversarial bias mitigation strategies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24842.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6475c2794766357252e69e9f",
            "avatarUrl": "/avatars/757ed789423113369868e972f21ce559.svg",
            "fullname": "i",
            "name": "iliashum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.01084",
            "authors": [
                {
                    "_id": "683ea4388be2e40086ea9056",
                    "user": {
                        "_id": "5fce0cfeb3dbf216ad31836a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625748458774-5fce0cfeb3dbf216ad31836a.png",
                        "isPro": false,
                        "fullname": "Saibo-creator",
                        "user": "Saibo-creator",
                        "type": "user"
                    },
                    "name": "Saibo Geng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T12:33:50.914Z",
                    "hidden": false
                },
                {
                    "_id": "683ea4388be2e40086ea9057",
                    "user": {
                        "_id": "6420afc71ccd411979dc12dc",
                        "avatarUrl": "/avatars/ee4a89ebc7a0716e21deaebc86e062e6.svg",
                        "isPro": false,
                        "fullname": "nathan ranchin",
                        "user": "nathanrchn",
                        "type": "user"
                    },
                    "name": "Nathan Ranchin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T09:39:27.313Z",
                    "hidden": false
                },
                {
                    "_id": "683ea4388be2e40086ea9058",
                    "name": "Yunzhen yao",
                    "hidden": false
                },
                {
                    "_id": "683ea4388be2e40086ea9059",
                    "name": "Maxime Peyrard",
                    "hidden": false
                },
                {
                    "_id": "683ea4388be2e40086ea905a",
                    "name": "Chris Wendler",
                    "hidden": false
                },
                {
                    "_id": "683ea4388be2e40086ea905b",
                    "name": "Michael Gastpar",
                    "hidden": false
                },
                {
                    "_id": "683ea4388be2e40086ea905c",
                    "name": "Robert West",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/VnqCCx4RG5-62le1tQcaW.png",
                "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/EGD7DC-lDXkTXOLD6r0IP.png",
                "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/YDPF1VlfQ0Yfynq9peK5s.png",
                "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/QY0t5DXqhIVBWrLpJvZm8.png",
                "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/af0uwzA8bV-7byretzGwW.png"
            ],
            "publishedAt": "2025-06-01T17:03:02.000Z",
            "submittedOnDailyAt": "2025-06-03T06:05:31.112Z",
            "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression",
            "submittedOnDailyBy": {
                "_id": "5fce0cfeb3dbf216ad31836a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625748458774-5fce0cfeb3dbf216ad31836a.png",
                "isPro": false,
                "fullname": "Saibo-creator",
                "user": "Saibo-creator",
                "type": "user"
            },
            "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.",
            "upvotes": 5,
            "discussionId": "683ea4398be2e40086ea90b7",
            "githubRepo": "https://github.com/epfl-dlab/zip2zip",
            "ai_summary": "A framework called zip2zip dynamically adjusts token vocabulary in LLMs at inference time using LZW compression, reducing token sequence length and improving inference speed.",
            "ai_keywords": [
                "LZW compression",
                "hypertokens",
                "embedding layer",
                "causal language modeling",
                "parameter-efficient finetuning"
            ]
        },
        "publishedAt": "2025-06-01T13:03:02.000Z",
        "title": "zip2zip: Inference-Time Adaptive Vocabularies for Language Models via\n  Token Compression",
        "summary": "Tokenization efficiency plays a critical role in the performance and cost of\nlarge language models (LLMs), yet most models rely on static tokenizers\noptimized for general-purpose corpora. These tokenizers' fixed vocabularies\noften fail to adapt to domain- or language-specific inputs, leading to longer\ntoken sequences and higher computational costs. We introduce zip2zip, a\nframework that enables LLMs to dynamically adjust token vocabulary at inference\ntime, allowing for fewer generated tokens and thus faster inference. zip2zip\nconsists of three key components: (1) a tokenizer based on Lempel-Ziv-Welch\n(LZW) compression that incrementally compresses tokens into reusable\n\"hypertokens\" on the fly; (2) an embedding layer that computes embeddings for\nnewly formed hypertokens at runtime; and (3) a causal language modeling variant\nthat trains the model to operate on hypertokenized, compressed sequences. We\nshow that an existing LLM can be zip2zip-fied in 10 GPU-hours via\nparameter-efficient finetuning. The resulting zip2zip LLMs effectively learn to\nuse hypertokens at inference time, reducing input and output sequence length by\n20-60\\%, with significant improvements in inference latency.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/VnqCCx4RG5-62le1tQcaW.png",
            "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/EGD7DC-lDXkTXOLD6r0IP.png",
            "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/YDPF1VlfQ0Yfynq9peK5s.png",
            "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/QY0t5DXqhIVBWrLpJvZm8.png",
            "https://cdn-uploads.huggingface.co/production/uploads/5fce0cfeb3dbf216ad31836a/af0uwzA8bV-7byretzGwW.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01084.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5fce0cfeb3dbf216ad31836a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625748458774-5fce0cfeb3dbf216ad31836a.png",
            "fullname": "Saibo-creator",
            "name": "Saibo-creator",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00512",
            "authors": [
                {
                    "_id": "683eb1f54c5b9f381d5b42ad",
                    "name": "Yang Zheng",
                    "hidden": false
                },
                {
                    "_id": "683eb1f54c5b9f381d5b42ae",
                    "name": "Mengqi Huang",
                    "hidden": false
                },
                {
                    "_id": "683eb1f54c5b9f381d5b42af",
                    "user": {
                        "_id": "6629d7c9fa14eaccf07d8633",
                        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
                        "isPro": false,
                        "fullname": "Nan Chen",
                        "user": "CNcreator0331",
                        "type": "user"
                    },
                    "name": "Nan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:36:45.797Z",
                    "hidden": false
                },
                {
                    "_id": "683eb1f54c5b9f381d5b42b0",
                    "name": "Zhendong Mao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T11:11:55.000Z",
            "submittedOnDailyAt": "2025-06-03T07:11:36.988Z",
            "title": "Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing",
            "submittedOnDailyBy": {
                "_id": "6629d7c9fa14eaccf07d8633",
                "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
                "isPro": false,
                "fullname": "Nan Chen",
                "user": "CNcreator0331",
                "type": "user"
            },
            "summary": "Text-guided 3D editing aims to precisely edit semantically relevant local 3D\nregions, which has significant potential for various practical applications\nranging from 3D games to film production. Existing methods typically follow a\nview-indiscriminate paradigm: editing 2D views indiscriminately and projecting\nthem back into 3D space. However, they overlook the different cross-view\ninterdependencies, resulting in inconsistent multi-view editing. In this study,\nwe argue that ideal consistent 3D editing can be achieved through a\nprogressive-views paradigm, which propagates editing semantics from\nthe editing-salient view to other editing-sparse views. Specifically, we\npropose Pro3D-Editor, a novel framework, which mainly includes\nPrimary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view\nSampler dynamically samples and edits the most editing-salient view as the\nprimary view. Key-view Render accurately propagates editing semantics from the\nprimary view to other key views through its Mixture-of-View-Experts Low-Rank\nAdaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based\non the edited multi-views. Extensive experiments demonstrate that our method\noutperforms existing methods in editing accuracy and spatial consistency.",
            "upvotes": 5,
            "discussionId": "683eb1f64c5b9f381d5b42ed",
            "projectPage": "https://shuoyueli4519.github.io/Pro3D-Editor",
            "ai_summary": "A progressive-views paradigm with Pro3D-Editor achieves consistent 3D editing by propagating semantics from key views to less edited ones.",
            "ai_keywords": [
                "progressive-views paradigm",
                "Primary-view Sampler",
                "Key-view Render",
                "Full-view Refiner",
                "Mixture-of-View-Experts Low-Rank Adaptation",
                "MoVE-LoRA"
            ]
        },
        "publishedAt": "2025-05-31T07:11:55.000Z",
        "title": "Pro3D-Editor : A Progressive-Views Perspective for Consistent and\n  Precise 3D Editing",
        "summary": "Text-guided 3D editing aims to precisely edit semantically relevant local 3D\nregions, which has significant potential for various practical applications\nranging from 3D games to film production. Existing methods typically follow a\nview-indiscriminate paradigm: editing 2D views indiscriminately and projecting\nthem back into 3D space. However, they overlook the different cross-view\ninterdependencies, resulting in inconsistent multi-view editing. In this study,\nwe argue that ideal consistent 3D editing can be achieved through a\nprogressive-views paradigm, which propagates editing semantics from\nthe editing-salient view to other editing-sparse views. Specifically, we\npropose Pro3D-Editor, a novel framework, which mainly includes\nPrimary-view Sampler, Key-view Render, and Full-view Refiner. Primary-view\nSampler dynamically samples and edits the most editing-salient view as the\nprimary view. Key-view Render accurately propagates editing semantics from the\nprimary view to other key views through its Mixture-of-View-Experts Low-Rank\nAdaption (MoVE-LoRA). Full-view Refiner edits and refines the 3D object based\non the edited multi-views. Extensive experiments demonstrate that our method\noutperforms existing methods in editing accuracy and spatial consistency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00512.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6629d7c9fa14eaccf07d8633",
            "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
            "fullname": "Nan Chen",
            "name": "CNcreator0331",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24452",
            "authors": [
                {
                    "_id": "683e8abd8e6e97efe0bf20d9",
                    "name": "Anda Tang",
                    "hidden": false
                },
                {
                    "_id": "683e8abd8e6e97efe0bf20da",
                    "name": "Yiming Dong",
                    "hidden": false
                },
                {
                    "_id": "683e8abd8e6e97efe0bf20db",
                    "user": {
                        "_id": "6371128eafbe42caa5a5222b",
                        "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
                        "isPro": false,
                        "fullname": "Yutao Zeng",
                        "user": "Taoer",
                        "type": "user"
                    },
                    "name": "Yutao Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:39:15.864Z",
                    "hidden": false
                },
                {
                    "_id": "683e8abd8e6e97efe0bf20dc",
                    "name": "zhou Xun",
                    "hidden": false
                },
                {
                    "_id": "683e8abd8e6e97efe0bf20dd",
                    "name": "Zhouchen Lin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/GfsrISs7G7K3rMBB1uws5.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eQqbJxhzxMNKVlyoqQPRA.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/q1H4zp_jhjZw57UYMnECl.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/TpjaRwRNbGOzWfL7y5C4P.png"
            ],
            "publishedAt": "2025-05-30T10:38:03.000Z",
            "submittedOnDailyAt": "2025-06-03T04:32:00.766Z",
            "title": "Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training",
            "submittedOnDailyBy": {
                "_id": "6371128eafbe42caa5a5222b",
                "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
                "isPro": false,
                "fullname": "Yutao Zeng",
                "user": "Taoer",
                "type": "user"
            },
            "summary": "The expanding computational costs and limited resources underscore the\ncritical need for budgeted-iteration training, which aims to achieve optimal\nlearning within predetermined iteration budgets.While learning rate schedules\nfundamentally govern the performance of different networks and tasks,\nparticularly in budgeted-iteration scenarios, their design remains largely\nheuristic, lacking theoretical foundations.In addition, the optimal learning\nrate schedule requires extensive trial-and-error selection, making the training\nprocess inefficient.In this work, we propose the Unified Budget-Aware (UBA)\nschedule, a theoretically grounded learning rate schedule that consistently\noutperforms commonly-used schedules among diverse architectures and tasks under\ndifferent constrained training budgets.First, we bridge the gap by constructing\na novel training budget-aware optimization framework, which explicitly accounts\nfor the robustness to landscape curvature variations.From this framework, we\nderive the UBA schedule, controlled by a single hyper-parameter varphi that\nprovides a trade-off between flexibility and simplicity, eliminating the need\nfor per-network numerical optimization. Moreover, we establish a theoretical\nconnection between varphi and the condition number, adding interpretation\nand justification to our approach. Besides, we prove the convergence for\ndifferent values of varphi.We offer practical guidelines for its selection\nvia theoretical analysis and empirical results.xtensive experimental results\nshow that UBA consistently surpasses the commonly-used schedules\nacross diverse vision and language tasks, spanning network architectures (e.g.,\nResNet, OLMo) and scales, under different training-iteration budgets.",
            "upvotes": 5,
            "discussionId": "683e8abf8e6e97efe0bf2155",
            "ai_summary": "A unified budget-aware learning rate schedule is proposed to optimize training within limited iteration budgets, outperforming traditional schedules across various tasks and network architectures.",
            "ai_keywords": [
                "budgeted-iteration training",
                "learning rate schedules",
                "Unified Budget-Aware (UBA) schedule",
                "training budget-aware optimization framework",
                "robustness to landscape curvature variations",
                "condition number",
                "convergence",
                "ResNet",
                "OLMo"
            ]
        },
        "publishedAt": "2025-05-30T06:38:03.000Z",
        "title": "Stepsize anything: A unified learning rate schedule for\n  budgeted-iteration training",
        "summary": "The expanding computational costs and limited resources underscore the\ncritical need for budgeted-iteration training, which aims to achieve optimal\nlearning within predetermined iteration budgets.While learning rate schedules\nfundamentally govern the performance of different networks and tasks,\nparticularly in budgeted-iteration scenarios, their design remains largely\nheuristic, lacking theoretical foundations.In addition, the optimal learning\nrate schedule requires extensive trial-and-error selection, making the training\nprocess inefficient.In this work, we propose the Unified Budget-Aware (UBA)\nschedule, a theoretically grounded learning rate schedule that consistently\noutperforms commonly-used schedules among diverse architectures and tasks under\ndifferent constrained training budgets.First, we bridge the gap by constructing\na novel training budget-aware optimization framework, which explicitly accounts\nfor the robustness to landscape curvature variations.From this framework, we\nderive the UBA schedule, controlled by a single hyper-parameter varphi that\nprovides a trade-off between flexibility and simplicity, eliminating the need\nfor per-network numerical optimization. Moreover, we establish a theoretical\nconnection between varphi and the condition number, adding interpretation\nand justification to our approach. Besides, we prove the convergence for\ndifferent values of varphi.We offer practical guidelines for its selection\nvia theoretical analysis and empirical results.xtensive experimental results\nshow that UBA consistently surpasses the commonly-used schedules\nacross diverse vision and language tasks, spanning network architectures (e.g.,\nResNet, OLMo) and scales, under different training-iteration budgets.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/GfsrISs7G7K3rMBB1uws5.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/eQqbJxhzxMNKVlyoqQPRA.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/q1H4zp_jhjZw57UYMnECl.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6371128eafbe42caa5a5222b/TpjaRwRNbGOzWfL7y5C4P.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24452.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6371128eafbe42caa5a5222b",
            "avatarUrl": "/avatars/c3b2ab35949c38aa3dfb2657a1300aac.svg",
            "fullname": "Yutao Zeng",
            "name": "Taoer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01920",
            "authors": [
                {
                    "_id": "683ec5047ec12b4ee9a21215",
                    "user": {
                        "_id": "63fc7fe6d44f50f559587f93",
                        "avatarUrl": "/avatars/0066af2fb1399a842c2ecca9e95b65dd.svg",
                        "isPro": false,
                        "fullname": "Serry Sibaee",
                        "user": "SerrySibaee",
                        "type": "user"
                    },
                    "name": "Serry Sibaee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:41:17.495Z",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a21216",
                    "user": {
                        "_id": "628f7a71dd993507cfcbe587",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                        "isPro": true,
                        "fullname": "Omartificial Intelligence Space",
                        "user": "Omartificial-Intelligence-Space",
                        "type": "user"
                    },
                    "name": "Omer Nacar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:41:40.815Z",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a21217",
                    "user": {
                        "_id": "647c60cd36e109abce3b3b15",
                        "avatarUrl": "/avatars/d702ec97cc2cde5ca7b5d16feb5f45c9.svg",
                        "isPro": false,
                        "fullname": "Adel Ammar",
                        "user": "ammaradel",
                        "type": "user"
                    },
                    "name": "Adel Ammar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:41:49.306Z",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a21218",
                    "name": "Yasser Al-Habashi",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a21219",
                    "name": "Abdulrahman Al-Batati",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a2121a",
                    "user": {
                        "_id": "67bde4936c7c7de3a68608d1",
                        "avatarUrl": "/avatars/e08afba50d8aae8f7698ccba4c1e7b4a.svg",
                        "isPro": false,
                        "fullname": "Wadii Boulila",
                        "user": "wboulila",
                        "type": "user"
                    },
                    "name": "Wadii Boulila",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:42:08.679Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T17:39:50.000Z",
            "submittedOnDailyAt": "2025-06-03T08:20:38.744Z",
            "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
            "submittedOnDailyBy": {
                "_id": "628f7a71dd993507cfcbe587",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                "isPro": true,
                "fullname": "Omartificial Intelligence Space",
                "user": "Omartificial-Intelligence-Space",
                "type": "user"
            },
            "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
            "upvotes": 4,
            "discussionId": "683ec5137ec12b4ee9a21496",
            "ai_summary": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.",
            "ai_keywords": [
                "evaluation framework",
                "Arabic Depth Mini Dataset (ADMD)",
                "GPT-4",
                "Claude 3.5 Sonnet",
                "Gemini Flash 1.5",
                "CommandR 100B",
                "Qwen-Max",
                "cultural understanding",
                "specialized knowledge",
                "cultural competence"
            ]
        },
        "publishedAt": "2025-06-02T13:39:50.000Z",
        "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
        "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01920.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628f7a71dd993507cfcbe587",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
            "fullname": "Omartificial Intelligence Space",
            "name": "Omartificial-Intelligence-Space",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 100
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01920",
            "authors": [
                {
                    "_id": "683ec5047ec12b4ee9a21215",
                    "user": {
                        "_id": "63fc7fe6d44f50f559587f93",
                        "avatarUrl": "/avatars/0066af2fb1399a842c2ecca9e95b65dd.svg",
                        "isPro": false,
                        "fullname": "Serry Sibaee",
                        "user": "SerrySibaee",
                        "type": "user"
                    },
                    "name": "Serry Sibaee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:41:17.495Z",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a21216",
                    "user": {
                        "_id": "628f7a71dd993507cfcbe587",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                        "isPro": true,
                        "fullname": "Omartificial Intelligence Space",
                        "user": "Omartificial-Intelligence-Space",
                        "type": "user"
                    },
                    "name": "Omer Nacar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:41:40.815Z",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a21217",
                    "user": {
                        "_id": "647c60cd36e109abce3b3b15",
                        "avatarUrl": "/avatars/d702ec97cc2cde5ca7b5d16feb5f45c9.svg",
                        "isPro": false,
                        "fullname": "Adel Ammar",
                        "user": "ammaradel",
                        "type": "user"
                    },
                    "name": "Adel Ammar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:41:49.306Z",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a21218",
                    "name": "Yasser Al-Habashi",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a21219",
                    "name": "Abdulrahman Al-Batati",
                    "hidden": false
                },
                {
                    "_id": "683ec5047ec12b4ee9a2121a",
                    "user": {
                        "_id": "67bde4936c7c7de3a68608d1",
                        "avatarUrl": "/avatars/e08afba50d8aae8f7698ccba4c1e7b4a.svg",
                        "isPro": false,
                        "fullname": "Wadii Boulila",
                        "user": "wboulila",
                        "type": "user"
                    },
                    "name": "Wadii Boulila",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:42:08.679Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T17:39:50.000Z",
            "submittedOnDailyAt": "2025-06-03T08:20:38.744Z",
            "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
            "submittedOnDailyBy": {
                "_id": "628f7a71dd993507cfcbe587",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                "isPro": true,
                "fullname": "Omartificial Intelligence Space",
                "user": "Omartificial-Intelligence-Space",
                "type": "user"
            },
            "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
            "upvotes": 4,
            "discussionId": "683ec5137ec12b4ee9a21496",
            "ai_summary": "A new evaluation framework and dataset, ADMD, are introduced to assess Arabic language models, highlighting variations in performance and emphasizing the importance of cultural competence.",
            "ai_keywords": [
                "evaluation framework",
                "Arabic Depth Mini Dataset (ADMD)",
                "GPT-4",
                "Claude 3.5 Sonnet",
                "Gemini Flash 1.5",
                "CommandR 100B",
                "Qwen-Max",
                "cultural understanding",
                "specialized knowledge",
                "cultural competence"
            ]
        },
        "publishedAt": "2025-06-02T13:39:50.000Z",
        "title": "From Guidelines to Practice: A New Paradigm for Arabic Language Model\n  Evaluation",
        "summary": "This paper addresses critical gaps in Arabic language model evaluation by\nestablishing comprehensive theoretical guidelines and introducing a novel\nevaluation framework. We first analyze existing Arabic evaluation datasets,\nidentifying significant issues in linguistic accuracy, cultural alignment, and\nmethodological rigor. To address these limitations in LLMs, we present the\nArabic Depth Mini Dataset (ADMD), a carefully curated collection of 490\nchallenging questions spanning ten major domains (42 sub-domains, see Figure 1.\nUsing ADMD, we evaluate five leading language models: GPT-4, Claude 3.5 Sonnet,\nGemini Flash 1.5, CommandR 100B, and Qwen-Max. Our results reveal significant\nvariations in model performance across different domains, with particular\nchallenges in areas requiring deep cultural understanding and specialized\nknowledge. Claude 3.5 Sonnet demonstrated the highest overall accuracy at 30\\%,\nshowing relative strength in mathematical theory in Arabic, Arabic language,\nand islamic domains. This work provides both theoretical foundations and\npractical insights for improving Arabic language model evaluation, emphasizing\nthe importance of cultural competence alongside technical capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01920.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628f7a71dd993507cfcbe587",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
            "fullname": "Omartificial Intelligence Space",
            "name": "Omartificial-Intelligence-Space",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 100
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01484",
            "authors": [
                {
                    "_id": "683eb167b3f3b41729e1d2e8",
                    "user": {
                        "_id": "662ce44c8b8705f30371fba8",
                        "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg",
                        "isPro": false,
                        "fullname": "Shuzhou Yuan",
                        "user": "shuzyuan",
                        "type": "user"
                    },
                    "name": "Shuzhou Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T10:45:13.850Z",
                    "hidden": false
                },
                {
                    "_id": "683eb167b3f3b41729e1d2e9",
                    "name": "Ercong Nie",
                    "hidden": false
                },
                {
                    "_id": "683eb167b3f3b41729e1d2ea",
                    "name": "Lukas Kouba",
                    "hidden": false
                },
                {
                    "_id": "683eb167b3f3b41729e1d2eb",
                    "name": "Ashish Yashwanth Kangen",
                    "hidden": false
                },
                {
                    "_id": "683eb167b3f3b41729e1d2ec",
                    "name": "Helmut Schmid",
                    "hidden": false
                },
                {
                    "_id": "683eb167b3f3b41729e1d2ed",
                    "name": "Hinrich Schutze",
                    "hidden": false
                },
                {
                    "_id": "683eb167b3f3b41729e1d2ee",
                    "name": "Michael Farber",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/662ce44c8b8705f30371fba8/7b4Mhx2INBhnjYZc55g7h.png"
            ],
            "publishedAt": "2025-06-02T09:45:05.000Z",
            "submittedOnDailyAt": "2025-06-03T08:19:44.458Z",
            "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification",
            "submittedOnDailyBy": {
                "_id": "662ce44c8b8705f30371fba8",
                "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg",
                "isPro": false,
                "fullname": "Shuzhou Yuan",
                "user": "shuzyuan",
                "type": "user"
            },
            "summary": "Detoxification, the task of rewriting harmful language into non-toxic text,\nhas become increasingly important amid the growing prevalence of toxic content\nonline. However, high-quality parallel datasets for detoxification, especially\nfor hate speech, remain scarce due to the cost and sensitivity of human\nannotation. In this paper, we propose a novel LLM-in-the-loop pipeline\nleveraging GPT-4o-mini for automated detoxification. We first replicate the\nParaDetox pipeline by replacing human annotators with an LLM and show that the\nLLM performs comparably to human annotation. Building on this, we construct\nPARADEHATE, a large-scale parallel dataset specifically for hatespeech\ndetoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate\ntext pairs and evaluate a wide range of baseline methods. Experimental results\nshow that models such as BART, fine-tuned on PARADEHATE, achieve better\nperformance in style accuracy, content preservation, and fluency, demonstrating\nthe effectiveness of LLM-generated detoxification text as a scalable\nalternative to human annotation.",
            "upvotes": 4,
            "discussionId": "683eb169b3f3b41729e1d370",
            "ai_summary": "A novel pipeline using GPT-4o-mini generates a large-scale dataset for hate speech detoxification, improving baseline model performance in style accuracy, content preservation, and fluency.",
            "ai_keywords": [
                "LLM-in-the-loop",
                "GPT-4o-mini",
                "ParaDetox",
                "PARADEHATE",
                "BART",
                "style accuracy",
                "content preservation",
                "fluency"
            ]
        },
        "publishedAt": "2025-06-02T05:45:05.000Z",
        "title": "LLM in the Loop: Creating the PARADEHATE Dataset for Hate Speech\n  Detoxification",
        "summary": "Detoxification, the task of rewriting harmful language into non-toxic text,\nhas become increasingly important amid the growing prevalence of toxic content\nonline. However, high-quality parallel datasets for detoxification, especially\nfor hate speech, remain scarce due to the cost and sensitivity of human\nannotation. In this paper, we propose a novel LLM-in-the-loop pipeline\nleveraging GPT-4o-mini for automated detoxification. We first replicate the\nParaDetox pipeline by replacing human annotators with an LLM and show that the\nLLM performs comparably to human annotation. Building on this, we construct\nPARADEHATE, a large-scale parallel dataset specifically for hatespeech\ndetoxification. We release PARADEHATE as a benchmark of over 8K hate/non-hate\ntext pairs and evaluate a wide range of baseline methods. Experimental results\nshow that models such as BART, fine-tuned on PARADEHATE, achieve better\nperformance in style accuracy, content preservation, and fluency, demonstrating\nthe effectiveness of LLM-generated detoxification text as a scalable\nalternative to human annotation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/662ce44c8b8705f30371fba8/7b4Mhx2INBhnjYZc55g7h.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01484.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662ce44c8b8705f30371fba8",
            "avatarUrl": "/avatars/b96a25a8c124e7caa9de06b7188bdc15.svg",
            "fullname": "Shuzhou Yuan",
            "name": "shuzyuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00789",
            "authors": [
                {
                    "_id": "683f23ffc7f66ac1b46b2e49",
                    "name": "Yixiao Zeng",
                    "hidden": false
                },
                {
                    "_id": "683f23ffc7f66ac1b46b2e4a",
                    "name": "Tianyu Cao",
                    "hidden": false
                },
                {
                    "_id": "683f23ffc7f66ac1b46b2e4b",
                    "name": "Danqing Wang",
                    "hidden": false
                },
                {
                    "_id": "683f23ffc7f66ac1b46b2e4c",
                    "name": "Xinran Zhao",
                    "hidden": false
                },
                {
                    "_id": "683f23ffc7f66ac1b46b2e4d",
                    "name": "Zimeng Qiu",
                    "hidden": false
                },
                {
                    "_id": "683f23ffc7f66ac1b46b2e4e",
                    "user": {
                        "_id": "6462db3052193f295764f95f",
                        "avatarUrl": "/avatars/fded932b65d6f7f2773fb2faecde122a.svg",
                        "isPro": false,
                        "fullname": "Morteza Ziyadi",
                        "user": "mziyadi",
                        "type": "user"
                    },
                    "name": "Morteza Ziyadi",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-03T16:37:16.891Z",
                    "hidden": false
                },
                {
                    "_id": "683f23ffc7f66ac1b46b2e4f",
                    "name": "Tongshuang Wu",
                    "hidden": false
                },
                {
                    "_id": "683f23ffc7f66ac1b46b2e50",
                    "name": "Lei Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-01T02:42:36.000Z",
            "submittedOnDailyAt": "2025-06-03T15:05:10.026Z",
            "title": "RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented\n  Generation Systems",
            "submittedOnDailyBy": {
                "_id": "64992b16406dbbb2c592634a",
                "avatarUrl": "/avatars/7d2e8f10f948db1974ca2d73e1834742.svg",
                "isPro": false,
                "fullname": "Yixiao Zeng",
                "user": "Rabinovich",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) enhances recency and factuality in\nanswers. However, existing evaluations rarely test how well these systems cope\nwith real-world noise, conflicting between internal and external retrieved\ncontexts, or fast-changing facts. We introduce Retrieval-Aware Robustness\nEvaluation (RARE), a unified framework and large-scale benchmark that jointly\nstress-tests query and document perturbations over dynamic, time-sensitive\ncorpora. One of the central features of RARE is a knowledge-graph-driven\nsynthesis pipeline (RARE-Get) that automatically extracts single and multi-hop\nrelations from the customized corpus and generates multi-level question sets\nwithout manual intervention. Leveraging this pipeline, we construct a dataset\n(RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and\npolicy documents and 48,322 questions whose distribution evolves as the\nunderlying sources change. To quantify resilience, we formalize\nretrieval-conditioned robustness metrics (RARE-Met) that capture a model's\nability to remain correct or recover when queries, documents, or real-world\nretrieval results are systematically altered. Our results show that RAG systems\nexhibit surprising vulnerability to perturbations, with document robustness\nconsistently being the weakest point regardless of generator size or\narchitecture. RAG systems consistently show lower robustness on multi-hop\nqueries than single-hop queries across all domains.",
            "upvotes": 4,
            "discussionId": "683f2400c7f66ac1b46b2e86",
            "githubRepo": "https://github.com/LeiLiLab/RARE",
            "ai_summary": "RARE evaluates the robustness of Retrieval-Augmented Generation (RAG) systems against real-world noise, context conflicts, and time-sensitive data with a knowledge-graph-driven benchmark.",
            "ai_keywords": [
                "Retrieval-Augmented Generation",
                "RAG",
                "Retrieval-Aware Robustness Evaluation",
                "RARE",
                "query perturbations",
                "document perturbations",
                "dynamic corpora",
                "knowledge-graph-driven synthesis pipeline",
                "RARE-Get",
                "RARE-Set",
                "retrieval-conditioned robustness metrics",
                "RARE-Met",
                "multi-hop queries",
                "single-hop queries"
            ]
        },
        "publishedAt": "2025-05-31T22:42:36.000Z",
        "title": "RARE: Retrieval-Aware Robustness Evaluation for Retrieval-Augmented\n  Generation Systems",
        "summary": "Retrieval-Augmented Generation (RAG) enhances recency and factuality in\nanswers. However, existing evaluations rarely test how well these systems cope\nwith real-world noise, conflicting between internal and external retrieved\ncontexts, or fast-changing facts. We introduce Retrieval-Aware Robustness\nEvaluation (RARE), a unified framework and large-scale benchmark that jointly\nstress-tests query and document perturbations over dynamic, time-sensitive\ncorpora. One of the central features of RARE is a knowledge-graph-driven\nsynthesis pipeline (RARE-Get) that automatically extracts single and multi-hop\nrelations from the customized corpus and generates multi-level question sets\nwithout manual intervention. Leveraging this pipeline, we construct a dataset\n(RARE-Set) spanning 400 expert-level time-sensitive finance, economics, and\npolicy documents and 48,322 questions whose distribution evolves as the\nunderlying sources change. To quantify resilience, we formalize\nretrieval-conditioned robustness metrics (RARE-Met) that capture a model's\nability to remain correct or recover when queries, documents, or real-world\nretrieval results are systematically altered. Our results show that RAG systems\nexhibit surprising vulnerability to perturbations, with document robustness\nconsistently being the weakest point regardless of generator size or\narchitecture. RAG systems consistently show lower robustness on multi-hop\nqueries than single-hop queries across all domains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00789.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64992b16406dbbb2c592634a",
            "avatarUrl": "/avatars/7d2e8f10f948db1974ca2d73e1834742.svg",
            "fullname": "Yixiao Zeng",
            "name": "Rabinovich",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24086",
            "authors": [
                {
                    "_id": "683ebe67140f76a0a5485d51",
                    "user": {
                        "_id": "668e6b47f59574a8ec2ae078",
                        "avatarUrl": "/avatars/1cbc80ee4fb4a832783bd3dbee032d6e.svg",
                        "isPro": false,
                        "fullname": "Zeeshan Khan",
                        "user": "zk95",
                        "type": "user"
                    },
                    "name": "Zeeshan Khan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T09:39:29.585Z",
                    "hidden": false
                },
                {
                    "_id": "683ebe67140f76a0a5485d52",
                    "name": "Shizhe Chen",
                    "hidden": false
                },
                {
                    "_id": "683ebe67140f76a0a5485d53",
                    "name": "Cordelia Schmid",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/638878e0c9a44f05de452e91/V8gVhCAESrievXk5RWL_7.png"
            ],
            "publishedAt": "2025-05-30T00:13:36.000Z",
            "submittedOnDailyAt": "2025-06-03T07:53:33.269Z",
            "title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation",
            "submittedOnDailyBy": {
                "_id": "638878e0c9a44f05de452e91",
                "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
                "isPro": false,
                "fullname": "Matthieu Futeral",
                "user": "matthieufp",
                "type": "user"
            },
            "summary": "Generating images from text involving complex and novel object arrangements\nremains a significant challenge for current text-to-image (T2I) models.\nAlthough prior layout-based methods improve object arrangements using spatial\nconstraints with 2D layouts, they often struggle to capture 3D positioning and\nsacrifice quality and coherence. In this work, we introduce ComposeAnything, a\nnovel framework for improving compositional image generation without retraining\nexisting T2I models. Our approach first leverages the chain-of-thought\nreasoning abilities of LLMs to produce 2.5D semantic layouts from text,\nconsisting of 2D object bounding boxes enriched with depth information and\ndetailed captions. Based on this layout, we generate a spatial and depth aware\ncoarse composite of objects that captures the intended composition, serving as\na strong and interpretable prior that replaces stochastic noise initialization\nin diffusion-based T2I models. This prior guides the denoising process through\nobject prior reinforcement and spatial-controlled denoising, enabling seamless\ngeneration of compositional objects and coherent backgrounds, while allowing\nrefinement of inaccurate priors. ComposeAnything outperforms state-of-the-art\nmethods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D\nspatial arrangements, high object counts, and surreal compositions. Human\nevaluations further demonstrate that our model generates high-quality images\nwith compositions that faithfully reflect the text.",
            "upvotes": 4,
            "discussionId": "683ebe6a140f76a0a5485e06",
            "projectPage": "https://zeeshank95.github.io/composeanything/ca.html",
            "ai_summary": "ComposeAnything improves text-to-image generation by using LLMs for 2.5D semantic layouts, enhancing object placement and coherence in diffusion-based models.",
            "ai_keywords": [
                "LLMs",
                "chain-of-thought reasoning",
                "2.5D semantic layouts",
                "object bounding boxes",
                "depth information",
                "spatial and depth aware",
                "coarse composite",
                "denoising process",
                "object prior reinforcement",
                "spatial-controlled denoising",
                "diffusion-based T2I models",
                "T2I-CompBench",
                "NSR-1K benchmarks"
            ]
        },
        "publishedAt": "2025-05-29T20:13:36.000Z",
        "title": "ComposeAnything: Composite Object Priors for Text-to-Image Generation",
        "summary": "Generating images from text involving complex and novel object arrangements\nremains a significant challenge for current text-to-image (T2I) models.\nAlthough prior layout-based methods improve object arrangements using spatial\nconstraints with 2D layouts, they often struggle to capture 3D positioning and\nsacrifice quality and coherence. In this work, we introduce ComposeAnything, a\nnovel framework for improving compositional image generation without retraining\nexisting T2I models. Our approach first leverages the chain-of-thought\nreasoning abilities of LLMs to produce 2.5D semantic layouts from text,\nconsisting of 2D object bounding boxes enriched with depth information and\ndetailed captions. Based on this layout, we generate a spatial and depth aware\ncoarse composite of objects that captures the intended composition, serving as\na strong and interpretable prior that replaces stochastic noise initialization\nin diffusion-based T2I models. This prior guides the denoising process through\nobject prior reinforcement and spatial-controlled denoising, enabling seamless\ngeneration of compositional objects and coherent backgrounds, while allowing\nrefinement of inaccurate priors. ComposeAnything outperforms state-of-the-art\nmethods on the T2I-CompBench and NSR-1K benchmarks for prompts with 2D/3D\nspatial arrangements, high object counts, and surreal compositions. Human\nevaluations further demonstrate that our model generates high-quality images\nwith compositions that faithfully reflect the text.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638878e0c9a44f05de452e91/V8gVhCAESrievXk5RWL_7.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24086.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "638878e0c9a44f05de452e91",
            "avatarUrl": "/avatars/5748195a0ac761e2776548aabc3a53e3.svg",
            "fullname": "Matthieu Futeral",
            "name": "matthieufp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22954",
            "authors": [
                {
                    "_id": "6839e5a195a2f11f0c38a83a",
                    "name": "Jenny Zhang",
                    "hidden": false
                },
                {
                    "_id": "6839e5a195a2f11f0c38a83b",
                    "user": {
                        "_id": "640530fd0ab5e22719fb199a",
                        "avatarUrl": "/avatars/6b023f496a2f8cb5f37ec617d84179b7.svg",
                        "isPro": false,
                        "fullname": "Shengran HU",
                        "user": "Shengran",
                        "type": "user"
                    },
                    "name": "Shengran Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:43:55.369Z",
                    "hidden": false
                },
                {
                    "_id": "6839e5a195a2f11f0c38a83c",
                    "name": "Cong Lu",
                    "hidden": false
                },
                {
                    "_id": "6839e5a195a2f11f0c38a83d",
                    "name": "Robert Lange",
                    "hidden": false
                },
                {
                    "_id": "6839e5a195a2f11f0c38a83e",
                    "name": "Jeff Clune",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T00:26:15.000Z",
            "submittedOnDailyAt": "2025-06-03T17:08:00.419Z",
            "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents",
            "submittedOnDailyBy": {
                "_id": "640530fd0ab5e22719fb199a",
                "avatarUrl": "/avatars/6b023f496a2f8cb5f37ec617d84179b7.svg",
                "isPro": false,
                "fullname": "Shengran HU",
                "user": "Shengran",
                "type": "user"
            },
            "summary": "Today's AI systems have human-designed, fixed architectures and cannot\nautonomously and continuously improve themselves. The advance of AI could\nitself be automated. If done safely, that would accelerate AI development and\nallow us to reap its benefits much sooner. Meta-learning can automate the\ndiscovery of novel algorithms, but is limited by first-order improvements and\nthe human design of a suitable search space. The G\\\"odel machine proposed a\ntheoretical alternative: a self-improving AI that repeatedly modifies itself in\na provably beneficial manner. Unfortunately, proving that most changes are net\nbeneficial is impossible in practice. We introduce the Darwin G\\\"odel Machine\n(DGM), a self-improving system that iteratively modifies its own code (thereby\nalso improving its ability to modify its own codebase) and empirically\nvalidates each change using coding benchmarks. Inspired by Darwinian evolution\nand open-endedness research, the DGM maintains an archive of generated coding\nagents. It grows the archive by sampling an agent from it and using a\nfoundation model to create a new, interesting, version of the sampled agent.\nThis open-ended exploration forms a growing tree of diverse, high-quality\nagents and allows the parallel exploration of many different paths through the\nsearch space. Empirically, the DGM automatically improves its coding\ncapabilities (e.g., better code editing tools, long-context window management,\npeer-review mechanisms), increasing performance on SWE-bench from 20.0% to\n50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly\noutperforms baselines without self-improvement or open-ended exploration. All\nexperiments were done with safety precautions (e.g., sandboxing, human\noversight). The DGM is a significant step toward self-improving AI, capable of\ngathering its own stepping stones along paths that unfold into endless\ninnovation.",
            "upvotes": 4,
            "discussionId": "6839e5a295a2f11f0c38a874",
            "ai_summary": "The Darwin G\\\"odel Machine improves its coding capabilities through iterative self-modification and open-ended exploration, surpassing other approaches in benchmarks.",
            "ai_keywords": [
                "G\\\"odel machine",
                "self-improving AI",
                "coding benchmarks",
                "SWE-bench",
                "Polyglot",
                "sandboxing",
                "human oversight",
                "foundation model"
            ]
        },
        "publishedAt": "2025-05-28T20:26:15.000Z",
        "title": "Darwin Godel Machine: Open-Ended Evolution of Self-Improving Agents",
        "summary": "Today's AI systems have human-designed, fixed architectures and cannot\nautonomously and continuously improve themselves. The advance of AI could\nitself be automated. If done safely, that would accelerate AI development and\nallow us to reap its benefits much sooner. Meta-learning can automate the\ndiscovery of novel algorithms, but is limited by first-order improvements and\nthe human design of a suitable search space. The G\\\"odel machine proposed a\ntheoretical alternative: a self-improving AI that repeatedly modifies itself in\na provably beneficial manner. Unfortunately, proving that most changes are net\nbeneficial is impossible in practice. We introduce the Darwin G\\\"odel Machine\n(DGM), a self-improving system that iteratively modifies its own code (thereby\nalso improving its ability to modify its own codebase) and empirically\nvalidates each change using coding benchmarks. Inspired by Darwinian evolution\nand open-endedness research, the DGM maintains an archive of generated coding\nagents. It grows the archive by sampling an agent from it and using a\nfoundation model to create a new, interesting, version of the sampled agent.\nThis open-ended exploration forms a growing tree of diverse, high-quality\nagents and allows the parallel exploration of many different paths through the\nsearch space. Empirically, the DGM automatically improves its coding\ncapabilities (e.g., better code editing tools, long-context window management,\npeer-review mechanisms), increasing performance on SWE-bench from 20.0% to\n50.0%, and on Polyglot from 14.2% to 30.7%. Furthermore, the DGM significantly\noutperforms baselines without self-improvement or open-ended exploration. All\nexperiments were done with safety precautions (e.g., sandboxing, human\noversight). The DGM is a significant step toward self-improving AI, capable of\ngathering its own stepping stones along paths that unfold into endless\ninnovation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22954.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640530fd0ab5e22719fb199a",
            "avatarUrl": "/avatars/6b023f496a2f8cb5f37ec617d84179b7.svg",
            "fullname": "Shengran HU",
            "name": "Shengran",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21724",
            "authors": [
                {
                    "_id": "683b44583f2842f6afcc5e6f",
                    "name": "Cheng Luo",
                    "hidden": false
                },
                {
                    "_id": "683b44583f2842f6afcc5e70",
                    "name": "Jianghui Wang",
                    "hidden": false
                },
                {
                    "_id": "683b44583f2842f6afcc5e71",
                    "user": {
                        "_id": "666ddb45c0f3d5afc27e85ba",
                        "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
                        "isPro": false,
                        "fullname": "Bing Li",
                        "user": "bing-li-ai",
                        "type": "user"
                    },
                    "name": "Bing Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:45:31.879Z",
                    "hidden": false
                },
                {
                    "_id": "683b44583f2842f6afcc5e72",
                    "name": "Siyang Song",
                    "hidden": false
                },
                {
                    "_id": "683b44583f2842f6afcc5e73",
                    "user": {
                        "_id": "6808bf97ffadd78ec71cb721",
                        "avatarUrl": "/avatars/9adca3142c06b8f69889fcbe85fa374d.svg",
                        "isPro": false,
                        "fullname": "Bernard Ghanem",
                        "user": "bernardghanem",
                        "type": "user"
                    },
                    "name": "Bernard Ghanem",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:45:46.322Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T20:12:46.000Z",
            "submittedOnDailyAt": "2025-06-03T08:26:23.533Z",
            "title": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions",
            "submittedOnDailyBy": {
                "_id": "666ddb45c0f3d5afc27e85ba",
                "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
                "isPro": false,
                "fullname": "Bing Li",
                "user": "bing-li-ai",
                "type": "user"
            },
            "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality.",
            "upvotes": 4,
            "discussionId": "683b445c3f2842f6afcc5f49",
            "ai_summary": "OmniResponse, a Multimodal Large Language Model, generates high-quality synchronized verbal and non-verbal listener responses using text as an intermediate modality.",
            "ai_keywords": [
                "Online Multimodal Conversational Response Generation",
                "OmniResponse",
                "Multimodal Large Language Model",
                "Chrono-Text",
                "TempoVoice",
                "ResponseNet",
                "audio-visual synchronization"
            ]
        },
        "publishedAt": "2025-05-27T16:12:46.000Z",
        "title": "OmniResponse: Online Multimodal Conversational Response Generation in\n  Dyadic Interactions",
        "summary": "In this paper, we introduce Online Multimodal Conversational Response\nGeneration (OMCRG), a novel task that aims to online generate synchronized\nverbal and non-verbal listener feedback, conditioned on the speaker's\nmultimodal input. OMCRG reflects natural dyadic interactions and poses new\nchallenges in achieving synchronization between the generated audio and facial\nresponses of the listener. To address these challenges, we innovatively\nintroduce text as an intermediate modality to bridge the audio and facial\nresponses. We hence propose OmniResponse, a Multimodal Large Language Model\n(MLLM) that autoregressively generates high-quality multi-modal listener\nresponses. OmniResponse leverages a pretrained LLM enhanced with two novel\ncomponents: Chrono-Text, which temporally anchors generated text tokens, and\nTempoVoice, a controllable online TTS module that produces speech synchronized\nwith facial reactions. To support further OMCRG research, we present\nResponseNet, a new dataset comprising 696 high-quality dyadic interactions\nfeaturing synchronized split-screen videos, multichannel audio, transcripts,\nand facial behavior annotations. Comprehensive evaluations conducted on\nResponseNet demonstrate that OmniResponse significantly outperforms baseline\nmodels in terms of semantic speech content, audio-visual synchronization, and\ngeneration quality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21724.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "666ddb45c0f3d5afc27e85ba",
            "avatarUrl": "/avatars/dce98fc77bd8cf9e348f2d91bc3c0225.svg",
            "fullname": "Bing Li",
            "name": "bing-li-ai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00723",
            "authors": [
                {
                    "_id": "683ee251668eea64b040a050",
                    "user": {
                        "_id": "62363277cd8e1f47c865190a",
                        "avatarUrl": "/avatars/fb42d42dedd4dde8df4109883cbe7ea9.svg",
                        "isPro": false,
                        "fullname": "Daniel Paleka",
                        "user": "dpaleka",
                        "type": "user"
                    },
                    "name": "Daniel Paleka",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:43:51.277Z",
                    "hidden": false
                },
                {
                    "_id": "683ee251668eea64b040a051",
                    "name": "Shashwat Goel",
                    "hidden": false
                },
                {
                    "_id": "683ee251668eea64b040a052",
                    "user": {
                        "_id": "63d86dbf3130cadcaf8bdd11",
                        "avatarUrl": "/avatars/29d79a0c6dcec01111ef192fecd0fa7a.svg",
                        "isPro": false,
                        "fullname": "Jonas Geiping",
                        "user": "JonasGeiping",
                        "type": "user"
                    },
                    "name": "Jonas Geiping",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:44:04.836Z",
                    "hidden": false
                },
                {
                    "_id": "683ee251668eea64b040a053",
                    "user": {
                        "_id": "63568f18ba90b4ea9fe91cb5",
                        "avatarUrl": "/avatars/3e8b3c573e20cf80d329a312bfc34728.svg",
                        "isPro": false,
                        "fullname": "Florian Tramer",
                        "user": "ftramer",
                        "type": "user"
                    },
                    "name": "Florian Tramèr",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:44:13.091Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T21:49:17.000Z",
            "submittedOnDailyAt": "2025-06-03T10:24:30.663Z",
            "title": "Pitfalls in Evaluating Language Model Forecasters",
            "submittedOnDailyBy": {
                "_id": "6506832221ac448013f94995",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
                "isPro": false,
                "fullname": "Shashwat Goel",
                "user": "shash42",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have recently been applied to forecasting tasks,\nwith some works claiming these systems match or exceed human performance. In\nthis paper, we argue that, as a community, we should be careful about such\nconclusions as evaluating LLM forecasters presents unique challenges. We\nidentify two broad categories of issues: (1) difficulty in trusting evaluation\nresults due to many forms of temporal leakage, and (2) difficulty in\nextrapolating from evaluation performance to real-world forecasting. Through\nsystematic analysis and concrete examples from prior work, we demonstrate how\nevaluation flaws can raise concerns about current and future performance\nclaims. We argue that more rigorous evaluation methodologies are needed to\nconfidently assess the forecasting abilities of LLMs.",
            "upvotes": 3,
            "discussionId": "683ee252668eea64b040a080",
            "ai_summary": "Evaluation of large language models for forecasting is flawed due to temporal leakage and difficulties in translating performance to real-world forecasting, necessitating more rigorous methodologies.",
            "ai_keywords": [
                "large language models",
                "forecasting",
                "temporal leakage",
                "evaluation methodologies"
            ]
        },
        "publishedAt": "2025-05-31T17:49:17.000Z",
        "title": "Pitfalls in Evaluating Language Model Forecasters",
        "summary": "Large language models (LLMs) have recently been applied to forecasting tasks,\nwith some works claiming these systems match or exceed human performance. In\nthis paper, we argue that, as a community, we should be careful about such\nconclusions as evaluating LLM forecasters presents unique challenges. We\nidentify two broad categories of issues: (1) difficulty in trusting evaluation\nresults due to many forms of temporal leakage, and (2) difficulty in\nextrapolating from evaluation performance to real-world forecasting. Through\nsystematic analysis and concrete examples from prior work, we demonstrate how\nevaluation flaws can raise concerns about current and future performance\nclaims. We argue that more rigorous evaluation methodologies are needed to\nconfidently assess the forecasting abilities of LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00723.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6506832221ac448013f94995",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
            "fullname": "Shashwat Goel",
            "name": "shash42",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.19621",
            "authors": [
                {
                    "_id": "683ea7297e58553a7f73c210",
                    "name": "George Kour",
                    "hidden": false
                },
                {
                    "_id": "683ea7297e58553a7f73c211",
                    "user": {
                        "_id": "671f8106d677d3a764a6f9a5",
                        "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
                        "isPro": false,
                        "fullname": "itay nakash",
                        "user": "itaynakash",
                        "type": "user"
                    },
                    "name": "Itay Nakash",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:44:49.716Z",
                    "hidden": false
                },
                {
                    "_id": "683ea7297e58553a7f73c212",
                    "name": "Ateret Anaby-Tavor",
                    "hidden": false
                },
                {
                    "_id": "683ea7297e58553a7f73c213",
                    "name": "Michal Shmueli-Scheuer",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/dQxbCsfx-MqMegYL0zCWu.png"
            ],
            "publishedAt": "2025-05-26T07:41:21.000Z",
            "submittedOnDailyAt": "2025-06-03T06:13:25.326Z",
            "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "671f8106d677d3a764a6f9a5",
                "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
                "isPro": false,
                "fullname": "itay nakash",
                "user": "itaynakash",
                "type": "user"
            },
            "summary": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS",
            "upvotes": 3,
            "discussionId": "683ea72b7e58553a7f73c277",
            "ai_summary": "The Preference, Opinion, and Belief survey assesses the subjective tendencies and biases of Large Language Models across various domains and highlights a trend of increased bias in newer model versions.",
            "ai_keywords": [
                "Large Language Models",
                "Preference",
                "Opinion",
                "and Belief survey",
                "reliability",
                "neutrality",
                "consistency",
                "reasoning mechanisms",
                "self-reflection mechanisms"
            ]
        },
        "publishedAt": "2025-05-26T03:41:21.000Z",
        "title": "Think Again! The Effect of Test-Time Compute on Preferences, Opinions,\n  and Beliefs of Large Language Models",
        "summary": "As Large Language Models (LLMs) become deeply integrated into human life and\nincreasingly influence decision-making, it's crucial to evaluate whether and to\nwhat extent they exhibit subjective preferences, opinions, and beliefs. These\ntendencies may stem from biases within the models, which may shape their\nbehavior, influence the advice and recommendations they offer to users, and\npotentially reinforce certain viewpoints. This paper presents the Preference,\nOpinion, and Belief survey (POBs), a benchmark developed to assess LLMs'\nsubjective inclinations across societal, cultural, ethical, and personal\ndomains. We applied our benchmark to evaluate leading open- and closed-source\nLLMs, measuring desired properties such as reliability, neutrality, and\nconsistency. In addition, we investigated the effect of increasing the\ntest-time compute, through reasoning and self-reflection mechanisms, on those\nmetrics. While effective in other tasks, our results show that these mechanisms\noffer only limited gains in our domain. Furthermore, we reveal that newer model\nversions are becoming less consistent and more biased toward specific\nviewpoints, highlighting a blind spot and a concerning trend. POBS:\nhttps://ibm.github.io/POBS",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/dQxbCsfx-MqMegYL0zCWu.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.19621.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "671f8106d677d3a764a6f9a5",
            "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
            "fullname": "itay nakash",
            "name": "itaynakash",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01074",
            "authors": [
                {
                    "_id": "683e923450c43488f7ad5e02",
                    "user": {
                        "_id": "61bf84c8ca59d6d196a1b4e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
                        "isPro": false,
                        "fullname": "Amir Hossein Kargaran",
                        "user": "kargaranamir",
                        "type": "user"
                    },
                    "name": "Amir Hossein Kargaran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:39:12.648Z",
                    "hidden": false
                },
                {
                    "_id": "683e923450c43488f7ad5e03",
                    "name": "Yihong Liu",
                    "hidden": false
                },
                {
                    "_id": "683e923450c43488f7ad5e04",
                    "name": "François Yvon",
                    "hidden": false
                },
                {
                    "_id": "683e923450c43488f7ad5e05",
                    "name": "Hinrich Schütze",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/61bf84c8ca59d6d196a1b4e8/wUPqk_AExNZUcWqQA39-k.png"
            ],
            "publishedAt": "2025-06-01T16:24:13.000Z",
            "submittedOnDailyAt": "2025-06-03T14:22:15.286Z",
            "title": "How Programming Concepts and Neurons Are Shared in Code Language Models",
            "submittedOnDailyBy": {
                "_id": "61bf84c8ca59d6d196a1b4e8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
                "isPro": false,
                "fullname": "Amir Hossein Kargaran",
                "user": "kargaranamir",
                "type": "user"
            },
            "summary": "Several studies have explored the mechanisms of large language models (LLMs)\nin coding tasks, but most have focused on programming languages (PLs) in a\nmonolingual setting. In this paper, we investigate the relationship between\nmultiple PLs and English in the concept space of LLMs. We perform a few-shot\ntranslation task on 21 PL pairs using two Llama-based models. By decoding the\nembeddings of intermediate layers during this task, we observe that the concept\nspace is closer to English (including PL keywords) and assigns high\nprobabilities to English tokens in the second half of the intermediate layers.\nWe analyze neuron activations for 11 PLs and English, finding that while\nlanguage-specific neurons are primarily concentrated in the bottom layers,\nthose exclusive to each PL tend to appear in the top layers. For PLs that are\nhighly aligned with multiple other PLs, identifying language-specific neurons\nis not feasible. These PLs also tend to have a larger keyword set than other\nPLs and are closer to the model's concept space regardless of the input/output\nPL in the translation task. Our findings provide insights into how LLMs\ninternally represent PLs, revealing structural patterns in the model's concept\nspace. Code is available at https://github.com/cisnlp/code-specific-neurons.",
            "upvotes": 2,
            "discussionId": "683e923550c43488f7ad5e2a",
            "githubRepo": "https://github.com/cisnlp/code-specific-neurons",
            "ai_summary": "LLMs representing multiple programming languages in their concept space tend to cluster closer to English and exhibit distinct neuron activations for specific languages, particularly in the upper layers, with highly aligned languages sharing similar representations.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "programming languages",
                "PLs",
                "concept space",
                "few-shot translation",
                "embeddings",
                "intermediate layers",
                "neuron activations",
                "language-specific neurons"
            ]
        },
        "publishedAt": "2025-06-01T12:24:13.000Z",
        "title": "How Programming Concepts and Neurons Are Shared in Code Language Models",
        "summary": "Several studies have explored the mechanisms of large language models (LLMs)\nin coding tasks, but most have focused on programming languages (PLs) in a\nmonolingual setting. In this paper, we investigate the relationship between\nmultiple PLs and English in the concept space of LLMs. We perform a few-shot\ntranslation task on 21 PL pairs using two Llama-based models. By decoding the\nembeddings of intermediate layers during this task, we observe that the concept\nspace is closer to English (including PL keywords) and assigns high\nprobabilities to English tokens in the second half of the intermediate layers.\nWe analyze neuron activations for 11 PLs and English, finding that while\nlanguage-specific neurons are primarily concentrated in the bottom layers,\nthose exclusive to each PL tend to appear in the top layers. For PLs that are\nhighly aligned with multiple other PLs, identifying language-specific neurons\nis not feasible. These PLs also tend to have a larger keyword set than other\nPLs and are closer to the model's concept space regardless of the input/output\nPL in the translation task. Our findings provide insights into how LLMs\ninternally represent PLs, revealing structural patterns in the model's concept\nspace. Code is available at https://github.com/cisnlp/code-specific-neurons.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61bf84c8ca59d6d196a1b4e8/wUPqk_AExNZUcWqQA39-k.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01074.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61bf84c8ca59d6d196a1b4e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61bf84c8ca59d6d196a1b4e8/L_NvUwlMYcye9X35z6f7e.jpeg",
            "fullname": "Amir Hossein Kargaran",
            "name": "kargaranamir",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 62
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00930",
            "authors": [
                {
                    "_id": "683e6f7a118b68a69239ef94",
                    "user": {
                        "_id": "64b73c51be30d6567a706974",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b73c51be30d6567a706974/VrpQoHC0ZltYT8lNmEHqK.png",
                        "isPro": false,
                        "fullname": "Yongqi Li",
                        "user": "YongqiLi",
                        "type": "user"
                    },
                    "name": "Yongqi Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T10:45:17.727Z",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef95",
                    "name": "Shen Zhou",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef96",
                    "name": "Xiaohu Li",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef97",
                    "name": "Xin Miao",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef98",
                    "name": "Jintao Wen",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef99",
                    "name": "Mayi Xu",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef9a",
                    "name": "Jianhao Chen",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef9b",
                    "name": "Birong Pan",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef9c",
                    "name": "Hankun Kang",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef9d",
                    "name": "Yuanyuan Zhu",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef9e",
                    "name": "Ming Zhong",
                    "hidden": false
                },
                {
                    "_id": "683e6f7a118b68a69239ef9f",
                    "name": "Tieyun Qian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-01T09:50:54.000Z",
            "submittedOnDailyAt": "2025-06-03T11:31:03.079Z",
            "title": "Aligning VLM Assistants with Personalized Situated Cognition",
            "submittedOnDailyBy": {
                "_id": "64b73c51be30d6567a706974",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b73c51be30d6567a706974/VrpQoHC0ZltYT8lNmEHqK.png",
                "isPro": false,
                "fullname": "Yongqi Li",
                "user": "YongqiLi",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) aligned with general human objectives, such as\nbeing harmless and hallucination-free, have become valuable assistants of\nhumans in managing visual tasks. However, people with diversified backgrounds\nhave different cognition even in the same situation. Consequently, they may\nhave personalized expectations for VLM assistants. This highlights the urgent\nneed to align VLM assistants with personalized situated cognition for\nreal-world assistance. To study this problem, we first simplify it by\ncharacterizing individuals based on the sociological concept of Role-Set. Then,\nwe propose to evaluate the individuals' actions to examine whether the\npersonalized alignment is achieved. Further, we construct a benchmark named\nPCogAlignBench, which includes 18k instances and 20 individuals with different\nRole-Sets. Finally, we present a framework called PCogAlign, which constructs a\ncognition-aware and action-based reward model for personalized alignment.\nExperimental results and human evaluations demonstrate the reliability of the\nPCogAlignBench and the effectiveness of our proposed PCogAlign. We will\nopen-source the constructed benchmark and code at\nhttps://github.com/NLPGM/PCogAlign.",
            "upvotes": 2,
            "discussionId": "683e6f7a118b68a69239efd1",
            "githubRepo": "https://github.com/liyongqi2002/PCogAlign",
            "ai_summary": "A framework called PCogAlign constructs a reward model for aligning vision-language models with personalized situated cognition, using a benchmark with varied Role-Sets.",
            "ai_keywords": [
                "Vision-language models",
                "Role-Set",
                "cognition-aware",
                "action-based",
                "reward model",
                "personalized alignment"
            ]
        },
        "publishedAt": "2025-06-01T05:50:54.000Z",
        "title": "Aligning VLM Assistants with Personalized Situated Cognition",
        "summary": "Vision-language models (VLMs) aligned with general human objectives, such as\nbeing harmless and hallucination-free, have become valuable assistants of\nhumans in managing visual tasks. However, people with diversified backgrounds\nhave different cognition even in the same situation. Consequently, they may\nhave personalized expectations for VLM assistants. This highlights the urgent\nneed to align VLM assistants with personalized situated cognition for\nreal-world assistance. To study this problem, we first simplify it by\ncharacterizing individuals based on the sociological concept of Role-Set. Then,\nwe propose to evaluate the individuals' actions to examine whether the\npersonalized alignment is achieved. Further, we construct a benchmark named\nPCogAlignBench, which includes 18k instances and 20 individuals with different\nRole-Sets. Finally, we present a framework called PCogAlign, which constructs a\ncognition-aware and action-based reward model for personalized alignment.\nExperimental results and human evaluations demonstrate the reliability of the\nPCogAlignBench and the effectiveness of our proposed PCogAlign. We will\nopen-source the constructed benchmark and code at\nhttps://github.com/NLPGM/PCogAlign.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00930.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b73c51be30d6567a706974",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b73c51be30d6567a706974/VrpQoHC0ZltYT8lNmEHqK.png",
            "fullname": "Yongqi Li",
            "name": "YongqiLi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00772",
            "authors": [
                {
                    "_id": "683ec2d53c81cc903bbb418c",
                    "name": "Zihang Liu",
                    "hidden": false
                },
                {
                    "_id": "683ec2d53c81cc903bbb418d",
                    "name": "Tianyu Pang",
                    "hidden": false
                },
                {
                    "_id": "683ec2d53c81cc903bbb418e",
                    "name": "Oleg Balabanov",
                    "hidden": false
                },
                {
                    "_id": "683ec2d53c81cc903bbb418f",
                    "name": "Chaoqun Yang",
                    "hidden": false
                },
                {
                    "_id": "683ec2d53c81cc903bbb4190",
                    "name": "Tianjin Huang",
                    "hidden": false
                },
                {
                    "_id": "683ec2d53c81cc903bbb4191",
                    "name": "Lu Yin",
                    "hidden": false
                },
                {
                    "_id": "683ec2d53c81cc903bbb4192",
                    "name": "Yaoqing Yang",
                    "hidden": false
                },
                {
                    "_id": "683ec2d53c81cc903bbb4193",
                    "name": "Shiwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-01T01:31:50.000Z",
            "submittedOnDailyAt": "2025-06-03T08:10:49.247Z",
            "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "65b04d2291e63920a7898c9e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
                "isPro": false,
                "fullname": "Liu",
                "user": "Shiweiliuiiiiiii",
                "type": "user"
            },
            "summary": "Recent studies have shown that supervised fine-tuning of LLMs on a small\nnumber of high-quality datasets can yield strong reasoning capabilities.\nHowever, full fine-tuning (Full FT), while powerful, is computationally\nexpensive and susceptible to overfitting and catastrophic forgetting,\nparticularly when data is limited. Sparse fine-tuning, which previously\nachieved notable success by updating only a small subset of model parameters,\noffers a promising trade-off between efficiency and effectiveness. Yet, it has\nlagged behind in the LLM era due to the difficulty of identifying parameters\ntruly critical for reasoning. In this work, we state that weights with the\nlargest magnitude after low-rank approximation are critical weights for\nfine-tuning, which we call Principal Weights. Surprisingly, while\nmagnitude-based sparse fine-tuning performs poorly as a baseline on LLM\nfine-tuning, it becomes highly effective after rank reduction. These insights\nmotivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only\nupdates the top 5% Principal Weights throughout training and consistently\nachieves better performance on reasoning tasks than Full FT, while maintaining\nmemory efficiency on par with popular parameter-efficient fine-tuning methods.\nIn addition to strong performance on target domains such as arithmetic\nreasoning, LIFT also retains up to 20% more source-domain knowledge, compared\nto Full FT and LoRA. Our code is available at:\nhttps://github.com/zihanghliu/LIFT.",
            "upvotes": 2,
            "discussionId": "683ec2d53c81cc903bbb41c4",
            "ai_summary": "Leveraging low-rank approximation to identify critical weights for sparse fine-tuning of large language models enhances performance and efficiency compared to full fine-tuning.",
            "ai_keywords": [
                "LLMs",
                "supervised fine-tuning",
                "full fine-tuning",
                "sparse fine-tuning",
                "low-rank approximation",
                "Principal Weights",
                "Low-rank Informed Sparse Fine-Tuning",
                "LIFT",
                "memory efficiency",
                "parameter-efficient fine-tuning",
                "reasoning tasks",
                "arithmetic reasoning",
                "source-domain knowledge",
                "LoRA"
            ]
        },
        "publishedAt": "2025-05-31T21:31:50.000Z",
        "title": "LIFT the Veil for the Truth: Principal Weights Emerge after Rank\n  Reduction for Reasoning-Focused Supervised Fine-Tuning",
        "summary": "Recent studies have shown that supervised fine-tuning of LLMs on a small\nnumber of high-quality datasets can yield strong reasoning capabilities.\nHowever, full fine-tuning (Full FT), while powerful, is computationally\nexpensive and susceptible to overfitting and catastrophic forgetting,\nparticularly when data is limited. Sparse fine-tuning, which previously\nachieved notable success by updating only a small subset of model parameters,\noffers a promising trade-off between efficiency and effectiveness. Yet, it has\nlagged behind in the LLM era due to the difficulty of identifying parameters\ntruly critical for reasoning. In this work, we state that weights with the\nlargest magnitude after low-rank approximation are critical weights for\nfine-tuning, which we call Principal Weights. Surprisingly, while\nmagnitude-based sparse fine-tuning performs poorly as a baseline on LLM\nfine-tuning, it becomes highly effective after rank reduction. These insights\nmotivate our method: Low-rank Informed Sparse Fine-Tuning (LIFT). LIFT only\nupdates the top 5% Principal Weights throughout training and consistently\nachieves better performance on reasoning tasks than Full FT, while maintaining\nmemory efficiency on par with popular parameter-efficient fine-tuning methods.\nIn addition to strong performance on target domains such as arithmetic\nreasoning, LIFT also retains up to 20% more source-domain knowledge, compared\nto Full FT and LoRA. Our code is available at:\nhttps://github.com/zihanghliu/LIFT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00772.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b04d2291e63920a7898c9e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b04d2291e63920a7898c9e/iUHs235G4bqK-KnH_94ti.jpeg",
            "fullname": "Liu",
            "name": "Shiweiliuiiiiiii",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.00530",
            "authors": [
                {
                    "_id": "683ee127345bccde696b3969",
                    "name": "Tianhui Liu",
                    "hidden": false
                },
                {
                    "_id": "683ee127345bccde696b396a",
                    "user": {
                        "_id": "6465d3bd63e7e09dd02e95c3",
                        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                        "isPro": false,
                        "fullname": "Jie Feng",
                        "user": "JJ-TMT",
                        "type": "user"
                    },
                    "name": "Jie Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T12:33:11.636Z",
                    "hidden": false
                },
                {
                    "_id": "683ee127345bccde696b396b",
                    "name": "Hetian Pang",
                    "hidden": false
                },
                {
                    "_id": "683ee127345bccde696b396c",
                    "name": "Xin Zhang",
                    "hidden": false
                },
                {
                    "_id": "683ee127345bccde696b396d",
                    "user": {
                        "_id": "6566fcb5118497d0af91dc3b",
                        "avatarUrl": "/avatars/e081d7d1b1657245cd818e5417cdcb2e.svg",
                        "isPro": false,
                        "fullname": "TIANJIAN OUYANG",
                        "user": "Ouyangtj",
                        "type": "user"
                    },
                    "name": "Tianjian Ouyang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:43:35.219Z",
                    "hidden": false
                },
                {
                    "_id": "683ee127345bccde696b396e",
                    "name": "Zhiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "683ee127345bccde696b396f",
                    "name": "Yong Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T12:25:33.000Z",
            "submittedOnDailyAt": "2025-06-03T10:22:25.230Z",
            "title": "CityLens: Benchmarking Large Language-Vision Models for Urban\n  Socioeconomic Sensing",
            "submittedOnDailyBy": {
                "_id": "6465d3bd63e7e09dd02e95c3",
                "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                "isPro": false,
                "fullname": "Jie Feng",
                "user": "JJ-TMT",
                "type": "user"
            },
            "summary": "Understanding urban socioeconomic conditions through visual data is a\nchallenging yet essential task for sustainable urban development and policy\nplanning. In this work, we introduce CityLens, a comprehensive\nbenchmark designed to evaluate the capabilities of large language-vision models\n(LLVMs) in predicting socioeconomic indicators from satellite and street view\nimagery. We construct a multi-modal dataset covering a total of 17 globally\ndistributed cities, spanning 6 key domains: economy, education, crime,\ntransport, health, and environment, reflecting the multifaceted nature of urban\nlife. Based on this dataset, we define 11 prediction tasks and utilize three\nevaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation,\nand Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across\nthese tasks. Our results reveal that while LLVMs demonstrate promising\nperceptual and reasoning capabilities, they still exhibit limitations in\npredicting urban socioeconomic indicators. CityLens provides a unified\nframework for diagnosing these limitations and guiding future efforts in using\nLLVMs to understand and predict urban socioeconomic patterns. Our codes and\ndatasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.",
            "upvotes": 2,
            "discussionId": "683ee128345bccde696b39ac",
            "githubRepo": "https://github.com/tsinghua-fib-lab/CityLens",
            "ai_summary": "CityLens evaluates large language-vision models' capabilities in predicting urban socioeconomic indicators using multi-modal datasets from various global cities.",
            "ai_keywords": [
                "CityLens",
                "large language-vision models",
                "LLVMs",
                "satellite imagery",
                "street view imagery",
                "multi-modal dataset",
                "socioeconomic indicators",
                "Direct Metric Prediction",
                "Normalized Metric Estimation",
                "Feature-Based Regression",
                "perceptual capabilities",
                "reasoning capabilities",
                "urban socioeconomic patterns"
            ]
        },
        "publishedAt": "2025-05-31T08:25:33.000Z",
        "title": "CityLens: Benchmarking Large Language-Vision Models for Urban\n  Socioeconomic Sensing",
        "summary": "Understanding urban socioeconomic conditions through visual data is a\nchallenging yet essential task for sustainable urban development and policy\nplanning. In this work, we introduce CityLens, a comprehensive\nbenchmark designed to evaluate the capabilities of large language-vision models\n(LLVMs) in predicting socioeconomic indicators from satellite and street view\nimagery. We construct a multi-modal dataset covering a total of 17 globally\ndistributed cities, spanning 6 key domains: economy, education, crime,\ntransport, health, and environment, reflecting the multifaceted nature of urban\nlife. Based on this dataset, we define 11 prediction tasks and utilize three\nevaluation paradigms: Direct Metric Prediction, Normalized Metric Estimation,\nand Feature-Based Regression. We benchmark 17 state-of-the-art LLVMs across\nthese tasks. Our results reveal that while LLVMs demonstrate promising\nperceptual and reasoning capabilities, they still exhibit limitations in\npredicting urban socioeconomic indicators. CityLens provides a unified\nframework for diagnosing these limitations and guiding future efforts in using\nLLVMs to understand and predict urban socioeconomic patterns. Our codes and\ndatasets are open-sourced via https://github.com/tsinghua-fib-lab/CityLens.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00530.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "fullname": "Jie Feng",
            "name": "JJ-TMT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00523",
            "authors": [
                {
                    "_id": "683efe2880e41655390ab9f5",
                    "name": "Xingtong Ge",
                    "hidden": false
                },
                {
                    "_id": "683efe2880e41655390ab9f6",
                    "name": "Xin Zhang",
                    "hidden": false
                },
                {
                    "_id": "683efe2880e41655390ab9f7",
                    "name": "Tongda Xu",
                    "hidden": false
                },
                {
                    "_id": "683efe2880e41655390ab9f8",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "683efe2880e41655390ab9f9",
                    "name": "Xinjie Zhang",
                    "hidden": false
                },
                {
                    "_id": "683efe2880e41655390ab9fa",
                    "name": "Yan Wang",
                    "hidden": false
                },
                {
                    "_id": "683efe2880e41655390ab9fb",
                    "name": "Jun Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T11:59:02.000Z",
            "submittedOnDailyAt": "2025-06-03T12:24:23.853Z",
            "title": "SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image\n  Distillation",
            "submittedOnDailyBy": {
                "_id": "65a94d2bbe0b2002351b6a8e",
                "avatarUrl": "/avatars/c2a7a4bff254c643c1158a20dd74dc0f.svg",
                "isPro": false,
                "fullname": "Xingtong Ge",
                "user": "domiso",
                "type": "user"
            },
            "summary": "The Distribution Matching Distillation (DMD) has been successfully applied to\ntext-to-image diffusion models such as Stable Diffusion (SD) 1.5. However,\nvanilla DMD suffers from convergence difficulties on large-scale flow-based\ntext-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze\nthe issues when applying vanilla DMD on large-scale models. Then, to overcome\nthe scalability challenge, we propose implicit distribution alignment (IDA) to\nregularize the distance between the generator and fake distribution.\nFurthermore, we propose intra-segment guidance (ISG) to relocate the timestep\nimportance distribution from the teacher model. With IDA alone, DMD converges\nfor SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1\ndev. Along with other improvements such as scaled up discriminator models, our\nfinal model, dubbed SenseFlow, achieves superior performance in\ndistillation for both diffusion based text-to-image models such as SDXL, and\nflow-matching models such as SD 3.5 Large and FLUX. The source code will be\navaliable at https://github.com/XingtongGe/SenseFlow.",
            "upvotes": 2,
            "discussionId": "683efe2b80e41655390abab9",
            "githubRepo": "https://github.com/XingtongGe/SenseFlow",
            "ai_summary": "Implicit distribution alignment and intra-segment guidance enhance distribution matching distillation for large-scale text-to-image and flow-based models, improving convergence and performance.",
            "ai_keywords": [
                "implicit distribution alignment",
                "intra-segment guidance",
                "distribution matching distillation",
                "text-to-image diffusion models",
                "flow-based models",
                "generator",
                "fake distribution",
                "timestep importance distribution",
                "SenseFlow",
                "discriminator models"
            ]
        },
        "publishedAt": "2025-05-31T07:59:02.000Z",
        "title": "SenseFlow: Scaling Distribution Matching for Flow-based Text-to-Image\n  Distillation",
        "summary": "The Distribution Matching Distillation (DMD) has been successfully applied to\ntext-to-image diffusion models such as Stable Diffusion (SD) 1.5. However,\nvanilla DMD suffers from convergence difficulties on large-scale flow-based\ntext-to-image models, such as SD 3.5 and FLUX. In this paper, we first analyze\nthe issues when applying vanilla DMD on large-scale models. Then, to overcome\nthe scalability challenge, we propose implicit distribution alignment (IDA) to\nregularize the distance between the generator and fake distribution.\nFurthermore, we propose intra-segment guidance (ISG) to relocate the timestep\nimportance distribution from the teacher model. With IDA alone, DMD converges\nfor SD 3.5; employing both IDA and ISG, DMD converges for SD 3.5 and FLUX.1\ndev. Along with other improvements such as scaled up discriminator models, our\nfinal model, dubbed SenseFlow, achieves superior performance in\ndistillation for both diffusion based text-to-image models such as SDXL, and\nflow-matching models such as SD 3.5 Large and FLUX. The source code will be\navaliable at https://github.com/XingtongGe/SenseFlow.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00523.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65a94d2bbe0b2002351b6a8e",
            "avatarUrl": "/avatars/c2a7a4bff254c643c1158a20dd74dc0f.svg",
            "fullname": "Xingtong Ge",
            "name": "domiso",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.00385",
            "authors": [
                {
                    "_id": "683e707763e27c6256f58a51",
                    "name": "Yakun Song",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a52",
                    "name": "Jiawei Chen",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a53",
                    "user": {
                        "_id": "63774ca43a63a2983ffc12f9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/jUJasGf5_2rwiEun_eYl7.png",
                        "isPro": false,
                        "fullname": "xiaobin zhuang",
                        "user": "xiaobinzhuang",
                        "type": "user"
                    },
                    "name": "Xiaobin Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:45:12.616Z",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a54",
                    "name": "Chenpeng Du",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a55",
                    "name": "Ziyang Ma",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a56",
                    "name": "Jian Wu",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a57",
                    "name": "Jian Cong",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a58",
                    "name": "Dongya Jia",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a59",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a5a",
                    "name": "Yuping Wang",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a5b",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "683e707763e27c6256f58a5c",
                    "name": "Xie Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T04:31:02.000Z",
            "submittedOnDailyAt": "2025-06-03T04:28:21.280Z",
            "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation",
            "submittedOnDailyBy": {
                "_id": "63774ca43a63a2983ffc12f9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/jUJasGf5_2rwiEun_eYl7.png",
                "isPro": false,
                "fullname": "xiaobin zhuang",
                "user": "xiaobinzhuang",
                "type": "user"
            },
            "summary": "Neural audio codecs have made significant strides in efficiently mapping raw\naudio waveforms into discrete token representations, which are foundational for\ncontemporary audio generative models. However, most existing codecs are\noptimized primarily for reconstruction quality, often at the expense of the\ndownstream modelability of the encoded tokens. Motivated by the need to\novercome this bottleneck, we introduce MagiCodec, a novel\nsingle-layer, streaming Transformer-based audio codec. MagiCodec is designed\nwith a multistage training pipeline that incorporates Gaussian noise injection\nand latent regularization, explicitly targeting the enhancement of semantic\nexpressiveness in the generated codes while preserving high reconstruction\nfidelity. We analytically derive the effect of noise injection in the frequency\ndomain, demonstrating its efficacy in attenuating high-frequency components and\nfostering robust tokenization. Extensive experimental evaluations show that\nMagiCodec surpasses state-of-the-art codecs in both reconstruction quality and\ndownstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like\ndistributions, as observed in natural languages, thereby improving\ncompatibility with language-model-based generative architectures. The code and\npre-trained models are available at https://github.com/Ereboas/MagiCodec.",
            "upvotes": 2,
            "discussionId": "683e707963e27c6256f58a98",
            "ai_summary": "MagiCodec, a Transformer-based audio codec, enhances semantic tokenization while maintaining high reconstruction quality, improving compatibility with generative models.",
            "ai_keywords": [
                "Transformer",
                "Gaussian noise injection",
                "latent regularization",
                "frequency domain",
                "Zipf-like distributions",
                "generative models"
            ]
        },
        "publishedAt": "2025-05-31T00:31:02.000Z",
        "title": "MagiCodec: Simple Masked Gaussian-Injected Codec for High-Fidelity\n  Reconstruction and Generation",
        "summary": "Neural audio codecs have made significant strides in efficiently mapping raw\naudio waveforms into discrete token representations, which are foundational for\ncontemporary audio generative models. However, most existing codecs are\noptimized primarily for reconstruction quality, often at the expense of the\ndownstream modelability of the encoded tokens. Motivated by the need to\novercome this bottleneck, we introduce MagiCodec, a novel\nsingle-layer, streaming Transformer-based audio codec. MagiCodec is designed\nwith a multistage training pipeline that incorporates Gaussian noise injection\nand latent regularization, explicitly targeting the enhancement of semantic\nexpressiveness in the generated codes while preserving high reconstruction\nfidelity. We analytically derive the effect of noise injection in the frequency\ndomain, demonstrating its efficacy in attenuating high-frequency components and\nfostering robust tokenization. Extensive experimental evaluations show that\nMagiCodec surpasses state-of-the-art codecs in both reconstruction quality and\ndownstream tasks. Notably, the tokens produced by MagiCodec exhibit Zipf-like\ndistributions, as observed in natural languages, thereby improving\ncompatibility with language-model-based generative architectures. The code and\npre-trained models are available at https://github.com/Ereboas/MagiCodec.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00385.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63774ca43a63a2983ffc12f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/jUJasGf5_2rwiEun_eYl7.png",
            "fullname": "xiaobin zhuang",
            "name": "xiaobinzhuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00381",
            "authors": [
                {
                    "_id": "683f147e791f5015dfcb644a",
                    "name": "Siavash Shams",
                    "hidden": false
                },
                {
                    "_id": "683f147e791f5015dfcb644b",
                    "name": "Richard Antonello",
                    "hidden": false
                },
                {
                    "_id": "683f147e791f5015dfcb644c",
                    "name": "Gavin Mischler",
                    "hidden": false
                },
                {
                    "_id": "683f147e791f5015dfcb644d",
                    "name": "Stephan Bickel",
                    "hidden": false
                },
                {
                    "_id": "683f147e791f5015dfcb644e",
                    "name": "Ashesh Mehta",
                    "hidden": false
                },
                {
                    "_id": "683f147e791f5015dfcb644f",
                    "name": "Nima Mesgarani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T04:17:19.000Z",
            "submittedOnDailyAt": "2025-06-03T13:58:33.621Z",
            "title": "Neuro2Semantic: A Transfer Learning Framework for Semantic\n  Reconstruction of Continuous Language from Human Intracranial EEG",
            "submittedOnDailyBy": {
                "_id": "6637d9685eedec13fcbd1f87",
                "avatarUrl": "/avatars/a72d4e289c28b4d91757241d7b25985d.svg",
                "isPro": false,
                "fullname": "Siavash",
                "user": "attentionisallyouneed369",
                "type": "user"
            },
            "summary": "Decoding continuous language from neural signals remains a significant\nchallenge in the intersection of neuroscience and artificial intelligence. We\nintroduce Neuro2Semantic, a novel framework that reconstructs the semantic\ncontent of perceived speech from intracranial EEG (iEEG) recordings. Our\napproach consists of two phases: first, an LSTM-based adapter aligns neural\nsignals with pre-trained text embeddings; second, a corrector module generates\ncontinuous, natural text directly from these aligned embeddings. This flexible\nmethod overcomes the limitations of previous decoding approaches and enables\nunconstrained text generation. Neuro2Semantic achieves strong performance with\nas little as 30 minutes of neural data, outperforming a recent state-of-the-art\nmethod in low-data settings. These results highlight the potential for\npractical applications in brain-computer interfaces and neural decoding\ntechnologies.",
            "upvotes": 2,
            "discussionId": "683f147f791f5015dfcb648a",
            "githubRepo": "https://github.com/SiavashShams/neuro2semantic",
            "ai_summary": "Neuro2Semantic reconstructs semantic content from neural signals using LSTM-based alignment and text generation, outperforming existing methods with limited data.",
            "ai_keywords": [
                "iEEG",
                "LSTM",
                "align",
                "text embeddings",
                "corrector module",
                "neural decoding",
                "brain-computer interfaces"
            ]
        },
        "publishedAt": "2025-05-31T00:17:19.000Z",
        "title": "Neuro2Semantic: A Transfer Learning Framework for Semantic\n  Reconstruction of Continuous Language from Human Intracranial EEG",
        "summary": "Decoding continuous language from neural signals remains a significant\nchallenge in the intersection of neuroscience and artificial intelligence. We\nintroduce Neuro2Semantic, a novel framework that reconstructs the semantic\ncontent of perceived speech from intracranial EEG (iEEG) recordings. Our\napproach consists of two phases: first, an LSTM-based adapter aligns neural\nsignals with pre-trained text embeddings; second, a corrector module generates\ncontinuous, natural text directly from these aligned embeddings. This flexible\nmethod overcomes the limitations of previous decoding approaches and enables\nunconstrained text generation. Neuro2Semantic achieves strong performance with\nas little as 30 minutes of neural data, outperforming a recent state-of-the-art\nmethod in low-data settings. These results highlight the potential for\npractical applications in brain-computer interfaces and neural decoding\ntechnologies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00381.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6637d9685eedec13fcbd1f87",
            "avatarUrl": "/avatars/a72d4e289c28b4d91757241d7b25985d.svg",
            "fullname": "Siavash",
            "name": "attentionisallyouneed369",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21668",
            "authors": [
                {
                    "_id": "683f11c08ba4378eeab62953",
                    "name": "Yongchao Chen",
                    "hidden": false
                },
                {
                    "_id": "683f11c08ba4378eeab62954",
                    "name": "Yueying Liu",
                    "hidden": false
                },
                {
                    "_id": "683f11c08ba4378eeab62955",
                    "name": "Junwei Zhou",
                    "hidden": false
                },
                {
                    "_id": "683f11c08ba4378eeab62956",
                    "user": {
                        "_id": "65c24ce188812bbe20724208",
                        "avatarUrl": "/avatars/f40826e6035dca7e217cb95724a4e952.svg",
                        "isPro": false,
                        "fullname": "Yilun Hao",
                        "user": "yilunhao",
                        "type": "user"
                    },
                    "name": "Yilun Hao",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-03T18:25:00.398Z",
                    "hidden": false
                },
                {
                    "_id": "683f11c08ba4378eeab62957",
                    "name": "Jingquan Wang",
                    "hidden": false
                },
                {
                    "_id": "683f11c08ba4378eeab62958",
                    "name": "Yang Zhang",
                    "hidden": false
                },
                {
                    "_id": "683f11c08ba4378eeab62959",
                    "name": "Chuchu Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T18:47:33.000Z",
            "submittedOnDailyAt": "2025-06-03T13:47:14.978Z",
            "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised\n  and Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "67266d21b7d88dbcf9e6c4aa",
                "avatarUrl": "/avatars/0328058e74c424473ef890d1fbdd3e4d.svg",
                "isPro": false,
                "fullname": "Yongchao Chen",
                "user": "yongchao98",
                "type": "user"
            },
            "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98.",
            "upvotes": 2,
            "discussionId": "683f11c18ba4378eeab629a3",
            "githubRepo": "https://github.com/yongchao98/R1-Code-Interpreter/",
            "ai_summary": "R1-Code-Interpreter extends text-only LLMs with improved code generation abilities through supervised fine-tuning and reinforcement learning, enhancing performance on diverse reasoning and planning tasks.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "textual reasoning",
                "code execution",
                "multi-turn supervised fine-tuning",
                "reinforcement learning",
                "R1-Code-Interpreter",
                "Qwen-2.5",
                "GRPO",
                "PPO",
                "masked vs. unmasked code outputs",
                "self-checking behavior"
            ]
        },
        "publishedAt": "2025-05-27T14:47:33.000Z",
        "title": "R1-Code-Interpreter: Training LLMs to Reason with Code via Supervised\n  and Reinforcement Learning",
        "summary": "Despite advances in reasoning and planning of R1-like models, Large Language\nModels (LLMs) still struggle with tasks requiring precise computation, symbolic\nmanipulation, optimization, and algorithmic reasoning, in which textual\nreasoning lacks the rigor of code execution. A key challenge is enabling LLMs\nto decide when to use textual reasoning versus code generation. While OpenAI\ntrains models to invoke a Code Interpreter as needed, public research lacks\nguidance on aligning pre-trained LLMs to effectively leverage code and\ngeneralize across diverse tasks. We present R1-Code-Interpreter, an extension\nof a text-only LLM trained via multi-turn supervised fine-tuning (SFT) and\nreinforcement learning (RL) to autonomously generate multiple code queries\nduring step-by-step reasoning. We curate 144 reasoning and planning tasks (107\nfor training, 37 for testing), each with over 200 diverse questions. We\nfine-tune Qwen-2.5 models (3B/7B/14B) using various SFT and RL strategies,\ninvestigating different answer formats, reasoning vs. non-reasoning models,\ncold vs. warm starts, GRPO vs. PPO, and masked vs. unmasked code outputs.\nUnlike prior RL work on narrow domains, we find that Code Interpreter training\nis significantly harder due to high task diversity and expensive code\nexecution, highlighting the critical role of the SFT stage. Our final model,\nR1-CI-14B, improves average accuracy on the 37 test tasks from 44.0\\% to\n64.1\\%, outperforming GPT-4o (text-only: 58.6\\%) and approaching GPT-4o with\nCode Interpreter (70.9\\%), with the emergent self-checking behavior via code\ngeneration. Datasets, Codes, and Models are available at\nhttps://github.com/yongchao98/R1-Code-Interpreter and\nhttps://huggingface.co/yongchao98.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21668.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67266d21b7d88dbcf9e6c4aa",
            "avatarUrl": "/avatars/0328058e74c424473ef890d1fbdd3e4d.svg",
            "fullname": "Yongchao Chen",
            "name": "yongchao98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20285",
            "authors": [
                {
                    "_id": "683eef14fa20482e30563cf2",
                    "name": "Weiqi Wu",
                    "hidden": false
                },
                {
                    "_id": "683eef14fa20482e30563cf3",
                    "name": "Xin Guan",
                    "hidden": false
                },
                {
                    "_id": "683eef14fa20482e30563cf4",
                    "name": "Shen Huang",
                    "hidden": false
                },
                {
                    "_id": "683eef14fa20482e30563cf5",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "683eef14fa20482e30563cf6",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "683eef14fa20482e30563cf7",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "683eef14fa20482e30563cf8",
                    "name": "Jiuxin Cao",
                    "hidden": false
                },
                {
                    "_id": "683eef14fa20482e30563cf9",
                    "name": "Hai Zhao",
                    "hidden": false
                },
                {
                    "_id": "683eef14fa20482e30563cfa",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-26T17:58:50.000Z",
            "submittedOnDailyAt": "2025-06-03T11:22:21.600Z",
            "title": "MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search\n  Capability",
            "submittedOnDailyBy": {
                "_id": "65351cbe6141b3927afaed17",
                "avatarUrl": "/avatars/5abf5f2c4ab329e63a7f45c15c9dfb93.svg",
                "isPro": false,
                "fullname": "weiqi wu",
                "user": "vickywu",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Language Models (RALMs) represent a classic paradigm\nwhere models enhance generative capabilities using external knowledge retrieved\nvia a specialized module. Recent advancements in Agent techniques enable Large\nLanguage Models (LLMs) to autonomously utilize tools for retrieval, planning,\nand reasoning. While existing training-based methods show promise, their\nagentic abilities are limited by inherent characteristics of the task-specific\ndata used during training. To further enhance the universal search capability\nof agents, we propose a novel pre-training framework, MaskSearch. In the\npre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)\ntask, where the model learns to leverage search tools to fill masked spans on a\nlarge number of pre-training data, thus acquiring universal retrieval and\nreasoning capabilities for LLMs. After that, the model is trained on downstream\ntasks to achieve further improvement. We apply both Supervised Fine-tuning\n(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine\nagent-based and distillation-based methods to generate training data, starting\nwith a multi-agent system consisting of a planner, rewriter, observer, and\nfollowed by a self-evolving teacher model. While for RL, we employ DAPO as the\ntraining framework and adopt a hybrid reward system consisting of answer\nrewards and format rewards. Additionally, we introduce a curriculum learning\napproach that allows the model to learn progressively from easier to more\nchallenging instances based on the number of masked spans. We evaluate the\neffectiveness of our framework in the scenario of open-domain multi-hop\nquestion answering. Through extensive experiments, we demonstrate that\nMaskSearch significantly enhances the performance of LLM-based search agents on\nboth in-domain and out-of-domain downstream tasks.",
            "upvotes": 2,
            "discussionId": "683eef15fa20482e30563d21",
            "githubRepo": "https://github.com/Alibaba-NLP/MaskSearch",
            "ai_summary": "A novel pre-training framework, MaskSearch, enhances Large Language Models with universal retrieval and reasoning capabilities through a Retrieval Augmented Mask Prediction task, improving their performance in open-domain multi-hop question answering.",
            "ai_keywords": [
                "Retrieval-Augmented Language Models",
                "Agent techniques",
                "Large Language Models",
                "MaskSearch",
                "Retrieval Augmented Mask Prediction",
                "Supervised Fine-tuning",
                "Reinforcement Learning",
                "DAPO",
                "curriculum learning",
                "open-domain multi-hop question answering"
            ]
        },
        "publishedAt": "2025-05-26T13:58:50.000Z",
        "title": "MaskSearch: A Universal Pre-Training Framework to Enhance Agentic Search\n  Capability",
        "summary": "Retrieval-Augmented Language Models (RALMs) represent a classic paradigm\nwhere models enhance generative capabilities using external knowledge retrieved\nvia a specialized module. Recent advancements in Agent techniques enable Large\nLanguage Models (LLMs) to autonomously utilize tools for retrieval, planning,\nand reasoning. While existing training-based methods show promise, their\nagentic abilities are limited by inherent characteristics of the task-specific\ndata used during training. To further enhance the universal search capability\nof agents, we propose a novel pre-training framework, MaskSearch. In the\npre-training stage, we introduce the Retrieval Augmented Mask Prediction (RAMP)\ntask, where the model learns to leverage search tools to fill masked spans on a\nlarge number of pre-training data, thus acquiring universal retrieval and\nreasoning capabilities for LLMs. After that, the model is trained on downstream\ntasks to achieve further improvement. We apply both Supervised Fine-tuning\n(SFT) and Reinforcement Learning (RL) for training. For SFT, we combine\nagent-based and distillation-based methods to generate training data, starting\nwith a multi-agent system consisting of a planner, rewriter, observer, and\nfollowed by a self-evolving teacher model. While for RL, we employ DAPO as the\ntraining framework and adopt a hybrid reward system consisting of answer\nrewards and format rewards. Additionally, we introduce a curriculum learning\napproach that allows the model to learn progressively from easier to more\nchallenging instances based on the number of masked spans. We evaluate the\neffectiveness of our framework in the scenario of open-domain multi-hop\nquestion answering. Through extensive experiments, we demonstrate that\nMaskSearch significantly enhances the performance of LLM-based search agents on\nboth in-domain and out-of-domain downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20285.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65351cbe6141b3927afaed17",
            "avatarUrl": "/avatars/5abf5f2c4ab329e63a7f45c15c9dfb93.svg",
            "fullname": "weiqi wu",
            "name": "vickywu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.16122",
            "authors": [
                {
                    "_id": "683f4eaacd9938eb060f931d",
                    "user": {
                        "_id": "6660de6a78c9d00c8d3bebfd",
                        "avatarUrl": "/avatars/77a77185e9e3d9a92ab5f35dfe76a1fc.svg",
                        "isPro": false,
                        "fullname": "Junhong Lin",
                        "user": "junhongmit",
                        "type": "user"
                    },
                    "name": "Junhong Lin",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-03T19:58:21.759Z",
                    "hidden": false
                },
                {
                    "_id": "683f4eaacd9938eb060f931e",
                    "name": "Xinyue Zeng",
                    "hidden": false
                },
                {
                    "_id": "683f4eaacd9938eb060f931f",
                    "name": "Jie Zhu",
                    "hidden": false
                },
                {
                    "_id": "683f4eaacd9938eb060f9320",
                    "name": "Song Wang",
                    "hidden": false
                },
                {
                    "_id": "683f4eaacd9938eb060f9321",
                    "name": "Julian Shun",
                    "hidden": false
                },
                {
                    "_id": "683f4eaacd9938eb060f9322",
                    "name": "Jun Wu",
                    "hidden": false
                },
                {
                    "_id": "683f4eaacd9938eb060f9323",
                    "name": "Dawei Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T01:56:29.000Z",
            "submittedOnDailyAt": "2025-06-03T18:42:54.251Z",
            "title": "Plan and Budget: Effective and Efficient Test-Time Scaling on Large\n  Language Model Reasoning",
            "submittedOnDailyBy": {
                "_id": "6660de6a78c9d00c8d3bebfd",
                "avatarUrl": "/avatars/77a77185e9e3d9a92ab5f35dfe76a1fc.svg",
                "isPro": false,
                "fullname": "Junhong Lin",
                "user": "junhongmit",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have achieved remarkable success in complex\nreasoning tasks, but their inference remains computationally inefficient. We\nobserve a common failure mode in many prevalent LLMs, overthinking, where\nmodels generate verbose and tangential reasoning traces even for simple\nqueries. Recent works have tried to mitigate this by enforcing fixed token\nbudgets, however, this can lead to underthinking, especially on harder\nproblems. Through empirical analysis, we identify that this inefficiency often\nstems from unclear problem-solving strategies. To formalize this, we develop a\ntheoretical model, BBAM (Bayesian Budget Allocation Model), which models\nreasoning as a sequence of sub-questions with varying uncertainty, and\nintroduce the E^3 metric to capture the trade-off between correctness and\ncomputation efficiency. Building on theoretical results from BBAM, we propose\nPlan-and-Budget, a model-agnostic, test-time framework that decomposes complex\nqueries into sub-questions and allocates token budgets based on estimated\ncomplexity using adaptive scheduling. Plan-and-Budget improves reasoning\nefficiency across a range of tasks and models, achieving up to +70% accuracy\ngains, -39% token reduction, and +187.5% improvement in E^3. Notably, it\nelevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger\nmodel (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close\nperformance gaps without retraining. Our code is available at\nanonymous.4open.science/r/P-and-B-6513/.",
            "upvotes": 2,
            "discussionId": "683f4eabcd9938eb060f9354",
            "ai_summary": "Plan-and-Budget framework enhances reasoning efficiency in LLMs by allocating token budgets based on estimated sub-question complexity, improving accuracy, reducing token usage, and boosting $E^3$ metric.",
            "ai_keywords": [
                "LLMs",
                "inference",
                "overthinking",
                "underthinking",
                "BBAM",
                "Bayesian Budget Allocation Model",
                "$E^3$",
                "Plan-and-Budget",
                "reasoning efficiency",
                "accuracy gains",
                "token reduction"
            ]
        },
        "publishedAt": "2025-05-21T21:56:29.000Z",
        "title": "Plan and Budget: Effective and Efficient Test-Time Scaling on Large\n  Language Model Reasoning",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in complex\nreasoning tasks, but their inference remains computationally inefficient. We\nobserve a common failure mode in many prevalent LLMs, overthinking, where\nmodels generate verbose and tangential reasoning traces even for simple\nqueries. Recent works have tried to mitigate this by enforcing fixed token\nbudgets, however, this can lead to underthinking, especially on harder\nproblems. Through empirical analysis, we identify that this inefficiency often\nstems from unclear problem-solving strategies. To formalize this, we develop a\ntheoretical model, BBAM (Bayesian Budget Allocation Model), which models\nreasoning as a sequence of sub-questions with varying uncertainty, and\nintroduce the E^3 metric to capture the trade-off between correctness and\ncomputation efficiency. Building on theoretical results from BBAM, we propose\nPlan-and-Budget, a model-agnostic, test-time framework that decomposes complex\nqueries into sub-questions and allocates token budgets based on estimated\ncomplexity using adaptive scheduling. Plan-and-Budget improves reasoning\nefficiency across a range of tasks and models, achieving up to +70% accuracy\ngains, -39% token reduction, and +187.5% improvement in E^3. Notably, it\nelevates a smaller model (DS-Qwen-32B) to match the efficiency of a larger\nmodel (DS-LLaMA-70B)-demonstrating Plan-and-Budget's ability to close\nperformance gaps without retraining. Our code is available at\nanonymous.4open.science/r/P-and-B-6513/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16122.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6660de6a78c9d00c8d3bebfd",
            "avatarUrl": "/avatars/77a77185e9e3d9a92ab5f35dfe76a1fc.svg",
            "fullname": "Junhong Lin",
            "name": "junhongmit",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.17127",
            "authors": [
                {
                    "_id": "683f4b081a2d176679fab869",
                    "name": "Michal Golovanevsky",
                    "hidden": false
                },
                {
                    "_id": "683f4b081a2d176679fab86a",
                    "name": "William Rudman",
                    "hidden": false
                },
                {
                    "_id": "683f4b081a2d176679fab86b",
                    "name": "Michael Lepori",
                    "hidden": false
                },
                {
                    "_id": "683f4b081a2d176679fab86c",
                    "name": "Amir Bar",
                    "hidden": false
                },
                {
                    "_id": "683f4b081a2d176679fab86d",
                    "name": "Ritambhara Singh",
                    "hidden": false
                },
                {
                    "_id": "683f4b081a2d176679fab86e",
                    "name": "Carsten Eickhoff",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T22:56:55.000Z",
            "submittedOnDailyAt": "2025-06-03T17:51:44.778Z",
            "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language\n  Models through Visual Counterfacts",
            "submittedOnDailyBy": {
                "_id": "666771f7ad9cd05ea75ec854",
                "avatarUrl": "/avatars/874f04306fbc1dda90b98141882598f4.svg",
                "isPro": false,
                "fullname": "Michal Golovanevsky",
                "user": "mgolov",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) perform well on tasks such as visual\nquestion answering, but it remains unclear whether their reasoning relies more\non memorized world knowledge or on the visual information present in the input\nimage. To investigate this, we introduce Visual CounterFact, a new dataset of\nvisually-realistic counterfactuals that put world knowledge priors (e.g, red\nstrawberry) into direct conflict with visual input (e.g, blue strawberry).\nUsing Visual CounterFact, we show that model predictions initially reflect\nmemorized priors, but shift toward visual evidence in mid-to-late layers. This\ndynamic reveals a competition between the two modalities, with visual input\nultimately overriding priors during evaluation. To control this behavior, we\npropose Pixels Versus Priors (PvP) steering vectors, a mechanism for\ncontrolling model outputs toward either world knowledge or visual input through\nactivation-level interventions. On average, PvP successfully shifts 92.5% of\ncolor and 74.6% of size predictions from priors to counterfactuals. Together,\nthese findings offer new tools for interpreting and controlling factual\nbehavior in multimodal models.",
            "upvotes": 2,
            "discussionId": "683f4b091a2d176679fab8bf",
            "githubRepo": "https://github.com/rsinghlab/pixels_vs_priors",
            "ai_summary": "Visual CounterFact and PvP steering vectors help interpret and control the competition between visual input and memorized world knowledge in multimodal large language models.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Visual CounterFact",
                "visually-realistic counterfactuals",
                "world knowledge priors",
                "visual input",
                "model predictions",
                "mid-to-late layers",
                "Pixels Versus Priors (PvP) steering vectors",
                "activation-level interventions"
            ]
        },
        "publishedAt": "2025-05-21T18:56:55.000Z",
        "title": "Pixels Versus Priors: Controlling Knowledge Priors in Vision-Language\n  Models through Visual Counterfacts",
        "summary": "Multimodal Large Language Models (MLLMs) perform well on tasks such as visual\nquestion answering, but it remains unclear whether their reasoning relies more\non memorized world knowledge or on the visual information present in the input\nimage. To investigate this, we introduce Visual CounterFact, a new dataset of\nvisually-realistic counterfactuals that put world knowledge priors (e.g, red\nstrawberry) into direct conflict with visual input (e.g, blue strawberry).\nUsing Visual CounterFact, we show that model predictions initially reflect\nmemorized priors, but shift toward visual evidence in mid-to-late layers. This\ndynamic reveals a competition between the two modalities, with visual input\nultimately overriding priors during evaluation. To control this behavior, we\npropose Pixels Versus Priors (PvP) steering vectors, a mechanism for\ncontrolling model outputs toward either world knowledge or visual input through\nactivation-level interventions. On average, PvP successfully shifts 92.5% of\ncolor and 74.6% of size predictions from priors to counterfactuals. Together,\nthese findings offer new tools for interpreting and controlling factual\nbehavior in multimodal models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.17127.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "666771f7ad9cd05ea75ec854",
            "avatarUrl": "/avatars/874f04306fbc1dda90b98141882598f4.svg",
            "fullname": "Michal Golovanevsky",
            "name": "mgolov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.01062",
            "authors": [
                {
                    "_id": "683f3b76b5399dccb436f5e9",
                    "name": "Thinh Pham",
                    "hidden": false
                },
                {
                    "_id": "683f3b76b5399dccb436f5ea",
                    "name": "Nguyen Nguyen",
                    "hidden": false
                },
                {
                    "_id": "683f3b76b5399dccb436f5eb",
                    "name": "Pratibha Zunjare",
                    "hidden": false
                },
                {
                    "_id": "683f3b76b5399dccb436f5ec",
                    "name": "Weiyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "683f3b76b5399dccb436f5ed",
                    "name": "Yu-Min Tseng",
                    "hidden": false
                },
                {
                    "_id": "683f3b76b5399dccb436f5ee",
                    "name": "Tu Vu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-01T16:04:34.000Z",
            "submittedOnDailyAt": "2025-06-03T16:51:24.288Z",
            "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "636828a644e19ccad213063a",
                "avatarUrl": "/avatars/e1551d4ee9ca94018103af3bf1249cf2.svg",
                "isPro": false,
                "fullname": "Tu Vu",
                "user": "tuvu",
                "type": "user"
            },
            "summary": "We introduce SealQA, a new challenge benchmark for evaluating\nSEarch-Augmented Language models on fact-seeking questions where web search\nyields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:\n(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and\nreasoning capabilities, with Seal-0 focusing on the most challenging questions\nwhere chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)\nLongSeal, which extends SealQA to test long-context, multi-document reasoning\nin \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations\nin current models: Even frontier LLMs perform poorly across all SealQA flavors.\nOn Seal-0, frontier agentic models equipped with tools like o3 and o4-mini\nachieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning\nefforts. We find that advanced reasoning models such as DeepSeek-R1-671B and\no3-mini are highly vulnerable to noisy search results. Notably, increasing\ntest-time compute does not yield reliable gains across o3-mini, o4-mini, and\no3, with performance often plateauing or even declining early. Additionally,\nwhile recent models are less affected by the \"lost-in-the-middle\" issue, they\nstill fail to reliably identify relevant documents in LongSeal when faced with\nnumerous distractors. To facilitate future work, we release SealQA at\nhuggingface.co/datasets/vtllms/sealqa.",
            "upvotes": 1,
            "discussionId": "683f3b77b5399dccb436f618",
            "ai_summary": "SealQA evaluates search-augmented language models' performance on fact-seeking questions with conflicting or noisy search results, revealing limitations in reasoning and factual accuracy.",
            "ai_keywords": [
                "SEarch-Augmented Language models",
                "fact-seeking questions",
                "SealQA",
                "Seal-0",
                "Seal-Hard",
                "LongSeal",
                "factual accuracy",
                "reasoning capabilities",
                "chat models",
                "GPT-4.1",
                "tools",
                "o3",
                "o4-mini",
                "DeepSeek-R1-671B",
                "test-time compute",
                "relevant documents",
                "huggingface.co/datasets/vtllms/sealqa"
            ]
        },
        "publishedAt": "2025-06-01T12:04:34.000Z",
        "title": "SealQA: Raising the Bar for Reasoning in Search-Augmented Language\n  Models",
        "summary": "We introduce SealQA, a new challenge benchmark for evaluating\nSEarch-Augmented Language models on fact-seeking questions where web search\nyields conflicting, noisy, or unhelpful results. SealQA comes in three flavors:\n(1) Seal-0 (main) and (2) Seal-Hard, which assess factual accuracy and\nreasoning capabilities, with Seal-0 focusing on the most challenging questions\nwhere chat models (e.g., GPT-4.1) typically achieve near-zero accuracy; and (3)\nLongSeal, which extends SealQA to test long-context, multi-document reasoning\nin \"needle-in-a-haystack\" settings. Our evaluation reveals critical limitations\nin current models: Even frontier LLMs perform poorly across all SealQA flavors.\nOn Seal-0, frontier agentic models equipped with tools like o3 and o4-mini\nachieve only 17.1% and 6.3% accuracy, respectively, at their best reasoning\nefforts. We find that advanced reasoning models such as DeepSeek-R1-671B and\no3-mini are highly vulnerable to noisy search results. Notably, increasing\ntest-time compute does not yield reliable gains across o3-mini, o4-mini, and\no3, with performance often plateauing or even declining early. Additionally,\nwhile recent models are less affected by the \"lost-in-the-middle\" issue, they\nstill fail to reliably identify relevant documents in LongSeal when faced with\nnumerous distractors. To facilitate future work, we release SealQA at\nhuggingface.co/datasets/vtllms/sealqa.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01062.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636828a644e19ccad213063a",
            "avatarUrl": "/avatars/e1551d4ee9ca94018103af3bf1249cf2.svg",
            "fullname": "Tu Vu",
            "name": "tuvu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.00469",
            "authors": [
                {
                    "_id": "683e9e0a1c5320ac91b85a19",
                    "user": {
                        "_id": "617a92e16f37340367d5d791",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
                        "isPro": false,
                        "fullname": "Shaoxiong",
                        "user": "jisx",
                        "type": "user"
                    },
                    "name": "Shaoxiong Ji",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:37:04.382Z",
                    "hidden": true
                },
                {
                    "_id": "683e9e0a1c5320ac91b85a1a",
                    "name": "Zihao Li",
                    "hidden": false
                },
                {
                    "_id": "683e9e0a1c5320ac91b85a1b",
                    "user": {
                        "_id": "6611225516a9b10274a2ea98",
                        "avatarUrl": "/avatars/60b90a165673ba3fa0736d3c48108fb3.svg",
                        "isPro": false,
                        "fullname": "Jaakko Paavola",
                        "user": "JaakkoP",
                        "type": "user"
                    },
                    "name": "Jaakko Paavola",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:43:07.193Z",
                    "hidden": false
                },
                {
                    "_id": "683e9e0a1c5320ac91b85a1c",
                    "user": {
                        "_id": "62054c0b522e40b4a18d8744",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62054c0b522e40b4a18d8744/e6bJyJCwEuOZI8BiKC0rx.jpeg",
                        "isPro": true,
                        "fullname": "Indraneil Paul",
                        "user": "iNeil77",
                        "type": "user"
                    },
                    "name": "Indraneil Paul",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:42:57.237Z",
                    "hidden": false
                },
                {
                    "_id": "683e9e0a1c5320ac91b85a1d",
                    "user": {
                        "_id": "64de06e5a8b70ab6b35a7c89",
                        "avatarUrl": "/avatars/45f287a9c07d3fe04c0782a6a99e9743.svg",
                        "isPro": false,
                        "fullname": "Hengyu Luo",
                        "user": "zuenmin",
                        "type": "user"
                    },
                    "name": "Hengyu Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:42:49.625Z",
                    "hidden": false
                },
                {
                    "_id": "683e9e0a1c5320ac91b85a1e",
                    "user": {
                        "_id": "5eac04968a595438195ab4a6",
                        "avatarUrl": "/avatars/0223584657947ca7393ba46d211d6970.svg",
                        "isPro": false,
                        "fullname": "Jörg Tiedemann",
                        "user": "tiedeman",
                        "type": "user"
                    },
                    "name": "Jörg Tiedemann",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-03T12:42:25.922Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T08:37:17.000Z",
            "submittedOnDailyAt": "2025-06-03T05:43:58.004Z",
            "title": "Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data",
            "submittedOnDailyBy": {
                "_id": "617a92e16f37340367d5d791",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
                "isPro": false,
                "fullname": "Shaoxiong",
                "user": "jisx",
                "type": "user"
            },
            "summary": "This paper investigates a critical design decision in the practice of\nmassively multilingual continual pre-training -- the inclusion of parallel\ndata. Specifically, we study the impact of bilingual translation data for\nmassively multilingual language adaptation of the Llama3 family of models to\n500 languages. To this end, we construct the MaLA bilingual translation corpus,\ncontaining data from more than 2,500 language pairs. Subsequently, we develop\nthe EMMA-500 Llama 3 suite of four massively multilingual models -- continually\npre-trained from the Llama 3 family of base models extensively on diverse data\nmixes up to 671B tokens -- and explore the effect of continual pre-training\nwith or without bilingual translation data. Comprehensive evaluation across 7\ntasks and 12 benchmarks demonstrates that bilingual data tends to enhance\nlanguage transfer and performance, particularly for low-resource languages. We\nopen-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model\ngenerations.",
            "upvotes": 1,
            "discussionId": "683e9e0a1c5320ac91b85a50",
            "projectPage": "https://mala-lm.github.io/emma-500-gen2.html",
            "githubRepo": "https://github.com/MaLA-LM/emma-500/",
            "ai_summary": "Bilingual translation data enhances language transfer and performance in massively multilingual language adaptation of the Llama3 family of models.",
            "ai_keywords": [
                "massively multilingual continual pre-training",
                "bilingual translation data",
                "Llama3",
                "MaLA bilingual translation corpus",
                "EMMA-500 Llama 3 suite",
                "continual pre-training",
                "language transfer",
                "low-resource languages"
            ]
        },
        "publishedAt": "2025-05-31T04:37:17.000Z",
        "title": "Massively Multilingual Adaptation of Large Language Models Using\n  Bilingual Translation Data",
        "summary": "This paper investigates a critical design decision in the practice of\nmassively multilingual continual pre-training -- the inclusion of parallel\ndata. Specifically, we study the impact of bilingual translation data for\nmassively multilingual language adaptation of the Llama3 family of models to\n500 languages. To this end, we construct the MaLA bilingual translation corpus,\ncontaining data from more than 2,500 language pairs. Subsequently, we develop\nthe EMMA-500 Llama 3 suite of four massively multilingual models -- continually\npre-trained from the Llama 3 family of base models extensively on diverse data\nmixes up to 671B tokens -- and explore the effect of continual pre-training\nwith or without bilingual translation data. Comprehensive evaluation across 7\ntasks and 12 benchmarks demonstrates that bilingual data tends to enhance\nlanguage transfer and performance, particularly for low-resource languages. We\nopen-source the MaLA corpus, EMMA-500 Llama 3 suite artefacts, code, and model\ngenerations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00469.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "617a92e16f37340367d5d791",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617a92e16f37340367d5d791/omgyzmaF90KBLa3YgFxhS.png",
            "fullname": "Shaoxiong",
            "name": "jisx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24216",
            "authors": [
                {
                    "_id": "683e626d1417d1073375ed9d",
                    "user": {
                        "_id": "68090a58090c3a12a2806615",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ap4nNCSMXic4-wC09SkpO.png",
                        "isPro": false,
                        "fullname": "PRASANNA REDDY PULAKURTHI",
                        "user": "prasannareddyp",
                        "type": "user"
                    },
                    "name": "Prasanna Reddy Pulakurthi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:46:21.114Z",
                    "hidden": false
                },
                {
                    "_id": "683e626d1417d1073375ed9e",
                    "name": "Majid Rabbani",
                    "hidden": false
                },
                {
                    "_id": "683e626d1417d1073375ed9f",
                    "name": "Jamison Heard",
                    "hidden": false
                },
                {
                    "_id": "683e626d1417d1073375eda0",
                    "name": "Sohail Dianat",
                    "hidden": false
                },
                {
                    "_id": "683e626d1417d1073375eda1",
                    "name": "Celso M. de Melo",
                    "hidden": false
                },
                {
                    "_id": "683e626d1417d1073375eda2",
                    "name": "Raghuveer Rao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/68090a58090c3a12a2806615/MGHtr_vzI22almU2lQLi7.png",
                "https://cdn-uploads.huggingface.co/production/uploads/68090a58090c3a12a2806615/iZOQlJcH4vnqQF0L4loK5.png",
                "https://cdn-uploads.huggingface.co/production/uploads/68090a58090c3a12a2806615/GX3UrHO7x5x2UPIGthXcW.png"
            ],
            "publishedAt": "2025-05-30T05:02:42.000Z",
            "submittedOnDailyAt": "2025-06-03T14:56:39.186Z",
            "title": "Shuffle PatchMix Augmentation with Confidence-Margin Weighted\n  Pseudo-Labels for Enhanced Source-Free Domain Adaptation",
            "submittedOnDailyBy": {
                "_id": "68090a58090c3a12a2806615",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ap4nNCSMXic4-wC09SkpO.png",
                "isPro": false,
                "fullname": "PRASANNA REDDY PULAKURTHI",
                "user": "prasannareddyp",
                "type": "user"
            },
            "summary": "This work investigates Source-Free Domain Adaptation (SFDA), where a model\nadapts to a target domain without access to source data. A new augmentation\ntechnique, Shuffle PatchMix (SPM), and a novel reweighting strategy are\nintroduced to enhance performance. SPM shuffles and blends image patches to\ngenerate diverse and challenging augmentations, while the reweighting strategy\nprioritizes reliable pseudo-labels to mitigate label noise. These techniques\nare particularly effective on smaller datasets like PACS, where overfitting and\npseudo-label noise pose greater risks. State-of-the-art results are achieved on\nthree major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS,\nimprovements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target\nand multi-target settings, respectively, while gains of 2.8% and 0.7% are\nattained on DomainNet-126 and VisDA-C. This combination of advanced\naugmentation and robust pseudo-label reweighting establishes a new benchmark\nfor SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM",
            "upvotes": 1,
            "discussionId": "683e626e1417d1073375edca",
            "githubRepo": "https://github.com/PrasannaPulakurthi/SPM",
            "ai_summary": "A new augmentation technique, Shuffle PatchMix, and a reweighting strategy improve performance in source-free domain adaptation, achieving state-of-the-art results on PACS, VisDA-C, and DomainNet-126 benchmarks.",
            "ai_keywords": [
                "Source-Free Domain Adaptation",
                "Shuffle PatchMix",
                "reweighting strategy",
                "pseudo-labels",
                "PACS",
                "VisDA-C",
                "DomainNet-126"
            ]
        },
        "publishedAt": "2025-05-30T01:02:42.000Z",
        "title": "Shuffle PatchMix Augmentation with Confidence-Margin Weighted\n  Pseudo-Labels for Enhanced Source-Free Domain Adaptation",
        "summary": "This work investigates Source-Free Domain Adaptation (SFDA), where a model\nadapts to a target domain without access to source data. A new augmentation\ntechnique, Shuffle PatchMix (SPM), and a novel reweighting strategy are\nintroduced to enhance performance. SPM shuffles and blends image patches to\ngenerate diverse and challenging augmentations, while the reweighting strategy\nprioritizes reliable pseudo-labels to mitigate label noise. These techniques\nare particularly effective on smaller datasets like PACS, where overfitting and\npseudo-label noise pose greater risks. State-of-the-art results are achieved on\nthree major benchmarks: PACS, VisDA-C, and DomainNet-126. Notably, on PACS,\nimprovements of 7.3% (79.4% to 86.7%) and 7.2% are observed in single-target\nand multi-target settings, respectively, while gains of 2.8% and 0.7% are\nattained on DomainNet-126 and VisDA-C. This combination of advanced\naugmentation and robust pseudo-label reweighting establishes a new benchmark\nfor SFDA. The code is available at: https://github.com/PrasannaPulakurthi/SPM",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/68090a58090c3a12a2806615/MGHtr_vzI22almU2lQLi7.png",
            "https://cdn-uploads.huggingface.co/production/uploads/68090a58090c3a12a2806615/iZOQlJcH4vnqQF0L4loK5.png",
            "https://cdn-uploads.huggingface.co/production/uploads/68090a58090c3a12a2806615/GX3UrHO7x5x2UPIGthXcW.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24216.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68090a58090c3a12a2806615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ap4nNCSMXic4-wC09SkpO.png",
            "fullname": "PRASANNA REDDY PULAKURTHI",
            "name": "prasannareddyp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.22865",
            "authors": [
                {
                    "_id": "683f167c791f5015dfcbee13",
                    "name": "Susan Liang",
                    "hidden": false
                },
                {
                    "_id": "683f167c791f5015dfcbee14",
                    "name": "Dejan Markovic",
                    "hidden": false
                },
                {
                    "_id": "683f167c791f5015dfcbee15",
                    "name": "Israel D. Gebru",
                    "hidden": false
                },
                {
                    "_id": "683f167c791f5015dfcbee16",
                    "name": "Steven Krenn",
                    "hidden": false
                },
                {
                    "_id": "683f167c791f5015dfcbee17",
                    "name": "Todd Keebler",
                    "hidden": false
                },
                {
                    "_id": "683f167c791f5015dfcbee18",
                    "name": "Jacob Sandakly",
                    "hidden": false
                },
                {
                    "_id": "683f167c791f5015dfcbee19",
                    "name": "Frank Yu",
                    "hidden": false
                },
                {
                    "_id": "683f167c791f5015dfcbee1a",
                    "name": "Samuel Hassel",
                    "hidden": false
                },
                {
                    "_id": "683f167c791f5015dfcbee1b",
                    "name": "Chenliang Xu",
                    "hidden": false
                },
                {
                    "_id": "683f167c791f5015dfcbee1c",
                    "name": "Alexander Richard",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-28T20:59:15.000Z",
            "submittedOnDailyAt": "2025-06-03T14:07:12.609Z",
            "title": "BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural\n  Speech Synthesis with Flow Matching Models",
            "submittedOnDailyBy": {
                "_id": "65763434a4ee9a4fe7cfb156",
                "avatarUrl": "/avatars/2c4a23ff309f750dd9c0d67bd9fd7abc.svg",
                "isPro": false,
                "fullname": "Susan Liang",
                "user": "susanliang",
                "type": "user"
            },
            "summary": "Binaural rendering aims to synthesize binaural audio that mimics natural\nhearing based on a mono audio and the locations of the speaker and listener.\nAlthough many methods have been proposed to solve this problem, they struggle\nwith rendering quality and streamable inference. Synthesizing high-quality\nbinaural audio that is indistinguishable from real-world recordings requires\nprecise modeling of binaural cues, room reverb, and ambient sounds.\nAdditionally, real-world applications demand streaming inference. To address\nthese challenges, we propose a flow matching based streaming binaural speech\nsynthesis framework called BinauralFlow. We consider binaural rendering to be a\ngeneration problem rather than a regression problem and design a conditional\nflow matching model to render high-quality audio. Moreover, we design a causal\nU-Net architecture that estimates the current audio frame solely based on past\ninformation to tailor generative models for streaming inference. Finally, we\nintroduce a continuous inference pipeline incorporating streaming STFT/ISTFT\noperations, a buffer bank, a midpoint solver, and an early skip schedule to\nimprove rendering continuity and speed. Quantitative and qualitative\nevaluations demonstrate the superiority of our method over SOTA approaches. A\nperceptual study further reveals that our model is nearly indistinguishable\nfrom real-world recordings, with a 42% confusion rate.",
            "upvotes": 1,
            "discussionId": "683f167f791f5015dfcbeedd",
            "ai_summary": "A flow matching based streaming binaural speech synthesis framework called BinauralFlow generates high-quality, indistinguishable binaural audio using a causal U-Net architecture and continuous inference pipeline.",
            "ai_keywords": [
                "flow matching",
                "streaming binaural speech synthesis",
                "BinauralFlow",
                "conditional flow matching model",
                "causal U-Net",
                "continuous inference pipeline",
                "streaming STFT/ISTFT",
                "buffer bank",
                "midpoint solver",
                "early skip schedule"
            ]
        },
        "publishedAt": "2025-05-28T16:59:15.000Z",
        "title": "BinauralFlow: A Causal and Streamable Approach for High-Quality Binaural\n  Speech Synthesis with Flow Matching Models",
        "summary": "Binaural rendering aims to synthesize binaural audio that mimics natural\nhearing based on a mono audio and the locations of the speaker and listener.\nAlthough many methods have been proposed to solve this problem, they struggle\nwith rendering quality and streamable inference. Synthesizing high-quality\nbinaural audio that is indistinguishable from real-world recordings requires\nprecise modeling of binaural cues, room reverb, and ambient sounds.\nAdditionally, real-world applications demand streaming inference. To address\nthese challenges, we propose a flow matching based streaming binaural speech\nsynthesis framework called BinauralFlow. We consider binaural rendering to be a\ngeneration problem rather than a regression problem and design a conditional\nflow matching model to render high-quality audio. Moreover, we design a causal\nU-Net architecture that estimates the current audio frame solely based on past\ninformation to tailor generative models for streaming inference. Finally, we\nintroduce a continuous inference pipeline incorporating streaming STFT/ISTFT\noperations, a buffer bank, a midpoint solver, and an early skip schedule to\nimprove rendering continuity and speed. Quantitative and qualitative\nevaluations demonstrate the superiority of our method over SOTA approaches. A\nperceptual study further reveals that our model is nearly indistinguishable\nfrom real-world recordings, with a 42% confusion rate.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22865.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65763434a4ee9a4fe7cfb156",
            "avatarUrl": "/avatars/2c4a23ff309f750dd9c0d67bd9fd7abc.svg",
            "fullname": "Susan Liang",
            "name": "susanliang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.18128",
            "authors": [
                {
                    "_id": "683f107dfde42c09123a1ea5",
                    "name": "Chau Minh Pham",
                    "hidden": false
                },
                {
                    "_id": "683f107dfde42c09123a1ea6",
                    "name": "Jenna Russell",
                    "hidden": false
                },
                {
                    "_id": "683f107dfde42c09123a1ea7",
                    "name": "Dzung Pham",
                    "hidden": false
                },
                {
                    "_id": "683f107dfde42c09123a1ea8",
                    "name": "Mohit Iyyer",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65b976fdf69f4d0377aef3fe/5N99ISmvaTUTxxYJx7mQP.png"
            ],
            "publishedAt": "2025-05-23T17:38:47.000Z",
            "submittedOnDailyAt": "2025-06-03T13:41:24.170Z",
            "title": "Frankentext: Stitching random text fragments into long-form narratives",
            "submittedOnDailyBy": {
                "_id": "65b976fdf69f4d0377aef3fe",
                "avatarUrl": "/avatars/1201194e2956c56b50098cc465a04c11.svg",
                "isPro": false,
                "fullname": "Chau Minh Pham",
                "user": "chtmp223",
                "type": "user"
            },
            "summary": "We introduce Frankentexts, a new type of long-form narratives produced by\nLLMs under the extreme constraint that most tokens (e.g., 90%) must be copied\nverbatim from human writings. This task presents a challenging test of\ncontrollable generation, requiring models to satisfy a writing prompt,\nintegrate disparate text fragments, and still produce a coherent narrative. To\ngenerate Frankentexts, we instruct the model to produce a draft by selecting\nand combining human-written passages, then iteratively revise the draft while\nmaintaining a user-specified copy ratio. We evaluate the resulting Frankentexts\nalong three axes: writing quality, instruction adherence, and detectability.\nGemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts\nare coherent and 100% relevant to the prompt. Notably, up to 59% of these\noutputs are misclassified as human-written by detectors like Pangram, revealing\nlimitations in AI text detectors. Human annotators can sometimes identify\nFrankentexts through their abrupt tone shifts and inconsistent grammar between\nsegments, especially in longer generations. Beyond presenting a challenging\ngeneration task, Frankentexts invite discussion on building effective detectors\nfor this new grey zone of authorship, provide training data for mixed\nauthorship detection, and serve as a sandbox for studying human-AI co-writing\nprocesses.",
            "upvotes": 1,
            "discussionId": "683f107ffde42c09123a1f03",
            "ai_summary": "Frankentexts challenge LLMs to create coherent long-form narratives using mostly copied human-written text, highlighting limitations in AI text detectors and offering insights into human-AI collaboration.",
            "ai_keywords": [
                "LLMs",
                "Frankentexts",
                "controllable generation",
                "writing quality",
                "instruction adherence",
                "detectability",
                "Gemini-2.5-Pro",
                "Pangram",
                "mixed authorship detection",
                "human-AI co-writing processes"
            ]
        },
        "publishedAt": "2025-05-23T13:38:47.000Z",
        "title": "Frankentext: Stitching random text fragments into long-form narratives",
        "summary": "We introduce Frankentexts, a new type of long-form narratives produced by\nLLMs under the extreme constraint that most tokens (e.g., 90%) must be copied\nverbatim from human writings. This task presents a challenging test of\ncontrollable generation, requiring models to satisfy a writing prompt,\nintegrate disparate text fragments, and still produce a coherent narrative. To\ngenerate Frankentexts, we instruct the model to produce a draft by selecting\nand combining human-written passages, then iteratively revise the draft while\nmaintaining a user-specified copy ratio. We evaluate the resulting Frankentexts\nalong three axes: writing quality, instruction adherence, and detectability.\nGemini-2.5-Pro performs surprisingly well on this task: 81% of its Frankentexts\nare coherent and 100% relevant to the prompt. Notably, up to 59% of these\noutputs are misclassified as human-written by detectors like Pangram, revealing\nlimitations in AI text detectors. Human annotators can sometimes identify\nFrankentexts through their abrupt tone shifts and inconsistent grammar between\nsegments, especially in longer generations. Beyond presenting a challenging\ngeneration task, Frankentexts invite discussion on building effective detectors\nfor this new grey zone of authorship, provide training data for mixed\nauthorship detection, and serve as a sandbox for studying human-AI co-writing\nprocesses.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65b976fdf69f4d0377aef3fe/5N99ISmvaTUTxxYJx7mQP.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18128.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65b976fdf69f4d0377aef3fe",
            "avatarUrl": "/avatars/1201194e2956c56b50098cc465a04c11.svg",
            "fullname": "Chau Minh Pham",
            "name": "chtmp223",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.15772",
            "authors": [
                {
                    "_id": "683ec87a4246cd3c413046e1",
                    "name": "Yifan Cheng",
                    "hidden": false
                },
                {
                    "_id": "683ec87a4246cd3c413046e2",
                    "name": "Ruoyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "683ec87a4246cd3c413046e3",
                    "name": "Jiatong Shi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-21T17:23:12.000Z",
            "submittedOnDailyAt": "2025-06-03T08:35:09.353Z",
            "title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling",
            "submittedOnDailyBy": {
                "_id": "6607d9c2d81d6112498810b9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6607d9c2d81d6112498810b9/mmwx-SFEP-6gjnAdjbcxb.png",
                "isPro": false,
                "fullname": "PoTaTo",
                "user": "PoTaTo721",
                "type": "user"
            },
            "summary": "Acquiring large-scale emotional speech data with strong consistency remains a\nchallenge for speech synthesis. This paper presents MIKU-PAL, a fully automated\nmultimodal pipeline for extracting high-consistency emotional speech from\nunlabeled video data. Leveraging face detection and tracking algorithms, we\ndeveloped an automatic emotion analysis system using a multimodal large\nlanguage model (MLLM). Our results demonstrate that MIKU-PAL can achieve\nhuman-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss\nkappa score) while being much cheaper and faster than human annotation. With\nthe high-quality, flexible, and consistent annotation from MIKU-PAL, we can\nannotate fine-grained speech emotion categories of up to 26 types, validated by\nhuman annotators with 83% rationality ratings. Based on our proposed system, we\nfurther released a fine-grained emotional speech dataset MIKU-EmoBench(131.2\nhours) as a new benchmark for emotional text-to-speech and visual voice\ncloning.",
            "upvotes": 1,
            "discussionId": "683ec87a4246cd3c41304708",
            "ai_summary": "MIKU-PAL, an automated pipeline using multimodal large language models, extracts high-consistency emotional speech from video data, achieving human-level accuracy and consistency at lower cost, and releases a benchmark emotional speech dataset.",
            "ai_keywords": [
                "face detection",
                "face tracking",
                "multimodal large language model",
                "emotion analysis",
                "human-level accuracy",
                "Fleiss kappa score",
                "fine-grained speech emotion categories",
                "emotional text-to-speech",
                "visual voice cloning"
            ]
        },
        "publishedAt": "2025-05-21T13:23:12.000Z",
        "title": "MIKU-PAL: An Automated and Standardized Multi-Modal Method for Speech\n  Paralinguistic and Affect Labeling",
        "summary": "Acquiring large-scale emotional speech data with strong consistency remains a\nchallenge for speech synthesis. This paper presents MIKU-PAL, a fully automated\nmultimodal pipeline for extracting high-consistency emotional speech from\nunlabeled video data. Leveraging face detection and tracking algorithms, we\ndeveloped an automatic emotion analysis system using a multimodal large\nlanguage model (MLLM). Our results demonstrate that MIKU-PAL can achieve\nhuman-level accuracy (68.5% on MELD) and superior consistency (0.93 Fleiss\nkappa score) while being much cheaper and faster than human annotation. With\nthe high-quality, flexible, and consistent annotation from MIKU-PAL, we can\nannotate fine-grained speech emotion categories of up to 26 types, validated by\nhuman annotators with 83% rationality ratings. Based on our proposed system, we\nfurther released a fine-grained emotional speech dataset MIKU-EmoBench(131.2\nhours) as a new benchmark for emotional text-to-speech and visual voice\ncloning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.15772.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6607d9c2d81d6112498810b9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6607d9c2d81d6112498810b9/mmwx-SFEP-6gjnAdjbcxb.png",
            "fullname": "PoTaTo",
            "name": "PoTaTo721",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.01666",
            "authors": [
                {
                    "_id": "683eb172e34a45d302c2439d",
                    "user": {
                        "_id": "638a67f6fc10b2be222531cd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670014915088-noauth.png",
                        "isPro": false,
                        "fullname": "Florian Fürrutter",
                        "user": "Floki00",
                        "type": "user"
                    },
                    "name": "Florian Fürrutter",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T09:16:37.160Z",
                    "hidden": false
                },
                {
                    "_id": "683eb172e34a45d302c2439e",
                    "name": "Zohim Chandani",
                    "hidden": false
                },
                {
                    "_id": "683eb172e34a45d302c2439f",
                    "name": "Ikko Hamamura",
                    "hidden": false
                },
                {
                    "_id": "683eb172e34a45d302c243a0",
                    "name": "Hans J. Briegel",
                    "hidden": false
                },
                {
                    "_id": "683eb172e34a45d302c243a1",
                    "name": "Gorka Muñoz-Gil",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/638a67f6fc10b2be222531cd/nUURUfMeGHQwS9ZbvW7Vp.gif"
            ],
            "publishedAt": "2025-06-02T13:35:33.000Z",
            "submittedOnDailyAt": "2025-06-03T09:27:24.947Z",
            "title": "Synthesis of discrete-continuous quantum circuits with multimodal\n  diffusion models",
            "submittedOnDailyBy": {
                "_id": "638a67f6fc10b2be222531cd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670014915088-noauth.png",
                "isPro": false,
                "fullname": "Florian Fürrutter",
                "user": "Floki00",
                "type": "user"
            },
            "summary": "Efficiently compiling quantum operations remains a major bottleneck in\nscaling quantum computing. Today's state-of-the-art methods achieve low\ncompilation error by combining search algorithms with gradient-based parameter\noptimization, but they incur long runtimes and require multiple calls to\nquantum hardware or expensive classical simulations, making their scaling\nprohibitive. Recently, machine-learning models have emerged as an alternative,\nthough they are currently restricted to discrete gate sets. Here, we introduce\na multimodal denoising diffusion model that simultaneously generates a\ncircuit's structure and its continuous parameters for compiling a target\nunitary. It leverages two independent diffusion processes, one for discrete\ngate selection and one for parameter prediction. We benchmark the model over\ndifferent experiments, analyzing the method's accuracy across varying qubit\ncounts, circuit depths, and proportions of parameterized gates. Finally, by\nexploiting its rapid circuit generation, we create large datasets of circuits\nfor particular operations and use these to extract valuable heuristics that can\nhelp us discover new insights into quantum circuit synthesis.",
            "upvotes": 0,
            "discussionId": "683eb172e34a45d302c243d1",
            "projectPage": "https://florianfuerrutter.github.io/genQC/",
            "githubRepo": "https://github.com/FlorianFuerrutter/genQC",
            "ai_summary": "A multimodal denoising diffusion model is introduced for generating both the structure and continuous parameters of quantum circuits, offering an efficient alternative to traditional quantum operation compilation methods.",
            "ai_keywords": [
                "denoising diffusion model",
                "circuit generation",
                "quantum operations",
                "quantum circuit synthesis",
                "discrete gate selection",
                "parameter prediction",
                "quantum hardware",
                "classical simulations",
                "quantum circuit structure",
                "continuous parameters",
                "qubit counts",
                "circuit depths",
                "parameterized gates"
            ]
        },
        "publishedAt": "2025-06-02T09:35:33.000Z",
        "title": "Synthesis of discrete-continuous quantum circuits with multimodal\n  diffusion models",
        "summary": "Efficiently compiling quantum operations remains a major bottleneck in\nscaling quantum computing. Today's state-of-the-art methods achieve low\ncompilation error by combining search algorithms with gradient-based parameter\noptimization, but they incur long runtimes and require multiple calls to\nquantum hardware or expensive classical simulations, making their scaling\nprohibitive. Recently, machine-learning models have emerged as an alternative,\nthough they are currently restricted to discrete gate sets. Here, we introduce\na multimodal denoising diffusion model that simultaneously generates a\ncircuit's structure and its continuous parameters for compiling a target\nunitary. It leverages two independent diffusion processes, one for discrete\ngate selection and one for parameter prediction. We benchmark the model over\ndifferent experiments, analyzing the method's accuracy across varying qubit\ncounts, circuit depths, and proportions of parameterized gates. Finally, by\nexploiting its rapid circuit generation, we create large datasets of circuits\nfor particular operations and use these to extract valuable heuristics that can\nhelp us discover new insights into quantum circuit synthesis.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638a67f6fc10b2be222531cd/nUURUfMeGHQwS9ZbvW7Vp.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01666.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "638a67f6fc10b2be222531cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670014915088-noauth.png",
            "fullname": "Florian Fürrutter",
            "name": "Floki00",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]