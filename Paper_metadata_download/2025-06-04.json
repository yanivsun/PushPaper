[
    {
        "paper": {
            "id": "2505.24726",
            "authors": [
                {
                    "_id": "683ffca568402c738a947f4e",
                    "name": "Shelly Bensal",
                    "hidden": false
                },
                {
                    "_id": "683ffca568402c738a947f4f",
                    "name": "Umar Jamil",
                    "hidden": false
                },
                {
                    "_id": "683ffca568402c738a947f50",
                    "name": "Christopher Bryant",
                    "hidden": false
                },
                {
                    "_id": "683ffca568402c738a947f51",
                    "user": {
                        "_id": "60e61b3969bd0df25c9375da",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Melisa Russak",
                        "user": "melisa",
                        "type": "user"
                    },
                    "name": "Melisa Russak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:53:45.047Z",
                    "hidden": false
                },
                {
                    "_id": "683ffca568402c738a947f52",
                    "user": {
                        "_id": "621d6f532165dc431641e438",
                        "avatarUrl": "/avatars/56ccef10a8426d7160ef3586a771bd63.svg",
                        "isPro": false,
                        "fullname": "Kiran Kamble",
                        "user": "kiranr",
                        "type": "user"
                    },
                    "name": "Kiran Kamble",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T15:03:25.497Z",
                    "hidden": false
                },
                {
                    "_id": "683ffca568402c738a947f53",
                    "name": "Dmytro Mozolevskyi",
                    "hidden": false
                },
                {
                    "_id": "683ffca568402c738a947f54",
                    "name": "Muayad Ali",
                    "hidden": false
                },
                {
                    "_id": "683ffca568402c738a947f55",
                    "user": {
                        "_id": "60cd486d723acf5eb46fe8d3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cd486d723acf5eb46fe8d3/Z1bD1kjvZ0QAOjZna41Xr.jpeg",
                        "isPro": false,
                        "fullname": "Waseem AlShikh",
                        "user": "wassemgtk",
                        "type": "user"
                    },
                    "name": "Waseem AlShikh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T15:03:22.388Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T15:49:42.000Z",
            "submittedOnDailyAt": "2025-06-04T06:34:02.563Z",
            "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "60e61b3969bd0df25c9375da",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
                "isPro": false,
                "fullname": "Melisa Russak",
                "user": "melisa",
                "type": "user"
            },
            "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback.",
            "upvotes": 143,
            "discussionId": "683ffca568402c738a947f7b",
            "ai_summary": "A method using self-reflection and reinforcement learning improves the performance of large language models, especially with limited feedback, by rewarding self-reflections that lead to better task performance.",
            "ai_keywords": [
                "self-reflection",
                "reinforcement learning",
                "self-reflective commentary",
                "performance gains",
                "math equation writing",
                "function calling",
                "parameter-efficient fine-tuning"
            ]
        },
        "publishedAt": "2025-05-30T11:49:42.000Z",
        "title": "Reflect, Retry, Reward: Self-Improving LLMs via Reinforcement Learning",
        "summary": "We explore a method for improving the performance of large language models\nthrough self-reflection and reinforcement learning. By incentivizing the model\nto generate better self-reflections when it answers incorrectly, we demonstrate\nthat a model's ability to solve complex, verifiable tasks can be enhanced even\nwhen generating synthetic data is infeasible and only binary feedback is\navailable. Our framework operates in two stages: first, upon failing a given\ntask, the model generates a self-reflective commentary analyzing its previous\nattempt; second, the model is given another attempt at the task with the\nself-reflection in context. If the subsequent attempt succeeds, the tokens\ngenerated during the self-reflection phase are rewarded. Our experimental\nresults show substantial performance gains across a variety of model\narchitectures, as high as 34.7% improvement at math equation writing and 18.1%\nimprovement at function calling. Notably, smaller fine-tuned models (1.5\nbillion to 7 billion parameters) outperform models in the same family that are\n10 times larger. Our novel paradigm is thus an exciting pathway to more useful\nand reliable language models that can self-improve on challenging tasks with\nlimited external feedback.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24726.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "60e61b3969bd0df25c9375da",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1625692968400-noauth.jpeg",
            "fullname": "Melisa Russak",
            "name": "melisa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 32
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02387",
            "authors": [
                {
                    "_id": "683fa95ea0770843560c7ae3",
                    "user": {
                        "_id": "653a5b0f7c01c693a16dd184",
                        "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
                        "isPro": false,
                        "fullname": "Zelai Xu",
                        "user": "zelaix",
                        "type": "user"
                    },
                    "name": "Zelai Xu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-04T02:03:11.372Z",
                    "hidden": false
                },
                {
                    "_id": "683fa95ea0770843560c7ae4",
                    "name": "Zhexuan Xu",
                    "hidden": false
                },
                {
                    "_id": "683fa95ea0770843560c7ae5",
                    "name": "Xiangmin Yi",
                    "hidden": false
                },
                {
                    "_id": "683fa95ea0770843560c7ae6",
                    "user": {
                        "_id": "683fb7bb7c8d720be437948d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/pWekQrjYM6lw9bDjvLHg7.png",
                        "isPro": false,
                        "fullname": "Huining Yuan",
                        "user": "HuiningYuan",
                        "type": "user"
                    },
                    "name": "Huining Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-04T10:26:15.430Z",
                    "hidden": false
                },
                {
                    "_id": "683fa95ea0770843560c7ae7",
                    "name": "Xinlei Chen",
                    "hidden": false
                },
                {
                    "_id": "683fa95ea0770843560c7ae8",
                    "name": "Yi Wu",
                    "hidden": false
                },
                {
                    "_id": "683fa95ea0770843560c7ae9",
                    "name": "Chao Yu",
                    "hidden": false
                },
                {
                    "_id": "683fa95ea0770843560c7aea",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T02:57:38.000Z",
            "submittedOnDailyAt": "2025-06-04T00:58:29.506Z",
            "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
            "submittedOnDailyBy": {
                "_id": "653a5b0f7c01c693a16dd184",
                "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
                "isPro": false,
                "fullname": "Zelai Xu",
                "user": "zelaix",
                "type": "user"
            },
            "summary": "Recent advancements in Vision Language Models (VLMs) have expanded their\ncapabilities to interactive agent tasks, yet existing benchmarks remain limited\nto single-agent or text-only environments. In contrast, real-world scenarios\noften involve multiple agents interacting within rich visual and linguistic\ncontexts, posing challenges with both multimodal observations and strategic\ninteractions. To bridge this gap, we introduce Visual Strategic Bench\n(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning\nand decision-making in multi-agent environments. VS-Bench comprises eight\nvision-grounded environments spanning cooperative, competitive, and\nmixed-motive interactions, designed to assess agents' ability to predict\nothers' future moves and optimize for long-term objectives. We consider two\ncomplementary evaluation dimensions, including offline evaluation of strategic\nreasoning by next-action prediction accuracy and online evaluation of\ndecision-making by normalized episode return. Extensive experiments of fourteen\nleading VLMs reveal a significant gap between current models and optimal\nperformance, with the best models attaining 47.8% prediction accuracy and 24.3%\nnormalized return. We further conduct in-depth analyses on multimodal\nobservations, test-time scaling, social behaviors, and failure cases of VLM\nagents. By standardizing the evaluation and highlighting the limitations of\nexisting models, we envision VS-Bench as a foundation for future research on\nstrategic multimodal agents. Code and data are available at\nhttps://vs-bench.github.io.",
            "upvotes": 51,
            "discussionId": "683fa95fa0770843560c7b3d",
            "projectPage": "https://vs-bench.github.io",
            "githubRepo": "https://github.com/zelaix/VS-Bench",
            "ai_summary": "VS-Bench is a multimodal benchmark designed to evaluate Vision Language Models' strategic reasoning and decision-making in complex multi-agent environments.",
            "ai_keywords": [
                "Vision Language Models",
                "VS-Bench",
                "multimodal benchmark",
                "strategic reasoning",
                "decision-making",
                "multi-agent environments",
                "vision-grounded environments",
                "cooperative",
                "competitive",
                "mixed-motive interactions",
                "next-action prediction",
                "normalized episode return",
                "multimodal observations",
                "test-time scaling",
                "social behaviors",
                "failure cases"
            ]
        },
        "publishedAt": "2025-06-02T22:57:38.000Z",
        "title": "VS-Bench: Evaluating VLMs for Strategic Reasoning and Decision-Making in\n  Multi-Agent Environments",
        "summary": "Recent advancements in Vision Language Models (VLMs) have expanded their\ncapabilities to interactive agent tasks, yet existing benchmarks remain limited\nto single-agent or text-only environments. In contrast, real-world scenarios\noften involve multiple agents interacting within rich visual and linguistic\ncontexts, posing challenges with both multimodal observations and strategic\ninteractions. To bridge this gap, we introduce Visual Strategic Bench\n(VS-Bench), a multimodal benchmark that evaluates VLMs for strategic reasoning\nand decision-making in multi-agent environments. VS-Bench comprises eight\nvision-grounded environments spanning cooperative, competitive, and\nmixed-motive interactions, designed to assess agents' ability to predict\nothers' future moves and optimize for long-term objectives. We consider two\ncomplementary evaluation dimensions, including offline evaluation of strategic\nreasoning by next-action prediction accuracy and online evaluation of\ndecision-making by normalized episode return. Extensive experiments of fourteen\nleading VLMs reveal a significant gap between current models and optimal\nperformance, with the best models attaining 47.8% prediction accuracy and 24.3%\nnormalized return. We further conduct in-depth analyses on multimodal\nobservations, test-time scaling, social behaviors, and failure cases of VLM\nagents. By standardizing the evaluation and highlighting the limitations of\nexisting models, we envision VS-Bench as a foundation for future research on\nstrategic multimodal agents. Code and data are available at\nhttps://vs-bench.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02387.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653a5b0f7c01c693a16dd184",
            "avatarUrl": "/avatars/4b43d88709dc8037250404452e81adcf.svg",
            "fullname": "Zelai Xu",
            "name": "zelaix",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03147",
            "authors": [
                {
                    "_id": "683fae55c6b71c5994ccd4fe",
                    "user": {
                        "_id": "6367a8175bb06007ea099b8f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6367a8175bb06007ea099b8f/IjG7HyWyWRlVt_XwRbxRW.jpeg",
                        "isPro": false,
                        "fullname": "linbin",
                        "user": "LanguageBind",
                        "type": "user"
                    },
                    "name": "Bin Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:56:49.923Z",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd4ff",
                    "user": {
                        "_id": "646df3c04ad7f907279f14c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646df3c04ad7f907279f14c3/WZjSDtAmezmjbczLtCP2B.jpeg",
                        "isPro": false,
                        "fullname": "Zongjian Li",
                        "user": "chestnutlzj",
                        "type": "user"
                    },
                    "name": "Zongjian Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T15:03:37.427Z",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd500",
                    "name": "Xinhua Cheng",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd501",
                    "name": "Yuwei Niu",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd502",
                    "name": "Yang Ye",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd503",
                    "name": "Xianyi He",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd504",
                    "user": {
                        "_id": "63468720dd6d90d82ccf3450",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                        "isPro": false,
                        "fullname": "YSH",
                        "user": "BestWishYsh",
                        "type": "user"
                    },
                    "name": "Shenghai Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:56:52.748Z",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd505",
                    "name": "Wangbo Yu",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd506",
                    "name": "Shaodong Wang",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd507",
                    "name": "Yunyang Ge",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd508",
                    "name": "Yatian Pang",
                    "hidden": false
                },
                {
                    "_id": "683fae55c6b71c5994ccd509",
                    "name": "Li Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:59:33.000Z",
            "submittedOnDailyAt": "2025-06-04T00:55:35.016Z",
            "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.",
            "upvotes": 49,
            "discussionId": "683fae56c6b71c5994ccd548",
            "githubRepo": "https://github.com/PKU-YuanGroup/UniWorld-V1",
            "ai_summary": "A unified generative framework called UniWorld uses semantic features from visual-language models for image perception and manipulation, outperforming BAGEL with reduced data.",
            "ai_keywords": [
                "GPT-4o-Image",
                "semantic encoders",
                "VAE",
                "UniWorld",
                "visual-language models",
                "contrastive semantic encoders",
                "image editing benchmarks",
                "image perception tasks"
            ]
        },
        "publishedAt": "2025-06-03T13:59:33.000Z",
        "title": "UniWorld: High-Resolution Semantic Encoders for Unified Visual\n  Understanding and Generation",
        "summary": "Although existing unified models deliver strong performance on\nvision-language understanding and text-to-image generation, their models are\nlimited in exploring image perception and manipulation tasks, which are\nurgently desired by users for wide applications. Recently, OpenAI released\ntheir powerful GPT-4o-Image model for comprehensive image perception and\nmanipulation, achieving expressive capability and attracting community\ninterests. By observing the performance of GPT-4o-Image in our carefully\nconstructed experiments, we infer that GPT-4o-Image leverages features\nextracted by semantic encoders instead of VAE, while VAEs are considered\nessential components in many image manipulation models. Motivated by such\ninspiring observations, we present a unified generative framework named\nUniWorld based on semantic features provided by powerful visual-language models\nand contrastive semantic encoders. As a result, we build a strong unified model\nusing only 1% amount of BAGEL's data, which consistently outperforms BAGEL on\nimage editing benchmarks. UniWorld also maintains competitive image\nunderstanding and generation capabilities, achieving strong performance across\nmultiple image perception tasks. We fully open-source our models, including\nmodel weights, training and evaluation scripts, and datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03147.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 56
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02096",
            "authors": [
                {
                    "_id": "683fa36f7ed99d0040761114",
                    "user": {
                        "_id": "626d268d5f7327906f05cad1",
                        "avatarUrl": "/avatars/18bda74612a3ee63a17f991bcc695106.svg",
                        "isPro": true,
                        "fullname": "Zijian Wu",
                        "user": "Jakumetsu",
                        "type": "user"
                    },
                    "name": "Zijian Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:57:09.765Z",
                    "hidden": false
                },
                {
                    "_id": "683fa36f7ed99d0040761115",
                    "name": "Jinjie Ni",
                    "hidden": false
                },
                {
                    "_id": "683fa36f7ed99d0040761116",
                    "name": "Xiangyan Liu",
                    "hidden": false
                },
                {
                    "_id": "683fa36f7ed99d0040761117",
                    "name": "Zichen Liu",
                    "hidden": false
                },
                {
                    "_id": "683fa36f7ed99d0040761118",
                    "name": "Hang Yan",
                    "hidden": false
                },
                {
                    "_id": "683fa36f7ed99d0040761119",
                    "name": "Michael Qizhe Shieh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T17:45:16.000Z",
            "submittedOnDailyAt": "2025-06-04T00:44:38.612Z",
            "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
            "submittedOnDailyBy": {
                "_id": "6486b09e8315b19342f0bf5e",
                "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
                "isPro": false,
                "fullname": "Xiangyan Liu",
                "user": "xyliu6",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose SynthRL-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.",
            "upvotes": 45,
            "discussionId": "683fa3707ed99d0040761154",
            "ai_summary": "SynthRL, a scalable pipeline for data synthesis in reinforcement learning with verifiable rewards, enhances visual math reasoning VLMs by generating challenging, verifiable questions.",
            "ai_keywords": [
                "reinforcement learning",
                "verifiable reward",
                "RLVR",
                "SynthRL",
                "seed questions",
                "data augmentation",
                "verification stage",
                "MMK12 dataset",
                "visual math reasoning benchmarks"
            ]
        },
        "publishedAt": "2025-06-02T13:45:16.000Z",
        "title": "SynthRL: Scaling Visual Reasoning with Verifiable Data Synthesis",
        "summary": "Vision-language models (VLMs) trained via reinforcement learning with\nverifiable reward (RLVR) have shown notable progress in scaling test-time\ncompute effectively. In this work, we investigate how synthesized RL data can\nfurther improve RLVR. To this end, we propose SynthRL-a scalable and\nguaranteed pipeline for automatic data scaling in reasoning-oriented RL\ntraining. SynthRL comprises three key stages: (1) selecting seed questions with\nappropriate distribution, (2) augmenting them into more challenging variants\nwhile preserving the original answers, and (3) a guaranteed verification stage\nthat ensures near-perfect correctness and difficulty enhancement. Our empirical\nexperiments demonstrate SynthRL's scalability and effectiveness. When applied\nto the MMK12 dataset, SynthRL synthesizes over 3.3K additional verifiable,\nchallenging questions from approximately 8K seed samples. Models trained with\nour synthesized data achieve consistent gains across five out-of-domain visual\nmath reasoning benchmarks, with a significant improvement over baseline models\ntrained on seed data alone. Notably, detailed analysis reveals that the gains\nare more pronounced on the most challenging evaluation samples, highlighting\nSynthRL's effectiveness in eliciting deeper and more complex reasoning\npatterns.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02096.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6486b09e8315b19342f0bf5e",
            "avatarUrl": "/avatars/bc5f22f231c884146d373fe1042d81bd.svg",
            "fullname": "Xiangyan Liu",
            "name": "xyliu6",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24120",
            "authors": [
                {
                    "_id": "683fc08da33aeee1124887c4",
                    "name": "Ai Jian",
                    "hidden": false
                },
                {
                    "_id": "683fc08da33aeee1124887c5",
                    "user": {
                        "_id": "660aab2c878289c5b34f9e97",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660aab2c878289c5b34f9e97/yxx1-lR8x5o6KaEpZDXQq.jpeg",
                        "isPro": false,
                        "fullname": "weijie qiu",
                        "user": "qiuwj",
                        "type": "user"
                    },
                    "name": "Weijie Qiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T09:56:39.772Z",
                    "hidden": false
                },
                {
                    "_id": "683fc08da33aeee1124887c6",
                    "user": {
                        "_id": "62be9b5aae56e75e4d689e7c",
                        "avatarUrl": "/avatars/6772bc09d6eeb4e86b1210481be91720.svg",
                        "isPro": false,
                        "fullname": "wangxiaokun",
                        "user": "shawn0wang",
                        "type": "user"
                    },
                    "name": "Xiaokun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:55:01.766Z",
                    "hidden": false
                },
                {
                    "_id": "683fc08da33aeee1124887c7",
                    "name": "Peiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "683fc08da33aeee1124887c8",
                    "name": "Yunzhuo Hao",
                    "hidden": false
                },
                {
                    "_id": "683fc08da33aeee1124887c9",
                    "name": "Jiangbo Pei",
                    "hidden": false
                },
                {
                    "_id": "683fc08da33aeee1124887ca",
                    "user": {
                        "_id": "66d3ff488da15c5151c372fb",
                        "avatarUrl": "/avatars/4e3ed8b675c822e768e17def7604f0d9.svg",
                        "isPro": false,
                        "fullname": "Yichen Wei",
                        "user": "rockman24",
                        "type": "user"
                    },
                    "name": "Yichen Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-04T10:25:28.093Z",
                    "hidden": false
                },
                {
                    "_id": "683fc08da33aeee1124887cb",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "683fc08da33aeee1124887cc",
                    "user": {
                        "_id": "6462b241b438438da3c25a5d",
                        "avatarUrl": "/avatars/606a67f1be639c9a5e36f293abd5f27a.svg",
                        "isPro": false,
                        "fullname": "Xuchen Song",
                        "user": "xuchensong",
                        "type": "user"
                    },
                    "name": "Xuchen Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-04T10:25:14.285Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
            ],
            "publishedAt": "2025-05-30T01:34:25.000Z",
            "submittedOnDailyAt": "2025-06-04T05:57:36.826Z",
            "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs",
            "submittedOnDailyBy": {
                "_id": "620f5a1c3f76c50e6458a9b6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
                "isPro": false,
                "fullname": "Peiyu Wang",
                "user": "OrlandoHugBot",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA.",
            "upvotes": 43,
            "discussionId": "683fc091a33aeee1124888a8",
            "githubRepo": "https://github.com/csvqa-benchmark/CSVQA/tree/main",
            "ai_summary": "A new benchmark, CSVQA, evaluates scientific reasoning in vision-language models through domain-specific visual question answering, highlighting the need for improvement in these models.",
            "ai_keywords": [
                "Vision-Language Models",
                "multimodal benchmark",
                "scientific reasoning",
                "domain-grounded",
                "visual question answering",
                "domain-specific knowledge",
                "higher-order reasoning",
                "evaluation protocol",
                "intermediate reasoning steps",
                "curated explanations"
            ]
        },
        "publishedAt": "2025-05-29T21:34:25.000Z",
        "title": "CSVQA: A Chinese Multimodal Benchmark for Evaluating STEM Reasoning\n  Capabilities of VLMs",
        "summary": "Vision-Language Models (VLMs) have demonstrated remarkable progress in\nmultimodal understanding, yet their capabilities for scientific reasoning\nremains inadequately assessed. Current multimodal benchmarks predominantly\nevaluate generic image comprehension or text-driven reasoning, lacking\nauthentic scientific contexts that require domain-specific knowledge\nintegration with visual evidence analysis. To fill this gap, we present CSVQA,\na diagnostic multimodal benchmark specifically designed for evaluating\nscientific reasoning through domain-grounded visual question answering.Our\nbenchmark features 1,378 carefully constructed question-answer pairs spanning\ndiverse STEM disciplines, each demanding domain knowledge, integration of\nvisual evidence, and higher-order reasoning. Compared to prior multimodal\nbenchmarks, CSVQA places greater emphasis on real-world scientific content and\ncomplex reasoning.We additionally propose a rigorous evaluation protocol to\nsystematically assess whether model predictions are substantiated by valid\nintermediate reasoning steps based on curated explanations. Our comprehensive\nevaluation of 15 VLMs on this benchmark reveals notable performance\ndisparities, as even the top-ranked proprietary model attains only 49.6\\%\naccuracy.This empirical evidence underscores the pressing need for advancing\nscientific reasoning capabilities in VLMs. Our CSVQA is released at\nhttps://huggingface.co/datasets/Skywork/CSVQA.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/620f5a1c3f76c50e6458a9b6/l5zN70u8jAQBCjl41TWoi.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24120.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "620f5a1c3f76c50e6458a9b6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
            "fullname": "Peiyu Wang",
            "name": "OrlandoHugBot",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24714",
            "authors": [
                {
                    "_id": "683d04c751706d12b2c262ea",
                    "user": {
                        "_id": "642da1cd99f3110ac27caca5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
                        "isPro": false,
                        "fullname": "junyu",
                        "user": "luojunyu",
                        "type": "user"
                    },
                    "name": "Junyu Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T09:03:26.483Z",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262eb",
                    "user": {
                        "_id": "65a9c8652bf3e0cbbfcab2c8",
                        "avatarUrl": "/avatars/fc690a78b5f2e94e08a40059ae40625c.svg",
                        "isPro": false,
                        "fullname": "Alan KOU",
                        "user": "alan1027",
                        "type": "user"
                    },
                    "name": "Zhizhuo Kou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T09:03:22.133Z",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262ec",
                    "user": {
                        "_id": "6434c115a5aed21dd11981c5",
                        "avatarUrl": "/avatars/d51e6e384cfc3affe578e7816bcebb35.svg",
                        "isPro": false,
                        "fullname": "Yang Liming",
                        "user": "chunfenri",
                        "type": "user"
                    },
                    "name": "Liming Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T09:03:24.346Z",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262ed",
                    "name": "Xiao Luo",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262ee",
                    "name": "Jinsheng Huang",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262ef",
                    "name": "Zhiping Xiao",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262f0",
                    "name": "Jingshu Peng",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262f1",
                    "name": "Chengzhong Liu",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262f2",
                    "name": "Jiaming Ji",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262f3",
                    "name": "Xuanzhe Liu",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262f4",
                    "name": "Sirui Han",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262f5",
                    "name": "Ming Zhang",
                    "hidden": false
                },
                {
                    "_id": "683d04c751706d12b2c262f6",
                    "name": "Yike Guo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
            ],
            "publishedAt": "2025-05-30T15:36:19.000Z",
            "submittedOnDailyAt": "2025-06-04T01:04:36.480Z",
            "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
            "submittedOnDailyBy": {
                "_id": "642da1cd99f3110ac27caca5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
                "isPro": false,
                "fullname": "junyu",
                "user": "luojunyu",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME.",
            "upvotes": 32,
            "discussionId": "683d04c951706d12b2c26367",
            "projectPage": "https://huggingface.co/datasets/luojunyu/FinMME",
            "githubRepo": "https://github.com/luo-junyu/FinMME",
            "ai_summary": "FinMME is a comprehensive multimodal dataset for financial research and FinScore is an evaluation system that highlights the challenges faced by even advanced models like GPT-4o in the finance domain.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "FinMME",
                "financial research samples",
                "high-quality dataset",
                "financial domains",
                "asset classes",
                "chart types",
                "data quality",
                "FinScore",
                "hallucination penalties",
                "multi-dimensional capability assessment",
                "benchmark dataset",
                "prediction robustness"
            ]
        },
        "publishedAt": "2025-05-30T11:36:19.000Z",
        "title": "FinMME: Benchmark Dataset for Financial Multi-Modal Reasoning Evaluation",
        "summary": "Multimodal Large Language Models (MLLMs) have experienced rapid development\nin recent years. However, in the financial domain, there is a notable lack of\neffective and specialized multimodal evaluation datasets. To advance the\ndevelopment of MLLMs in the finance domain, we introduce FinMME, encompassing\nmore than 11,000 high-quality financial research samples across 18 financial\ndomains and 6 asset classes, featuring 10 major chart types and 21 subtypes. We\nensure data quality through 20 annotators and carefully designed validation\nmechanisms. Additionally, we develop FinScore, an evaluation system\nincorporating hallucination penalties and multi-dimensional capability\nassessment to provide an unbiased evaluation. Extensive experimental results\ndemonstrate that even state-of-the-art models like GPT-4o exhibit\nunsatisfactory performance on FinMME, highlighting its challenging nature. The\nbenchmark exhibits high robustness with prediction variations under different\nprompts remaining below 1%, demonstrating superior reliability compared to\nexisting datasets. Our dataset and evaluation protocol are available at\nhttps://huggingface.co/datasets/luojunyu/FinMME and\nhttps://github.com/luo-junyu/FinMME.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642da1cd99f3110ac27caca5/ilkQ2K5LD4SPQ5oCcV2aB.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24714.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "642da1cd99f3110ac27caca5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642da1cd99f3110ac27caca5/C1QJY3R_ZdaeANG1y8iW7.jpeg",
            "fullname": "junyu",
            "name": "luojunyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03135",
            "authors": [
                {
                    "_id": "683fe55868402c738a8e5ee4",
                    "name": "Mengdi Jia",
                    "hidden": false
                },
                {
                    "_id": "683fe55868402c738a8e5ee5",
                    "user": {
                        "_id": "63c3e8abc7d7f4c63a515a02",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
                        "isPro": false,
                        "fullname": "Zekun Qi",
                        "user": "qizekun",
                        "type": "user"
                    },
                    "name": "Zekun Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:22.042Z",
                    "hidden": false
                },
                {
                    "_id": "683fe55868402c738a8e5ee6",
                    "name": "Shaochen Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fe55868402c738a8e5ee7",
                    "name": "Wenyao Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fe55868402c738a8e5ee8",
                    "name": "Xinqiang Yu",
                    "hidden": false
                },
                {
                    "_id": "683fe55868402c738a8e5ee9",
                    "name": "Jiawei He",
                    "hidden": false
                },
                {
                    "_id": "683fe55868402c738a8e5eea",
                    "name": "He Wang",
                    "hidden": false
                },
                {
                    "_id": "683fe55868402c738a8e5eeb",
                    "name": "Li Yi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
            ],
            "publishedAt": "2025-06-03T17:58:29.000Z",
            "submittedOnDailyAt": "2025-06-04T05:47:51.969Z",
            "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models",
            "submittedOnDailyBy": {
                "_id": "63c3e8abc7d7f4c63a515a02",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
                "isPro": false,
                "fullname": "Zekun Qi",
                "user": "qizekun",
                "type": "user"
            },
            "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.",
            "upvotes": 29,
            "discussionId": "683fe55c68402c738a8e5ff4",
            "ai_summary": "A comprehensive benchmark called OmniSpatial evaluates vision-language models' understanding of advanced spatial reasoning tasks, revealing significant limitations across various models.",
            "ai_keywords": [
                "vision-language models",
                "spatial reasoning",
                "cognitive psychology",
                "dynamic reasoning",
                "complex spatial logic",
                "spatial interaction",
                "perspective-taking",
                "question-answer pairs"
            ]
        },
        "publishedAt": "2025-06-03T13:58:29.000Z",
        "title": "OmniSpatial: Towards Comprehensive Spatial Reasoning Benchmark for\n  Vision Language Models",
        "summary": "Spatial reasoning is a key aspect of cognitive psychology and remains a major\nbottleneck for current vision-language models (VLMs). While extensive research\nhas aimed to evaluate or improve VLMs' understanding of basic spatial\nrelations, such as distinguishing left from right, near from far, and object\ncounting, these tasks represent only the most fundamental level of spatial\nreasoning. In this work, we introduce OmniSpatial, a comprehensive and\nchallenging benchmark for spatial reasoning, grounded in cognitive psychology.\nOmniSpatial covers four major categories: dynamic reasoning, complex spatial\nlogic, spatial interaction, and perspective-taking, with 50 fine-grained\nsubcategories. Through Internet data crawling and careful manual annotation, we\nconstruct over 1.5K question-answer pairs. Extensive experiments show that both\nopen- and closed-source VLMs, as well as existing reasoning and spatial\nunderstanding models, exhibit significant limitations in comprehensive spatial\nunderstanding. We further analyze failure cases and propose potential\ndirections for future research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63c3e8abc7d7f4c63a515a02/ZZsfC9We_mrTAc2s-h5ng.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03135.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63c3e8abc7d7f4c63a515a02",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c3e8abc7d7f4c63a515a02/npMHnVP2hHLbvoUGe7C4O.jpeg",
            "fullname": "Zekun Qi",
            "name": "qizekun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02397",
            "authors": [
                {
                    "_id": "683ff28ae06dd31ada292230",
                    "user": {
                        "_id": "683fec6e84131e802ab1a746",
                        "avatarUrl": "/avatars/5357ef43d963c6765cd3d3f3d1c4e535.svg",
                        "isPro": false,
                        "fullname": "Shengjia Zhang",
                        "user": "Cynthia-1628",
                        "type": "user"
                    },
                    "name": "Shengjia Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:06.870Z",
                    "hidden": false
                },
                {
                    "_id": "683ff28ae06dd31ada292231",
                    "name": "Junjie Wu",
                    "hidden": false
                },
                {
                    "_id": "683ff28ae06dd31ada292232",
                    "name": "Jiawei Chen",
                    "hidden": false
                },
                {
                    "_id": "683ff28ae06dd31ada292233",
                    "name": "Changwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "683ff28ae06dd31ada292234",
                    "name": "Xingyu Lou",
                    "hidden": false
                },
                {
                    "_id": "683ff28ae06dd31ada292235",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "683ff28ae06dd31ada292236",
                    "name": "Sheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "683ff28ae06dd31ada292237",
                    "name": "Can Wang",
                    "hidden": false
                },
                {
                    "_id": "683ff28ae06dd31ada292238",
                    "user": {
                        "_id": "66da6c9e84f243eba3b49cf1",
                        "avatarUrl": "/avatars/d30d96025141d68f18a28005a5d6f5af.svg",
                        "isPro": false,
                        "fullname": "junwang.lu",
                        "user": "jwanglux",
                        "type": "user"
                    },
                    "name": "Jun Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-04T07:15:23.472Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T03:31:30.000Z",
            "submittedOnDailyAt": "2025-06-04T09:47:01.150Z",
            "title": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation",
            "submittedOnDailyBy": {
                "_id": "683fec6e84131e802ab1a746",
                "avatarUrl": "/avatars/5357ef43d963c6765cd3d3f3d1c4e535.svg",
                "isPro": false,
                "fullname": "Shengjia Zhang",
                "user": "Cynthia-1628",
                "type": "user"
            },
            "summary": "Recent advanced large reasoning models (LRMs) leverage extended\nchain-of-thought (CoT) reasoning to solve complex tasks, achieving\nstate-of-the-art performance. Despite their success, we identify a critical\nissue: a substantial portion of simple tasks solved by LRMs can also be\naddressed by non-reasoning LLMs using significantly fewer tokens, indicating\nthe complex reasoning may not always be necessary. To address this, we\nsystematically analyze the reasoning trajectories of LRMs and present a method\nutilizing identified paradigms and LLM-Judge to classify these trajectories as\neither Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,\na method that prunes redundant reasoning steps while preserving logical\nvalidity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)\nfor straightforward problems while engaging in deliberate thinking\n(slow-thinking) for complex problems. Experiments across mathematical and\nquestion-answering tasks demonstrate that OThink-R1 reduces reasoning\nredundancy by almost 23\\% on average without compromising accuracy, offering\npractical guidelines for efficient reasoning models. The code is available at\nhttps://github.com/AgenticIR-Lab/OThink-R1.",
            "upvotes": 28,
            "discussionId": "683ff28be06dd31ada292267",
            "ai_summary": "OThink-R1 is introduced to reduce reasoning redundancy in complex problem-solving by classifying reasoning steps as essential or redundant and dynamically switching thinking modes based on task complexity.",
            "ai_keywords": [
                "chain-of-thought (CoT) reasoning",
                "large reasoning models (LRMs)",
                "non-reasoning LLMs",
                "reasoning trajectories",
                "LLM-Judge",
                "Redundant Reasoning",
                "Essential Reasoning",
                "OThink-R1",
                "fast-thinking",
                "slow-thinking",
                "mathematical tasks",
                "question-answering tasks"
            ]
        },
        "publishedAt": "2025-06-02T23:31:30.000Z",
        "title": "OThink-R1: Intrinsic Fast/Slow Thinking Mode Switching for\n  Over-Reasoning Mitigation",
        "summary": "Recent advanced large reasoning models (LRMs) leverage extended\nchain-of-thought (CoT) reasoning to solve complex tasks, achieving\nstate-of-the-art performance. Despite their success, we identify a critical\nissue: a substantial portion of simple tasks solved by LRMs can also be\naddressed by non-reasoning LLMs using significantly fewer tokens, indicating\nthe complex reasoning may not always be necessary. To address this, we\nsystematically analyze the reasoning trajectories of LRMs and present a method\nutilizing identified paradigms and LLM-Judge to classify these trajectories as\neither Redundant Reasoning or Essential Reasoning. And we introduce OThink-R1,\na method that prunes redundant reasoning steps while preserving logical\nvalidity. OThink-R1 dynamically employs the non-thinking mode (fast-thinking)\nfor straightforward problems while engaging in deliberate thinking\n(slow-thinking) for complex problems. Experiments across mathematical and\nquestion-answering tasks demonstrate that OThink-R1 reduces reasoning\nredundancy by almost 23\\% on average without compromising accuracy, offering\npractical guidelines for efficient reasoning models. The code is available at\nhttps://github.com/AgenticIR-Lab/OThink-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02397.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "683fec6e84131e802ab1a746",
            "avatarUrl": "/avatars/5357ef43d963c6765cd3d3f3d1c4e535.svg",
            "fullname": "Shengjia Zhang",
            "name": "Cynthia-1628",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00123",
            "authors": [
                {
                    "_id": "683e709a3a4c2c3b2750fc32",
                    "name": "Gen Luo",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc33",
                    "user": {
                        "_id": "6565d7149afd51867e55520b",
                        "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
                        "isPro": false,
                        "fullname": "Ganlin Yang",
                        "user": "ganlinyang",
                        "type": "user"
                    },
                    "name": "Ganlin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:45:03.857Z",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc34",
                    "user": {
                        "_id": "660691330be1fbe3b9e4c33d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660691330be1fbe3b9e4c33d/TxrDFH_cRu3AlpMC3xmhv.jpeg",
                        "isPro": false,
                        "fullname": "ZiYang Gong",
                        "user": "Cusyoung",
                        "type": "user"
                    },
                    "name": "Ziyang Gong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:45:00.364Z",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc35",
                    "name": "Guanzhou Chen",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc36",
                    "user": {
                        "_id": "66ab30dfd456f0408b93f27b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66ab30dfd456f0408b93f27b/nps4Kni_eOExO5Z92RiiF.jpeg",
                        "isPro": false,
                        "fullname": "Haonan Duan",
                        "user": "robot-haonan",
                        "type": "user"
                    },
                    "name": "Haonan Duan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T09:03:03.236Z",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc37",
                    "name": "Erfei Cui",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc38",
                    "name": "Ronglei Tong",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc39",
                    "name": "Zhi Hou",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc3a",
                    "name": "Tianyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc3b",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc3c",
                    "name": "Shenglong Ye",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc3d",
                    "name": "Lewei Lu",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc3e",
                    "name": "Jingbo Wang",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc3f",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc40",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc41",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc42",
                    "name": "Rongrong Ji",
                    "hidden": false
                },
                {
                    "_id": "683e709a3a4c2c3b2750fc43",
                    "name": "Xizhou Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T18:00:34.000Z",
            "submittedOnDailyAt": "2025-06-04T03:43:23.019Z",
            "title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces",
            "submittedOnDailyBy": {
                "_id": "6565d7149afd51867e55520b",
                "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
                "isPro": false,
                "fullname": "Ganlin Yang",
                "user": "ganlinyang",
                "type": "user"
            },
            "summary": "The remarkable progress of Multimodal Large Language Models (MLLMs) has\nattracted increasing attention to extend them to physical entities like legged\nrobot. This typically requires MLLMs to not only grasp multimodal understanding\nabilities, but also integrate visual-spatial reasoning and physical interaction\ncapabilities. Nevertheless,existing methods struggle to unify these\ncapabilities due to their fundamental differences.In this paper, we present the\nVisual Embodied Brain (VeBrain), a unified framework for perception, reasoning,\nand control in real world. VeBrain reformulates robotic control into common\ntext-based MLLM tasks in the 2D visual space, thus unifying the objectives and\nmapping spaces of different tasks. Then, a novel robotic adapter is proposed to\nconvert textual control signals from MLLMs to motion policies of real robots.\nFrom the data perspective, we further introduce VeBrain-600k, a high-quality\ninstruction dataset encompassing various capabilities of VeBrain. In\nVeBrain-600k, we take hundreds of hours to collect, curate and annotate the\ndata, and adopt multimodal chain-of-thought(CoT) to mix the different\ncapabilities into a single conversation. Extensive experiments on 13 multimodal\nbenchmarks and 5 spatial intelligence benchmarks demonstrate the superior\nperformance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to\nlegged robots and robotic arms, VeBrain shows strong adaptability, flexibility,\nand compositional capabilities compared to existing methods. For example,\ncompared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by\n+5.6%, but also excels in legged robot tasks with +50% average gains.",
            "upvotes": 28,
            "discussionId": "683e70a13a4c2c3b2750fd76",
            "projectPage": "https://internvl.github.io/blog/2025-05-26-VeBrain/",
            "githubRepo": "https://github.com/OpenGVLab/VeBrain",
            "ai_summary": "VeBrain is a unified framework that integrates multimodal understanding, visual-spatial reasoning, and physical interaction for legged robots, demonstrating superior performance compared to existing methods across various benchmarks.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "VeBrain",
                "Visual Embodied Brain",
                "text-based MLLM tasks",
                "robotic adapter",
                "VeBrain-600k",
                "multimodal chain-of-thought",
                "MMVet",
                "compositional capabilities"
            ]
        },
        "publishedAt": "2025-05-30T14:00:34.000Z",
        "title": "Visual Embodied Brain: Let Multimodal Large Language Models See, Think,\n  and Control in Spaces",
        "summary": "The remarkable progress of Multimodal Large Language Models (MLLMs) has\nattracted increasing attention to extend them to physical entities like legged\nrobot. This typically requires MLLMs to not only grasp multimodal understanding\nabilities, but also integrate visual-spatial reasoning and physical interaction\ncapabilities. Nevertheless,existing methods struggle to unify these\ncapabilities due to their fundamental differences.In this paper, we present the\nVisual Embodied Brain (VeBrain), a unified framework for perception, reasoning,\nand control in real world. VeBrain reformulates robotic control into common\ntext-based MLLM tasks in the 2D visual space, thus unifying the objectives and\nmapping spaces of different tasks. Then, a novel robotic adapter is proposed to\nconvert textual control signals from MLLMs to motion policies of real robots.\nFrom the data perspective, we further introduce VeBrain-600k, a high-quality\ninstruction dataset encompassing various capabilities of VeBrain. In\nVeBrain-600k, we take hundreds of hours to collect, curate and annotate the\ndata, and adopt multimodal chain-of-thought(CoT) to mix the different\ncapabilities into a single conversation. Extensive experiments on 13 multimodal\nbenchmarks and 5 spatial intelligence benchmarks demonstrate the superior\nperformance of VeBrain to existing MLLMs like Qwen2.5-VL. When deployed to\nlegged robots and robotic arms, VeBrain shows strong adaptability, flexibility,\nand compositional capabilities compared to existing methods. For example,\ncompared to Qwen2.5-VL, VeBrain not only achieves substantial gains on MMVet by\n+5.6%, but also excels in legged robot tasks with +50% average gains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00123.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6565d7149afd51867e55520b",
            "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
            "fullname": "Ganlin Yang",
            "name": "ganlinyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03143",
            "authors": [
                {
                    "_id": "683fc5599363b50c19f17d42",
                    "user": {
                        "_id": "63ef330b1e695b35aa484e11",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ef330b1e695b35aa484e11/bXwpGy0dl8JXeJwJ--ilr.jpeg",
                        "isPro": false,
                        "fullname": "Qianhui WU",
                        "user": "qianhuiwu",
                        "type": "user"
                    },
                    "name": "Qianhui Wu",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-04T04:21:31.171Z",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d43",
                    "user": {
                        "_id": "63340dbbd92c5842ae71d1e9",
                        "avatarUrl": "/avatars/3a3182996bd41b526dcbfa8687d91963.svg",
                        "isPro": false,
                        "fullname": "Kanzhi Cheng",
                        "user": "cckevinn",
                        "type": "user"
                    },
                    "name": "Kanzhi Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:48.476Z",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d44",
                    "user": {
                        "_id": "64d45451c34a346181b130dd",
                        "avatarUrl": "/avatars/9bb8205b889337df5d321539c9b5d69d.svg",
                        "isPro": false,
                        "fullname": "Rui Yang",
                        "user": "Ray2333",
                        "type": "user"
                    },
                    "name": "Rui Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:51.070Z",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d45",
                    "user": {
                        "_id": "654dbac9938fbf1e696be8aa",
                        "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                        "isPro": false,
                        "fullname": "Chaoyun Zhang",
                        "user": "vyokky",
                        "type": "user"
                    },
                    "name": "Chaoyun Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:54.267Z",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d46",
                    "name": "Jianwei Yang",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d47",
                    "name": "Huiqiang Jiang",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d48",
                    "name": "Jian Mu",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d49",
                    "name": "Baolin Peng",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d4a",
                    "name": "Bo Qiao",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d4b",
                    "name": "Reuben Tan",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d4c",
                    "name": "Si Qin",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d4d",
                    "name": "Lars Liden",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d4e",
                    "name": "Qingwei Lin",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d4f",
                    "name": "Huan Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d50",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d51",
                    "name": "Jianbing Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d52",
                    "name": "Dongmei Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fc5599363b50c19f17d53",
                    "name": "Jianfeng Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:59:08.000Z",
            "submittedOnDailyAt": "2025-06-04T02:44:41.730Z",
            "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
            "submittedOnDailyBy": {
                "_id": "654dbac9938fbf1e696be8aa",
                "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                "isPro": false,
                "fullname": "Chaoyun Zhang",
                "user": "vyokky",
                "type": "user"
            },
            "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.",
            "upvotes": 26,
            "discussionId": "683fc55d9363b50c19f17e6b",
            "projectPage": "https://microsoft.github.io/GUI-Actor/",
            "githubRepo": "https://github.com/microsoft/GUI-Actor",
            "ai_summary": "GUI-Actor, a VLM-based method using attention for coordinate-free GUI grounding, outperforms existing methods with better generalization and efficient fine-tuning.",
            "ai_keywords": [
                "visual grounding",
                "text-based coordinate generation",
                "spatial-semantic alignment",
                "Vision Transformers",
                "attention-based action head",
                "grounding verifier",
                "GUI action grounding benchmarks",
                "GUI-Actor",
                "GUI-Actor-7B",
                "UI-TARS-72B",
                "ScreenSpot-Pro",
                "Qwen2-VL",
                "Qwen2.5-VL",
                "parameter-efficient fine-tuning"
            ]
        },
        "publishedAt": "2025-06-03T13:59:08.000Z",
        "title": "GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents",
        "summary": "One of the principal challenges in building VLM-powered GUI agents is visual\ngrounding, i.e., localizing the appropriate screen region for action execution\nbased on both the visual content and the textual plans. Most existing work\nformulates this as a text-based coordinate generation task. However, these\napproaches suffer from several limitations: weak spatial-semantic alignment,\ninability to handle ambiguous supervision targets, and a mismatch between the\ndense nature of screen coordinates and the coarse, patch-level granularity of\nvisual features extracted by models like Vision Transformers. In this paper, we\npropose GUI-Actor, a VLM-based method for coordinate-free GUI grounding. At its\ncore, GUI-Actor introduces an attention-based action head that learns to align\na dedicated <ACTOR> token with all relevant visual patch tokens, enabling the\nmodel to propose one or more action regions in a single forward pass. In line\nwith this, we further design a grounding verifier to evaluate and select the\nmost plausible action region from the candidates proposed for action execution.\nExtensive experiments show that GUI-Actor outperforms prior state-of-the-art\nmethods on multiple GUI action grounding benchmarks, with improved\ngeneralization to unseen screen resolutions and layouts. Notably, GUI-Actor-7B\neven surpasses UI-TARS-72B (38.1) on ScreenSpot-Pro, achieving scores of 40.7\nwith Qwen2-VL and 44.6 with Qwen2.5-VL as backbones. Furthermore, by\nincorporating the verifier, we find that fine-tuning only the newly introduced\naction head (~100M parameters for 7B model) while keeping the VLM backbone\nfrozen is sufficient to achieve performance comparable to previous\nstate-of-the-art models, highlighting that GUI-Actor can endow the underlying\nVLM with effective grounding capabilities without compromising its\ngeneral-purpose strengths.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03143.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "fullname": "Chaoyun Zhang",
            "name": "vyokky",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23061",
            "authors": [
                {
                    "_id": "683fc4028de3ffc5838c3fa8",
                    "name": "Tarun Suresh",
                    "hidden": false
                },
                {
                    "_id": "683fc4028de3ffc5838c3fa9",
                    "name": "Debangshu Banerjee",
                    "hidden": false
                },
                {
                    "_id": "683fc4028de3ffc5838c3faa",
                    "user": {
                        "_id": "656c29ed271c5c4e3308a008",
                        "avatarUrl": "/avatars/9b58072230baf3ac941d476d69356fda.svg",
                        "isPro": false,
                        "fullname": "Shubham Ugare",
                        "user": "shubhamugare",
                        "type": "user"
                    },
                    "name": "Shubham Ugare",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T15:03:34.977Z",
                    "hidden": false
                },
                {
                    "_id": "683fc4028de3ffc5838c3fab",
                    "name": "Sasa Misailovic",
                    "hidden": false
                },
                {
                    "_id": "683fc4028de3ffc5838c3fac",
                    "name": "Gagandeep Singh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T04:04:54.000Z",
            "submittedOnDailyAt": "2025-06-04T02:27:09.092Z",
            "title": "DINGO: Constrained Inference for Diffusion LLMs",
            "submittedOnDailyBy": {
                "_id": "65e7bb35e5e78134ab049942",
                "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
                "isPro": false,
                "fullname": "Tarun Suresh",
                "user": "tarsur909",
                "type": "user"
            },
            "summary": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
            "upvotes": 24,
            "discussionId": "683fc4038de3ffc5838c3fd8",
            "ai_summary": "DINGO, a dynamic programming-based decoding strategy, enhances diffusion language models by enforcing structured output constraints, significantly improving performance on symbolic math and JSON generation tasks.",
            "ai_keywords": [
                "diffusion LLMs",
                "autoregressive LLMs",
                "formal constraints",
                "regular expressions",
                "sequential token prediction",
                "parallel token prediction",
                "dynamic programming",
                "constrained decoding",
                "output distribution",
                "symbolic math generation",
                "JSON generation"
            ]
        },
        "publishedAt": "2025-05-29T00:04:54.000Z",
        "title": "DINGO: Constrained Inference for Diffusion LLMs",
        "summary": "Diffusion LLMs have emerged as a promising alternative to conventional\nautoregressive LLMs, offering significant potential for improved runtime\nefficiency. However, existing diffusion models lack the ability to provably\nenforce user-specified formal constraints, such as regular expressions, which\nmakes them unreliable for tasks that require structured outputs, such as\nfixed-schema JSON generation. Unlike autoregressive models that generate tokens\nsequentially, diffusion LLMs predict a block of tokens in parallel. This\nparallelism makes traditional constrained decoding algorithms, which are\ndesigned for sequential token prediction, ineffective at preserving the true\noutput distribution. To address this limitation, we propose DINGO, a dynamic\nprogramming-based constrained decoding strategy that is both efficient and\nprovably distribution-preserving. DINGO enables sampling of output strings with\nthe highest probability under the model's predicted distribution, while\nstrictly satisfying any user-specified regular expression. On standard symbolic\nmath and JSON generation benchmarks, DINGO achieves up to a 68 percentage point\nimprovement over unconstrained inference",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23061.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e7bb35e5e78134ab049942",
            "avatarUrl": "/avatars/3c0972f0d59e51ebb5c218ee736d4458.svg",
            "fullname": "Tarun Suresh",
            "name": "tarsur909",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03065",
            "authors": [
                {
                    "_id": "683fc6241de14546d5e02775",
                    "user": {
                        "_id": "64c9bac33cfe45b07179568d",
                        "avatarUrl": "/avatars/4a8206cdb1770a8cdaae0d0a2b7b59f2.svg",
                        "isPro": false,
                        "fullname": "Pengtao Chen",
                        "user": "PengtaoChen",
                        "type": "user"
                    },
                    "name": "Pengtao Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:45.116Z",
                    "hidden": false
                },
                {
                    "_id": "683fc6241de14546d5e02776",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "683fc6241de14546d5e02777",
                    "name": "Maosen Zhao",
                    "hidden": false
                },
                {
                    "_id": "683fc6241de14546d5e02778",
                    "name": "Peng Ye",
                    "hidden": false
                },
                {
                    "_id": "683fc6241de14546d5e02779",
                    "name": "Mingzhu Shen",
                    "hidden": false
                },
                {
                    "_id": "683fc6241de14546d5e0277a",
                    "user": {
                        "_id": "64b914c8ace99c0723ad83a9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
                        "isPro": false,
                        "fullname": "Wei Cheng",
                        "user": "wchengad",
                        "type": "user"
                    },
                    "name": "Wei Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:42.268Z",
                    "hidden": false
                },
                {
                    "_id": "683fc6241de14546d5e0277b",
                    "user": {
                        "_id": "63417332c5565a4b8d43a0d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63417332c5565a4b8d43a0d8/MmnYG7Wu2Z_lqCBvTfJmy.png",
                        "isPro": false,
                        "fullname": "Gang Yu",
                        "user": "skicy",
                        "type": "user"
                    },
                    "name": "Gang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:39.068Z",
                    "hidden": false
                },
                {
                    "_id": "683fc6241de14546d5e0277c",
                    "name": "Tao Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
            ],
            "publishedAt": "2025-06-03T16:42:37.000Z",
            "submittedOnDailyAt": "2025-06-04T04:37:13.352Z",
            "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
            "submittedOnDailyBy": {
                "_id": "64b914c8ace99c0723ad83a9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
                "isPro": false,
                "fullname": "Wei Cheng",
                "user": "wchengad",
                "type": "user"
            },
            "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical\nFLOP reduction, and actual inference speedups of 1.76times, 1.85times,\nand 1.58times, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
            "upvotes": 21,
            "discussionId": "683fc62b1de14546d5e02931",
            "githubRepo": "https://github.com/Peyton-Chen/Sparse-vDiT",
            "ai_summary": "Sparse-vDiT accelerates Video Diffusion Transformer (vDiT) by leveraging sparsity patterns in attention maps, reducing theoretical FLOPs and improving inference speed without significant loss in visual quality.",
            "ai_keywords": [
                "Diffusion Transformers",
                "DiTs",
                "Video Diffusion Transformer",
                "vDiT",
                "sparsity patterns",
                "diagonal",
                "multi-diagonal",
                "vertical-stripe structures",
                "sparse kernels",
                "sparse diffusion search algorithm",
                "FLOP reduction",
                "inference speedups",
                "PSNR",
                "latent structural sparsity"
            ]
        },
        "publishedAt": "2025-06-03T12:42:37.000Z",
        "title": "Sparse-vDiT: Unleashing the Power of Sparse Attention to Accelerate\n  Video Diffusion Transformers",
        "summary": "While Diffusion Transformers (DiTs) have achieved breakthroughs in video\ngeneration, this long sequence generation task remains constrained by the\nquadratic complexity of attention mechanisms, resulting in significant\ninference latency. Through detailed analysis of attention maps in Video\nDiffusion Transformer (vDiT), we identify three recurring sparsity patterns:\ndiagonal, multi-diagonal, and vertical-stripe structures. And even 3-6\\%\nattention heads can be skipped. Crucially, these patterns exhibit strong\nlayer-depth and head-position correlations but show limited dependence on the\ninput content. Leveraging these findings, we propose Sparse-vDiT, a sparsity\nacceleration framework for vDiT comprising: 1) Pattern-optimized sparse kernels\nthat replace dense attention with computationally efficient implementations for\neach identified sparsity pattern. 2) An offline sparse diffusion search\nalgorithm that selects the optimal sparse computation strategy per layer and\nhead via hardware-aware cost modeling. After determining the optimal\nconfiguration, we fuse heads within the same layer that share the same\nattention strategy, enhancing inference efficiency. Integrated into\nstate-of-the-art vDiT models (CogVideoX1.5, HunyuanVideo, and Wan2.1),\nSparse-vDiT achieves 2.09times, 2.38times, and 1.67times theoretical\nFLOP reduction, and actual inference speedups of 1.76times, 1.85times,\nand 1.58times, respectively, while maintaining high visual fidelity, with\nPSNR values reaching 24.13, 27.09, and 22.59. Our work demonstrates that latent\nstructural sparsity in vDiTs can be systematically exploited for long video\nsynthesis.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64b914c8ace99c0723ad83a9/iewcVFqJgelW9EQTlH_LH.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03065.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b914c8ace99c0723ad83a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b914c8ace99c0723ad83a9/B4gxNByeVY_xaOcjwiN1j.jpeg",
            "fullname": "Wei Cheng",
            "name": "wchengad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01674",
            "authors": [
                {
                    "_id": "683faa6515abeae85e13336b",
                    "name": "Yipeng Du",
                    "hidden": false
                },
                {
                    "_id": "683faa6515abeae85e13336c",
                    "name": "Tiehan Fan",
                    "hidden": false
                },
                {
                    "_id": "683faa6515abeae85e13336d",
                    "name": "Kepan Nan",
                    "hidden": false
                },
                {
                    "_id": "683faa6515abeae85e13336e",
                    "name": "Rui Xie",
                    "hidden": false
                },
                {
                    "_id": "683faa6515abeae85e13336f",
                    "name": "Penghao Zhou",
                    "hidden": false
                },
                {
                    "_id": "683faa6515abeae85e133370",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "683faa6515abeae85e133371",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "683faa6515abeae85e133372",
                    "name": "Zhenheng Yang",
                    "hidden": false
                },
                {
                    "_id": "683faa6515abeae85e133373",
                    "user": {
                        "_id": "65734004769f3ee9bde1af10",
                        "avatarUrl": "/avatars/d6310ed861972fd691687d8f47413f33.svg",
                        "isPro": false,
                        "fullname": "Ying Tai",
                        "user": "yingtai",
                        "type": "user"
                    },
                    "name": "Ying Tai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:56:59.253Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T13:44:56.000Z",
            "submittedOnDailyAt": "2025-06-04T00:38:03.935Z",
            "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "65927f3b754092f6b1e187a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
                "isPro": false,
                "fullname": "tiehan fan",
                "user": "AnonMegumi",
                "type": "user"
            },
            "summary": "Despite advancements in Multimodal Large Language Models (MLLMs), their\nproficiency in fine-grained video motion understanding remains critically\nlimited. They often lack inter-frame differencing and tend to average or ignore\nsubtle visual cues. Furthermore, while visual prompting has shown potential in\nstatic images, its application to video's temporal complexities, particularly\nfor fine-grained motion understanding, remains largely unexplored. We\ninvestigate whether inherent capability can be unlocked and boost MLLMs' motion\nperception and enable distinct visual signatures tailored to decouple object\nand camera motion cues. In this study, we introduce MotionSight, a novel\nzero-shot method pioneering object-centric visual spotlight and motion blur as\nvisual prompts to effectively improve fine-grained motion understanding without\ntraining. To convert this into valuable data assets, we curated MotionVid-QA,\nthe first large-scale dataset for fine-grained video motion understanding, with\nhierarchical annotations including SFT and preference data, {\\Theta}(40K) video\nclips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves\nstate-of-the-art open-source performance and competitiveness with commercial\nmodels. In particular, for fine-grained motion understanding we present a novel\nzero-shot technique and a large-scale, high-quality dataset. All the code and\nannotations will be publicly available.",
            "upvotes": 21,
            "discussionId": "683faa6615abeae85e1333c2",
            "projectPage": "https://nju-pcalab.github.io/projects/MotionSight/",
            "githubRepo": "https://github.com/NJU-PCALab/MotionSight",
            "ai_summary": "MotionSight, a zero-shot method using object-centric visual spotlight and motion blur as prompts, enhances fine-grained video motion understanding and achieves state-of-the-art performance on MotionVid-QA, a large-scale dataset with hierarchical annotations.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "fine-grained video motion understanding",
                "inter-frame differencing",
                "visual prompting",
                "temporal complexities",
                "MotionSight",
                "object-centric visual spotlight",
                "motion blur",
                "MotionVid-QA",
                "hierarchical annotations",
                "SFT",
                "preference data",
                "state-of-the-art performance"
            ]
        },
        "publishedAt": "2025-06-02T09:44:56.000Z",
        "title": "MotionSight: Boosting Fine-Grained Motion Understanding in Multimodal\n  LLMs",
        "summary": "Despite advancements in Multimodal Large Language Models (MLLMs), their\nproficiency in fine-grained video motion understanding remains critically\nlimited. They often lack inter-frame differencing and tend to average or ignore\nsubtle visual cues. Furthermore, while visual prompting has shown potential in\nstatic images, its application to video's temporal complexities, particularly\nfor fine-grained motion understanding, remains largely unexplored. We\ninvestigate whether inherent capability can be unlocked and boost MLLMs' motion\nperception and enable distinct visual signatures tailored to decouple object\nand camera motion cues. In this study, we introduce MotionSight, a novel\nzero-shot method pioneering object-centric visual spotlight and motion blur as\nvisual prompts to effectively improve fine-grained motion understanding without\ntraining. To convert this into valuable data assets, we curated MotionVid-QA,\nthe first large-scale dataset for fine-grained video motion understanding, with\nhierarchical annotations including SFT and preference data, {\\Theta}(40K) video\nclips and {\\Theta}(87K) QAs. Experiments show MotionSight achieves\nstate-of-the-art open-source performance and competitiveness with commercial\nmodels. In particular, for fine-grained motion understanding we present a novel\nzero-shot technique and a large-scale, high-quality dataset. All the code and\nannotations will be publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01674.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65927f3b754092f6b1e187a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65927f3b754092f6b1e187a7/gUrNvIQHmsl1vLwSUxpmL.jpeg",
            "fullname": "tiehan fan",
            "name": "AnonMegumi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.00070",
            "authors": [
                {
                    "_id": "683fd7d79d4fb703271ad9ac",
                    "name": "Dongyoung Kim",
                    "hidden": false
                },
                {
                    "_id": "683fd7d79d4fb703271ad9ad",
                    "name": "Sumin Park",
                    "hidden": false
                },
                {
                    "_id": "683fd7d79d4fb703271ad9ae",
                    "name": "Huiwon Jang",
                    "hidden": false
                },
                {
                    "_id": "683fd7d79d4fb703271ad9af",
                    "name": "Jinwoo Shin",
                    "hidden": false
                },
                {
                    "_id": "683fd7d79d4fb703271ad9b0",
                    "name": "Jaehyung Kim",
                    "hidden": false
                },
                {
                    "_id": "683fd7d79d4fb703271ad9b1",
                    "name": "Younggyo Seo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T16:41:12.000Z",
            "submittedOnDailyAt": "2025-06-04T03:53:04.651Z",
            "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics",
            "submittedOnDailyBy": {
                "_id": "658d79663a5202a485a76d9b",
                "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
                "isPro": false,
                "fullname": "dongyoung kim",
                "user": "vangard703",
                "type": "user"
            },
            "summary": "Large Vision-Language Models (LVLMs) have recently shown great promise in\nadvancing robotics by combining embodied reasoning with robot control. A common\napproach involves training on embodied reasoning tasks related to robot control\nusing Supervised Fine-Tuning (SFT). However, SFT datasets are often\nheuristically constructed and not explicitly optimized for improving robot\ncontrol. Furthermore, SFT often leads to issues such as catastrophic forgetting\nand reduced generalization performance. To address these limitations, we\nintroduce Robot-R1, a novel framework that leverages reinforcement learning to\nenhance embodied reasoning specifically for robot control. Robot-R1 learns to\npredict the next keypoint state required for task completion, conditioned on\nthe current scene image and environment metadata derived from expert\ndemonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples\nreasoning-based responses and reinforces those that lead to more accurate\npredictions. Our experiments show that models trained with Robot-R1 outperform\nSFT methods on embodied reasoning tasks. Despite having only 7B parameters,\nRobot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action\ncontrol, such as spatial and primitive movement reasoning.",
            "upvotes": 19,
            "discussionId": "683fd7d79d4fb703271ad9d9",
            "ai_summary": "Robot-R1, a reinforcement learning framework, enhances embodied reasoning for robotics by predicting keypoint states, outperforming supervised fine-tuning methods and even surpassing GPT-4o in low-level action control tasks.",
            "ai_keywords": [
                "Large Vision-Language Models (LVLMs)",
                "embodied reasoning",
                "robot control",
                "Supervised Fine-Tuning (SFT)",
                "catastrophic forgetting",
                "generalization performance",
                "reinforcement learning",
                "keypoint state",
                "scene image",
                "environment metadata",
                "expert demonstrations",
                "DeepSeek-R1",
                "reasoning-based responses",
                "parameter-efficient fine-tuning"
            ]
        },
        "publishedAt": "2025-05-29T12:41:12.000Z",
        "title": "Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in\n  Robotics",
        "summary": "Large Vision-Language Models (LVLMs) have recently shown great promise in\nadvancing robotics by combining embodied reasoning with robot control. A common\napproach involves training on embodied reasoning tasks related to robot control\nusing Supervised Fine-Tuning (SFT). However, SFT datasets are often\nheuristically constructed and not explicitly optimized for improving robot\ncontrol. Furthermore, SFT often leads to issues such as catastrophic forgetting\nand reduced generalization performance. To address these limitations, we\nintroduce Robot-R1, a novel framework that leverages reinforcement learning to\nenhance embodied reasoning specifically for robot control. Robot-R1 learns to\npredict the next keypoint state required for task completion, conditioned on\nthe current scene image and environment metadata derived from expert\ndemonstrations. Inspired by the DeepSeek-R1 learning approach, Robot-R1 samples\nreasoning-based responses and reinforces those that lead to more accurate\npredictions. Our experiments show that models trained with Robot-R1 outperform\nSFT methods on embodied reasoning tasks. Despite having only 7B parameters,\nRobot-R1 even surpasses GPT-4o on reasoning tasks related to low-level action\ncontrol, such as spatial and primitive movement reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00070.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "658d79663a5202a485a76d9b",
            "avatarUrl": "/avatars/1893384f28a7cb82d4588576c4c264f1.svg",
            "fullname": "dongyoung kim",
            "name": "vangard703",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03136",
            "authors": [
                {
                    "_id": "683fa2ddbde0ae60c2f16183",
                    "name": "Yinjie Wang",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddbde0ae60c2f16184",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddbde0ae60c2f16185",
                    "name": "Ye Tian",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddbde0ae60c2f16186",
                    "name": "Ke Shen",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddbde0ae60c2f16187",
                    "name": "Mengdi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:58:42.000Z",
            "submittedOnDailyAt": "2025-06-04T00:39:07.151Z",
            "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64fde4e252e82dd432b74ce9",
                "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                "isPro": false,
                "fullname": "Ling Yang",
                "user": "Lingaaaaaaa",
                "type": "user"
            },
            "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
            "upvotes": 18,
            "discussionId": "683fa2debde0ae60c2f161cb",
            "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-coder-6833109ed9300c62deb32c6b",
            "githubRepo": "https://github.com/Gen-Verse/CURE",
            "ai_summary": "CURE, a reinforcement learning framework, improves code and unit test generation accuracy without ground-truth supervision, enhancing performance in various coding and testing tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "reward design",
                "coding",
                "unit test generation",
                "ReasonFlux-Coder",
                "Qwen2.5-Instruct",
                "Qwen-Coder",
                "DeepSeek-Coder",
                "Seed-Coder",
                "test-time scaling",
                "agentic coding",
                "long-CoT",
                "inference efficiency",
                "reward model"
            ]
        },
        "publishedAt": "2025-06-03T13:58:42.000Z",
        "title": "Co-Evolving LLM Coder and Unit Tester via Reinforcement Learning",
        "summary": "We propose CURE, a novel reinforcement learning framework with a dedicated\nreward design that co-evolves coding and unit test generation capabilities\nbased on their interaction outcomes, without any ground-truth code as\nsupervision. This approach enables flexible and scalable training and allows\nthe unit tester to learn directly from the coder's mistakes. Our derived\nReasonFlux-Coder-7B and 14B models improve code generation accuracy by 5.3% and\nBest-of-N accuracy by 9.0% after optimization on Qwen2.5-Instruct models,\noutperforming similarly sized Qwen-Coder, DeepSeek-Coder, and Seed-Coder. They\nnaturally extend to downstream tasks such as test-time scaling and agentic\ncoding-achieving a 8.1% improvement over the base model. For the long-CoT\nmodel, our ReasonFlux-Coder-4B consistently outperforms Qwen3-4B while\nachieving 64.8% inference efficiency in unit test generation. Notably, we also\nfind that our model can serve as an effective reward model for reinforcement\nlearning on base models. Project: https://github.com/Gen-Verse/CURE",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03136.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03131",
            "authors": [
                {
                    "_id": "683fb2786a2b978ca4e62493",
                    "user": {
                        "_id": "64b7aa374df206a3ed1947d2",
                        "avatarUrl": "/avatars/a7c7e703ccf8824259fc5a8a90a25746.svg",
                        "isPro": false,
                        "fullname": "wzd",
                        "user": "GoodEnough",
                        "type": "user"
                    },
                    "name": "Zidong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:56:13.762Z",
                    "hidden": false
                },
                {
                    "_id": "683fb2786a2b978ca4e62494",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "683fb2786a2b978ca4e62495",
                    "name": "Xiangyu Yue",
                    "hidden": false
                },
                {
                    "_id": "683fb2786a2b978ca4e62496",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "683fb2786a2b978ca4e62497",
                    "name": "Yiyuan Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
            ],
            "publishedAt": "2025-06-03T17:57:33.000Z",
            "submittedOnDailyAt": "2025-06-04T01:13:44.155Z",
            "title": "Native-Resolution Image Synthesis",
            "submittedOnDailyBy": {
                "_id": "63176933b58b0184630d2c74",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
                "isPro": false,
                "fullname": "Yiyuan Zhang",
                "user": "Yiyuan",
                "type": "user"
            },
            "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
            "upvotes": 16,
            "discussionId": "683fb27e6a2b978ca4e625bd",
            "projectPage": "https://wzdthu.github.io/NiT/",
            "githubRepo": "https://github.com/WZDTHU/NiT",
            "ai_summary": "A novel generative model, Native-resolution diffusion Transformer (NiT), synthesizes high-resolution and varied aspect ratio images with state-of-the-art performance and zero-shot generalization capabilities.",
            "ai_keywords": [
                "native-resolution image synthesis",
                "generative modeling paradigm",
                "variable-length visual tokens",
                "diffusion Transformer",
                "denoising process",
                "ImageNet-256x256",
                "ImageNet-512x512",
                "high-fidelity images",
                "aspect ratios",
                "zero-shot generalization"
            ]
        },
        "publishedAt": "2025-06-03T13:57:33.000Z",
        "title": "Native-Resolution Image Synthesis",
        "summary": "We introduce native-resolution image synthesis, a novel generative modeling\nparadigm that enables the synthesis of images at arbitrary resolutions and\naspect ratios. This approach overcomes the limitations of conventional\nfixed-resolution, square-image methods by natively handling variable-length\nvisual tokens, a core challenge for traditional techniques. To this end, we\nintroduce the Native-resolution diffusion Transformer (NiT), an architecture\ndesigned to explicitly model varying resolutions and aspect ratios within its\ndenoising process. Free from the constraints of fixed formats, NiT learns\nintrinsic visual distributions from images spanning a broad range of\nresolutions and aspect ratios. Notably, a single NiT model simultaneously\nachieves the state-of-the-art performance on both ImageNet-256x256 and 512x512\nbenchmarks. Surprisingly, akin to the robust zero-shot capabilities seen in\nadvanced large language models, NiT, trained solely on ImageNet, demonstrates\nexcellent zero-shot generalization performance. It successfully generates\nhigh-fidelity images at previously unseen high resolutions (e.g., 1536 x 1536)\nand diverse aspect ratios (e.g., 16:9, 3:1, 4:3), as shown in Figure 1. These\nfindings indicate the significant potential of native-resolution modeling as a\nbridge between visual generative modeling and advanced LLM methodologies.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63176933b58b0184630d2c74/VS2G-SBuSY5ltQmCLAaq3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03131.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63176933b58b0184630d2c74",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
            "fullname": "Yiyuan Zhang",
            "name": "Yiyuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03126",
            "authors": [
                {
                    "_id": "683fb3719f37285365b080c9",
                    "user": {
                        "_id": "672a037c19f1f942483f680c",
                        "avatarUrl": "/avatars/a48464044e9eb11a2bc062be05d9aa9a.svg",
                        "isPro": false,
                        "fullname": "qiulu",
                        "user": "qiulu66",
                        "type": "user"
                    },
                    "name": "Lu Qiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:56:04.988Z",
                    "hidden": false
                },
                {
                    "_id": "683fb3719f37285365b080ca",
                    "user": {
                        "_id": "630b094f8b327c7b8b94d24c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
                        "isPro": false,
                        "fullname": "Yizhuo Li",
                        "user": "liyz",
                        "type": "user"
                    },
                    "name": "Yizhuo Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:56:02.456Z",
                    "hidden": false
                },
                {
                    "_id": "683fb3719f37285365b080cb",
                    "name": "Yuying Ge",
                    "hidden": false
                },
                {
                    "_id": "683fb3719f37285365b080cc",
                    "name": "Yixiao Ge",
                    "hidden": false
                },
                {
                    "_id": "683fb3719f37285365b080cd",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "683fb3719f37285365b080ce",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:55:18.000Z",
            "submittedOnDailyAt": "2025-06-04T03:58:52.500Z",
            "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "630b094f8b327c7b8b94d24c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
                "isPro": false,
                "fullname": "Yizhuo Li",
                "user": "liyz",
                "type": "user"
            },
            "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.",
            "upvotes": 15,
            "discussionId": "683fb3749f37285365b08167",
            "projectPage": "https://qiulu66.github.io/animeshooter",
            "githubRepo": "https://github.com/qiulu66/Anime-Shooter",
            "ai_summary": "AnimeShooter, a reference-guided multi-shot animation dataset, enhances coherent animated video generation by incorporating comprehensive hierarchical annotations and visual consistency, and AnimeShooterGen leverages MLLMs and video diffusion models to achieve superior results.",
            "ai_keywords": [
                "AnimeShooter",
                "reference-guided",
                "multimodal large language models (MLLMs)",
                "video diffusion models",
                "hierarchical annotations",
                "visual consistency",
                "cross-shot visual consistency",
                "reference visual guidance"
            ]
        },
        "publishedAt": "2025-06-03T13:55:18.000Z",
        "title": "AnimeShooter: A Multi-Shot Animation Dataset for Reference-Guided Video\n  Generation",
        "summary": "Recent advances in AI-generated content (AIGC) have significantly accelerated\nanimation production. To produce engaging animations, it is essential to\ngenerate coherent multi-shot video clips with narrative scripts and character\nreferences. However, existing public datasets primarily focus on real-world\nscenarios with global descriptions, and lack reference images for consistent\ncharacter guidance. To bridge this gap, we present AnimeShooter, a\nreference-guided multi-shot animation dataset. AnimeShooter features\ncomprehensive hierarchical annotations and strong visual consistency across\nshots through an automated pipeline. Story-level annotations provide an\noverview of the narrative, including the storyline, key scenes, and main\ncharacter profiles with reference images, while shot-level annotations\ndecompose the story into consecutive shots, each annotated with scene,\ncharacters, and both narrative and descriptive visual captions. Additionally, a\ndedicated subset, AnimeShooter-audio, offers synchronized audio tracks for each\nshot, along with audio descriptions and sound sources. To demonstrate the\neffectiveness of AnimeShooter and establish a baseline for the reference-guided\nmulti-shot video generation task, we introduce AnimeShooterGen, which leverages\nMultimodal Large Language Models (MLLMs) and video diffusion models. The\nreference image and previously generated shots are first processed by MLLM to\nproduce representations aware of both reference and context, which are then\nused as the condition for the diffusion model to decode the subsequent shot.\nExperimental results show that the model trained on AnimeShooter achieves\nsuperior cross-shot visual consistency and adherence to reference visual\nguidance, which highlight the value of our dataset for coherent animated video\ngeneration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03126.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630b094f8b327c7b8b94d24c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
            "fullname": "Yizhuo Li",
            "name": "liyz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02497",
            "authors": [
                {
                    "_id": "683fbc32917306517315589f",
                    "name": "Jiahao Chen",
                    "hidden": false
                },
                {
                    "_id": "683fbc3291730651731558a0",
                    "name": "Hangjie Yuan",
                    "hidden": false
                },
                {
                    "_id": "683fbc3291730651731558a1",
                    "name": "Yichen Qian",
                    "hidden": false
                },
                {
                    "_id": "683fbc3291730651731558a2",
                    "name": "Jingyun Liang",
                    "hidden": false
                },
                {
                    "_id": "683fbc3291730651731558a3",
                    "name": "Jiazheng Xing",
                    "hidden": false
                },
                {
                    "_id": "683fbc3291730651731558a4",
                    "name": "Pengwei Liu",
                    "hidden": false
                },
                {
                    "_id": "683fbc3291730651731558a5",
                    "name": "Weihua Chen",
                    "hidden": false
                },
                {
                    "_id": "683fbc3291730651731558a6",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "683fbc3291730651731558a7",
                    "name": "Bing Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T06:25:00.000Z",
            "submittedOnDailyAt": "2025-06-04T01:55:34.274Z",
            "title": "LumosFlow: Motion-Guided Long Video Generation",
            "submittedOnDailyBy": {
                "_id": "649d54b314afbb10ce2a9eeb",
                "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                "isPro": false,
                "fullname": "Hangjie Yuan",
                "user": "JacobYuan",
                "type": "user"
            },
            "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/",
            "upvotes": 15,
            "discussionId": "683fbc36917306517315597d",
            "projectPage": "https://jiahaochen1.github.io/LumosFlow/",
            "ai_summary": "LumosFlow uses LMTV-DM for key frame generation and LOF-DM followed by MotionControlNet for smooth intermediate frame interpolation, ensuring temporally coherent long video generation.",
            "ai_keywords": [
                "Large Motion Text-to-Video Diffusion Model",
                "LMTV-DM",
                "Latent Optical Flow Diffusion Model",
                "LOF-DM",
                "MotionControlNet",
                "optical flows",
                "frame interpolation",
                "key frames",
                "long video generation",
                "motion guidance",
                "synthetic long videos"
            ]
        },
        "publishedAt": "2025-06-03T02:25:00.000Z",
        "title": "LumosFlow: Motion-Guided Long Video Generation",
        "summary": "Long video generation has gained increasing attention due to its widespread\napplications in fields such as entertainment and simulation. Despite advances,\nsynthesizing temporally coherent and visually compelling long sequences remains\na formidable challenge. Conventional approaches often synthesize long videos by\nsequentially generating and concatenating short clips, or generating key frames\nand then interpolate the intermediate frames in a hierarchical manner. However,\nboth of them still remain significant challenges, leading to issues such as\ntemporal repetition or unnatural transitions. In this paper, we revisit the\nhierarchical long video generation pipeline and introduce LumosFlow, a\nframework introduce motion guidance explicitly. Specifically, we first employ\nthe Large Motion Text-to-Video Diffusion Model (LMTV-DM) to generate key frames\nwith larger motion intervals, thereby ensuring content diversity in the\ngenerated long videos. Given the complexity of interpolating contextual\ntransitions between key frames, we further decompose the intermediate frame\ninterpolation into motion generation and post-hoc refinement. For each pair of\nkey frames, the Latent Optical Flow Diffusion Model (LOF-DM) synthesizes\ncomplex and large-motion optical flows, while MotionControlNet subsequently\nrefines the warped results to enhance quality and guide intermediate frame\ngeneration. Compared with traditional video frame interpolation, we achieve 15x\ninterpolation, ensuring reasonable and continuous motion between adjacent\nframes. Experiments show that our method can generate long videos with\nconsistent motion and appearance. Code and models will be made publicly\navailable upon acceptance. Our project page:\nhttps://jiahaochen1.github.io/LumosFlow/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02497.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "fullname": "Hangjie Yuan",
            "name": "JacobYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03621",
            "authors": [
                {
                    "_id": "6840ee8b6b106ae42f553c3b",
                    "name": "Chaehun Shin",
                    "hidden": false
                },
                {
                    "_id": "6840ee8b6b106ae42f553c3c",
                    "name": "Jooyoung Choi",
                    "hidden": false
                },
                {
                    "_id": "6840ee8b6b106ae42f553c3d",
                    "name": "Johan Barthelemy",
                    "hidden": false
                },
                {
                    "_id": "6840ee8b6b106ae42f553c3e",
                    "name": "Jungbeom Lee",
                    "hidden": false
                },
                {
                    "_id": "6840ee8b6b106ae42f553c3f",
                    "name": "Sungroh Yoon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T06:59:25.000Z",
            "submittedOnDailyAt": "2025-06-04T23:42:33.735Z",
            "title": "Negative-Guided Subject Fidelity Optimization for Zero-Shot\n  Subject-Driven Generation",
            "submittedOnDailyBy": {
                "_id": "631842124d24c00131a66656",
                "avatarUrl": "/avatars/590633e163da425bf041e3988d8dd015.svg",
                "isPro": false,
                "fullname": "shin",
                "user": "chaehun",
                "type": "user"
            },
            "summary": "We present Subject Fidelity Optimization (SFO), a novel comparative learning\nframework for zero-shot subject-driven generation that enhances subject\nfidelity. Beyond supervised fine-tuning methods that rely only on positive\ntargets and use the diffusion loss as in the pre-training stage, SFO introduces\nsynthetic negative targets and explicitly guides the model to favor positives\nover negatives through pairwise comparison. For negative targets, we propose\nCondition-Degradation Negative Sampling (CDNS), which automatically generates\ndistinctive and informative negatives by intentionally degrading visual and\ntextual cues without expensive human annotations. Moreover, we reweight the\ndiffusion timesteps to focus finetuning on intermediate steps where subject\ndetails emerge. Extensive experiments demonstrate that SFO with CDNS\nsignificantly outperforms baselines in terms of both subject fidelity and text\nalignment on a subject-driven generation benchmark. Project page:\nhttps://subjectfidelityoptimization.github.io/",
            "upvotes": 14,
            "discussionId": "6840ee8f6b106ae42f553cfa",
            "projectPage": "https://subjectfidelityoptimization.github.io/",
            "ai_summary": "Subject Fidelity Optimization (SFO) enhances zero-shot subject-driven generation by introducing synthetic negative samples and optimizing diffusion timesteps, outperforming existing methods in subject fidelity and text alignment.",
            "ai_keywords": [
                "Subject Fidelity Optimization (SFO)",
                "comparative learning framework",
                "zero-shot subject-driven generation",
                "diffusion loss",
                "synthetic negative targets",
                "Condition-Degradation Negative Sampling (CDNS)",
                "diffusion timesteps"
            ]
        },
        "publishedAt": "2025-06-04T02:59:25.000Z",
        "title": "Negative-Guided Subject Fidelity Optimization for Zero-Shot\n  Subject-Driven Generation",
        "summary": "We present Subject Fidelity Optimization (SFO), a novel comparative learning\nframework for zero-shot subject-driven generation that enhances subject\nfidelity. Beyond supervised fine-tuning methods that rely only on positive\ntargets and use the diffusion loss as in the pre-training stage, SFO introduces\nsynthetic negative targets and explicitly guides the model to favor positives\nover negatives through pairwise comparison. For negative targets, we propose\nCondition-Degradation Negative Sampling (CDNS), which automatically generates\ndistinctive and informative negatives by intentionally degrading visual and\ntextual cues without expensive human annotations. Moreover, we reweight the\ndiffusion timesteps to focus finetuning on intermediate steps where subject\ndetails emerge. Extensive experiments demonstrate that SFO with CDNS\nsignificantly outperforms baselines in terms of both subject fidelity and text\nalignment on a subject-driven generation benchmark. Project page:\nhttps://subjectfidelityoptimization.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03621.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631842124d24c00131a66656",
            "avatarUrl": "/avatars/590633e163da425bf041e3988d8dd015.svg",
            "fullname": "shin",
            "name": "chaehun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.02528",
            "authors": [
                {
                    "_id": "683fb8bc3bcb592f18f5b866",
                    "name": "Yan Gong",
                    "hidden": false
                },
                {
                    "_id": "683fb8bc3bcb592f18f5b867",
                    "name": "Yiren Song",
                    "hidden": false
                },
                {
                    "_id": "683fb8bc3bcb592f18f5b868",
                    "name": "Yicheng Li",
                    "hidden": false
                },
                {
                    "_id": "683fb8bc3bcb592f18f5b869",
                    "name": "Chenglin Li",
                    "hidden": false
                },
                {
                    "_id": "683fb8bc3bcb592f18f5b86a",
                    "name": "Yin Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T07:06:35.000Z",
            "submittedOnDailyAt": "2025-06-04T01:39:07.675Z",
            "title": "RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers",
            "submittedOnDailyBy": {
                "_id": "64311a95034ecbefddd141ef",
                "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                "isPro": true,
                "fullname": "Yiren Song",
                "user": "yiren98",
                "type": "user"
            },
            "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance.",
            "upvotes": 13,
            "discussionId": "683fb8bd3bcb592f18f5b89f",
            "ai_summary": "RelationAdapter, a lightweight module, enhances Diffusion Transformer models to capture and apply visual transformations effectively using source-target image pairs, improving editing performance and generalization on diverse tasks.",
            "ai_keywords": [
                "RelationAdapter",
                "Diffusion Transformer",
                "DiT",
                "visual prompt-based image editing",
                "content-aware editing intent",
                "Relation252K",
                "visual transformations",
                "editing intent",
                "generation quality",
                "overall editing performance"
            ]
        },
        "publishedAt": "2025-06-03T03:06:35.000Z",
        "title": "RelationAdapter: Learning and Transferring Visual Relation with\n  Diffusion Transformers",
        "summary": "Inspired by the in-context learning mechanism of large language models\n(LLMs), a new paradigm of generalizable visual prompt-based image editing is\nemerging. Existing single-reference methods typically focus on style or\nappearance adjustments and struggle with non-rigid transformations. To address\nthese limitations, we propose leveraging source-target image pairs to extract\nand transfer content-aware editing intent to novel query images. To this end,\nwe introduce RelationAdapter, a lightweight module that enables Diffusion\nTransformer (DiT) based models to effectively capture and apply visual\ntransformations from minimal examples. We also introduce Relation252K, a\ncomprehensive dataset comprising 218 diverse editing tasks, to evaluate model\ngeneralization and adaptability in visual prompt-driven scenarios. Experiments\non Relation252K show that RelationAdapter significantly improves the model's\nability to understand and transfer editing intent, leading to notable gains in\ngeneration quality and overall editing performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02528.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64311a95034ecbefddd141ef",
            "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
            "fullname": "Yiren Song",
            "name": "yiren98",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.01144",
            "authors": [
                {
                    "_id": "683fca92c1e51fea3a470e93",
                    "name": "Ariel Shaulov",
                    "hidden": false
                },
                {
                    "_id": "683fca92c1e51fea3a470e94",
                    "user": {
                        "_id": "64972b8d27e41e26a32835d4",
                        "avatarUrl": "/avatars/00c76991b5421f592d632a750ec8b998.svg",
                        "isPro": false,
                        "fullname": "Itay Hazan",
                        "user": "itayhzn",
                        "type": "user"
                    },
                    "name": "Itay Hazan",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-04T06:31:03.556Z",
                    "hidden": false
                },
                {
                    "_id": "683fca92c1e51fea3a470e95",
                    "name": "Lior Wolf",
                    "hidden": false
                },
                {
                    "_id": "683fca92c1e51fea3a470e96",
                    "user": {
                        "_id": "6181c72cdcc1df2c9de8a4d8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
                        "isPro": false,
                        "fullname": "Hila Chefer",
                        "user": "Hila",
                        "type": "user"
                    },
                    "name": "Hila Chefer",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:36.782Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
            ],
            "publishedAt": "2025-06-01T19:55:33.000Z",
            "submittedOnDailyAt": "2025-06-04T02:58:51.483Z",
            "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6181c72cdcc1df2c9de8a4d8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
                "isPro": false,
                "fullname": "Hila Chefer",
                "user": "Hila",
                "type": "user"
            },
            "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.",
            "upvotes": 12,
            "discussionId": "683fca94c1e51fea3a470eee",
            "projectPage": "https://arielshaulov.github.io/FlowMo/",
            "githubRepo": "https://github.com/arielshaulov/FlowMo",
            "ai_summary": "FlowMo, a training-free method, enhances motion coherence in pre-trained text-to-video diffusion models by leveraging their own predictions to reduce patch-wise temporal variance.",
            "ai_keywords": [
                "text-to-video diffusion models",
                "temporal aspects",
                "motion",
                "physics",
                "dynamic interactions",
                "pre-trained model",
                "FlowMo",
                "guidance method",
                "appearance-debiased",
                "temporal representation",
                "latents",
                "patch-wise variance",
                "sampling",
                "temporal fidelity"
            ]
        },
        "publishedAt": "2025-06-01T15:55:33.000Z",
        "title": "FlowMo: Variance-Based Flow Guidance for Coherent Motion in Video\n  Generation",
        "summary": "Text-to-video diffusion models are notoriously limited in their ability to\nmodel temporal aspects such as motion, physics, and dynamic interactions.\nExisting approaches address this limitation by retraining the model or\nintroducing external conditioning signals to enforce temporal consistency. In\nthis work, we explore whether a meaningful temporal representation can be\nextracted directly from the predictions of a pre-trained model without any\nadditional training or auxiliary inputs. We introduce FlowMo, a novel\ntraining-free guidance method that enhances motion coherence using only the\nmodel's own predictions in each diffusion step. FlowMo first derives an\nappearance-debiased temporal representation by measuring the distance between\nlatents corresponding to consecutive frames. This highlights the implicit\ntemporal structure predicted by the model. It then estimates motion coherence\nby measuring the patch-wise variance across the temporal dimension and guides\nthe model to reduce this variance dynamically during sampling. Extensive\nexperiments across multiple text-to-video models demonstrate that FlowMo\nsignificantly improves motion coherence without sacrificing visual quality or\nprompt alignment, offering an effective plug-and-play solution for enhancing\nthe temporal fidelity of pre-trained video diffusion models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6181c72cdcc1df2c9de8a4d8/z3VoHZmxOtL3agHCaWWg4.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01144.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6181c72cdcc1df2c9de8a4d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655248010394-6181c72cdcc1df2c9de8a4d8.jpeg",
            "fullname": "Hila Chefer",
            "name": "Hila",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00910",
            "authors": [
                {
                    "_id": "683fcd9956d0f1cfb34f1d51",
                    "name": "Seongjae Kang",
                    "hidden": false
                },
                {
                    "_id": "683fcd9956d0f1cfb34f1d52",
                    "name": "Dong Bok Lee",
                    "hidden": false
                },
                {
                    "_id": "683fcd9956d0f1cfb34f1d53",
                    "name": "Hyungjoon Jang",
                    "hidden": false
                },
                {
                    "_id": "683fcd9956d0f1cfb34f1d54",
                    "name": "Dongseop Kim",
                    "hidden": false
                },
                {
                    "_id": "683fcd9956d0f1cfb34f1d55",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-01T08:54:37.000Z",
            "submittedOnDailyAt": "2025-06-04T03:10:05.743Z",
            "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "6357a08f8ed056fa1ccd3b38",
                "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
                "isPro": false,
                "fullname": "erjui",
                "user": "erjui",
                "type": "user"
            },
            "summary": "Knowledge distillation (KD) is a widely used framework for training compact,\ntask-specific models by leveraging the knowledge of teacher models. However,\nits application to active learning (AL), which aims to minimize annotation\ncosts through iterative sample selection, remains underexplored. This gap stems\nfrom the fact that KD typically assumes access to sufficient labeled data,\nwhereas AL operates in data-scarce scenarios where task-specific teacher models\nare often unavailable. In this paper, we introduce ActiveKD, a framework that\nintegrates AL with KD by leveraging the zero- and few-shot capabilities of\nlarge vision-language models (VLMs). A key aspect of ActiveKD is the structured\nprediction bias of VLMs -- i.e., their predictions form clusters in the\nprobability space. We regard this structure as an inductive bias of the teacher\nmodel, capturing generalizable output patterns beneficial to student learning.\nTo exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection\nstrategy that maximizes coverage in the probability space rather than the\nfeature space. PCoreSet strategically selects categorically diverse unlabeled\nsamples, facilitating more efficient transfer of teacher knowledge under\nlimited annotation budgets. Evaluations on 11 datasets show that PCoreSet\nconsistently outperforms existing selection methods within the ActiveKD\nframework, advancing research at the intersection of AL and KD.",
            "upvotes": 10,
            "discussionId": "683fcd9d56d0f1cfb34f1eb1",
            "githubRepo": "https://github.com/erjui/PCoreSet",
            "ai_summary": "ActiveKD integrates active learning with knowledge distillation using large vision-language models to efficiently select diverse, unlabeled samples for annotation.",
            "ai_keywords": [
                "knowledge distillation",
                "active learning",
                "task-specific models",
                "teacher models",
                "zero-shot",
                "few-shot",
                "large vision-language models",
                "structured prediction bias",
                "inductive bias",
                "Probabilistic CoreSet",
                "PCoreSet",
                "probability space",
                "categorically diverse samples"
            ]
        },
        "publishedAt": "2025-06-01T04:54:37.000Z",
        "title": "PCoreSet: Effective Active Learning through Knowledge Distillation from\n  Vision-Language Models",
        "summary": "Knowledge distillation (KD) is a widely used framework for training compact,\ntask-specific models by leveraging the knowledge of teacher models. However,\nits application to active learning (AL), which aims to minimize annotation\ncosts through iterative sample selection, remains underexplored. This gap stems\nfrom the fact that KD typically assumes access to sufficient labeled data,\nwhereas AL operates in data-scarce scenarios where task-specific teacher models\nare often unavailable. In this paper, we introduce ActiveKD, a framework that\nintegrates AL with KD by leveraging the zero- and few-shot capabilities of\nlarge vision-language models (VLMs). A key aspect of ActiveKD is the structured\nprediction bias of VLMs -- i.e., their predictions form clusters in the\nprobability space. We regard this structure as an inductive bias of the teacher\nmodel, capturing generalizable output patterns beneficial to student learning.\nTo exploit this bias, we propose Probabilistic CoreSet (PCoreSet), a selection\nstrategy that maximizes coverage in the probability space rather than the\nfeature space. PCoreSet strategically selects categorically diverse unlabeled\nsamples, facilitating more efficient transfer of teacher knowledge under\nlimited annotation budgets. Evaluations on 11 datasets show that PCoreSet\nconsistently outperforms existing selection methods within the ActiveKD\nframework, advancing research at the intersection of AL and KD.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00910.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6357a08f8ed056fa1ccd3b38",
            "avatarUrl": "/avatars/07d4ca8f3197a6945ad71e6150801135.svg",
            "fullname": "erjui",
            "name": "erjui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03123",
            "authors": [
                {
                    "_id": "684050b04e6c9ec4722c2154",
                    "name": "Zhengyao Lv",
                    "hidden": false
                },
                {
                    "_id": "684050b04e6c9ec4722c2155",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "684050b04e6c9ec4722c2156",
                    "name": "Tianlin Pan",
                    "hidden": false
                },
                {
                    "_id": "684050b04e6c9ec4722c2157",
                    "name": "Zhaoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "684050b04e6c9ec4722c2158",
                    "name": "Kwan-Yee K. Wong",
                    "hidden": false
                },
                {
                    "_id": "684050b04e6c9ec4722c2159",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "684050b04e6c9ec4722c215a",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:55:04.000Z",
            "submittedOnDailyAt": "2025-06-04T12:30:30.335Z",
            "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "635f8ed47c05eb9f59963d3a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
                "isPro": false,
                "fullname": "ChenyangSi",
                "user": "ChenyangSi",
                "type": "user"
            },
            "summary": "Diffusion Models have achieved remarkable results in video synthesis but\nrequire iterative denoising steps, leading to substantial computational\noverhead. Consistency Models have made significant progress in accelerating\ndiffusion models. However, directly applying them to video diffusion models\noften results in severe degradation of temporal consistency and appearance\ndetails. In this paper, by analyzing the training dynamics of Consistency\nModels, we identify a key conflicting learning dynamics during the distillation\nprocess: there is a significant discrepancy in the optimization gradients and\nloss contributions across different timesteps. This discrepancy prevents the\ndistilled student model from achieving an optimal state, leading to compromised\ntemporal consistency and degraded appearance details. To address this issue, we\npropose a parameter-efficient Dual-Expert Consistency Model~(DCM),\nwhere a semantic expert focuses on learning semantic layout and motion, while a\ndetail expert specializes in fine detail refinement. Furthermore, we introduce\nTemporal Coherence Loss to improve motion consistency for the semantic expert\nand apply GAN and Feature Matching Loss to enhance the synthesis quality of the\ndetail expert.Our approach achieves state-of-the-art visual quality with\nsignificantly reduced sampling steps, demonstrating the effectiveness of expert\nspecialization in video diffusion model distillation. Our code and models are\navailable at\nhttps://github.com/Vchitect/DCM{https://github.com/Vchitect/DCM}.",
            "upvotes": 9,
            "discussionId": "684050b24e6c9ec4722c220d",
            "projectPage": "https://vchitect.github.io/DCM/",
            "githubRepo": "https://github.com/Vchitect/DCM",
            "ai_summary": "The Dual-Expert Consistency Model improves video diffusion model distillation by addressing learning dynamics issues, achieving state-of-the-art quality with reduced sampling steps.",
            "ai_keywords": [
                "Diffusion Models",
                "Consistency Models",
                "training dynamics",
                "dual-expert",
                "semantic expert",
                "detail expert",
                "Temporal Coherence Loss",
                "GAN",
                "Feature Matching Loss"
            ]
        },
        "publishedAt": "2025-06-03T13:55:04.000Z",
        "title": "DCM: Dual-Expert Consistency Model for Efficient and High-Quality Video\n  Generation",
        "summary": "Diffusion Models have achieved remarkable results in video synthesis but\nrequire iterative denoising steps, leading to substantial computational\noverhead. Consistency Models have made significant progress in accelerating\ndiffusion models. However, directly applying them to video diffusion models\noften results in severe degradation of temporal consistency and appearance\ndetails. In this paper, by analyzing the training dynamics of Consistency\nModels, we identify a key conflicting learning dynamics during the distillation\nprocess: there is a significant discrepancy in the optimization gradients and\nloss contributions across different timesteps. This discrepancy prevents the\ndistilled student model from achieving an optimal state, leading to compromised\ntemporal consistency and degraded appearance details. To address this issue, we\npropose a parameter-efficient Dual-Expert Consistency Model~(DCM),\nwhere a semantic expert focuses on learning semantic layout and motion, while a\ndetail expert specializes in fine detail refinement. Furthermore, we introduce\nTemporal Coherence Loss to improve motion consistency for the semantic expert\nand apply GAN and Feature Matching Loss to enhance the synthesis quality of the\ndetail expert.Our approach achieves state-of-the-art visual quality with\nsignificantly reduced sampling steps, demonstrating the effectiveness of expert\nspecialization in video diffusion model distillation. Our code and models are\navailable at\nhttps://github.com/Vchitect/DCM{https://github.com/Vchitect/DCM}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03123.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635f8ed47c05eb9f59963d3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f8ed47c05eb9f59963d3a/uQf4p9N9pSaFy87Wg9v4k.jpeg",
            "fullname": "ChenyangSi",
            "name": "ChenyangSi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.01789",
            "authors": [
                {
                    "_id": "683fb99c7c8d720be438000a",
                    "name": "Genta Indra Winata",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be438000b",
                    "name": "David Anugraha",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be438000c",
                    "name": "Emmy Liu",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be438000d",
                    "name": "Alham Fikri Aji",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be438000e",
                    "name": "Shou-Yi Hung",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be438000f",
                    "name": "Aditya Parashar",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380010",
                    "user": {
                        "_id": "64d1e3a87e20ec9ea0020d03",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d1e3a87e20ec9ea0020d03/xm-afh1AaqS0e9qpaPo7y.jpeg",
                        "isPro": false,
                        "fullname": "Patrick Amadeus Irawan",
                        "user": "patrickamadeus",
                        "type": "user"
                    },
                    "name": "Patrick Amadeus Irawan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:55:30.937Z",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380011",
                    "name": "Ruochen Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380012",
                    "name": "Zheng-Xin Yong",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380013",
                    "name": "Jan Christian Blaise Cruz",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380014",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380015",
                    "user": {
                        "_id": "6469949654873f0043b09c22",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6469949654873f0043b09c22/Lk7IJAR16Wa_sGJ2g81AQ.jpeg",
                        "isPro": false,
                        "fullname": "Seungone Kim",
                        "user": "seungone",
                        "type": "user"
                    },
                    "name": "Seungone Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:55:22.158Z",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380016",
                    "name": "Hanyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380017",
                    "user": {
                        "_id": "63139ff6b46fc4e24332fa84",
                        "avatarUrl": "/avatars/ee6923a7cb218f22535064a87761e497.svg",
                        "isPro": false,
                        "fullname": "Sudipta Kar",
                        "user": "cryptexcode",
                        "type": "user"
                    },
                    "name": "Sudipta Kar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:55:28.286Z",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380018",
                    "name": "Kezia Erina Suryoraharjo",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be4380019",
                    "name": "M. Farid Adilazuarda",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be438001a",
                    "name": "En-Shiun Annie Lee",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be438001b",
                    "name": "Ayu Purwarianti",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be438001c",
                    "name": "Derry Tanti Wijaya",
                    "hidden": false
                },
                {
                    "_id": "683fb99c7c8d720be438001d",
                    "name": "Monojit Choudhury",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T15:31:52.000Z",
            "submittedOnDailyAt": "2025-06-04T01:42:44.251Z",
            "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability",
            "submittedOnDailyBy": {
                "_id": "5f5c4b20e56d546cd6233098",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
                "isPro": false,
                "fullname": "Genta Indra Winata",
                "user": "gentaiscool",
                "type": "user"
            },
            "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics.",
            "upvotes": 9,
            "discussionId": "683fb99e7c8d720be4380090"
        },
        "publishedAt": "2025-06-02T11:31:52.000Z",
        "title": "Datasheets Aren't Enough: DataRubrics for Automated Quality Metrics and\n  Accountability",
        "summary": "High-quality datasets are fundamental to training and evaluating machine\nlearning models, yet their creation-especially with accurate human\nannotations-remains a significant challenge. Many dataset paper submissions\nlack originality, diversity, or rigorous quality control, and these\nshortcomings are often overlooked during peer review. Submissions also\nfrequently omit essential details about dataset construction and properties.\nWhile existing tools such as datasheets aim to promote transparency, they are\nlargely descriptive and do not provide standardized, measurable methods for\nevaluating data quality. Similarly, metadata requirements at conferences\npromote accountability but are inconsistently enforced. To address these\nlimitations, this position paper advocates for the integration of systematic,\nrubric-based evaluation metrics into the dataset review process-particularly as\nsubmission volumes continue to grow. We also explore scalable, cost-effective\nmethods for synthetic data generation, including dedicated tools and\nLLM-as-a-judge approaches, to support more efficient evaluation. As a call to\naction, we introduce DataRubrics, a structured framework for assessing the\nquality of both human- and model-generated datasets. Leveraging recent advances\nin LLM-based evaluation, DataRubrics offers a reproducible, scalable, and\nactionable solution for dataset quality assessment, enabling both authors and\nreviewers to uphold higher standards in data-centric research. We also release\ncode to support reproducibility of LLM-based evaluations at\nhttps://github.com/datarubrics/datarubrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01789.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f5c4b20e56d546cd6233098",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1637813888895-5f5c4b20e56d546cd6233098.jpeg",
            "fullname": "Genta Indra Winata",
            "name": "gentaiscool",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.22704",
            "authors": [
                {
                    "_id": "68409fbfeb249b555b0e4c60",
                    "name": "Feng Yao",
                    "hidden": false
                },
                {
                    "_id": "68409fbfeb249b555b0e4c61",
                    "name": "Zilong Wang",
                    "hidden": false
                },
                {
                    "_id": "68409fbfeb249b555b0e4c62",
                    "name": "Liyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68409fbfeb249b555b0e4c63",
                    "name": "Junxia Cui",
                    "hidden": false
                },
                {
                    "_id": "68409fbfeb249b555b0e4c64",
                    "name": "Li Zhong",
                    "hidden": false
                },
                {
                    "_id": "68409fbfeb249b555b0e4c65",
                    "name": "Xiaohan Fu",
                    "hidden": false
                },
                {
                    "_id": "68409fbfeb249b555b0e4c66",
                    "name": "Haohui Mai",
                    "hidden": false
                },
                {
                    "_id": "68409fbfeb249b555b0e4c67",
                    "name": "Vish Krishnan",
                    "hidden": false
                },
                {
                    "_id": "68409fbfeb249b555b0e4c68",
                    "name": "Jianfeng Gao",
                    "hidden": false
                },
                {
                    "_id": "68409fbfeb249b555b0e4c69",
                    "name": "Jingbo Shang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64c8b2c5c547ed5243e14a6e/XzhzsqQzLmV5PLpuXw9xo.png"
            ],
            "publishedAt": "2025-05-28T17:57:47.000Z",
            "submittedOnDailyAt": "2025-06-04T18:09:29.884Z",
            "title": "Training Language Models to Generate Quality Code with Program Analysis\n  Feedback",
            "submittedOnDailyBy": {
                "_id": "64c8b2c5c547ed5243e14a6e",
                "avatarUrl": "/avatars/96d4a9010f96001c8cff235915926390.svg",
                "isPro": false,
                "fullname": "Feng Yao",
                "user": "fengyao1909",
                "type": "user"
            },
            "summary": "Code generation with large language models (LLMs), often termed vibe coding,\nis increasingly adopted in production but fails to ensure code quality,\nparticularly in security (e.g., SQL injection vulnerabilities) and\nmaintainability (e.g., missing type annotations). Existing methods, such as\nsupervised fine-tuning and rule-based post-processing, rely on labor-intensive\nannotations or brittle heuristics, limiting their scalability and\neffectiveness. We propose REAL, a reinforcement learning framework that\nincentivizes LLMs to generate production-quality code using program\nanalysis-guided feedback. Specifically, REAL integrates two automated signals:\n(1) program analysis detecting security or maintainability defects and (2) unit\ntests ensuring functional correctness. Unlike prior work, our framework is\nprompt-agnostic and reference-free, enabling scalable supervision without\nmanual intervention. Experiments across multiple datasets and model scales\ndemonstrate that REAL outperforms state-of-the-art methods in simultaneous\nassessments of functionality and code quality. Our work bridges the gap between\nrapid prototyping and production-ready code, enabling LLMs to deliver both\nspeed and quality.",
            "upvotes": 8,
            "discussionId": "68409fc0eb249b555b0e4c99",
            "ai_summary": "A reinforcement learning framework improves code quality in large language models by using automated feedback from program analysis and unit tests.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "code generation",
                "program analysis",
                "unit tests",
                "prompt-agnostic",
                "reference-free"
            ]
        },
        "publishedAt": "2025-05-28T13:57:47.000Z",
        "title": "Training Language Models to Generate Quality Code with Program Analysis\n  Feedback",
        "summary": "Code generation with large language models (LLMs), often termed vibe coding,\nis increasingly adopted in production but fails to ensure code quality,\nparticularly in security (e.g., SQL injection vulnerabilities) and\nmaintainability (e.g., missing type annotations). Existing methods, such as\nsupervised fine-tuning and rule-based post-processing, rely on labor-intensive\nannotations or brittle heuristics, limiting their scalability and\neffectiveness. We propose REAL, a reinforcement learning framework that\nincentivizes LLMs to generate production-quality code using program\nanalysis-guided feedback. Specifically, REAL integrates two automated signals:\n(1) program analysis detecting security or maintainability defects and (2) unit\ntests ensuring functional correctness. Unlike prior work, our framework is\nprompt-agnostic and reference-free, enabling scalable supervision without\nmanual intervention. Experiments across multiple datasets and model scales\ndemonstrate that REAL outperforms state-of-the-art methods in simultaneous\nassessments of functionality and code quality. Our work bridges the gap between\nrapid prototyping and production-ready code, enabling LLMs to deliver both\nspeed and quality.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64c8b2c5c547ed5243e14a6e/XzhzsqQzLmV5PLpuXw9xo.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.22704.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64c8b2c5c547ed5243e14a6e",
            "avatarUrl": "/avatars/96d4a9010f96001c8cff235915926390.svg",
            "fullname": "Feng Yao",
            "name": "fengyao1909",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.01716",
            "authors": [
                {
                    "_id": "6840541af682212cae6d3548",
                    "name": "Yifei Zhou",
                    "hidden": false
                },
                {
                    "_id": "6840541af682212cae6d3549",
                    "name": "Sergey Levine",
                    "hidden": false
                },
                {
                    "_id": "6840541af682212cae6d354a",
                    "name": "Jason Weston",
                    "hidden": false
                },
                {
                    "_id": "6840541af682212cae6d354b",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "6840541af682212cae6d354c",
                    "name": "Sainbayar Sukhbaatar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T14:23:33.000Z",
            "submittedOnDailyAt": "2025-06-04T12:42:20.422Z",
            "title": "Self-Challenging Language Model Agents",
            "submittedOnDailyBy": {
                "_id": "62f023a36a027498eaa2f9cc",
                "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                "isPro": false,
                "fullname": "Jason Weston",
                "user": "spermwhale",
                "type": "user"
            },
            "summary": "Large language models are quickly becoming the foundation for intelligent\nagents that are capable of using tools. However, training such agents is\nchallenging because it requires human creation and annotation of a diverse set\nof tasks, tools, and evaluation criteria. In this paper, we propose the\nSelf-Challenging framework for training an agent on high-quality tasks that are\ngenerated by itself. The agent first plays the role of challenger and generates\na task after interacting with the given tools. The tasks take the form of a\nnovel general class of problems termed Code-as-Task, which are defined by an\ninstruction, a verification function and solution and failure cases which serve\nas tests, allowing to filter only for high-quality tasks. The agent then takes\nan executor role and trains on those tasks with reinforcement learning using\nthe evaluation feedback as a reward. Evaluation on two existing multi-turn\ntool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging\nframework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,\ndespite using only self-generated training data.",
            "upvotes": 5,
            "discussionId": "6840541af682212cae6d3581",
            "ai_summary": "The Self-Challenging framework trains intelligent agents using self-generated tasks defined as Code-as-Task, improving performance in multi-turn tool-use benchmarks.",
            "ai_keywords": [
                "Self-Challenging framework",
                "Code-as-Task",
                "reinforcement learning",
                "M3ToolEval",
                "TauBench",
                "Llama-3.1-8B-Instruct"
            ]
        },
        "publishedAt": "2025-06-02T10:23:33.000Z",
        "title": "Self-Challenging Language Model Agents",
        "summary": "Large language models are quickly becoming the foundation for intelligent\nagents that are capable of using tools. However, training such agents is\nchallenging because it requires human creation and annotation of a diverse set\nof tasks, tools, and evaluation criteria. In this paper, we propose the\nSelf-Challenging framework for training an agent on high-quality tasks that are\ngenerated by itself. The agent first plays the role of challenger and generates\na task after interacting with the given tools. The tasks take the form of a\nnovel general class of problems termed Code-as-Task, which are defined by an\ninstruction, a verification function and solution and failure cases which serve\nas tests, allowing to filter only for high-quality tasks. The agent then takes\nan executor role and trains on those tasks with reinforcement learning using\nthe evaluation feedback as a reward. Evaluation on two existing multi-turn\ntool-use agent benchmarks, M3ToolEval and TauBench, shows the Self-Challenging\nframework achieves over a two-fold improvement in Llama-3.1-8B-Instruct,\ndespite using only self-generated training data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01716.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f023a36a027498eaa2f9cc",
            "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
            "fullname": "Jason Weston",
            "name": "spermwhale",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.00413",
            "authors": [
                {
                    "_id": "683e703c33ac56778c2e51cd",
                    "user": {
                        "_id": "630139f1f6bea7dd15bdaf4e",
                        "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
                        "isPro": false,
                        "fullname": "Daniel Israel",
                        "user": "danielmisrael",
                        "type": "user"
                    },
                    "name": "Daniel Israel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-03T08:45:22.988Z",
                    "hidden": false
                },
                {
                    "_id": "683e703c33ac56778c2e51ce",
                    "name": "Guy Van den Broeck",
                    "hidden": false
                },
                {
                    "_id": "683e703c33ac56778c2e51cf",
                    "name": "Aditya Grover",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T06:10:10.000Z",
            "submittedOnDailyAt": "2025-06-04T03:04:01.663Z",
            "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
            "submittedOnDailyBy": {
                "_id": "630139f1f6bea7dd15bdaf4e",
                "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
                "isPro": false,
                "fullname": "Daniel Israel",
                "user": "danielmisrael",
                "type": "user"
            },
            "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
            "upvotes": 5,
            "discussionId": "683e703d33ac56778c2e51fe",
            "ai_summary": "Adaptive parallel decoding (APD) enhances the throughput of diffusion large language models (dLLMs) by dynamically adjusting parallel token generation without significantly diminishing quality.",
            "ai_keywords": [
                "autoregressive decoding",
                "diffusion large language models",
                "dLLMs",
                "adaptive parallel decoding",
                "APD",
                "marginal probabilities",
                "joint probability",
                "speculative decoding",
                "KV caching",
                "masked input"
            ]
        },
        "publishedAt": "2025-05-31T02:10:10.000Z",
        "title": "Accelerating Diffusion LLMs via Adaptive Parallel Decoding",
        "summary": "The generation speed of LLMs are bottlenecked by autoregressive decoding,\nwhere tokens are predicted sequentially one by one. Alternatively, diffusion\nlarge language models (dLLMs) theoretically allow for parallel token\ngeneration, but in practice struggle to achieve the speed of autoregressive\nmodels without significantly sacrificing quality. We therefore introduce\nadaptive parallel decoding (APD), a novel method that dynamically adjusts the\nnumber of tokens sampled in parallel. We achieve this by defining a\nmultiplicative mixture between the dLLM marginal probabilities and the joint\nprobability of sequences under a small auxiliary autoregressive model. This\ninverts the standard setup of speculative decoding, where the goal is to sample\nfrom a large autoregressive verifier by drafting from a smaller model. We\nfurther optimize APD by enabling KV caching and limiting the size of the masked\ninput. Altogether, our method puts forward three tunable parameters to flexibly\ntradeoff throughput and quality. We show that APD provides markedly higher\nthroughput with minimal quality degradations on downstream benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00413.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630139f1f6bea7dd15bdaf4e",
            "avatarUrl": "/avatars/263536f6160f8c522d2a76ca2c4e4cc0.svg",
            "fullname": "Daniel Israel",
            "name": "danielmisrael",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03079",
            "authors": [
                {
                    "_id": "683fee0a179d710da07d4352",
                    "user": {
                        "_id": "634aab35dcf125e4dafc87b1",
                        "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
                        "isPro": false,
                        "fullname": "YangXiuyu",
                        "user": "gzzyyxy",
                        "type": "user"
                    },
                    "name": "Xiuyu Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:13.216Z",
                    "hidden": true
                },
                {
                    "_id": "683fee0a179d710da07d4353",
                    "name": "Bohan Li",
                    "hidden": false
                },
                {
                    "_id": "683fee0a179d710da07d4354",
                    "name": "Shaocong Xu",
                    "hidden": false
                },
                {
                    "_id": "683fee0a179d710da07d4355",
                    "name": "Nan Wang",
                    "hidden": false
                },
                {
                    "_id": "683fee0a179d710da07d4356",
                    "name": "Chongjie Ye",
                    "hidden": false
                },
                {
                    "_id": "683fee0a179d710da07d4357",
                    "name": "Zhaoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "683fee0a179d710da07d4358",
                    "name": "Minghan Qin",
                    "hidden": false
                },
                {
                    "_id": "683fee0a179d710da07d4359",
                    "name": "Yikang Ding",
                    "hidden": false
                },
                {
                    "_id": "683fee0a179d710da07d435a",
                    "name": "Xin Jin",
                    "hidden": false
                },
                {
                    "_id": "683fee0a179d710da07d435b",
                    "name": "Hang Zhao",
                    "hidden": false
                },
                {
                    "_id": "683fee0a179d710da07d435c",
                    "name": "Hao Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:00:32.000Z",
            "submittedOnDailyAt": "2025-06-04T05:27:07.241Z",
            "title": "ORV: 4D Occupancy-centric Robot Video Generation",
            "submittedOnDailyBy": {
                "_id": "634aab35dcf125e4dafc87b1",
                "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
                "isPro": false,
                "fullname": "YangXiuyu",
                "user": "gzzyyxy",
                "type": "user"
            },
            "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV",
            "upvotes": 4,
            "discussionId": "683fee12179d710da07d45f4",
            "projectPage": "https://orangesodahub.github.io/ORV/",
            "githubRepo": "https://github.com/OrangeSodahub/ORV",
            "ai_summary": "ORV, an Occupancy-centric Robot Video generation framework, uses 4D semantic occupancy sequences to produce photorealistic, temporally consistent, and precisely controllable robot videos, enhancing existing methods.",
            "ai_keywords": [
                "action-driven generative models",
                "occupancy-centric",
                "4D semantic occupancy sequences",
                "video generation",
                "photorealistic robot videos",
                "temporal consistency",
                "precise controllability",
                "multi-view videos"
            ]
        },
        "publishedAt": "2025-06-03T13:00:32.000Z",
        "title": "ORV: 4D Occupancy-centric Robot Video Generation",
        "summary": "Acquiring real-world robotic simulation data through teleoperation is\nnotoriously time-consuming and labor-intensive. Recently, action-driven\ngenerative models have gained widespread adoption in robot learning and\nsimulation, as they eliminate safety concerns and reduce maintenance efforts.\nHowever, the action sequences used in these methods often result in limited\ncontrol precision and poor generalization due to their globally coarse\nalignment. To address these limitations, we propose ORV, an Occupancy-centric\nRobot Video generation framework, which utilizes 4D semantic occupancy\nsequences as a fine-grained representation to provide more accurate semantic\nand geometric guidance for video generation. By leveraging occupancy-based\nrepresentations, ORV enables seamless translation of simulation data into\nphotorealistic robot videos, while ensuring high temporal consistency and\nprecise controllability. Furthermore, our framework supports the simultaneous\ngeneration of multi-view videos of robot gripping operations - an important\ncapability for downstream robotic learning tasks. Extensive experimental\nresults demonstrate that ORV consistently outperforms existing baseline methods\nacross various datasets and sub-tasks. Demo, Code and Model:\nhttps://orangesodahub.github.io/ORV",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03079.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634aab35dcf125e4dafc87b1",
            "avatarUrl": "/avatars/aa2db84fd423e9eefe3ef3167c9d3999.svg",
            "fullname": "YangXiuyu",
            "name": "gzzyyxy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01274",
            "authors": [
                {
                    "_id": "683fe4caf8916dcd6d1c936a",
                    "user": {
                        "_id": "673060959e631f353ae1b5e0",
                        "avatarUrl": "/avatars/d4b1e23de90ff1d02c38186a259b8d1e.svg",
                        "isPro": false,
                        "fullname": "Hosu Lee",
                        "user": "lakelee",
                        "type": "user"
                    },
                    "name": "Hosu Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:26.062Z",
                    "hidden": false
                },
                {
                    "_id": "683fe4caf8916dcd6d1c936b",
                    "user": {
                        "_id": "653238bed0f5a9e537ed966d",
                        "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
                        "isPro": false,
                        "fullname": "Junho Kim",
                        "user": "arkimjh",
                        "type": "user"
                    },
                    "name": "Junho Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:30.239Z",
                    "hidden": false
                },
                {
                    "_id": "683fe4caf8916dcd6d1c936c",
                    "name": "Hyunjun Kim",
                    "hidden": false
                },
                {
                    "_id": "683fe4caf8916dcd6d1c936d",
                    "name": "Yong Man Ro",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T03:08:07.000Z",
            "submittedOnDailyAt": "2025-06-04T04:50:22.651Z",
            "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "653238bed0f5a9e537ed966d",
                "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
                "isPro": false,
                "fullname": "Junho Kim",
                "user": "arkimjh",
                "type": "user"
            },
            "summary": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective\nvision-language reasoning, yet the ability to understand video content remains\nconstrained by suboptimal frame selection strategies. Existing approaches often\nrely on static heuristics or external retrieval modules to feed frame\ninformation into video-LLMs, which may fail to provide the query-relevant\ninformation. In this work, we introduce ReFoCUS (Reinforcement-guided Frame\nOptimization for Contextual UnderStanding), a novel frame-level policy\noptimization framework that shifts the optimization target from textual\nresponses to visual input selection. ReFoCUS learns a frame selection policy\nvia reinforcement learning, using reward signals derived from a reference LMM\nto reflect the model's intrinsic preferences for frames that best support\ntemporally grounded responses. To efficiently explore the large combinatorial\nframe space, we employ an autoregressive, conditional selection architecture\nthat ensures temporal coherence while reducing complexity. Our approach does\nnot require explicit supervision at the frame-level and consistently improves\nreasoning performance across multiple video QA benchmarks, highlighting the\nbenefits of aligning frame selection with model-internal utility.",
            "upvotes": 4,
            "discussionId": "683fe4ccf8916dcd6d1c93b6",
            "ai_summary": "ReFoCUS uses reinforcement learning to optimize frame selection for video-LLM, enhancing reasoning performance in video QA by aligning with model preferences.",
            "ai_keywords": [
                "Large Multi-modal Models",
                "vision-language reasoning",
                "frame selection strategies",
                "reinforcement learning",
                "frame selection policy",
                "reference LMM",
                "autoregressive architecture",
                "conditional selection architecture",
                "temporal coherence",
                "video QA benchmarks"
            ]
        },
        "publishedAt": "2025-06-01T23:08:07.000Z",
        "title": "ReFoCUS: Reinforcement-guided Frame Optimization for Contextual\n  Understanding",
        "summary": "Recent progress in Large Multi-modal Models (LMMs) has enabled effective\nvision-language reasoning, yet the ability to understand video content remains\nconstrained by suboptimal frame selection strategies. Existing approaches often\nrely on static heuristics or external retrieval modules to feed frame\ninformation into video-LLMs, which may fail to provide the query-relevant\ninformation. In this work, we introduce ReFoCUS (Reinforcement-guided Frame\nOptimization for Contextual UnderStanding), a novel frame-level policy\noptimization framework that shifts the optimization target from textual\nresponses to visual input selection. ReFoCUS learns a frame selection policy\nvia reinforcement learning, using reward signals derived from a reference LMM\nto reflect the model's intrinsic preferences for frames that best support\ntemporally grounded responses. To efficiently explore the large combinatorial\nframe space, we employ an autoregressive, conditional selection architecture\nthat ensures temporal coherence while reducing complexity. Our approach does\nnot require explicit supervision at the frame-level and consistently improves\nreasoning performance across multiple video QA benchmarks, highlighting the\nbenefits of aligning frame selection with model-internal utility.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01274.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653238bed0f5a9e537ed966d",
            "avatarUrl": "/avatars/e97a83e68e770baa1c5df847777cf213.svg",
            "fullname": "Junho Kim",
            "name": "arkimjh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03096",
            "authors": [
                {
                    "_id": "684002718bd5bff9918ce018",
                    "user": {
                        "_id": "6310a6bb0a43f97f6c5567d3",
                        "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
                        "isPro": false,
                        "fullname": "Christian Schlarmann",
                        "user": "chs20",
                        "type": "user"
                    },
                    "name": "Christian Schlarmann",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:53:34.919Z",
                    "hidden": false
                },
                {
                    "_id": "684002718bd5bff9918ce019",
                    "name": "Francesco Croce",
                    "hidden": false
                },
                {
                    "_id": "684002718bd5bff9918ce01a",
                    "name": "Nicolas Flammarion",
                    "hidden": false
                },
                {
                    "_id": "684002718bd5bff9918ce01b",
                    "name": "Matthias Hein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:27:12.000Z",
            "submittedOnDailyAt": "2025-06-04T08:12:15.755Z",
            "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens",
            "submittedOnDailyBy": {
                "_id": "6310a6bb0a43f97f6c5567d3",
                "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
                "isPro": false,
                "fullname": "Christian Schlarmann",
                "user": "chs20",
                "type": "user"
            },
            "summary": "Contrastive language-image pre-training aligns the features of text-image\npairs in a common latent space via distinct encoders for each modality. While\nthis approach achieves impressive performance in several zero-shot tasks, it\ncannot natively handle multimodal inputs, i.e., encoding image and text into a\nsingle feature vector. As a remedy, it is common practice to use additional\nmodules to merge the features extracted by the unimodal encoders. In this work,\nwe present FuseLIP, an alternative architecture for multimodal embedding.\nLeveraging recent progress in discrete image tokenizers, we propose to use a\nsingle transformer model which operates on an extended vocabulary of text and\nimage tokens. This early fusion approach allows the different modalities to\ninteract at each depth of encoding and obtain richer representations compared\nto common late fusion. We collect new datasets for multimodal pre-training and\nevaluation, designing challenging tasks for multimodal encoder models. We show\nthat FuseLIP outperforms other approaches in multimodal embedding tasks such as\nVQA and text-guided image transformation retrieval, while being comparable to\nbaselines on unimodal tasks.",
            "upvotes": 3,
            "discussionId": "684002758bd5bff9918ce109",
            "ai_summary": "FuseLIP is a transformer-based architecture that uses a shared vocabulary for text and image tokens to enhance multimodal embedding and outperforms existing models in tasks such as VQA and text-guided image retrieval.",
            "ai_keywords": [
                "contrastive language-image pre-training",
                "latent space",
                "unimodal encoders",
                "multimodal inputs",
                "feature vector",
                "late fusion",
                "early fusion",
                "transformer model",
                "discrete image tokenizers",
                "multimodal pre-training",
                "VQA",
                "text-guided image transformation retrieval"
            ]
        },
        "publishedAt": "2025-06-03T13:27:12.000Z",
        "title": "FuseLIP: Multimodal Embeddings via Early Fusion of Discrete Tokens",
        "summary": "Contrastive language-image pre-training aligns the features of text-image\npairs in a common latent space via distinct encoders for each modality. While\nthis approach achieves impressive performance in several zero-shot tasks, it\ncannot natively handle multimodal inputs, i.e., encoding image and text into a\nsingle feature vector. As a remedy, it is common practice to use additional\nmodules to merge the features extracted by the unimodal encoders. In this work,\nwe present FuseLIP, an alternative architecture for multimodal embedding.\nLeveraging recent progress in discrete image tokenizers, we propose to use a\nsingle transformer model which operates on an extended vocabulary of text and\nimage tokens. This early fusion approach allows the different modalities to\ninteract at each depth of encoding and obtain richer representations compared\nto common late fusion. We collect new datasets for multimodal pre-training and\nevaluation, designing challenging tasks for multimodal encoder models. We show\nthat FuseLIP outperforms other approaches in multimodal embedding tasks such as\nVQA and text-guided image transformation retrieval, while being comparable to\nbaselines on unimodal tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03096.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6310a6bb0a43f97f6c5567d3",
            "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
            "fullname": "Christian Schlarmann",
            "name": "chs20",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02338",
            "authors": [
                {
                    "_id": "683fbc730ffa93c1611d513b",
                    "user": {
                        "_id": "64c8f4cec547ed5243ebd0a8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
                        "isPro": false,
                        "fullname": "Hyungjoo Chae",
                        "user": "hyungjoochae",
                        "type": "user"
                    },
                    "name": "Hyungjoo Chae",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:55:14.169Z",
                    "hidden": false
                },
                {
                    "_id": "683fbc730ffa93c1611d513c",
                    "name": "Dongjin Kang",
                    "hidden": false
                },
                {
                    "_id": "683fbc730ffa93c1611d513d",
                    "name": "Jihyuk Kim",
                    "hidden": false
                },
                {
                    "_id": "683fbc730ffa93c1611d513e",
                    "name": "Beong-woo Kwak",
                    "hidden": false
                },
                {
                    "_id": "683fbc730ffa93c1611d513f",
                    "name": "Sunghyun Park",
                    "hidden": false
                },
                {
                    "_id": "683fbc730ffa93c1611d5140",
                    "name": "Haeju Park",
                    "hidden": false
                },
                {
                    "_id": "683fbc730ffa93c1611d5141",
                    "name": "Jinyoung Yeo",
                    "hidden": false
                },
                {
                    "_id": "683fbc730ffa93c1611d5142",
                    "name": "Moontae Lee",
                    "hidden": false
                },
                {
                    "_id": "683fbc730ffa93c1611d5143",
                    "name": "Kyungjae Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T00:29:15.000Z",
            "submittedOnDailyAt": "2025-06-04T01:55:22.453Z",
            "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL",
            "submittedOnDailyBy": {
                "_id": "64c8f4cec547ed5243ebd0a8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
                "isPro": false,
                "fullname": "Hyungjoo Chae",
                "user": "hyungjoochae",
                "type": "user"
            },
            "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.",
            "upvotes": 3,
            "discussionId": "683fbc760ffa93c1611d51cc",
            "ai_summary": "The Long CoT Collection dataset, generated by short CoT LLMs, enhances general reasoning skills and provides a strong foundation for reinforcement learning, achieving quality comparable to R1.",
            "ai_keywords": [
                "long chain-of-thought",
                "CoT inferences",
                "LRMs",
                "direct distillation",
                "inference-time scaling",
                "CoT rationales",
                "short CoT LLMs",
                "reasoning strategies",
                "thought budget",
                "overthinking",
                "reinforcement learning",
                "RLVR"
            ]
        },
        "publishedAt": "2025-06-02T20:29:15.000Z",
        "title": "One Missing Piece for Open-Source Reasoning Models: A Dataset to\n  Mitigate Cold-Starting Short CoT LLMs in RL",
        "summary": "With the release of R1, a publicly available large reasoning model (LRM),\nresearchers commonly train new LRMs by training language models on R1's long\nchain-of-thought (CoT) inferences. While prior works show that LRMs'\ncapabilities can be reproduced through direct distillation, the continued\nreliance on the existing models (e.g., R1) remains a critical limitation in\nadvancing the field. As a first step toward independent LRM development, this\npaper explores the possibility of constructing a long CoT dataset with LLMs\nthat are not trained for inference-time scaling. To this end, we present the\nLong CoT Collection, a dataset of 100K CoT rationales annotated using existing\nshort CoT LLMs. We develop a pipeline that induces o1's novel reasoning\nstrategies into short CoT LLMs, enabling them to think longer and introducing\ncontrollability over the thought budget to better manage the overthinking\nproblem. Our extensive analyses validate that our dataset achieves quality\ncomparable to--or slightly below--R1. Furthermore, our experiments demonstrate\nthat training on our dataset not only strengthens general reasoning skills, but\nalso provides a strong foundation for reinforcement learning--models\ninitialized on our data achieve 2-3x larger gains with RLVR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02338.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c8f4cec547ed5243ebd0a8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c8f4cec547ed5243ebd0a8/MiOH5YbMg8Gh9KYlQsLmX.jpeg",
            "fullname": "Hyungjoo Chae",
            "name": "hyungjoochae",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01004",
            "authors": [
                {
                    "_id": "68403832646314cfc20a5de2",
                    "user": {
                        "_id": "65c898197faf326d067e2c0d",
                        "avatarUrl": "/avatars/dde3c047e627bacd69e0f063a4a833b2.svg",
                        "isPro": false,
                        "fullname": "Tong Zhang",
                        "user": "Tong98Zhang",
                        "type": "user"
                    },
                    "name": "Tong Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T15:02:54.641Z",
                    "hidden": false
                },
                {
                    "_id": "68403832646314cfc20a5de3",
                    "name": "Juan C Leon Alcazar",
                    "hidden": false
                },
                {
                    "_id": "68403832646314cfc20a5de4",
                    "name": "Bernard Ghanem",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-01T13:28:04.000Z",
            "submittedOnDailyAt": "2025-06-04T16:49:11.835Z",
            "title": "Motion-Aware Concept Alignment for Consistent Video Editing",
            "submittedOnDailyBy": {
                "_id": "65c898197faf326d067e2c0d",
                "avatarUrl": "/avatars/dde3c047e627bacd69e0f063a4a833b2.svg",
                "isPro": false,
                "fullname": "Tong Zhang",
                "user": "Tong98Zhang",
                "type": "user"
            },
            "summary": "We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a\ntraining-free framework bridging the gap between image-domain semantic mixing\nand video. Given a generated video and a user-provided reference image,\nMoCA-Video injects the semantic features of the reference image into a specific\nobject within the video, while preserving the original motion and visual\ncontext. Our approach leverages a diagonal denoising schedule and\nclass-agnostic segmentation to detect and track objects in the latent space and\nprecisely control the spatial location of the blended objects. To ensure\ntemporal coherence, we incorporate momentum-based semantic corrections and\ngamma residual noise stabilization for smooth frame transitions. We evaluate\nMoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS,\nand introduce a novel metric CASS (Conceptual Alignment Shift Score) to\nevaluate the consistency and effectiveness of the visual shifts between the\nsource prompt and the modified video frames. Using self-constructed dataset,\nMoCA-Video outperforms current baselines, achieving superior spatial\nconsistency, coherent motion, and a significantly higher CASS score, despite\nhaving no training or fine-tuning. MoCA-Video demonstrates that structured\nmanipulation in the diffusion noise trajectory allows for controllable,\nhigh-quality video synthesis.",
            "upvotes": 3,
            "discussionId": "68403834646314cfc20a5e59",
            "ai_summary": "MoCA-Video injects semantic features from a reference image into a video object, preserving motion and visual context, and outperforms baselines using novel metrics.",
            "ai_keywords": [
                "MoCA-Video",
                "Motion-Aware Concept Alignment",
                "image-domain semantic mixing",
                "diagonal denoising schedule",
                "class-agnostic segmentation",
                "latent space",
                "momentum-based semantic corrections",
                "gamma residual noise stabilization",
                "SSIM",
                "image-level LPIPS",
                "temporal LPIPS",
                "CASS",
                "Conceptual Alignment Shift Score",
                "diffusion noise trajectory",
                "video synthesis",
                "structured manipulation"
            ]
        },
        "publishedAt": "2025-06-01T09:28:04.000Z",
        "title": "Motion-Aware Concept Alignment for Consistent Video Editing",
        "summary": "We introduce MoCA-Video (Motion-Aware Concept Alignment in Video), a\ntraining-free framework bridging the gap between image-domain semantic mixing\nand video. Given a generated video and a user-provided reference image,\nMoCA-Video injects the semantic features of the reference image into a specific\nobject within the video, while preserving the original motion and visual\ncontext. Our approach leverages a diagonal denoising schedule and\nclass-agnostic segmentation to detect and track objects in the latent space and\nprecisely control the spatial location of the blended objects. To ensure\ntemporal coherence, we incorporate momentum-based semantic corrections and\ngamma residual noise stabilization for smooth frame transitions. We evaluate\nMoCA's performance using the standard SSIM, image-level LPIPS, temporal LPIPS,\nand introduce a novel metric CASS (Conceptual Alignment Shift Score) to\nevaluate the consistency and effectiveness of the visual shifts between the\nsource prompt and the modified video frames. Using self-constructed dataset,\nMoCA-Video outperforms current baselines, achieving superior spatial\nconsistency, coherent motion, and a significantly higher CASS score, despite\nhaving no training or fine-tuning. MoCA-Video demonstrates that structured\nmanipulation in the diffusion noise trajectory allows for controllable,\nhigh-quality video synthesis.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01004.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65c898197faf326d067e2c0d",
            "avatarUrl": "/avatars/dde3c047e627bacd69e0f063a4a833b2.svg",
            "fullname": "Tong Zhang",
            "name": "Tong98Zhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00391",
            "authors": [
                {
                    "_id": "683fd0aca44e6cbbe3c185b2",
                    "user": {
                        "_id": "6419435385030eca6ac94701",
                        "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
                        "isPro": false,
                        "fullname": "Ge Qu",
                        "user": "gq2138",
                        "type": "user"
                    },
                    "name": "Ge Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:54:33.919Z",
                    "hidden": false
                },
                {
                    "_id": "683fd0aca44e6cbbe3c185b3",
                    "name": "Jinyang Li",
                    "hidden": false
                },
                {
                    "_id": "683fd0aca44e6cbbe3c185b4",
                    "name": "Bowen Qin",
                    "hidden": false
                },
                {
                    "_id": "683fd0aca44e6cbbe3c185b5",
                    "name": "Xiaolong Li",
                    "hidden": false
                },
                {
                    "_id": "683fd0aca44e6cbbe3c185b6",
                    "name": "Nan Huo",
                    "hidden": false
                },
                {
                    "_id": "683fd0aca44e6cbbe3c185b7",
                    "name": "Chenhao Ma",
                    "hidden": false
                },
                {
                    "_id": "683fd0aca44e6cbbe3c185b8",
                    "name": "Reynold Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T04:51:12.000Z",
            "submittedOnDailyAt": "2025-06-04T12:17:47.037Z",
            "title": "SHARE: An SLM-based Hierarchical Action CorREction Assistant for\n  Text-to-SQL",
            "submittedOnDailyBy": {
                "_id": "6419435385030eca6ac94701",
                "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
                "isPro": false,
                "fullname": "Ge Qu",
                "user": "gq2138",
                "type": "user"
            },
            "summary": "Current self-correction approaches in text-to-SQL face two critical\nlimitations: 1) Conventional self-correction methods rely on recursive\nself-calls of LLMs, resulting in multiplicative computational overhead, and 2)\nLLMs struggle to implement effective error detection and correction for\ndeclarative SQL queries, as they fail to demonstrate the underlying reasoning\npath. In this work, we propose SHARE, an SLM-based Hierarchical Action\ncorREction assistant that enables LLMs to perform more precise error\nlocalization and efficient correction. SHARE orchestrates three specialized\nSmall Language Models (SLMs) in a sequential pipeline, where it first\ntransforms declarative SQL queries into stepwise action trajectories that\nreveal underlying reasoning, followed by a two-phase granular refinement. We\nfurther propose a novel hierarchical self-evolution strategy for data-efficient\ntraining. Experimental results demonstrate that SHARE effectively enhances\nself-correction capabilities while proving robust across various LLMs.\nFurthermore, our comprehensive analysis shows that SHARE maintains strong\nperformance even in low-resource training settings, which is particularly\nvaluable for text-to-SQL applications with data privacy constraints.",
            "upvotes": 3,
            "discussionId": "683fd0ada44e6cbbe3c185fe",
            "ai_summary": "SHARE, an SLM-based Hierarchical Action RECorrection system, enhances LLMs in text-to-SQL by transforming SQL queries into action trajectories and employing a granular refinement process, improving error detection and correction efficiency.",
            "ai_keywords": [
                "self-correction",
                "LLMs",
                "SLM-based",
                "hierarchical action correction",
                "stepwise action trajectories",
                "granular refinement",
                "hierarchical self-evolution strategy",
                "data-efficient training"
            ]
        },
        "publishedAt": "2025-05-31T00:51:12.000Z",
        "title": "SHARE: An SLM-based Hierarchical Action CorREction Assistant for\n  Text-to-SQL",
        "summary": "Current self-correction approaches in text-to-SQL face two critical\nlimitations: 1) Conventional self-correction methods rely on recursive\nself-calls of LLMs, resulting in multiplicative computational overhead, and 2)\nLLMs struggle to implement effective error detection and correction for\ndeclarative SQL queries, as they fail to demonstrate the underlying reasoning\npath. In this work, we propose SHARE, an SLM-based Hierarchical Action\ncorREction assistant that enables LLMs to perform more precise error\nlocalization and efficient correction. SHARE orchestrates three specialized\nSmall Language Models (SLMs) in a sequential pipeline, where it first\ntransforms declarative SQL queries into stepwise action trajectories that\nreveal underlying reasoning, followed by a two-phase granular refinement. We\nfurther propose a novel hierarchical self-evolution strategy for data-efficient\ntraining. Experimental results demonstrate that SHARE effectively enhances\nself-correction capabilities while proving robust across various LLMs.\nFurthermore, our comprehensive analysis shows that SHARE maintains strong\nperformance even in low-resource training settings, which is particularly\nvaluable for text-to-SQL applications with data privacy constraints.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00391.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6419435385030eca6ac94701",
            "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
            "fullname": "Ge Qu",
            "name": "gq2138",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00227",
            "authors": [
                {
                    "_id": "683f162def3fb0fc6576af67",
                    "user": {
                        "_id": "66feec9a1add69636bd1fff9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66feec9a1add69636bd1fff9/U2NWIli3ibS7JWdl98Ftp.png",
                        "isPro": false,
                        "fullname": "Anthony Gosselin",
                        "user": "AnthonyGosselin",
                        "type": "user"
                    },
                    "name": "Anthony Gosselin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T09:01:46.764Z",
                    "hidden": false
                },
                {
                    "_id": "683f162def3fb0fc6576af68",
                    "name": "Ge Ya Luo",
                    "hidden": false
                },
                {
                    "_id": "683f162def3fb0fc6576af69",
                    "name": "Luis Lara",
                    "hidden": false
                },
                {
                    "_id": "683f162def3fb0fc6576af6a",
                    "name": "Florian Golemo",
                    "hidden": false
                },
                {
                    "_id": "683f162def3fb0fc6576af6b",
                    "name": "Derek Nowrouzezahrai",
                    "hidden": false
                },
                {
                    "_id": "683f162def3fb0fc6576af6c",
                    "name": "Liam Paull",
                    "hidden": false
                },
                {
                    "_id": "683f162def3fb0fc6576af6d",
                    "name": "Alexia Jolicoeur-Martineau",
                    "hidden": false
                },
                {
                    "_id": "683f162def3fb0fc6576af6e",
                    "name": "Christopher Pal",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66feec9a1add69636bd1fff9/tWPIzy9kSKm2coeEMp-zl.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66feec9a1add69636bd1fff9/Hw8hrPsyIoJsmsah8OJe9.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66feec9a1add69636bd1fff9/6nmfGG21BcmU2zkWdEc1R.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/66feec9a1add69636bd1fff9/BfZ6G5e9ke-tPVuWNE1GA.mp4"
            ],
            "publishedAt": "2025-05-30T21:04:38.000Z",
            "submittedOnDailyAt": "2025-06-04T15:52:38.415Z",
            "title": "Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes",
            "submittedOnDailyBy": {
                "_id": "66feec9a1add69636bd1fff9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66feec9a1add69636bd1fff9/U2NWIli3ibS7JWdl98Ftp.png",
                "isPro": false,
                "fullname": "Anthony Gosselin",
                "user": "AnthonyGosselin",
                "type": "user"
            },
            "summary": "Video diffusion techniques have advanced significantly in recent years;\nhowever, they struggle to generate realistic imagery of car crashes due to the\nscarcity of accident events in most driving datasets. Improving traffic safety\nrequires realistic and controllable accident simulations. To tackle the\nproblem, we propose Ctrl-Crash, a controllable car crash video generation model\nthat conditions on signals such as bounding boxes, crash types, and an initial\nimage frame. Our approach enables counterfactual scenario generation where\nminor variations in input can lead to dramatically different crash outcomes. To\nsupport fine-grained control at inference time, we leverage classifier-free\nguidance with independently tunable scales for each conditioning signal.\nCtrl-Crash achieves state-of-the-art performance across quantitative video\nquality metrics (e.g., FVD and JEDi) and qualitative measurements based on a\nhuman-evaluation of physical realism and video quality compared to prior\ndiffusion-based methods.",
            "upvotes": 3,
            "discussionId": "683f1630ef3fb0fc6576b037",
            "ai_summary": "Ctrl-Crash, a controllable car crash video generation model using classifier-free guidance, achieves top performance in video quality and realism compared to existing diffusion-based methods.",
            "ai_keywords": [
                "video diffusion techniques",
                "car crashes",
                "driving datasets",
                "accident simulations",
                "classifier-free guidance",
                "bounding boxes",
                "crash types",
                "initial image frame",
                "counterfactual scenario generation",
                "FVD",
                "JEDi",
                "video quality",
                "physical realism"
            ]
        },
        "publishedAt": "2025-05-30T17:04:38.000Z",
        "title": "Ctrl-Crash: Controllable Diffusion for Realistic Car Crashes",
        "summary": "Video diffusion techniques have advanced significantly in recent years;\nhowever, they struggle to generate realistic imagery of car crashes due to the\nscarcity of accident events in most driving datasets. Improving traffic safety\nrequires realistic and controllable accident simulations. To tackle the\nproblem, we propose Ctrl-Crash, a controllable car crash video generation model\nthat conditions on signals such as bounding boxes, crash types, and an initial\nimage frame. Our approach enables counterfactual scenario generation where\nminor variations in input can lead to dramatically different crash outcomes. To\nsupport fine-grained control at inference time, we leverage classifier-free\nguidance with independently tunable scales for each conditioning signal.\nCtrl-Crash achieves state-of-the-art performance across quantitative video\nquality metrics (e.g., FVD and JEDi) and qualitative measurements based on a\nhuman-evaluation of physical realism and video quality compared to prior\ndiffusion-based methods.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66feec9a1add69636bd1fff9/tWPIzy9kSKm2coeEMp-zl.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66feec9a1add69636bd1fff9/Hw8hrPsyIoJsmsah8OJe9.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66feec9a1add69636bd1fff9/6nmfGG21BcmU2zkWdEc1R.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/66feec9a1add69636bd1fff9/BfZ6G5e9ke-tPVuWNE1GA.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00227.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66feec9a1add69636bd1fff9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66feec9a1add69636bd1fff9/U2NWIli3ibS7JWdl98Ftp.png",
            "fullname": "Anthony Gosselin",
            "name": "AnthonyGosselin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24273",
            "authors": [
                {
                    "_id": "683f21ed6ba11d78e3e383f6",
                    "user": {
                        "_id": "65f7c56fc6356b5cc5ab8245",
                        "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
                        "isPro": false,
                        "fullname": "James Cai",
                        "user": "jamescai20",
                        "type": "user"
                    },
                    "name": "Hongyi James Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T09:01:32.571Z",
                    "hidden": false
                },
                {
                    "_id": "683f21ed6ba11d78e3e383f7",
                    "name": "Junlin Wang",
                    "hidden": false
                },
                {
                    "_id": "683f21ed6ba11d78e3e383f8",
                    "user": {
                        "_id": "65d66cb2b06abf924b07ff76",
                        "avatarUrl": "/avatars/de94e2fe07040b7dc3053bcaafa64ffe.svg",
                        "isPro": false,
                        "fullname": "Xiaoyin Chen",
                        "user": "chenyn66",
                        "type": "user"
                    },
                    "name": "Xiaoyin Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-03T16:25:17.851Z",
                    "hidden": false
                },
                {
                    "_id": "683f21ed6ba11d78e3e383f9",
                    "name": "Bhuwan Dhingra",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T06:49:00.000Z",
            "submittedOnDailyAt": "2025-06-04T08:05:26.080Z",
            "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "65f7c56fc6356b5cc5ab8245",
                "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
                "isPro": false,
                "fullname": "James Cai",
                "user": "jamescai20",
                "type": "user"
            },
            "summary": "Recent breakthroughs in large language models (LLMs) have effectively\nimproved their reasoning abilities, particularly on mathematical and logical\nproblems that have verifiable answers, through techniques such as supervised\nfinetuning (SFT) and reinforcement learning (RL). Prior research indicates that\nRL effectively internalizes search strategies, enabling long chain-of-thought\n(CoT) reasoning, with backtracking emerging naturally as a learned capability.\nHowever, the precise benefits of backtracking, specifically, how significantly\nit contributes to reasoning improvements and the optimal extent of its use,\nremain poorly understood. In this work, we systematically investigate the\ndynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc\n1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self\nReference. Our findings highlight that short CoT sequences used in SFT as a\nwarm-up do have moderate contribution to RL training, compared with cold-start\nRL; however such contribution diminishes when tasks become increasingly\ndifficult. Motivated by this observation, we construct synthetic datasets\nvarying systematically in the number of backtracking steps and conduct\ncontrolled experiments to isolate the influence of either the correctness\n(content) or the structure (i.e., backtrack frequency). We find that (1) longer\nCoT with backtracks generally induce better and more stable RL training, (2)\nmore challenging problems with larger search space tend to need higher numbers\nof backtracks during the SFT stage. Additionally, we demonstrate through\nexperiments on distilled data that RL training is largely unaffected by the\ncorrectness of long CoT sequences, suggesting that RL prioritizes structural\npatterns over content correctness. Collectively, our results offer practical\ninsights into designing optimal training strategies to effectively scale\nreasoning in LLMs.",
            "upvotes": 3,
            "discussionId": "683f21ed6ba11d78e3e38434",
            "ai_summary": "This study investigates the interplay between supervised fine-tuning and reinforcement learning in large language models, focusing on the role of backtracking in enhancing reasoning capabilities across various tasks.",
            "ai_keywords": [
                "supervised finetuning",
                "reinforcement learning",
                "chain-of-thought",
                "backtracking",
                "countdown",
                "sudoku",
                "arc 1d",
                "geometry",
                "color cube rotation",
                "list functions",
                "zebra puzzles",
                "self reference",
                "synthetic datasets",
                "distilled data"
            ]
        },
        "publishedAt": "2025-05-30T02:49:00.000Z",
        "title": "How Much Backtracking is Enough? Exploring the Interplay of SFT and RL\n  in Enhancing LLM Reasoning",
        "summary": "Recent breakthroughs in large language models (LLMs) have effectively\nimproved their reasoning abilities, particularly on mathematical and logical\nproblems that have verifiable answers, through techniques such as supervised\nfinetuning (SFT) and reinforcement learning (RL). Prior research indicates that\nRL effectively internalizes search strategies, enabling long chain-of-thought\n(CoT) reasoning, with backtracking emerging naturally as a learned capability.\nHowever, the precise benefits of backtracking, specifically, how significantly\nit contributes to reasoning improvements and the optimal extent of its use,\nremain poorly understood. In this work, we systematically investigate the\ndynamics between SFT and RL on eight reasoning tasks: Countdown, Sudoku, Arc\n1D, Geometry, Color Cube Rotation, List Functions, Zebra Puzzles, and Self\nReference. Our findings highlight that short CoT sequences used in SFT as a\nwarm-up do have moderate contribution to RL training, compared with cold-start\nRL; however such contribution diminishes when tasks become increasingly\ndifficult. Motivated by this observation, we construct synthetic datasets\nvarying systematically in the number of backtracking steps and conduct\ncontrolled experiments to isolate the influence of either the correctness\n(content) or the structure (i.e., backtrack frequency). We find that (1) longer\nCoT with backtracks generally induce better and more stable RL training, (2)\nmore challenging problems with larger search space tend to need higher numbers\nof backtracks during the SFT stage. Additionally, we demonstrate through\nexperiments on distilled data that RL training is largely unaffected by the\ncorrectness of long CoT sequences, suggesting that RL prioritizes structural\npatterns over content correctness. Collectively, our results offer practical\ninsights into designing optimal training strategies to effectively scale\nreasoning in LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24273.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f7c56fc6356b5cc5ab8245",
            "avatarUrl": "/avatars/ff8d3f3526e915f269274cc0dc5ca8ef.svg",
            "fullname": "James Cai",
            "name": "jamescai20",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.18079",
            "authors": [
                {
                    "_id": "68354eec0830dfc6782ba1c4",
                    "name": "Xiaoyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68354eec0830dfc6782ba1c5",
                    "name": "Zhaoyang Jia",
                    "hidden": false
                },
                {
                    "_id": "68354eec0830dfc6782ba1c6",
                    "name": "Zongyu Guo",
                    "hidden": false
                },
                {
                    "_id": "68354eec0830dfc6782ba1c7",
                    "name": "Jiahao Li",
                    "hidden": false
                },
                {
                    "_id": "68354eec0830dfc6782ba1c8",
                    "name": "Bin Li",
                    "hidden": false
                },
                {
                    "_id": "68354eec0830dfc6782ba1c9",
                    "name": "Houqiang Li",
                    "hidden": false
                },
                {
                    "_id": "68354eec0830dfc6782ba1ca",
                    "name": "Yan Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-23T16:37:36.000Z",
            "submittedOnDailyAt": "2025-06-04T07:20:42.262Z",
            "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "6582b79aafc6b50a2cbaa5c8",
                "avatarUrl": "/avatars/65fa21ca144177f232347084b0e057c5.svg",
                "isPro": false,
                "fullname": "Xiaoyi Zhang",
                "user": "xyzhang626",
                "type": "user"
            },
            "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.",
            "upvotes": 3,
            "discussionId": "68354eed0830dfc6782ba1fe",
            "ai_summary": "The Deep Video Discovery agent uses an autonomous agentic search strategy with large language models to overcome limitations in long-form video understanding, achieving state-of-the-art results on benchmarks like LVBench.",
            "ai_keywords": [
                "Deep Video Discovery agent",
                "agentic search strategy",
                "segmented video clips",
                "multi-granular video database",
                "advanced reasoning capability",
                "LLM",
                "autonomous nature",
                "observation state",
                "search-centric tools",
                "internal reasoning",
                "long video understanding benchmarks",
                "LVBench",
                "ablation studies",
                "tool analyses"
            ]
        },
        "publishedAt": "2025-05-23T12:37:36.000Z",
        "title": "Deep Video Discovery: Agentic Search with Tool Use for Long-form Video\n  Understanding",
        "summary": "Long-form video understanding presents significant challenges due to\nextensive temporal-spatial complexity and the difficulty of question answering\nunder such extended contexts. While Large Language Models (LLMs) have\ndemonstrated considerable advancements in video analysis capabilities and long\ncontext handling, they continue to exhibit limitations when processing\ninformation-dense hour-long videos. To overcome such limitations, we propose\nthe Deep Video Discovery agent to leverage an agentic search strategy over\nsegmented video clips. Different from previous video agents manually designing\na rigid workflow, our approach emphasizes the autonomous nature of agents. By\nproviding a set of search-centric tools on multi-granular video database, our\nDVD agent leverages the advanced reasoning capability of LLM to plan on its\ncurrent observation state, strategically selects tools, formulates appropriate\nparameters for actions, and iteratively refines its internal reasoning in light\nof the gathered information. We perform comprehensive evaluation on multiple\nlong video understanding benchmarks that demonstrates the advantage of the\nentire system design. Our DVD agent achieves SOTA performance, significantly\nsurpassing prior works by a large margin on the challenging LVBench dataset.\nComprehensive ablation studies and in-depth tool analyses are also provided,\nyielding insights to further advance intelligent agents tailored for long-form\nvideo understanding tasks. The code will be released later.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.18079.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6582b79aafc6b50a2cbaa5c8",
            "avatarUrl": "/avatars/65fa21ca144177f232347084b0e057c5.svg",
            "fullname": "Xiaoyi Zhang",
            "name": "xyzhang626",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03119",
            "authors": [
                {
                    "_id": "68407ac4769b3772e8fdb16a",
                    "name": "Zujin Guo",
                    "hidden": false
                },
                {
                    "_id": "68407ac4769b3772e8fdb16b",
                    "name": "Size Wu",
                    "hidden": false
                },
                {
                    "_id": "68407ac4769b3772e8fdb16c",
                    "name": "Zhongang Cai",
                    "hidden": false
                },
                {
                    "_id": "68407ac4769b3772e8fdb16d",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68407ac4769b3772e8fdb16e",
                    "name": "Chen Change Loy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:50:05.000Z",
            "submittedOnDailyAt": "2025-06-04T15:27:38.422Z",
            "title": "Controllable Human-centric Keyframe Interpolation with Generative Prior",
            "submittedOnDailyBy": {
                "_id": "62f4b57a9437c4d04fb5e6d6",
                "avatarUrl": "/avatars/3681e4948fba1317c80bbbf2326a5fcd.svg",
                "isPro": false,
                "fullname": "ZUJIN GUO",
                "user": "GSean",
                "type": "user"
            },
            "summary": "Existing interpolation methods use pre-trained video diffusion priors to\ngenerate intermediate frames between sparsely sampled keyframes. In the absence\nof 3D geometric guidance, these methods struggle to produce plausible results\nfor complex, articulated human motions and offer limited control over the\nsynthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe\nInterpolator (PoseFuse3D-KI), a novel framework that integrates 3D human\nguidance signals into the diffusion process for Controllable Human-centric\nKeyframe Interpolation (CHKI). To provide rich spatial and structural cues for\ninterpolation, our PoseFuse3D, a 3D-informed control model, features a novel\nSMPL-X encoder that transforms 3D geometry and shape into the 2D latent\nconditioning space, alongside a fusion network that integrates these 3D cues\nwith 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset\nannotated with both 2D poses and 3D SMPL-X parameters. We show that\nPoseFuse3D-KI consistently outperforms state-of-the-art baselines on\nCHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS.\nComprehensive ablations demonstrate that our PoseFuse3D model improves\ninterpolation fidelity.",
            "upvotes": 2,
            "discussionId": "68407ac8769b3772e8fdb23b",
            "projectPage": "https://gseancdat.github.io/projects/PoseFuse3D_KI",
            "ai_summary": "PoseFuse3D Keyframe Interpolator integrates 3D human guidance into diffusion models for more accurate and controllable video frame interpolation.",
            "ai_keywords": [
                "video diffusion priors",
                "interpolation",
                "3D human guidance",
                "PoseFuse3D Keyframe Interpolator",
                "Controllable Human-centric Keyframe Interpolation",
                "CHKI",
                "SMPL-X encoder",
                "3D-informed control model",
                "fusion network",
                "2D pose embeddings",
                "CHKI-Video",
                "PSNR",
                "LPIPS"
            ]
        },
        "publishedAt": "2025-06-03T13:50:05.000Z",
        "title": "Controllable Human-centric Keyframe Interpolation with Generative Prior",
        "summary": "Existing interpolation methods use pre-trained video diffusion priors to\ngenerate intermediate frames between sparsely sampled keyframes. In the absence\nof 3D geometric guidance, these methods struggle to produce plausible results\nfor complex, articulated human motions and offer limited control over the\nsynthesized dynamics. In this paper, we introduce PoseFuse3D Keyframe\nInterpolator (PoseFuse3D-KI), a novel framework that integrates 3D human\nguidance signals into the diffusion process for Controllable Human-centric\nKeyframe Interpolation (CHKI). To provide rich spatial and structural cues for\ninterpolation, our PoseFuse3D, a 3D-informed control model, features a novel\nSMPL-X encoder that transforms 3D geometry and shape into the 2D latent\nconditioning space, alongside a fusion network that integrates these 3D cues\nwith 2D pose embeddings. For evaluation, we build CHKI-Video, a new dataset\nannotated with both 2D poses and 3D SMPL-X parameters. We show that\nPoseFuse3D-KI consistently outperforms state-of-the-art baselines on\nCHKI-Video, achieving a 9% improvement in PSNR and a 38% reduction in LPIPS.\nComprehensive ablations demonstrate that our PoseFuse3D model improves\ninterpolation fidelity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03119.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f4b57a9437c4d04fb5e6d6",
            "avatarUrl": "/avatars/3681e4948fba1317c80bbbf2326a5fcd.svg",
            "fullname": "ZUJIN GUO",
            "name": "GSean",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.02678",
            "authors": [
                {
                    "_id": "68407021a88ca303f9bfd02f",
                    "name": "Zhong-Zhi Li",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd030",
                    "user": {
                        "_id": "63fb6e281b4b1bd4e7ffc5be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liu",
                        "user": "lx865712528",
                        "type": "user"
                    },
                    "name": "Xiao Liang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-04T16:11:34.763Z",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd031",
                    "name": "Zihao Tang",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd032",
                    "name": "Lei Ji",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd033",
                    "name": "Peijie Wang",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd034",
                    "name": "Haotian Xu",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd035",
                    "name": "Xing W",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd036",
                    "name": "Haizhen Huang",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd037",
                    "name": "Weiwei Deng",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd038",
                    "name": "Ying Nian Wu",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd039",
                    "name": "Yeyun Gong",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd03a",
                    "name": "Zhijiang Guo",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd03b",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd03c",
                    "name": "Fei Yin",
                    "hidden": false
                },
                {
                    "_id": "68407021a88ca303f9bfd03d",
                    "name": "Cheng-Lin Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T09:23:41.000Z",
            "submittedOnDailyAt": "2025-06-04T14:42:11.776Z",
            "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
            "submittedOnDailyBy": {
                "_id": "63fb6e281b4b1bd4e7ffc5be",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
                "isPro": false,
                "fullname": "Xiao Liu",
                "user": "lx865712528",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by\nleveraging Reinforcement Learning and extended Chain-of-Thought (CoT)\ntechniques. However, the challenge of performing efficient language\nreasoning--especially during inference with extremely long outputs--has drawn\nincreasing attention from the research community. In this work, we propose a\ndynamic ratio-based training pipeline that does not rely on sophisticated data\nannotations or interpolation between multiple models. We continuously balance\nthe weights between the model's System-1 and System-2 data to eliminate\nredundant reasoning processes while preserving the model's reasoning\ncapability. We validate our approach across models on DeepSeek-R1-Distill-7B\nand DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying\ndifficulty levels. Our method significantly reduces the number of output tokens\nby nearly 40% while maintaining the accuracy of the reasoning. Our code and\ndata will be available soon.",
            "upvotes": 2,
            "discussionId": "68407022a88ca303f9bfd080",
            "ai_summary": "A dynamic ratio-based training pipeline reduces output tokens in large language models without sacrificing reasoning accuracy.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Chain-of-Thought",
                "language reasoning",
                "dynamic ratio-based training",
                "System-1",
                "System-2",
                "DeepSeek-R1-Distill-7B",
                "DeepSeek-R1-Distill-14B"
            ]
        },
        "publishedAt": "2025-06-03T05:23:41.000Z",
        "title": "TL;DR: Too Long, Do Re-weighting for Effcient LLM Reasoning Compression",
        "summary": "Large Language Models (LLMs) have recently achieved remarkable progress by\nleveraging Reinforcement Learning and extended Chain-of-Thought (CoT)\ntechniques. However, the challenge of performing efficient language\nreasoning--especially during inference with extremely long outputs--has drawn\nincreasing attention from the research community. In this work, we propose a\ndynamic ratio-based training pipeline that does not rely on sophisticated data\nannotations or interpolation between multiple models. We continuously balance\nthe weights between the model's System-1 and System-2 data to eliminate\nredundant reasoning processes while preserving the model's reasoning\ncapability. We validate our approach across models on DeepSeek-R1-Distill-7B\nand DeepSeek-R1-Distill-14B and on a diverse set of benchmarks with varying\ndifficulty levels. Our method significantly reduces the number of output tokens\nby nearly 40% while maintaining the accuracy of the reasoning. Our code and\ndata will be available soon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02678.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fb6e281b4b1bd4e7ffc5be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677422062937-noauth.jpeg",
            "fullname": "Xiao Liu",
            "name": "lx865712528",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02510",
            "authors": [
                {
                    "_id": "683faa31f0564d1fb4b9ffc6",
                    "user": {
                        "_id": "642656cbad1e3b0e6e91b752",
                        "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
                        "isPro": false,
                        "fullname": "Jie Zhu",
                        "user": "amazingj",
                        "type": "user"
                    },
                    "name": "Jie Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:57:01.885Z",
                    "hidden": false
                },
                {
                    "_id": "683faa31f0564d1fb4b9ffc7",
                    "name": "Junhui Li",
                    "hidden": false
                },
                {
                    "_id": "683faa31f0564d1fb4b9ffc8",
                    "name": "Yalong Wen",
                    "hidden": false
                },
                {
                    "_id": "683faa31f0564d1fb4b9ffc9",
                    "name": "Xiandong Li",
                    "hidden": false
                },
                {
                    "_id": "683faa31f0564d1fb4b9ffca",
                    "name": "Lifan Guo",
                    "hidden": false
                },
                {
                    "_id": "683faa31f0564d1fb4b9ffcb",
                    "name": "Feng Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T06:41:09.000Z",
            "submittedOnDailyAt": "2025-06-04T00:38:17.377Z",
            "title": "M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset",
            "submittedOnDailyBy": {
                "_id": "642656cbad1e3b0e6e91b752",
                "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
                "isPro": false,
                "fullname": "Jie Zhu",
                "user": "amazingj",
                "type": "user"
            },
            "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called M^3FinMeeting, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, M^3FinMeeting supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\nM^3FinMeeting includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\nM^3FinMeeting as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.",
            "upvotes": 2,
            "discussionId": "683faa32f0564d1fb4ba0005",
            "projectPage": "https://github.com/aliyun/qwen-dianjin",
            "githubRepo": "https://github.com/aliyun/qwen-dianjin",
            "ai_summary": "A new multilingual, multi-sector, and multi-task benchmark, M³FinMeeting, evaluates large language models' performance in understanding financial meetings across different languages and industries.",
            "ai_keywords": [
                "large language models",
                "multilingual",
                "multi-sector",
                "multi-task",
                "benchmark",
                "financial meeting understanding",
                "Global Industry Classification Standard (GICS)",
                "summarization",
                "question-answer pair extraction",
                "question answering"
            ]
        },
        "publishedAt": "2025-06-03T02:41:09.000Z",
        "title": "M^3FinMeeting: A Multilingual, Multi-Sector, and Multi-Task Financial\n  Meeting Understanding Evaluation Dataset",
        "summary": "Recent breakthroughs in large language models (LLMs) have led to the\ndevelopment of new benchmarks for evaluating their performance in the financial\ndomain. However, current financial benchmarks often rely on news articles,\nearnings reports, or announcements, making it challenging to capture the\nreal-world dynamics of financial meetings. To address this gap, we propose a\nnovel benchmark called M^3FinMeeting, which is a multilingual,\nmulti-sector, and multi-task dataset designed for financial meeting\nunderstanding. First, M^3FinMeeting supports English, Chinese, and\nJapanese, enhancing comprehension of financial discussions in diverse\nlinguistic contexts. Second, it encompasses various industry sectors defined by\nthe Global Industry Classification Standard (GICS), ensuring that the benchmark\nspans a broad range of financial activities. Finally,\nM^3FinMeeting includes three tasks: summarization, question-answer\n(QA) pair extraction, and question answering, facilitating a more realistic and\ncomprehensive evaluation of understanding. Experimental results with seven\npopular LLMs reveal that even the most advanced long-context models have\nsignificant room for improvement, demonstrating the effectiveness of\nM^3FinMeeting as a benchmark for assessing LLMs' financial meeting\ncomprehension skills.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02510.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642656cbad1e3b0e6e91b752",
            "avatarUrl": "/avatars/3bf0ee15fd528e09b2b889f5cce3cbd0.svg",
            "fullname": "Jie Zhu",
            "name": "amazingj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02454",
            "authors": [
                {
                    "_id": "683fb5d592425f86f2be5c40",
                    "name": "Zhaorui Yang",
                    "hidden": false
                },
                {
                    "_id": "683fb5d592425f86f2be5c41",
                    "name": "Bo Pan",
                    "hidden": false
                },
                {
                    "_id": "683fb5d592425f86f2be5c42",
                    "name": "Han Wang",
                    "hidden": false
                },
                {
                    "_id": "683fb5d592425f86f2be5c43",
                    "name": "Yiyao Wang",
                    "hidden": false
                },
                {
                    "_id": "683fb5d592425f86f2be5c44",
                    "name": "Xingyu Liu",
                    "hidden": false
                },
                {
                    "_id": "683fb5d592425f86f2be5c45",
                    "name": "Minfeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "683fb5d592425f86f2be5c46",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fb5d592425f86f2be5c47",
                    "name": "Wei Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
            ],
            "publishedAt": "2025-06-03T05:18:19.000Z",
            "submittedOnDailyAt": "2025-06-04T01:28:03.959Z",
            "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework",
            "submittedOnDailyBy": {
                "_id": "64a568f5764b1dce366f9fd2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
                "isPro": false,
                "fullname": "Zhaorui Yang",
                "user": "zhaoruiyang",
                "type": "user"
            },
            "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.",
            "upvotes": 2,
            "discussionId": "683fb5d792425f86f2be5c78",
            "projectPage": "https://rickyang1114.github.io/multimodal-deepresearcher/",
            "ai_summary": "A new framework, Multimodal DeepResearcher, enables Large Language Models to generate high-quality multimodal reports combining text and diverse visualizations through structured textual representations.",
            "ai_keywords": [
                "Formal Description of Visualization",
                "FDV",
                "Multimodal DeepResearcher",
                "researching",
                "exemplar report textualization",
                "planning",
                "multimodal report generation",
                "MultimodalReportBench",
                "multimodal reports"
            ]
        },
        "publishedAt": "2025-06-03T01:18:19.000Z",
        "title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports\n  From Scratch with Agentic Framework",
        "summary": "Visualizations play a crucial part in effective communication of concepts and\ninformation. Recent advances in reasoning and retrieval augmented generation\nhave enabled Large Language Models (LLMs) to perform deep research and generate\ncomprehensive reports. Despite its progress, existing deep research frameworks\nprimarily focus on generating text-only content, leaving the automated\ngeneration of interleaved texts and visualizations underexplored. This novel\ntask poses key challenges in designing informative visualizations and\neffectively integrating them with text reports. To address these challenges, we\npropose Formal Description of Visualization (FDV), a structured textual\nrepresentation of charts that enables LLMs to learn from and generate diverse,\nhigh-quality visualizations. Building on this representation, we introduce\nMultimodal DeepResearcher, an agentic framework that decomposes the task into\nfour stages: (1) researching, (2) exemplar report textualization, (3) planning,\nand (4) multimodal report generation. For the evaluation of generated\nmultimodal reports, we develop MultimodalReportBench, which contains 100\ndiverse topics served as inputs along with 5 dedicated metrics. Extensive\nexperiments across models and evaluation methods demonstrate the effectiveness\nof Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet\nmodel, Multimodal DeepResearcher achieves an 82\\% overall win rate over the\nbaseline method.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a568f5764b1dce366f9fd2/Jup3eQX_IL2nKhCQVaSWS.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02454.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a568f5764b1dce366f9fd2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a568f5764b1dce366f9fd2/9THW2AJhEVmzltEAm9mMY.jpeg",
            "fullname": "Zhaorui Yang",
            "name": "zhaoruiyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.02295",
            "authors": [
                {
                    "_id": "683fa2ddf0564d1fb4b863c1",
                    "user": {
                        "_id": "630535e0c7fed54edfaa1a75",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630535e0c7fed54edfaa1a75/k6YZEDTexG4oidsWElebB.jpeg",
                        "isPro": true,
                        "fullname": "Ahmed Wasfy",
                        "user": "oddadmix",
                        "type": "user"
                    },
                    "name": "Ahmed Wasfy",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-04T01:36:46.007Z",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddf0564d1fb4b863c2",
                    "name": "Omer Nacar",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddf0564d1fb4b863c3",
                    "name": "Abdelakreem Elkhateb",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddf0564d1fb4b863c4",
                    "name": "Mahmoud Reda",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddf0564d1fb4b863c5",
                    "name": "Omar Elshehy",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddf0564d1fb4b863c6",
                    "name": "Adel Ammar",
                    "hidden": false
                },
                {
                    "_id": "683fa2ddf0564d1fb4b863c7",
                    "name": "Wadii Boulila",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T22:21:06.000Z",
            "submittedOnDailyAt": "2025-06-04T09:10:43.759Z",
            "title": "QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large\n  Language Model Adaptation",
            "submittedOnDailyBy": {
                "_id": "628f7a71dd993507cfcbe587",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                "isPro": true,
                "fullname": "Omartificial Intelligence Space",
                "user": "Omartificial-Intelligence-Space",
                "type": "user"
            },
            "summary": "The inherent complexities of Arabic script; its cursive nature, diacritical\nmarks (tashkeel), and varied typography, pose persistent challenges for Optical\nCharacter Recognition (OCR). We present Qari-OCR, a series of vision-language\nmodels derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic\nthrough iterative fine-tuning on specialized synthetic datasets. Our leading\nmodel, QARI v0.2, establishes a new open-source state-of-the-art with a Word\nError Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score\nof 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling\nof tashkeel, diverse fonts, and document layouts, alongside impressive\nperformance on low-resolution images. Further explorations (QARI v0.3) showcase\nstrong potential for structural document understanding and handwritten text.\nThis work delivers a marked improvement in Arabic OCR accuracy and efficiency,\nwith all models and datasets released to foster further research.",
            "upvotes": 2,
            "discussionId": "683fa2def0564d1fb4b863f1",
            "ai_summary": "Qari-OCR, a series of fine-tuned vision-language models, achieves state-of-the-art performance in Arabic OCR through iterative optimization on specialized datasets, handling diacritics, fonts, layouts, and low-resolution images.",
            "ai_keywords": [
                "vision-language models",
                "Qwen2-VL-2B-Instruct",
                "iterative fine-tuning",
                "synthetic datasets",
                "Word Error Rate",
                "Character Error Rate",
                "BLEU score",
                "tashkeel",
                "document layouts",
                "handwritten text",
                "structural document understanding"
            ]
        },
        "publishedAt": "2025-06-02T18:21:06.000Z",
        "title": "QARI-OCR: High-Fidelity Arabic Text Recognition through Multimodal Large\n  Language Model Adaptation",
        "summary": "The inherent complexities of Arabic script; its cursive nature, diacritical\nmarks (tashkeel), and varied typography, pose persistent challenges for Optical\nCharacter Recognition (OCR). We present Qari-OCR, a series of vision-language\nmodels derived from Qwen2-VL-2B-Instruct, progressively optimized for Arabic\nthrough iterative fine-tuning on specialized synthetic datasets. Our leading\nmodel, QARI v0.2, establishes a new open-source state-of-the-art with a Word\nError Rate (WER) of 0.160, Character Error Rate (CER) of 0.061, and BLEU score\nof 0.737 on diacritically-rich texts. Qari-OCR demonstrates superior handling\nof tashkeel, diverse fonts, and document layouts, alongside impressive\nperformance on low-resolution images. Further explorations (QARI v0.3) showcase\nstrong potential for structural document understanding and handwritten text.\nThis work delivers a marked improvement in Arabic OCR accuracy and efficiency,\nwith all models and datasets released to foster further research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02295.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628f7a71dd993507cfcbe587",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
            "fullname": "Omartificial Intelligence Space",
            "name": "Omartificial-Intelligence-Space",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 100
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.01565",
            "authors": [
                {
                    "_id": "684009f8a33aeee1125b5765",
                    "name": "Li Zhou",
                    "hidden": false
                },
                {
                    "_id": "684009f8a33aeee1125b5766",
                    "name": "Lutong Yu",
                    "hidden": false
                },
                {
                    "_id": "684009f8a33aeee1125b5767",
                    "name": "Dongchu Xie",
                    "hidden": false
                },
                {
                    "_id": "684009f8a33aeee1125b5768",
                    "name": "Shaohuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "684009f8a33aeee1125b5769",
                    "name": "Wenyan Li",
                    "hidden": false
                },
                {
                    "_id": "684009f8a33aeee1125b576a",
                    "name": "Haizhou Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T11:43:46.000Z",
            "submittedOnDailyAt": "2025-06-04T07:35:11.966Z",
            "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation",
            "submittedOnDailyBy": {
                "_id": "619b506f70d03780cbec5806",
                "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
                "isPro": false,
                "fullname": "wenyan li",
                "user": "lyan62",
                "type": "user"
            },
            "summary": "Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.",
            "upvotes": 2,
            "discussionId": "684009faa33aeee1125b57bd",
            "githubRepo": "https://github.com/lizhou21/TemporalCulture",
            "ai_summary": "Hanfu-Bench, a new expert-curated dataset, addresses temporal aspects of cultural understanding and creative adaptation using vision-language models, highlighting significant challenges in these tasks.",
            "ai_keywords": [
                "vision-language models",
                "multimodal dataset",
                "cultural visual understanding",
                "cultural image transcreation",
                "visual question answering",
                "cultural element inheritance",
                "modern context adaptation"
            ]
        },
        "publishedAt": "2025-06-02T07:43:46.000Z",
        "title": "Hanfu-Bench: A Multimodal Benchmark on Cross-Temporal Cultural\n  Understanding and Transcreation",
        "summary": "Culture is a rich and dynamic domain that evolves across both geography and\ntime. However, existing studies on cultural understanding with vision-language\nmodels (VLMs) primarily emphasize geographic diversity, often overlooking the\ncritical temporal dimensions. To bridge this gap, we introduce Hanfu-Bench, a\nnovel, expert-curated multimodal dataset. Hanfu, a traditional garment spanning\nancient Chinese dynasties, serves as a representative cultural heritage that\nreflects the profound temporal aspects of Chinese culture while remaining\nhighly popular in Chinese contemporary society. Hanfu-Bench comprises two core\ntasks: cultural visual understanding and cultural image transcreation.The\nformer task examines temporal-cultural feature recognition based on single- or\nmulti-image inputs through multiple-choice visual question answering, while the\nlatter focuses on transforming traditional attire into modern designs through\ncultural element inheritance and modern context adaptation. Our evaluation\nshows that closed VLMs perform comparably to non-experts on visual cutural\nunderstanding but fall short by 10\\% to human experts, while open VLMs lags\nfurther behind non-experts. For the transcreation task, multi-faceted human\nevaluation indicates that the best-performing model achieves a success rate of\nonly 42\\%. Our benchmark provides an essential testbed, revealing significant\nchallenges in this new direction of temporal cultural understanding and\ncreative adaptation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01565.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "619b506f70d03780cbec5806",
            "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
            "fullname": "wenyan li",
            "name": "lyan62",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.16994",
            "authors": [
                {
                    "_id": "683fb5d3a09aefea70733fa3",
                    "user": {
                        "_id": "64e14c5b12a5504dda70e60d",
                        "avatarUrl": "/avatars/944b486bb037364ef7d9d2c826526708.svg",
                        "isPro": false,
                        "fullname": "Runyang",
                        "user": "dd101bb",
                        "type": "user"
                    },
                    "name": "Runyang You",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-04T07:34:53.226Z",
                    "hidden": false
                },
                {
                    "_id": "683fb5d3a09aefea70733fa4",
                    "user": {
                        "_id": "674038313ccfb67446ae2b35",
                        "avatarUrl": "/avatars/8a3c0fdf971363988731f9eb8b13658c.svg",
                        "isPro": false,
                        "fullname": "tensorslow",
                        "user": "tensorslow",
                        "type": "user"
                    },
                    "name": "Yongqi Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-04T02:56:21.008Z",
                    "hidden": false
                },
                {
                    "_id": "683fb5d3a09aefea70733fa5",
                    "name": "Xinyu Lin",
                    "hidden": false
                },
                {
                    "_id": "683fb5d3a09aefea70733fa6",
                    "user": {
                        "_id": "63b6dbc8ccebeadccc888456",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
                        "isPro": false,
                        "fullname": "Xin Zhang",
                        "user": "izhx",
                        "type": "user"
                    },
                    "name": "Xin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:55:59.107Z",
                    "hidden": false
                },
                {
                    "_id": "683fb5d3a09aefea70733fa7",
                    "name": "Wenjie Wang",
                    "hidden": false
                },
                {
                    "_id": "683fb5d3a09aefea70733fa8",
                    "name": "Wenjie Li",
                    "hidden": false
                },
                {
                    "_id": "683fb5d3a09aefea70733fa9",
                    "name": "Liqiang Nie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-22T17:55:43.000Z",
            "submittedOnDailyAt": "2025-06-04T01:27:54.567Z",
            "title": "R^2ec: Towards Large Recommender Models with Reasoning",
            "submittedOnDailyBy": {
                "_id": "63b6dbc8ccebeadccc888456",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
                "isPro": false,
                "fullname": "Xin Zhang",
                "user": "izhx",
                "type": "user"
            },
            "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
            "upvotes": 2,
            "discussionId": "683fb5d5a09aefea70734001",
            "githubRepo": "https://github.com/YRYangang/RRec",
            "ai_summary": "A unified large recommender model with intrinsic reasoning capabilities is proposed, facilitating interleaved reasoning and recommendation using a reinforcement learning framework called RecPO.",
            "ai_keywords": [
                "recommender models",
                "LLMs",
                "intrinsic reasoning",
                "autoregressive process",
                "reinforcement learning",
                "RecPO",
                "fused reward scheme",
                "Hit@5",
                "NDCG@20"
            ]
        },
        "publishedAt": "2025-05-22T13:55:43.000Z",
        "title": "R^2ec: Towards Large Recommender Models with Reasoning",
        "summary": "Large recommender models have extended LLMs as powerful recommenders via\nencoding or item generation, and recent breakthroughs in LLM reasoning\nsynchronously motivate the exploration of reasoning in recommendation. Current\nstudies usually position LLMs as external reasoning modules to yield auxiliary\nthought for augmenting conventional recommendation pipelines. However, such\ndecoupled designs are limited in significant resource cost and suboptimal joint\noptimization. To address these issues, we propose \\name, a unified large\nrecommender model with intrinsic reasoning capabilities. Initially, we\nreconceptualize the model architecture to facilitate interleaved reasoning and\nrecommendation in the autoregressive process. Subsequently, we propose RecPO, a\ncorresponding reinforcement learning framework that optimizes \\name\\ both the\nreasoning and recommendation capabilities simultaneously in a single policy\nupdate; RecPO introduces a fused reward scheme that solely leverages\nrecommendation labels to simulate the reasoning capability, eliminating\ndependency on specialized reasoning annotations. Experiments on three datasets\nwith various baselines verify the effectiveness of \\name, showing relative\nimprovements of 68.67\\% in Hit@5 and 45.21\\% in NDCG@20. Code available at\nhttps://github.com/YRYangang/RRec.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16994.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b6dbc8ccebeadccc888456",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673396893898-63b6dbc8ccebeadccc888456.jpeg",
            "fullname": "Xin Zhang",
            "name": "izhx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03144",
            "authors": [
                {
                    "_id": "684015f08bd5bff99191dff4",
                    "user": {
                        "_id": "644b71ddb2e7823a76abcf91",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
                        "isPro": false,
                        "fullname": "zhou wei",
                        "user": "WeiChow",
                        "type": "user"
                    },
                    "name": "Wei Chow",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T09:56:45.423Z",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dff5",
                    "name": "Yuan Gao",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dff6",
                    "name": "Linfeng Li",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dff7",
                    "name": "Xian Wang",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dff8",
                    "name": "Qi Xu",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dff9",
                    "name": "Hang Song",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dffa",
                    "name": "Lingdong Kong",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dffb",
                    "name": "Ran Zhou",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dffc",
                    "name": "Yi Zeng",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dffd",
                    "name": "Yidong Cai",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dffe",
                    "name": "Botian Jiang",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191dfff",
                    "name": "Shilin Xu",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191e000",
                    "name": "Jiajun Zhang",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191e001",
                    "name": "Minghui Qiu",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191e002",
                    "name": "Xiangtai Li",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191e003",
                    "name": "Tianshu Yang",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191e004",
                    "name": "Siliang Tang",
                    "hidden": false
                },
                {
                    "_id": "684015f08bd5bff99191e005",
                    "name": "Juncheng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:59:14.000Z",
            "submittedOnDailyAt": "2025-06-04T08:16:40.362Z",
            "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query",
            "submittedOnDailyBy": {
                "_id": "644b71ddb2e7823a76abcf91",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
                "isPro": false,
                "fullname": "zhou wei",
                "user": "WeiChow",
                "type": "user"
            },
            "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.",
            "upvotes": 1,
            "discussionId": "684015f38bd5bff99191e158",
            "projectPage": "https://merit-2025.github.io/",
            "ai_summary": "MERIT, a multilingual dataset for interleaved multi-condition semantic retrieval, highlights limitations in existing models and introduces Coral, a fine-tuning framework integrating embedding reconstruction and contrastive learning to improve retrieval performance.",
            "ai_keywords": [
                "MULTILINGUAL dataset",
                "interleaved multi-condition semantic retrieval",
                "pre-trained MLLMs",
                "embedding reconstruction",
                "contrastive learning",
                "retrieval benchmarks"
            ]
        },
        "publishedAt": "2025-06-03T13:59:14.000Z",
        "title": "MERIT: Multilingual Semantic Retrieval with Interleaved Multi-Condition\n  Query",
        "summary": "Semantic retrieval is crucial for modern applications yet remains\nunderexplored in current research. Existing datasets are limited to single\nlanguages, single images, or singular retrieval conditions, often failing to\nfully exploit the expressive capacity of visual information as evidenced by\nmaintained performance when images are replaced with captions. However,\npractical retrieval scenarios frequently involve interleaved multi-condition\nqueries with multiple images. Hence, this paper introduces MERIT, the first\nmultilingual dataset for interleaved multi-condition semantic retrieval,\ncomprising 320,000 queries with 135,000 products in 5 languages, covering 7\ndistinct product categories. Extensive experiments on MERIT identify existing\nmodels's limitation: focusing solely on global semantic information while\nneglecting specific conditional elements in queries. Consequently, we propose\nCoral, a novel fine-tuning framework that adapts pre-trained MLLMs by\nintegrating embedding reconstruction to preserve fine-grained conditional\nelements and contrastive learning to extract comprehensive global semantics.\nExperiments demonstrate that Coral achieves a 45.9% performance improvement\nover conventional approaches on MERIT, with strong generalization capabilities\nvalidated across 8 established retrieval benchmarks. Collectively, our\ncontributions - a novel dataset, identification of critical limitations in\nexisting approaches, and an innovative fine-tuning framework - establish a\nfoundation for future research in interleaved multi-condition semantic\nretrieval.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03144.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644b71ddb2e7823a76abcf91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644b71ddb2e7823a76abcf91/JPF7Eqeq2jx8i79nQ962K.jpeg",
            "fullname": "zhou wei",
            "name": "WeiChow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03107",
            "authors": [
                {
                    "_id": "683ff7f54f32bd7bbca62163",
                    "user": {
                        "_id": "64a5d8219f3b568c202b3137",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a5d8219f3b568c202b3137/TI20Z1lWHzZpLsMkayDbU.png",
                        "isPro": true,
                        "fullname": "Di Chang",
                        "user": "Boese0601",
                        "type": "user"
                    },
                    "name": "Di Chang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-04T07:38:32.749Z",
                    "hidden": false
                },
                {
                    "_id": "683ff7f54f32bd7bbca62164",
                    "name": "Mingdeng Cao",
                    "hidden": false
                },
                {
                    "_id": "683ff7f54f32bd7bbca62165",
                    "name": "Yichun Shi",
                    "hidden": false
                },
                {
                    "_id": "683ff7f54f32bd7bbca62166",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "683ff7f54f32bd7bbca62167",
                    "name": "Shengqu Cai",
                    "hidden": false
                },
                {
                    "_id": "683ff7f54f32bd7bbca62168",
                    "name": "Shijie Zhou",
                    "hidden": false
                },
                {
                    "_id": "683ff7f54f32bd7bbca62169",
                    "name": "Weilin Huang",
                    "hidden": false
                },
                {
                    "_id": "683ff7f54f32bd7bbca6216a",
                    "name": "Gordon Wetzstein",
                    "hidden": false
                },
                {
                    "_id": "683ff7f54f32bd7bbca6216b",
                    "name": "Mohammad Soleymani",
                    "hidden": false
                },
                {
                    "_id": "683ff7f54f32bd7bbca6216c",
                    "user": {
                        "_id": "619c4a5cb392787f0f3ead53",
                        "avatarUrl": "/avatars/e62bc6fed6e9cbe4e0339e050cc3be5f.svg",
                        "isPro": false,
                        "fullname": "peng  wang",
                        "user": "Peng-Wang",
                        "type": "user"
                    },
                    "name": "Peng Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-04T07:38:32.749Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a5d8219f3b568c202b3137/0fIqE16TjZcIVIVAJcB0A.jpeg"
            ],
            "publishedAt": "2025-06-03T17:39:47.000Z",
            "submittedOnDailyAt": "2025-06-04T23:56:06.604Z",
            "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid\n  Motions",
            "submittedOnDailyBy": {
                "_id": "64a5d8219f3b568c202b3137",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a5d8219f3b568c202b3137/TI20Z1lWHzZpLsMkayDbU.png",
                "isPro": true,
                "fullname": "Di Chang",
                "user": "Boese0601",
                "type": "user"
            },
            "summary": "Editing images with instructions to reflect non-rigid motions, camera\nviewpoint shifts, object deformations, human articulations, and complex\ninteractions, poses a challenging yet underexplored problem in computer vision.\nExisting approaches and datasets predominantly focus on static scenes or rigid\ntransformations, limiting their capacity to handle expressive edits involving\ndynamic motion. To address this gap, we introduce ByteMorph, a comprehensive\nframework for instruction-based image editing with an emphasis on non-rigid\nmotions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong\nbaseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.\nByteMorph-6M includes over 6 million high-resolution image editing pairs for\ntraining, along with a carefully curated evaluation benchmark ByteMorph-Bench.\nBoth capture a wide variety of non-rigid motion types across diverse\nenvironments, human figures, and object categories. The dataset is constructed\nusing motion-guided data generation, layered compositing techniques, and\nautomated captioning to ensure diversity, realism, and semantic coherence. We\nfurther conduct a comprehensive evaluation of recent instruction-based image\nediting methods from both academic and commercial domains.",
            "upvotes": 1,
            "discussionId": "683ff7f84f32bd7bbca62230",
            "projectPage": "https://boese0601.github.io/bytemorph",
            "githubRepo": "https://github.com/ByteDance-Seed/BM-code",
            "ai_summary": "ByteMorph, a framework using the Diffusion Transformer, addresses non-rigid motion in image editing with a large-scale dataset and comprehensive evaluation.",
            "ai_keywords": [
                "Diffusion Transformer",
                "ByteMorph",
                "ByteMorph-6M",
                "ByteMorpher",
                "ByteMorph-Bench",
                "motion-guided data generation",
                "layered compositing",
                "automated captioning",
                "instruction-based image editing"
            ]
        },
        "publishedAt": "2025-06-03T13:39:47.000Z",
        "title": "ByteMorph: Benchmarking Instruction-Guided Image Editing with Non-Rigid\n  Motions",
        "summary": "Editing images with instructions to reflect non-rigid motions, camera\nviewpoint shifts, object deformations, human articulations, and complex\ninteractions, poses a challenging yet underexplored problem in computer vision.\nExisting approaches and datasets predominantly focus on static scenes or rigid\ntransformations, limiting their capacity to handle expressive edits involving\ndynamic motion. To address this gap, we introduce ByteMorph, a comprehensive\nframework for instruction-based image editing with an emphasis on non-rigid\nmotions. ByteMorph comprises a large-scale dataset, ByteMorph-6M, and a strong\nbaseline model built upon the Diffusion Transformer (DiT), named ByteMorpher.\nByteMorph-6M includes over 6 million high-resolution image editing pairs for\ntraining, along with a carefully curated evaluation benchmark ByteMorph-Bench.\nBoth capture a wide variety of non-rigid motion types across diverse\nenvironments, human figures, and object categories. The dataset is constructed\nusing motion-guided data generation, layered compositing techniques, and\nautomated captioning to ensure diversity, realism, and semantic coherence. We\nfurther conduct a comprehensive evaluation of recent instruction-based image\nediting methods from both academic and commercial domains.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a5d8219f3b568c202b3137/0fIqE16TjZcIVIVAJcB0A.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03107.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a5d8219f3b568c202b3137",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a5d8219f3b568c202b3137/TI20Z1lWHzZpLsMkayDbU.png",
            "fullname": "Di Chang",
            "name": "Boese0601",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02281",
            "authors": [
                {
                    "_id": "6840c71b2361fd58c84588b1",
                    "name": "Qinsi Wang",
                    "hidden": false
                },
                {
                    "_id": "6840c71b2361fd58c84588b2",
                    "name": "Jinghan Ke",
                    "hidden": false
                },
                {
                    "_id": "6840c71b2361fd58c84588b3",
                    "name": "Hancheng Ye",
                    "hidden": false
                },
                {
                    "_id": "6840c71b2361fd58c84588b4",
                    "name": "Yueqian Lin",
                    "hidden": false
                },
                {
                    "_id": "6840c71b2361fd58c84588b5",
                    "name": "Yuzhe Fu",
                    "hidden": false
                },
                {
                    "_id": "6840c71b2361fd58c84588b6",
                    "name": "Jianyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6840c71b2361fd58c84588b7",
                    "name": "Kurt Keutzer",
                    "hidden": false
                },
                {
                    "_id": "6840c71b2361fd58c84588b8",
                    "name": "Chenfeng Xu",
                    "hidden": false
                },
                {
                    "_id": "6840c71b2361fd58c84588b9",
                    "name": "Yiran Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T21:40:38.000Z",
            "submittedOnDailyAt": "2025-06-04T20:52:56.126Z",
            "title": "Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's\n  Own Signals",
            "submittedOnDailyBy": {
                "_id": "66128e8b7e0e7a64652dbbdf",
                "avatarUrl": "/avatars/b72ea5b14b55ff3af920c06b69a60b3f.svg",
                "isPro": false,
                "fullname": "Wang",
                "user": "Qinsi1",
                "type": "user"
            },
            "summary": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models\n(LLMs) suffer from sample inefficiency due to the redundant exposure of\nidentical queries under uniform data sampling. While previous work has explored\ncurriculum learning via heuristic difficulty metrics, these strategies exhibit\nlimitations by neglecting the intrinsic learning signals generated by the model\nitself, thus leading to suboptimal training regimes. In this paper, we identify\na model-inherent signal termed angle concentration that effectively reflects an\nLLM's capacity to learn from specific data. We theoretically and empirically\ndemonstrate a correlation between the angular distribution of token hidden\nstate vectors and the resulting gradient, revealing a learning preference for\ndata exhibiting higher angle concentration. Inspired by this finding, we\npropose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By\nleveraging the model's intrinsic angle concentration signal, GAIN-RL\ndynamically selects training data in each epoch, ensuring consistently\nimpactful gradient updates and thus significantly enhancing overall training\nefficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x\nacceleration in training efficiency across diverse mathematical and coding\ntasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient\nsampling yields data-efficient training, achieving better performance with half\nthe original data compared to vanilla GRPO with full training data. Code is\nrealsed at https://github.com/wangqinsi1/GAINRL/tree/main.",
            "upvotes": 1,
            "discussionId": "6840c71e2361fd58c845897d",
            "ai_summary": "GAIN-RL leverages angle concentration signals to improve training efficiency and data efficiency in Reinforcement Fine-tuning of Large Language Models.",
            "ai_keywords": [
                "Reinforcement Fine-tuning",
                "Large Language Models",
                "sample inefficiency",
                "curriculum learning",
                "angle concentration",
                "gradient updates",
                "token hidden state vectors",
                "GAIN-RL",
                "Gradient-driven Angle-Informed Navigated RL"
            ]
        },
        "publishedAt": "2025-06-02T17:40:38.000Z",
        "title": "Angles Don't Lie: Unlocking Training-Efficient RL Through the Model's\n  Own Signals",
        "summary": "Current Reinforcement Fine-tuning (RFT) paradigms for Large Language Models\n(LLMs) suffer from sample inefficiency due to the redundant exposure of\nidentical queries under uniform data sampling. While previous work has explored\ncurriculum learning via heuristic difficulty metrics, these strategies exhibit\nlimitations by neglecting the intrinsic learning signals generated by the model\nitself, thus leading to suboptimal training regimes. In this paper, we identify\na model-inherent signal termed angle concentration that effectively reflects an\nLLM's capacity to learn from specific data. We theoretically and empirically\ndemonstrate a correlation between the angular distribution of token hidden\nstate vectors and the resulting gradient, revealing a learning preference for\ndata exhibiting higher angle concentration. Inspired by this finding, we\npropose GAIN-RL, a Gradient-driven Angle-Informed Navigated RL framework. By\nleveraging the model's intrinsic angle concentration signal, GAIN-RL\ndynamically selects training data in each epoch, ensuring consistently\nimpactful gradient updates and thus significantly enhancing overall training\nefficiency. Empirical evaluations show that GAIN-RL (GRPO) achieves over a 2.5x\nacceleration in training efficiency across diverse mathematical and coding\ntasks and varying model scales. Furthermore, GAIN-RL (GRPO)'s efficient\nsampling yields data-efficient training, achieving better performance with half\nthe original data compared to vanilla GRPO with full training data. Code is\nrealsed at https://github.com/wangqinsi1/GAINRL/tree/main.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02281.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66128e8b7e0e7a64652dbbdf",
            "avatarUrl": "/avatars/b72ea5b14b55ff3af920c06b69a60b3f.svg",
            "fullname": "Wang",
            "name": "Qinsi1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.02138",
            "authors": [
                {
                    "_id": "683fe27c4f32bd7bbca087fc",
                    "name": "Yarden Bakish",
                    "hidden": false
                },
                {
                    "_id": "683fe27c4f32bd7bbca087fd",
                    "name": "Itamar Zimerman",
                    "hidden": false
                },
                {
                    "_id": "683fe27c4f32bd7bbca087fe",
                    "name": "Hila Chefer",
                    "hidden": false
                },
                {
                    "_id": "683fe27c4f32bd7bbca087ff",
                    "name": "Lior Wolf",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T18:07:55.000Z",
            "submittedOnDailyAt": "2025-06-04T04:51:22.070Z",
            "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability",
            "submittedOnDailyBy": {
                "_id": "65376feed325b3f02fb92c69",
                "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
                "isPro": false,
                "fullname": "Itamar Zimerman",
                "user": "ItamarZ",
                "type": "user"
            },
            "summary": "The development of effective explainability tools for Transformers is a\ncrucial pursuit in deep learning research. One of the most promising approaches\nin this domain is Layer-wise Relevance Propagation (LRP), which propagates\nrelevance scores backward through the network to the input space by\nredistributing activation values based on predefined rules. However, existing\nLRP-based methods for Transformer explainability entirely overlook a critical\ncomponent of the Transformer architecture: its positional encoding (PE),\nresulting in violation of the conservation property, and the loss of an\nimportant and unique type of relevance, which is also associated with\nstructural and positional features. To address this limitation, we reformulate\nthe input space for Transformer explainability as a set of position-token\npairs. This allows us to propose specialized theoretically-grounded LRP rules\ndesigned to propagate attributions across various positional encoding methods,\nincluding Rotary, Learnable, and Absolute PE. Extensive experiments with both\nfine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,\ndemonstrate that our method significantly outperforms the state-of-the-art in\nboth vision and NLP explainability tasks. Our code is publicly available.",
            "upvotes": 1,
            "discussionId": "683fe27d4f32bd7bbca08867",
            "githubRepo": "https://github.com/YardenBakish/PE-AWARE-LRP",
            "ai_summary": "A specialized LRP method for Transformer explainability considers positional encoding, improving relevance propagation and outperforming existing methods.",
            "ai_keywords": [
                "Layer-wise Relevance Propagation (LRP)",
                "Transformers",
                "positional encoding (PE)",
                "Rotary",
                "Learnable",
                "Absolute PE",
                "vision",
                "NLP explainability tasks"
            ]
        },
        "publishedAt": "2025-06-02T14:07:55.000Z",
        "title": "Revisiting LRP: Positional Attribution as the Missing Ingredient for\n  Transformer Explainability",
        "summary": "The development of effective explainability tools for Transformers is a\ncrucial pursuit in deep learning research. One of the most promising approaches\nin this domain is Layer-wise Relevance Propagation (LRP), which propagates\nrelevance scores backward through the network to the input space by\nredistributing activation values based on predefined rules. However, existing\nLRP-based methods for Transformer explainability entirely overlook a critical\ncomponent of the Transformer architecture: its positional encoding (PE),\nresulting in violation of the conservation property, and the loss of an\nimportant and unique type of relevance, which is also associated with\nstructural and positional features. To address this limitation, we reformulate\nthe input space for Transformer explainability as a set of position-token\npairs. This allows us to propose specialized theoretically-grounded LRP rules\ndesigned to propagate attributions across various positional encoding methods,\nincluding Rotary, Learnable, and Absolute PE. Extensive experiments with both\nfine-tuned classifiers and zero-shot foundation models, such as LLaMA 3,\ndemonstrate that our method significantly outperforms the state-of-the-art in\nboth vision and NLP explainability tasks. Our code is publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02138.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65376feed325b3f02fb92c69",
            "avatarUrl": "/avatars/e952918cf434d5302e9b1a404eccaf0e.svg",
            "fullname": "Itamar Zimerman",
            "name": "ItamarZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.01265",
            "authors": [
                {
                    "_id": "68406f484024b3c937eebb9b",
                    "name": "Do Xuan Long",
                    "hidden": false
                },
                {
                    "_id": "68406f484024b3c937eebb9c",
                    "name": "Duong Ngoc Yen",
                    "hidden": false
                },
                {
                    "_id": "68406f484024b3c937eebb9d",
                    "name": "Do Xuan Trong",
                    "hidden": false
                },
                {
                    "_id": "68406f484024b3c937eebb9e",
                    "name": "Luu Anh Tuan",
                    "hidden": false
                },
                {
                    "_id": "68406f484024b3c937eebb9f",
                    "name": "Kenji Kawaguchi",
                    "hidden": false
                },
                {
                    "_id": "68406f484024b3c937eebba0",
                    "name": "Shafiq Joty",
                    "hidden": false
                },
                {
                    "_id": "68406f484024b3c937eebba1",
                    "name": "Min-Yen Kan",
                    "hidden": false
                },
                {
                    "_id": "68406f484024b3c937eebba2",
                    "name": "Nancy F. Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T02:35:24.000Z",
            "submittedOnDailyAt": "2025-06-04T14:38:01.911Z",
            "title": "Beyond In-Context Learning: Aligning Long-form Generation of Large\n  Language Models via Task-Inherent Attribute Guidelines",
            "submittedOnDailyBy": {
                "_id": "63a9a0d13453852ef53c0b37",
                "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
                "isPro": false,
                "fullname": "Do Xuan Long",
                "user": "dxlong2000",
                "type": "user"
            },
            "summary": "In-context learning (ICL) is an important yet not fully understood ability of\npre-trained large language models (LLMs). It can greatly enhance task\nperformance using a few examples, termed demonstrations, without fine-tuning.\nAlthough effective in question answering, ICL often underperforms in long-form\ngeneration tasks such as summarization. Under appropriately realistic\nassumptions, we empirically and theoretically show that ICL demonstrations\nalone are insufficient to teach LLMs the task language and format distributions\nfor generation. We argue for explicit exposure to the task distributions and\nhypothesize that defining them by prompting enhances model performance. To this\nend, we present LongGuide, which efficiently generates two parallel streams of\nguidelines capturing task language and format properties: (i) Metric Guidelines\n(MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output\nConstraint Guidelines (OCGs) that constrain generation at both token and\nsentence levels. LongGuide automatically selects the best combination of\nguidelines, improving both strong open- and closed-source LLMs by over 5% in\nboth zero- and few-shot settings. We show that LongGuide is generalizable,\nlearnable by weak models to enhance strong ones, and integrates synergistically\nwith automatic prompt optimizers.",
            "upvotes": 1,
            "discussionId": "68406f4a4024b3c937eebc23",
            "ai_summary": "LongGuide enhances pre-trained LLMs in long-form generation tasks by providing metric and output constraint guidelines, improving model performance in zero- and few-shot settings.",
            "ai_keywords": [
                "in-context learning",
                "pre-trained large language models",
                "task performance",
                "demonstrations",
                "long-form generation",
                "summarization",
                "ICL",
                "task language",
                "format distributions",
                "Metric Guidelines",
                "Output Constraint Guidelines",
                "zero-shot",
                "few-shot"
            ]
        },
        "publishedAt": "2025-06-01T22:35:24.000Z",
        "title": "Beyond In-Context Learning: Aligning Long-form Generation of Large\n  Language Models via Task-Inherent Attribute Guidelines",
        "summary": "In-context learning (ICL) is an important yet not fully understood ability of\npre-trained large language models (LLMs). It can greatly enhance task\nperformance using a few examples, termed demonstrations, without fine-tuning.\nAlthough effective in question answering, ICL often underperforms in long-form\ngeneration tasks such as summarization. Under appropriately realistic\nassumptions, we empirically and theoretically show that ICL demonstrations\nalone are insufficient to teach LLMs the task language and format distributions\nfor generation. We argue for explicit exposure to the task distributions and\nhypothesize that defining them by prompting enhances model performance. To this\nend, we present LongGuide, which efficiently generates two parallel streams of\nguidelines capturing task language and format properties: (i) Metric Guidelines\n(MGs) that instruct models to optimize self-evaluated metrics; and (ii) Output\nConstraint Guidelines (OCGs) that constrain generation at both token and\nsentence levels. LongGuide automatically selects the best combination of\nguidelines, improving both strong open- and closed-source LLMs by over 5% in\nboth zero- and few-shot settings. We show that LongGuide is generalizable,\nlearnable by weak models to enhance strong ones, and integrates synergistically\nwith automatic prompt optimizers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01265.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a9a0d13453852ef53c0b37",
            "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
            "fullname": "Do Xuan Long",
            "name": "dxlong2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.24362",
            "authors": [
                {
                    "_id": "68401045dd25841d998788cc",
                    "user": {
                        "_id": "61fbe8d2c5e6410373a76b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
                        "isPro": false,
                        "fullname": "Anum Afzal",
                        "user": "anumafzal94",
                        "type": "user"
                    },
                    "name": "Anum Afzal",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-04T09:25:18.174Z",
                    "hidden": false
                },
                {
                    "_id": "68401045dd25841d998788cd",
                    "name": "Florian Matthes",
                    "hidden": false
                },
                {
                    "_id": "68401045dd25841d998788ce",
                    "user": {
                        "_id": "6493393f357b252af72196c5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6493393f357b252af72196c5/EWSy18XRcMRa_4XMM3Fu-.jpeg",
                        "isPro": false,
                        "fullname": "Gal Chechik",
                        "user": "galchechik",
                        "type": "user"
                    },
                    "name": "Gal Chechik",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-04T09:22:13.900Z",
                    "hidden": false
                },
                {
                    "_id": "68401045dd25841d998788cf",
                    "user": {
                        "_id": "66810e5877ed01ba880a4b40",
                        "avatarUrl": "/avatars/3068f4b16f03a51772e652d76b37f9c3.svg",
                        "isPro": false,
                        "fullname": "Yftah Ziser",
                        "user": "yziser",
                        "type": "user"
                    },
                    "name": "Yftah Ziser",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-04T09:22:13.900Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T08:54:28.000Z",
            "submittedOnDailyAt": "2025-06-04T07:54:00.025Z",
            "title": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion",
            "submittedOnDailyBy": {
                "_id": "61fbe8d2c5e6410373a76b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
                "isPro": false,
                "fullname": "Anum Afzal",
                "user": "anumafzal94",
                "type": "user"
            },
            "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well even before a\nsingle token is generated, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.",
            "upvotes": 1,
            "discussionId": "68401045dd25841d998788f9",
            "ai_summary": "A classifier based on LLM representations predicts successful zero-shot CoT processes before generating tokens, suggesting that key reasoning information is captured early, and early stopping can improve performance.",
            "ai_keywords": [
                "zero-shot Chain-of-Thought",
                "LLM representations",
                "BERT-based baseline",
                "shallow linguistic cues",
                "deeper reasoning dynamics",
                "early stopping experiments",
                "supervised learning",
                "reinforcement learning"
            ]
        },
        "publishedAt": "2025-05-30T04:54:28.000Z",
        "title": "Knowing Before Saying: LLM Representations Encode Information About\n  Chain-of-Thought Success Before Completion",
        "summary": "We investigate whether the success of a zero-shot Chain-of-Thought (CoT)\nprocess can be predicted before completion. We discover that a probing\nclassifier, based on LLM representations, performs well even before a\nsingle token is generated, suggesting that crucial information about the\nreasoning process is already present in the initial steps representations. In\ncontrast, a strong BERT-based baseline, which relies solely on the generated\ntokens, performs worse, likely because it depends on shallow linguistic cues\nrather than deeper reasoning dynamics. Surprisingly, using later reasoning\nsteps does not always improve classification. When additional context is\nunhelpful, earlier representations resemble later ones more, suggesting LLMs\nencode key information early. This implies reasoning can often stop early\nwithout loss. To test this, we conduct early stopping experiments, showing that\ntruncating CoT reasoning still improves performance over not using CoT at all,\nthough a gap remains compared to full reasoning. However, approaches like\nsupervised learning or reinforcement learning designed to shorten CoT chains\ncould leverage our classifier's guidance to identify when early stopping is\neffective. Our findings provide insights that may support such methods, helping\nto optimize CoT's efficiency while preserving its benefits.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24362.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61fbe8d2c5e6410373a76b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61fbe8d2c5e6410373a76b2a/m4yM6m2uagJxMselzEZfo.jpeg",
            "fullname": "Anum Afzal",
            "name": "anumafzal94",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00189",
            "authors": [
                {
                    "_id": "6840ee2f1d13437e974089ac",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "di-zhang-fdu",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-05T01:09:04.306Z",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089ad",
                    "name": "Weida Wang",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089ae",
                    "name": "Junxian Li",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089af",
                    "name": "Xunzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089b0",
                    "name": "Jiatong Li",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089b1",
                    "name": "Jianbo Wu",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089b2",
                    "name": "Jingdi Lei",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089b3",
                    "name": "Haonan He",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089b4",
                    "name": "Peng Ye",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089b5",
                    "name": "Shufei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089b6",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089b7",
                    "name": "Yuqiang Li",
                    "hidden": false
                },
                {
                    "_id": "6840ee2f1d13437e974089b8",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T19:59:44.000Z",
            "submittedOnDailyAt": "2025-06-04T23:39:14.540Z",
            "title": "Control-R: Towards controllable test-time scaling",
            "submittedOnDailyBy": {
                "_id": "64bce15bafd1e46c5504ad38",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
                "isPro": false,
                "fullname": "Di Zhang",
                "user": "di-zhang-fdu",
                "type": "user"
            },
            "summary": "This paper target in addressing the challenges of underthinking and\noverthinking in long chain-of-thought (CoT) reasoning for Large Reasoning\nModels (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time\napproach that injects structured control signals to guide reasoning from a tree\nsearch perspective. RCF enables models to adjust reasoning effort according to\ngiven control conditions when solving complex tasks. Additionally, we present\nthe Control-R-4K dataset, which consists of challenging problems annotated with\ndetailed reasoning processes and corresponding control fields. To further\nenhance reasoning control, we propose a Conditional Distillation Finetuning\n(CDF) method, which trains model--particularly Control-R-32B--to effectively\nadjust reasoning effort during test time. Experimental results on benchmarks\nsuch as AIME2024 and MATH500 demonstrate that our approach achieves\nstate-of-the-art performance at the 32B scale while enabling a controllable\nLong CoT reasoning process (L-CoT). Overall, this work introduces an effective\nparadigm for controllable test-time scaling reasoning.",
            "upvotes": 0,
            "discussionId": "6840ee301d13437e974089ed",
            "ai_summary": "A novel approach using Reasoning Control Fields (RCF) and Conditional Distillation Finetuning (CDF) enables controllable reasoning in Large Reasoning Models (LRMs) to achieve state-of-the-art performance on complex tasks.",
            "ai_keywords": [
                "Reasoning Control Fields",
                "RCF",
                "long chain-of-thought reasoning",
                "Large Reasoning Models",
                "LRMs",
                "Control-R-4K dataset",
                "Conditional Distillation Finetuning",
                "CDF",
                "AIME2024",
                "MATH500",
                "controllable Long CoT reasoning",
                "L-CoT"
            ]
        },
        "publishedAt": "2025-05-30T15:59:44.000Z",
        "title": "Control-R: Towards controllable test-time scaling",
        "summary": "This paper target in addressing the challenges of underthinking and\noverthinking in long chain-of-thought (CoT) reasoning for Large Reasoning\nModels (LRMs) by introducing Reasoning Control Fields (RCF)--a novel test-time\napproach that injects structured control signals to guide reasoning from a tree\nsearch perspective. RCF enables models to adjust reasoning effort according to\ngiven control conditions when solving complex tasks. Additionally, we present\nthe Control-R-4K dataset, which consists of challenging problems annotated with\ndetailed reasoning processes and corresponding control fields. To further\nenhance reasoning control, we propose a Conditional Distillation Finetuning\n(CDF) method, which trains model--particularly Control-R-32B--to effectively\nadjust reasoning effort during test time. Experimental results on benchmarks\nsuch as AIME2024 and MATH500 demonstrate that our approach achieves\nstate-of-the-art performance at the 32B scale while enabling a controllable\nLong CoT reasoning process (L-CoT). Overall, this work introduces an effective\nparadigm for controllable test-time scaling reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00189.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64bce15bafd1e46c5504ad38",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
            "fullname": "Di Zhang",
            "name": "di-zhang-fdu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 149
        },
        "isAuthorParticipating": true
    }
]