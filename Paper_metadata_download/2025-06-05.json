[
    {
        "paper": {
            "id": "2506.03569",
            "authors": [
                {
                    "_id": "6841003e45e7d8a890731765",
                    "name": "Xiaomi LLM-Core Team",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731767",
                    "user": {
                        "_id": "6517f0df593b3af3120d242e",
                        "avatarUrl": "/avatars/bc28ceb48b206502d8cf9e1c2e130066.svg",
                        "isPro": false,
                        "fullname": "Zihao Yue",
                        "user": "yuezih",
                        "type": "user"
                    },
                    "name": "Zihao Yue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T12:42:03.793Z",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731768",
                    "name": "Zhenru Lin",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731769",
                    "name": "Yifan Song",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073176a",
                    "name": "Weikun Wang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073176b",
                    "user": {
                        "_id": "60d2e681b8448e1785bbda06",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1624434302056-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Shuhuai Ren",
                        "user": "ShuhuaiRen",
                        "type": "user"
                    },
                    "name": "Shuhuai Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:27:00.497Z",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073176c",
                    "user": {
                        "_id": "642e72cec1b0f8e4e76af16d",
                        "avatarUrl": "/avatars/f900811d3c22a114c67283b646949f86.svg",
                        "isPro": false,
                        "fullname": "shuhao gu",
                        "user": "gsh33",
                        "type": "user"
                    },
                    "name": "Shuhao Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:27:04.948Z",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073176d",
                    "user": {
                        "_id": "64539588fc2b5f69e8faac76",
                        "avatarUrl": "/avatars/111441eeb0dd4d8ad2f0d3f28277952a.svg",
                        "isPro": false,
                        "fullname": "Li Shicheng",
                        "user": "lscpku",
                        "type": "user"
                    },
                    "name": "Shicheng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T12:42:01.493Z",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073176e",
                    "name": "Peidian Li",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073176f",
                    "user": {
                        "_id": "680caf5b11555c276d5bfa65",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/KQJTgmVZ1v3E4a9fQmbOp.png",
                        "isPro": false,
                        "fullname": "Liang Zhao",
                        "user": "zhao1iang-mimo",
                        "type": "user"
                    },
                    "name": "Liang Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T12:41:59.333Z",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731770",
                    "user": {
                        "_id": "6038d6d0612f5eef3cc05ea9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
                        "isPro": false,
                        "fullname": "Lei Li",
                        "user": "tobiaslee",
                        "type": "user"
                    },
                    "name": "Lei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:27:07.044Z",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731771",
                    "name": "Kainan Bao",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731772",
                    "name": "Hao Tian",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731773",
                    "name": "Hailin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731774",
                    "name": "Gang Wang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731775",
                    "user": {
                        "_id": "64d2fce8129a210e569e0c76",
                        "avatarUrl": "/avatars/a79a832dc3a46ece1b9e542369fc4888.svg",
                        "isPro": false,
                        "fullname": "Dawei Zhu",
                        "user": "dwzhu",
                        "type": "user"
                    },
                    "name": "Dawei Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:27:02.720Z",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731776",
                    "name": "Cici",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731777",
                    "name": "Chenhong He",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731778",
                    "name": "Bowen Ye",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731779",
                    "name": "Bowen Shen",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073177a",
                    "name": "Zihan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073177b",
                    "name": "Zihan Jiang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073177c",
                    "name": "Zhixian Zheng",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073177d",
                    "name": "Zhichao Song",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073177e",
                    "name": "Zhenbo Luo",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073177f",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731780",
                    "name": "Yudong Wang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731781",
                    "name": "Yuanyuan Tian",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731782",
                    "name": "Yu Tu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731783",
                    "name": "Yihan Yan",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731784",
                    "name": "Yi Huang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731785",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731786",
                    "name": "Xinzhe Xu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731787",
                    "name": "Xingchen Song",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731788",
                    "name": "Xing Zhang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731789",
                    "name": "Xing Yong",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073178a",
                    "name": "Xin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073178b",
                    "name": "Xiangwei Deng",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073178c",
                    "name": "Wenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073178d",
                    "name": "Wenhan Ma",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073178e",
                    "name": "Weiwei Lv",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073178f",
                    "name": "Weiji Zhuang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731790",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731791",
                    "name": "Sirui Deng",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731792",
                    "name": "Shuo Liu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731793",
                    "name": "Shimao Chen",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731794",
                    "name": "Shihua Yu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731795",
                    "name": "Shaohui Liu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731796",
                    "name": "Shande Wang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731797",
                    "name": "Rui Ma",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731798",
                    "name": "Qiantong Wang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a890731799",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073179a",
                    "name": "Nuo Chen",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073179b",
                    "name": "Menghang Zhu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073179c",
                    "name": "Kangyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073179d",
                    "name": "Kang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073179e",
                    "name": "Kai Fang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a89073179f",
                    "name": "Jun Shi",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a0",
                    "name": "Jinhao Dong",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a1",
                    "name": "Jiebao Xiao",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a2",
                    "name": "Jiaming Xu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a3",
                    "user": {
                        "_id": "680e9a219e529f779991be0c",
                        "avatarUrl": "/avatars/327b945649192b0881fe290298d10e23.svg",
                        "isPro": false,
                        "fullname": "Huaqiu Liu",
                        "user": "Prestonprom",
                        "type": "user"
                    },
                    "name": "Huaqiu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:58.279Z",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a4",
                    "name": "Hongshen Xu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a5",
                    "name": "Heng Qu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a6",
                    "name": "Haochen Zhao",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a7",
                    "name": "Hanglong Lv",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a8",
                    "name": "Guoan Wang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317a9",
                    "name": "Duo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317aa",
                    "name": "Dong Zhang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317ab",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317ac",
                    "name": "Chong Ma",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317ad",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317ae",
                    "name": "Can Cai",
                    "hidden": false
                },
                {
                    "_id": "6841003e45e7d8a8907317af",
                    "name": "Bingquan Xia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T04:32:54.000Z",
            "submittedOnDailyAt": "2025-06-05T00:57:27.734Z",
            "title": "MiMo-VL Technical Report",
            "submittedOnDailyBy": {
                "_id": "6038d6d0612f5eef3cc05ea9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
                "isPro": false,
                "fullname": "Lei Li",
                "user": "tobiaslee",
                "type": "user"
            },
            "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
            "upvotes": 58,
            "discussionId": "6841004145e7d8a890731853",
            "githubRepo": "https://github.com/XiaomiMiMo/MiMo-VL",
            "ai_summary": "MiMo-VL-7B-SFT and MiMo-VL-7B-RL provide state-of-the-art general visual understanding and multimodal reasoning through four-stage pre-training and Mixed On-policy Reinforcement Learning, outperforming models with up to 78B parameters.",
            "ai_keywords": [
                "vision-language models",
                "multimodal reasoning",
                "four-stage pre-training",
                "Mixed On-policy Reinforcement Learning",
                "MORL",
                "Chain-of-Thought",
                "reproducibility"
            ]
        },
        "publishedAt": "2025-06-04T00:32:54.000Z",
        "title": "MiMo-VL Technical Report",
        "summary": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language\nmodels delivering state-of-the-art performance in both general visual\nunderstanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B\non 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing\nmodels with up to 78B parameters. For GUI grounding applications, it sets a new\nstandard with 56.1 on OSWorld-G, even outperforming specialized models such as\nUI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens)\nwith Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward\nsignals. We identify the importance of incorporating high-quality reasoning\ndata with long Chain-of-Thought into pre-training stages, and the benefits of\nmixed RL despite challenges in simultaneous multi-domain optimization. We also\ncontribute a comprehensive evaluation suite covering 50+ tasks to promote\nreproducibility and advance the field. The model checkpoints and full\nevaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03569.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6038d6d0612f5eef3cc05ea9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6038d6d0612f5eef3cc05ea9/ryhvAX5djQpD5OrIlZQ1f.jpeg",
            "fullname": "Lei Li",
            "name": "tobiaslee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04207",
            "authors": [
                {
                    "_id": "684117e22db29aa7b403af8d",
                    "name": "Shuang Chen",
                    "hidden": false
                },
                {
                    "_id": "684117e22db29aa7b403af8e",
                    "name": "Yue Guo",
                    "hidden": false
                },
                {
                    "_id": "684117e22db29aa7b403af8f",
                    "user": {
                        "_id": "64264095ba51f8a2136946a0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64264095ba51f8a2136946a0/FR33boVpkDXcrvGMBmprF.jpeg",
                        "isPro": false,
                        "fullname": "Zhaochen Su",
                        "user": "Warrieryes",
                        "type": "user"
                    },
                    "name": "Zhaochen Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:45.759Z",
                    "hidden": false
                },
                {
                    "_id": "684117e22db29aa7b403af90",
                    "name": "Yafu Li",
                    "hidden": false
                },
                {
                    "_id": "684117e22db29aa7b403af91",
                    "name": "Yulun Wu",
                    "hidden": false
                },
                {
                    "_id": "684117e22db29aa7b403af92",
                    "user": {
                        "_id": "65352acb7139c5dd8d9a8590",
                        "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
                        "isPro": false,
                        "fullname": "JiachengChen",
                        "user": "JC-Chen",
                        "type": "user"
                    },
                    "name": "Jiacheng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:48:38.463Z",
                    "hidden": false
                },
                {
                    "_id": "684117e22db29aa7b403af93",
                    "name": "Jiayu Chen",
                    "hidden": false
                },
                {
                    "_id": "684117e22db29aa7b403af94",
                    "name": "Weijie Wang",
                    "hidden": false
                },
                {
                    "_id": "684117e22db29aa7b403af95",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "684117e22db29aa7b403af96",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T17:51:08.000Z",
            "submittedOnDailyAt": "2025-06-05T02:38:24.366Z",
            "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "65352acb7139c5dd8d9a8590",
                "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
                "isPro": false,
                "fullname": "JiachengChen",
                "user": "JC-Chen",
                "type": "user"
            },
            "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
            "upvotes": 40,
            "discussionId": "684117e32db29aa7b403afc2",
            "githubRepo": "https://github.com/CSfufu/Revisual-R1"
        },
        "publishedAt": "2025-06-04T13:51:08.000Z",
        "title": "Advancing Multimodal Reasoning: From Optimized Cold Start to Staged\n  Reinforcement Learning",
        "summary": "Inspired by the remarkable reasoning capabilities of Deepseek-R1 in complex\ntextual tasks, many works attempt to incentivize similar capabilities in\nMultimodal Large Language Models (MLLMs) by directly applying reinforcement\nlearning (RL). However, they still struggle to activate complex reasoning. In\nthis paper, rather than examining multimodal RL in isolation, we delve into\ncurrent training pipelines and identify three crucial phenomena: 1) Effective\ncold start initialization is critical for enhancing MLLM reasoning.\nIntriguingly, we find that initializing with carefully selected text data alone\ncan lead to performance surpassing many recent multimodal reasoning models,\neven before multimodal RL. 2) Standard GRPO applied to multimodal RL suffers\nfrom gradient stagnation, which degrades training stability and performance. 3)\nSubsequent text-only RL training, following the multimodal RL phase, further\nenhances multimodal reasoning. This staged training approach effectively\nbalances perceptual grounding and cognitive reasoning development. By\nincorporating the above insights and addressing multimodal RL issues, we\nintroduce ReVisual-R1, achieving a new state-of-the-art among open-source 7B\nMLLMs on challenging benchmarks including MathVerse, MathVision, WeMath,\nLogicVista, DynaMath, and challenging AIME2024 and AIME2025.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04207.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "65352acb7139c5dd8d9a8590",
            "avatarUrl": "/avatars/e2ff22b596aee45cdfb8f68dc15572f9.svg",
            "fullname": "JiachengChen",
            "name": "JC-Chen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04089",
            "authors": [
                {
                    "_id": "684153cf911d1b3135fa5dfe",
                    "name": "Anastasiia Ivanova",
                    "hidden": false
                },
                {
                    "_id": "684153cf911d1b3135fa5dff",
                    "user": {
                        "_id": "661af24d8328f43c6abc2d11",
                        "avatarUrl": "/avatars/afe7eaf1f7a378dbcdba5cd3e86adf9c.svg",
                        "isPro": false,
                        "fullname": "Eva",
                        "user": "tenebrissilvam",
                        "type": "user"
                    },
                    "name": "Eva Bakaeva",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T10:00:02.435Z",
                    "hidden": false
                },
                {
                    "_id": "684153cf911d1b3135fa5e00",
                    "user": {
                        "_id": "64198f70ed725fef6442b37e",
                        "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
                        "isPro": false,
                        "fullname": "Alexey Kovalev",
                        "user": "AlexeyKov",
                        "type": "user"
                    },
                    "name": "Zoya Volovikova",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-05T08:22:39.926Z",
                    "hidden": false
                },
                {
                    "_id": "684153cf911d1b3135fa5e01",
                    "name": "Alexey K. Kovalev",
                    "hidden": false
                },
                {
                    "_id": "684153cf911d1b3135fa5e02",
                    "name": "Aleksandr I. Panov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T15:47:07.000Z",
            "submittedOnDailyAt": "2025-06-05T07:08:16.935Z",
            "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
            "submittedOnDailyBy": {
                "_id": "64198f70ed725fef6442b37e",
                "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
                "isPro": false,
                "fullname": "Alexey Kovalev",
                "user": "AlexeyKov",
                "type": "user"
            },
            "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.",
            "upvotes": 39,
            "discussionId": "684153cf911d1b3135fa5e2e",
            "ai_summary": "AmbiK, a textual dataset of ambiguous instructions for kitchen robots, enables unified comparison of ambiguity detection methods.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "behavior planning",
                "ambiguous instructions",
                "task ambiguity detection",
                "AmbiK",
                "dataset",
                "human-validated",
                "ambiguity types",
                "Human Preferences",
                "Common Sense Knowledge",
                "Safety",
                "environment descriptions",
                "clarifying questions",
                "user intents",
                "task plans"
            ]
        },
        "publishedAt": "2025-06-04T11:47:07.000Z",
        "title": "AmbiK: Dataset of Ambiguous Tasks in Kitchen Environment",
        "summary": "As a part of an embodied agent, Large Language Models (LLMs) are typically\nused for behavior planning given natural language instructions from the user.\nHowever, dealing with ambiguous instructions in real-world environments remains\na challenge for LLMs. Various methods for task ambiguity detection have been\nproposed. However, it is difficult to compare them because they are tested on\ndifferent datasets and there is no universal benchmark. For this reason, we\npropose AmbiK (Ambiguous Tasks in Kitchen Environment), the fully textual\ndataset of ambiguous instructions addressed to a robot in a kitchen\nenvironment. AmbiK was collected with the assistance of LLMs and is\nhuman-validated. It comprises 1000 pairs of ambiguous tasks and their\nunambiguous counterparts, categorized by ambiguity type (Human Preferences,\nCommon Sense Knowledge, Safety), with environment descriptions, clarifying\nquestions and answers, user intents, and task plans, for a total of 2000 tasks.\nWe hope that AmbiK will enable researchers to perform a unified comparison of\nambiguity detection methods. AmbiK is available at\nhttps://github.com/cog-model/AmbiK-dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04089.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64198f70ed725fef6442b37e",
            "avatarUrl": "/avatars/580ab07a3067a9deb2977b0894226fe3.svg",
            "fullname": "Alexey Kovalev",
            "name": "AlexeyKov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.16968",
            "authors": [
                {
                    "_id": "683656aefd55e753bf26ed3e",
                    "user": {
                        "_id": "656864e12d73834278a8dea7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                        "isPro": true,
                        "fullname": "Ahmed Heakl",
                        "user": "ahmedheakl",
                        "type": "user"
                    },
                    "name": "Ahmed Heakl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-05-28T08:58:30.760Z",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed3f",
                    "user": {
                        "_id": "62676a94dacab364889bb36c",
                        "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
                        "isPro": false,
                        "fullname": "SARIM HASHMI",
                        "user": "Sarim-Hash",
                        "type": "user"
                    },
                    "name": "Sarim Hashmi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:49:01.879Z",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed40",
                    "user": {
                        "_id": "62eaadf4086bd1debb30a122",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62eaadf4086bd1debb30a122/wgxsPVnkOuEfq1oqlUhiB.jpeg",
                        "isPro": false,
                        "fullname": "Gustavo Stahl",
                        "user": "GustavoStahl",
                        "type": "user"
                    },
                    "name": "Gustavo Bertolo Stahl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T08:31:48.782Z",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed41",
                    "name": "Seung Hun Eddie Han",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed42",
                    "name": "Salman Khan",
                    "hidden": false
                },
                {
                    "_id": "683656aefd55e753bf26ed43",
                    "name": "Abdulrahman Mahmoud",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
            ],
            "publishedAt": "2025-05-22T17:48:53.000Z",
            "submittedOnDailyAt": "2025-06-05T06:33:02.615Z",
            "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
            "submittedOnDailyBy": {
                "_id": "656864e12d73834278a8dea7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                "isPro": true,
                "fullname": "Ahmed Heakl",
                "user": "ahmedheakl",
                "type": "user"
            },
            "summary": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\nleftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train the CASS family of domain-specific language\nmodels, achieving 95% source translation accuracy and 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, and Hipify. Our generated code matches native performance in\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress in GPU compiler tooling, binary\ncompatibility, and LLM-guided hardware translation. Dataset and benchmark are\non\nhttps://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}},\nwith code at\nhttps://github.com/GustavoStahl/CASS{blue{GitHub}}.",
            "upvotes": 35,
            "discussionId": "683656b0fd55e753bf26edf7",
            "githubRepo": "https://github.com/GustavoStahl/CASS",
            "ai_summary": "CASS is a dataset and model suite for GPU code transpilation at both source and assembly levels, achieving high accuracy and performance matching with native code.",
            "ai_keywords": [
                "cross-architecture GPU code transpilation",
                "CASS",
                "CUDA",
                "HIP",
                "Nvidia SASS",
                "AMD RDNA3",
                "domain-specific language models",
                "source translation accuracy",
                "assembly translation accuracy",
                "native performance",
                "CASS-Bench",
                "GPU compiler tooling",
                "binary compatibility",
                "LLM-guided hardware translation"
            ]
        },
        "publishedAt": "2025-05-22T13:48:53.000Z",
        "title": "CASS: Nvidia to AMD Transpilation with Data, Models, and Benchmark",
        "summary": "We introduce CASS, the first large-scale dataset and model suite for\ncross-architecture GPU code transpilation, targeting both source-level (CUDA\nleftrightarrow HIP) and assembly-level (Nvidia SASS leftrightarrow AMD\nRDNA3) translation. The dataset comprises 70k verified code pairs across host\nand device, addressing a critical gap in low-level GPU code portability.\nLeveraging this resource, we train the CASS family of domain-specific language\nmodels, achieving 95% source translation accuracy and 37.5% assembly\ntranslation accuracy, substantially outperforming commercial baselines such as\nGPT-4o, Claude, and Hipify. Our generated code matches native performance in\nover 85% of test cases, preserving runtime and memory behavior. To support\nrigorous evaluation, we introduce CASS-Bench, a curated benchmark spanning 16\nGPU domains with ground-truth execution. All data, models, and evaluation tools\nare released as open source to foster progress in GPU compiler tooling, binary\ncompatibility, and LLM-guided hardware translation. Dataset and benchmark are\non\nhttps://huggingface.co/datasets/MBZUAI/cass{blue{HuggingFace}},\nwith code at\nhttps://github.com/GustavoStahl/CASS{blue{GitHub}}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/T4ESSrZsC7163P3I8p17C.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/WF-SJEyKKtpa3Zq0JvBXA.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/Hl8Dkgmc4QL_l9YKPhRvD.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/p-io7OU8TtxwvBp4_M1Hd.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/bu6bpeVfonZgrXopd9f79.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.16968.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "fullname": "Ahmed Heakl",
            "name": "ahmedheakl",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 39
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02921",
            "authors": [
                {
                    "_id": "683ff4dcfbc9041ef7274c51",
                    "user": {
                        "_id": "657eea68f4f72f2c4c44640d",
                        "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
                        "isPro": false,
                        "fullname": "Yijun YANG",
                        "user": "thomasyyj",
                        "type": "user"
                    },
                    "name": "Yijun Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:53:47.455Z",
                    "hidden": false
                },
                {
                    "_id": "683ff4dcfbc9041ef7274c52",
                    "name": "Zeyu Huang",
                    "hidden": false
                },
                {
                    "_id": "683ff4dcfbc9041ef7274c53",
                    "user": {
                        "_id": "649d7d8968586ca9bf7f5fe6",
                        "avatarUrl": "/avatars/b444240770d4025dea41871cf38126dc.svg",
                        "isPro": false,
                        "fullname": "Wenhao Zhu",
                        "user": "Wenhao97",
                        "type": "user"
                    },
                    "name": "Wenhao Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:47:37.569Z",
                    "hidden": false
                },
                {
                    "_id": "683ff4dcfbc9041ef7274c54",
                    "user": {
                        "_id": "647ccbd6e07cf9bb2d485244",
                        "avatarUrl": "/avatars/e8915abaff04f6762247e196b7cf84df.svg",
                        "isPro": false,
                        "fullname": "Zihan Qiu",
                        "user": "QwQZh",
                        "type": "user"
                    },
                    "name": "Zihan Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:47:15.684Z",
                    "hidden": false
                },
                {
                    "_id": "683ff4dcfbc9041ef7274c55",
                    "name": "Fei Yuan",
                    "hidden": false
                },
                {
                    "_id": "683ff4dcfbc9041ef7274c56",
                    "name": "Jeff Z. Pan",
                    "hidden": false
                },
                {
                    "_id": "683ff4dcfbc9041ef7274c57",
                    "user": {
                        "_id": "666f6375623133f1ce79c021",
                        "avatarUrl": "/avatars/b57a457cff81d685fdd9eadee53445fc.svg",
                        "isPro": false,
                        "fullname": "Ivan Titov",
                        "user": "Ivanchoo",
                        "type": "user"
                    },
                    "name": "Ivan Titov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:46:49.924Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T14:23:06.000Z",
            "submittedOnDailyAt": "2025-06-05T02:04:55.586Z",
            "title": "A Controllable Examination for Long-Context Language Models",
            "submittedOnDailyBy": {
                "_id": "657eea68f4f72f2c4c44640d",
                "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
                "isPro": false,
                "fullname": "Yijun YANG",
                "user": "thomasyyj",
                "type": "user"
            },
            "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: seamless context, controllable setting, and\nsound evaluation. This study introduces LongBioBench, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\nunderstanding, reasoning, and trustworthiness.\nOur experimental evaluation, which includes 18 LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.",
            "upvotes": 30,
            "discussionId": "683ff4ddfbc9041ef7274c73",
            "githubRepo": "https://github.com/Thomasyyj/LongBio-Benchmark",
            "ai_summary": "LongBioBench is a new benchmark using artificially generated biographies to evaluate long-context language models across understanding, reasoning, and trustworthiness dimensions, addressing limitations in existing frameworks.",
            "ai_keywords": [
                "long-context language models (LCLM)",
                "real-world tasks",
                "synthetic tasks",
                "needle-in-the-haystack (NIAH)",
                "seamless context",
                "controllable setting",
                "sound evaluation",
                "LongBioBench",
                "semantic understanding",
                "elementary reasoning",
                "trustworthiness",
                "long-context continual pretraining",
                "RoPE embedding"
            ]
        },
        "publishedAt": "2025-06-03T10:23:06.000Z",
        "title": "A Controllable Examination for Long-Context Language Models",
        "summary": "Existing frameworks for evaluating long-context language models (LCLM) can be\nbroadly categorized into real-world and synthetic tasks. Despite their utility,\nboth approaches are accompanied by certain intrinsic limitations. Real-world\ntasks are too complex to interpret or characterize and are susceptible to data\ncontamination. In contrast, synthetic tasks often adopt the\nneedle-in-the-haystack (NIAH) format, wherein a lack of coherence between the\n\"needle\" and the \"haystack\" compromises their validity as proxies for realistic\napplications. In response to these challenges, we posit that an ideal\nlong-context evaluation framework should be characterized by three essential\nfeatures: seamless context, controllable setting, and\nsound evaluation. This study introduces LongBioBench, a\nnovel benchmark that utilizes artificially generated biographies as a\ncontrolled environment for assessing LCLMs across dimensions of\nunderstanding, reasoning, and trustworthiness.\nOur experimental evaluation, which includes 18 LCLMs in total,\ndemonstrates that most models still exhibit deficiencies in semantic\nunderstanding and elementary reasoning over retrieved results and are less\ntrustworthy as context length increases. Our further analysis indicates some\ndesign choices employed by existing synthetic benchmarks, such as contextual\nnon-coherence, numerical needles, and the absence of distractors, rendering\nthem vulnerable to test the model long-context capabilities. Moreover, we also\nreveal that long-context continual pretraining primarily adjusts RoPE embedding\nto accommodate extended context lengths. To sum up, compared to previous\nsynthetic benchmarks, LongBioBench achieves a better trade-off between\nmirroring authentic language tasks and maintaining controllability, and is\nhighly interpretable and configurable.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02921.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "657eea68f4f72f2c4c44640d",
            "avatarUrl": "/avatars/033bc4f063cd36a79a0b4761f6ebe32c.svg",
            "fullname": "Yijun YANG",
            "name": "thomasyyj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04141",
            "authors": [
                {
                    "_id": "684106fc8cb0edba3ab212bb",
                    "name": "Kejian Zhu",
                    "hidden": false
                },
                {
                    "_id": "684106fc8cb0edba3ab212bc",
                    "user": {
                        "_id": "643379416c6ecd58798421b3",
                        "avatarUrl": "/avatars/831db7eab2663abc33b176cf386b02f2.svg",
                        "isPro": false,
                        "fullname": "Zhuoran Jin",
                        "user": "jinzhuoran",
                        "type": "user"
                    },
                    "name": "Zhuoran Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:49:35.145Z",
                    "hidden": false
                },
                {
                    "_id": "684106fc8cb0edba3ab212bd",
                    "user": {
                        "_id": "6538d3089d66a6c304ee2f4b",
                        "avatarUrl": "/avatars/6e93ea6c937128765e5323fb4a5d1cea.svg",
                        "isPro": false,
                        "fullname": "HongbangYuan",
                        "user": "HongbangYuan",
                        "type": "user"
                    },
                    "name": "Hongbang Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:49:42.552Z",
                    "hidden": false
                },
                {
                    "_id": "684106fc8cb0edba3ab212be",
                    "user": {
                        "_id": "64be6db0e38420aabaef8e12",
                        "avatarUrl": "/avatars/4d71169bdfd437a8fb9a026ac0dc46de.svg",
                        "isPro": false,
                        "fullname": "JIACHUN LI",
                        "user": "ChristinaLJC",
                        "type": "user"
                    },
                    "name": "Jiachun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:49:50.618Z",
                    "hidden": false
                },
                {
                    "_id": "684106fc8cb0edba3ab212bf",
                    "user": {
                        "_id": "648c48d8c0ddeee6df5b6d22",
                        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
                        "isPro": false,
                        "fullname": "Shangqing Tu",
                        "user": "tsq2000",
                        "type": "user"
                    },
                    "name": "Shangqing Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:49:22.312Z",
                    "hidden": false
                },
                {
                    "_id": "684106fc8cb0edba3ab212c0",
                    "name": "Pengfei Cao",
                    "hidden": false
                },
                {
                    "_id": "684106fc8cb0edba3ab212c1",
                    "name": "Yubo Chen",
                    "hidden": false
                },
                {
                    "_id": "684106fc8cb0edba3ab212c2",
                    "name": "Kang Liu",
                    "hidden": false
                },
                {
                    "_id": "684106fc8cb0edba3ab212c3",
                    "name": "Jun Zhao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
            ],
            "publishedAt": "2025-06-04T16:33:41.000Z",
            "submittedOnDailyAt": "2025-06-05T04:38:26.132Z",
            "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
            "submittedOnDailyBy": {
                "_id": "648c48d8c0ddeee6df5b6d22",
                "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
                "isPro": false,
                "fullname": "Shangqing Tu",
                "user": "tsq2000",
                "type": "user"
            },
            "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
            "upvotes": 25,
            "discussionId": "684106ff8cb0edba3ab21374",
            "projectPage": "https://mmr-v.github.io/",
            "githubRepo": "https://github.com/GaryStack/MMR-V",
            "ai_summary": "A new benchmark, MMR-V, is proposed to challenge multimodal large language models with long-range, multi-frame reasoning and hidden information processing in videos, revealing their limitations and inspiring further research.",
            "ai_keywords": [
                "multimodal large language models",
                "MMR-V",
                "long-range",
                "multi-frame reasoning",
                "multimodal reasoning",
                "manual annotation",
                "distractor annotation strategy",
                "Chain-of-Thought"
            ]
        },
        "publishedAt": "2025-06-04T12:33:41.000Z",
        "title": "MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in\n  Videos",
        "summary": "The sequential structure of videos poses a challenge to the ability of\nmultimodal large language models (MLLMs) to locate multi-frame evidence and\nconduct multimodal reasoning. However, existing video benchmarks mainly focus\non understanding tasks, which only require models to match frames mentioned in\nthe question (hereafter referred to as \"question frame\") and perceive a few\nadjacent frames. To address this gap, we propose MMR-V: A Benchmark for\nMultimodal Deep Reasoning in Videos. The benchmark is characterized by the\nfollowing features. (1) Long-range, multi-frame reasoning: Models are required\nto infer and analyze evidence frames that may be far from the question frame.\n(2) Beyond perception: Questions cannot be answered through direct perception\nalone but require reasoning over hidden information. (3) Reliability: All tasks\nare manually annotated, referencing extensive real-world user understanding to\nalign with common perceptions. (4) Confusability: Carefully designed distractor\nannotation strategies to reduce model shortcuts. MMR-V consists of 317 videos\nand 1,257 tasks. Our experiments reveal that current models still struggle with\nmulti-modal reasoning; even the best-performing model, o4-mini, achieves only\n52.5% accuracy. Additionally, current reasoning enhancement strategies\n(Chain-of-Thought and scaling test-time compute) bring limited gains. Further\nanalysis indicates that the CoT demanded for multi-modal reasoning differs from\nit in textual reasoning, which partly explains the limited performance gains.\nWe hope that MMR-V can inspire further research into enhancing multi-modal\nreasoning capabilities.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648c48d8c0ddeee6df5b6d22/_-WD0IU9jQMInqhXgGfUP.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04141.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648c48d8c0ddeee6df5b6d22",
            "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
            "fullname": "Shangqing Tu",
            "name": "tsq2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04180",
            "authors": [
                {
                    "_id": "6840fefb3098ab525906d852",
                    "name": "Yuhao Wu",
                    "hidden": false
                },
                {
                    "_id": "6840fefb3098ab525906d853",
                    "user": {
                        "_id": "64ed568ccf6118a9379a61b8",
                        "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
                        "isPro": false,
                        "fullname": "Yushi Bai",
                        "user": "bys0318",
                        "type": "user"
                    },
                    "name": "Yushi Bai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:46:26.352Z",
                    "hidden": false
                },
                {
                    "_id": "6840fefb3098ab525906d854",
                    "user": {
                        "_id": "637f228152229c63921119c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
                        "isPro": false,
                        "fullname": "Zhiqiang Hu",
                        "user": "Zhiqiang007",
                        "type": "user"
                    },
                    "name": "Zhiqiang Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:27:09.305Z",
                    "hidden": false
                },
                {
                    "_id": "6840fefb3098ab525906d855",
                    "name": "Juanzi Li",
                    "hidden": false
                },
                {
                    "_id": "6840fefb3098ab525906d856",
                    "user": {
                        "_id": "64b94cc0e3d41dbd6974ae45",
                        "avatarUrl": "/avatars/5edb9d9465addfceccef04c4465a34e6.svg",
                        "isPro": false,
                        "fullname": "Roy Ka-Wei lee",
                        "user": "sroylee",
                        "type": "user"
                    },
                    "name": "Roy Ka-Wei Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:46:33.575Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T17:27:42.000Z",
            "submittedOnDailyAt": "2025-06-05T00:58:27.883Z",
            "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "64ed568ccf6118a9379a61b8",
                "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
                "isPro": false,
                "fullname": "Yushi Bai",
                "user": "bys0318",
                "type": "user"
            },
            "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
            "upvotes": 23,
            "discussionId": "6840fefc3098ab525906d89c",
            "ai_summary": "SuperWriter-Agent enhances long-form text generation by integrating structured planning and refinement, achieving top performance with a 7B model and hierarchical Direct Preference Optimization.",
            "ai_keywords": [
                "agent-based framework",
                "structured thinking-through planning",
                "refinement stages",
                "SuperWriter-Agent",
                "SuperWriter-LM",
                "hierarchical Direct Preference Optimization",
                "Monte Carlo Tree Search",
                "DPO",
                "automatic evaluation",
                "human evaluation",
                "ablation studies"
            ]
        },
        "publishedAt": "2025-06-04T13:27:42.000Z",
        "title": "SuperWriter: Reflection-Driven Long-Form Generation with Large Language\n  Models",
        "summary": "Long-form text generation remains a significant challenge for large language\nmodels (LLMs), particularly in maintaining coherence, ensuring logical\nconsistency, and preserving text quality as sequence length increases. To\naddress these limitations, we propose SuperWriter-Agent, an agent-based\nframework designed to enhance the quality and consistency of long-form text\ngeneration. SuperWriter-Agent introduces explicit structured thinking-through\nplanning and refinement stages into the generation pipeline, guiding the model\nto follow a more deliberate and cognitively grounded process akin to that of a\nprofessional writer. Based on this framework, we construct a supervised\nfine-tuning dataset to train a 7B SuperWriter-LM. We further develop a\nhierarchical Direct Preference Optimization (DPO) procedure that uses Monte\nCarlo Tree Search (MCTS) to propagate final quality assessments and optimize\neach generation step accordingly. Empirical results across diverse benchmarks\ndemonstrate that SuperWriter-LM achieves state-of-the-art performance,\nsurpassing even larger-scale baseline models in both automatic evaluation and\nhuman evaluation. Furthermore, comprehensive ablation studies demonstrate the\neffectiveness of hierarchical DPO and underscore the value of incorporating\nstructured thinking steps to improve the quality of long-form text generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04180.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ed568ccf6118a9379a61b8",
            "avatarUrl": "/avatars/6d040cbcb4a9b624cbe64c9d01cd5c88.svg",
            "fullname": "Yushi Bai",
            "name": "bys0318",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04142",
            "authors": [
                {
                    "_id": "684132cb725b7fb67f68ffb8",
                    "name": "Kejian Zhu",
                    "hidden": false
                },
                {
                    "_id": "684132cb725b7fb67f68ffb9",
                    "user": {
                        "_id": "648c48d8c0ddeee6df5b6d22",
                        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
                        "isPro": false,
                        "fullname": "Shangqing Tu",
                        "user": "tsq2000",
                        "type": "user"
                    },
                    "name": "Shangqing Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:56:11.501Z",
                    "hidden": false
                },
                {
                    "_id": "684132cb725b7fb67f68ffba",
                    "user": {
                        "_id": "643379416c6ecd58798421b3",
                        "avatarUrl": "/avatars/831db7eab2663abc33b176cf386b02f2.svg",
                        "isPro": false,
                        "fullname": "Zhuoran Jin",
                        "user": "jinzhuoran",
                        "type": "user"
                    },
                    "name": "Zhuoran Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:56:17.500Z",
                    "hidden": false
                },
                {
                    "_id": "684132cb725b7fb67f68ffbb",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "684132cb725b7fb67f68ffbc",
                    "user": {
                        "_id": "65df8cbc2705d9672f55d1aa",
                        "avatarUrl": "/avatars/63e46f15bb76bd9d4508fd0f54f39829.svg",
                        "isPro": false,
                        "fullname": "Juanzi Li",
                        "user": "juanli",
                        "type": "user"
                    },
                    "name": "Juanzi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:56:23.839Z",
                    "hidden": false
                },
                {
                    "_id": "684132cb725b7fb67f68ffbd",
                    "name": "Jun Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T16:33:44.000Z",
            "submittedOnDailyAt": "2025-06-05T04:32:19.273Z",
            "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
            "submittedOnDailyBy": {
                "_id": "648c48d8c0ddeee6df5b6d22",
                "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
                "isPro": false,
                "fullname": "Shangqing Tu",
                "user": "tsq2000",
                "type": "user"
            },
            "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient (rho)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
            "upvotes": 22,
            "discussionId": "684132cc725b7fb67f68fff5",
            "githubRepo": "https://github.com/GaryStack/Trustworthy-Evaluation",
            "ai_summary": "A method called shortcut neuron patching identifies and suppresses shortcut neurons in language models to mitigate data contamination issues in trustworthy evaluations.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "trustworthy evaluation",
                "data contamination",
                "benchmarks",
                "dynamic benchmarks",
                "shortcut solutions",
                "shortcut neurons",
                "comparative analysis",
                "causal analysis",
                "shortcut neuron patching",
                "MixEval",
                "Spearman coefficient"
            ]
        },
        "publishedAt": "2025-06-04T12:33:44.000Z",
        "title": "Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis",
        "summary": "The development of large language models (LLMs) depends on trustworthy\nevaluation. However, most current evaluations rely on public benchmarks, which\nare prone to data contamination issues that significantly compromise fairness.\nPrevious researches have focused on constructing dynamic benchmarks to address\ncontamination. However, continuously building new benchmarks is costly and\ncyclical. In this work, we aim to tackle contamination by analyzing the\nmechanisms of contaminated models themselves. Through our experiments, we\ndiscover that the overestimation of contaminated models is likely due to\nparameters acquiring shortcut solutions in training. We further propose a novel\nmethod for identifying shortcut neurons through comparative and causal\nanalysis. Building on this, we introduce an evaluation method called shortcut\nneuron patching to suppress shortcut neurons. Experiments validate the\neffectiveness of our approach in mitigating contamination. Additionally, our\nevaluation results exhibit a strong linear correlation with MixEval, a recently\nreleased trustworthy benchmark, achieving a Spearman coefficient (rho)\nexceeding 0.95. This high correlation indicates that our method closely reveals\ntrue capabilities of the models and is trustworthy. We conduct further\nexperiments to demonstrate the generalizability of our method across various\nbenchmarks and hyperparameter settings. Code:\nhttps://github.com/GaryStack/Trustworthy-Evaluation",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04142.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648c48d8c0ddeee6df5b6d22",
            "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
            "fullname": "Shangqing Tu",
            "name": "tsq2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04178",
            "authors": [
                {
                    "_id": "68417762d777f13c594dd03c",
                    "name": "Etash Guha",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd03d",
                    "name": "Ryan Marten",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd03e",
                    "name": "Sedrick Keh",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd03f",
                    "name": "Negin Raoof",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd040",
                    "name": "Georgios Smyrnis",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd041",
                    "name": "Hritik Bansal",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd042",
                    "name": "Marianna Nezhurina",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd043",
                    "name": "Jean Mercat",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd044",
                    "name": "Trung Vu",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd045",
                    "name": "Zayne Sprague",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd046",
                    "name": "Ashima Suvarna",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd047",
                    "name": "Benjamin Feuer",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd048",
                    "name": "Liangyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd049",
                    "name": "Zaid Khan",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd04a",
                    "name": "Eric Frankel",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd04b",
                    "name": "Sachin Grover",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd04c",
                    "name": "Caroline Choi",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd04d",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd04e",
                    "name": "Shiye Su",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd04f",
                    "name": "Wanjia Zhao",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd050",
                    "name": "John Yang",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd051",
                    "name": "Shreyas Pimpalgaonkar",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd052",
                    "name": "Kartik Sharma",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd053",
                    "name": "Charlie Cheng-Jie Ji",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd054",
                    "name": "Yichuan Deng",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd055",
                    "name": "Sarah Pratt",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd056",
                    "name": "Vivek Ramanujan",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd057",
                    "name": "Jon Saad-Falcon",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd058",
                    "name": "Jeffrey Li",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd059",
                    "name": "Achal Dave",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd05a",
                    "name": "Alon Albalak",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd05b",
                    "name": "Kushal Arora",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd05c",
                    "name": "Blake Wulfe",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd05d",
                    "name": "Chinmay Hegde",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd05e",
                    "name": "Greg Durrett",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd05f",
                    "name": "Sewoong Oh",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd060",
                    "name": "Mohit Bansal",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd061",
                    "name": "Saadia Gabriel",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd062",
                    "name": "Aditya Grover",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd063",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd064",
                    "name": "Vaishaal Shankar",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd065",
                    "name": "Aaron Gokaslan",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd066",
                    "name": "Mike A. Merrill",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd067",
                    "name": "Tatsunori Hashimoto",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd068",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd069",
                    "name": "Jenia Jitsev",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd06a",
                    "name": "Reinhard Heckel",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd06b",
                    "name": "Maheswaran Sathiamoorthy",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd06c",
                    "name": "Alexandros G. Dimakis",
                    "hidden": false
                },
                {
                    "_id": "68417762d777f13c594dd06d",
                    "name": "Ludwig Schmidt",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65a8c9dff84e045590476618/d9LCy-Rdi_X-7Brofjm1h.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65a8c9dff84e045590476618/V1vDfV2EktH_CjqbdSxLv.png"
            ],
            "publishedAt": "2025-06-04T17:25:39.000Z",
            "submittedOnDailyAt": "2025-06-05T15:46:43.237Z",
            "title": "OpenThoughts: Data Recipes for Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "65a8c9dff84e045590476618",
                "avatarUrl": "/avatars/9caef727784cda09c7bbcd373cbe3053.svg",
                "isPro": false,
                "fullname": "Etash Guha",
                "user": "EtashGuha",
                "type": "user"
            },
            "summary": "Reasoning models have made rapid progress on many benchmarks involving math,\ncode, and science. Yet, there are still many open questions about the best\ntraining recipes for reasoning since state-of-the-art models often rely on\nproprietary datasets with little to no public information available. To address\nthis, the goal of the OpenThoughts project is to create open-source datasets\nfor training reasoning models. After initial explorations, our OpenThoughts2-1M\ndataset led to OpenThinker2-32B, the first model trained on public reasoning\ndata to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as\nAIME and LiveCodeBench. We then improve our dataset further by systematically\ninvestigating each step of our data generation pipeline with 1,000+ controlled\nexperiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples\nand using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves\nstate-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,\nand 54% on GPQA Diamond. All of our datasets and models are available on\nhttps://openthoughts.ai.",
            "upvotes": 20,
            "discussionId": "68417763d777f13c594dd0af",
            "ai_summary": "The OpenThoughts project created open-source datasets leading to reasoning models that match or exceed state-of-the-art benchmarks in math, code, and science.",
            "ai_keywords": [
                "reasoning models",
                "OpenThoughts project",
                "OpenThoughts2-1M",
                "OpenThinker2-32B",
                "DeepSeek-R1-Distill-32B",
                "standard reasoning benchmarks",
                "AIME",
                "LiveCodeBench",
                "OpenThoughts3",
                "OpenThinker3-7B",
                "GPQA Diamond"
            ]
        },
        "publishedAt": "2025-06-04T13:25:39.000Z",
        "title": "OpenThoughts: Data Recipes for Reasoning Models",
        "summary": "Reasoning models have made rapid progress on many benchmarks involving math,\ncode, and science. Yet, there are still many open questions about the best\ntraining recipes for reasoning since state-of-the-art models often rely on\nproprietary datasets with little to no public information available. To address\nthis, the goal of the OpenThoughts project is to create open-source datasets\nfor training reasoning models. After initial explorations, our OpenThoughts2-1M\ndataset led to OpenThinker2-32B, the first model trained on public reasoning\ndata to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as\nAIME and LiveCodeBench. We then improve our dataset further by systematically\ninvestigating each step of our data generation pipeline with 1,000+ controlled\nexperiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples\nand using QwQ-32B as teacher yields our OpenThinker3-7B model, which achieves\nstate-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25,\nand 54% on GPQA Diamond. All of our datasets and models are available on\nhttps://openthoughts.ai.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65a8c9dff84e045590476618/d9LCy-Rdi_X-7Brofjm1h.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65a8c9dff84e045590476618/V1vDfV2EktH_CjqbdSxLv.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04178.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65a8c9dff84e045590476618",
            "avatarUrl": "/avatars/9caef727784cda09c7bbcd373cbe3053.svg",
            "fullname": "Etash Guha",
            "name": "EtashGuha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.04225",
            "authors": [
                {
                    "_id": "68413366adeec0116d071af2",
                    "user": {
                        "_id": "63425d394c9a81858b36aeb5",
                        "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
                        "isPro": false,
                        "fullname": "Tianyu Huang",
                        "user": "tyhuang",
                        "type": "user"
                    },
                    "name": "Tianyu Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:41.198Z",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071af3",
                    "name": "Wangguandong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071af4",
                    "name": "Tengfei Wang",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071af5",
                    "name": "Yuhao Liu",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071af6",
                    "name": "Zhenwei Wang",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071af7",
                    "name": "Junta Wu",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071af8",
                    "name": "Jie Jiang",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071af9",
                    "name": "Hui Li",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071afa",
                    "name": "Rynson W. H. Lau",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071afb",
                    "name": "Wangmeng Zuo",
                    "hidden": false
                },
                {
                    "_id": "68413366adeec0116d071afc",
                    "name": "Chunchao Guo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
            ],
            "publishedAt": "2025-06-04T17:59:04.000Z",
            "submittedOnDailyAt": "2025-06-05T06:00:26.815Z",
            "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
            "submittedOnDailyBy": {
                "_id": "63425d394c9a81858b36aeb5",
                "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
                "isPro": false,
                "fullname": "Tianyu Huang",
                "user": "tyhuang",
                "type": "user"
            },
            "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
            "upvotes": 18,
            "discussionId": "6841336badeec0116d071c2b",
            "projectPage": "https://voyager-world.github.io",
            "githubRepo": "https://github.com/Voyager-World/Voyager",
            "ai_summary": "Voyager is a video diffusion framework that generates world-consistent 3D point-cloud sequences from a single image, enabling long-range, consistent 3D scene exploration with user-defined camera paths.",
            "ai_keywords": [
                "video diffusion",
                "world-consistent video diffusion",
                "3D point-cloud sequences",
                "camera path",
                "end-to-end scene generation",
                "consistent frames",
                "unified architecture",
                "RGB and depth video sequences",
                "world observation",
                "global coherence",
                "long-range world exploration",
                "world cache",
                "point culling",
                "auto-regressive inference",
                "smooth video sampling",
                "scene extension",
                "context-aware consistency",
                "scalable data engine",
                "camera pose estimation",
                "metric depth prediction",
                "large-scale",
                "diverse training data"
            ]
        },
        "publishedAt": "2025-06-04T13:59:04.000Z",
        "title": "Voyager: Long-Range and World-Consistent Video Diffusion for Explorable\n  3D Scene Generation",
        "summary": "Real-world applications like video gaming and virtual reality often demand\nthe ability to model 3D scenes that users can explore along custom camera\ntrajectories. While significant progress has been made in generating 3D objects\nfrom text or images, creating long-range, 3D-consistent, explorable 3D scenes\nremains a complex and challenging problem. In this work, we present Voyager, a\nnovel video diffusion framework that generates world-consistent 3D point-cloud\nsequences from a single image with user-defined camera path. Unlike existing\napproaches, Voyager achieves end-to-end scene generation and reconstruction\nwith inherent consistency across frames, eliminating the need for 3D\nreconstruction pipelines (e.g., structure-from-motion or multi-view stereo).\nOur method integrates three key components: 1) World-Consistent Video\nDiffusion: A unified architecture that jointly generates aligned RGB and depth\nvideo sequences, conditioned on existing world observation to ensure global\ncoherence 2) Long-Range World Exploration: An efficient world cache with point\nculling and an auto-regressive inference with smooth video sampling for\niterative scene extension with context-aware consistency, and 3) Scalable Data\nEngine: A video reconstruction pipeline that automates camera pose estimation\nand metric depth prediction for arbitrary videos, enabling large-scale, diverse\ntraining data curation without manual 3D annotations. Collectively, these\ndesigns result in a clear improvement over existing methods in visual quality\nand geometric accuracy, with versatile applications.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63425d394c9a81858b36aeb5/cjZH2kR6B3y9IAmmRHNJS.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04225.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63425d394c9a81858b36aeb5",
            "avatarUrl": "/avatars/511ad6a75bd1c10fc510ef527e7f8e5b.svg",
            "fullname": "Tianyu Huang",
            "name": "tyhuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03930",
            "authors": [
                {
                    "_id": "6841090145662bb7d322ecc6",
                    "user": {
                        "_id": "64de37ee5e192985054be575",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
                        "isPro": false,
                        "fullname": "Yuansheng Ni",
                        "user": "yuanshengni",
                        "type": "user"
                    },
                    "name": "Yuansheng Ni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T10:00:04.196Z",
                    "hidden": false
                },
                {
                    "_id": "6841090145662bb7d322ecc7",
                    "name": "Ping Nie",
                    "hidden": false
                },
                {
                    "_id": "6841090145662bb7d322ecc8",
                    "name": "Kai Zou",
                    "hidden": false
                },
                {
                    "_id": "6841090145662bb7d322ecc9",
                    "name": "Xiang Yue",
                    "hidden": false
                },
                {
                    "_id": "6841090145662bb7d322ecca",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:53:53.184Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T13:24:44.000Z",
            "submittedOnDailyAt": "2025-06-05T05:54:39.464Z",
            "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
            "submittedOnDailyBy": {
                "_id": "64de37ee5e192985054be575",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
                "isPro": false,
                "fullname": "Yuansheng Ni",
                "user": "yuanshengni",
                "type": "user"
            },
            "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.",
            "upvotes": 18,
            "discussionId": "6841090245662bb7d322ed1f",
            "projectPage": "https://tiger-ai-lab.github.io/VisCoder/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/VisCoder",
            "ai_summary": "VisCode-200K, a large-scale dataset for visualization, improves plot generation performance by integrating execution-grounded supervision and iterative code correction, outperforming open-source models and rivaling proprietary ones.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "visualization tasks",
                "plot generation",
                "execution-grounded supervision",
                "iterative code correction",
                "VisCode-200K",
                "Python-based visualization",
                "validated plotting code",
                "natural language instructions",
                "rendered plots",
                "correction dialogues",
                "Qwen2.5-Coder-Instruct",
                "VisCoder",
                "PandasPlotBench",
                "self-debug evaluation",
                "feedback-driven learning"
            ]
        },
        "publishedAt": "2025-06-04T09:24:44.000Z",
        "title": "VisCoder: Fine-Tuning LLMs for Executable Python Visualization Code\n  Generation",
        "summary": "Large language models (LLMs) often struggle with visualization tasks like\nplotting diagrams, charts, where success depends on both code correctness and\nvisual semantics. Existing instruction-tuning datasets lack execution-grounded\nsupervision and offer limited support for iterative code correction, resulting\nin fragile and unreliable plot generation. We present VisCode-200K, a\nlarge-scale instruction tuning dataset for Python-based visualization and\nself-correction. It contains over 200K examples from two sources: (1) validated\nplotting code from open-source repositories, paired with natural language\ninstructions and rendered plots; and (2) 45K multi-turn correction dialogues\nfrom Code-Feedback, enabling models to revise faulty code using runtime\nfeedback. We fine-tune Qwen2.5-Coder-Instruct on VisCode-200K to create\nVisCoder, and evaluate it on PandasPlotBench. VisCoder significantly\noutperforms strong open-source baselines and approaches the performance of\nproprietary models like GPT-4o-mini. We further adopt a self-debug evaluation\nprotocol to assess iterative repair, demonstrating the benefits of\nfeedback-driven learning for executable, visually accurate code generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03930.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64de37ee5e192985054be575",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64de37ee5e192985054be575/fVV7JQMtp_J3uFqszJJHH.jpeg",
            "fullname": "Yuansheng Ni",
            "name": "yuanshengni",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03150",
            "authors": [
                {
                    "_id": "68417439723213944b275699",
                    "user": {
                        "_id": "684178483b8c48d81babcbf5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/684178483b8c48d81babcbf5/cFmEFnm_CPaqxb7yw17zv.jpeg",
                        "isPro": false,
                        "fullname": "Yuanze Lin",
                        "user": "YuanzeLin",
                        "type": "user"
                    },
                    "name": "Yuanze Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T15:08:30.655Z",
                    "hidden": false
                },
                {
                    "_id": "68417439723213944b27569a",
                    "user": {
                        "_id": "65d5b580dd06ca7bac2364bf",
                        "avatarUrl": "/avatars/f380d2846507a1936eed6d2903e0d460.svg",
                        "isPro": false,
                        "fullname": "Yi-Wen Chen",
                        "user": "wenz116",
                        "type": "user"
                    },
                    "name": "Yi-Wen Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:52:20.026Z",
                    "hidden": false
                },
                {
                    "_id": "68417439723213944b27569b",
                    "user": {
                        "_id": "67d3a457faaad4ed2df4aba8",
                        "avatarUrl": "/avatars/fb74dff4aeb3f84b8c9862aa73e4620b.svg",
                        "isPro": false,
                        "fullname": "Tsai Yi-Hsuan",
                        "user": "Jodiiiiie",
                        "type": "user"
                    },
                    "name": "Yi-Hsuan Tsai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:52:13.475Z",
                    "hidden": false
                },
                {
                    "_id": "68417439723213944b27569c",
                    "name": "Ronald Clark",
                    "hidden": false
                },
                {
                    "_id": "68417439723213944b27569d",
                    "name": "Ming-Hsuan Yang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/633be9e4c0fb6fd232ee679d/wVy8Y-cSIPQotVC0IAKWi.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/633be9e4c0fb6fd232ee679d/CamHhgmk6MAfMJG7kCK7E.gif"
            ],
            "publishedAt": "2025-06-03T17:59:52.000Z",
            "submittedOnDailyAt": "2025-06-05T09:14:22.365Z",
            "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for\n  Controllable Video Generation",
            "submittedOnDailyBy": {
                "_id": "633be9e4c0fb6fd232ee679d",
                "avatarUrl": "/avatars/04984852e6d139db5975015d1a7c9d5f.svg",
                "isPro": false,
                "fullname": "yz",
                "user": "Yuanze",
                "type": "user"
            },
            "summary": "Although diffusion-based models can generate high-quality and high-resolution\nvideo sequences from textual or image inputs, they lack explicit integration of\ngeometric cues when controlling scene lighting and visual appearance across\nframes. To address this limitation, we propose IllumiCraft, an end-to-end\ndiffusion framework accepting three complementary inputs: (1)\nhigh-dynamic-range (HDR) video maps for detailed lighting control; (2)\nsynthetically relit frames with randomized illumination changes (optionally\npaired with a static background reference image) to provide appearance cues;\nand (3) 3D point tracks that capture precise 3D geometry information. By\nintegrating the lighting, appearance, and geometry cues within a unified\ndiffusion architecture, IllumiCraft generates temporally coherent videos\naligned with user-defined prompts. It supports background-conditioned and\ntext-conditioned video relighting and provides better fidelity than existing\ncontrollable video generation methods. Project Page:\nhttps://yuanze-lin.me/IllumiCraft_page",
            "upvotes": 18,
            "discussionId": "6841743c723213944b27573e",
            "projectPage": "https://yuanze-lin.me/IllumiCraft_page/",
            "githubRepo": "https://github.com/yuanze-lin/IllumiCraft",
            "ai_summary": "IllumiCraft integrates geometric cues in a diffusion framework to generate high-fidelity, temporally coherent videos from textual or image inputs.",
            "ai_keywords": [
                "diffusion-based models",
                "HDR video maps",
                "synthetically relit frames",
                "3D point tracks",
                "unified diffusion architecture",
                "temporally coherent videos",
                "background-conditioned",
                "text-conditioned video relighting"
            ]
        },
        "publishedAt": "2025-06-03T13:59:52.000Z",
        "title": "IllumiCraft: Unified Geometry and Illumination Diffusion for\n  Controllable Video Generation",
        "summary": "Although diffusion-based models can generate high-quality and high-resolution\nvideo sequences from textual or image inputs, they lack explicit integration of\ngeometric cues when controlling scene lighting and visual appearance across\nframes. To address this limitation, we propose IllumiCraft, an end-to-end\ndiffusion framework accepting three complementary inputs: (1)\nhigh-dynamic-range (HDR) video maps for detailed lighting control; (2)\nsynthetically relit frames with randomized illumination changes (optionally\npaired with a static background reference image) to provide appearance cues;\nand (3) 3D point tracks that capture precise 3D geometry information. By\nintegrating the lighting, appearance, and geometry cues within a unified\ndiffusion architecture, IllumiCraft generates temporally coherent videos\naligned with user-defined prompts. It supports background-conditioned and\ntext-conditioned video relighting and provides better fidelity than existing\ncontrollable video generation methods. Project Page:\nhttps://yuanze-lin.me/IllumiCraft_page",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/633be9e4c0fb6fd232ee679d/wVy8Y-cSIPQotVC0IAKWi.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/633be9e4c0fb6fd232ee679d/CamHhgmk6MAfMJG7kCK7E.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03150.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "633be9e4c0fb6fd232ee679d",
            "avatarUrl": "/avatars/04984852e6d139db5975015d1a7c9d5f.svg",
            "fullname": "yz",
            "name": "Yuanze",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.04158",
            "authors": [
                {
                    "_id": "6840fb71d4e16ff5f95108aa",
                    "user": {
                        "_id": "67da3534e6387c9c9bc954b3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1IiNF_DgWQcvMYIlW8ikC.png",
                        "isPro": true,
                        "fullname": "HU YUJIA",
                        "user": "Cicici1109",
                        "type": "user"
                    },
                    "name": "Yujia Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T12:42:05.838Z",
                    "hidden": false
                },
                {
                    "_id": "6840fb71d4e16ff5f95108ab",
                    "name": "Songhua Liu",
                    "hidden": false
                },
                {
                    "_id": "6840fb71d4e16ff5f95108ac",
                    "name": "Zhenxiong Tan",
                    "hidden": false
                },
                {
                    "_id": "6840fb71d4e16ff5f95108ad",
                    "user": {
                        "_id": "634cfebc350bcee9bed20a4d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
                        "isPro": false,
                        "fullname": "Xingyi Yang",
                        "user": "adamdad",
                        "type": "user"
                    },
                    "name": "Xingyi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:27:11.256Z",
                    "hidden": false
                },
                {
                    "_id": "6840fb71d4e16ff5f95108ae",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
            ],
            "publishedAt": "2025-06-04T16:57:24.000Z",
            "submittedOnDailyAt": "2025-06-05T00:38:20.484Z",
            "title": "Image Editing As Programs with Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "634cfebc350bcee9bed20a4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
                "isPro": false,
                "fullname": "Xingyi Yang",
                "user": "adamdad",
                "type": "user"
            },
            "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
            "upvotes": 16,
            "discussionId": "6840fb73d4e16ff5f9510950",
            "projectPage": "https://yujiahu1109.github.io/IEAP/",
            "githubRepo": "https://github.com/YujiaHu1109/IEAP",
            "ai_summary": "A unified image editing framework, IEAP, built on Diffusion Transformer (DiT) decomposes complex editing instructions into operations performed by vision-language models for robust editing across various tasks.",
            "ai_keywords": [
                "diffusion models",
                "text-to-image generation",
                "instruction-driven image editing",
                "structurally inconsistent edits",
                "Image Editing As Programs (IEAP)",
                "Diffusion Transformer (DiT)",
                "atomic operations",
                "lightweight adapter",
                "vision-language model (VLM)",
                "modularizing edits"
            ]
        },
        "publishedAt": "2025-06-04T12:57:24.000Z",
        "title": "Image Editing As Programs with Diffusion Models",
        "summary": "While diffusion models have achieved remarkable success in text-to-image\ngeneration, they encounter significant challenges with instruction-driven image\nediting. Our research highlights a key challenge: these models particularly\nstruggle with structurally inconsistent edits that involve substantial layout\nchanges. To mitigate this gap, we introduce Image Editing As Programs (IEAP), a\nunified image editing framework built upon the Diffusion Transformer (DiT)\narchitecture. At its core, IEAP approaches instructional editing through a\nreductionist lens, decomposing complex editing instructions into sequences of\natomic operations. Each operation is implemented via a lightweight adapter\nsharing the same DiT backbone and is specialized for a specific type of edit.\nProgrammed by a vision-language model (VLM)-based agent, these operations\ncollaboratively support arbitrary and structurally inconsistent\ntransformations. By modularizing and sequencing edits in this way, IEAP\ngeneralizes robustly across a wide range of editing tasks, from simple\nadjustments to substantial structural changes. Extensive experiments\ndemonstrate that IEAP significantly outperforms state-of-the-art methods on\nstandard benchmarks across various editing scenarios. In these evaluations, our\nframework delivers superior accuracy and semantic fidelity, particularly for\ncomplex, multi-step instructions. Codes are available at\nhttps://github.com/YujiaHu1109/IEAP.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/634cfebc350bcee9bed20a4d/tp0cLz8OZhdI3vs50cxhF.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04158.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634cfebc350bcee9bed20a4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634cfebc350bcee9bed20a4d/fN47nN5rhw-HJaFLBZWQy.png",
            "fullname": "Xingyi Yang",
            "name": "adamdad",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03295",
            "authors": [
                {
                    "_id": "6840e7d81fadbc85ae3bdc0f",
                    "user": {
                        "_id": "636a35eff8d9af4aea181608",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636a35eff8d9af4aea181608/s9GFJYd_QXVbg0Lb4JpKj.jpeg",
                        "isPro": false,
                        "fullname": "yubo",
                        "user": "ubowang",
                        "type": "user"
                    },
                    "name": "Yubo Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:51:07.108Z",
                    "hidden": false
                },
                {
                    "_id": "6840e7d81fadbc85ae3bdc10",
                    "name": "Ping Nie",
                    "hidden": false
                },
                {
                    "_id": "6840e7d81fadbc85ae3bdc11",
                    "name": "Kai Zou",
                    "hidden": false
                },
                {
                    "_id": "6840e7d81fadbc85ae3bdc12",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "6840e7d81fadbc85ae3bdc13",
                    "user": {
                        "_id": "6313a86154e6e5d9f0f94e04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662232951344-6313a86154e6e5d9f0f94e04.jpeg",
                        "isPro": false,
                        "fullname": "Wenhu Chen",
                        "user": "wenhu",
                        "type": "user"
                    },
                    "name": "Wenhu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:51:23.993Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T18:35:52.000Z",
            "submittedOnDailyAt": "2025-06-05T02:48:14.083Z",
            "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
            "submittedOnDailyBy": {
                "_id": "636a35eff8d9af4aea181608",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636a35eff8d9af4aea181608/s9GFJYd_QXVbg0Lb4JpKj.jpeg",
                "isPro": false,
                "fullname": "yubo",
                "user": "ubowang",
                "type": "user"
            },
            "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.",
            "upvotes": 16,
            "discussionId": "6840e7d81fadbc85ae3bdc45",
            "ai_summary": "Critique Fine-Tuning on a single problem can efficiently enhance the reasoning capabilities of large language models with significant performance gains and reduced computational cost compared to reinforcement learning.",
            "ai_keywords": [
                "Critique Fine-Tuning",
                "teacher LLMs",
                "Qwen-Math",
                "Llama family models",
                "reasoning tasks",
                "one-shot CFT",
                "performance gains",
                "logic reasoning benchmarks",
                "math benchmarks",
                "prompt problems"
            ]
        },
        "publishedAt": "2025-06-03T14:35:52.000Z",
        "title": "Unleashing the Reasoning Potential of Pre-trained LLMs by Critique\n  Fine-Tuning on One Problem",
        "summary": "We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess\nimmense reasoning potential inherited from the pre-training stage. With\nreinforcement learning (RL), these models can improve dramatically on reasoning\ntasks. Recent studies have shown that even RL on a single problem can unleash\nthese models' reasoning capabilities. However, RL is not only expensive but\nalso unstable. Even one-shot RL requires hundreds of GPU hours. This raises a\ncritical question: Is there a more efficient way to unleash the reasoning\npotential of these powerful base LLMs? In this work, we demonstrate that\nCritique Fine-Tuning (CFT) on only one problem can effectively unleash the\nreasoning potential of LLMs. Our method constructs critique data by collecting\ndiverse model-generated solutions to a single problem and using teacher LLMs to\nprovide detailed critiques. We fine-tune Qwen and Llama family models, ranging\nfrom 1.5B to 14B parameters, on the CFT data and observe significant\nperformance gains across diverse reasoning tasks. For example, with just 5 GPU\nhours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six\nmath benchmarks and 16% on three logic reasoning benchmarks. These results are\ncomparable to or even surpass the results from RL with 20x less compute.\nAblation studies reveal the robustness of one-shot CFT across different prompt\nproblems. These results highlight one-shot CFT as a simple, general, and\ncompute-efficient approach to unleashing the reasoning capabilities of modern\nLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03295.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636a35eff8d9af4aea181608",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636a35eff8d9af4aea181608/s9GFJYd_QXVbg0Lb4JpKj.jpeg",
            "fullname": "yubo",
            "name": "ubowang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01320",
            "authors": [
                {
                    "_id": "684124368cb0edba3ab8f738",
                    "user": {
                        "_id": "64df25a16466b911722ca1da",
                        "avatarUrl": "/avatars/da92419e4d8ba412f163f85e09055f49.svg",
                        "isPro": false,
                        "fullname": "TaeHoon Yoon",
                        "user": "sillsill777",
                        "type": "user"
                    },
                    "name": "Taehoon Yoon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:43:45.956Z",
                    "hidden": false
                },
                {
                    "_id": "684124368cb0edba3ab8f739",
                    "user": {
                        "_id": "66ee81b676a8038cb42c8caa",
                        "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
                        "isPro": false,
                        "fullname": "Yunhong Min",
                        "user": "myhong",
                        "type": "user"
                    },
                    "name": "Yunhong Min",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:43:53.200Z",
                    "hidden": false
                },
                {
                    "_id": "684124368cb0edba3ab8f73a",
                    "user": {
                        "_id": "659e42cfb65ee9ee1fd11e61",
                        "avatarUrl": "/avatars/a9220d099f32800fc43ae79bb519c1e9.svg",
                        "isPro": false,
                        "fullname": "Kyeongmin Yeo",
                        "user": "32V",
                        "type": "user"
                    },
                    "name": "Kyeongmin Yeo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:44:00.132Z",
                    "hidden": false
                },
                {
                    "_id": "684124368cb0edba3ab8f73b",
                    "user": {
                        "_id": "631f432b5ba8c026340a7890",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631f432b5ba8c026340a7890/9PK7A_TRMpugwYjCsNBf1.jpeg",
                        "isPro": false,
                        "fullname": "Minhyuk Sung",
                        "user": "Minhyuk",
                        "type": "user"
                    },
                    "name": "Minhyuk Sung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:44:05.457Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T05:02:33.000Z",
            "submittedOnDailyAt": "2025-06-05T03:38:33.152Z",
            "title": "-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models",
            "submittedOnDailyBy": {
                "_id": "66ee81b676a8038cb42c8caa",
                "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
                "isPro": false,
                "fullname": "Yunhong Min",
                "user": "myhong",
                "type": "user"
            },
            "summary": "We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.",
            "upvotes": 15,
            "discussionId": "6841243c8cb0edba3ab8f8bf",
            "ai_summary": "The framework $\\Psi$-Sampler uses SMC with pCNL for efficient posterior sampling and reward alignment in score-based generative models, enhancing performance across various tasks.",
            "ai_keywords": [
                "SMC-based framework",
                "pCNL-based initial particle sampling",
                "inference-time reward alignment",
                "score-based generative model",
                "Sequential Monte Carlo",
                "denoising process",
                "Gaussian prior",
                "reward-aware posterior",
                "preconditioned Crank-Nicolson Langevin",
                "layout-to-image generation",
                "quantity-aware generation",
                "aesthetic-preference generation"
            ]
        },
        "publishedAt": "2025-06-02T01:02:33.000Z",
        "title": "-Sampler: Initial Particle Sampling for SMC-Based Inference-Time\n  Reward Alignment in Score Models",
        "summary": "We introduce Psi-Sampler, an SMC-based framework incorporating pCNL-based\ninitial particle sampling for effective inference-time reward alignment with a\nscore-based generative model. Inference-time reward alignment with score-based\ngenerative models has recently gained significant traction, following a broader\nparadigm shift from pre-training to post-training optimization. At the core of\nthis trend is the application of Sequential Monte Carlo (SMC) to the denoising\nprocess. However, existing methods typically initialize particles from the\nGaussian prior, which inadequately captures reward-relevant regions and results\nin reduced sampling efficiency. We demonstrate that initializing from the\nreward-aware posterior significantly improves alignment performance. To enable\nposterior sampling in high-dimensional latent spaces, we introduce the\npreconditioned Crank-Nicolson Langevin (pCNL) algorithm, which combines\ndimension-robust proposals with gradient-informed dynamics. This approach\nenables efficient and scalable posterior sampling and consistently improves\nperformance across various reward alignment tasks, including layout-to-image\ngeneration, quantity-aware generation, and aesthetic-preference generation, as\ndemonstrated in our experiments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01320.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66ee81b676a8038cb42c8caa",
            "avatarUrl": "/avatars/9b4c5ded9c94788c35ce7ffbc2f8d24b.svg",
            "fullname": "Yunhong Min",
            "name": "myhong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04228",
            "authors": [
                {
                    "_id": "684103aed45a1fc5540ddc10",
                    "user": {
                        "_id": "680c7488195bc1af6f557b05",
                        "avatarUrl": "/avatars/e6c5a88023ab167c777a6e8915c560a2.svg",
                        "isPro": false,
                        "fullname": "Sihui Ji",
                        "user": "Jish2",
                        "type": "user"
                    },
                    "name": "Sihui Ji",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:48:05.938Z",
                    "hidden": false
                },
                {
                    "_id": "684103aed45a1fc5540ddc11",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "684103aed45a1fc5540ddc12",
                    "user": {
                        "_id": "644a1b6401e18bf93a6f45c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
                        "isPro": false,
                        "fullname": "xichen",
                        "user": "xichenhku",
                        "type": "user"
                    },
                    "name": "Xi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:50.027Z",
                    "hidden": false
                },
                {
                    "_id": "684103aed45a1fc5540ddc13",
                    "name": "Yuanpeng Tu",
                    "hidden": false
                },
                {
                    "_id": "684103aed45a1fc5540ddc14",
                    "name": "Yiyang Wang",
                    "hidden": false
                },
                {
                    "_id": "684103aed45a1fc5540ddc15",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T17:59:58.000Z",
            "submittedOnDailyAt": "2025-06-05T01:11:16.967Z",
            "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
            "submittedOnDailyBy": {
                "_id": "644a1b6401e18bf93a6f45c1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
                "isPro": false,
                "fullname": "xichen",
                "user": "xichenhku",
                "type": "user"
            },
            "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers.",
            "upvotes": 13,
            "discussionId": "684103b0d45a1fc5540ddca8",
            "ai_summary": "LayerFlow is a unified framework for generating layer-aware videos using a text-to-video diffusion transformer and layer embeddings, supporting various video generation tasks with a multi-stage training strategy.",
            "ai_keywords": [
                "LayerFlow",
                "text-to-video diffusion transformer",
                "layer embeddings",
                "sub-clips",
                "multi-stage training strategy",
                "motion LoRA",
                "content LoRA",
                "layered images",
                "copy-pasted video data",
                "smooth videos"
            ]
        },
        "publishedAt": "2025-06-04T13:59:58.000Z",
        "title": "LayerFlow: A Unified Model for Layer-aware Video Generation",
        "summary": "We present LayerFlow, a unified solution for layer-aware video generation.\nGiven per-layer prompts, LayerFlow generates videos for the transparent\nforeground, clean background, and blended scene. It also supports versatile\nvariants like decomposing a blended video or generating the background for the\ngiven foreground and vice versa. Starting from a text-to-video diffusion\ntransformer, we organize the videos for different layers as sub-clips, and\nleverage layer embeddings to distinguish each clip and the corresponding\nlayer-wise prompts. In this way, we seamlessly support the aforementioned\nvariants in one unified framework. For the lack of high-quality layer-wise\ntraining videos, we design a multi-stage training strategy to accommodate\nstatic images with high-quality layer annotations. Specifically, we first train\nthe model with low-quality video data. Then, we tune a motion LoRA to make the\nmodel compatible with static frames. Afterward, we train the content LoRA on\nthe mixture of image data with high-quality layered images along with\ncopy-pasted video data. During inference, we remove the motion LoRA thus\ngenerating smooth videos with desired layers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04228.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a1b6401e18bf93a6f45c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
            "fullname": "xichen",
            "name": "xichenhku",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 41
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03139",
            "authors": [
                {
                    "_id": "683fb0a7be8421eda3152283",
                    "user": {
                        "_id": "676b86e79ff0244316f7202f",
                        "avatarUrl": "/avatars/3e1d26312a96752356895ab88eeb3ce0.svg",
                        "isPro": false,
                        "fullname": "chensiqi",
                        "user": "xiaoooobai",
                        "type": "user"
                    },
                    "name": "Siqi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:28:25.792Z",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda3152284",
                    "name": "Xinyu Dong",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda3152285",
                    "user": {
                        "_id": "6692aff88db712bad780f02a",
                        "avatarUrl": "/avatars/5dc4b1c27c70f6a64864711dbff4910f.svg",
                        "isPro": false,
                        "fullname": "xhl",
                        "user": "zjuxhl",
                        "type": "user"
                    },
                    "name": "Haolei Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:56:42.705Z",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda3152286",
                    "name": "Xingyu Wu",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda3152287",
                    "name": "Fei Tang",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda3152288",
                    "name": "Hang Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda3152289",
                    "user": {
                        "_id": "64098738342c26884c792c93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                        "isPro": false,
                        "fullname": "Yuchen Yan",
                        "user": "yanyc",
                        "type": "user"
                    },
                    "name": "Yuchen Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:56:47.244Z",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda315228a",
                    "name": "Linjuan Wu",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda315228b",
                    "name": "Wenqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda315228c",
                    "name": "Guiyang Hou",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda315228d",
                    "name": "Yongliang Shen",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda315228e",
                    "name": "Weiming Lu",
                    "hidden": false
                },
                {
                    "_id": "683fb0a7be8421eda315228f",
                    "name": "Yueting Zhuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:58:57.000Z",
            "submittedOnDailyAt": "2025-06-05T03:44:24.728Z",
            "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
            "submittedOnDailyBy": {
                "_id": "5e1058e9fcf41d740b69966d",
                "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                "isPro": false,
                "fullname": "Yongliang Shen",
                "user": "tricktreat",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
            "upvotes": 13,
            "discussionId": "683fb0a7be8421eda31522ca",
            "projectPage": "https://zju-real.github.io/SVGenius/",
            "githubRepo": "https://github.com/ZJU-REAL/SVGenius",
            "ai_summary": "SVGenius evaluates Large Language Models and Multimodal LLMs for SVG processing using a comprehensive benchmark across three dimensions: understanding, editing, and generation, revealing insights into model capabilities and limitations.",
            "ai_keywords": [
                "Large Language Models",
                "Multimodal LLMs",
                "SVG processing",
                "SVGenius",
                "complexity stratification",
                "reasoning-enhanced training",
                "style transfer"
            ]
        },
        "publishedAt": "2025-06-03T13:58:57.000Z",
        "title": "SVGenius: Benchmarking LLMs in SVG Understanding, Editing and Generation",
        "summary": "Large Language Models (LLMs) and Multimodal LLMs have shown promising\ncapabilities for SVG processing, yet existing benchmarks suffer from limited\nreal-world coverage, lack of complexity stratification, and fragmented\nevaluation paradigms. We introduce SVGenius, a comprehensive benchmark\ncomprising 2,377 queries across three progressive dimensions: understanding,\nediting, and generation. Built on real-world data from 24 application domains\nwith systematic complexity stratification, SVGenius evaluates models through 8\ntask categories and 18 metrics. We assess 22 mainstream models spanning\ndifferent scales, architectures, training paradigms, and accessibility levels.\nOur analysis reveals that while proprietary models significantly outperform\nopen-source counterparts, all models exhibit systematic performance degradation\nwith increasing complexity, indicating fundamental limitations in current\napproaches; however, reasoning-enhanced training proves more effective than\npure scaling for overcoming these limitations, though style transfer remains\nthe most challenging capability across all model types. SVGenius establishes\nthe first systematic evaluation framework for SVG processing, providing crucial\ninsights for developing more capable vector graphics models and advancing\nautomated graphic design applications. Appendix and supplementary materials\n(including all data and code) are available at\nhttps://zju-real.github.io/SVGenius.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03139.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "fullname": "Yongliang Shen",
            "name": "tricktreat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03517",
            "authors": [
                {
                    "_id": "68412f853c22997c7329f3a0",
                    "user": {
                        "_id": "62980664ff0acd7e027d6686",
                        "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
                        "isPro": false,
                        "fullname": "Ziyi Wu",
                        "user": "Dazitu616",
                        "type": "user"
                    },
                    "name": "Ziyi Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:43.314Z",
                    "hidden": false
                },
                {
                    "_id": "68412f853c22997c7329f3a1",
                    "user": {
                        "_id": "66b01ee8e53bbad918362856",
                        "avatarUrl": "/avatars/293529589a91dd7a95909d66727db224.svg",
                        "isPro": false,
                        "fullname": "Anil Kag",
                        "user": "anilkagak2",
                        "type": "user"
                    },
                    "name": "Anil Kag",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:55:11.687Z",
                    "hidden": false
                },
                {
                    "_id": "68412f853c22997c7329f3a2",
                    "user": {
                        "_id": "63610db13c7147fae7de88e3",
                        "avatarUrl": "/avatars/d7e97a16cfee39e1e50d7a5b747876f1.svg",
                        "isPro": false,
                        "fullname": "Ivan Skorokhodov",
                        "user": "universome",
                        "type": "user"
                    },
                    "name": "Ivan Skorokhodov",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:55:18.188Z",
                    "hidden": false
                },
                {
                    "_id": "68412f853c22997c7329f3a3",
                    "user": {
                        "_id": "6315358a362e3e95ea538081",
                        "avatarUrl": "/avatars/3b089a25a87c2e83c6b23ccb5d2dc73e.svg",
                        "isPro": false,
                        "fullname": "Willi Menapace",
                        "user": "willi-menapace",
                        "type": "user"
                    },
                    "name": "Willi Menapace",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:55:30.315Z",
                    "hidden": false
                },
                {
                    "_id": "68412f853c22997c7329f3a4",
                    "user": {
                        "_id": "634db15dd00bb5d92c3bd94f",
                        "avatarUrl": "/avatars/9a6a1231bc5205911272d83527593f1a.svg",
                        "isPro": false,
                        "fullname": "Ashkan Mirzaei",
                        "user": "ashmrz",
                        "type": "user"
                    },
                    "name": "Ashkan Mirzaei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:55:36.943Z",
                    "hidden": false
                },
                {
                    "_id": "68412f853c22997c7329f3a5",
                    "user": {
                        "_id": "64893f0b9ab28735f1a1ec0b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64893f0b9ab28735f1a1ec0b/2lPBjV16l2qLEWOKU2zwR.jpeg",
                        "isPro": false,
                        "fullname": "Igor Gilitschenski",
                        "user": "igilitschenski",
                        "type": "user"
                    },
                    "name": "Igor Gilitschenski",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:55:42.860Z",
                    "hidden": false
                },
                {
                    "_id": "68412f853c22997c7329f3a6",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                },
                {
                    "_id": "68412f853c22997c7329f3a7",
                    "user": {
                        "_id": "64276311eb9a0ed86180715b",
                        "avatarUrl": "/avatars/76f933cd549f10e5e2db379de235d304.svg",
                        "isPro": false,
                        "fullname": "Aliaksandr Siarohin",
                        "user": "aliaksandr-siarohin",
                        "type": "user"
                    },
                    "name": "Aliaksandr Siarohin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:55:52.883Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T03:06:08.000Z",
            "submittedOnDailyAt": "2025-06-05T04:18:57.242Z",
            "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "62980664ff0acd7e027d6686",
                "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
                "isPro": false,
                "fullname": "Ziyi Wu",
                "user": "Dazitu616",
                "type": "user"
            },
            "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.",
            "upvotes": 12,
            "discussionId": "68412f8a3c22997c7329f4ff",
            "projectPage": "https://snap-research.github.io/DenseDPO/"
        },
        "publishedAt": "2025-06-03T23:06:08.000Z",
        "title": "DenseDPO: Fine-Grained Temporal Preference Optimization for Video\n  Diffusion Models",
        "summary": "Direct Preference Optimization (DPO) has recently been applied as a\npost-training technique for text-to-video diffusion models. To obtain training\ndata, annotators are asked to provide preferences between two videos generated\nfrom independent noise. However, this approach prohibits fine-grained\ncomparisons, and we point out that it biases the annotators towards low-motion\nclips as they often contain fewer visual artifacts. In this work, we introduce\nDenseDPO, a method that addresses these shortcomings by making three\ncontributions. First, we create each video pair for DPO by denoising corrupted\ncopies of a ground truth video. This results in aligned pairs with similar\nmotion structures while differing in local details, effectively neutralizing\nthe motion bias. Second, we leverage the resulting temporal alignment to label\npreferences on short segments rather than entire clips, yielding a denser and\nmore precise learning signal. With only one-third of the labeled data, DenseDPO\ngreatly improves motion generation over vanilla DPO, while matching it in text\nalignment, visual quality, and temporal consistency. Finally, we show that\nDenseDPO unlocks automatic preference annotation using off-the-shelf Vision\nLanguage Models (VLMs): GPT accurately predicts segment-level preferences\nsimilar to task-specifically fine-tuned video reward models, and DenseDPO\ntrained on these labels achieves performance close to using human labels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03517.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62980664ff0acd7e027d6686",
            "avatarUrl": "/avatars/364d4c8432c24775a099641fc576dbdc.svg",
            "fullname": "Ziyi Wu",
            "name": "Dazitu616",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.24500",
            "authors": [
                {
                    "_id": "683fb063ef97de05eb2a44cc",
                    "user": {
                        "_id": "67c03110e8c7d56a8e135ac8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
                        "isPro": false,
                        "fullname": "Hou",
                        "user": "Guiyang1001",
                        "type": "user"
                    },
                    "name": "Guiyang Hou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:52:47.325Z",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44cd",
                    "name": "Xing Gao",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44ce",
                    "user": {
                        "_id": "677f6b5883442ad1180a0087",
                        "avatarUrl": "/avatars/fe928cbed47b21439db90e75264e9a72.svg",
                        "isPro": false,
                        "fullname": "yuchuan wu",
                        "user": "yuchuan123",
                        "type": "user"
                    },
                    "name": "Yuchuan Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:53:41.047Z",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44cf",
                    "name": "Xiang Huang",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44d0",
                    "name": "Wenqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44d1",
                    "name": "Zhe Zheng",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44d2",
                    "user": {
                        "_id": "5e1058e9fcf41d740b69966d",
                        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                        "isPro": false,
                        "fullname": "Yongliang Shen",
                        "user": "tricktreat",
                        "type": "user"
                    },
                    "name": "Yongliang Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:53:22.591Z",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44d3",
                    "user": {
                        "_id": "682e8a9b51706f69070c4b13",
                        "avatarUrl": "/avatars/0233c62d21bccc9de6816d74f78f6f38.svg",
                        "isPro": false,
                        "fullname": "Jialu Du",
                        "user": "dujialulu",
                        "type": "user"
                    },
                    "name": "Jialu Du",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:53:09.282Z",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44d4",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44d5",
                    "user": {
                        "_id": "66641b2fd8e1e34bc621e688",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66641b2fd8e1e34bc621e688/csPETwnx2zCIHSWi9uAi-.png",
                        "isPro": false,
                        "fullname": "Yongbin Li",
                        "user": "Yongbin-Li",
                        "type": "user"
                    },
                    "name": "Yongbin Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:53:01.584Z",
                    "hidden": false
                },
                {
                    "_id": "683fb063ef97de05eb2a44d6",
                    "name": "Weiming Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-30T12:01:06.000Z",
            "submittedOnDailyAt": "2025-06-05T00:45:50.911Z",
            "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
            "submittedOnDailyBy": {
                "_id": "67c03110e8c7d56a8e135ac8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
                "isPro": false,
                "fullname": "Hou",
                "user": "Guiyang1001",
                "type": "user"
            },
            "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.",
            "upvotes": 11,
            "discussionId": "683fb064ef97de05eb2a452b",
            "ai_summary": "Temporal-aware Hierarchical Cognitive Reinforcement Learning enhances LLMs' social intelligence by addressing the distinct cognitive demands of social domains.",
            "ai_keywords": [
                "Large Language Models",
                "Temporal-aware Hierarchical Cognitive Reinforcement Learning",
                "TimeHC-RL",
                "System 1",
                "System 2",
                "RL",
                "DeepSeek-R1",
                "OpenAI-O3"
            ]
        },
        "publishedAt": "2025-05-30T08:01:06.000Z",
        "title": "TimeHC-RL: Temporal-aware Hierarchical Cognitive Reinforcement Learning\n  for Enhancing LLMs' Social Intelligence",
        "summary": "Recently, Large Language Models (LLMs) have made significant progress in\nIQ-related domains that require careful thinking, such as mathematics and\ncoding. However, enhancing LLMs' cognitive development in social domains,\nparticularly from a post-training perspective, remains underexplored.\nRecognizing that the social world follows a distinct timeline and requires a\nricher blend of cognitive modes (from intuitive reactions (System 1) and\nsurface-level thinking to deliberate thinking (System 2)) than mathematics,\nwhich primarily relies on System 2 cognition (careful, step-by-step reasoning),\nwe introduce Temporal-aware Hierarchical Cognitive Reinforcement Learning\n(TimeHC-RL) for enhancing LLMs' social intelligence. In our experiments, we\nsystematically explore improving LLMs' social intelligence and validate the\neffectiveness of the TimeHC-RL method, through five other post-training\nparadigms and two test-time intervention paradigms on eight datasets with\ndiverse data patterns. Experimental results reveal the superiority of our\nproposed TimeHC-RL method compared to the widely adopted System 2 RL method. It\ngives the 7B backbone model wings, enabling it to rival the performance of\nadvanced models like DeepSeek-R1 and OpenAI-O3. Additionally, the systematic\nexploration from post-training and test-time interventions perspectives to\nimprove LLMs' social intelligence has uncovered several valuable insights.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.24500.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67c03110e8c7d56a8e135ac8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/eP3y_8_tyB8tcrT7py4L7.png",
            "fullname": "Hou",
            "name": "Guiyang1001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04108",
            "authors": [
                {
                    "_id": "684104a16b106ae42f5acc1a",
                    "user": {
                        "_id": "6300ef4779c5ddbc6cf83e1a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Yutao Sun",
                        "user": "sunyt32",
                        "type": "user"
                    },
                    "name": "Yutao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:54:25.439Z",
                    "hidden": false
                },
                {
                    "_id": "684104a16b106ae42f5acc1b",
                    "name": "Tianzhu Ye",
                    "hidden": false
                },
                {
                    "_id": "684104a16b106ae42f5acc1c",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "684104a16b106ae42f5acc1d",
                    "name": "Yuqing Xia",
                    "hidden": false
                },
                {
                    "_id": "684104a16b106ae42f5acc1e",
                    "name": "Jian Chen",
                    "hidden": false
                },
                {
                    "_id": "684104a16b106ae42f5acc1f",
                    "name": "Yizhao Gao",
                    "hidden": false
                },
                {
                    "_id": "684104a16b106ae42f5acc20",
                    "name": "Shijie Cao",
                    "hidden": false
                },
                {
                    "_id": "684104a16b106ae42f5acc21",
                    "name": "Jianyong Wang",
                    "hidden": false
                },
                {
                    "_id": "684104a16b106ae42f5acc22",
                    "user": {
                        "_id": "6368c512fbfe97c16a40baba",
                        "avatarUrl": "/avatars/1c23bc7c0b6d9225699ce27647623d7a.svg",
                        "isPro": false,
                        "fullname": "Furu Wei",
                        "user": "thegenerality",
                        "type": "user"
                    },
                    "name": "Furu Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:54:47.127Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T16:01:48.000Z",
            "submittedOnDailyAt": "2025-06-05T01:16:28.444Z",
            "title": "Rectified Sparse Attention",
            "submittedOnDailyBy": {
                "_id": "6300ef4779c5ddbc6cf83e1a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
                "isPro": false,
                "fullname": "Yutao Sun",
                "user": "sunyt32",
                "type": "user"
            },
            "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42times end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
            "upvotes": 8,
            "discussionId": "684104a26b106ae42f5acc50",
            "ai_summary": "Rectified Sparse Attention (ReSA) improves the efficiency of long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification, maintaining high-quality generation.",
            "ai_keywords": [
                "sparse decoding",
                "KV cache misalignment",
                "Rectified Sparse Attention",
                "ReSA",
                "block-sparse attention",
                "dense rectification",
                "pretraining distribution",
                "long-context inference"
            ]
        },
        "publishedAt": "2025-06-04T12:01:48.000Z",
        "title": "Rectified Sparse Attention",
        "summary": "Efficient long-sequence generation is a critical challenge for Large Language\nModels. While recent sparse decoding methods improve efficiency, they suffer\nfrom KV cache misalignment, where approximation errors accumulate and degrade\ngeneration quality. In this work, we propose Rectified Sparse Attention (ReSA),\na simple yet effective method that combines block-sparse attention with\nperiodic dense rectification. By refreshing the KV cache at fixed intervals\nusing a dense forward pass, ReSA bounds error accumulation and preserves\nalignment with the pretraining distribution. Experiments across math reasoning,\nlanguage modeling, and retrieval tasks demonstrate that ReSA achieves\nnear-lossless generation quality with significantly improved efficiency.\nNotably, ReSA delivers up to 2.42times end-to-end speedup under decoding at\n256K sequence length, making it a practical solution for scalable long-context\ninference. Code is available at https://aka.ms/ReSA-LM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04108.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6300ef4779c5ddbc6cf83e1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661005591657-noauth.jpeg",
            "fullname": "Yutao Sun",
            "name": "sunyt32",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02592",
            "authors": [
                {
                    "_id": "684104c89ec96d9991484c24",
                    "user": {
                        "_id": "65309a1d657ae56cdb65e0e7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
                        "isPro": false,
                        "fullname": "Zhi-Yuan Chen",
                        "user": "JaxChen",
                        "type": "user"
                    },
                    "name": "Zhi-Yuan Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-05T02:45:29.221Z",
                    "hidden": false
                },
                {
                    "_id": "684104c89ec96d9991484c25",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "684104c89ec96d9991484c26",
                    "name": "Xinyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "684104c89ec96d9991484c27",
                    "name": "Enrui Hu",
                    "hidden": false
                },
                {
                    "_id": "684104c89ec96d9991484c28",
                    "name": "Yankai Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T08:12:47.000Z",
            "submittedOnDailyAt": "2025-06-05T01:30:17.419Z",
            "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
            "submittedOnDailyBy": {
                "_id": "65309a1d657ae56cdb65e0e7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
                "isPro": false,
                "fullname": "Zhi-Yuan Chen",
                "user": "JaxChen",
                "type": "user"
            },
            "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.",
            "upvotes": 8,
            "discussionId": "684104c99ec96d9991484c5e",
            "githubRepo": "https://github.com/zhiyuanc2001/self-preference",
            "ai_summary": "The DBG score is introduced to measure self-preference bias in large language models by using gold judgments as proxies for response quality, addressing the confounding effect of response quality.",
            "ai_keywords": [
                "large language models",
                "self-preference bias",
                "judge model",
                "gold judgments",
                "DBG score",
                "response quality",
                "attention-based perspective"
            ]
        },
        "publishedAt": "2025-06-03T04:12:47.000Z",
        "title": "Beyond the Surface: Measuring Self-Preference in LLM Judgments",
        "summary": "Recent studies show that large language models (LLMs) exhibit self-preference\nbias when serving as judges, meaning they tend to favor their own responses\nover those generated by other models. Existing methods typically measure this\nbias by calculating the difference between the scores a judge model assigns to\nits own responses and those it assigns to responses from other models. However,\nthis approach conflates self-preference bias with response quality, as\nhigher-quality responses from the judge model may also lead to positive score\ndifferences, even in the absence of bias. To address this issue, we introduce\ngold judgments as proxies for the actual quality of responses and propose the\nDBG score, which measures self-preference bias as the difference between the\nscores assigned by the judge model to its own responses and the corresponding\ngold judgments. Since gold judgments reflect true response quality, the DBG\nscore mitigates the confounding effect of response quality on bias measurement.\nUsing the DBG score, we conduct comprehensive experiments to assess\nself-preference bias across LLMs of varying versions, sizes, and reasoning\nabilities. Additionally, we investigate two factors that influence and help\nalleviate self-preference bias: response text style and the post-training data\nof judge models. Finally, we explore potential underlying mechanisms of\nself-preference bias from an attention-based perspective. Our code and data are\navailable at https://github.com/zhiyuanc2001/self-preference.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02592.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65309a1d657ae56cdb65e0e7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHQI9RNjfz8E5v1uyCGeV.png",
            "fullname": "Zhi-Yuan Chen",
            "name": "JaxChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03610",
            "authors": [
                {
                    "_id": "684194728986af36f6a17bd6",
                    "name": "Dongmin Park",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17bd7",
                    "name": "Minkyu Kim",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17bd8",
                    "name": "Beongjun Choi",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17bd9",
                    "name": "Junhyuck Kim",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17bda",
                    "name": "Keon Lee",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17bdb",
                    "name": "Jonghyun Lee",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17bdc",
                    "name": "Inkyu Park",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17bdd",
                    "name": "Byeong-Uk Lee",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17bde",
                    "name": "Jaeyoung Hwang",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17bdf",
                    "user": {
                        "_id": "64bb081c01f1983a863654dc",
                        "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
                        "isPro": false,
                        "fullname": "Jaewoo Ahn",
                        "user": "ahnpersie",
                        "type": "user"
                    },
                    "name": "Jaewoo Ahn",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T15:07:52.251Z",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17be0",
                    "name": "Ameya S. Mahabaleshwarkar",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17be1",
                    "name": "Bilal Kartal",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17be2",
                    "name": "Pritam Biswas",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17be3",
                    "name": "Yoshi Suhara",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17be4",
                    "name": "Kangwook Lee",
                    "hidden": false
                },
                {
                    "_id": "684194728986af36f6a17be5",
                    "name": "Jaewoong Cho",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T06:40:33.000Z",
            "submittedOnDailyAt": "2025-06-05T11:32:47.155Z",
            "title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on\n  Diverse Video Games",
            "submittedOnDailyBy": {
                "_id": "638a15ec2e5eabcc722954a5",
                "avatarUrl": "/avatars/30abc5608ef8e93c04065a873c102d7d.svg",
                "isPro": false,
                "fullname": "Dongmin Park",
                "user": "dongminpark",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) agents are reshaping the game industry,\nparticularly with more intelligent and human-preferable game characters.\nHowever, existing game benchmarks fall short of practical needs: they lack\nevaluations of diverse LLM capabilities across various game genres, studies of\nagentic modules crucial for complex gameplay, and fine-tuning datasets for\naligning pre-trained LLMs into gaming agents. To fill these gaps, we present\n\\benchname{}, a foundational benchmark designed to train and evaluate\nLLM agents across diverse real-world video games. Unlike existing benchmarks,\nOrak includes 12 popular video games spanning all major genres, enabling\ncomprehensive studies of LLM capabilities and agentic modules essential for\nintricate game scenarios. To support consistent evaluation of LLMs, we\nintroduce a plug-and-play interface based on Model Context Protocol (MCP) that\nenables LLMs to seamlessly connect with games and manipulate agentic modules.\nAdditionally, we propose a fine-tuning dataset, consisting of LLM gameplay\ntrajectories across diverse game genres. Orak offers a comprehensive evaluation\nframework, encompassing general game score leaderboards, LLM battle arenas, and\nin-depth analyses of visual input state, agentic strategies, and fine-tuning\neffects, establishing a foundation towards building generic gaming agents. Code\nis available at https://github.com/krafton-ai/Orak.",
            "upvotes": 7,
            "discussionId": "684194748986af36f6a17c58",
            "githubRepo": "https://github.com/krafton-ai/Orak",
            "ai_summary": "Orak is a benchmark for training and evaluating LLM agents across diverse video games, featuring a plug-and-play interface and fine-tuning datasets to enhance agentic modules and gameplay.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "game characters",
                "game benchmarks",
                "agentic modules",
                "fine-tuning datasets",
                "Orak",
                "Model Context Protocol (MCP)",
                "gameplay trajectories",
                "game score leaderboards",
                "LLM battle arenas",
                "visual input state",
                "agentic strategies"
            ]
        },
        "publishedAt": "2025-06-04T02:40:33.000Z",
        "title": "Orak: A Foundational Benchmark for Training and Evaluating LLM Agents on\n  Diverse Video Games",
        "summary": "Large Language Model (LLM) agents are reshaping the game industry,\nparticularly with more intelligent and human-preferable game characters.\nHowever, existing game benchmarks fall short of practical needs: they lack\nevaluations of diverse LLM capabilities across various game genres, studies of\nagentic modules crucial for complex gameplay, and fine-tuning datasets for\naligning pre-trained LLMs into gaming agents. To fill these gaps, we present\n\\benchname{}, a foundational benchmark designed to train and evaluate\nLLM agents across diverse real-world video games. Unlike existing benchmarks,\nOrak includes 12 popular video games spanning all major genres, enabling\ncomprehensive studies of LLM capabilities and agentic modules essential for\nintricate game scenarios. To support consistent evaluation of LLMs, we\nintroduce a plug-and-play interface based on Model Context Protocol (MCP) that\nenables LLMs to seamlessly connect with games and manipulate agentic modules.\nAdditionally, we propose a fine-tuning dataset, consisting of LLM gameplay\ntrajectories across diverse game genres. Orak offers a comprehensive evaluation\nframework, encompassing general game score leaderboards, LLM battle arenas, and\nin-depth analyses of visual input state, agentic strategies, and fine-tuning\neffects, establishing a foundation towards building generic gaming agents. Code\nis available at https://github.com/krafton-ai/Orak.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03610.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638a15ec2e5eabcc722954a5",
            "avatarUrl": "/avatars/30abc5608ef8e93c04065a873c102d7d.svg",
            "fullname": "Dongmin Park",
            "name": "dongminpark",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03099",
            "authors": [
                {
                    "_id": "684107c6142b5c0b4226025f",
                    "name": "Chetwin Low",
                    "hidden": false
                },
                {
                    "_id": "684107c6142b5c0b42260260",
                    "name": "Weimin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:29:28.000Z",
            "submittedOnDailyAt": "2025-06-05T01:30:00.782Z",
            "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "62b43ffec624a43b1a1ada46",
                "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
                "isPro": false,
                "fullname": "weimin wang ",
                "user": "weiminwang",
                "type": "user"
            },
            "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
            "upvotes": 7,
            "discussionId": "684107c8142b5c0b42260293",
            "projectPage": "https://aaxwaz.github.io/TalkingMachines/",
            "githubRepo": "https://github.com/aaxwaz/TalkingMachines",
            "ai_summary": "TalkingMachines transforms a pretrained image-to-video model into an audio-driven avatar generator, supports infinite video streaming, and uses engineering optimizations for real-time performance.",
            "ai_keywords": [
                "DiT",
                "audio large language model",
                "asymmetric knowledge distillation",
                "bidirectional teacher model",
                "sparse causal",
                "autoregressive student model",
                "inference pipeline",
                "CUDA streams",
                "frame-generation throughput"
            ]
        },
        "publishedAt": "2025-06-03T13:29:28.000Z",
        "title": "TalkingMachines: Real-Time Audio-Driven FaceTime-Style Video via\n  Autoregressive Diffusion Models",
        "summary": "In this paper, we present TalkingMachines -- an efficient framework that\ntransforms pretrained video generation models into real-time, audio-driven\ncharacter animators. TalkingMachines enables natural conversational experiences\nby integrating an audio large language model (LLM) with our video generation\nfoundation model. Our primary contributions include: (1) We adapt a pretrained\nSOTA image-to-video DiT into an audio-driven avatar generation model of 18\nbillion parameters; (2) We enable infinite video streaming without error\naccumulation through asymmetric knowledge distillation from a bidirectional\nteacher model into a sparse causal, autoregressive student model; (3) We design\na high-throughput, low-latency inference pipeline incorporating several key\nengineering optimizations such as: (a) disaggregation of the DiT and VAE\ndecoder across separate devices, (b) efficient overlap of inter-device\ncommunication and computation using CUDA streams, (c) elimination of redundant\nrecomputations to maximize frame-generation throughput. Please see demo videos\nhere - https://aaxwaz.github.io/TalkingMachines/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03099.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b43ffec624a43b1a1ada46",
            "avatarUrl": "/avatars/77298e99d2797cf917fdddc6d6de46eb.svg",
            "fullname": "weimin wang ",
            "name": "weiminwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03355",
            "authors": [
                {
                    "_id": "68415c5cce09e3eca94e9839",
                    "name": "Elias Abad Rocamora",
                    "hidden": false
                },
                {
                    "_id": "68415c5cce09e3eca94e983a",
                    "user": {
                        "_id": "6310a6bb0a43f97f6c5567d3",
                        "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
                        "isPro": false,
                        "fullname": "Christian Schlarmann",
                        "user": "chs20",
                        "type": "user"
                    },
                    "name": "Christian Schlarmann",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T09:59:48.051Z",
                    "hidden": false
                },
                {
                    "_id": "68415c5cce09e3eca94e983b",
                    "name": "Naman Deep Singh",
                    "hidden": false
                },
                {
                    "_id": "68415c5cce09e3eca94e983c",
                    "name": "Yongtao Wu",
                    "hidden": false
                },
                {
                    "_id": "68415c5cce09e3eca94e983d",
                    "name": "Matthias Hein",
                    "hidden": false
                },
                {
                    "_id": "68415c5cce09e3eca94e983e",
                    "name": "Volkan Cevher",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T19:57:09.000Z",
            "submittedOnDailyAt": "2025-06-05T07:29:30.561Z",
            "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
            "submittedOnDailyBy": {
                "_id": "6310a6bb0a43f97f6c5567d3",
                "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
                "isPro": false,
                "fullname": "Christian Schlarmann",
                "user": "chs20",
                "type": "user"
            },
            "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization.",
            "upvotes": 6,
            "discussionId": "68415c5ece09e3eca94e98e4",
            "ai_summary": "LEAF, an adversarial finetuning method, enhances the robustness of CLIP text encoders, improving zero-shot accuracy and multimodal retrieval performance under adversarial noise.",
            "ai_keywords": [
                "adversarial input attacks",
                "CLIP embeddings",
                "text-to-image generative models",
                "large vision language models",
                "adversarial finetuning",
                "zero-shot adversarial accuracy",
                "text-to-image diffusion models",
                "multimodal retrieval tasks",
                "robust text encoders"
            ]
        },
        "publishedAt": "2025-06-03T15:57:09.000Z",
        "title": "Robustness in Both Domains: CLIP Needs a Robust Text Encoder",
        "summary": "Adversarial input attacks can cause a significant shift of CLIP embeddings.\nThis can affect the downstream robustness of models incorporating CLIP in the\npipeline, such as text-to-image generative models or large vision language\nmodels. While some efforts have been done towards making the CLIP image\nencoders robust, the robustness of text encoders remains unexplored. In this\nwork, we cover this gap in the literature. We propose LEAF: an efficient\nadversarial finetuning method for the text domain, with the ability to scale to\nlarge CLIP models. Our models significantly improve the zero-shot adversarial\naccuracy in the text domain, while maintaining the vision performance provided\nby robust image encoders. When combined with text-to-image diffusion models, we\ncan improve the generation quality under adversarial noise. When employing our\nrobust CLIP encoders in multimodal retrieval tasks, we improve the recall under\nadversarial noise over standard CLIP models. Finally, we show that robust text\nencoders facilitate better reconstruction of input text from its embedding via\ndirect optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03355.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6310a6bb0a43f97f6c5567d3",
            "avatarUrl": "/avatars/04b07c3a6c811337939c951567cb2bf2.svg",
            "fullname": "Christian Schlarmann",
            "name": "chs20",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03106",
            "authors": [
                {
                    "_id": "684104d9ee7646c073776b2e",
                    "name": "Xiaoying Zhang",
                    "hidden": false
                },
                {
                    "_id": "684104d9ee7646c073776b2f",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "684104d9ee7646c073776b30",
                    "user": {
                        "_id": "666e91b1623133f1ce35acc5",
                        "avatarUrl": "/avatars/cc78520e6cfb83817c1d0c1ac867ebdd.svg",
                        "isPro": false,
                        "fullname": "YipengZhang",
                        "user": "YipengZhang",
                        "type": "user"
                    },
                    "name": "Yipeng Zhang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-05T02:45:46.851Z",
                    "hidden": false
                },
                {
                    "_id": "684104d9ee7646c073776b31",
                    "name": "Kaituo Feng",
                    "hidden": false
                },
                {
                    "_id": "684104d9ee7646c073776b32",
                    "name": "Chaochao Lu",
                    "hidden": false
                },
                {
                    "_id": "684104d9ee7646c073776b33",
                    "name": "Chao Yang",
                    "hidden": false
                },
                {
                    "_id": "684104d9ee7646c073776b34",
                    "name": "Helen Meng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T17:39:02.000Z",
            "submittedOnDailyAt": "2025-06-05T01:16:53.836Z",
            "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
            "submittedOnDailyBy": {
                "_id": "67079840a9bcb7459b8d2a46",
                "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
                "isPro": false,
                "fullname": "Kaituo Feng",
                "user": "KaituoFeng",
                "type": "user"
            },
            "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
            "upvotes": 6,
            "discussionId": "684104daee7646c073776b88",
            "githubRepo": "https://github.com/zhangxy-2019/critique-GRPO",
            "ai_summary": "Critique-GRPO, an RL framework combining numerical and natural language feedback, enhances LLM reasoning across tasks and outperforms existing methods.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "large language models",
                "LLMs",
                "scalar rewards",
                "performance plateaus",
                "self-reflection",
                "persistent failures",
                "natural language feedback",
                "critiques",
                "policy optimization",
                "Qwen2.5-7B-Base",
                "Qwen3-8B-Base",
                "pass@1",
                "policy exploration",
                "entropy",
                "response length"
            ]
        },
        "publishedAt": "2025-06-03T13:39:02.000Z",
        "title": "Critique-GRPO: Advancing LLM Reasoning with Natural Language and\n  Numerical Feedback",
        "summary": "Recent advances in reinforcement learning (RL) with numerical feedback, such\nas scalar rewards, have significantly enhanced the complex reasoning\ncapabilities of large language models (LLMs). Despite this success, we identify\nthree key challenges encountered by RL with solely numerical feedback:\nperformance plateaus, limited effectiveness of self-reflection, and persistent\nfailures. We then demonstrate that RL-finetuned models, even after exhibiting\nperformance plateaus, can generate correct refinements on persistently failed\nproblems by leveraging natural language feedback in the form of critiques.\nBuilding on this insight, we propose Critique-GRPO, an online RL framework that\nintegrates both natural language and numerical feedback for effective policy\noptimization. Critique-GRPO enables LLMs to learn from initial responses and\ncritique-guided refinements simultaneously while maintaining exploration.\nExtensive experiments using Qwen2.5-7B-Base and Qwen3-8B-Base show that\nCritique-GRPO consistently outperforms supervised learning-based and RL-based\nfine-tuning approaches across eight challenging mathematical, STEM, and general\nreasoning tasks, improving average pass@1 scores by approximately 4.5% and 5%,\nrespectively. Notably, Critique-GRPO surpasses a strong baseline that\nincorporates expert demonstrations within online RL. Further analysis reveals\ntwo critical insights about policy exploration: (1) higher entropy does not\nalways guarantee efficient learning from exploration, and (2) longer responses\ndo not necessarily lead to more effective exploration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03106.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67079840a9bcb7459b8d2a46",
            "avatarUrl": "/avatars/32466863c5554f20cb2775b138832ac3.svg",
            "fullname": "Kaituo Feng",
            "name": "KaituoFeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.00482",
            "authors": [
                {
                    "_id": "68402986a50b67f983749710",
                    "user": {
                        "_id": "6576ace7769f3ee9bd7b1b88",
                        "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
                        "isPro": false,
                        "fullname": "Eunsu Kim",
                        "user": "EunsuKim",
                        "type": "user"
                    },
                    "name": "Eunsu Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:27:49.056Z",
                    "hidden": false
                },
                {
                    "_id": "68402986a50b67f983749711",
                    "name": "Haneul Yoo",
                    "hidden": false
                },
                {
                    "_id": "68402986a50b67f983749712",
                    "name": "Guijin Son",
                    "hidden": false
                },
                {
                    "_id": "68402986a50b67f983749713",
                    "name": "Hitesh Patel",
                    "hidden": false
                },
                {
                    "_id": "68402986a50b67f983749714",
                    "name": "Amit Agarwal",
                    "hidden": false
                },
                {
                    "_id": "68402986a50b67f983749715",
                    "user": {
                        "_id": "60e0251ea9b5d8282481f2b7",
                        "avatarUrl": "/avatars/43441373af054a6184c22097bfeb97e4.svg",
                        "isPro": false,
                        "fullname": "Alice Oh",
                        "user": "aliceoh",
                        "type": "user"
                    },
                    "name": "Alice Oh",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-05T06:37:21.889Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T09:24:32.000Z",
            "submittedOnDailyAt": "2025-06-05T04:40:48.000Z",
            "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation",
            "submittedOnDailyBy": {
                "_id": "6576ace7769f3ee9bd7b1b88",
                "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
                "isPro": false,
                "fullname": "Eunsu Kim",
                "user": "EunsuKim",
                "type": "user"
            },
            "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research.",
            "upvotes": 6,
            "discussionId": "68402987a50b67f983749746",
            "projectPage": "https://huggingface.co/BenchHub",
            "githubRepo": "https://github.com/rladmstn1714/BenchHub",
            "ai_summary": "BenchHub is a dynamic benchmark repository that aggregates and classifies datasets for large language models, facilitating domain-specific evaluations and improving model comparisons.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "BenchHub",
                "benchmark repository",
                "domain-specific models",
                "benchmark datasets",
                "continuous updates",
                "scalable data management",
                "model performance",
                "domain-aware benchmarking"
            ]
        },
        "publishedAt": "2025-05-31T05:24:32.000Z",
        "title": "BenchHub: A Unified Benchmark Suite for Holistic and Customizable LLM\n  Evaluation",
        "summary": "As large language models (LLMs) continue to advance, the need for up-to-date\nand well-organized benchmarks becomes increasingly critical. However, many\nexisting datasets are scattered, difficult to manage, and make it challenging\nto perform evaluations tailored to specific needs or domains, despite the\ngrowing importance of domain-specific models in areas such as math or code. In\nthis paper, we introduce BenchHub, a dynamic benchmark repository that empowers\nresearchers and developers to evaluate LLMs more effectively. BenchHub\naggregates and automatically classifies benchmark datasets from diverse\ndomains, integrating 303K questions across 38 benchmarks. It is designed to\nsupport continuous updates and scalable data management, enabling flexible and\ncustomizable evaluation tailored to various domains or use cases. Through\nextensive experiments with various LLM families, we demonstrate that model\nperformance varies significantly across domain-specific subsets, emphasizing\nthe importance of domain-aware benchmarking. We believe BenchHub can encourage\nbetter dataset reuse, more transparent model comparisons, and easier\nidentification of underrepresented areas in existing benchmarks, offering a\ncritical infrastructure for advancing LLM evaluation research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00482.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6576ace7769f3ee9bd7b1b88",
            "avatarUrl": "/avatars/5b5921e54413a37afde6ce017809c86e.svg",
            "fullname": "Eunsu Kim",
            "name": "EunsuKim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.21541",
            "authors": [
                {
                    "_id": "6840f79ceb249b555b244efc",
                    "name": "Zitong Wang",
                    "hidden": false
                },
                {
                    "_id": "6840f79ceb249b555b244efd",
                    "name": "Hang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6840f79ceb249b555b244efe",
                    "name": "Qianyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "6840f79ceb249b555b244eff",
                    "name": "Xuequan Lu",
                    "hidden": false
                },
                {
                    "_id": "6840f79ceb249b555b244f00",
                    "user": {
                        "_id": "63958b4414513eaf9029ebf1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
                        "isPro": false,
                        "fullname": "Xiangtai Li",
                        "user": "LXT",
                        "type": "user"
                    },
                    "name": "Xiangtai Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:57:04.933Z",
                    "hidden": false
                },
                {
                    "_id": "6840f79ceb249b555b244f01",
                    "user": {
                        "_id": "64311a95034ecbefddd141ef",
                        "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                        "isPro": true,
                        "fullname": "Yiren Song",
                        "user": "yiren98",
                        "type": "user"
                    },
                    "name": "Yiren Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:56:53.866Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-24T16:08:04.000Z",
            "submittedOnDailyAt": "2025-06-05T00:21:34.798Z",
            "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
            "submittedOnDailyBy": {
                "_id": "64311a95034ecbefddd141ef",
                "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                "isPro": true,
                "fullname": "Yiren Song",
                "user": "yiren98",
                "type": "user"
            },
            "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.",
            "upvotes": 6,
            "discussionId": "6840f7a1eb249b555b244ffe",
            "ai_summary": "DiffDecompose, a diffusion Transformer-based framework, effectively decomposes images into constituent layers with semantic prompts, addressing challenges in transparent layer decomposition.",
            "ai_keywords": [
                "diffusion models",
                "diffusion Transformer",
                "posterior",
                "semantic prompts",
                "blending type",
                "In-Context Decomposition",
                "Layer Position Encoding Cloning",
                "AlphaBlend dataset",
                "translucent flare removal",
                "semi-transparent cell decomposition",
                "glassware decomposition",
                "LOGO dataset"
            ]
        },
        "publishedAt": "2025-05-24T12:08:04.000Z",
        "title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via\n  Diffusion Transformers",
        "summary": "Diffusion models have recently motivated great success in many generation\ntasks like object removal. Nevertheless, existing image decomposition methods\nstruggle to disentangle semi-transparent or transparent layer occlusions due to\nmask prior dependencies, static object assumptions, and the lack of datasets.\nIn this paper, we delve into a novel task: Layer-Wise Decomposition of\nAlpha-Composited Images, aiming to recover constituent layers from single\noverlapped images under the condition of semi-transparent/transparent alpha\nlayer non-linear occlusion. To address challenges in layer ambiguity,\ngeneralization, and data scarcity, we first introduce AlphaBlend, the first\nlarge-scale and high-quality dataset for transparent and semi-transparent layer\ndecomposition, supporting six real-world subtasks (e.g., translucent flare\nremoval, semi-transparent cell decomposition, glassware decomposition).\nBuilding on this dataset, we present DiffDecompose, a diffusion\nTransformer-based framework that learns the posterior over possible layer\ndecompositions conditioned on the input image, semantic prompts, and blending\ntype. Rather than regressing alpha mattes directly, DiffDecompose performs\nIn-Context Decomposition, enabling the model to predict one or multiple layers\nwithout per-layer supervision, and introduces Layer Position Encoding Cloning\nto maintain pixel-level correspondence across layers. Extensive experiments on\nthe proposed AlphaBlend dataset and public LOGO dataset verify the\neffectiveness of DiffDecompose. The code and dataset will be available upon\npaper acceptance. Our code will be available at:\nhttps://github.com/Wangzt1121/DiffDecompose.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21541.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64311a95034ecbefddd141ef",
            "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
            "fullname": "Yiren Song",
            "name": "yiren98",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03956",
            "authors": [
                {
                    "_id": "6841396eee27975702b57e87",
                    "user": {
                        "_id": "6759546743971eff5a12a087",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
                        "isPro": false,
                        "fullname": "Aojun Lu",
                        "user": "Kurt1024",
                        "type": "user"
                    },
                    "name": "Aojun Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:29.156Z",
                    "hidden": false
                },
                {
                    "_id": "6841396eee27975702b57e88",
                    "name": "Tao Feng",
                    "hidden": false
                },
                {
                    "_id": "6841396eee27975702b57e89",
                    "user": {
                        "_id": "649d54b314afbb10ce2a9eeb",
                        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                        "isPro": false,
                        "fullname": "Hangjie Yuan",
                        "user": "JacobYuan",
                        "type": "user"
                    },
                    "name": "Hangjie Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:31.411Z",
                    "hidden": false
                },
                {
                    "_id": "6841396eee27975702b57e8a",
                    "name": "Chunhui Ding",
                    "hidden": false
                },
                {
                    "_id": "6841396eee27975702b57e8b",
                    "name": "Yanan Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T13:46:33.000Z",
            "submittedOnDailyAt": "2025-06-05T05:00:41.771Z",
            "title": "Adapt before Continual Learning",
            "submittedOnDailyBy": {
                "_id": "649d54b314afbb10ce2a9eeb",
                "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                "isPro": false,
                "fullname": "Hangjie Yuan",
                "user": "JacobYuan",
                "type": "user"
            },
            "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.",
            "upvotes": 5,
            "discussionId": "6841396eee27975702b57eb7",
            "projectPage": "https://github.com/byyx666/ACL_code",
            "githubRepo": "https://github.com/byyx666/ACL_code",
            "ai_summary": "Adapting Pre-trained Models before the core CL process (ACL) improves Continual Learning by enhancing plasticity while maintaining stability.",
            "ai_keywords": [
                "Continual Learning",
                "CL",
                "Pre-trained models",
                "PTMs",
                "plasticity",
                "stability",
                "domain gaps",
                "catastrophic forgetting",
                "prompt tuning",
                "embeddings",
                "class prototypes"
            ]
        },
        "publishedAt": "2025-06-04T09:46:33.000Z",
        "title": "Adapt before Continual Learning",
        "summary": "Continual Learning (CL) seeks to enable neural networks to incrementally\nacquire new knowledge (plasticity) while retaining existing knowledge\n(stability). While pre-trained models (PTMs) have become pivotal in CL,\nprevailing approaches freeze the PTM backbone to preserve stability, limiting\ntheir plasticity, particularly when encountering significant domain gaps in\nincremental tasks. Conversely, sequentially finetuning the entire PTM risks\ncatastrophic forgetting of generalizable knowledge, exposing a critical\nstability-plasticity trade-off. To address this challenge, we propose Adapting\nPTMs before the core CL process (ACL), a novel framework that refines the PTM\nbackbone through a plug-and-play adaptation phase before learning each new task\nwith existing CL approaches (e.g., prompt tuning). ACL enhances plasticity by\naligning embeddings with their original class prototypes while distancing them\nfrom others, theoretically and empirically shown to balance stability and\nplasticity. Extensive experiments demonstrate that ACL significantly improves\nCL performance across benchmarks and integrated methods, offering a versatile\nsolution for PTM-based CL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03956.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "fullname": "Hangjie Yuan",
            "name": "JacobYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02863",
            "authors": [
                {
                    "_id": "683fa315cd4314c4b3d53451",
                    "name": "Helin Wang",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d53452",
                    "name": "Jiarui Hai",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d53453",
                    "name": "Dading Chong",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d53454",
                    "name": "Karan Thakkar",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d53455",
                    "name": "Tiantian Feng",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d53456",
                    "name": "Dongchao Yang",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d53457",
                    "name": "Junhyeok Lee",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d53458",
                    "name": "Laureano Moro Velazquez",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d53459",
                    "name": "Jesus Villalba",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d5345a",
                    "name": "Zengyi Qin",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d5345b",
                    "name": "Shrikanth Narayanan",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d5345c",
                    "name": "Mounya Elhiali",
                    "hidden": false
                },
                {
                    "_id": "683fa315cd4314c4b3d5345d",
                    "name": "Najim Dehak",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T13:28:55.000Z",
            "submittedOnDailyAt": "2025-06-05T19:00:43.134Z",
            "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned\n  Text-to-Speech",
            "submittedOnDailyBy": {
                "_id": "63ecfb5ec5b3c734085db9ed",
                "avatarUrl": "/avatars/0b1d03dcd7997ad1daa764fb76f88993.svg",
                "isPro": false,
                "fullname": "Helin Wang",
                "user": "westbrook",
                "type": "user"
            },
            "summary": "Recent advancements in generative artificial intelligence have significantly\ntransformed the field of style-captioned text-to-speech synthesis (CapTTS).\nHowever, adapting CapTTS to real-world applications remains challenging due to\nthe lack of standardized, comprehensive datasets and limited research on\ndownstream tasks built upon CapTTS. To address these gaps, we introduce\nCapSpeech, a new benchmark designed for a series of CapTTS-related tasks,\nincluding style-captioned text-to-speech synthesis with sound events\n(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS\n(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech\ncomprises over 10 million machine-annotated audio-caption pairs and nearly 0.36\nmillion human-annotated audio-caption pairs. In addition, we introduce two new\ndatasets collected and recorded by a professional voice actor and experienced\naudio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside\nthe datasets, we conduct comprehensive experiments using both autoregressive\nand non-autoregressive models on CapSpeech. Our results demonstrate\nhigh-fidelity and highly intelligible speech synthesis across a diverse range\nof speaking styles. To the best of our knowledge, CapSpeech is the largest\navailable dataset offering comprehensive annotations for CapTTS-related tasks.\nThe experiments and findings further provide valuable insights into the\nchallenges of developing CapTTS systems.",
            "upvotes": 5,
            "discussionId": "683fa316cd4314c4b3d53485",
            "ai_summary": "CapSpeech introduces a large benchmark dataset for various captioned text-to-speech tasks, facilitating advancements in style, accent, emotion, and chat-agent synthesis.",
            "ai_keywords": [
                "autoregressive models",
                "non-autoregressive models",
                "CapTTS",
                "CapTTS-SE",
                "AccCapTTS",
                "EmoCapTTS",
                "AgentTTS",
                "machine-annotated",
                "human-annotated"
            ]
        },
        "publishedAt": "2025-06-03T09:28:55.000Z",
        "title": "CapSpeech: Enabling Downstream Applications in Style-Captioned\n  Text-to-Speech",
        "summary": "Recent advancements in generative artificial intelligence have significantly\ntransformed the field of style-captioned text-to-speech synthesis (CapTTS).\nHowever, adapting CapTTS to real-world applications remains challenging due to\nthe lack of standardized, comprehensive datasets and limited research on\ndownstream tasks built upon CapTTS. To address these gaps, we introduce\nCapSpeech, a new benchmark designed for a series of CapTTS-related tasks,\nincluding style-captioned text-to-speech synthesis with sound events\n(CapTTS-SE), accent-captioned TTS (AccCapTTS), emotion-captioned TTS\n(EmoCapTTS), and text-to-speech synthesis for chat agent (AgentTTS). CapSpeech\ncomprises over 10 million machine-annotated audio-caption pairs and nearly 0.36\nmillion human-annotated audio-caption pairs. In addition, we introduce two new\ndatasets collected and recorded by a professional voice actor and experienced\naudio engineers, specifically for the AgentTTS and CapTTS-SE tasks. Alongside\nthe datasets, we conduct comprehensive experiments using both autoregressive\nand non-autoregressive models on CapSpeech. Our results demonstrate\nhigh-fidelity and highly intelligible speech synthesis across a diverse range\nof speaking styles. To the best of our knowledge, CapSpeech is the largest\navailable dataset offering comprehensive annotations for CapTTS-related tasks.\nThe experiments and findings further provide valuable insights into the\nchallenges of developing CapTTS systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02863.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63ecfb5ec5b3c734085db9ed",
            "avatarUrl": "/avatars/0b1d03dcd7997ad1daa764fb76f88993.svg",
            "fullname": "Helin Wang",
            "name": "westbrook",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03566",
            "authors": [
                {
                    "_id": "6841db4b1a53c86e5eceae97",
                    "name": "Langlin Huang",
                    "hidden": false
                },
                {
                    "_id": "6841db4b1a53c86e5eceae98",
                    "name": "Chengsong Huang",
                    "hidden": false
                },
                {
                    "_id": "6841db4b1a53c86e5eceae99",
                    "name": "Jixuan Leng",
                    "hidden": false
                },
                {
                    "_id": "6841db4b1a53c86e5eceae9a",
                    "name": "Di Huang",
                    "hidden": false
                },
                {
                    "_id": "6841db4b1a53c86e5eceae9b",
                    "name": "Jiaxin Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T04:30:30.000Z",
            "submittedOnDailyAt": "2025-06-05T16:31:59.474Z",
            "title": "POSS: Position Specialist Generates Better Draft for Speculative\n  Decoding",
            "submittedOnDailyBy": {
                "_id": "62ea79dd01ed9b0e8f61ccd3",
                "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
                "isPro": false,
                "fullname": "Chengsong Huang",
                "user": "ChengsongHuang",
                "type": "user"
            },
            "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS.",
            "upvotes": 4,
            "discussionId": "6841db4d1a53c86e5eceaed7",
            "githubRepo": "https://github.com/shrango/PosS",
            "ai_summary": "Position Specialists (PosS) enhance Large Language Model (LLM) inference by using position-specialized draft layers to improve token prediction accuracy and acceptance rates.",
            "ai_keywords": [
                "Speculative decoding",
                "LLM",
                "Large Language Model",
                "draft model",
                "target model",
                "token prediction",
                "Position Specialists",
                "PosS",
                "token acceptance rate",
                "feature deviation",
                "Llama-3-8B-Instruct",
                "Llama-2-13B-chat",
                "acceptance length",
                "speed-up ratio"
            ]
        },
        "publishedAt": "2025-06-04T00:30:30.000Z",
        "title": "POSS: Position Specialist Generates Better Draft for Speculative\n  Decoding",
        "summary": "Speculative decoding accelerates Large Language Model (LLM) inference by\nusing a small draft model to predict multiple tokens, and a large target model\nto verify these tokens in parallel. Recent studies leverage the hidden state of\nthe target model to enhance draft model prediction accuracy. However, existing\nmethods suffer from the degrading quality of draft token predictions at later\npositions, due to error accumulation in draft model generated features. In this\npaper, we propose Position Specialists (PosS), which consist of multiple\nposition-specialized draft layers to generate tokens at assigned position(s).\nPosition specialists greatly improve token acceptance rate at later positions\nper drafting round, as each specialist only needs to focus on handling a\ncertain level of draft model feature deviation. Experiment results on\nLlama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that\nPosS effectively improves over baselines on average acceptance length and\nspeed-up ratio. Our codebase is available at https://github.com/shrango/PosS.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03566.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ea79dd01ed9b0e8f61ccd3",
            "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
            "fullname": "Chengsong Huang",
            "name": "ChengsongHuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03525",
            "authors": [
                {
                    "_id": "68419d4b9892f271ddba2ff5",
                    "name": "Daeun Lee",
                    "hidden": false
                },
                {
                    "_id": "68419d4b9892f271ddba2ff6",
                    "user": {
                        "_id": "652066649004117947e46ed6",
                        "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
                        "isPro": false,
                        "fullname": "Jaehong Yoon",
                        "user": "jaehong31",
                        "type": "user"
                    },
                    "name": "Jaehong Yoon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T17:08:12.857Z",
                    "hidden": false
                },
                {
                    "_id": "68419d4b9892f271ddba2ff7",
                    "name": "Jaemin Cho",
                    "hidden": false
                },
                {
                    "_id": "68419d4b9892f271ddba2ff8",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T03:18:01.000Z",
            "submittedOnDailyAt": "2025-06-05T12:06:44.582Z",
            "title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "5ffe32d8942cf3533d364449",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
                "isPro": false,
                "fullname": "Jaemin Cho",
                "user": "j-min",
                "type": "user"
            },
            "summary": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains.",
            "upvotes": 4,
            "discussionId": "68419d4c9892f271ddba302e",
            "ai_summary": "Video-SKoT framework improves domain-adaptive video reasoning by constructing skill-aware Chain-of-Thought supervisions and specialized expert modules.",
            "ai_keywords": [
                "Chain-of-Thought",
                "CoT reasoning",
                "skill-aware CoT",
                "skill taxonomy",
                "multi-step CoT rationale",
                "skill-specific expert learning",
                "lightweight adapters",
                "domain-adaptive video reasoning"
            ]
        },
        "publishedAt": "2025-06-03T23:18:01.000Z",
        "title": "Video-Skill-CoT: Skill-based Chain-of-Thoughts for Domain-Adaptive Video\n  Reasoning",
        "summary": "Recent advances in Chain-of-Thought (CoT) reasoning have improved complex\nvideo understanding, but existing methods often struggle to adapt to\ndomain-specific skills (e.g., event detection, spatial relation understanding,\nemotion understanding) over various video content. To address this, we propose\nVideo-Skill-CoT (a.k.a. Video-SKoT), a framework that automatically constructs\nand leverages skill-aware CoT supervisions for domain-adaptive video reasoning.\nFirst, we construct skill-based CoT annotations: we extract domain-relevant\nreasoning skills from training questions, cluster them into a shared skill\ntaxonomy, and create detailed multi-step CoT rationale tailored to each\nvideo-question pair for training. Second, we introduce a skill-specific expert\nlearning framework. Each expert module specializes in a subset of reasoning\nskills and is trained with lightweight adapters using the collected CoT\nsupervision. We demonstrate the effectiveness of the proposed approach on three\nvideo understanding benchmarks, where Video-SKoT consistently outperforms\nstrong baselines. We also provide in-depth analyses on comparing different CoT\nannotation pipelines and learned skills over multiple video domains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03525.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5ffe32d8942cf3533d364449",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
            "fullname": "Jaemin Cho",
            "name": "j-min",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03448",
            "authors": [
                {
                    "_id": "684105479060432bf302b432",
                    "name": "Bimsara Pathiraja",
                    "hidden": false
                },
                {
                    "_id": "684105479060432bf302b433",
                    "user": {
                        "_id": "622d2ff38d04fd29a9ccf1a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
                        "isPro": false,
                        "fullname": "Maitreya Patel",
                        "user": "mpatel57",
                        "type": "user"
                    },
                    "name": "Maitreya Patel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:48.135Z",
                    "hidden": false
                },
                {
                    "_id": "684105479060432bf302b434",
                    "name": "Shivam Singh",
                    "hidden": false
                },
                {
                    "_id": "684105479060432bf302b435",
                    "name": "Yezhou Yang",
                    "hidden": false
                },
                {
                    "_id": "684105479060432bf302b436",
                    "name": "Chitta Baral",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T23:20:24.000Z",
            "submittedOnDailyAt": "2025-06-05T01:19:11.023Z",
            "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions",
            "submittedOnDailyBy": {
                "_id": "622d2ff38d04fd29a9ccf1a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
                "isPro": false,
                "fullname": "Maitreya Patel",
                "user": "mpatel57",
                "type": "user"
            },
            "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility.",
            "upvotes": 4,
            "discussionId": "6841054b9060432bf302b559",
            "projectPage": "https://refedit.vercel.app",
            "githubRepo": "https://github.com/bimsarapathiraja/refedit",
            "ai_summary": "RefEdit, an instruction-based editing model trained on synthetic data, outperforms baselines in complex scene editing and referring expression tasks.",
            "ai_keywords": [
                "RefEdit-Bench",
                "RefCOCO",
                "instruction-based editing model",
                "scalable synthetic data generation pipeline",
                "Flux/SD3",
                "state-of-the-art results"
            ]
        },
        "publishedAt": "2025-06-03T19:20:24.000Z",
        "title": "RefEdit: A Benchmark and Method for Improving Instruction-based Image\n  Editing Model on Referring Expressions",
        "summary": "Despite recent advances in inversion and instruction-based image editing,\nexisting approaches primarily excel at editing single, prominent objects but\nsignificantly struggle when applied to complex scenes containing multiple\nentities. To quantify this gap, we first introduce RefEdit-Bench, a rigorous\nreal-world benchmark rooted in RefCOCO, where even baselines trained on\nmillions of samples perform poorly. To overcome this limitation, we introduce\nRefEdit -- an instruction-based editing model trained on our scalable synthetic\ndata generation pipeline. Our RefEdit, trained on only 20,000 editing triplets,\noutperforms the Flux/SD3 model-based baselines trained on millions of data.\nExtensive evaluations across various benchmarks demonstrate that our model not\nonly excels in referring expression tasks but also enhances performance on\ntraditional benchmarks, achieving state-of-the-art results comparable to\nclosed-source methods. We release data \\& checkpoint for reproducibility.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03448.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "622d2ff38d04fd29a9ccf1a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/622d2ff38d04fd29a9ccf1a7/ORpGrlU8Lm_oSEVftcZEK.jpeg",
            "fullname": "Maitreya Patel",
            "name": "mpatel57",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02945",
            "authors": [
                {
                    "_id": "6841008f2f66f731bf010feb",
                    "name": "Aishwarya Sahoo",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010fec",
                    "name": "Jeevana Kruthi Karnuthala",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010fed",
                    "name": "Tushar Parmanand Budhwani",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010fee",
                    "name": "Pranchal Agarwal",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010fef",
                    "name": "Sankaran Vaidyanathan",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010ff0",
                    "name": "Alexa Siu",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010ff1",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:55.170Z",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010ff2",
                    "name": "Jennifer Healey",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010ff3",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010ff4",
                    "name": "Ryan Rossi",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010ff5",
                    "name": "Uttaran Bhattacharya",
                    "hidden": false
                },
                {
                    "_id": "6841008f2f66f731bf010ff6",
                    "name": "Branislav Kveton",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T14:44:23.000Z",
            "submittedOnDailyAt": "2025-06-05T00:57:33.123Z",
            "title": "Quantitative LLM Judges",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
            "upvotes": 4,
            "discussionId": "684100902f66f731bf01101e",
            "ai_summary": "A framework uses quantitative LLM judges to align existing LLM evaluation scores with human scores, improving predictive power and efficiency through regression models.",
            "ai_keywords": [
                "LLM-as-a-judge",
                "large language model",
                "quantitative LLM judges",
                "regression models",
                "score alignment",
                "predictive power",
                "post-hoc modeling"
            ]
        },
        "publishedAt": "2025-06-03T10:44:23.000Z",
        "title": "Quantitative LLM Judges",
        "summary": "LLM-as-a-judge is a framework in which a large language model (LLM)\nautomatically evaluates the output of another LLM. We propose quantitative LLM\njudges, which align evaluation scores of existing LLM judges to human scores in\na given domain using regression models. The models are trained to improve the\nscore of the original judge by using the judge's textual evaluation and score.\nWe present four quantitative judges for different types of absolute and\nrelative feedback, which showcases the generality and versatility of our\nframework. Our framework is more computationally efficient than supervised\nfine-tuning and can be more statistically efficient when human feedback is\nlimited, which is expected in most applications of our work. We validate these\nclaims empirically on four datasets using two base judges. Our experiments show\nthat quantitative judges can effectively improve the predictive power of\nexisting judges through post-hoc modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02945.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02294",
            "authors": [
                {
                    "_id": "6840cd169241913d43af9d28",
                    "user": {
                        "_id": "655646baf8a2d3c020546ec8",
                        "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
                        "isPro": false,
                        "fullname": "Niclas P",
                        "user": "NPBP26",
                        "type": "user"
                    },
                    "name": "Niclas Popp",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T13:00:50.249Z",
                    "hidden": false
                },
                {
                    "_id": "6840cd169241913d43af9d29",
                    "user": {
                        "_id": "662a1098fc86170808ac598d",
                        "avatarUrl": "/avatars/91b4910c96f21468c97c97cdfac2ab53.svg",
                        "isPro": false,
                        "fullname": "Kevin Alexander Laube",
                        "user": "laubekebosch",
                        "type": "user"
                    },
                    "name": "Kevin Alexander Laube",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T13:00:56.090Z",
                    "hidden": false
                },
                {
                    "_id": "6840cd169241913d43af9d2a",
                    "name": "Matthias Hein",
                    "hidden": false
                },
                {
                    "_id": "6840cd169241913d43af9d2b",
                    "name": "Lukas Schott",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T22:15:59.000Z",
            "submittedOnDailyAt": "2025-06-05T00:15:23.870Z",
            "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
            "submittedOnDailyBy": {
                "_id": "655646baf8a2d3c020546ec8",
                "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
                "isPro": false,
                "fullname": "Niclas P",
                "user": "NPBP26",
                "type": "user"
            },
            "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines",
            "upvotes": 4,
            "discussionId": "6840cd199241913d43af9dac",
            "ai_summary": "A diffusion-based data augmentation strategy improves robustness in knowledge distillation by generating challenging samples, enhancing accuracy and spurious feature resilience.",
            "ai_keywords": [
                "knowledge distillation",
                "diffusion-based data augmentation",
                "covariate shift",
                "teacher-student model",
                "CelebA",
                "SpuCo Birds",
                "spurious ImageNet",
                "mean group accuracy",
                "worst group accuracy",
                "spurious mAUC"
            ]
        },
        "publishedAt": "2025-06-02T18:15:59.000Z",
        "title": "Improving Knowledge Distillation Under Unknown Covariate Shift Through\n  Confidence-Guided Data Augmentation",
        "summary": "Large foundation models trained on extensive datasets demonstrate strong\nzero-shot capabilities in various domains. To replicate their success when data\nand model size are constrained, knowledge distillation has become an\nestablished tool for transferring knowledge from foundation models to small\nstudent networks. However, the effectiveness of distillation is critically\nlimited by the available training data. This work addresses the common\npractical issue of covariate shift in knowledge distillation, where spurious\nfeatures appear during training but not at test time. We ask the question: when\nthese spurious features are unknown, yet a robust teacher is available, is it\npossible for a student to also become robust to them? We address this problem\nby introducing a novel diffusion-based data augmentation strategy that\ngenerates images by maximizing the disagreement between the teacher and the\nstudent, effectively creating challenging samples that the student struggles\nwith. Experiments demonstrate that our approach significantly improves worst\ngroup and mean group accuracy on CelebA and SpuCo Birds as well as the spurious\nmAUC on spurious ImageNet under covariate shift, outperforming state-of-the-art\ndiffusion-based data augmentation baselines",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02294.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655646baf8a2d3c020546ec8",
            "avatarUrl": "/avatars/4ca8de82745bb5a4fda511569bb6bd94.svg",
            "fullname": "Niclas P",
            "name": "NPBP26",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01344",
            "authors": [
                {
                    "_id": "6841009bdf863485e04879c8",
                    "name": "Manan Suri",
                    "hidden": false
                },
                {
                    "_id": "6841009bdf863485e04879c9",
                    "user": {
                        "_id": "65c16444d4c3b8dff2f0d78d",
                        "avatarUrl": "/avatars/4ed764c1657bd260d2a12ba61c111062.svg",
                        "isPro": false,
                        "fullname": "Puneet Mathur",
                        "user": "puneetm",
                        "type": "user"
                    },
                    "name": "Puneet Mathur",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-05T02:27:41.018Z",
                    "hidden": false
                },
                {
                    "_id": "6841009bdf863485e04879ca",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "6841009bdf863485e04879cb",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:26:52.205Z",
                    "hidden": false
                },
                {
                    "_id": "6841009bdf863485e04879cc",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "6841009bdf863485e04879cd",
                    "name": "Vivek Gupta",
                    "hidden": false
                },
                {
                    "_id": "6841009bdf863485e04879ce",
                    "name": "Dinesh Manocha",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T06:02:41.000Z",
            "submittedOnDailyAt": "2025-06-05T00:57:44.204Z",
            "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.",
            "upvotes": 4,
            "discussionId": "6841009ddf863485e0487a38",
            "ai_summary": "FlowPathAgent, a neurosymbolic agent, enhances the reliability of LLM predictions for flowchart interpretation by tracing specific components and generating accurate attribution paths.",
            "ai_keywords": [
                "Flowcharts",
                "Fine-grained Flowchart Attribution",
                "FlowPathAgent",
                "graph-based reasoning",
                "symbolic graph",
                "neurosymbolic agent",
                "flowExplainBench",
                "flowchart QA"
            ]
        },
        "publishedAt": "2025-06-02T02:02:41.000Z",
        "title": "Follow the Flow: Fine-grained Flowchart Attribution with Neurosymbolic\n  Agents",
        "summary": "Flowcharts are a critical tool for visualizing decision-making processes.\nHowever, their non-linear structure and complex visual-textual relationships\nmake it challenging to interpret them using LLMs, as vision-language models\nfrequently hallucinate nonexistent connections and decision paths when\nanalyzing these diagrams. This leads to compromised reliability for automated\nflowchart processing in critical domains such as logistics, health, and\nengineering. We introduce the task of Fine-grained Flowchart Attribution, which\ntraces specific components grounding a flowchart referring LLM response.\nFlowchart Attribution ensures the verifiability of LLM predictions and improves\nexplainability by linking generated responses to the flowchart's structure. We\npropose FlowPathAgent, a neurosymbolic agent that performs fine-grained post\nhoc attribution through graph-based reasoning. It first segments the flowchart,\nthen converts it into a structured symbolic graph, and then employs an agentic\napproach to dynamically interact with the graph, to generate attribution paths.\nAdditionally, we present FlowExplainBench, a novel benchmark for evaluating\nflowchart attributions across diverse styles, domains, and question types.\nExperimental results show that FlowPathAgent mitigates visual hallucinations in\nLLM answers over flowchart QA, outperforming strong baselines by 10-14% on our\nproposed FlowExplainBench dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01344.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2505.23807",
            "authors": [
                {
                    "_id": "683fec0a9f37285365be6142",
                    "user": {
                        "_id": "656201912d309fa7e27ddf40",
                        "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
                        "isPro": false,
                        "fullname": "Yuli chen",
                        "user": "yulichen",
                        "type": "user"
                    },
                    "name": "Yuli Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T15:03:32.826Z",
                    "hidden": false
                },
                {
                    "_id": "683fec0a9f37285365be6143",
                    "name": "Bo Cheng",
                    "hidden": false
                },
                {
                    "_id": "683fec0a9f37285365be6144",
                    "name": "Jiale Han",
                    "hidden": false
                },
                {
                    "_id": "683fec0a9f37285365be6145",
                    "name": "Yingying Zhang",
                    "hidden": false
                },
                {
                    "_id": "683fec0a9f37285365be6146",
                    "name": "Yingting Li",
                    "hidden": false
                },
                {
                    "_id": "683fec0a9f37285365be6147",
                    "name": "Shuhao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T07:35:00.000Z",
            "submittedOnDailyAt": "2025-06-05T00:42:31.891Z",
            "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "656201912d309fa7e27ddf40",
                "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
                "isPro": false,
                "fullname": "Yuli chen",
                "user": "yulichen",
                "type": "user"
            },
            "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.",
            "upvotes": 4,
            "discussionId": "683fec0a9f37285365be617f",
            "ai_summary": "A dynamic layerwise pruning method adaptively determines layer importance by combining model weights and activation information to maintain performance in large language models at high sparsity.",
            "ai_keywords": [
                "pruning",
                "Large Language Models (LLMs)",
                "uniform layerwise pruning",
                "non-uniform layerwise pruning",
                "Dynamic Layerwise Pruning (DLP)",
                "perplexity",
                "Parameter-Efficient Fine-Tuning (PEFT)"
            ]
        },
        "publishedAt": "2025-05-27T03:35:00.000Z",
        "title": "DLP: Dynamic Layerwise Pruning in Large Language Models",
        "summary": "Pruning has recently been widely adopted to reduce the parameter scale and\nimprove the inference efficiency of Large Language Models (LLMs). Mainstream\npruning techniques often rely on uniform layerwise pruning strategies, which\ncan lead to severe performance degradation at high sparsity levels. Recognizing\nthe varying contributions of different layers in LLMs, recent studies have\nshifted their focus toward non-uniform layerwise pruning. However, these\napproaches often rely on pre-defined values, which can result in suboptimal\nperformance. To overcome these limitations, we propose a novel method called\nDynamic Layerwise Pruning (DLP). This approach adaptively determines the\nrelative importance of each layer by integrating model weights with input\nactivation information, assigning pruning rates accordingly. Experimental\nresults show that DLP effectively preserves model performance at high sparsity\nlevels across multiple LLMs. Specifically, at 70% sparsity, DLP reduces the\nperplexity of LLaMA2-7B by 7.79 and improves the average accuracy by 2.7%\ncompared to state-of-the-art methods. Moreover, DLP is compatible with various\nexisting LLM compression techniques and can be seamlessly integrated into\nParameter-Efficient Fine-Tuning (PEFT). We release the code at\nhttps://github.com/ironartisan/DLP to facilitate future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23807.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656201912d309fa7e27ddf40",
            "avatarUrl": "/avatars/d1bb9b263a758a0b0e7f803f4f888e95.svg",
            "fullname": "Yuli chen",
            "name": "yulichen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05332",
            "authors": [
                {
                    "_id": "684240b70e7e14aac456fa59",
                    "name": "Jingyang Lin",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa5a",
                    "name": "Jialian Wu",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa5b",
                    "name": "Ximeng Sun",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa5c",
                    "name": "Ze Wang",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa5d",
                    "name": "Jiang Liu",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa5e",
                    "name": "Yusheng Su",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa5f",
                    "name": "Xiaodong Yu",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa60",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa61",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa62",
                    "name": "Zicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "684240b70e7e14aac456fa63",
                    "name": "Emad Barsoum",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T17:59:04.000Z",
            "submittedOnDailyAt": "2025-06-05T23:46:17.907Z",
            "title": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "64c939307dba66c3a7e4d215",
                "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
                "isPro": false,
                "fullname": "BruceLyu",
                "user": "brucelyu",
                "type": "user"
            },
            "summary": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.",
            "upvotes": 3,
            "discussionId": "684240b90e7e14aac456fb2a",
            "projectPage": "https://videomarathon.github.io/",
            "githubRepo": "https://github.com/jylins/hourllava",
            "ai_summary": "Hour-LLaVA, a memory-augmented Video-LMM, achieves top performance on long video-language benchmarks using the VideoMarathon dataset, which extends video durations to one hour for comprehensive training and inference.",
            "ai_keywords": [
                "Video-LMMs",
                "VideoMarathon",
                "hour-long video instruction-following dataset",
                "QA pairs",
                "temporality",
                "spatiality",
                "object",
                "action",
                "scene",
                "event",
                "long-term video comprehension",
                "Hour-LLaVA",
                "memory augmentation module",
                "spatiotemporal-informative semantics",
                "cached full video context"
            ]
        },
        "publishedAt": "2025-06-05T13:59:04.000Z",
        "title": "Unleashing Hour-Scale Video Training for Long Video-Language\n  Understanding",
        "summary": "Recent long-form video-language understanding benchmarks have driven progress\nin video large multimodal models (Video-LMMs). However, the scarcity of\nwell-annotated long videos has left the training of hour-long Video-LLMs\nunderexplored. To close this gap, we present VideoMarathon, a large-scale\nhour-long video instruction-following dataset. This dataset includes around\n9,700 hours of long videos sourced from diverse domains, ranging from 3 to 60\nminutes per video. Specifically, it contains 3.3M high-quality QA pairs,\nspanning six fundamental topics: temporality, spatiality, object, action,\nscene, and event. Compared to existing video instruction datasets,\nVideoMarathon significantly extends training video durations up to 1 hour, and\nsupports 22 diverse tasks requiring both short- and long-term video\ncomprehension. Building on VideoMarathon, we propose Hour-LLaVA, a powerful and\nefficient Video-LMM for hour-scale video-language modeling. It enables\nhour-long video training and inference at 1-FPS sampling by leveraging a memory\naugmentation module, which adaptively integrates user question-relevant and\nspatiotemporal-informative semantics from a cached full video context. In our\nexperiments, Hour-LLaVA achieves the best performance on multiple long\nvideo-language benchmarks, demonstrating the high quality of the VideoMarathon\ndataset and the superiority of the Hour-LLaVA model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05332.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64c939307dba66c3a7e4d215",
            "avatarUrl": "/avatars/0b662cc1799525188476f3e6e1f97d29.svg",
            "fullname": "BruceLyu",
            "name": "brucelyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03837",
            "authors": [
                {
                    "_id": "68418b4fa88edf9e4b783062",
                    "user": {
                        "_id": "68416c7e2deb112bf3c0f0d7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6wcC033AOagYXs39FIWUy.png",
                        "isPro": false,
                        "fullname": "xiao-qi han",
                        "user": "xiao-qi",
                        "type": "user"
                    },
                    "name": "Xiao-Qi Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T12:41:46.123Z",
                    "hidden": false
                },
                {
                    "_id": "68418b4fa88edf9e4b783063",
                    "name": "Ze-Feng Gao",
                    "hidden": false
                },
                {
                    "_id": "68418b4fa88edf9e4b783064",
                    "name": "Xin-De Wang",
                    "hidden": false
                },
                {
                    "_id": "68418b4fa88edf9e4b783065",
                    "name": "Zhenfeng Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68418b4fa88edf9e4b783066",
                    "name": "Peng-Jie Guo",
                    "hidden": false
                },
                {
                    "_id": "68418b4fa88edf9e4b783067",
                    "name": "Zhong-Yi Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T11:14:00.000Z",
            "submittedOnDailyAt": "2025-06-05T11:32:19.196Z",
            "title": "HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature\n  Superconductors for AI-Driven Critical Temperature Prediction",
            "submittedOnDailyBy": {
                "_id": "68416c7e2deb112bf3c0f0d7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6wcC033AOagYXs39FIWUy.png",
                "isPro": false,
                "fullname": "xiao-qi han",
                "user": "xiao-qi",
                "type": "user"
            },
            "summary": "The discovery of high-temperature superconducting materials holds great\nsignificance for human industry and daily life. In recent years, research on\npredicting superconducting transition temperatures using artificial\nintelligence~(AI) has gained popularity, with most of these tools claiming to\nachieve remarkable accuracy. However, the lack of widely accepted benchmark\ndatasets in this field has severely hindered fair comparisons between different\nAI algorithms and impeded further advancement of these methods. In this work,\nwe present the HTSC-2025, an ambient-pressure high-temperature superconducting\nbenchmark dataset. This comprehensive compilation encompasses theoretically\npredicted superconducting materials discovered by theoretical physicists from\n2023 to 2025 based on BCS superconductivity theory, including the renowned\nX_2YH_6 system, perovskite MXH_3 system, M_3XH_8 system, cage-like\nBCN-doped metal atomic systems derived from LaH_{10} structural evolution,\nand two-dimensional honeycomb-structured systems evolving from MgB_2. The\nHTSC-2025 benchmark has been open-sourced at\nhttps://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This\nbenchmark holds significant importance for accelerating the discovery of\nsuperconducting materials using AI-based methods.",
            "upvotes": 3,
            "discussionId": "68418b50a88edf9e4b78309b",
            "ai_summary": "HTSC-2025, a benchmark dataset for high-temperature superconducting materials, is presented to facilitate AI-based discovery in this field.",
            "ai_keywords": [
                ""
            ]
        },
        "publishedAt": "2025-06-04T07:14:00.000Z",
        "title": "HTSC-2025: A Benchmark Dataset of Ambient-Pressure High-Temperature\n  Superconductors for AI-Driven Critical Temperature Prediction",
        "summary": "The discovery of high-temperature superconducting materials holds great\nsignificance for human industry and daily life. In recent years, research on\npredicting superconducting transition temperatures using artificial\nintelligence~(AI) has gained popularity, with most of these tools claiming to\nachieve remarkable accuracy. However, the lack of widely accepted benchmark\ndatasets in this field has severely hindered fair comparisons between different\nAI algorithms and impeded further advancement of these methods. In this work,\nwe present the HTSC-2025, an ambient-pressure high-temperature superconducting\nbenchmark dataset. This comprehensive compilation encompasses theoretically\npredicted superconducting materials discovered by theoretical physicists from\n2023 to 2025 based on BCS superconductivity theory, including the renowned\nX_2YH_6 system, perovskite MXH_3 system, M_3XH_8 system, cage-like\nBCN-doped metal atomic systems derived from LaH_{10} structural evolution,\nand two-dimensional honeycomb-structured systems evolving from MgB_2. The\nHTSC-2025 benchmark has been open-sourced at\nhttps://github.com/xqh19970407/HTSC-2025 and will be continuously updated. This\nbenchmark holds significant importance for accelerating the discovery of\nsuperconducting materials using AI-based methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03837.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68416c7e2deb112bf3c0f0d7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6wcC033AOagYXs39FIWUy.png",
            "fullname": "xiao-qi han",
            "name": "xiao-qi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04034",
            "authors": [
                {
                    "_id": "6840ff0b535bfb4942b31576",
                    "user": {
                        "_id": "647f46b6838ac3601fc89852",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
                        "isPro": true,
                        "fullname": "Qing Jiang",
                        "user": "Mountchicken",
                        "type": "user"
                    },
                    "name": "Qing Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T13:00:10.958Z",
                    "hidden": false
                },
                {
                    "_id": "6840ff0b535bfb4942b31577",
                    "user": {
                        "_id": "660cefee15392b34d995e542",
                        "avatarUrl": "/avatars/ab062a569757ad30ab7d347d737e3998.svg",
                        "isPro": false,
                        "fullname": "Xingyu Chen",
                        "user": "AsixaChen",
                        "type": "user"
                    },
                    "name": "Xingyu Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T13:00:18.113Z",
                    "hidden": false
                },
                {
                    "_id": "6840ff0b535bfb4942b31578",
                    "user": {
                        "_id": "669f0abdc13da2bbe268c5c3",
                        "avatarUrl": "/avatars/87961bfd01c53c420ae13133d643f22c.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Zeng",
                        "user": "zengzhaoyang",
                        "type": "user"
                    },
                    "name": "Zhaoyang Zeng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T13:00:25.218Z",
                    "hidden": false
                },
                {
                    "_id": "6840ff0b535bfb4942b31579",
                    "name": "Junzhi Yu",
                    "hidden": false
                },
                {
                    "_id": "6840ff0b535bfb4942b3157a",
                    "name": "Lei Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
            ],
            "publishedAt": "2025-06-04T14:56:57.000Z",
            "submittedOnDailyAt": "2025-06-05T00:56:30.698Z",
            "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
            "submittedOnDailyBy": {
                "_id": "647f46b6838ac3601fc89852",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
                "isPro": true,
                "fullname": "Qing Jiang",
                "user": "Mountchicken",
                "type": "user"
            },
            "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings.",
            "upvotes": 2,
            "discussionId": "6840ff0e535bfb4942b3165f",
            "projectPage": "https://rexthinker.github.io/",
            "githubRepo": "https://github.com/IDEA-Research/Rex-Thinker",
            "ai_summary": "Rex-Thinker is a CoT-based model that enhances object referring by performing step-by-step reasoning over candidate objects, leading to improved interpretability and rejection of mismatched queries.",
            "ai_keywords": [
                "CoT reasoning",
                "HumanRef-CoT",
                "GPT-4o",
                "structured reasoning",
                "cold-start supervised fine-tuning",
                "GRPO-based RL learning"
            ]
        },
        "publishedAt": "2025-06-04T10:56:57.000Z",
        "title": "Rex-Thinker: Grounded Object Referring via Chain-of-Thought Reasoning",
        "summary": "Object referring aims to detect all objects in an image that match a given\nnatural language description. We argue that a robust object referring model\nshould be grounded, meaning its predictions should be both explainable and\nfaithful to the visual content. Specifically, it should satisfy two key\nproperties: 1) Verifiable, by producing interpretable reasoning that justifies\nits predictions and clearly links them to visual evidence; and 2) Trustworthy,\nby learning to abstain when no object in the image satisfies the given\nexpression. However, most methods treat referring as a direct bounding box\nprediction task, offering limited interpretability and struggling to reject\nexpressions with no matching object. In this work, we propose Rex-Thinker, a\nmodel that formulates object referring as an explicit CoT reasoning task. Given\na referring expression, we first identify all candidate object instances\ncorresponding to the referred object category. Rex-Thinker then performs\nstep-by-step reasoning over each candidate to assess whether it matches the\ngiven expression, before making a final prediction. To support this paradigm,\nwe construct a large-scale CoT-style referring dataset named HumanRef-CoT by\nprompting GPT-4o on the HumanRef dataset. Each reasoning trace follows a\nstructured planning, action, and summarization format, enabling the model to\nlearn decomposed, interpretable reasoning over object candidates. We then train\nRex-Thinker in two stages: a cold-start supervised fine-tuning phase to teach\nthe model how to perform structured reasoning, followed by GRPO-based RL\nlearning to improve accuracy and generalization. Experiments show that our\napproach outperforms standard baselines in both precision and interpretability\non in-domain evaluation, while also demonstrating improved ability to reject\nhallucinated outputs and strong generalization in out-of-domain settings.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647f46b6838ac3601fc89852/0J-cvgz2dA6bVQJJNb_Yz.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04034.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647f46b6838ac3601fc89852",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647f46b6838ac3601fc89852/N5cr1MFEtgKLJ4sVAhS04.jpeg",
            "fullname": "Qing Jiang",
            "name": "Mountchicken",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03951",
            "authors": [
                {
                    "_id": "68415a1cce09e3eca94e02ef",
                    "user": {
                        "_id": "6759546743971eff5a12a087",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/esJm_83zW1R6NqWltof8P.png",
                        "isPro": false,
                        "fullname": "Aojun Lu",
                        "user": "Kurt1024",
                        "type": "user"
                    },
                    "name": "Aojun Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T09:59:50.242Z",
                    "hidden": false
                },
                {
                    "_id": "68415a1cce09e3eca94e02f0",
                    "user": {
                        "_id": "649d54b314afbb10ce2a9eeb",
                        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                        "isPro": false,
                        "fullname": "Hangjie Yuan",
                        "user": "JacobYuan",
                        "type": "user"
                    },
                    "name": "Hangjie Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T09:59:52.907Z",
                    "hidden": false
                },
                {
                    "_id": "68415a1cce09e3eca94e02f1",
                    "name": "Tao Feng",
                    "hidden": false
                },
                {
                    "_id": "68415a1cce09e3eca94e02f2",
                    "name": "Yanan Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T13:40:41.000Z",
            "submittedOnDailyAt": "2025-06-05T07:19:58.382Z",
            "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective",
            "submittedOnDailyBy": {
                "_id": "649d54b314afbb10ce2a9eeb",
                "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                "isPro": false,
                "fullname": "Hangjie Yuan",
                "user": "JacobYuan",
                "type": "user"
            },
            "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters.",
            "upvotes": 2,
            "discussionId": "68415a1dce09e3eca94e0314",
            "projectPage": "https://github.com/byyx666/Dual-Arch",
            "githubRepo": "https://github.com/byyx666/Dual-Arch",
            "ai_summary": "A novel framework, Dual-Arch, enhances Continual Learning by addressing the stability-plasticity dilemma at the architectural level using two specialized networks.",
            "ai_keywords": [
                "Continual Learning",
                "stability-plasticity dilemma",
                "deep networks",
                "wide networks",
                "Dual-Arch"
            ]
        },
        "publishedAt": "2025-06-04T09:40:41.000Z",
        "title": "Rethinking the Stability-Plasticity Trade-off in Continual Learning from\n  an Architectural Perspective",
        "summary": "The quest for Continual Learning (CL) seeks to empower neural networks with\nthe ability to learn and adapt incrementally. Central to this pursuit is\naddressing the stability-plasticity dilemma, which involves striking a balance\nbetween two conflicting objectives: preserving previously learned knowledge and\nacquiring new knowledge. While numerous CL methods aim to achieve this\ntrade-off, they often overlook the impact of network architecture on stability\nand plasticity, restricting the trade-off to the parameter level. In this\npaper, we delve into the conflict between stability and plasticity at the\narchitectural level. We reveal that under an equal parameter constraint, deeper\nnetworks exhibit better plasticity, while wider networks are characterized by\nsuperior stability. To address this architectural-level dilemma, we introduce a\nnovel framework denoted Dual-Arch, which serves as a plug-in component for CL.\nThis framework leverages the complementary strengths of two distinct and\nindependent networks: one dedicated to plasticity and the other to stability.\nEach network is designed with a specialized and lightweight architecture,\ntailored to its respective objective. Extensive experiments demonstrate that\nDual-Arch enhances the performance of existing CL methods while being up to 87%\nmore compact in terms of parameters.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03951.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "fullname": "Hangjie Yuan",
            "name": "JacobYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03822",
            "authors": [
                {
                    "_id": "6841be30ea0023757b685a3b",
                    "user": {
                        "_id": "636c1de8cfb49b468218e21e",
                        "avatarUrl": "/avatars/ab7d7b4b7c1f0a9b2a9625f292d0a6a5.svg",
                        "isPro": false,
                        "fullname": "Fabian Karl",
                        "user": "FabianKarl",
                        "type": "user"
                    },
                    "name": "Fabian Karl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T17:07:58.514Z",
                    "hidden": false
                },
                {
                    "_id": "6841be30ea0023757b685a3c",
                    "name": "Ansgar Scherp",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T10:52:55.000Z",
            "submittedOnDailyAt": "2025-06-05T14:40:45.357Z",
            "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents",
            "submittedOnDailyBy": {
                "_id": "636c1de8cfb49b468218e21e",
                "avatarUrl": "/avatars/ab7d7b4b7c1f0a9b2a9625f292d0a6a5.svg",
                "isPro": false,
                "fullname": "Fabian Karl",
                "user": "FabianKarl",
                "type": "user"
            },
            "summary": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.",
            "upvotes": 2,
            "discussionId": "6841be30ea0023757b685a5a",
            "githubRepo": "https://github.com/FKarl/CRAWLDoc",
            "ai_summary": "CRAWLDoc is a method for ranking linked web documents by embedding resources and metadata into a unified representation, enabling robust metadata extraction across diverse web layouts and formats.",
            "ai_keywords": [
                "contextual ranking",
                "linked web documents",
                "CRAWLDoc",
                "landing page",
                "PDFs",
                "ORCID profiles",
                "supplementary materials",
                "unified representation",
                "anchor texts",
                "URL",
                "metadata extraction",
                "layout-independent ranking"
            ]
        },
        "publishedAt": "2025-06-04T06:52:55.000Z",
        "title": "CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents",
        "summary": "Publication databases rely on accurate metadata extraction from diverse web\nsources, yet variations in web layouts and data formats present challenges for\nmetadata providers. This paper introduces CRAWLDoc, a new method for contextual\nranking of linked web documents. Starting with a publication's URL, such as a\ndigital object identifier, CRAWLDoc retrieves the landing page and all linked\nweb resources, including PDFs, ORCID profiles, and supplementary materials. It\nembeds these resources, along with anchor texts and the URLs, into a unified\nrepresentation. For evaluating CRAWLDoc, we have created a new, manually\nlabeled dataset of 600 publications from six top publishers in computer\nscience. Our method CRAWLDoc demonstrates a robust and layout-independent\nranking of relevant documents across publishers and data formats. It lays the\nfoundation for improved metadata extraction from web documents with various\nlayouts and formats. Our source code and dataset can be accessed at\nhttps://github.com/FKarl/CRAWLDoc.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03822.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636c1de8cfb49b468218e21e",
            "avatarUrl": "/avatars/ab7d7b4b7c1f0a9b2a9625f292d0a6a5.svg",
            "fullname": "Fabian Karl",
            "name": "FabianKarl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03614",
            "authors": [
                {
                    "_id": "684134ca20ff8abcccb11302",
                    "user": {
                        "_id": "642e5a7ba0b65dce1f87a7a2",
                        "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
                        "isPro": false,
                        "fullname": "Zhanhui Zhou",
                        "user": "ZHZisZZ",
                        "type": "user"
                    },
                    "name": "Zhanhui Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T13:01:32.367Z",
                    "hidden": false
                },
                {
                    "_id": "684134ca20ff8abcccb11303",
                    "name": "Lingjie Chen",
                    "hidden": false
                },
                {
                    "_id": "684134ca20ff8abcccb11304",
                    "name": "Chao Yang",
                    "hidden": false
                },
                {
                    "_id": "684134ca20ff8abcccb11305",
                    "name": "Chaochao Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T06:46:06.000Z",
            "submittedOnDailyAt": "2025-06-05T04:41:50.905Z",
            "title": "VLMs Can Aggregate Scattered Training Patches",
            "submittedOnDailyBy": {
                "_id": "642e5a7ba0b65dce1f87a7a2",
                "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
                "isPro": false,
                "fullname": "Zhanhui Zhou",
                "user": "ZHZisZZ",
                "type": "user"
            },
            "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as visual\nstitching -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each (image, ID) pair into {(patch,\nID)} pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.",
            "upvotes": 2,
            "discussionId": "684134cb20ff8abcccb11334",
            "githubRepo": "https://github.com/ZHZisZZ/visual-stitching",
            "ai_summary": "VLMs exhibit visual stitching, an ability to integrate fragmented visual information, which enables harmful content to evade data moderation and be reconstructed during inference.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "visual stitching",
                "data moderation",
                "adversarial data poisoning",
                "image patches",
                "textual descriptions",
                "inference"
            ]
        },
        "publishedAt": "2025-06-04T02:46:06.000Z",
        "title": "VLMs Can Aggregate Scattered Training Patches",
        "summary": "One way to mitigate risks in vision-language models (VLMs) is to remove\ndangerous samples in their training data. However, such data moderation can be\neasily bypassed when harmful images are split into small, benign-looking\npatches, scattered across many training samples. VLMs may then learn to piece\nthese fragments together during training and generate harmful responses at\ninference, either from full images or text references. For instance, if trained\non image patches from a bloody scene paired with the descriptions \"safe,\" VLMs\nmay later describe, the full image or a text reference to the scene, as \"safe.\"\nWe define the core ability of VLMs enabling this attack as visual\nstitching -- the ability to integrate visual information spread across\nmultiple training samples that share the same textual descriptions. In our\nwork, we first demonstrate visual stitching abilities in common open-source\nVLMs on three datasets where each image is labeled with a unique synthetic ID:\nwe split each (image, ID) pair into {(patch,\nID)} pairs at different granularity for finetuning, and we find that\ntuned models can verbalize the correct IDs from full images or text reference.\nBuilding on this, we simulate the adversarial data poisoning scenario mentioned\nabove by using patches from dangerous images and replacing IDs with text\ndescriptions like ``safe'' or ``unsafe'', demonstrating how harmful content can\nevade moderation in patches and later be reconstructed through visual\nstitching, posing serious VLM safety risks. Code is available at\nhttps://github.com/ZHZisZZ/visual-stitching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03614.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642e5a7ba0b65dce1f87a7a2",
            "avatarUrl": "/avatars/3ae01c9330a47e98fac9f1eb0ba94073.svg",
            "fullname": "Zhanhui Zhou",
            "name": "ZHZisZZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02515",
            "authors": [
                {
                    "_id": "6841704275b84c68e75dd774",
                    "user": {
                        "_id": "62d7f921720a579b3bd84a72",
                        "avatarUrl": "/avatars/eea38e90c3595eab1d7e0fdb7418b0d5.svg",
                        "isPro": false,
                        "fullname": "Zhuohan Xie",
                        "user": "Zhuohan",
                        "type": "user"
                    },
                    "name": "Zhuohan Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T12:41:57.397Z",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd775",
                    "name": "Dhruv Sahnan",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd776",
                    "name": "Debopriyo Banerjee",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd777",
                    "name": "Georgi Georgiev",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd778",
                    "name": "Rushil Thareja",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd779",
                    "name": "Hachem Madmoun",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd77a",
                    "name": "Jinyan Su",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd77b",
                    "name": "Aaryamonvikram Singh",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd77c",
                    "name": "Yuxia Wang",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd77d",
                    "name": "Rui Xing",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd77e",
                    "name": "Fajri Koto",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd77f",
                    "name": "Haonan Li",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd780",
                    "name": "Ivan Koychev",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd781",
                    "name": "Tanmoy Chakraborty",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd782",
                    "name": "Salem Lahlou",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd783",
                    "name": "Veselin Stoyanov",
                    "hidden": false
                },
                {
                    "_id": "6841704275b84c68e75dd784",
                    "name": "Preslav Nakov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-03T06:44:42.000Z",
            "submittedOnDailyAt": "2025-06-05T08:55:17.492Z",
            "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "62d7f921720a579b3bd84a72",
                "avatarUrl": "/avatars/eea38e90c3595eab1d7e0fdb7418b0d5.svg",
                "isPro": false,
                "fullname": "Zhuohan Xie",
                "user": "Zhuohan",
                "type": "user"
            },
            "summary": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain.",
            "upvotes": 2,
            "discussionId": "6841704375b84c68e75dd7a7",
            "ai_summary": "A new benchmark called FinChain evaluates multi-step symbolic reasoning in financial tasks with a focus on intermediate reasoning steps, introducing ChainEval as a metric for assessing both final answers and reasoning processes.",
            "ai_keywords": [
                "symbolic reasoning",
                "FinQA",
                "ConvFinQA",
                "Chain-of-Thought (CoT)",
                "FinChain",
                "multi-step financial reasoning",
                "parameterized templates",
                "reasoning complexity",
                "domain expertise",
                "executable Python trace",
                "ChainEval",
                "LLMs"
            ]
        },
        "publishedAt": "2025-06-03T02:44:42.000Z",
        "title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial\n  Reasoning",
        "summary": "Multi-step symbolic reasoning is critical for advancing downstream\nperformance on financial tasks. Yet, benchmarks for systematically evaluating\nthis capability are lacking. Existing datasets like FinQA and ConvFinQA\nsupervise only final numerical answers, without assessing intermediate\nreasoning steps. To address this, we introduce FinChain, the first symbolic\nbenchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning.\nSpanning 54 topics across 12 financial domains, Fin- Chain offers five\nparameterized templates per topic, each varying in reasoning complexity and\ndomain expertise required. Each dataset instance includes an executable Python\ntrace, enabling automatic generation of extensive training data and easy\nadaptation to other domains. We also introduce ChainEval, a new metric for\nautomatic evaluation of both final answers and intermediate reasoning.\nBenchmarking 30 LLMs on our dataset, we find that even state-of-the-art models\nhave considerable room for improvement in multi-step financial reasoning. All\ntemplates and evaluation metrics for FinChain are available at https:\n//github.com/mbzuai-nlp/finchain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02515.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d7f921720a579b3bd84a72",
            "avatarUrl": "/avatars/eea38e90c3595eab1d7e0fdb7418b0d5.svg",
            "fullname": "Zhuohan Xie",
            "name": "Zhuohan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02153",
            "authors": [
                {
                    "_id": "6841c780655161e7f1207108",
                    "name": "Peter Belcak",
                    "hidden": false
                },
                {
                    "_id": "6841c780655161e7f1207109",
                    "name": "Greg Heinrich",
                    "hidden": false
                },
                {
                    "_id": "6841c780655161e7f120710a",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "6841c780655161e7f120710b",
                    "name": "Yonggan Fu",
                    "hidden": false
                },
                {
                    "_id": "6841c780655161e7f120710c",
                    "name": "Xin Dong",
                    "hidden": false
                },
                {
                    "_id": "6841c780655161e7f120710d",
                    "name": "Saurav Muralidharan",
                    "hidden": false
                },
                {
                    "_id": "6841c780655161e7f120710e",
                    "name": "Yingyan Celine Lin",
                    "hidden": false
                },
                {
                    "_id": "6841c780655161e7f120710f",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T18:35:16.000Z",
            "submittedOnDailyAt": "2025-06-05T15:07:25.938Z",
            "title": "Small Language Models are the Future of Agentic AI",
            "submittedOnDailyBy": {
                "_id": "63e8cccddd2c4effdd6283cf",
                "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
                "isPro": false,
                "fullname": "Peter Belcak",
                "user": "pbelcak",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are often praised for exhibiting near-human\nperformance on a wide range of tasks and valued for their ability to hold a\ngeneral conversation. The rise of agentic AI systems is, however, ushering in a\nmass of applications in which language models perform a small number of\nspecialized tasks repetitively and with little variation.\n  Here we lay out the position that small language models (SLMs) are\nsufficiently powerful, inherently more suitable, and necessarily more\neconomical for many invocations in agentic systems, and are therefore the\nfuture of agentic AI. Our argumentation is grounded in the current level of\ncapabilities exhibited by SLMs, the common architectures of agentic systems,\nand the economy of LM deployment. We further argue that in situations where\ngeneral-purpose conversational abilities are essential, heterogeneous agentic\nsystems (i.e., agents invoking multiple different models) are the natural\nchoice. We discuss the potential barriers for the adoption of SLMs in agentic\nsystems and outline a general LLM-to-SLM agent conversion algorithm.\n  Our position, formulated as a value statement, highlights the significance of\nthe operational and economic impact even a partial shift from LLMs to SLMs is\nto have on the AI agent industry. We aim to stimulate the discussion on the\neffective use of AI resources and hope to advance the efforts to lower the\ncosts of AI of the present day. Calling for both contributions to and critique\nof our position, we commit to publishing all such correspondence at\nhttps://research.nvidia.com/labs/lpr/slm-agents.",
            "upvotes": 2,
            "discussionId": "6841c780655161e7f120712b",
            "ai_summary": "Small language models are argued to be more suitable, powerful, and economical for specialized, repetitive tasks in agentic systems, with heterogeneous systems using multiple models where general conversational abilities are needed.",
            "ai_keywords": [
                "large language models (LLMs)",
                "small language models (SLMs)",
                "agentic AI systems",
                "heterogeneous agentic systems",
                "LLM-to-SLM agent conversion"
            ]
        },
        "publishedAt": "2025-06-02T14:35:16.000Z",
        "title": "Small Language Models are the Future of Agentic AI",
        "summary": "Large language models (LLMs) are often praised for exhibiting near-human\nperformance on a wide range of tasks and valued for their ability to hold a\ngeneral conversation. The rise of agentic AI systems is, however, ushering in a\nmass of applications in which language models perform a small number of\nspecialized tasks repetitively and with little variation.\n  Here we lay out the position that small language models (SLMs) are\nsufficiently powerful, inherently more suitable, and necessarily more\neconomical for many invocations in agentic systems, and are therefore the\nfuture of agentic AI. Our argumentation is grounded in the current level of\ncapabilities exhibited by SLMs, the common architectures of agentic systems,\nand the economy of LM deployment. We further argue that in situations where\ngeneral-purpose conversational abilities are essential, heterogeneous agentic\nsystems (i.e., agents invoking multiple different models) are the natural\nchoice. We discuss the potential barriers for the adoption of SLMs in agentic\nsystems and outline a general LLM-to-SLM agent conversion algorithm.\n  Our position, formulated as a value statement, highlights the significance of\nthe operational and economic impact even a partial shift from LLMs to SLMs is\nto have on the AI agent industry. We aim to stimulate the discussion on the\neffective use of AI resources and hope to advance the efforts to lower the\ncosts of AI of the present day. Calling for both contributions to and critique\nof our position, we commit to publishing all such correspondence at\nhttps://research.nvidia.com/labs/lpr/slm-agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02153.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e8cccddd2c4effdd6283cf",
            "avatarUrl": "/avatars/b289d4dda0aafd60af3f14e19837b69c.svg",
            "fullname": "Peter Belcak",
            "name": "pbelcak",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.23564",
            "authors": [
                {
                    "_id": "6841771545662bb7d33ec3c7",
                    "user": {
                        "_id": "652be5ba703b3743c2675448",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AOZ0lE57OYGd7r1ujZTr0.jpeg",
                        "isPro": false,
                        "fullname": "Yiran Guo",
                        "user": "gyr66",
                        "type": "user"
                    },
                    "name": "Yiran Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T12:41:51.856Z",
                    "hidden": false
                },
                {
                    "_id": "6841771545662bb7d33ec3c8",
                    "name": "Lijie Xu",
                    "hidden": false
                },
                {
                    "_id": "6841771545662bb7d33ec3c9",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "6841771545662bb7d33ec3ca",
                    "name": "Dan Ye",
                    "hidden": false
                },
                {
                    "_id": "6841771545662bb7d33ec3cb",
                    "user": {
                        "_id": "6841784d883860ad472631c5",
                        "avatarUrl": "/avatars/290e1b9785f79cbbaccffdef24354282.svg",
                        "isPro": false,
                        "fullname": "Shuang Qiu",
                        "user": "shq-ml",
                        "type": "user"
                    },
                    "name": "Shuang Qiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T12:41:49.841Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-29T15:38:19.000Z",
            "submittedOnDailyAt": "2025-06-05T13:02:12.705Z",
            "title": "Segment Policy Optimization: Effective Segment-Level Credit Assignment\n  in RL for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "652be5ba703b3743c2675448",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AOZ0lE57OYGd7r1ujZTr0.jpeg",
                "isPro": false,
                "fullname": "Yiran Guo",
                "user": "gyr66",
                "type": "user"
            },
            "summary": "Enhancing the reasoning capabilities of large language models effectively\nusing reinforcement learning (RL) remains a crucial challenge. Existing\napproaches primarily adopt two contrasting advantage estimation granularities:\nToken-level methods (e.g., PPO) aim to provide the fine-grained advantage\nsignals but suffer from inaccurate estimation due to difficulties in training\nan accurate critic model. On the other extreme, trajectory-level methods (e.g.,\nGRPO) solely rely on a coarse-grained advantage signal from the final reward,\nleading to imprecise credit assignment. To address these limitations, we\npropose Segment Policy Optimization (SPO), a novel RL framework that leverages\nsegment-level advantage estimation at an intermediate granularity, achieving a\nbetter balance by offering more precise credit assignment than trajectory-level\nmethods and requiring fewer estimation points than token-level methods,\nenabling accurate advantage estimation based on Monte Carlo (MC) without a\ncritic model. SPO features three components with novel strategies: (1) flexible\nsegment partition; (2) accurate segment advantage estimation; and (3) policy\noptimization using segment advantages, including a novel probability-mask\nstrategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain\nfor short chain-of-thought (CoT), featuring novel cutpoint-based partition and\nchain-based advantage estimation, achieving 6-12 percentage point\nimprovements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,\nfeaturing novel tree-based advantage estimation, which significantly reduces\nthe cost of MC estimation, achieving 7-11 percentage point improvements\nover GRPO on MATH500 under 2K and 4K context evaluation. We make our code\npublicly available at https://github.com/AIFrameResearch/SPO.",
            "upvotes": 2,
            "discussionId": "6841771645662bb7d33ec400",
            "githubRepo": "https://github.com/AIFrameResearch/SPO",
            "ai_summary": "The Segment Policy Optimization (SPO) framework improves large language model reasoning via reinforcement learning by offering intermediate granularity advantage estimation, balancing precision and computational efficiency.",
            "ai_keywords": [
                "Segment Policy Optimization",
                "SPO",
                "token-level methods",
                "PPO",
                "trajectory-level methods",
                "GRPO",
                "segment-level advantage estimation",
                "Monte Carlo (MC) estimation",
                "flexible segment partition",
                "segment advantage estimation",
                "probability-mask strategy",
                "short chain-of-thought (CoT)",
                "cutpoint-based partition",
                "tree-based advantage estimation",
                "GSM8K",
                "MATH500"
            ]
        },
        "publishedAt": "2025-05-29T11:38:19.000Z",
        "title": "Segment Policy Optimization: Effective Segment-Level Credit Assignment\n  in RL for Large Language Models",
        "summary": "Enhancing the reasoning capabilities of large language models effectively\nusing reinforcement learning (RL) remains a crucial challenge. Existing\napproaches primarily adopt two contrasting advantage estimation granularities:\nToken-level methods (e.g., PPO) aim to provide the fine-grained advantage\nsignals but suffer from inaccurate estimation due to difficulties in training\nan accurate critic model. On the other extreme, trajectory-level methods (e.g.,\nGRPO) solely rely on a coarse-grained advantage signal from the final reward,\nleading to imprecise credit assignment. To address these limitations, we\npropose Segment Policy Optimization (SPO), a novel RL framework that leverages\nsegment-level advantage estimation at an intermediate granularity, achieving a\nbetter balance by offering more precise credit assignment than trajectory-level\nmethods and requiring fewer estimation points than token-level methods,\nenabling accurate advantage estimation based on Monte Carlo (MC) without a\ncritic model. SPO features three components with novel strategies: (1) flexible\nsegment partition; (2) accurate segment advantage estimation; and (3) policy\noptimization using segment advantages, including a novel probability-mask\nstrategy. We further instantiate SPO for two specific scenarios: (1) SPO-chain\nfor short chain-of-thought (CoT), featuring novel cutpoint-based partition and\nchain-based advantage estimation, achieving 6-12 percentage point\nimprovements in accuracy over PPO and GRPO on GSM8K. (2) SPO-tree for long CoT,\nfeaturing novel tree-based advantage estimation, which significantly reduces\nthe cost of MC estimation, achieving 7-11 percentage point improvements\nover GRPO on MATH500 under 2K and 4K context evaluation. We make our code\npublicly available at https://github.com/AIFrameResearch/SPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.23564.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652be5ba703b3743c2675448",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AOZ0lE57OYGd7r1ujZTr0.jpeg",
            "fullname": "Yiran Guo",
            "name": "gyr66",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04214",
            "authors": [
                {
                    "_id": "6841b0a2d198c64464545ae9",
                    "name": "Tingle Li",
                    "hidden": false
                },
                {
                    "_id": "6841b0a2d198c64464545aea",
                    "name": "Baihe Huang",
                    "hidden": false
                },
                {
                    "_id": "6841b0a2d198c64464545aeb",
                    "user": {
                        "_id": "63774ca43a63a2983ffc12f9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/jUJasGf5_2rwiEun_eYl7.png",
                        "isPro": false,
                        "fullname": "xiaobin zhuang",
                        "user": "xiaobinzhuang",
                        "type": "user"
                    },
                    "name": "Xiaobin Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T15:07:43.456Z",
                    "hidden": false
                },
                {
                    "_id": "6841b0a2d198c64464545aec",
                    "name": "Dongya Jia",
                    "hidden": false
                },
                {
                    "_id": "6841b0a2d198c64464545aed",
                    "name": "Jiawei Chen",
                    "hidden": false
                },
                {
                    "_id": "6841b0a2d198c64464545aee",
                    "name": "Yuping Wang",
                    "hidden": false
                },
                {
                    "_id": "6841b0a2d198c64464545aef",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "6841b0a2d198c64464545af0",
                    "name": "Gopala Anumanchipalli",
                    "hidden": false
                },
                {
                    "_id": "6841b0a2d198c64464545af1",
                    "name": "Yuxuan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T17:57:26.000Z",
            "submittedOnDailyAt": "2025-06-05T13:28:57.001Z",
            "title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation",
            "submittedOnDailyBy": {
                "_id": "63774ca43a63a2983ffc12f9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/jUJasGf5_2rwiEun_eYl7.png",
                "isPro": false,
                "fullname": "xiaobin zhuang",
                "user": "xiaobinzhuang",
                "type": "user"
            },
            "summary": "Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/",
            "upvotes": 1,
            "discussionId": "6841b0a4d198c64464545b45",
            "ai_summary": "An interactive object-aware audio generation model aligns sounds with user-selected visual objects in images using a conditional latent diffusion model with multi-modal attention.",
            "ai_keywords": [
                "interactive object-aware audio generation",
                "object-centric learning",
                "conditional latent diffusion model",
                "multi-modal attention",
                "image segmentation",
                "quantitative evaluation",
                "qualitative evaluation"
            ]
        },
        "publishedAt": "2025-06-04T13:57:26.000Z",
        "title": "Sounding that Object: Interactive Object-Aware Image to Audio Generation",
        "summary": "Generating accurate sounds for complex audio-visual scenes is challenging,\nespecially in the presence of multiple objects and sound sources. In this\npaper, we propose an {\\em interactive object-aware audio generation} model that\ngrounds sound generation in user-selected visual objects within images. Our\nmethod integrates object-centric learning into a conditional latent diffusion\nmodel, which learns to associate image regions with their corresponding sounds\nthrough multi-modal attention. At test time, our model employs image\nsegmentation to allow users to interactively generate sounds at the {\\em\nobject} level. We theoretically validate that our attention mechanism\nfunctionally approximates test-time segmentation masks, ensuring the generated\naudio aligns with selected objects. Quantitative and qualitative evaluations\nshow that our model outperforms baselines, achieving better alignment between\nobjects and their associated sounds. Project page:\nhttps://tinglok.netlify.app/files/avobject/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04214.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63774ca43a63a2983ffc12f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63774ca43a63a2983ffc12f9/jUJasGf5_2rwiEun_eYl7.png",
            "fullname": "xiaobin zhuang",
            "name": "xiaobinzhuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03817",
            "authors": [
                {
                    "_id": "68415ee454d7c6b3f9786deb",
                    "user": {
                        "_id": "637638fa1f0421002b42facb",
                        "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
                        "isPro": false,
                        "fullname": "Julius Gonsior",
                        "user": "jgonsior",
                        "type": "user"
                    },
                    "name": "Julius Gonsior",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:58:30.615Z",
                    "hidden": false
                },
                {
                    "_id": "68415ee454d7c6b3f9786dec",
                    "user": {
                        "_id": "6728a69e4d0fe229c96eea4c",
                        "avatarUrl": "/avatars/0c6ee4d8dd0752654ca5d86f27bebaea.svg",
                        "isPro": false,
                        "fullname": "Tim Rie",
                        "user": "Timischaf",
                        "type": "user"
                    },
                    "name": "Tim Rie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:58:21.628Z",
                    "hidden": false
                },
                {
                    "_id": "68415ee454d7c6b3f9786ded",
                    "name": "Anja Reusch",
                    "hidden": false
                },
                {
                    "_id": "68415ee454d7c6b3f9786dee",
                    "name": "Claudio Hartmann",
                    "hidden": false
                },
                {
                    "_id": "68415ee454d7c6b3f9786def",
                    "user": {
                        "_id": "65f5af9309cf7381af5e261b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f5af9309cf7381af5e261b/2ciaq2fzRwkpf7jk0k9JZ.png",
                        "isPro": false,
                        "fullname": "Maik Thiele",
                        "user": "DILHTWD",
                        "type": "user"
                    },
                    "name": "Maik Thiele",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:58:04.471Z",
                    "hidden": false
                },
                {
                    "_id": "68415ee454d7c6b3f9786df0",
                    "name": "Wolfgang Lehner",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T10:41:37.000Z",
            "submittedOnDailyAt": "2025-06-05T07:40:58.805Z",
            "title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid",
            "submittedOnDailyBy": {
                "_id": "637638fa1f0421002b42facb",
                "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
                "isPro": false,
                "fullname": "Julius Gonsior",
                "user": "jgonsior",
                "type": "user"
            },
            "summary": "Annotating data is a time-consuming and costly task, but it is inherently\nrequired for supervised machine learning. Active Learning (AL) is an\nestablished method that minimizes human labeling effort by iteratively\nselecting the most informative unlabeled samples for expert annotation, thereby\nimproving the overall classification performance. Even though AL has been known\nfor decades, AL is still rarely used in real-world applications. As indicated\nin the two community web surveys among the NLP community about AL, two main\nreasons continue to hold practitioners back from using AL: first, the\ncomplexity of setting AL up, and second, a lack of trust in its effectiveness.\nWe hypothesize that both reasons share the same culprit: the large\nhyperparameter space of AL. This mostly unexplored hyperparameter space often\nleads to misleading and irreproducible AL experiment results. In this study, we\nfirst compiled a large hyperparameter grid of over 4.6 million hyperparameter\ncombinations, second, recorded the performance of all combinations in the\nso-far biggest conducted AL study, and third, analyzed the impact of each\nhyperparameter in the experiment results. In the end, we give recommendations\nabout the influence of each hyperparameter, demonstrate the surprising\ninfluence of the concrete AL strategy implementation, and outline an\nexperimental study design for reproducible AL experiments with minimal\ncomputational effort, thus contributing to more reproducible and trustworthy AL\nresearch in the future.",
            "upvotes": 1,
            "discussionId": "68415ee554d7c6b3f9786e15",
            "githubRepo": "https://github.com/jgonsior/olympic-games-of-active-learning",
            "ai_summary": "The study investigates the impact of hyperparameters on Active Learning performance, providing insights to improve its practical application and reproducibility.",
            "ai_keywords": [
                "Active Learning",
                "hyperparameter space",
                "hyperparameter grid",
                "experimental study design",
                "reproducibility",
                "trustworthiness"
            ]
        },
        "publishedAt": "2025-06-04T06:41:37.000Z",
        "title": "Survey of Active Learning Hyperparameters: Insights from a Large-Scale\n  Experimental Grid",
        "summary": "Annotating data is a time-consuming and costly task, but it is inherently\nrequired for supervised machine learning. Active Learning (AL) is an\nestablished method that minimizes human labeling effort by iteratively\nselecting the most informative unlabeled samples for expert annotation, thereby\nimproving the overall classification performance. Even though AL has been known\nfor decades, AL is still rarely used in real-world applications. As indicated\nin the two community web surveys among the NLP community about AL, two main\nreasons continue to hold practitioners back from using AL: first, the\ncomplexity of setting AL up, and second, a lack of trust in its effectiveness.\nWe hypothesize that both reasons share the same culprit: the large\nhyperparameter space of AL. This mostly unexplored hyperparameter space often\nleads to misleading and irreproducible AL experiment results. In this study, we\nfirst compiled a large hyperparameter grid of over 4.6 million hyperparameter\ncombinations, second, recorded the performance of all combinations in the\nso-far biggest conducted AL study, and third, analyzed the impact of each\nhyperparameter in the experiment results. In the end, we give recommendations\nabout the influence of each hyperparameter, demonstrate the surprising\ninfluence of the concrete AL strategy implementation, and outline an\nexperimental study design for reproducible AL experiments with minimal\ncomputational effort, thus contributing to more reproducible and trustworthy AL\nresearch in the future.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03817.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637638fa1f0421002b42facb",
            "avatarUrl": "/avatars/ed1e3024cd2ed7284b437db4bbeb2668.svg",
            "fullname": "Julius Gonsior",
            "name": "jgonsior",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03538",
            "authors": [
                {
                    "_id": "6841585dd777f13c59460b47",
                    "user": {
                        "_id": "66390dcf3ec6a9595ab36b73",
                        "avatarUrl": "/avatars/a93e2471082b52abcb018db5ff1e8032.svg",
                        "isPro": false,
                        "fullname": "Chengqi Li",
                        "user": "lichengqi617",
                        "type": "user"
                    },
                    "name": "Chengqi Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:59:09.705Z",
                    "hidden": false
                },
                {
                    "_id": "6841585dd777f13c59460b48",
                    "name": "Zhihao Shi",
                    "hidden": false
                },
                {
                    "_id": "6841585dd777f13c59460b49",
                    "user": {
                        "_id": "6444ca1a1cfc9ae6bb3a0bd0",
                        "avatarUrl": "/avatars/599a40c5bd1a93d87740c594f5069fe7.svg",
                        "isPro": false,
                        "fullname": "yang",
                        "user": "yangdilu",
                        "type": "user"
                    },
                    "name": "Yangdi Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:58:57.071Z",
                    "hidden": false
                },
                {
                    "_id": "6841585dd777f13c59460b4a",
                    "user": {
                        "_id": "668a09f43e1066772fdb5364",
                        "avatarUrl": "/avatars/c23accd322ab276f750dd53117fee352.svg",
                        "isPro": false,
                        "fullname": "Wenbo He",
                        "user": "hewb14",
                        "type": "user"
                    },
                    "name": "Wenbo He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:58:50.568Z",
                    "hidden": false
                },
                {
                    "_id": "6841585dd777f13c59460b4b",
                    "user": {
                        "_id": "634e60454677a5891c0902f4",
                        "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Xu",
                        "user": "xjcvcvxj",
                        "type": "user"
                    },
                    "name": "Xiangyu Xu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-05T08:42:10.367Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T03:40:33.000Z",
            "submittedOnDailyAt": "2025-06-05T07:13:24.079Z",
            "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting",
            "submittedOnDailyBy": {
                "_id": "634e60454677a5891c0902f4",
                "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
                "isPro": false,
                "fullname": "Xiangyu Xu",
                "user": "xjcvcvxj",
                "type": "user"
            },
            "summary": "3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released.",
            "upvotes": 1,
            "discussionId": "68415862d777f13c59460c85",
            "ai_summary": "A novel Asymmetric Dual 3DGS framework improves 3D reconstruction by training dual models with consistency constraints and divergent masking, outperforming existing methods with high efficiency.",
            "ai_keywords": [
                "3D reconstruction",
                "3D Gaussian Splatting (3DGS)",
                "stochastic artifacts",
                "consistency constraint",
                "confirmation bias",
                "divergent masking",
                "multi-cue adaptive mask",
                "self-supervised soft mask",
                "Dynamic EMA Proxy",
                "lightweight variant",
                "Exponential Moving Average (EMA)",
                "alternating masking strategy"
            ]
        },
        "publishedAt": "2025-06-03T23:40:33.000Z",
        "title": "Robust Neural Rendering in the Wild with Asymmetric Dual 3D Gaussian\n  Splatting",
        "summary": "3D reconstruction from in-the-wild images remains a challenging task due to\ninconsistent lighting conditions and transient distractors. Existing methods\ntypically rely on heuristic strategies to handle the low-quality training data,\nwhich often struggle to produce stable and consistent reconstructions,\nfrequently resulting in visual artifacts. In this work, we propose Asymmetric\nDual 3DGS, a novel framework that leverages the stochastic nature of these\nartifacts: they tend to vary across different training runs due to minor\nrandomness. Specifically, our method trains two 3D Gaussian Splatting (3DGS)\nmodels in parallel, enforcing a consistency constraint that encourages\nconvergence on reliable scene geometry while suppressing inconsistent\nartifacts. To prevent the two models from collapsing into similar failure modes\ndue to confirmation bias, we introduce a divergent masking strategy that\napplies two complementary masks: a multi-cue adaptive mask and a\nself-supervised soft mask, which leads to an asymmetric training process of the\ntwo models, reducing shared error modes. In addition, to improve the efficiency\nof model training, we introduce a lightweight variant called Dynamic EMA Proxy,\nwhich replaces one of the two models with a dynamically updated Exponential\nMoving Average (EMA) proxy, and employs an alternating masking strategy to\npreserve divergence. Extensive experiments on challenging real-world datasets\ndemonstrate that our method consistently outperforms existing approaches while\nachieving high efficiency. Codes and trained models will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03538.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634e60454677a5891c0902f4",
            "avatarUrl": "/avatars/4dc143719afe7686e05b7f2c2c5c1871.svg",
            "fullname": "Xiangyu Xu",
            "name": "xjcvcvxj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.02680",
            "authors": [
                {
                    "_id": "6840496b4f36daae2e47efcb",
                    "user": {
                        "_id": "651ffc29261e2d84a261bb90",
                        "avatarUrl": "/avatars/e58fc23a4499d1a855f77ceee7a9512c.svg",
                        "isPro": false,
                        "fullname": "Julius Erbach",
                        "user": "juliuse",
                        "type": "user"
                    },
                    "name": "Julius Erbach",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T07:27:46.678Z",
                    "hidden": false
                },
                {
                    "_id": "6840496b4f36daae2e47efcc",
                    "user": {
                        "_id": "674995c2d523f2743e6c3859",
                        "avatarUrl": "/avatars/41b60f776a41d85da9ea0b1ed7d7615b.svg",
                        "isPro": false,
                        "fullname": "Dominik Narnhofer",
                        "user": "dnarnhofer",
                        "type": "user"
                    },
                    "name": "Dominik Narnhofer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:59:23.740Z",
                    "hidden": false
                },
                {
                    "_id": "6840496b4f36daae2e47efcd",
                    "user": {
                        "_id": "66f1afd751dca07c002dbd78",
                        "avatarUrl": "/avatars/de1262add2c455611f361a2bc8627e18.svg",
                        "isPro": false,
                        "fullname": "Andreas Dombos",
                        "user": "andreeee27",
                        "type": "user"
                    },
                    "name": "Andreas Dombos",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:59:29.844Z",
                    "hidden": false
                },
                {
                    "_id": "6840496b4f36daae2e47efce",
                    "name": "Bernt Schiele",
                    "hidden": false
                },
                {
                    "_id": "6840496b4f36daae2e47efcf",
                    "name": "Jan Eric Lenssen",
                    "hidden": false
                },
                {
                    "_id": "6840496b4f36daae2e47efd0",
                    "user": {
                        "_id": "6750649b7edd6a98a1bbcd06",
                        "avatarUrl": "/avatars/ba27f12d0333cf2d400d4405af7efe97.svg",
                        "isPro": false,
                        "fullname": "Konrad Schindler",
                        "user": "konradschindler",
                        "type": "user"
                    },
                    "name": "Konrad Schindler",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-05T12:59:43.140Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/651ffc29261e2d84a261bb90/ofR37I03AujZX_70aiJbl.qt"
            ],
            "publishedAt": "2025-06-03T09:29:47.000Z",
            "submittedOnDailyAt": "2025-06-05T10:11:20.544Z",
            "title": "Solving Inverse Problems with FLAIR",
            "submittedOnDailyBy": {
                "_id": "651ffc29261e2d84a261bb90",
                "avatarUrl": "/avatars/e58fc23a4499d1a855f77ceee7a9512c.svg",
                "isPro": false,
                "fullname": "Julius Erbach",
                "user": "juliuse",
                "type": "user"
            },
            "summary": "Flow-based latent generative models such as Stable Diffusion 3 are able to\ngenerate images with remarkable quality, even enabling photorealistic\ntext-to-image generation. Their impressive performance suggests that these\nmodels should also constitute powerful priors for inverse imaging problems, but\nthat approach has not yet led to comparable fidelity. There are several key\nobstacles: (i) the encoding into a lower-dimensional latent space makes the\nunderlying (forward) mapping non-linear; (ii) the data likelihood term is\nusually intractable; and (iii) learned generative models struggle to recover\nrare, atypical data modes during inference. We present FLAIR, a novel training\nfree variational framework that leverages flow-based generative models as a\nprior for inverse problems. To that end, we introduce a variational objective\nfor flow matching that is agnostic to the type of degradation, and combine it\nwith deterministic trajectory adjustments to recover atypical modes. To enforce\nexact consistency with the observed data, we decouple the optimization of the\ndata fidelity and regularization terms. Moreover, we introduce a time-dependent\ncalibration scheme in which the strength of the regularization is modulated\naccording to off-line accuracy estimates. Results on standard imaging\nbenchmarks demonstrate that FLAIR consistently outperforms existing diffusion-\nand flow-based methods in terms of reconstruction quality and sample diversity.",
            "upvotes": 1,
            "discussionId": "684049734f36daae2e47f1d0",
            "ai_summary": "FLAIR, a novel training-free variational framework, leverages flow-based generative models to enhance inverse problem solutions, achieving superior reconstruction quality and sample diversity.",
            "ai_keywords": [
                "flow-based latent generative models",
                "Stable Diffusion 3",
                "inverse imaging problems",
                "variational framework",
                "flow matching",
                "deterministic trajectory adjustments",
                "data fidelity",
                "regularization terms",
                "time-dependent calibration scheme"
            ]
        },
        "publishedAt": "2025-06-03T05:29:47.000Z",
        "title": "Solving Inverse Problems with FLAIR",
        "summary": "Flow-based latent generative models such as Stable Diffusion 3 are able to\ngenerate images with remarkable quality, even enabling photorealistic\ntext-to-image generation. Their impressive performance suggests that these\nmodels should also constitute powerful priors for inverse imaging problems, but\nthat approach has not yet led to comparable fidelity. There are several key\nobstacles: (i) the encoding into a lower-dimensional latent space makes the\nunderlying (forward) mapping non-linear; (ii) the data likelihood term is\nusually intractable; and (iii) learned generative models struggle to recover\nrare, atypical data modes during inference. We present FLAIR, a novel training\nfree variational framework that leverages flow-based generative models as a\nprior for inverse problems. To that end, we introduce a variational objective\nfor flow matching that is agnostic to the type of degradation, and combine it\nwith deterministic trajectory adjustments to recover atypical modes. To enforce\nexact consistency with the observed data, we decouple the optimization of the\ndata fidelity and regularization terms. Moreover, we introduce a time-dependent\ncalibration scheme in which the strength of the regularization is modulated\naccording to off-line accuracy estimates. Results on standard imaging\nbenchmarks demonstrate that FLAIR consistently outperforms existing diffusion-\nand flow-based methods in terms of reconstruction quality and sample diversity.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/651ffc29261e2d84a261bb90/ofR37I03AujZX_70aiJbl.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02680.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651ffc29261e2d84a261bb90",
            "avatarUrl": "/avatars/e58fc23a4499d1a855f77ceee7a9512c.svg",
            "fullname": "Julius Erbach",
            "name": "juliuse",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.00618",
            "authors": [
                {
                    "_id": "6840035d7ed0da422d1fda21",
                    "user": {
                        "_id": "64f73a44102fbfb26410962e",
                        "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
                        "isPro": false,
                        "fullname": "jingyi Yang",
                        "user": "JY-Young",
                        "type": "user"
                    },
                    "name": "Jingyi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-04T08:53:32.008Z",
                    "hidden": false
                },
                {
                    "_id": "6840035d7ed0da422d1fda22",
                    "name": "Shuai Shao",
                    "hidden": false
                },
                {
                    "_id": "6840035d7ed0da422d1fda23",
                    "name": "Dongrui Liu",
                    "hidden": false
                },
                {
                    "_id": "6840035d7ed0da422d1fda24",
                    "name": "Jing Shao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-31T16:04:59.000Z",
            "submittedOnDailyAt": "2025-06-05T14:50:46.482Z",
            "title": "RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents",
            "submittedOnDailyBy": {
                "_id": "64f73a44102fbfb26410962e",
                "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
                "isPro": false,
                "fullname": "jingyi Yang",
                "user": "JY-Young",
                "type": "user"
            },
            "summary": "With the rapid development of multimodal large language models (MLLMs), they\nare increasingly deployed as autonomous computer-use agents capable of\naccomplishing complex computer tasks. However, a pressing issue arises: Can the\nsafety risk principles designed and aligned for general MLLMs in dialogue\nscenarios be effectively transferred to real-world computer-use scenarios?\nExisting research on evaluating the safety risks of MLLM-based computer-use\nagents suffers from several limitations: it either lacks realistic interactive\nenvironments, or narrowly focuses on one or a few specific risk types. These\nlimitations ignore the complexity, variability, and diversity of real-world\nenvironments, thereby restricting comprehensive risk evaluation for\ncomputer-use agents. To this end, we introduce RiOSWorld, a benchmark\ndesigned to evaluate the potential risks of MLLM-based agents during real-world\ncomputer manipulations. Our benchmark includes 492 risky tasks spanning various\ncomputer applications, involving web, social media, multimedia, os, email, and\noffice software. We categorize these risks into two major classes based on\ntheir risk source: (i) User-originated risks and (ii) Environmental risks. For\nthe evaluation, we evaluate safety risks from two perspectives: (i) Risk goal\nintention and (ii) Risk goal completion. Extensive experiments with multimodal\nagents on RiOSWorld demonstrate that current computer-use agents\nconfront significant safety risks in real-world scenarios. Our findings\nhighlight the necessity and urgency of safety alignment for computer-use agents\nin real-world computer manipulation, providing valuable insights for developing\ntrustworthy computer-use agents. Our benchmark is publicly available at\nhttps://yjyddq.github.io/RiOSWorld.github.io/.",
            "upvotes": 1,
            "discussionId": "684003617ed0da422d1fdb48",
            "projectPage": "https://yjyddq.github.io/RiOSWorld.github.io/",
            "githubRepo": "https://github.com/yjyddq/RiOSWorld",
            "ai_summary": "RIOSWorld is a benchmark for evaluating safety risks of multimodal large language models in real-world computer tasks, revealing significant risks that necessitate safety alignment.",
            "ai_keywords": [
                "multimodal large language models",
                "autonomous computer-use agents",
                "safety risk principles",
                "RiOSWorld",
                "risky tasks",
                "computer applications",
                "user-originated risks",
                "environmental risks",
                "risk goal intention",
                "risk goal completion"
            ]
        },
        "publishedAt": "2025-05-31T12:04:59.000Z",
        "title": "RiOSWorld: Benchmarking the Risk of Multimodal Compter-Use Agents",
        "summary": "With the rapid development of multimodal large language models (MLLMs), they\nare increasingly deployed as autonomous computer-use agents capable of\naccomplishing complex computer tasks. However, a pressing issue arises: Can the\nsafety risk principles designed and aligned for general MLLMs in dialogue\nscenarios be effectively transferred to real-world computer-use scenarios?\nExisting research on evaluating the safety risks of MLLM-based computer-use\nagents suffers from several limitations: it either lacks realistic interactive\nenvironments, or narrowly focuses on one or a few specific risk types. These\nlimitations ignore the complexity, variability, and diversity of real-world\nenvironments, thereby restricting comprehensive risk evaluation for\ncomputer-use agents. To this end, we introduce RiOSWorld, a benchmark\ndesigned to evaluate the potential risks of MLLM-based agents during real-world\ncomputer manipulations. Our benchmark includes 492 risky tasks spanning various\ncomputer applications, involving web, social media, multimedia, os, email, and\noffice software. We categorize these risks into two major classes based on\ntheir risk source: (i) User-originated risks and (ii) Environmental risks. For\nthe evaluation, we evaluate safety risks from two perspectives: (i) Risk goal\nintention and (ii) Risk goal completion. Extensive experiments with multimodal\nagents on RiOSWorld demonstrate that current computer-use agents\nconfront significant safety risks in real-world scenarios. Our findings\nhighlight the necessity and urgency of safety alignment for computer-use agents\nin real-world computer manipulation, providing valuable insights for developing\ntrustworthy computer-use agents. Our benchmark is publicly available at\nhttps://yjyddq.github.io/RiOSWorld.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00618.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f73a44102fbfb26410962e",
            "avatarUrl": "/avatars/328302a495de6a4418be835456d1d3c6.svg",
            "fullname": "jingyi Yang",
            "name": "JY-Young",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]