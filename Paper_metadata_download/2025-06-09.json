[
    {
        "paper": {
            "id": "2505.21115",
            "authors": [
                {
                    "_id": "68372d97e4af3c39dcec8e65",
                    "user": {
                        "_id": "5dfa8e07da6d0311fd3d5430",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
                        "isPro": false,
                        "fullname": "Sergey Pletenev",
                        "user": "memyprokotow",
                        "type": "user"
                    },
                    "name": "Sergey Pletenev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:12:55.604Z",
                    "hidden": false
                },
                {
                    "_id": "68372d97e4af3c39dcec8e66",
                    "user": {
                        "_id": "660ee18e2dcd816ad14b3739",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
                        "isPro": false,
                        "fullname": "Maria Marina",
                        "user": "zlatamaria",
                        "type": "user"
                    },
                    "name": "Maria Marina",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:12:59.278Z",
                    "hidden": false
                },
                {
                    "_id": "68372d97e4af3c39dcec8e67",
                    "user": {
                        "_id": "6682607ece294ddc5e72f4fb",
                        "avatarUrl": "/avatars/2a304bc3eb56ec7d13297d28fbb062ae.svg",
                        "isPro": false,
                        "fullname": "Ivanov",
                        "user": "VirVen",
                        "type": "user"
                    },
                    "name": "Nikolay Ivanov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-02T07:48:42.811Z",
                    "hidden": false
                },
                {
                    "_id": "68372d97e4af3c39dcec8e68",
                    "name": "Daria Galimzianova",
                    "hidden": false
                },
                {
                    "_id": "68372d97e4af3c39dcec8e69",
                    "user": {
                        "_id": "643010b2ff56d6c2004699a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/OYsf1Hp-KAievw_M8XBG9.jpeg",
                        "isPro": false,
                        "fullname": "Krayko Nikita",
                        "user": "nakrayko",
                        "type": "user"
                    },
                    "name": "Nikita Krayko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:12:53.523Z",
                    "hidden": false
                },
                {
                    "_id": "68372d97e4af3c39dcec8e6a",
                    "name": "Mikhail Salnikov",
                    "hidden": false
                },
                {
                    "_id": "68372d97e4af3c39dcec8e6b",
                    "name": "Vasily Konovalov",
                    "hidden": false
                },
                {
                    "_id": "68372d97e4af3c39dcec8e6c",
                    "name": "Alexander Panchenko",
                    "hidden": false
                },
                {
                    "_id": "68372d97e4af3c39dcec8e6d",
                    "user": {
                        "_id": "63bbfd74141c7d395c471768",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673264437106-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Viktor Moskvoretskii",
                        "user": "VityaVitalich",
                        "type": "user"
                    },
                    "name": "Viktor Moskvoretskii",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:12:57.325Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T12:35:13.000Z",
            "submittedOnDailyAt": "2025-06-09T07:27:12.232Z",
            "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA",
            "submittedOnDailyBy": {
                "_id": "660ee18e2dcd816ad14b3739",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
                "isPro": false,
                "fullname": "Maria Marina",
                "user": "zlatamaria",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.",
            "upvotes": 83,
            "discussionId": "68372d98e4af3c39dcec8e88",
            "githubRepo": "https://github.com/s-nlp/Evergreen-classification",
            "ai_summary": "EverGreenQA, a multilingual QA dataset with evergreen labels, is introduced to benchmark LLMs on temporality encoding and assess their performance through verbalized judgments and uncertainty signals.",
            "ai_keywords": [
                "Large Language Models",
                "QA",
                "evergreen",
                "mutable",
                "temporality",
                "Multilingual QA dataset",
                "EG-E5",
                "lightweight multilingual classifier",
                "SoTA performance",
                "self-knowledge estimation",
                "filtering QA datasets",
                "GPT-4o retrieval behavior"
            ]
        },
        "publishedAt": "2025-05-27T08:35:13.000Z",
        "title": "Will It Still Be True Tomorrow? Multilingual Evergreen Question\n  Classification to Improve Trustworthy QA",
        "summary": "Large Language Models (LLMs) often hallucinate in question answering (QA)\ntasks. A key yet underexplored factor contributing to this is the temporality\nof questions -- whether they are evergreen (answers remain stable over time) or\nmutable (answers change). In this work, we introduce EverGreenQA, the first\nmultilingual QA dataset with evergreen labels, supporting both evaluation and\ntraining. Using EverGreenQA, we benchmark 12 modern LLMs to assess whether they\nencode question temporality explicitly (via verbalized judgments) or implicitly\n(via uncertainty signals). We also train EG-E5, a lightweight multilingual\nclassifier that achieves SoTA performance on this task. Finally, we demonstrate\nthe practical utility of evergreen classification across three applications:\nimproving self-knowledge estimation, filtering QA datasets, and explaining\nGPT-4o retrieval behavior.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21115.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "660ee18e2dcd816ad14b3739",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/660ee18e2dcd816ad14b3739/2pPMurtSOHMA96eVk0k7w.jpeg",
            "fullname": "Maria Marina",
            "name": "zlatamaria",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01111",
            "authors": [
                {
                    "_id": "6845b6a33ec10bdd8ab4da1b",
                    "name": "Shunian Chen",
                    "hidden": false
                },
                {
                    "_id": "6845b6a33ec10bdd8ab4da1c",
                    "user": {
                        "_id": "66440e86bfe15e84d369cb03",
                        "avatarUrl": "/avatars/d15b3b3831bc74138206071612169f64.svg",
                        "isPro": false,
                        "fullname": "Xinyuan Xie",
                        "user": "SatsukiVie",
                        "type": "user"
                    },
                    "name": "Xinyuan Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:11:42.759Z",
                    "hidden": false
                },
                {
                    "_id": "6845b6a33ec10bdd8ab4da1d",
                    "name": "Zheshu Chen",
                    "hidden": false
                },
                {
                    "_id": "6845b6a33ec10bdd8ab4da1e",
                    "name": "Liyan Zhao",
                    "hidden": false
                },
                {
                    "_id": "6845b6a33ec10bdd8ab4da1f",
                    "name": "Owen Lee",
                    "hidden": false
                },
                {
                    "_id": "6845b6a33ec10bdd8ab4da20",
                    "name": "Zhan Su",
                    "hidden": false
                },
                {
                    "_id": "6845b6a33ec10bdd8ab4da21",
                    "name": "Qilin Sun",
                    "hidden": false
                },
                {
                    "_id": "6845b6a33ec10bdd8ab4da22",
                    "name": "Benyou Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-01T18:29:17.000Z",
            "submittedOnDailyAt": "2025-06-09T01:59:54.914Z",
            "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
            "submittedOnDailyBy": {
                "_id": "623be9e1d1eb227788764959",
                "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
                "isPro": false,
                "fullname": "Shunian Chen",
                "user": "Shunian",
                "type": "user"
            },
            "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
            "upvotes": 27,
            "discussionId": "6845b6a43ec10bdd8ab4da23",
            "githubRepo": "https://github.com/FreedomIntelligence/FusionAudio",
            "ai_summary": "A novel two-stage pipeline using specialized pretrained models and a large language model enhances audio caption quality by integrating diverse multimodal cues and contextual information.",
            "ai_keywords": [
                "audio captioning",
                "auditory perception",
                "auditory scene analysis",
                "pretrained models",
                "large language model",
                "FusionAudio",
                "CLAP-based audio encoder",
                "audio-text alignment",
                "instruction following"
            ]
        },
        "publishedAt": "2025-06-01T14:29:17.000Z",
        "title": "FusionAudio-1.2M: Towards Fine-grained Audio Captioning with Multimodal\n  Contextual Fusion",
        "summary": "High-quality, large-scale audio captioning is crucial for advancing audio\nunderstanding, yet current automated methods often generate captions that lack\nfine-grained detail and contextual accuracy, primarily due to their reliance on\nlimited unimodal or superficial multimodal information. Drawing inspiration\nfrom human auditory perception, which adeptly integrates cross-modal cues and\nperforms sophisticated auditory scene analysis, we introduce a novel two-stage\nautomated pipeline. This pipeline first employs specialized pretrained models\nto extract diverse contextual cues (e.g., speech, music, general sounds, and\nvisual information from associated video). A large language model (LLM) then\nsynthesizes these rich, multimodal inputs to generate detailed and\ncontext-aware audio captions. Key contributions of this work include: (1) the\nproposed scalable method for fine-grained audio caption generation; (2)\nFusionAudio, a new large-scale dataset comprising 1.2 million such detailed\ncaptions, combined with 6 million QA pairs; and (3) enhanced audio models\ndeveloped using FusionAudio, specifically a CLAP-based audio encoder with\nsuperior audio-text alignment and instruction following. This paper paves the\nway for more nuanced and accurate automated understanding of complex audio\nenvironments. Code and data can be found in\nhttps://github.com/satsuki2486441738/FusionAudio.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01111.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "623be9e1d1eb227788764959",
            "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
            "fullname": "Shunian Chen",
            "name": "Shunian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.05523",
            "authors": [
                {
                    "_id": "6846657d3ec10bdd8ab4daca",
                    "user": {
                        "_id": "630bc5ae86b8b9904c33e94b",
                        "avatarUrl": "/avatars/b176d9b1691c05cc941409dd6c2b2228.svg",
                        "isPro": false,
                        "fullname": "Zikui Cai",
                        "user": "Zikui",
                        "type": "user"
                    },
                    "name": "Zikui Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:11:17.551Z",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dacb",
                    "name": "Andrew Wang",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dacc",
                    "name": "Anirudh Satheesh",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dacd",
                    "name": "Ankit Nakhawa",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dace",
                    "name": "Hyunwoo Jae",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dacf",
                    "name": "Keenan Powell",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dad0",
                    "name": "Minghui Liu",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dad1",
                    "name": "Neel Jay",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dad2",
                    "name": "Sungbin Oh",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dad3",
                    "name": "Xiyao Wang",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dad4",
                    "name": "Yongyuan Liang",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dad5",
                    "name": "Tom Goldstein",
                    "hidden": false
                },
                {
                    "_id": "6846657d3ec10bdd8ab4dad6",
                    "name": "Furong Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T19:12:45.000Z",
            "submittedOnDailyAt": "2025-06-09T03:10:23.755Z",
            "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
            "submittedOnDailyBy": {
                "_id": "655fed9fdef5905d38b84af3",
                "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
                "isPro": false,
                "fullname": "Xiyao Wang",
                "user": "russwang",
                "type": "user"
            },
            "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks\nfor multimodal reasoning fall short in three key dimensions. First, they\noverwhelmingly rely on static images, failing to capture the temporal\ncomplexity of real-world environments. Second, they narrowly focus on\nmathematical problem-solving, neglecting the broader spectrum of reasoning\nskills -- including abstract, physical, planning, spatial, and temporal\ncapabilities -- required for robust multimodal intelligence. Third, many\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\nscripted clips with embedded questions spanning six complementary reasoning\ncategories. Each instance is programmatically generated using deterministic\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\ncurated real footage. This script-driven design allows fine-grained control\nover visual complexity, distractor density, and temporal dynamics -- enabling\ndifficulty to be scaled systematically as models improve. Unlike static\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\nits controllable generation pipeline supports the creation of arbitrarily\nchallenging new instances, making it ideally suited for stress-testing\nnext-generation models. Initial experiments with state-of-the-art systems --\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\navailable at the time, alongside strong open-source models -- reveal\nsubstantial performance gaps across all categories, with particularly large\ndeficits in abstract and planning tasks. We release the full dataset,\ngeneration scripts, and evaluation harness to support transparent,\nreproducible, and forward-looking multimodal reasoning research.",
            "upvotes": 26,
            "discussionId": "6846657d3ec10bdd8ab4dad7",
            "projectPage": "https://morse-500.github.io/",
            "githubRepo": "https://github.com/morse-benchmark/morse-500",
            "ai_summary": "MORSE-500, a video benchmark with 500 scripted clips, evaluates multimodal reasoning across six categories, highlighting performance gaps in abstract and planning tasks.",
            "ai_keywords": [
                "Vision-language models",
                "MORSE-500",
                "multimodal reasoning",
                "video benchmark",
                "scripted clips",
                "reasoning categories",
                "Manim",
                "Matplotlib",
                "MoviePy",
                "generative video models",
                "controllable generation",
                "spatial capabilities",
                "temporal capabilities",
                "abstract reasoning",
                "planning tasks"
            ]
        },
        "publishedAt": "2025-06-05T15:12:45.000Z",
        "title": "MORSE-500: A Programmatically Controllable Video Benchmark to\n  Stress-Test Multimodal Reasoning",
        "summary": "Despite rapid advances in vision-language models (VLMs), current benchmarks\nfor multimodal reasoning fall short in three key dimensions. First, they\noverwhelmingly rely on static images, failing to capture the temporal\ncomplexity of real-world environments. Second, they narrowly focus on\nmathematical problem-solving, neglecting the broader spectrum of reasoning\nskills -- including abstract, physical, planning, spatial, and temporal\ncapabilities -- required for robust multimodal intelligence. Third, many\nbenchmarks quickly saturate, offering limited headroom for diagnosing failure\nmodes or measuring continued progress. We introduce MORSE-500 (Multimodal\nReasoning Stress-test Environment), a video benchmark composed of 500 fully\nscripted clips with embedded questions spanning six complementary reasoning\ncategories. Each instance is programmatically generated using deterministic\nPython scripts (via Manim, Matplotlib, MoviePy), generative video models, and\ncurated real footage. This script-driven design allows fine-grained control\nover visual complexity, distractor density, and temporal dynamics -- enabling\ndifficulty to be scaled systematically as models improve. Unlike static\nbenchmarks that become obsolete once saturated, MORSE-500 is built to evolve:\nits controllable generation pipeline supports the creation of arbitrarily\nchallenging new instances, making it ideally suited for stress-testing\nnext-generation models. Initial experiments with state-of-the-art systems --\nincluding various Gemini 2.5 Pro and OpenAI o3 which represent the strongest\navailable at the time, alongside strong open-source models -- reveal\nsubstantial performance gaps across all categories, with particularly large\ndeficits in abstract and planning tasks. We release the full dataset,\ngeneration scripts, and evaluation harness to support transparent,\nreproducible, and forward-looking multimodal reasoning research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05523.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "655fed9fdef5905d38b84af3",
            "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
            "fullname": "Xiyao Wang",
            "name": "russwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.05629",
            "authors": [
                {
                    "_id": "68464b273ec10bdd8ab4da86",
                    "user": {
                        "_id": "64a6518132cf858d6386ac52",
                        "avatarUrl": "/avatars/4cabf3dab8b1ba06245ad8024f334181.svg",
                        "isPro": false,
                        "fullname": "Ananth Muppidi",
                        "user": "ananthmuppidi",
                        "type": "user"
                    },
                    "name": "Ananth Muppidi",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
                    "hidden": false
                },
                {
                    "_id": "68464b273ec10bdd8ab4da87",
                    "user": {
                        "_id": "5f89da6c5d083370c711f37c",
                        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
                        "isPro": false,
                        "fullname": "Abhilash Nandy",
                        "user": "abhi1nandy2",
                        "type": "user"
                    },
                    "name": "Abhilash Nandy",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-09T03:49:29.446Z",
                    "hidden": false
                },
                {
                    "_id": "68464b273ec10bdd8ab4da88",
                    "user": {
                        "_id": "65238ea295df08170c93933d",
                        "avatarUrl": "/avatars/8364301e324274a550d12f2b184ea10e.svg",
                        "isPro": false,
                        "fullname": "Sambaran Bandyopadhyay",
                        "user": "sambaran",
                        "type": "user"
                    },
                    "name": "Sambaran Bandyopadhyay",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-09T02:47:03.971Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T23:13:22.000Z",
            "submittedOnDailyAt": "2025-06-09T01:27:41.831Z",
            "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
            "submittedOnDailyBy": {
                "_id": "5f89da6c5d083370c711f37c",
                "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
                "isPro": false,
                "fullname": "Abhilash Nandy",
                "user": "abhi1nandy2",
                "type": "user"
            },
            "summary": "The performance of large language models in domain-specific tasks\nnecessitates fine-tuning, which is computationally expensive and technically\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\nprompting, a promising approach that adapts pre-trained models to downstream\ntasks by learning a small set of parameters. We propose a novel Input Dependent\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\ngenerates soft prompts based on the input tokens and attends different tokens\nwith varying importance. Our method is simple and efficient, keeping the number\nof trainable parameters small. We show the merits of the proposed approach\ncompared to state-of-the-art techniques on various tasks and show the improved\nzero shot domain transfer capability.",
            "upvotes": 25,
            "discussionId": "68464b273ec10bdd8ab4da89",
            "ai_summary": "A new method using input-dependent soft prompting with a self-attention mechanism improves parameter-efficient fine-tuning for large language models, enhancing zero-shot domain transfer.",
            "ai_keywords": [
                "soft prompting",
                "parameter-efficient fine-tuning",
                "pre-trained models",
                "downstream tasks",
                "Input Dependent Soft Prompting technique",
                "self-Attention Mechanism",
                "zero shot domain transfer"
            ]
        },
        "publishedAt": "2025-06-05T19:13:22.000Z",
        "title": "Leveraging Self-Attention for Input-Dependent Soft Prompting in LLMs",
        "summary": "The performance of large language models in domain-specific tasks\nnecessitates fine-tuning, which is computationally expensive and technically\nchallenging. This paper focuses on parameter-efficient fine-tuning using soft\nprompting, a promising approach that adapts pre-trained models to downstream\ntasks by learning a small set of parameters. We propose a novel Input Dependent\nSoft Prompting technique with a self-Attention Mechanism (ID-SPAM) that\ngenerates soft prompts based on the input tokens and attends different tokens\nwith varying importance. Our method is simple and efficient, keeping the number\nof trainable parameters small. We show the merits of the proposed approach\ncompared to state-of-the-art techniques on various tasks and show the improved\nzero shot domain transfer capability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05629.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f89da6c5d083370c711f37c",
            "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
            "fullname": "Abhilash Nandy",
            "name": "abhi1nandy2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05446",
            "authors": [
                {
                    "_id": "6846f9533ec10bdd8ab4dc81",
                    "user": {
                        "_id": "6536152811c8c094d4074d67",
                        "avatarUrl": "/avatars/4d7cd7b4eca75e268392286eecd46138.svg",
                        "isPro": false,
                        "fullname": "Ivry",
                        "user": "dror44",
                        "type": "user"
                    },
                    "name": "Dror Ivry",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-09T15:32:03.651Z",
                    "hidden": false
                },
                {
                    "_id": "6846f9533ec10bdd8ab4dc82",
                    "user": {
                        "_id": "674d84e250a805a04d8c04ce",
                        "avatarUrl": "/avatars/8eb05aae3c3a930c63caffc8561083c3.svg",
                        "isPro": false,
                        "fullname": "nahum",
                        "user": "oran-qualifire",
                        "type": "user"
                    },
                    "name": "Oran Nahum",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-09T15:10:11.772Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6536152811c8c094d4074d67/W6VGKTJFnW7BIazDAAHiZ.png"
            ],
            "publishedAt": "2025-06-05T14:07:15.000Z",
            "submittedOnDailyAt": "2025-06-09T13:59:42.184Z",
            "title": "Sentinel: SOTA model to protect against prompt injections",
            "submittedOnDailyBy": {
                "_id": "6536152811c8c094d4074d67",
                "avatarUrl": "/avatars/4d7cd7b4eca75e268392286eecd46138.svg",
                "isPro": false,
                "fullname": "Ivry",
                "user": "dror44",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are increasingly powerful but remain vulnerable\nto prompt injection attacks, where malicious inputs cause the model to deviate\nfrom its intended instructions. This paper introduces Sentinel, a novel\ndetection model, qualifire/prompt-injection-sentinel, based on the\n\\answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced\nfeatures and fine-tuning on an extensive and diverse dataset comprising a few\nopen-source and private collections, Sentinel achieves state-of-the-art\nperformance. This dataset amalgamates varied attack types, from role-playing\nand instruction hijacking to attempts to generate biased content, alongside a\nbroad spectrum of benign instructions, with private datasets specifically\ntargeting nuanced error correction and real-world misclassifications. On a\ncomprehensive, unseen internal test set, Sentinel demonstrates an average\naccuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on\npublic benchmarks, it consistently outperforms strong baselines like\nprotectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's\narchitecture, its meticulous dataset curation, its training methodology, and a\nthorough evaluation, highlighting its superior detection capabilities.",
            "upvotes": 19,
            "discussionId": "6846f9533ec10bdd8ab4dc83",
            "ai_summary": "Sentinel, a detection model based on ModernBERT-large, effectively identifies prompt injection attacks with high accuracy and outperforms existing baselines.",
            "ai_keywords": [
                "ModernBERT-large",
                "prompt injection attacks",
                "dataset curation",
                "training methodology",
                "evaluation",
                "role-playing",
                "instruction hijacking",
                "biased content",
                "error correction",
                "real-world misclassifications",
                "F1-score"
            ]
        },
        "publishedAt": "2025-06-05T10:07:15.000Z",
        "title": "Sentinel: SOTA model to protect against prompt injections",
        "summary": "Large Language Models (LLMs) are increasingly powerful but remain vulnerable\nto prompt injection attacks, where malicious inputs cause the model to deviate\nfrom its intended instructions. This paper introduces Sentinel, a novel\ndetection model, qualifire/prompt-injection-sentinel, based on the\n\\answerdotai/ModernBERT-large architecture. By leveraging ModernBERT's advanced\nfeatures and fine-tuning on an extensive and diverse dataset comprising a few\nopen-source and private collections, Sentinel achieves state-of-the-art\nperformance. This dataset amalgamates varied attack types, from role-playing\nand instruction hijacking to attempts to generate biased content, alongside a\nbroad spectrum of benign instructions, with private datasets specifically\ntargeting nuanced error correction and real-world misclassifications. On a\ncomprehensive, unseen internal test set, Sentinel demonstrates an average\naccuracy of 0.987 and an F1-score of 0.980. Furthermore, when evaluated on\npublic benchmarks, it consistently outperforms strong baselines like\nprotectai/deberta-v3-base-prompt-injection-v2. This work details Sentinel's\narchitecture, its meticulous dataset curation, its training methodology, and a\nthorough evaluation, highlighting its superior detection capabilities.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6536152811c8c094d4074d67/W6VGKTJFnW7BIazDAAHiZ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05446.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6536152811c8c094d4074d67",
            "avatarUrl": "/avatars/4d7cd7b4eca75e268392286eecd46138.svg",
            "fullname": "Ivry",
            "name": "dror44",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05573",
            "authors": [
                {
                    "_id": "6846902a3ec10bdd8ab4db61",
                    "name": "Yuchen Lin",
                    "hidden": false
                },
                {
                    "_id": "6846902a3ec10bdd8ab4db62",
                    "user": {
                        "_id": "62e18206926f4892a4c782bd",
                        "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
                        "isPro": false,
                        "fullname": "Chenguo Lin",
                        "user": "chenguolin",
                        "type": "user"
                    },
                    "name": "Chenguo Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:11:09.640Z",
                    "hidden": false
                },
                {
                    "_id": "6846902a3ec10bdd8ab4db63",
                    "name": "Panwang Pan",
                    "hidden": false
                },
                {
                    "_id": "6846902a3ec10bdd8ab4db64",
                    "name": "Honglei Yan",
                    "hidden": false
                },
                {
                    "_id": "6846902a3ec10bdd8ab4db65",
                    "name": "Yiqiang Feng",
                    "hidden": false
                },
                {
                    "_id": "6846902a3ec10bdd8ab4db66",
                    "name": "Yadong Mu",
                    "hidden": false
                },
                {
                    "_id": "6846902a3ec10bdd8ab4db67",
                    "name": "Katerina Fragkiadaki",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
            ],
            "publishedAt": "2025-06-05T20:30:28.000Z",
            "submittedOnDailyAt": "2025-06-09T06:14:01.450Z",
            "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
            "submittedOnDailyBy": {
                "_id": "62e18206926f4892a4c782bd",
                "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
                "isPro": false,
                "fullname": "Chenguo Lin",
                "user": "chenguolin",
                "type": "user"
            },
            "summary": "We introduce PartCrafter, the first structured 3D generative model that\njointly synthesizes multiple semantically meaningful and geometrically distinct\n3D meshes from a single RGB image. Unlike existing methods that either produce\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\nimage and then reconstructing each segment, PartCrafter adopts a unified,\ncompositional generation architecture that does not rely on pre-segmented\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\nparts, enabling end-to-end part-aware generation of both individual objects and\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\nweights, encoder, and decoder, and introduces two key innovations: (1) A\ncompositional latent space, where each 3D part is represented by a set of\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\nstructured information flow both within individual parts and across all parts,\nensuring global coherence while preserving part-level detail during generation.\nTo support part-level supervision, we curate a new dataset by mining part-level\nannotations from large-scale 3D object datasets. Experiments show that\nPartCrafter outperforms existing approaches in generating decomposable 3D\nmeshes, including parts that are not directly visible in input images,\ndemonstrating the strength of part-aware generative priors for 3D understanding\nand synthesis. Code and training data will be released.",
            "upvotes": 18,
            "discussionId": "6846902a3ec10bdd8ab4db68",
            "projectPage": "https://wgsxm.github.io/projects/partcrafter",
            "githubRepo": "https://github.com/wgsxm/PartCrafter",
            "ai_summary": "PartCrafter is a unified 3D generative model that synthesizes multiple semantically meaningful 3D meshes from a single image using a compositional latent space and hierarchical attention mechanism.",
            "ai_keywords": [
                "3D generative model",
                "multiple 3D meshes",
                "RGB image",
                "unified compositional generation architecture",
                "denoising",
                "3D diffusion transformer (DiT)",
                "compositional latent space",
                "disentangled latent tokens",
                "hierarchical attention mechanism",
                "part-level supervision",
                "part-aware generative priors"
            ]
        },
        "publishedAt": "2025-06-05T16:30:28.000Z",
        "title": "PartCrafter: Structured 3D Mesh Generation via Compositional Latent\n  Diffusion Transformers",
        "summary": "We introduce PartCrafter, the first structured 3D generative model that\njointly synthesizes multiple semantically meaningful and geometrically distinct\n3D meshes from a single RGB image. Unlike existing methods that either produce\nmonolithic 3D shapes or follow two-stage pipelines, i.e., first segmenting an\nimage and then reconstructing each segment, PartCrafter adopts a unified,\ncompositional generation architecture that does not rely on pre-segmented\ninputs. Conditioned on a single image, it simultaneously denoises multiple 3D\nparts, enabling end-to-end part-aware generation of both individual objects and\ncomplex multi-object scenes. PartCrafter builds upon a pretrained 3D mesh\ndiffusion transformer (DiT) trained on whole objects, inheriting the pretrained\nweights, encoder, and decoder, and introduces two key innovations: (1) A\ncompositional latent space, where each 3D part is represented by a set of\ndisentangled latent tokens; (2) A hierarchical attention mechanism that enables\nstructured information flow both within individual parts and across all parts,\nensuring global coherence while preserving part-level detail during generation.\nTo support part-level supervision, we curate a new dataset by mining part-level\nannotations from large-scale 3D object datasets. Experiments show that\nPartCrafter outperforms existing approaches in generating decomposable 3D\nmeshes, including parts that are not directly visible in input images,\ndemonstrating the strength of part-aware generative priors for 3D understanding\nand synthesis. Code and training data will be released.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62e18206926f4892a4c782bd/iNc42ij-Kj0Z4V0jD5lCp.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05573.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e18206926f4892a4c782bd",
            "avatarUrl": "/avatars/0f89091a5eb72165d2e860d15b339539.svg",
            "fullname": "Chenguo Lin",
            "name": "chenguolin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.01872",
            "authors": [
                {
                    "_id": "683e77d41417d107337abf6e",
                    "user": {
                        "_id": "643f9e2288d9d4488fd81c52",
                        "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
                        "isPro": false,
                        "fullname": "Tinghui Zhu",
                        "user": "DarthZhu",
                        "type": "user"
                    },
                    "name": "Tinghui Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:12:47.572Z",
                    "hidden": false
                },
                {
                    "_id": "683e77d41417d107337abf6f",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "683e77d41417d107337abf70",
                    "name": "Muhao Chen",
                    "hidden": false
                },
                {
                    "_id": "683e77d41417d107337abf71",
                    "name": "Yu Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-02T17:01:40.000Z",
            "submittedOnDailyAt": "2025-06-09T07:51:42.399Z",
            "title": "Is Extending Modality The Right Path Towards Omni-Modality?",
            "submittedOnDailyBy": {
                "_id": "643f9e2288d9d4488fd81c52",
                "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
                "isPro": false,
                "fullname": "Tinghui Zhu",
                "user": "DarthZhu",
                "type": "user"
            },
            "summary": "Omni-modal language models (OLMs) aim to integrate and reason over diverse\ninput modalities--such as text, images, video, and audio--while maintaining\nstrong language capabilities. Despite recent advancements, existing models,\nespecially open-source ones, remain far from true omni-modality, struggling to\ngeneralize beyond the specific modality pairs they are trained on or to achieve\nstrong performance when processing multi-modal inputs. We study the effect of\nextending modality, the dominant technique for training multimodal models,\nwhere an off-the-shelf language model is fine-tuned on target-domain and\nlanguage data. Specifically, we investigate three key questions: (1) Does\nmodality extension compromise core language abilities? (2) Can model merging\neffectively integrate independently fine-tuned modality-specific models to\nachieve omni-modality? (3) Does omni-modality extension lead to better\nknowledge sharing and generalization compared to sequential extension? Through\nextensive experiments, we analyze these trade-offs and provide insights into\nthe feasibility of achieving true omni-modality using current approaches.",
            "upvotes": 17,
            "discussionId": "683e77d41417d107337abf8f",
            "projectPage": "https://darthzhu.github.io/lm-extend-page/",
            "githubRepo": "https://github.com/DarthZhu/lm-extend",
            "ai_summary": "Research investigates the impact of extending modality and model merging on maintaining language abilities and generalization in omni-modal language models.",
            "ai_keywords": [
                "omni-modal language models",
                "modality extension",
                "fine-tuning",
                "language abilities",
                "model merging",
                "generalization",
                "true omni-modality"
            ]
        },
        "publishedAt": "2025-06-02T13:01:40.000Z",
        "title": "Is Extending Modality The Right Path Towards Omni-Modality?",
        "summary": "Omni-modal language models (OLMs) aim to integrate and reason over diverse\ninput modalities--such as text, images, video, and audio--while maintaining\nstrong language capabilities. Despite recent advancements, existing models,\nespecially open-source ones, remain far from true omni-modality, struggling to\ngeneralize beyond the specific modality pairs they are trained on or to achieve\nstrong performance when processing multi-modal inputs. We study the effect of\nextending modality, the dominant technique for training multimodal models,\nwhere an off-the-shelf language model is fine-tuned on target-domain and\nlanguage data. Specifically, we investigate three key questions: (1) Does\nmodality extension compromise core language abilities? (2) Can model merging\neffectively integrate independently fine-tuned modality-specific models to\nachieve omni-modality? (3) Does omni-modality extension lead to better\nknowledge sharing and generalization compared to sequential extension? Through\nextensive experiments, we analyze these trade-offs and provide insights into\nthe feasibility of achieving true omni-modality using current approaches.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.01872.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643f9e2288d9d4488fd81c52",
            "avatarUrl": "/avatars/e589c9cbd47022883cf33d7555bee89c.svg",
            "fullname": "Tinghui Zhu",
            "name": "DarthZhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.06276",
            "authors": [
                {
                    "_id": "68466dfb3ec10bdd8ab4dae2",
                    "name": "Jiatao Gu",
                    "hidden": false
                },
                {
                    "_id": "68466dfb3ec10bdd8ab4dae3",
                    "name": "Tianrong Chen",
                    "hidden": false
                },
                {
                    "_id": "68466dfb3ec10bdd8ab4dae4",
                    "name": "David Berthelot",
                    "hidden": false
                },
                {
                    "_id": "68466dfb3ec10bdd8ab4dae5",
                    "name": "Huangjie Zheng",
                    "hidden": false
                },
                {
                    "_id": "68466dfb3ec10bdd8ab4dae6",
                    "name": "Yuyang Wang",
                    "hidden": false
                },
                {
                    "_id": "68466dfb3ec10bdd8ab4dae7",
                    "name": "Ruixiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68466dfb3ec10bdd8ab4dae8",
                    "name": "Laurent Dinh",
                    "hidden": false
                },
                {
                    "_id": "68466dfb3ec10bdd8ab4dae9",
                    "name": "Miguel Angel Bautista",
                    "hidden": false
                },
                {
                    "_id": "68466dfb3ec10bdd8ab4daea",
                    "name": "Josh Susskind",
                    "hidden": false
                },
                {
                    "_id": "68466dfb3ec10bdd8ab4daeb",
                    "name": "Shuangfei Zhai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T17:58:39.000Z",
            "submittedOnDailyAt": "2025-06-09T03:58:52.022Z",
            "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
            "submittedOnDailyBy": {
                "_id": "6164e72d73996c363c52e66d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
                "isPro": false,
                "fullname": "Jiatao Gu",
                "user": "thomagram",
                "type": "user"
            },
            "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.",
            "upvotes": 15,
            "discussionId": "68466dfb3ec10bdd8ab4daec",
            "ai_summary": "STARFlow, a generative model combining normalizing flows with autoregressive Transformers, achieves competitive image synthesis performance with innovations in architecture and latent space modeling.",
            "ai_keywords": [
                "normalizing flows",
                "Transformer Autoregressive Flow",
                "TARFlow",
                "theoretical universality",
                "deep-shallow design",
                "pretrained autoencoders",
                "latent space",
                "guidance algorithm",
                "end-to-end normalizing flow",
                "exact maximum likelihood training",
                "class-conditional",
                "text-conditional image generation",
                "state-of-the-art diffusion models"
            ]
        },
        "publishedAt": "2025-06-06T13:58:39.000Z",
        "title": "STARFlow: Scaling Latent Normalizing Flows for High-resolution Image\n  Synthesis",
        "summary": "We present STARFlow, a scalable generative model based on normalizing flows\nthat achieves strong performance in high-resolution image synthesis. The core\nof STARFlow is Transformer Autoregressive Flow (TARFlow), which combines the\nexpressive power of normalizing flows with the structured modeling capabilities\nof Autoregressive Transformers. We first establish the theoretical universality\nof TARFlow for modeling continuous distributions. Building on this foundation,\nwe introduce several key architectural and algorithmic innovations to\nsignificantly enhance scalability: (1) a deep-shallow design, wherein a deep\nTransformer block captures most of the model representational capacity,\ncomplemented by a few shallow Transformer blocks that are computationally\nefficient yet substantially beneficial; (2) modeling in the latent space of\npretrained autoencoders, which proves more effective than direct pixel-level\nmodeling; and (3) a novel guidance algorithm that significantly boosts sample\nquality. Crucially, our model remains an end-to-end normalizing flow, enabling\nexact maximum likelihood training in continuous spaces without discretization.\nSTARFlow achieves competitive performance in both class-conditional and\ntext-conditional image generation tasks, approaching state-of-the-art diffusion\nmodels in sample quality. To our knowledge, this work is the first successful\ndemonstration of normalizing flows operating effectively at this scale and\nresolution.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06276.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6164e72d73996c363c52e66d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1634002684894-noauth.png",
            "fullname": "Jiatao Gu",
            "name": "thomagram",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.05984",
            "authors": [
                {
                    "_id": "68463ee43ec10bdd8ab4da6f",
                    "user": {
                        "_id": "622326ae0129f2097d69a3e2",
                        "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
                        "isPro": false,
                        "fullname": "Cheng-Han Chiang",
                        "user": "dcml0714",
                        "type": "user"
                    },
                    "name": "Cheng-Han Chiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:11:26.041Z",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da70",
                    "user": {
                        "_id": "64dc191bc307ee5369fbcb04",
                        "avatarUrl": "/avatars/5a8a0db63a187e85d4ae2fff93a838f0.svg",
                        "isPro": false,
                        "fullname": "Xiaofei Wang",
                        "user": "xiaofei-wang",
                        "type": "user"
                    },
                    "name": "Xiaofei Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-09T01:54:46.319Z",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da71",
                    "name": "Chung-Ching Lin",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da72",
                    "name": "Kevin Lin",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da73",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da74",
                    "name": "Radu Kopetz",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da75",
                    "name": "Yao Qian",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da76",
                    "name": "Zhendong Wang",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da77",
                    "name": "Zhengyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da78",
                    "name": "Hung-yi Lee",
                    "hidden": false
                },
                {
                    "_id": "68463ee43ec10bdd8ab4da79",
                    "name": "Lijuan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T11:05:48.000Z",
            "submittedOnDailyAt": "2025-06-09T00:28:11.753Z",
            "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
            "submittedOnDailyBy": {
                "_id": "622326ae0129f2097d69a3e2",
                "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
                "isPro": false,
                "fullname": "Cheng-Han Chiang",
                "user": "dcml0714",
                "type": "user"
            },
            "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.",
            "upvotes": 12,
            "discussionId": "68463ee43ec10bdd8ab4da7a",
            "ai_summary": "Audio-aware large language models can assess speaking styles in audio inputs, demonstrating performance comparable to human judges in evaluating synthesized speech along dimensions like emotion, volume, and pitch.",
            "ai_keywords": [
                "audio-aware large language models",
                "ALLMs",
                "speaking styles",
                "SLMs",
                "voice style instruction",
                "role-playing",
                "emotion",
                "volume",
                "speaking pace",
                "word emphasis",
                "pitch control",
                "non-verbal elements",
                "GPT-4o-audio",
                "Gemini-2.5-pro",
                "human evaluation",
                "agreement",
                "speaking style control",
                "natural dialogues"
            ]
        },
        "publishedAt": "2025-06-06T07:05:48.000Z",
        "title": "Audio-Aware Large Language Models as Judges for Speaking Styles",
        "summary": "Audio-aware large language models (ALLMs) can understand the textual and\nnon-textual information in the audio input. In this paper, we explore using\nALLMs as an automatic judge to assess the speaking styles of speeches. We use\nALLM judges to evaluate the speeches generated by SLMs on two tasks: voice\nstyle instruction following and role-playing. The speaking style we consider\nincludes emotion, volume, speaking pace, word emphasis, pitch control, and\nnon-verbal elements. We use four spoken language models (SLMs) to complete the\ntwo tasks and use humans and ALLMs to judge the SLMs' responses. We compare two\nALLM judges, GPT-4o-audio and Gemini-2.5-pro, with human evaluation results and\nshow that the agreement between Gemini and human judges is comparable to the\nagreement between human evaluators. These promising results show that ALLMs can\nbe used as a judge to evaluate SLMs. Our results also reveal that current SLMs,\neven GPT-4o-audio, still have room for improvement in controlling the speaking\nstyle and generating natural dialogues.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05984.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "622326ae0129f2097d69a3e2",
            "avatarUrl": "/avatars/7665223e3fc8b820ce001e6003daf4d2.svg",
            "fullname": "Cheng-Han Chiang",
            "name": "dcml0714",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.06253",
            "authors": [
                {
                    "_id": "68469b773ec10bdd8ab4db88",
                    "name": "Yuping He",
                    "hidden": false
                },
                {
                    "_id": "68469b773ec10bdd8ab4db89",
                    "name": "Yifei Huang",
                    "hidden": false
                },
                {
                    "_id": "68469b773ec10bdd8ab4db8a",
                    "user": {
                        "_id": "6392c73390b8e99a6779a7b0",
                        "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
                        "isPro": false,
                        "fullname": "Guo Chen",
                        "user": "cg1177",
                        "type": "user"
                    },
                    "name": "Guo Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:11:02.757Z",
                    "hidden": false
                },
                {
                    "_id": "68469b773ec10bdd8ab4db8b",
                    "name": "Lidong Lu",
                    "hidden": false
                },
                {
                    "_id": "68469b773ec10bdd8ab4db8c",
                    "name": "Baoqi Pei",
                    "hidden": false
                },
                {
                    "_id": "68469b773ec10bdd8ab4db8d",
                    "name": "Jilan Xu",
                    "hidden": false
                },
                {
                    "_id": "68469b773ec10bdd8ab4db8e",
                    "name": "Tong Lu",
                    "hidden": false
                },
                {
                    "_id": "68469b773ec10bdd8ab4db8f",
                    "name": "Yoichi Sato",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T17:25:48.000Z",
            "submittedOnDailyAt": "2025-06-09T07:00:04.801Z",
            "title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision",
            "submittedOnDailyBy": {
                "_id": "6392c73390b8e99a6779a7b0",
                "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
                "isPro": false,
                "fullname": "Guo Chen",
                "user": "cg1177",
                "type": "user"
            },
            "summary": "Perceiving the world from both egocentric (first-person) and exocentric\n(third-person) perspectives is fundamental to human cognition, enabling rich\nand complementary understanding of dynamic environments. In recent years,\nallowing the machines to leverage the synergistic potential of these dual\nperspectives has emerged as a compelling research direction in video\nunderstanding. In this survey, we provide a comprehensive review of video\nunderstanding from both exocentric and egocentric viewpoints. We begin by\nhighlighting the practical applications of integrating egocentric and\nexocentric techniques, envisioning their potential collaboration across\ndomains. We then identify key research tasks to realize these applications.\nNext, we systematically organize and review recent advancements into three main\nresearch directions: (1) leveraging egocentric data to enhance exocentric\nunderstanding, (2) utilizing exocentric data to improve egocentric analysis,\nand (3) joint learning frameworks that unify both perspectives. For each\ndirection, we analyze a diverse set of tasks and relevant works. Additionally,\nwe discuss benchmark datasets that support research in both perspectives,\nevaluating their scope, diversity, and applicability. Finally, we discuss\nlimitations in current works and propose promising future research directions.\nBy synthesizing insights from both perspectives, our goal is to inspire\nadvancements in video understanding and artificial intelligence, bringing\nmachines closer to perceiving the world in a human-like manner. A GitHub repo\nof related works can be found at\nhttps://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
            "upvotes": 6,
            "discussionId": "68469b773ec10bdd8ab4db90",
            "projectPage": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
            "githubRepo": "https://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision",
            "ai_summary": "A survey on leveraging both egocentric and exocentric video understanding for enhancing complementary tasks with a focus on three research directions and benchmark datasets.",
            "ai_keywords": [
                "egocentric",
                "exocentric",
                "video understanding",
                "research tasks",
                "benchmark datasets"
            ]
        },
        "publishedAt": "2025-06-06T13:25:48.000Z",
        "title": "Bridging Perspectives: A Survey on Cross-view Collaborative Intelligence\n  with Egocentric-Exocentric Vision",
        "summary": "Perceiving the world from both egocentric (first-person) and exocentric\n(third-person) perspectives is fundamental to human cognition, enabling rich\nand complementary understanding of dynamic environments. In recent years,\nallowing the machines to leverage the synergistic potential of these dual\nperspectives has emerged as a compelling research direction in video\nunderstanding. In this survey, we provide a comprehensive review of video\nunderstanding from both exocentric and egocentric viewpoints. We begin by\nhighlighting the practical applications of integrating egocentric and\nexocentric techniques, envisioning their potential collaboration across\ndomains. We then identify key research tasks to realize these applications.\nNext, we systematically organize and review recent advancements into three main\nresearch directions: (1) leveraging egocentric data to enhance exocentric\nunderstanding, (2) utilizing exocentric data to improve egocentric analysis,\nand (3) joint learning frameworks that unify both perspectives. For each\ndirection, we analyze a diverse set of tasks and relevant works. Additionally,\nwe discuss benchmark datasets that support research in both perspectives,\nevaluating their scope, diversity, and applicability. Finally, we discuss\nlimitations in current works and propose promising future research directions.\nBy synthesizing insights from both perspectives, our goal is to inspire\nadvancements in video understanding and artificial intelligence, bringing\nmachines closer to perceiving the world in a human-like manner. A GitHub repo\nof related works can be found at\nhttps://github.com/ayiyayi/Awesome-Egocentric-and-Exocentric-Vision.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06253.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6392c73390b8e99a6779a7b0",
            "avatarUrl": "/avatars/9ff824ab02848120aec5e8de6780bcf1.svg",
            "fullname": "Guo Chen",
            "name": "cg1177",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.06199",
            "authors": [
                {
                    "_id": "684637733ec10bdd8ab4da66",
                    "user": {
                        "_id": "674b2406591d7232820252cd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
                        "isPro": false,
                        "fullname": "Hongyan Zhi",
                        "user": "Hoyard",
                        "type": "user"
                    },
                    "name": "Hongyan Zhi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:11:27.821Z",
                    "hidden": false
                },
                {
                    "_id": "684637733ec10bdd8ab4da67",
                    "name": "Peihao Chen",
                    "hidden": false
                },
                {
                    "_id": "684637733ec10bdd8ab4da68",
                    "name": "Siyuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "684637733ec10bdd8ab4da69",
                    "name": "Yubo Dong",
                    "hidden": false
                },
                {
                    "_id": "684637733ec10bdd8ab4da6a",
                    "name": "Quanxi Wu",
                    "hidden": false
                },
                {
                    "_id": "684637733ec10bdd8ab4da6b",
                    "name": "Lei Han",
                    "hidden": false
                },
                {
                    "_id": "684637733ec10bdd8ab4da6c",
                    "name": "Mingkui Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T16:00:31.000Z",
            "submittedOnDailyAt": "2025-06-09T01:42:52.611Z",
            "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
            "submittedOnDailyBy": {
                "_id": "674b2406591d7232820252cd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
                "isPro": false,
                "fullname": "Hongyan Zhi",
                "user": "Hoyard",
                "type": "user"
            },
            "summary": "Manipulation has long been a challenging task for robots, while humans can\neffortlessly perform complex interactions with objects, such as hanging a cup\non the mug rack. A key reason is the lack of a large and uniform dataset for\nteaching robots manipulation skills. Current robot datasets often record robot\naction in different action spaces within a simple scene. This hinders the robot\nto learn a unified and robust action representation for different robots within\ndiverse scenes. Observing how humans understand a manipulation task, we find\nthat understanding how the objects should move in the 3D space is a critical\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\nworld model from both human and robot manipulation data. This model predicts\nthe future movement of the interacting objects in 3D space, guiding action\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\npipeline. A video diffusion-based world model then learns manipulation physics\nfrom these data, generating 3D optical flow trajectories conditioned on\nlanguage instructions. With the generated 3D object optical flow, we propose a\nflow-guided rendering mechanism, which renders the predicted final state and\nleverages GPT-4o to assess whether the predicted flow aligns with the task\ndescription. This equips the robot with a closed-loop planning ability.\nFinally, we consider the predicted 3D optical flow as constraints for an\noptimization policy to determine a chunk of robot actions for manipulation.\nExtensive experiments demonstrate strong generalization across diverse robotic\nmanipulation tasks and reliable cross-embodiment adaptation without\nhardware-specific training.",
            "upvotes": 5,
            "discussionId": "684637733ec10bdd8ab4da6d",
            "githubRepo": "https://github.com/Hoyyyaard/3DFlowAction/",
            "ai_summary": "A 3D flow world model learned from human and robot manipulation data, using video diffusion and GPT-4o, enables robots to perform diverse manipulation tasks with strong generalization and cross-embodiment adaptation.",
            "ai_keywords": [
                "3D flow world model",
                "moving object auto-detect pipeline",
                "video diffusion-based world model",
                "3D optical flow dataset",
                "ManiFlow-110k",
                "3D optical flow trajectories",
                "flow-guided rendering mechanism",
                "GPT-4o",
                "closed-loop planning",
                "optimization policy",
                "cross-embodiment adaptation"
            ]
        },
        "publishedAt": "2025-06-06T12:00:31.000Z",
        "title": "3DFlowAction: Learning Cross-Embodiment Manipulation from 3D Flow World\n  Model",
        "summary": "Manipulation has long been a challenging task for robots, while humans can\neffortlessly perform complex interactions with objects, such as hanging a cup\non the mug rack. A key reason is the lack of a large and uniform dataset for\nteaching robots manipulation skills. Current robot datasets often record robot\naction in different action spaces within a simple scene. This hinders the robot\nto learn a unified and robust action representation for different robots within\ndiverse scenes. Observing how humans understand a manipulation task, we find\nthat understanding how the objects should move in the 3D space is a critical\nclue for guiding actions. This clue is embodiment-agnostic and suitable for\nboth humans and different robots. Motivated by this, we aim to learn a 3D flow\nworld model from both human and robot manipulation data. This model predicts\nthe future movement of the interacting objects in 3D space, guiding action\nplanning for manipulation. Specifically, we synthesize a large-scale 3D optical\nflow dataset, named ManiFlow-110k, through a moving object auto-detect\npipeline. A video diffusion-based world model then learns manipulation physics\nfrom these data, generating 3D optical flow trajectories conditioned on\nlanguage instructions. With the generated 3D object optical flow, we propose a\nflow-guided rendering mechanism, which renders the predicted final state and\nleverages GPT-4o to assess whether the predicted flow aligns with the task\ndescription. This equips the robot with a closed-loop planning ability.\nFinally, we consider the predicted 3D optical flow as constraints for an\noptimization policy to determine a chunk of robot actions for manipulation.\nExtensive experiments demonstrate strong generalization across diverse robotic\nmanipulation tasks and reliable cross-embodiment adaptation without\nhardware-specific training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06199.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "674b2406591d7232820252cd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/bhDlUVkFQ66yt3BEVs6WU.png",
            "fullname": "Hongyan Zhi",
            "name": "Hoyard",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05817",
            "authors": [
                {
                    "_id": "68467e323ec10bdd8ab4db44",
                    "user": {
                        "_id": "68345ea8beb0d467e37cd421",
                        "avatarUrl": "/avatars/70ddbf00db1c517d61af3a3d283edf42.svg",
                        "isPro": false,
                        "fullname": "Zihan Wang",
                        "user": "zhwang01",
                        "type": "user"
                    },
                    "name": "Zihan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:11:13.717Z",
                    "hidden": false
                },
                {
                    "_id": "68467e323ec10bdd8ab4db45",
                    "name": "Siyao Liu",
                    "hidden": false
                },
                {
                    "_id": "68467e323ec10bdd8ab4db46",
                    "name": "Yang Sun",
                    "hidden": false
                },
                {
                    "_id": "68467e323ec10bdd8ab4db47",
                    "name": "Hongyan Li",
                    "hidden": false
                },
                {
                    "_id": "68467e323ec10bdd8ab4db48",
                    "name": "Kai Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T07:29:01.000Z",
            "submittedOnDailyAt": "2025-06-09T10:26:25.604Z",
            "title": "CodeContests+: High-Quality Test Case Generation for Competitive\n  Programming",
            "submittedOnDailyBy": {
                "_id": "68345ea8beb0d467e37cd421",
                "avatarUrl": "/avatars/70ddbf00db1c517d61af3a3d283edf42.svg",
                "isPro": false,
                "fullname": "Zihan Wang",
                "user": "zhwang01",
                "type": "user"
            },
            "summary": "Competitive programming, due to its high reasoning difficulty and precise\ncorrectness feedback, has become a key task for both training and evaluating\nthe reasoning capabilities of large language models (LLMs). However, while a\nlarge amount of public problem data, such as problem statements and solutions,\nis available, the test cases of these problems are often difficult to obtain.\nTherefore, test case generation is a necessary task for building large-scale\ndatasets, and the quality of the test cases directly determines the accuracy of\nthe evaluation. In this paper, we introduce an LLM-based agent system that\ncreates high-quality test cases for competitive programming problems. We apply\nthis system to the CodeContests dataset and propose a new version with improved\ntest cases, named CodeContests+. We evaluated the quality of test cases in\nCodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels\nto examine the accuracy of these test cases in evaluation. The results\nindicated that CodeContests+ achieves significantly higher accuracy than\nCodeContests, particularly with a notably higher True Positive Rate (TPR).\nSubsequently, our experiments in LLM Reinforcement Learning (RL) further\nconfirmed that improvements in test case quality yield considerable advantages\nfor RL.",
            "upvotes": 5,
            "discussionId": "68467e323ec10bdd8ab4db49",
            "ai_summary": "An LLM-based system generates high-quality test cases for competitive programming problems, enhancing the accuracy of model evaluation and RL performance.",
            "ai_keywords": [
                "LLM-based agent system",
                "test case generation",
                "CodeContests",
                "CodeContests+",
                "True Positive Rate (TPR)",
                "LLM Reinforcement Learning (RL)"
            ]
        },
        "publishedAt": "2025-06-06T03:29:01.000Z",
        "title": "CodeContests+: High-Quality Test Case Generation for Competitive\n  Programming",
        "summary": "Competitive programming, due to its high reasoning difficulty and precise\ncorrectness feedback, has become a key task for both training and evaluating\nthe reasoning capabilities of large language models (LLMs). However, while a\nlarge amount of public problem data, such as problem statements and solutions,\nis available, the test cases of these problems are often difficult to obtain.\nTherefore, test case generation is a necessary task for building large-scale\ndatasets, and the quality of the test cases directly determines the accuracy of\nthe evaluation. In this paper, we introduce an LLM-based agent system that\ncreates high-quality test cases for competitive programming problems. We apply\nthis system to the CodeContests dataset and propose a new version with improved\ntest cases, named CodeContests+. We evaluated the quality of test cases in\nCodeContestsPlus. First, we used 1.72 million submissions with pass/fail labels\nto examine the accuracy of these test cases in evaluation. The results\nindicated that CodeContests+ achieves significantly higher accuracy than\nCodeContests, particularly with a notably higher True Positive Rate (TPR).\nSubsequently, our experiments in LLM Reinforcement Learning (RL) further\nconfirmed that improvements in test case quality yield considerable advantages\nfor RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05817.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "68345ea8beb0d467e37cd421",
            "avatarUrl": "/avatars/70ddbf00db1c517d61af3a3d283edf42.svg",
            "fullname": "Zihan Wang",
            "name": "zhwang01",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04255",
            "authors": [
                {
                    "_id": "684312988f9ec8394c514883",
                    "user": {
                        "_id": "65c43d6d2b723dbc4ddc29d2",
                        "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
                        "isPro": false,
                        "fullname": "Kunal Pai",
                        "user": "guineapig",
                        "type": "user"
                    },
                    "name": "Kunal Pai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-07T05:45:12.032Z",
                    "hidden": false
                },
                {
                    "_id": "684312988f9ec8394c514884",
                    "user": {
                        "_id": "62a0dbe7bff710e3fb05f9ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a0dbe7bff710e3fb05f9ae/uZK0Zkv7YG7jWbweh5tQb.png",
                        "isPro": false,
                        "fullname": "Parth Shah",
                        "user": "helloparthshah",
                        "type": "user"
                    },
                    "name": "Parth Shah",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-06T16:08:57.423Z",
                    "hidden": false
                },
                {
                    "_id": "684312988f9ec8394c514885",
                    "name": "Harshil Patel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-01T17:33:16.000Z",
            "submittedOnDailyAt": "2025-06-09T03:49:09.306Z",
            "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
            "submittedOnDailyBy": {
                "_id": "65c43d6d2b723dbc4ddc29d2",
                "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
                "isPro": false,
                "fullname": "Kunal Pai",
                "user": "guineapig",
                "type": "user"
            },
            "summary": "Rapid Large Language Model (LLM) advancements are fueling autonomous\nMulti-Agent System (MAS) development. However, current frameworks often lack\nflexibility, resource awareness, model diversity, and autonomous tool creation.\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\nResource Utilization), a novel MAS framework enhancing flexibility, resource\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\nmanaging specialized \"employee\" agents, instantiated based on task needs and\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\nmodels when necessary. An economic model with hiring/firing costs promotes team\nstability and efficient resource allocation. The system also includes\nautonomous API tool creation and a memory function. Evaluations on tasks like\nacademic paper review (58% success), safety assessments (100% on a\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\nHASHIRU's capabilities. Case studies illustrate its self-improvement via\nautonomous cost model generation, tool integration, and budget management.\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\nand autonomous functional extension. Source code and benchmarks are available\nat https://github.com/HASHIRU-AI/HASHIRU and\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\navailable at https://hashiruagentx-hashiruai.hf.space upon request.",
            "upvotes": 5,
            "discussionId": "684312998f9ec8394c514886",
            "githubRepo": "https://github.com/HASHIRU-AI/HASHIRU",
            "ai_summary": "HASHIRU, a novel MAS framework, enhances flexibility, resource efficiency, and adaptability by dynamically managing specialized agents and using a hybrid intelligence approach with smaller, local LLMs and external APIs.",
            "ai_keywords": [
                "Hierarchical Agent System",
                "Hybrid Intelligent Resource Utilization",
                "HASHIRU",
                "CEO agent",
                "employee agents",
                "Ollama",
                "external APIs",
                "economic model",
                "hiring/firing costs",
                "autonomous API tool creation",
                "academic paper review",
                "safety assessments",
                "GSM8K",
                "JEEBench",
                "SVAMP",
                "Gemini 2.0 Flash",
                "self-improvement",
                "autonomous cost model generation",
                "tool integration",
                "budget management"
            ]
        },
        "publishedAt": "2025-06-01T13:33:16.000Z",
        "title": "HASHIRU: Hierarchical Agent System for Hybrid Intelligent Resource\n  Utilization",
        "summary": "Rapid Large Language Model (LLM) advancements are fueling autonomous\nMulti-Agent System (MAS) development. However, current frameworks often lack\nflexibility, resource awareness, model diversity, and autonomous tool creation.\nThis paper introduces HASHIRU (Hierarchical Agent System for Hybrid Intelligent\nResource Utilization), a novel MAS framework enhancing flexibility, resource\nefficiency, and adaptability. HASHIRU features a \"CEO\" agent dynamically\nmanaging specialized \"employee\" agents, instantiated based on task needs and\nresource constraints (cost, memory). Its hybrid intelligence prioritizes\nsmaller, local LLMs (via Ollama) while flexibly using external APIs and larger\nmodels when necessary. An economic model with hiring/firing costs promotes team\nstability and efficient resource allocation. The system also includes\nautonomous API tool creation and a memory function. Evaluations on tasks like\nacademic paper review (58% success), safety assessments (100% on a\nJailbreakBench subset), and complex reasoning (outperforming Gemini 2.0 Flash\non GSM8K: 96% vs. 61%; JEEBench: 80% vs. 68.3%; SVAMP: 92% vs. 84%) demonstrate\nHASHIRU's capabilities. Case studies illustrate its self-improvement via\nautonomous cost model generation, tool integration, and budget management.\nHASHIRU offers a promising approach for more robust, efficient, and adaptable\nMAS through dynamic hierarchical control, resource-aware hybrid intelligence,\nand autonomous functional extension. Source code and benchmarks are available\nat https://github.com/HASHIRU-AI/HASHIRU and\nhttps://github.com/HASHIRU-AI/HASHIRUBench respectively, and a live demo is\navailable at https://hashiruagentx-hashiruai.hf.space upon request.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04255.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65c43d6d2b723dbc4ddc29d2",
            "avatarUrl": "/avatars/ffd685be7f309866c38a164245a917aa.svg",
            "fullname": "Kunal Pai",
            "name": "guineapig",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05673",
            "authors": [
                {
                    "_id": "684682c03ec10bdd8ab4db4b",
                    "name": "Sajjad Abdoli",
                    "hidden": false
                },
                {
                    "_id": "684682c03ec10bdd8ab4db4c",
                    "user": {
                        "_id": "6740ab3e584b3f5cdba7669c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740ab3e584b3f5cdba7669c/kHF-ks5ueIvKGAn_2Gc4U.png",
                        "isPro": false,
                        "fullname": "Emet Research",
                        "user": "EmetTheGolum",
                        "type": "user"
                    },
                    "name": "Freeman Lewin",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-09T07:03:11.134Z",
                    "hidden": false
                },
                {
                    "_id": "684682c03ec10bdd8ab4db4d",
                    "name": "Gediminas Vasiliauskas",
                    "hidden": false
                },
                {
                    "_id": "684682c03ec10bdd8ab4db4e",
                    "name": "Fabian Schonholz",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6740ab3e584b3f5cdba7669c/kab-Bwb4alqNMS3NrmEKW.mp4"
            ],
            "publishedAt": "2025-06-06T01:50:28.000Z",
            "submittedOnDailyAt": "2025-06-09T17:09:45.982Z",
            "title": "Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning\n  Vision Models from DataSeeds' Annotated Imagery",
            "submittedOnDailyBy": {
                "_id": "6740ab3e584b3f5cdba7669c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740ab3e584b3f5cdba7669c/kHF-ks5ueIvKGAn_2Gc4U.png",
                "isPro": false,
                "fullname": "Emet Research",
                "user": "EmetTheGolum",
                "type": "user"
            },
            "summary": "The development of modern Artificial Intelligence (AI) models, particularly\ndiffusion-based models employed in computer vision and image generation tasks,\nis undergoing a paradigmatic shift in development methodologies. Traditionally\ndominated by a \"Model Centric\" approach, in which performance gains were\nprimarily pursued through increasingly complex model architectures and\nhyperparameter optimization, the field is now recognizing a more nuanced\n\"Data-Centric\" approach. This emergent framework foregrounds the quality,\nstructure, and relevance of training data as the principal driver of model\nperformance. To operationalize this paradigm shift, we introduce the\nDataSeeds.AI sample dataset (the \"DSD\"), initially comprised of approximately\n10,610 high-quality human peer-ranked photography images accompanied by\nextensive multi-tier annotations. The DSD is a foundational computer vision\ndataset designed to usher in a new standard for commercial image datasets.\nRepresenting a small fraction of DataSeed.AI's 100 million-plus image catalog,\nthe DSD provides a scalable foundation necessary for robust commercial and\nmultimodal AI development. Through this in-depth exploratory analysis, we\ndocument the quantitative improvements generated by the DSD on specific models\nagainst known benchmarks and make the code and the trained models used in our\nevaluation publicly available.",
            "upvotes": 4,
            "discussionId": "684682c03ec10bdd8ab4db4f",
            "ai_summary": "The DataSeeds.AI dataset enhances computer vision models by providing high-quality, peer-ranked images with extensive annotations, leading to improved performance over existing benchmarks.",
            "ai_keywords": [
                "diffusion-based models",
                "Data-Centric approach",
                "DataSeeds.AI",
                "DSD",
                "computer vision",
                "high-quality images",
                "peer-ranked",
                "multi-tier annotations",
                "commercial AI development"
            ]
        },
        "publishedAt": "2025-06-05T21:50:28.000Z",
        "title": "Peer-Ranked Precision: Creating a Foundational Dataset for Fine-Tuning\n  Vision Models from DataSeeds' Annotated Imagery",
        "summary": "The development of modern Artificial Intelligence (AI) models, particularly\ndiffusion-based models employed in computer vision and image generation tasks,\nis undergoing a paradigmatic shift in development methodologies. Traditionally\ndominated by a \"Model Centric\" approach, in which performance gains were\nprimarily pursued through increasingly complex model architectures and\nhyperparameter optimization, the field is now recognizing a more nuanced\n\"Data-Centric\" approach. This emergent framework foregrounds the quality,\nstructure, and relevance of training data as the principal driver of model\nperformance. To operationalize this paradigm shift, we introduce the\nDataSeeds.AI sample dataset (the \"DSD\"), initially comprised of approximately\n10,610 high-quality human peer-ranked photography images accompanied by\nextensive multi-tier annotations. The DSD is a foundational computer vision\ndataset designed to usher in a new standard for commercial image datasets.\nRepresenting a small fraction of DataSeed.AI's 100 million-plus image catalog,\nthe DSD provides a scalable foundation necessary for robust commercial and\nmultimodal AI development. Through this in-depth exploratory analysis, we\ndocument the quantitative improvements generated by the DSD on specific models\nagainst known benchmarks and make the code and the trained models used in our\nevaluation publicly available.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6740ab3e584b3f5cdba7669c/kab-Bwb4alqNMS3NrmEKW.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05673.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6740ab3e584b3f5cdba7669c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6740ab3e584b3f5cdba7669c/kHF-ks5ueIvKGAn_2Gc4U.png",
            "fullname": "Emet Research",
            "name": "EmetTheGolum",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05433",
            "authors": [
                {
                    "_id": "68469df13ec10bdd8ab4db92",
                    "name": "Zikang Liu",
                    "hidden": false
                },
                {
                    "_id": "68469df13ec10bdd8ab4db93",
                    "name": "Tongtian Yue",
                    "hidden": false
                },
                {
                    "_id": "68469df13ec10bdd8ab4db94",
                    "name": "Yepeng Tang",
                    "hidden": false
                },
                {
                    "_id": "68469df13ec10bdd8ab4db95",
                    "name": "Longteng Guo",
                    "hidden": false
                },
                {
                    "_id": "68469df13ec10bdd8ab4db96",
                    "name": "Junxian Cai",
                    "hidden": false
                },
                {
                    "_id": "68469df13ec10bdd8ab4db97",
                    "name": "Qingbin Liu",
                    "hidden": false
                },
                {
                    "_id": "68469df13ec10bdd8ab4db98",
                    "name": "Xi Chen",
                    "hidden": false
                },
                {
                    "_id": "68469df13ec10bdd8ab4db99",
                    "name": "Jing Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T09:13:37.000Z",
            "submittedOnDailyAt": "2025-06-09T07:17:09.252Z",
            "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward",
            "submittedOnDailyBy": {
                "_id": "6448dcf1b6ac93fe6512e342",
                "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
                "isPro": false,
                "fullname": "Zikang Liu",
                "user": "JohnCage",
                "type": "user"
            },
            "summary": "Group Relative Policy Optimization (GRPO) enhances policy learning by\ncomputing gradients from relative comparisons among candidate outputs that\nshare a common input prefix. Despite its effectiveness, GRPO introduces\nsubstantial computational overhead when processing long shared prefixes, which\nmust be redundantly encoded for each group member. This inefficiency becomes a\nmajor scalability bottleneck in long-context learning scenarios. We propose\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\nrestructuring self-attention into two parts, our method enables the shared\nprefix to be encoded only once, while preserving full differentiability and\ncompatibility with end-to-end training. We provide both theoretical and\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\nit yields identical forward outputs and backward gradients, ensuring that the\noptimization dynamics and final policy performance remain unchanged.\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\nresults while significantly reducing the computational cost of training,\nparticularly in long-prefix scenarios. The proposed method is fully\nplug-and-play: it is compatible with existing GRPO-based architectures and can\nbe seamlessly integrated into current training pipelines as a drop-in\nreplacement, requiring no structural modifications and only minimal changes to\ninput construction and attention computation. Prefix Grouper enables the use of\nlarger group sizes under the same computational budget, thereby improving the\nscalability of GRPO to more complex tasks and larger models. Code is now\navailable at https://github.com/johncaged/PrefixGrouper",
            "upvotes": 4,
            "discussionId": "68469df13ec10bdd8ab4db9a",
            "githubRepo": "https://github.com/johncaged/PrefixGrouper",
            "ai_summary": "Prefix Grouper reduces computational overhead in GRPO by encoding shared prefixes only once, improving scalability in long-context scenarios without altering training dynamics or policy performance.",
            "ai_keywords": [
                "Group Relative Policy Optimization (GRPO)",
                "self-attention",
                "Shared-Prefix Forward strategy",
                "computational overhead",
                "long-context learning scenarios",
                "differentiability",
                "end-to-end training",
                "training-equivalent"
            ]
        },
        "publishedAt": "2025-06-05T05:13:37.000Z",
        "title": "Prefix Grouper: Efficient GRPO Training through Shared-Prefix Forward",
        "summary": "Group Relative Policy Optimization (GRPO) enhances policy learning by\ncomputing gradients from relative comparisons among candidate outputs that\nshare a common input prefix. Despite its effectiveness, GRPO introduces\nsubstantial computational overhead when processing long shared prefixes, which\nmust be redundantly encoded for each group member. This inefficiency becomes a\nmajor scalability bottleneck in long-context learning scenarios. We propose\nPrefix Grouper, an efficient GRPO training algorithm that eliminates redundant\nprefix computation via a Shared-Prefix Forward strategy. In particular, by\nrestructuring self-attention into two parts, our method enables the shared\nprefix to be encoded only once, while preserving full differentiability and\ncompatibility with end-to-end training. We provide both theoretical and\nempirical evidence that Prefix Grouper is training-equivalent to standard GRPO:\nit yields identical forward outputs and backward gradients, ensuring that the\noptimization dynamics and final policy performance remain unchanged.\nEmpirically, our experiments confirm that Prefix Grouper achieves consistent\nresults while significantly reducing the computational cost of training,\nparticularly in long-prefix scenarios. The proposed method is fully\nplug-and-play: it is compatible with existing GRPO-based architectures and can\nbe seamlessly integrated into current training pipelines as a drop-in\nreplacement, requiring no structural modifications and only minimal changes to\ninput construction and attention computation. Prefix Grouper enables the use of\nlarger group sizes under the same computational budget, thereby improving the\nscalability of GRPO to more complex tasks and larger models. Code is now\navailable at https://github.com/johncaged/PrefixGrouper",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05433.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6448dcf1b6ac93fe6512e342",
            "avatarUrl": "/avatars/a6441f89eabd156181bafc47c0b2f8c8.svg",
            "fullname": "Zikang Liu",
            "name": "JohnCage",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.06091",
            "authors": [
                {
                    "_id": "68473f1b3ec10bdd8ab4dce1",
                    "name": "Qinyue Zheng",
                    "hidden": false
                },
                {
                    "_id": "68473f1b3ec10bdd8ab4dce2",
                    "name": "Salman Abdullah",
                    "hidden": false
                },
                {
                    "_id": "68473f1b3ec10bdd8ab4dce3",
                    "name": "Sam Rawal",
                    "hidden": false
                },
                {
                    "_id": "68473f1b3ec10bdd8ab4dce4",
                    "name": "Cyril Zakka",
                    "hidden": false
                },
                {
                    "_id": "68473f1b3ec10bdd8ab4dce5",
                    "name": "Sophie Ostmeier",
                    "hidden": false
                },
                {
                    "_id": "68473f1b3ec10bdd8ab4dce6",
                    "name": "Maximilian Purk",
                    "hidden": false
                },
                {
                    "_id": "68473f1b3ec10bdd8ab4dce7",
                    "name": "Eduardo Reis",
                    "hidden": false
                },
                {
                    "_id": "68473f1b3ec10bdd8ab4dce8",
                    "name": "Eric J. Topol",
                    "hidden": false
                },
                {
                    "_id": "68473f1b3ec10bdd8ab4dce9",
                    "name": "Jure Leskovec",
                    "hidden": false
                },
                {
                    "_id": "68473f1b3ec10bdd8ab4dcea",
                    "name": "Michael Moor",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/646a8ef48dfd6ff79b6c33d2/NV1Kdij_yGESgRVwQBaq-.jpeg"
            ],
            "publishedAt": "2025-06-06T13:52:32.000Z",
            "submittedOnDailyAt": "2025-06-09T18:45:06.808Z",
            "title": "MIRIAD: Augmenting LLMs with millions of medical query-response pairs",
            "submittedOnDailyBy": {
                "_id": "646a8ef48dfd6ff79b6c33d2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646a8ef48dfd6ff79b6c33d2/Od83-X9bIwDo5MEirKX3f.jpeg",
                "isPro": false,
                "fullname": "Salman Abdullah",
                "user": "salman-abdullah",
                "type": "user"
            },
            "summary": "LLMs are bound to transform healthcare with advanced decision support and\nflexible chat assistants. However, LLMs are prone to generate inaccurate\nmedical content. To ground LLMs in high-quality medical knowledge, LLMs have\nbeen equipped with external knowledge via RAG, where unstructured medical\nknowledge is split into small text chunks that can be selectively retrieved and\nintegrated into the LLMs context. Yet, existing RAG pipelines rely on raw,\nunstructured medical text, which can be noisy, uncurated and difficult for LLMs\nto effectively leverage. Systematic approaches to organize medical knowledge to\nbest surface it to LLMs are generally lacking. To address these challenges, we\nintroduce MIRIAD, a large-scale, curated corpus of 5,821,948 medical QA pairs,\neach rephrased from and grounded in a passage from peer-reviewed medical\nliterature using a semi-automated pipeline combining LLM generation, filtering,\ngrounding, and human annotation. Unlike prior medical corpora, which rely on\nunstructured text, MIRIAD encapsulates web-scale medical knowledge in an\noperationalized query-response format, which enables more targeted retrieval.\nExperiments on challenging medical QA benchmarks show that augmenting LLMs with\nMIRIAD improves accuracy up to 6.7% compared to unstructured RAG baselines with\nthe same source corpus and with the same amount of retrieved text. Moreover,\nMIRIAD improved the ability of LLMs to detect medical hallucinations by 22.5 to\n37% (increase in F1 score). We further introduce MIRIAD-Atlas, an interactive\nmap of MIRIAD spanning 56 medical disciplines, enabling clinical users to\nvisually explore, search, and refine medical knowledge. MIRIAD promises to\nunlock a wealth of down-stream applications, including medical information\nretrievers, enhanced RAG applications, and knowledge-grounded chat interfaces,\nwhich ultimately enables more reliable LLM applications in healthcare.",
            "upvotes": 3,
            "discussionId": "68473f1b3ec10bdd8ab4dceb",
            "projectPage": "https://med-miriad.github.io",
            "githubRepo": "https://github.com/eth-medical-ai-lab/MIRIAD",
            "ai_summary": "MIRIAD, a large-scale, curated medical QA corpus, enhances LLM accuracy and hallucination detection in healthcare applications.",
            "ai_keywords": [
                "LLMs",
                "RAG",
                "medical QA pairs",
                "semi-automated pipeline",
                "peer-reviewed literature",
                "MIRIAD-Atlas",
                "medical hallucinations",
                "F1 score",
                "medical information retrievers",
                "knowledge-grounded chat interfaces"
            ]
        },
        "publishedAt": "2025-06-06T09:52:32.000Z",
        "title": "MIRIAD: Augmenting LLMs with millions of medical query-response pairs",
        "summary": "LLMs are bound to transform healthcare with advanced decision support and\nflexible chat assistants. However, LLMs are prone to generate inaccurate\nmedical content. To ground LLMs in high-quality medical knowledge, LLMs have\nbeen equipped with external knowledge via RAG, where unstructured medical\nknowledge is split into small text chunks that can be selectively retrieved and\nintegrated into the LLMs context. Yet, existing RAG pipelines rely on raw,\nunstructured medical text, which can be noisy, uncurated and difficult for LLMs\nto effectively leverage. Systematic approaches to organize medical knowledge to\nbest surface it to LLMs are generally lacking. To address these challenges, we\nintroduce MIRIAD, a large-scale, curated corpus of 5,821,948 medical QA pairs,\neach rephrased from and grounded in a passage from peer-reviewed medical\nliterature using a semi-automated pipeline combining LLM generation, filtering,\ngrounding, and human annotation. Unlike prior medical corpora, which rely on\nunstructured text, MIRIAD encapsulates web-scale medical knowledge in an\noperationalized query-response format, which enables more targeted retrieval.\nExperiments on challenging medical QA benchmarks show that augmenting LLMs with\nMIRIAD improves accuracy up to 6.7% compared to unstructured RAG baselines with\nthe same source corpus and with the same amount of retrieved text. Moreover,\nMIRIAD improved the ability of LLMs to detect medical hallucinations by 22.5 to\n37% (increase in F1 score). We further introduce MIRIAD-Atlas, an interactive\nmap of MIRIAD spanning 56 medical disciplines, enabling clinical users to\nvisually explore, search, and refine medical knowledge. MIRIAD promises to\nunlock a wealth of down-stream applications, including medical information\nretrievers, enhanced RAG applications, and knowledge-grounded chat interfaces,\nwhich ultimately enables more reliable LLM applications in healthcare.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/646a8ef48dfd6ff79b6c33d2/NV1Kdij_yGESgRVwQBaq-.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06091.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "646a8ef48dfd6ff79b6c33d2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646a8ef48dfd6ff79b6c33d2/Od83-X9bIwDo5MEirKX3f.jpeg",
            "fullname": "Salman Abdullah",
            "name": "salman-abdullah",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.05579",
            "authors": [
                {
                    "_id": "6847057b3ec10bdd8ab4dc8b",
                    "name": "Quan Shi",
                    "hidden": false
                },
                {
                    "_id": "6847057b3ec10bdd8ab4dc8c",
                    "name": "Carlos E. Jimenez",
                    "hidden": false
                },
                {
                    "_id": "6847057b3ec10bdd8ab4dc8d",
                    "name": "Shunyu Yao",
                    "hidden": false
                },
                {
                    "_id": "6847057b3ec10bdd8ab4dc8e",
                    "name": "Nick Haber",
                    "hidden": false
                },
                {
                    "_id": "6847057b3ec10bdd8ab4dc8f",
                    "name": "Diyi Yang",
                    "hidden": false
                },
                {
                    "_id": "6847057b3ec10bdd8ab4dc90",
                    "name": "Karthik Narasimhan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T20:48:16.000Z",
            "submittedOnDailyAt": "2025-06-09T14:33:04.893Z",
            "title": "When Models Know More Than They Can Explain: Quantifying Knowledge\n  Transfer in Human-AI Collaboration",
            "submittedOnDailyBy": {
                "_id": "654fe67e45c0dccd570cf4bb",
                "avatarUrl": "/avatars/5bf84b02b452acf9c13e9254fc820a5b.svg",
                "isPro": false,
                "fullname": "Ben Shi",
                "user": "benshi34",
                "type": "user"
            },
            "summary": "Recent advancements in AI reasoning have driven substantial improvements\nacross diverse tasks. A critical open question is whether these improvements\nalso yields better knowledge transfer: the ability of models to communicate\nreasoning in ways humans can understand, apply, and learn from. To investigate\nthis, we introduce Knowledge Integration and Transfer Evaluation (KITE), a\nconceptual and experimental framework for Human-AI knowledge transfer\ncapabilities and conduct the first large-scale human study (N=118) explicitly\ndesigned to measure it. In our two-phase setup, humans first ideate with an AI\non problem-solving strategies, then independently implement solutions,\nisolating model explanations' influence on human understanding. Our findings\nreveal that although model benchmark performance correlates with collaborative\noutcomes, this relationship is notably inconsistent, featuring significant\noutliers, indicating that knowledge transfer requires dedicated optimization.\nOur analysis identifies behavioral and strategic factors mediating successful\nknowledge transfer. We release our code, dataset, and evaluation framework to\nsupport future work on communicatively aligned models.",
            "upvotes": 3,
            "discussionId": "6847057b3ec10bdd8ab4dc91",
            "ai_summary": "Research investigates human-AI knowledge transfer through a large-scale study, revealing that AI performance does not consistently correlate with human understanding, requiring dedicated optimization for effective communication.",
            "ai_keywords": [
                "Knowledge Integration and Transfer Evaluation",
                "KITE",
                "Human-AI knowledge transfer",
                "problem-solving strategies",
                "model explanations",
                "communicationally aligned models"
            ]
        },
        "publishedAt": "2025-06-05T16:48:16.000Z",
        "title": "When Models Know More Than They Can Explain: Quantifying Knowledge\n  Transfer in Human-AI Collaboration",
        "summary": "Recent advancements in AI reasoning have driven substantial improvements\nacross diverse tasks. A critical open question is whether these improvements\nalso yields better knowledge transfer: the ability of models to communicate\nreasoning in ways humans can understand, apply, and learn from. To investigate\nthis, we introduce Knowledge Integration and Transfer Evaluation (KITE), a\nconceptual and experimental framework for Human-AI knowledge transfer\ncapabilities and conduct the first large-scale human study (N=118) explicitly\ndesigned to measure it. In our two-phase setup, humans first ideate with an AI\non problem-solving strategies, then independently implement solutions,\nisolating model explanations' influence on human understanding. Our findings\nreveal that although model benchmark performance correlates with collaborative\noutcomes, this relationship is notably inconsistent, featuring significant\noutliers, indicating that knowledge transfer requires dedicated optimization.\nOur analysis identifies behavioral and strategic factors mediating successful\nknowledge transfer. We release our code, dataset, and evaluation framework to\nsupport future work on communicatively aligned models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05579.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654fe67e45c0dccd570cf4bb",
            "avatarUrl": "/avatars/5bf84b02b452acf9c13e9254fc820a5b.svg",
            "fullname": "Ben Shi",
            "name": "benshi34",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.04120",
            "authors": [
                {
                    "_id": "6846b4453ec10bdd8ab4dbc9",
                    "name": "Ben Moran",
                    "hidden": false
                },
                {
                    "_id": "6846b4453ec10bdd8ab4dbca",
                    "name": "Mauro Comi",
                    "hidden": false
                },
                {
                    "_id": "6846b4453ec10bdd8ab4dbcb",
                    "name": "Steven Bohez",
                    "hidden": false
                },
                {
                    "_id": "6846b4453ec10bdd8ab4dbcc",
                    "name": "Tom Erez",
                    "hidden": false
                },
                {
                    "_id": "6846b4453ec10bdd8ab4dbcd",
                    "name": "Zhibin Li",
                    "hidden": false
                },
                {
                    "_id": "6846b4453ec10bdd8ab4dbce",
                    "name": "Leonard Hasenclever",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T16:14:31.000Z",
            "submittedOnDailyAt": "2025-06-09T09:17:22.520Z",
            "title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data",
            "submittedOnDailyBy": {
                "_id": "62f6dd9b2e53c2efd33f8207",
                "avatarUrl": "/avatars/887d795e0e650a8cc67e66f552187a73.svg",
                "isPro": false,
                "fullname": "Mauro Comi",
                "user": "MauroC",
                "type": "user"
            },
            "summary": "Creating accurate, physical simulations directly from real-world robot motion\nholds great value for safe, scalable, and affordable robot learning, yet\nremains exceptionally challenging. Real robot data suffers from occlusions,\nnoisy camera poses, dynamic scene elements, which hinder the creation of\ngeometrically accurate and photorealistic digital twins of unseen objects. We\nintroduce a novel real-to-sim framework tackling all these challenges at once.\nOur key insight is a hybrid scene representation merging the photorealistic\nrendering of 3D Gaussian Splatting with explicit object meshes suitable for\nphysics simulation within a single representation. We propose an end-to-end\noptimization pipeline that leverages differentiable rendering and\ndifferentiable physics within MuJoCo to jointly refine all scene components -\nfrom object geometry and appearance to robot poses and physical parameters -\ndirectly from raw and imprecise robot trajectories. This unified optimization\nallows us to simultaneously achieve high-fidelity object mesh reconstruction,\ngenerate photorealistic novel views, and perform annotation-free robot pose\ncalibration. We demonstrate the effectiveness of our approach both in\nsimulation and on challenging real-world sequences using an ALOHA 2 bi-manual\nmanipulator, enabling more practical and robust real-to-simulation pipelines.",
            "upvotes": 3,
            "discussionId": "6846b4453ec10bdd8ab4dbcf",
            "ai_summary": "A novel real-to-sim framework merges 3D Gaussian Splatting and object meshes for accurate physics simulation, refining geometry, appearance, and robot poses from raw trajectories.",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "object meshes",
                "physics simulation",
                "differentiable rendering",
                "differentiable physics",
                "MuJoCo",
                "scene reconstruction",
                "photorealistic rendering",
                "robot pose calibration",
                "ALOHA 2"
            ]
        },
        "publishedAt": "2025-06-04T12:14:31.000Z",
        "title": "Splatting Physical Scenes: End-to-End Real-to-Sim from Imperfect Robot\n  Data",
        "summary": "Creating accurate, physical simulations directly from real-world robot motion\nholds great value for safe, scalable, and affordable robot learning, yet\nremains exceptionally challenging. Real robot data suffers from occlusions,\nnoisy camera poses, dynamic scene elements, which hinder the creation of\ngeometrically accurate and photorealistic digital twins of unseen objects. We\nintroduce a novel real-to-sim framework tackling all these challenges at once.\nOur key insight is a hybrid scene representation merging the photorealistic\nrendering of 3D Gaussian Splatting with explicit object meshes suitable for\nphysics simulation within a single representation. We propose an end-to-end\noptimization pipeline that leverages differentiable rendering and\ndifferentiable physics within MuJoCo to jointly refine all scene components -\nfrom object geometry and appearance to robot poses and physical parameters -\ndirectly from raw and imprecise robot trajectories. This unified optimization\nallows us to simultaneously achieve high-fidelity object mesh reconstruction,\ngenerate photorealistic novel views, and perform annotation-free robot pose\ncalibration. We demonstrate the effectiveness of our approach both in\nsimulation and on challenging real-world sequences using an ALOHA 2 bi-manual\nmanipulator, enabling more practical and robust real-to-simulation pipelines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04120.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f6dd9b2e53c2efd33f8207",
            "avatarUrl": "/avatars/887d795e0e650a8cc67e66f552187a73.svg",
            "fullname": "Mauro Comi",
            "name": "MauroC",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.05551",
            "authors": [
                {
                    "_id": "684753683ec10bdd8ab4dd1d",
                    "name": "Yan Shu",
                    "hidden": false
                },
                {
                    "_id": "684753683ec10bdd8ab4dd1e",
                    "name": "Hangui Lin",
                    "hidden": false
                },
                {
                    "_id": "684753683ec10bdd8ab4dd1f",
                    "name": "Yexin Liu",
                    "hidden": false
                },
                {
                    "_id": "684753683ec10bdd8ab4dd20",
                    "name": "Yan Zhang",
                    "hidden": false
                },
                {
                    "_id": "684753683ec10bdd8ab4dd21",
                    "name": "Gangyan Zeng",
                    "hidden": false
                },
                {
                    "_id": "684753683ec10bdd8ab4dd22",
                    "name": "Yan Li",
                    "hidden": false
                },
                {
                    "_id": "684753683ec10bdd8ab4dd23",
                    "name": "Yu Zhou",
                    "hidden": false
                },
                {
                    "_id": "684753683ec10bdd8ab4dd24",
                    "name": "Ser-Nam Lim",
                    "hidden": false
                },
                {
                    "_id": "684753683ec10bdd8ab4dd25",
                    "name": "Harry Yang",
                    "hidden": false
                },
                {
                    "_id": "684753683ec10bdd8ab4dd26",
                    "name": "Nicu Sebe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T19:53:19.000Z",
            "submittedOnDailyAt": "2025-06-09T20:05:32.431Z",
            "title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models\n  Hallucinations in Scene Text Spotting and Understanding",
            "submittedOnDailyBy": {
                "_id": "65c4f99b27736b5b86c2cbda",
                "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
                "isPro": false,
                "fullname": "Yan Shu",
                "user": "sy1998",
                "type": "user"
            },
            "summary": "Large Multimodal Models (LMMs) have achieved impressive progress in visual\nperception and reasoning. However, when confronted with visually ambiguous or\nnon-semantic scene text, they often struggle to accurately spot and understand\nthe content, frequently generating semantically plausible yet visually\nincorrect answers, which we refer to as semantic hallucination. In this work,\nwe investigate the underlying causes of semantic hallucination and identify a\nkey finding: Transformer layers in LLM with stronger attention focus on scene\ntext regions are less prone to producing semantic hallucinations. Thus, we\npropose a training-free semantic hallucination mitigation framework comprising\ntwo key components: (1) ZoomText, a coarse-to-fine strategy that identifies\npotential text regions without external detectors; and (2) Grounded Layer\nCorrection, which adaptively leverages the internal representations from layers\nless prone to hallucination to guide decoding, correcting hallucinated outputs\nfor non-semantic samples while preserving the semantics of meaningful ones. To\nenable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over\n1,730 samples spanning both semantic and non-semantic cases, with manually\ncurated question-answer pairs designed to probe model hallucinations. Extensive\nexperiments demonstrate that our method not only effectively mitigates semantic\nhallucination but also achieves strong performance on public benchmarks for\nscene text spotting and understanding.",
            "upvotes": 2,
            "discussionId": "684753693ec10bdd8ab4dd27",
            "ai_summary": "A framework using Transformer layers with strong attention and a coarse-to-fine strategy reduces semantic hallucination in LMMs for visual perception and reasoning.",
            "ai_keywords": [
                "Transformers",
                "attention focus",
                "semantic hallucination",
                "ZoomText",
                "Grounded Layer Correction",
                "TextHalu-Bench",
                "scene text spotting",
                "scene text understanding"
            ]
        },
        "publishedAt": "2025-06-05T15:53:19.000Z",
        "title": "When Semantics Mislead Vision: Mitigating Large Multimodal Models\n  Hallucinations in Scene Text Spotting and Understanding",
        "summary": "Large Multimodal Models (LMMs) have achieved impressive progress in visual\nperception and reasoning. However, when confronted with visually ambiguous or\nnon-semantic scene text, they often struggle to accurately spot and understand\nthe content, frequently generating semantically plausible yet visually\nincorrect answers, which we refer to as semantic hallucination. In this work,\nwe investigate the underlying causes of semantic hallucination and identify a\nkey finding: Transformer layers in LLM with stronger attention focus on scene\ntext regions are less prone to producing semantic hallucinations. Thus, we\npropose a training-free semantic hallucination mitigation framework comprising\ntwo key components: (1) ZoomText, a coarse-to-fine strategy that identifies\npotential text regions without external detectors; and (2) Grounded Layer\nCorrection, which adaptively leverages the internal representations from layers\nless prone to hallucination to guide decoding, correcting hallucinated outputs\nfor non-semantic samples while preserving the semantics of meaningful ones. To\nenable rigorous evaluation, we introduce TextHalu-Bench, a benchmark of over\n1,730 samples spanning both semantic and non-semantic cases, with manually\ncurated question-answer pairs designed to probe model hallucinations. Extensive\nexperiments demonstrate that our method not only effectively mitigates semantic\nhallucination but also achieves strong performance on public benchmarks for\nscene text spotting and understanding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05551.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65c4f99b27736b5b86c2cbda",
            "avatarUrl": "/avatars/8789b231ec16073ea0229c28f1f1dd06.svg",
            "fullname": "Yan Shu",
            "name": "sy1998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.04755",
            "authors": [
                {
                    "_id": "6846ba9d3ec10bdd8ab4dbe3",
                    "name": "Shenshen Li",
                    "hidden": false
                },
                {
                    "_id": "6846ba9d3ec10bdd8ab4dbe4",
                    "name": "Kaiyuan Deng",
                    "hidden": false
                },
                {
                    "_id": "6846ba9d3ec10bdd8ab4dbe5",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "6846ba9d3ec10bdd8ab4dbe6",
                    "name": "Hao Yang",
                    "hidden": false
                },
                {
                    "_id": "6846ba9d3ec10bdd8ab4dbe7",
                    "name": "Chong Peng",
                    "hidden": false
                },
                {
                    "_id": "6846ba9d3ec10bdd8ab4dbe8",
                    "name": "Peng Yan",
                    "hidden": false
                },
                {
                    "_id": "6846ba9d3ec10bdd8ab4dbe9",
                    "name": "Fumin Shen",
                    "hidden": false
                },
                {
                    "_id": "6846ba9d3ec10bdd8ab4dbea",
                    "name": "Heng Tao Shen",
                    "hidden": false
                },
                {
                    "_id": "6846ba9d3ec10bdd8ab4dbeb",
                    "name": "Xing Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T08:40:24.000Z",
            "submittedOnDailyAt": "2025-06-09T09:15:34.666Z",
            "title": "Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "684526dd103a93da4dc7d850",
                "avatarUrl": "/avatars/5e756e1f5da6b68afb6c9ed165d3bc28.svg",
                "isPro": false,
                "fullname": "Shenshen Li",
                "user": "lss727",
                "type": "user"
            },
            "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in complex reasoning tasks via reinforcement learning, it is commonly\nbelieved that extensive training data is necessary for improving multi-modal\nreasoning ability, inevitably leading to data redundancy and substantial\ncomputational costs. However, can smaller high-value datasets match or\noutperform full corpora for multi-modal reasoning in MLLMs? In this work, we\nchallenge this assumption through a key observation: meaningful multi-modal\nreasoning is triggered by only a sparse subset of training samples, termed\ncognitive samples, whereas the majority contribute marginally. Building on this\ninsight, we propose a novel data selection paradigm termed Reasoning Activation\nPotential (RAP), which identifies cognitive samples by estimating each sample's\npotential to stimulate genuine multi-modal reasoning by two complementary\nestimators: 1) Causal Discrepancy Estimator (CDE) based on the potential\noutcome model principle, eliminates samples that overly rely on language priors\nby comparing outputs between multi-modal and text-only inputs; 2) Attention\nConfidence Estimator (ACE), which exploits token-level self-attention to\ndiscard samples dominated by irrelevant but over-emphasized tokens in\nintermediate reasoning stages. Moreover, we introduce a Difficulty-aware\nReplacement Module (DRM) to substitute trivial instances with cognitively\nchallenging ones, thereby ensuring complexity for robust multi-modal reasoning.\nExperiments on six datasets show that our RAP method consistently achieves\nsuperior performance using only 9.3% of the training data, while reducing\ncomputational costs by over 43%. Our code is available at\nhttps://github.com/Leo-ssl/RAP.",
            "upvotes": 2,
            "discussionId": "6846ba9d3ec10bdd8ab4dbec",
            "ai_summary": "A new data selection paradigm, Reasoning Activation Potential (RAP), enhances multi-modal reasoning in large language models using minimal high-value datasets, improving performance and reducing computational costs.",
            "ai_keywords": [
                "multi-modal large language models (MLLMs)",
                "reinforcement learning",
                "cognitive samples",
                "Reasoning Activation Potential (RAP)",
                "Causal Discrepancy Estimator (CDE)",
                "Attention Confidence Estimator (ACE)",
                "token-level self-attention",
                "Difficulty-aware Replacement Module (DRM)"
            ]
        },
        "publishedAt": "2025-06-05T04:40:24.000Z",
        "title": "Truth in the Few: High-Value Data Selection for Efficient Multi-Modal\n  Reasoning",
        "summary": "While multi-modal large language models (MLLMs) have made significant\nprogress in complex reasoning tasks via reinforcement learning, it is commonly\nbelieved that extensive training data is necessary for improving multi-modal\nreasoning ability, inevitably leading to data redundancy and substantial\ncomputational costs. However, can smaller high-value datasets match or\noutperform full corpora for multi-modal reasoning in MLLMs? In this work, we\nchallenge this assumption through a key observation: meaningful multi-modal\nreasoning is triggered by only a sparse subset of training samples, termed\ncognitive samples, whereas the majority contribute marginally. Building on this\ninsight, we propose a novel data selection paradigm termed Reasoning Activation\nPotential (RAP), which identifies cognitive samples by estimating each sample's\npotential to stimulate genuine multi-modal reasoning by two complementary\nestimators: 1) Causal Discrepancy Estimator (CDE) based on the potential\noutcome model principle, eliminates samples that overly rely on language priors\nby comparing outputs between multi-modal and text-only inputs; 2) Attention\nConfidence Estimator (ACE), which exploits token-level self-attention to\ndiscard samples dominated by irrelevant but over-emphasized tokens in\nintermediate reasoning stages. Moreover, we introduce a Difficulty-aware\nReplacement Module (DRM) to substitute trivial instances with cognitively\nchallenging ones, thereby ensuring complexity for robust multi-modal reasoning.\nExperiments on six datasets show that our RAP method consistently achieves\nsuperior performance using only 9.3% of the training data, while reducing\ncomputational costs by over 43%. Our code is available at\nhttps://github.com/Leo-ssl/RAP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04755.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "684526dd103a93da4dc7d850",
            "avatarUrl": "/avatars/5e756e1f5da6b68afb6c9ed165d3bc28.svg",
            "fullname": "Shenshen Li",
            "name": "lss727",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.00649",
            "authors": [
                {
                    "_id": "6842edd6a4e3571765bb4916",
                    "user": {
                        "_id": "64fee0a7a10455384ebba184",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fee0a7a10455384ebba184/KIzdANoxlbWLLqP2pEaka.jpeg",
                        "isPro": false,
                        "fullname": "Neil de la fuente",
                        "user": "neildlf",
                        "type": "user"
                    },
                    "name": "Neil De La Fuente",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-07T05:45:23.265Z",
                    "hidden": false
                },
                {
                    "_id": "6842edd6a4e3571765bb4917",
                    "user": {
                        "_id": "6253d0cc27a8414d3bbea683",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6253d0cc27a8414d3bbea683/fRck2wXEg-PS11ph9u8ZI.jpeg",
                        "isPro": false,
                        "fullname": "Oscar Sainz",
                        "user": "OSainz",
                        "type": "user"
                    },
                    "name": "Oscar Sainz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-07T05:45:25.760Z",
                    "hidden": false
                },
                {
                    "_id": "6842edd6a4e3571765bb4918",
                    "name": "Iker Garca-Ferrero",
                    "hidden": false
                },
                {
                    "_id": "6842edd6a4e3571765bb4919",
                    "name": "Eneko Agirre",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/ncvZw46LU97dolkK36sui.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/jQdtGSQJ4ixjSVHuzUwh2.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/j8nPbiL7B_r33slPVfi9a.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/WYVjvaRSoDKI0Oy0wf0De.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/zRQJseogEtf-qR6IwWgxz.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/D7jgqjQ5dlCSFblJQ04LV.jpeg"
            ],
            "publishedAt": "2025-05-31T17:36:18.000Z",
            "submittedOnDailyAt": "2025-06-09T09:01:33.414Z",
            "title": "GuideX: Guided Synthetic Data Generation for Zero-Shot Information\n  Extraction",
            "submittedOnDailyBy": {
                "_id": "64fee0a7a10455384ebba184",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fee0a7a10455384ebba184/KIzdANoxlbWLLqP2pEaka.jpeg",
                "isPro": false,
                "fullname": "Neil de la fuente",
                "user": "neildlf",
                "type": "user"
            },
            "summary": "Information Extraction (IE) systems are traditionally domain-specific,\nrequiring costly adaptation that involves expert schema design, data\nannotation, and model training. While Large Language Models have shown promise\nin zero-shot IE, performance degrades significantly in unseen domains where\nlabel definitions differ. This paper introduces GUIDEX, a novel method that\nautomatically defines domain-specific schemas, infers guidelines, and generates\nsynthetically labeled instances, allowing for better out-of-domain\ngeneralization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art\nacross seven zeroshot Named Entity Recognition benchmarks. Models trained with\nGUIDEX gain up to 7 F1 points over previous methods without humanlabeled data,\nand nearly 2 F1 points higher when combined with it. Models trained on GUIDEX\ndemonstrate enhanced comprehension of complex, domain-specific annotation\nschemas. Code, models, and synthetic datasets are available at\nneilus03.github.io/guidex.com",
            "upvotes": 2,
            "discussionId": "6842edd7a4e3571765bb491a",
            "projectPage": "https://neilus03.github.io/guidex.com/",
            "githubRepo": "https://github.com/Neilus03/GUIDEX",
            "ai_summary": "GUIDEX enhances zero-shot Named Entity Recognition by automatically defining schemas and inferring guidelines, setting new benchmarks without extensive human-labeled data.",
            "ai_keywords": [
                "Large Language Models",
                "zero-shot IE",
                "domain-specific schemas",
                "Named Entity Recognition",
                "fine-tuning",
                "Llama 3.1",
                "synthetic labeled instances",
                "F1 points"
            ]
        },
        "publishedAt": "2025-05-31T13:36:18.000Z",
        "title": "GuideX: Guided Synthetic Data Generation for Zero-Shot Information\n  Extraction",
        "summary": "Information Extraction (IE) systems are traditionally domain-specific,\nrequiring costly adaptation that involves expert schema design, data\nannotation, and model training. While Large Language Models have shown promise\nin zero-shot IE, performance degrades significantly in unseen domains where\nlabel definitions differ. This paper introduces GUIDEX, a novel method that\nautomatically defines domain-specific schemas, infers guidelines, and generates\nsynthetically labeled instances, allowing for better out-of-domain\ngeneralization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art\nacross seven zeroshot Named Entity Recognition benchmarks. Models trained with\nGUIDEX gain up to 7 F1 points over previous methods without humanlabeled data,\nand nearly 2 F1 points higher when combined with it. Models trained on GUIDEX\ndemonstrate enhanced comprehension of complex, domain-specific annotation\nschemas. Code, models, and synthetic datasets are available at\nneilus03.github.io/guidex.com",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/ncvZw46LU97dolkK36sui.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/jQdtGSQJ4ixjSVHuzUwh2.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/j8nPbiL7B_r33slPVfi9a.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/WYVjvaRSoDKI0Oy0wf0De.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/zRQJseogEtf-qR6IwWgxz.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64fee0a7a10455384ebba184/D7jgqjQ5dlCSFblJQ04LV.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.00649.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64fee0a7a10455384ebba184",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fee0a7a10455384ebba184/KIzdANoxlbWLLqP2pEaka.jpeg",
            "fullname": "Neil de la fuente",
            "name": "neildlf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03828",
            "authors": [
                {
                    "_id": "68473c983ec10bdd8ab4dcd3",
                    "name": "Dhaval Patel",
                    "hidden": false
                },
                {
                    "_id": "68473c983ec10bdd8ab4dcd4",
                    "user": {
                        "_id": "66ee36c1789ce1b57bb01e9d",
                        "avatarUrl": "/avatars/1d31fa8ce2eb73107df641a73c3077c2.svg",
                        "isPro": false,
                        "fullname": "Shuxin Lin",
                        "user": "shuxinl",
                        "type": "user"
                    },
                    "name": "Shuxin Lin",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-09T19:57:13.307Z",
                    "hidden": false
                },
                {
                    "_id": "68473c983ec10bdd8ab4dcd5",
                    "name": "James Rayfield",
                    "hidden": false
                },
                {
                    "_id": "68473c983ec10bdd8ab4dcd6",
                    "name": "Nianjun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68473c983ec10bdd8ab4dcd7",
                    "name": "Roman Vaculin",
                    "hidden": false
                },
                {
                    "_id": "68473c983ec10bdd8ab4dcd8",
                    "name": "Natalia Martinez",
                    "hidden": false
                },
                {
                    "_id": "68473c983ec10bdd8ab4dcd9",
                    "name": "Fearghal O'donncha",
                    "hidden": false
                },
                {
                    "_id": "68473c983ec10bdd8ab4dcda",
                    "name": "Jayant Kalagnanam",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T10:57:35.000Z",
            "submittedOnDailyAt": "2025-06-09T18:28:15.310Z",
            "title": "AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial\n  Asset Operations and Maintenance",
            "submittedOnDailyBy": {
                "_id": "64c47f731d44fc06afc80953",
                "avatarUrl": "/avatars/ece59a98ae932fea48d269de6d4fb485.svg",
                "isPro": false,
                "fullname": "Dhaval",
                "user": "DhavalPatel",
                "type": "user"
            },
            "summary": "AI for Industrial Asset Lifecycle Management aims to automate complex\noperational workflows -- such as condition monitoring, maintenance planning,\nand intervention scheduling -- to reduce human workload and minimize system\ndowntime. Traditional AI/ML approaches have primarily tackled these problems in\nisolation, solving narrow tasks within the broader operational pipeline. In\ncontrast, the emergence of AI agents and large language models (LLMs)\nintroduces a next-generation opportunity: enabling end-to-end automation across\nthe entire asset lifecycle. This paper envisions a future where AI agents\nautonomously manage tasks that previously required distinct expertise and\nmanual coordination. To this end, we introduce AssetOpsBench -- a unified\nframework and environment designed to guide the development, orchestration, and\nevaluation of domain-specific agents tailored for Industry 4.0 applications. We\noutline the key requirements for such holistic systems and provide actionable\ninsights into building agents that integrate perception, reasoning, and control\nfor real-world industrial operations. The software is available at\nhttps://github.com/IBM/AssetOpsBench.",
            "upvotes": 0,
            "discussionId": "68473c983ec10bdd8ab4dcdb",
            "projectPage": "https://github.com/IBM/AssetOpsBench",
            "ai_summary": "A unified framework, AssetOpsBench, is introduced to enable end-to-end automation of industrial asset lifecycle management through domain-specific AI agents.",
            "ai_keywords": [
                "AI agents",
                "large language models",
                "LLMs",
                "end-to-end automation",
                "AssetOpsBench",
                "Industry 4.0",
                "perception",
                "reasoning",
                "control"
            ]
        },
        "publishedAt": "2025-06-04T06:57:35.000Z",
        "title": "AssetOpsBench: Benchmarking AI Agents for Task Automation in Industrial\n  Asset Operations and Maintenance",
        "summary": "AI for Industrial Asset Lifecycle Management aims to automate complex\noperational workflows -- such as condition monitoring, maintenance planning,\nand intervention scheduling -- to reduce human workload and minimize system\ndowntime. Traditional AI/ML approaches have primarily tackled these problems in\nisolation, solving narrow tasks within the broader operational pipeline. In\ncontrast, the emergence of AI agents and large language models (LLMs)\nintroduces a next-generation opportunity: enabling end-to-end automation across\nthe entire asset lifecycle. This paper envisions a future where AI agents\nautonomously manage tasks that previously required distinct expertise and\nmanual coordination. To this end, we introduce AssetOpsBench -- a unified\nframework and environment designed to guide the development, orchestration, and\nevaluation of domain-specific agents tailored for Industry 4.0 applications. We\noutline the key requirements for such holistic systems and provide actionable\ninsights into building agents that integrate perception, reasoning, and control\nfor real-world industrial operations. The software is available at\nhttps://github.com/IBM/AssetOpsBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03828.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c47f731d44fc06afc80953",
            "avatarUrl": "/avatars/ece59a98ae932fea48d269de6d4fb485.svg",
            "fullname": "Dhaval",
            "name": "DhavalPatel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.02327",
            "authors": [
                {
                    "_id": "68476fdd3ec10bdd8ab4dd64",
                    "name": "Yijun Yang",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd65",
                    "name": "Zhao-Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd66",
                    "name": "Qiuping Liu",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd67",
                    "name": "Shuwen Sun",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd68",
                    "name": "Kang Wang",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd69",
                    "name": "Rama Chellappa",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd6a",
                    "name": "Zongwei Zhou",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd6b",
                    "name": "Alan Yuille",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd6c",
                    "name": "Lei Zhu",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd6d",
                    "name": "Yu-Dong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68476fdd3ec10bdd8ab4dd6e",
                    "name": "Jieneng Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/673c3d68a90d746ff6b8ba64/hdFtT3FU4BFmdr9pRQhmy.jpeg"
            ],
            "publishedAt": "2025-06-02T23:50:40.000Z",
            "submittedOnDailyAt": "2025-06-09T23:23:14.005Z",
            "title": "Medical World Model: Generative Simulation of Tumor Evolution for\n  Treatment Planning",
            "submittedOnDailyBy": {
                "_id": "673c3d68a90d746ff6b8ba64",
                "avatarUrl": "/avatars/9fcd3f915e6cbacc304bd1f54d0223a2.svg",
                "isPro": false,
                "fullname": "Yijun Yang",
                "user": "scott-yjyang",
                "type": "user"
            },
            "summary": "Providing effective treatment and making informed clinical decisions are\nessential goals of modern medicine and clinical care. We are interested in\nsimulating disease dynamics for clinical decision-making, leveraging recent\nadvances in large generative models. To this end, we introduce the Medical\nWorld Model (MeWM), the first world model in medicine that visually predicts\nfuture disease states based on clinical decisions. MeWM comprises (i)\nvision-language models to serve as policy models, and (ii) tumor generative\nmodels as dynamics models. The policy model generates action plans, such as\nclinical treatments, while the dynamics model simulates tumor progression or\nregression under given treatment conditions. Building on this, we propose the\ninverse dynamics model that applies survival analysis to the simulated\npost-treatment tumor, enabling the evaluation of treatment efficacy and the\nselection of the optimal clinical action plan. As a result, the proposed MeWM\nsimulates disease dynamics by synthesizing post-treatment tumors, with\nstate-of-the-art specificity in Turing tests evaluated by radiologists.\nSimultaneously, its inverse dynamics model outperforms medical-specialized GPTs\nin optimizing individualized treatment protocols across all metrics. Notably,\nMeWM improves clinical decision-making for interventional physicians, boosting\nF1-score in selecting the optimal TACE protocol by 13%, paving the way for\nfuture integration of medical world models as the second readers.",
            "upvotes": 0,
            "discussionId": "68476fde3ec10bdd8ab4dd6f",
            "projectPage": "https://yijun-yang.github.io/MeWM/",
            "githubRepo": "https://github.com/scott-yjyang/MeWM",
            "ai_summary": "MeWM, a medical world model incorporating vision-language and tumor generative models, simulates disease dynamics and optimizes clinical decision-making with state-of-the-art specificity and efficacy.",
            "ai_keywords": [
                "world model",
                "vision-language models",
                "policy models",
                "tumor generative models",
                "dynamics models",
                "inverse dynamics model",
                "survival analysis",
                "Turing tests",
                "medical-specialized GPTs",
                "F1-score",
                "TACE protocol"
            ]
        },
        "publishedAt": "2025-06-02T19:50:40.000Z",
        "title": "Medical World Model: Generative Simulation of Tumor Evolution for\n  Treatment Planning",
        "summary": "Providing effective treatment and making informed clinical decisions are\nessential goals of modern medicine and clinical care. We are interested in\nsimulating disease dynamics for clinical decision-making, leveraging recent\nadvances in large generative models. To this end, we introduce the Medical\nWorld Model (MeWM), the first world model in medicine that visually predicts\nfuture disease states based on clinical decisions. MeWM comprises (i)\nvision-language models to serve as policy models, and (ii) tumor generative\nmodels as dynamics models. The policy model generates action plans, such as\nclinical treatments, while the dynamics model simulates tumor progression or\nregression under given treatment conditions. Building on this, we propose the\ninverse dynamics model that applies survival analysis to the simulated\npost-treatment tumor, enabling the evaluation of treatment efficacy and the\nselection of the optimal clinical action plan. As a result, the proposed MeWM\nsimulates disease dynamics by synthesizing post-treatment tumors, with\nstate-of-the-art specificity in Turing tests evaluated by radiologists.\nSimultaneously, its inverse dynamics model outperforms medical-specialized GPTs\nin optimizing individualized treatment protocols across all metrics. Notably,\nMeWM improves clinical decision-making for interventional physicians, boosting\nF1-score in selecting the optimal TACE protocol by 13%, paving the way for\nfuture integration of medical world models as the second readers.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/673c3d68a90d746ff6b8ba64/hdFtT3FU4BFmdr9pRQhmy.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.02327.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "673c3d68a90d746ff6b8ba64",
            "avatarUrl": "/avatars/9fcd3f915e6cbacc304bd1f54d0223a2.svg",
            "fullname": "Yijun Yang",
            "name": "scott-yjyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.20698",
            "authors": [
                {
                    "_id": "6847486d3ec10bdd8ab4dd07",
                    "name": "Woomin Song",
                    "hidden": false
                },
                {
                    "_id": "6847486d3ec10bdd8ab4dd08",
                    "name": "Jihoon Tack",
                    "hidden": false
                },
                {
                    "_id": "6847486d3ec10bdd8ab4dd09",
                    "name": "Sangwoo Mo",
                    "hidden": false
                },
                {
                    "_id": "6847486d3ec10bdd8ab4dd0a",
                    "name": "Seunghyuk Oh",
                    "hidden": false
                },
                {
                    "_id": "6847486d3ec10bdd8ab4dd0b",
                    "name": "Jinwoo Shin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T04:07:23.000Z",
            "submittedOnDailyAt": "2025-06-09T19:20:59.530Z",
            "title": "Sparsified State-Space Models are Efficient Highway Networks",
            "submittedOnDailyBy": {
                "_id": "64b77883be30d6567a774491",
                "avatarUrl": "/avatars/0e7718dbf93d670da6e7e3709a8074e5.svg",
                "isPro": false,
                "fullname": "Woomin Song",
                "user": "totolacky",
                "type": "user"
            },
            "summary": "State-space models (SSMs) offer a promising architecture for sequence\nmodeling, providing an alternative to Transformers by replacing expensive\nself-attention with linear recurrences. In this paper, we propose a simple yet\neffective trick to enhance SSMs within given computational budgets by\nsparsifying them. Our intuition is that tokens in SSMs are highly redundant due\nto gradual recurrent updates, and dense recurrence operations block the\ndelivery of past information. In particular, we observe that upper layers of\nSSMs tend to be more redundant as they encode global information, while lower\nlayers encode local information. Motivated by this, we introduce Simba, a\nhierarchical sparsification method for SSMs based on token pruning. Simba\nsparsifies upper layers more than lower layers, encouraging the upper layers to\nbehave like highways. To achieve this, we propose a novel token pruning\ncriterion for SSMs, measuring the global impact of tokens on the final output\nby accumulating local recurrences. We demonstrate that Simba outperforms the\nbaseline model, Mamba, with the same FLOPS in various natural language tasks.\nMoreover, we illustrate the effect of highways, showing that Simba not only\nenhances efficiency but also improves the information flow across long\nsequences. Code is available at https://github.com/woominsong/Simba.",
            "upvotes": 0,
            "discussionId": "6847486d3ec10bdd8ab4dd0c",
            "ai_summary": "Simba, a hierarchical sparsification method for state-space models, enhances efficiency and information flow in natural language tasks by pruning tokens more aggressively in upper layers.",
            "ai_keywords": [
                "state-space models",
                "sequence modeling",
                "self-attention",
                "linear recurrences",
                "sparsification",
                "token pruning",
                "hierarchical sparsification",
                "Simba",
                "global information",
                "local information",
                "highways",
                "FLOPS",
                "natural language tasks"
            ]
        },
        "publishedAt": "2025-05-27T00:07:23.000Z",
        "title": "Sparsified State-Space Models are Efficient Highway Networks",
        "summary": "State-space models (SSMs) offer a promising architecture for sequence\nmodeling, providing an alternative to Transformers by replacing expensive\nself-attention with linear recurrences. In this paper, we propose a simple yet\neffective trick to enhance SSMs within given computational budgets by\nsparsifying them. Our intuition is that tokens in SSMs are highly redundant due\nto gradual recurrent updates, and dense recurrence operations block the\ndelivery of past information. In particular, we observe that upper layers of\nSSMs tend to be more redundant as they encode global information, while lower\nlayers encode local information. Motivated by this, we introduce Simba, a\nhierarchical sparsification method for SSMs based on token pruning. Simba\nsparsifies upper layers more than lower layers, encouraging the upper layers to\nbehave like highways. To achieve this, we propose a novel token pruning\ncriterion for SSMs, measuring the global impact of tokens on the final output\nby accumulating local recurrences. We demonstrate that Simba outperforms the\nbaseline model, Mamba, with the same FLOPS in various natural language tasks.\nMoreover, we illustrate the effect of highways, showing that Simba not only\nenhances efficiency but also improves the information flow across long\nsequences. Code is available at https://github.com/woominsong/Simba.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.20698.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b77883be30d6567a774491",
            "avatarUrl": "/avatars/0e7718dbf93d670da6e7e3709a8074e5.svg",
            "fullname": "Woomin Song",
            "name": "totolacky",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]