[
    {
        "paper": {
            "id": "2506.06751",
            "authors": [
                {
                    "_id": "6848fecf42e4f9106973f315",
                    "user": {
                        "_id": "62bd6c6baaf1480f1aa2222e",
                        "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
                        "isPro": false,
                        "fullname": "Mikhail Salnikov",
                        "user": "msalnikov",
                        "type": "user"
                    },
                    "name": "Mikhail Salnikov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:34:47.630Z",
                    "hidden": false
                },
                {
                    "_id": "6848fecf42e4f9106973f316",
                    "name": "Dmitrii Korzh",
                    "hidden": false
                },
                {
                    "_id": "6848fecf42e4f9106973f317",
                    "user": {
                        "_id": "657c4a8dfb0285d857d86e4c",
                        "avatarUrl": "/avatars/17635a4c2c804dd3837ae01833bb940d.svg",
                        "isPro": false,
                        "fullname": "Ivan",
                        "user": "IvanLazichny",
                        "type": "user"
                    },
                    "name": "Ivan Lazichny",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-11T09:26:44.876Z",
                    "hidden": false
                },
                {
                    "_id": "6848fecf42e4f9106973f318",
                    "name": "Elvir Karimov",
                    "hidden": false
                },
                {
                    "_id": "6848fecf42e4f9106973f319",
                    "name": "Artyom Iudin",
                    "hidden": false
                },
                {
                    "_id": "6848fecf42e4f9106973f31a",
                    "name": "Ivan Oseledets",
                    "hidden": false
                },
                {
                    "_id": "6848fecf42e4f9106973f31b",
                    "name": "Oleg Y. Rogov",
                    "hidden": false
                },
                {
                    "_id": "6848fecf42e4f9106973f31c",
                    "user": {
                        "_id": "605473729d7c1d4d81b7e52b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg",
                        "isPro": false,
                        "fullname": "Alexander Panchenko",
                        "user": "apanc",
                        "type": "user"
                    },
                    "name": "Alexander Panchenko",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-11T09:27:06.218Z",
                    "hidden": false
                },
                {
                    "_id": "6848fecf42e4f9106973f31d",
                    "name": "Natalia Loukachevitch",
                    "hidden": false
                },
                {
                    "_id": "6848fecf42e4f9106973f31e",
                    "user": {
                        "_id": "662f8d645c4db70c77a203b0",
                        "avatarUrl": "/avatars/72f9a3c39b3ba5114388d16a35524835.svg",
                        "isPro": false,
                        "fullname": "Elena Tutubalina",
                        "user": "tlenusik",
                        "type": "user"
                    },
                    "name": "Elena Tutubalina",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-11T09:26:55.840Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-07T10:45:17.000Z",
            "submittedOnDailyAt": "2025-06-11T02:53:12.616Z",
            "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
            "submittedOnDailyBy": {
                "_id": "62bd6c6baaf1480f1aa2222e",
                "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
                "isPro": false,
                "fullname": "Mikhail Salnikov",
                "user": "msalnikov",
                "type": "user"
            },
            "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
            "upvotes": 53,
            "discussionId": "6848fed042e4f9106973f31f",
            "projectPage": "https://airi-institute.github.io/geopolitical_llm_bias",
            "githubRepo": "https://github.com/AIRI-Institute/geopolitical_llm_bias",
            "ai_summary": "LLMs exhibit significant geopolitical biases in their interpretation of historical events, and simple debiasing methods have limited effectiveness; a novel dataset for further research is provided.",
            "ai_keywords": [
                "LLMs",
                "geopolitical biases",
                "historical events",
                "national narratives",
                "debiasing prompts"
            ]
        },
        "publishedAt": "2025-06-07T06:45:17.000Z",
        "title": "Geopolitical biases in LLMs: what are the \"good\" and the \"bad\" countries\n  according to contemporary language models",
        "summary": "This paper evaluates geopolitical biases in LLMs with respect to various\ncountries though an analysis of their interpretation of historical events with\nconflicting national perspectives (USA, UK, USSR, and China). We introduce a\nnovel dataset with neutral event descriptions and contrasting viewpoints from\ndifferent countries. Our findings show significant geopolitical biases, with\nmodels favoring specific national narratives. Additionally, simple debiasing\nprompts had a limited effect in reducing these biases. Experiments with\nmanipulated participant labels reveal models' sensitivity to attribution,\nsometimes amplifying biases or recognizing inconsistencies, especially with\nswapped labels. This work highlights national narrative biases in LLMs,\nchallenges the effectiveness of simple debiasing methods, and offers a\nframework and dataset for future geopolitical bias research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06751.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62bd6c6baaf1480f1aa2222e",
            "avatarUrl": "/avatars/fd92ae2986d435a47eb1e382ac11d8e0.svg",
            "fullname": "Mikhail Salnikov",
            "name": "msalnikov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09040",
            "authors": [
                {
                    "_id": "6848fff842e4f9106973f321",
                    "name": "Dianyi Wang",
                    "hidden": false
                },
                {
                    "_id": "6848fff842e4f9106973f322",
                    "user": {
                        "_id": "665eccf5ffd59344a22533a8",
                        "avatarUrl": "/avatars/2ae2710753ce34a04937384bc6dddf70.svg",
                        "isPro": false,
                        "fullname": "Wei Song",
                        "user": "Songweii",
                        "type": "user"
                    },
                    "name": "Wei Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T20:21:19.066Z",
                    "hidden": false
                },
                {
                    "_id": "6848fff842e4f9106973f323",
                    "user": {
                        "_id": "627b73728b6ecd7ece822825",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/627b73728b6ecd7ece822825/QV-sT0vwupGZYg-loLPRw.jpeg",
                        "isPro": false,
                        "fullname": "Yikun Wang",
                        "user": "LibraTree",
                        "type": "user"
                    },
                    "name": "Yikun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T16:02:20.996Z",
                    "hidden": false
                },
                {
                    "_id": "6848fff842e4f9106973f324",
                    "name": "Siyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6848fff842e4f9106973f325",
                    "name": "Kaicheng Yu",
                    "hidden": false
                },
                {
                    "_id": "6848fff842e4f9106973f326",
                    "name": "Zhongyu Wei",
                    "hidden": false
                },
                {
                    "_id": "6848fff842e4f9106973f327",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:57:50.000Z",
            "submittedOnDailyAt": "2025-06-11T05:55:58.221Z",
            "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
            "submittedOnDailyBy": {
                "_id": "64b4eec4faa3181a5eab9c46",
                "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                "isPro": true,
                "fullname": "Jiaqi Wang",
                "user": "myownskyW7",
                "type": "user"
            },
            "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.",
            "upvotes": 26,
            "discussionId": "6848fff842e4f9106973f328",
            "ai_summary": "Autoregressive Semantic Visual Reconstruction (ASVR) improves multimodal understanding by focusing on semantic reconstruction rather than raw visual appearance, enhancing performance across various benchmarks.",
            "ai_keywords": [
                "autoregressive supervision",
                "large vision-language models (LVLMs)",
                "visual modality",
                "image captions",
                "autoregressive image generation",
                "multimodal learning",
                "semantic representation",
                "discrete semantic tokens",
                "multimodal understanding benchmarks",
                "LLaVA-1.5"
            ]
        },
        "publishedAt": "2025-06-10T13:57:50.000Z",
        "title": "Autoregressive Semantic Visual Reconstruction Helps VLMs Understand\n  Better",
        "summary": "Typical large vision-language models (LVLMs) apply autoregressive supervision\nsolely to textual sequences, without fully incorporating the visual modality\ninto the learning process. This results in three key limitations: (1) an\ninability to utilize images without accompanying captions, (2) the risk that\ncaptions omit critical visual details, and (3) the challenge that certain\nvision-centric content cannot be adequately conveyed through text. As a result,\ncurrent LVLMs often prioritize vision-to-language alignment while potentially\noverlooking fine-grained visual information. While some prior works have\nexplored autoregressive image generation, effectively leveraging autoregressive\nvisual supervision to enhance image understanding remains an open challenge. In\nthis paper, we introduce Autoregressive Semantic Visual Reconstruction (ASVR),\nwhich enables joint learning of visual and textual modalities within a unified\nautoregressive framework. We show that autoregressively reconstructing the raw\nvisual appearance of images does not enhance and may even impair multimodal\nunderstanding. In contrast, autoregressively reconstructing the semantic\nrepresentation of images consistently improves comprehension. Notably, we find\nthat even when models are given continuous image features as input, they can\neffectively reconstruct discrete semantic tokens, resulting in stable and\nconsistent improvements across a wide range of multimodal understanding\nbenchmarks. Our approach delivers significant performance gains across varying\ndata scales (556k-2M) and types of LLM bacbones. Specifically, ASVR improves\nLLaVA-1.5 by 5% in average scores across 14 multimodal benchmarks. The code is\navailable at https://github.com/AlenjandroWang/ASVR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09040.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b4eec4faa3181a5eab9c46",
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "fullname": "Jiaqi Wang",
            "name": "myownskyW7",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.08672",
            "authors": [
                {
                    "_id": "684936e842e4f9106973f45e",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "684936e842e4f9106973f45f",
                    "name": "Jiaqi Li",
                    "hidden": false
                },
                {
                    "_id": "684936e842e4f9106973f460",
                    "user": {
                        "_id": "63a95a6a7930fa8c7dd63d4e",
                        "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
                        "isPro": false,
                        "fullname": "Zilong Zheng",
                        "user": "zlzheng",
                        "type": "user"
                    },
                    "name": "Zilong Zheng",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-11T07:57:29.119Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T10:31:21.000Z",
            "submittedOnDailyAt": "2025-06-11T07:49:21.042Z",
            "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling",
            "submittedOnDailyBy": {
                "_id": "63a95a6a7930fa8c7dd63d4e",
                "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
                "isPro": false,
                "fullname": "Zilong Zheng",
                "user": "zlzheng",
                "type": "user"
            },
            "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1%\naverage points on eight ID tasks and Delta10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.",
            "upvotes": 25,
            "discussionId": "684936e842e4f9106973f461",
            "githubRepo": "https://github.com/bigai-nlco/RuleReasoner",
            "ai_summary": "RuleReasoner enhances rule-based reasoning in small models through dynamic domain sampling, achieving superior performance and efficiency compared to large models.",
            "ai_keywords": [
                "reinforcement learning",
                "rule-based reasoning",
                "large reasoning models",
                "small reasoning models",
                "domain-aware dynamic sampling",
                "historical rewards",
                "in-distribution",
                "out-of-distribution",
                "computational efficiency"
            ]
        },
        "publishedAt": "2025-06-10T06:31:21.000Z",
        "title": "RuleReasoner: Reinforced Rule-based Reasoning via Domain-aware Dynamic\n  Sampling",
        "summary": "Rule-based reasoning has been acknowledged as one of the fundamental problems\nin reasoning, while deviations in rule formats, types, and complexity in\nreal-world applications pose severe challenges. Recent studies have shown that\nlarge reasoning models (LRMs) have remarkable reasoning capabilities, and their\nperformance is substantially enhanced by reinforcement learning (RL). However,\nit remains an open question whether small reasoning models (SRMs) can learn\nrule-based reasoning effectively with robust generalization across diverse\ntasks and domains. To address this, we introduce Reinforced Rule-based\nReasoning, a.k.a. RuleReasoner, a simple yet effective method to conduct\nrule-based reasoning via a wide collection of curated tasks and a novel\ndomain-aware dynamic sampling approach. Specifically, RuleReasoner resamples\neach training batch by updating the sampling weights of different domains based\non historical rewards. This facilitates domain augmentation and flexible online\nlearning schedules for RL, obviating the need for pre-hoc human-engineered\nmix-training recipes used in existing methods. Empirical evaluations on\nin-distribution (ID) and out-of-distribution (OOD) benchmarks reveal that\nRuleReasoner outperforms frontier LRMs by a significant margin (Delta4.1%\naverage points on eight ID tasks and Delta10.4% average points on three OOD\ntasks over OpenAI-o1). Notably, our approach also exhibits higher computational\nefficiency compared to prior dynamic sampling methods for RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08672.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a95a6a7930fa8c7dd63d4e",
            "avatarUrl": "/avatars/d9d0420f7ddfe2f3a7e029fb05f1c89f.svg",
            "fullname": "Zilong Zheng",
            "name": "zlzheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.07927",
            "authors": [
                {
                    "_id": "684794003ec10bdd8ab4de11",
                    "name": "Jiayi Sheng",
                    "hidden": false
                },
                {
                    "_id": "684794003ec10bdd8ab4de12",
                    "name": "Luna Lyu",
                    "hidden": false
                },
                {
                    "_id": "684794003ec10bdd8ab4de13",
                    "name": "Jikai Jin",
                    "hidden": false
                },
                {
                    "_id": "684794003ec10bdd8ab4de14",
                    "name": "Tony Xia",
                    "hidden": false
                },
                {
                    "_id": "684794003ec10bdd8ab4de15",
                    "name": "Alex Gu",
                    "hidden": false
                },
                {
                    "_id": "684794003ec10bdd8ab4de16",
                    "name": "James Zou",
                    "hidden": false
                },
                {
                    "_id": "684794003ec10bdd8ab4de17",
                    "name": "Pan Lu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
            ],
            "publishedAt": "2025-06-09T16:43:38.000Z",
            "submittedOnDailyAt": "2025-06-11T04:15:25.994Z",
            "title": "Solving Inequality Proofs with Large Language Models",
            "submittedOnDailyBy": {
                "_id": "60f5f68fa7fd83d025749234",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
                "isPro": true,
                "fullname": "Pan Lu",
                "user": "lupantech",
                "type": "user"
            },
            "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
            "upvotes": 14,
            "discussionId": "684794013ec10bdd8ab4de18",
            "projectPage": "https://ineqmath.github.io/",
            "githubRepo": "https://github.com/lupantech/ineqmath",
            "ai_summary": "The investigation into inequality proving using large language models uncovers significant challenges in constructing rigorous proofs, revealing gaps between finding answers and generating valid step-wise solutions.",
            "ai_keywords": [
                "LLMs",
                "IneqMath",
                "bound estimation",
                "relation prediction",
                "theorem-guided reasoning",
                "self-refinement"
            ]
        },
        "publishedAt": "2025-06-09T12:43:38.000Z",
        "title": "Solving Inequality Proofs with Large Language Models",
        "summary": "Inequality proving, crucial across diverse scientific and mathematical\nfields, tests advanced reasoning skills such as discovering tight bounds and\nstrategic theorem application. This makes it a distinct, demanding frontier for\nlarge language models (LLMs), offering insights beyond general mathematical\nproblem-solving. Progress in this area is hampered by existing datasets that\nare often scarce, synthetic, or rigidly formal. We address this by proposing an\ninformal yet verifiable task formulation, recasting inequality proving into two\nautomatically checkable subtasks: bound estimation and relation prediction.\nBuilding on this, we release IneqMath, an expert-curated dataset of\nOlympiad-level inequalities, including a test set and training corpus enriched\nwith step-wise solutions and theorem annotations. We also develop a novel\nLLM-as-judge evaluation framework, combining a final-answer judge with four\nstep-wise judges designed to detect common reasoning flaws. A systematic\nevaluation of 29 leading LLMs on IneqMath reveals a surprising reality: even\ntop models like o1 achieve less than 10% overall accuracy under step-wise\nscrutiny; this is a drop of up to 65.5% from their accuracy considering only\nfinal answer equivalence. This discrepancy exposes fragile deductive chains and\na critical gap for current LLMs between merely finding an answer and\nconstructing a rigorous proof. Scaling model size and increasing test-time\ncomputation yield limited gains in overall proof correctness. Instead, our\nfindings highlight promising research directions such as theorem-guided\nreasoning and self-refinement. Code and data are available at\nhttps://ineqmath.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60f5f68fa7fd83d025749234/ahvR-ZmwDrUNm3-jcQ4o1.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07927.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f5f68fa7fd83d025749234",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f5f68fa7fd83d025749234/gCeJAZfzaANAcEvI6v5-P.jpeg",
            "fullname": "Pan Lu",
            "name": "lupantech",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.08009",
            "authors": [
                {
                    "_id": "68485e5b4fe3b60e21b258bd",
                    "name": "Xun Huang",
                    "hidden": false
                },
                {
                    "_id": "68485e5b4fe3b60e21b258be",
                    "name": "Zhengqi Li",
                    "hidden": false
                },
                {
                    "_id": "68485e5b4fe3b60e21b258bf",
                    "user": {
                        "_id": "67492ee82ad3cfc108a41bbb",
                        "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
                        "isPro": false,
                        "fullname": "Guande He",
                        "user": "gdhe17",
                        "type": "user"
                    },
                    "name": "Guande He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:35:40.950Z",
                    "hidden": false
                },
                {
                    "_id": "68485e5b4fe3b60e21b258c0",
                    "name": "Mingyuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68485e5b4fe3b60e21b258c1",
                    "name": "Eli Shechtman",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
            ],
            "publishedAt": "2025-06-09T17:59:55.000Z",
            "submittedOnDailyAt": "2025-06-11T04:34:32.742Z",
            "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
            "submittedOnDailyBy": {
                "_id": "67492ee82ad3cfc108a41bbb",
                "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
                "isPro": false,
                "fullname": "Guande He",
                "user": "gdhe17",
                "type": "user"
            },
            "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
            "upvotes": 13,
            "discussionId": "68485e5b4fe3b60e21b258c2",
            "projectPage": "https://self-forcing.github.io/",
            "githubRepo": "https://github.com/guandeh17/Self-Forcing",
            "ai_summary": "Self Forcing, a novel training method for autoregressive video diffusion models, reduces exposure bias and improves generation quality through holistic video-level supervision and efficient caching mechanisms.",
            "ai_keywords": [
                "Self Forcing",
                "autoregressive video diffusion models",
                "exposure bias",
                "denoising",
                "key-value (KV) caching",
                "autoregressive rollout",
                "holistic loss",
                "few-step diffusion model",
                "stochastic gradient truncation",
                "rolling KV cache mechanism",
                "video extrapolation"
            ]
        },
        "publishedAt": "2025-06-09T13:59:55.000Z",
        "title": "Self Forcing: Bridging the Train-Test Gap in Autoregressive Video\n  Diffusion",
        "summary": "We introduce Self Forcing, a novel training paradigm for autoregressive video\ndiffusion models. It addresses the longstanding issue of exposure bias, where\nmodels trained on ground-truth context must generate sequences conditioned on\ntheir own imperfect outputs during inference. Unlike prior methods that denoise\nfuture frames based on ground-truth context frames, Self Forcing conditions\neach frame's generation on previously self-generated outputs by performing\nautoregressive rollout with key-value (KV) caching during training. This\nstrategy enables supervision through a holistic loss at the video level that\ndirectly evaluates the quality of the entire generated sequence, rather than\nrelying solely on traditional frame-wise objectives. To ensure training\nefficiency, we employ a few-step diffusion model along with a stochastic\ngradient truncation strategy, effectively balancing computational cost and\nperformance. We further introduce a rolling KV cache mechanism that enables\nefficient autoregressive video extrapolation. Extensive experiments demonstrate\nthat our approach achieves real-time streaming video generation with sub-second\nlatency on a single GPU, while matching or even surpassing the generation\nquality of significantly slower and non-causal diffusion models. Project\nwebsite: http://self-forcing.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67492ee82ad3cfc108a41bbb/bEQLc--MCz7a-4ZBIBbaJ.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08009.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67492ee82ad3cfc108a41bbb",
            "avatarUrl": "/avatars/7ad03e55a8791c62f1271a5c9bf8cc60.svg",
            "fullname": "Guande He",
            "name": "gdhe17",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08002",
            "authors": [
                {
                    "_id": "6848f8c242e4f9106973f2f6",
                    "name": "Aadarsh Sahoo",
                    "hidden": false
                },
                {
                    "_id": "6848f8c242e4f9106973f2f7",
                    "name": "Vansh Tibrewal",
                    "hidden": false
                },
                {
                    "_id": "6848f8c242e4f9106973f2f8",
                    "name": "Georgia Gkioxari",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
                "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
                "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
            ],
            "publishedAt": "2025-06-09T17:59:37.000Z",
            "submittedOnDailyAt": "2025-06-11T02:05:41.608Z",
            "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
            "submittedOnDailyBy": {
                "_id": "638e5fc6485360fbdfeb1301",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
                "isPro": false,
                "fullname": "Aadarsh Sahoo",
                "user": "aadarsh99",
                "type": "user"
            },
            "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/",
            "upvotes": 13,
            "discussionId": "6848f8c242e4f9106973f2f9",
            "projectPage": "https://glab-caltech.github.io/kyvo/",
            "githubRepo": "https://github.com/AadSah/kyvo",
            "ai_summary": "A unified language, image, and 3D scene model framework is proposed, achieving optimal training and performance across various 3D tasks and datasets.",
            "ai_keywords": [
                "autoregressive models",
                "LLM framework",
                "data representation",
                "modality-specific objectives",
                "3D rendering",
                "3D recognition",
                "instruction-following",
                "question-answering",
                "3D datasets",
                "quantized shape encodings"
            ]
        },
        "publishedAt": "2025-06-09T13:59:37.000Z",
        "title": "Aligning Text, Images, and 3D Structure Token-by-Token",
        "summary": "Creating machines capable of understanding the world in 3D is essential in\nassisting designers that build and edit 3D environments and robots navigating\nand interacting within a three-dimensional space. Inspired by advances in\nlanguage and image modeling, we investigate the potential of autoregressive\nmodels for a new modality: structured 3D scenes. To this end, we propose a\nunified LLM framework that aligns language, images, and 3D scenes and provide a\ndetailed ''cookbook'' outlining critical design choices for achieving optimal\ntraining and performance addressing key questions related to data\nrepresentation, modality-specific objectives, and more. We evaluate performance\nacross four core 3D tasks -- rendering, recognition, instruction-following, and\nquestion-answering -- and four 3D datasets, synthetic and real-world. We extend\nour approach to reconstruct complex 3D object shapes by enriching our 3D\nmodality with quantized shape encodings, and show our model's effectiveness on\nreal-world 3D object recognition tasks. Project webpage:\nhttps://glab-caltech.github.io/kyvo/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/2iX6eNaXCKBiwTpAmSk2Z.qt",
            "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/EUswqANZ-bRURwRZ0sv3m.qt",
            "https://cdn-uploads.huggingface.co/production/uploads/638e5fc6485360fbdfeb1301/dLiQzQHFyFye4cIgj3ivo.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08002.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638e5fc6485360fbdfeb1301",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638e5fc6485360fbdfeb1301/_8oqLMbkn_Ig-Jqa0fZCJ.png",
            "fullname": "Aadarsh Sahoo",
            "name": "aadarsh99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.04614",
            "authors": [
                {
                    "_id": "684921e342e4f9106973f3e7",
                    "name": "Yuyang Wanyan",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3e8",
                    "name": "Xi Zhang",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3e9",
                    "user": {
                        "_id": "645b10e80c73ea27d13f7aca",
                        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
                        "isPro": false,
                        "fullname": "xuhaiyang",
                        "user": "xhyandwyy",
                        "type": "user"
                    },
                    "name": "Haiyang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T16:02:16.976Z",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3ea",
                    "name": "Haowei Liu",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3eb",
                    "name": "Junyang Wang",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3ec",
                    "name": "Jiabo Ye",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3ed",
                    "name": "Yutong Kou",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3ee",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3ef",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3f0",
                    "name": "Xiaoshan Yang",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3f1",
                    "name": "Weiming Dong",
                    "hidden": false
                },
                {
                    "_id": "684921e342e4f9106973f3f2",
                    "name": "Changsheng Xu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
            ],
            "publishedAt": "2025-06-05T04:12:36.000Z",
            "submittedOnDailyAt": "2025-06-11T04:59:22.770Z",
            "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
            "submittedOnDailyBy": {
                "_id": "645b10e80c73ea27d13f7aca",
                "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
                "isPro": false,
                "fullname": "xuhaiyang",
                "user": "xhyandwyy",
                "type": "user"
            },
            "summary": "In recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.",
            "upvotes": 13,
            "discussionId": "684921e442e4f9106973f3f3",
            "githubRepo": "https://github.com/X-PLUG/MobileAgent",
            "ai_summary": "A pre-operative critic mechanism with Suggestion-aware Gradient Relative Policy Optimization enhances the reliability of multimodal reasoning tasks in GUI automation.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Suggestion-aware Gradient Relative Policy Optimization",
                "pre-operative critic mechanism",
                "reasoning-bootstrapping",
                "GUI automation",
                "GUI-Critic-R1",
                "GUI-Critic-Test",
                "GUI-Critic-Train"
            ]
        },
        "publishedAt": "2025-06-05T00:12:36.000Z",
        "title": "Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error\n  Diagnosis in GUI Automation",
        "summary": "In recent years, Multimodal Large Language Models (MLLMs) have been\nextensively utilized for multimodal reasoning tasks, including Graphical User\nInterface (GUI) automation. Unlike general offline multimodal tasks, GUI\nautomation is executed in online interactive environments, necessitating\nstep-by-step decision-making based on real-time status of the environment. This\ntask has a lower tolerance for decision-making errors at each step, as any\nmistakes may cumulatively disrupt the process and potentially lead to\nirreversible outcomes like deletions or payments. To address these issues, we\nintroduce a pre-operative critic mechanism that provides effective feedback\nprior to the actual execution, by reasoning about the potential outcome and\ncorrectness of actions. Specifically, we propose a Suggestion-aware Gradient\nRelative Policy Optimization (S-GRPO) strategy to construct our pre-operative\ncritic model GUI-Critic-R1, incorporating a novel suggestion reward to enhance\nthe reliability of the model's feedback. Furthermore, we develop a\nreasoning-bootstrapping based data collection pipeline to create a\nGUI-Critic-Train and a GUI-Critic-Test, filling existing gaps in GUI critic\ndata. Static experiments on the GUI-Critic-Test across both mobile and web\ndomains reveal that our GUI-Critic-R1 offers significant advantages in critic\naccuracy compared to current MLLMs. Dynamic evaluation on GUI automation\nbenchmark further highlights the effectiveness and superiority of our model, as\nevidenced by improved success rates and operational efficiency.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/u6BK1EJr5-c6EUSfCkYqH.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/ySTNcOWpa_W2ZpbmkS50q.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04614.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "fullname": "xuhaiyang",
            "name": "xhyandwyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.07177",
            "authors": [
                {
                    "_id": "6849036342e4f9106973f32a",
                    "name": "Sangwon Jang",
                    "hidden": false
                },
                {
                    "_id": "6849036342e4f9106973f32b",
                    "user": {
                        "_id": "66b57c77778c98d29446c8ec",
                        "avatarUrl": "/avatars/63a7da38ee3808858f0f786a3a4a8dae.svg",
                        "isPro": false,
                        "fullname": "Taekyung Ki",
                        "user": "tkkitkki",
                        "type": "user"
                    },
                    "name": "Taekyung Ki",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:34:42.228Z",
                    "hidden": false
                },
                {
                    "_id": "6849036342e4f9106973f32c",
                    "name": "Jaehyeong Jo",
                    "hidden": false
                },
                {
                    "_id": "6849036342e4f9106973f32d",
                    "user": {
                        "_id": "652066649004117947e46ed6",
                        "avatarUrl": "/avatars/972c97df6f26d2c3d6ce71ec579984bb.svg",
                        "isPro": false,
                        "fullname": "Jaehong Yoon",
                        "user": "jaehong31",
                        "type": "user"
                    },
                    "name": "Jaehong Yoon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:34:44.146Z",
                    "hidden": false
                },
                {
                    "_id": "6849036342e4f9106973f32e",
                    "name": "Soo Ye Kim",
                    "hidden": false
                },
                {
                    "_id": "6849036342e4f9106973f32f",
                    "name": "Zhe Lin",
                    "hidden": false
                },
                {
                    "_id": "6849036342e4f9106973f330",
                    "name": "Sung Ju Hwang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
            ],
            "publishedAt": "2025-06-08T14:54:41.000Z",
            "submittedOnDailyAt": "2025-06-11T02:49:49.936Z",
            "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "63bbf972d8d676a2299cdb44",
                "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
                "isPro": false,
                "fullname": "Sangwon",
                "user": "agwmon",
                "type": "user"
            },
            "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.",
            "upvotes": 12,
            "discussionId": "6849036342e4f9106973f331",
            "ai_summary": "Frame Guidance offers a training-free method for controlling video generation using frame-level signals, reducing memory usage and enhancing globally coherent video output.",
            "ai_keywords": [
                "diffusion models",
                "frame-level signals",
                "keyframes",
                "style reference images",
                "sketches",
                "depth maps",
                "latent processing",
                "latent optimization",
                "globally coherent video generation",
                "video models",
                "keyframe guidance",
                "stylization",
                "looping"
            ]
        },
        "publishedAt": "2025-06-08T10:54:41.000Z",
        "title": "Frame Guidance: Training-Free Guidance for Frame-Level Control in Video\n  Diffusion Models",
        "summary": "Advancements in diffusion models have significantly improved video quality,\ndirecting attention to fine-grained controllability. However, many existing\nmethods depend on fine-tuning large-scale video models for specific tasks,\nwhich becomes increasingly impractical as model sizes continue to grow. In this\nwork, we present Frame Guidance, a training-free guidance for controllable\nvideo generation based on frame-level signals, such as keyframes, style\nreference images, sketches, or depth maps. For practical training-free\nguidance, we propose a simple latent processing method that dramatically\nreduces memory usage, and apply a novel latent optimization strategy designed\nfor globally coherent video generation. Frame Guidance enables effective\ncontrol across diverse tasks, including keyframe guidance, stylization, and\nlooping, without any training, compatible with any video models. Experimental\nresults show that Frame Guidance can produce high-quality controlled videos for\na wide range of tasks and input signals.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63bbf972d8d676a2299cdb44/n94aSArXRHEapWS5MwzkR.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07177.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bbf972d8d676a2299cdb44",
            "avatarUrl": "/avatars/cd038f11dc1007b1267324b34c165dda.svg",
            "fullname": "Sangwon",
            "name": "agwmon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.08279",
            "authors": [
                {
                    "_id": "6849948548021d7f53998b64",
                    "user": {
                        "_id": "6849ab862603ba46f81d7d73",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/02xo4XQJx-1KxEcmughku.jpeg",
                        "isPro": false,
                        "fullname": "aditi sundararaman",
                        "user": "n0taditi",
                        "type": "user"
                    },
                    "name": "Aditi Sundararaman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T20:21:16.802Z",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b65",
                    "name": "Amogh Adishesha",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b66",
                    "name": "Andrew Jaegle",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b67",
                    "name": "Dan Bigioi",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b68",
                    "user": {
                        "_id": "61f44bab7eba274ea80b74ce",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
                        "isPro": false,
                        "fullname": "Hyoung-Kyu Song",
                        "user": "deepkyu",
                        "type": "user"
                    },
                    "name": "Hyoung-Kyu Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T16:01:41.363Z",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b69",
                    "user": {
                        "_id": "65b848353112ff9a78fdc940",
                        "avatarUrl": "/avatars/609576c2145316e225fe8066a76bde2d.svg",
                        "isPro": false,
                        "fullname": "Jon Kyl",
                        "user": "jon-kyl",
                        "type": "user"
                    },
                    "name": "Jon Kyl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T20:21:15.027Z",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b6a",
                    "name": "Justin Mao",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b6b",
                    "name": "Kevin Lan",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b6c",
                    "name": "Mojtaba Komeili",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b6d",
                    "name": "ShahRukh Athar",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b6e",
                    "name": "Sheila Babayan",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b6f",
                    "name": "Stanislau Beliasau",
                    "hidden": false
                },
                {
                    "_id": "6849948548021d7f53998b70",
                    "name": "William Buchwalter",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/rkNkbzPQXq7ZidAyDoj8f.mp4"
            ],
            "publishedAt": "2025-06-09T22:56:02.000Z",
            "submittedOnDailyAt": "2025-06-11T14:27:13.454Z",
            "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage",
            "submittedOnDailyBy": {
                "_id": "61f44bab7eba274ea80b74ce",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
                "isPro": false,
                "fullname": "Hyoung-Kyu Song",
                "user": "deepkyu",
                "type": "user"
            },
            "summary": "From professional filmmaking to user-generated content, creators and\nconsumers have long recognized that the power of video depends on the\nharmonious integration of what we hear (the video's audio track) with what we\nsee (the video's image sequence). Current approaches to video generation either\nignore sound to focus on general-purpose but silent image sequence generation\nor address both visual and audio elements but focus on restricted application\ndomains such as re-dubbing. We introduce Mirage, an audio-to-video foundation\nmodel that excels at generating realistic, expressive output imagery from\nscratch given an audio input. When integrated with existing methods for speech\nsynthesis (text-to-speech, or TTS), Mirage results in compelling multimodal\nvideo. When trained on audio-video footage of people talking (A-roll) and\nconditioned on audio containing speech, Mirage generates video of people\ndelivering a believable interpretation of the performance implicit in input\naudio. Our central technical contribution is a unified method for training\nself-attention-based audio-to-video generation models, either from scratch or\ngiven existing weights. This methodology allows Mirage to retain generality as\nan approach to audio-to-video generation while producing outputs of superior\nsubjective quality to methods that incorporate audio-specific architectures or\nloss components specific to people, speech, or details of how images or audio\nare captured. We encourage readers to watch and listen to the results of Mirage\nfor themselves (see paper and comments for links).",
            "upvotes": 9,
            "discussionId": "6849948548021d7f53998b71",
            "ai_summary": "Mirage generates realistic video from audio inputs, integrating with speech synthesis to create compelling multimodal content through a unified, self-attention-based training approach.",
            "ai_keywords": [
                "audio-to-video foundation model",
                "self-attention-based audio-to-video generation models",
                "audio-video footage",
                "text-to-speech",
                "TTS",
                "A-roll"
            ]
        },
        "publishedAt": "2025-06-09T18:56:02.000Z",
        "title": "Seeing Voices: Generating A-Roll Video from Audio with Mirage",
        "summary": "From professional filmmaking to user-generated content, creators and\nconsumers have long recognized that the power of video depends on the\nharmonious integration of what we hear (the video's audio track) with what we\nsee (the video's image sequence). Current approaches to video generation either\nignore sound to focus on general-purpose but silent image sequence generation\nor address both visual and audio elements but focus on restricted application\ndomains such as re-dubbing. We introduce Mirage, an audio-to-video foundation\nmodel that excels at generating realistic, expressive output imagery from\nscratch given an audio input. When integrated with existing methods for speech\nsynthesis (text-to-speech, or TTS), Mirage results in compelling multimodal\nvideo. When trained on audio-video footage of people talking (A-roll) and\nconditioned on audio containing speech, Mirage generates video of people\ndelivering a believable interpretation of the performance implicit in input\naudio. Our central technical contribution is a unified method for training\nself-attention-based audio-to-video generation models, either from scratch or\ngiven existing weights. This methodology allows Mirage to retain generality as\nan approach to audio-to-video generation while producing outputs of superior\nsubjective quality to methods that incorporate audio-specific architectures or\nloss components specific to people, speech, or details of how images or audio\nare captured. We encourage readers to watch and listen to the results of Mirage\nfor themselves (see paper and comments for links).",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/61f44bab7eba274ea80b74ce/rkNkbzPQXq7ZidAyDoj8f.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08279.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61f44bab7eba274ea80b74ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61f44bab7eba274ea80b74ce/BRbKX1jephdZ7D44FATl4.jpeg",
            "fullname": "Hyoung-Kyu Song",
            "name": "deepkyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.07045",
            "authors": [
                {
                    "_id": "68494f9542e4f9106973f4c7",
                    "name": "Yikun Ji",
                    "hidden": false
                },
                {
                    "_id": "68494f9542e4f9106973f4c8",
                    "name": "Hong Yan",
                    "hidden": false
                },
                {
                    "_id": "68494f9542e4f9106973f4c9",
                    "name": "Jun Lan",
                    "hidden": false
                },
                {
                    "_id": "68494f9542e4f9106973f4ca",
                    "name": "Huijia Zhu",
                    "hidden": false
                },
                {
                    "_id": "68494f9542e4f9106973f4cb",
                    "name": "Weiqiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68494f9542e4f9106973f4cc",
                    "user": {
                        "_id": "64b8a72952b7353d8c669086",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8a72952b7353d8c669086/3PUTNmx9kd17gtZ9-Yviw.jpeg",
                        "isPro": false,
                        "fullname": "Qi Fan",
                        "user": "fanqiNO1",
                        "type": "user"
                    },
                    "name": "Qi Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T16:02:12.458Z",
                    "hidden": false
                },
                {
                    "_id": "68494f9542e4f9106973f4cd",
                    "name": "Liqing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68494f9542e4f9106973f4ce",
                    "name": "Jianfu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-08T08:47:44.000Z",
            "submittedOnDailyAt": "2025-06-11T11:01:31.694Z",
            "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded\n  Reasoning in MLLMs",
            "submittedOnDailyBy": {
                "_id": "64b8a72952b7353d8c669086",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8a72952b7353d8c669086/3PUTNmx9kd17gtZ9-Yviw.jpeg",
                "isPro": false,
                "fullname": "Qi Fan",
                "user": "fanqiNO1",
                "type": "user"
            },
            "summary": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods.",
            "upvotes": 7,
            "discussionId": "68494f9642e4f9106973f4cf",
            "githubRepo": "https://github.com/Gennadiyev/mllm-defake",
            "ai_summary": "A dataset with annotations aids in fine-tuning MLLMs for accurate detection and localization of AI-generated images with meaningful explanations.",
            "ai_keywords": [
                "Multi-modal Large Language Models",
                "MLLMs",
                "forgery detection",
                "AI-generated images",
                "bounding boxes",
                "descriptive captions",
                "synthesis artifacts",
                "human-aligned visual-textual grounded reasoning",
                "multi-stage optimization strategy",
                "visual localization",
                "coherent textual explanation"
            ]
        },
        "publishedAt": "2025-06-08T04:47:44.000Z",
        "title": "Interpretable and Reliable Detection of AI-Generated Images via Grounded\n  Reasoning in MLLMs",
        "summary": "The rapid advancement of image generation technologies intensifies the demand\nfor interpretable and robust detection methods. Although existing approaches\noften attain high accuracy, they typically operate as black boxes without\nproviding human-understandable justifications. Multi-modal Large Language\nModels (MLLMs), while not originally intended for forgery detection, exhibit\nstrong analytical and reasoning capabilities. When properly fine-tuned, they\ncan effectively identify AI-generated images and offer meaningful explanations.\nHowever, existing MLLMs still struggle with hallucination and often fail to\nalign their visual interpretations with actual image content and human\nreasoning. To bridge this gap, we construct a dataset of AI-generated images\nannotated with bounding boxes and descriptive captions that highlight synthesis\nartifacts, establishing a foundation for human-aligned visual-textual grounded\nreasoning. We then finetune MLLMs through a multi-stage optimization strategy\nthat progressively balances the objectives of accurate detection, visual\nlocalization, and coherent textual explanation. The resulting model achieves\nsuperior performance in both detecting AI-generated images and localizing\nvisual flaws, significantly outperforming baseline methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07045.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b8a72952b7353d8c669086",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b8a72952b7353d8c669086/3PUTNmx9kd17gtZ9-Yviw.jpeg",
            "fullname": "Qi Fan",
            "name": "fanqiNO1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05167",
            "authors": [
                {
                    "_id": "68468cb23ec10bdd8ab4db5b",
                    "user": {
                        "_id": "645aedd221ab438e732bff43",
                        "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
                        "isPro": false,
                        "fullname": "Yeonseok Jeong",
                        "user": "yeonseokjeong",
                        "type": "user"
                    },
                    "name": "Yeonseok Jeong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-09T10:11:11.745Z",
                    "hidden": false
                },
                {
                    "_id": "68468cb23ec10bdd8ab4db5c",
                    "name": "Jinsu Kim",
                    "hidden": false
                },
                {
                    "_id": "68468cb23ec10bdd8ab4db5d",
                    "name": "Dohyeon Lee",
                    "hidden": false
                },
                {
                    "_id": "68468cb23ec10bdd8ab4db5e",
                    "name": "Seung-won Hwang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T15:43:49.000Z",
            "submittedOnDailyAt": "2025-06-11T00:47:14.627Z",
            "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
            "submittedOnDailyBy": {
                "_id": "645aedd221ab438e732bff43",
                "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
                "isPro": false,
                "fullname": "Yeonseok Jeong",
                "user": "yeonseokjeong",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
            "upvotes": 7,
            "discussionId": "68468cb23ec10bdd8ab4db5f",
            "githubRepo": "https://github.com/ldilab/ECoRAG",
            "ai_summary": "ECoRAG framework enhances LLM performance in ODQA by compressing retrieved documents based on evidentiality, reducing latency and token usage.",
            "ai_keywords": [
                "Retrieval-Augmented Generation (RAG)",
                "context compression",
                "evidentiality",
                "LLM",
                "Open-Domain Question Answering (ODQA)"
            ]
        },
        "publishedAt": "2025-06-05T11:43:49.000Z",
        "title": "ECoRAG: Evidentiality-guided Compression for Long Context RAG",
        "summary": "Large Language Models (LLMs) have shown remarkable performance in Open-Domain\nQuestion Answering (ODQA) by leveraging external documents through\nRetrieval-Augmented Generation (RAG). To reduce RAG overhead, from longer\ncontext, context compression is necessary. However, prior compression methods\ndo not focus on filtering out non-evidential information, which limit the\nperformance in LLM-based RAG. We thus propose Evidentiality-guided RAG, or\nECoRAG framework. ECoRAG improves LLM performance by compressing retrieved\ndocuments based on evidentiality, ensuring whether answer generation is\nsupported by the correct evidence. As an additional step, ECoRAG reflects\nwhether the compressed content provides sufficient evidence, and if not,\nretrieves more until sufficient. Experiments show that ECoRAG improves LLM\nperformance on ODQA tasks, outperforming existing compression methods.\nFurthermore, ECoRAG is highly cost-efficient, as it not only reduces latency\nbut also minimizes token usage by retaining only the necessary information to\ngenerate the correct answer. Code is available at\nhttps://github.com/ldilab/ECoRAG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05167.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645aedd221ab438e732bff43",
            "avatarUrl": "/avatars/31e14fee670fd5ddd296ea0249dbf710.svg",
            "fullname": "Yeonseok Jeong",
            "name": "yeonseokjeong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.07932",
            "authors": [
                {
                    "_id": "68487f6342e4f9106973f17a",
                    "user": {
                        "_id": "60796959c59d9e1697fa2324",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
                        "isPro": false,
                        "fullname": "Rishit Dagli",
                        "user": "rishitdagli",
                        "type": "user"
                    },
                    "name": "Rishit Dagli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:35:32.138Z",
                    "hidden": false
                },
                {
                    "_id": "68487f6342e4f9106973f17b",
                    "name": "Yushi Guan",
                    "hidden": false
                },
                {
                    "_id": "68487f6342e4f9106973f17c",
                    "name": "Sankeerth Durvasula",
                    "hidden": false
                },
                {
                    "_id": "68487f6342e4f9106973f17d",
                    "name": "Mohammadreza Mofayezi",
                    "hidden": false
                },
                {
                    "_id": "68487f6342e4f9106973f17e",
                    "name": "Nandita Vijaykumar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T16:52:10.000Z",
            "submittedOnDailyAt": "2025-06-11T02:25:06.388Z",
            "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
            "submittedOnDailyBy": {
                "_id": "60796959c59d9e1697fa2324",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
                "isPro": false,
                "fullname": "Rishit Dagli",
                "user": "rishitdagli",
                "type": "user"
            },
            "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.",
            "upvotes": 6,
            "discussionId": "68487f6442e4f9106973f17f",
            "projectPage": "https://squeeze3d.github.io/",
            "ai_summary": "A novel framework called Squeeze3D uses pre-trained models to compress 3D data efficiently, achieving high compression ratios while maintaining visual quality.",
            "ai_keywords": [
                "pre-trained 3D generative models",
                "latent spaces",
                "encode",
                "latent code",
                "mapping networks",
                "radiance fields",
                "synthetic data",
                "compression ratios",
                "visual quality",
                "compression latency",
                "decompression latency"
            ]
        },
        "publishedAt": "2025-06-09T12:52:10.000Z",
        "title": "Squeeze3D: Your 3D Generation Model is Secretly an Extreme Neural\n  Compressor",
        "summary": "We propose Squeeze3D, a novel framework that leverages implicit prior\nknowledge learnt by existing pre-trained 3D generative models to compress 3D\ndata at extremely high compression ratios. Our approach bridges the latent\nspaces between a pre-trained encoder and a pre-trained generation model through\ntrainable mapping networks. Any 3D model represented as a mesh, point cloud, or\na radiance field is first encoded by the pre-trained encoder and then\ntransformed (i.e. compressed) into a highly compact latent code. This latent\ncode can effectively be used as an extremely compressed representation of the\nmesh or point cloud. A mapping network transforms the compressed latent code\ninto the latent space of a powerful generative model, which is then conditioned\nto recreate the original 3D model (i.e. decompression). Squeeze3D is trained\nentirely on generated synthetic data and does not require any 3D datasets. The\nSqueeze3D architecture can be flexibly used with existing pre-trained 3D\nencoders and existing generative models. It can flexibly support different\nformats, including meshes, point clouds, and radiance fields. Our experiments\ndemonstrate that Squeeze3D achieves compression ratios of up to 2187x for\ntextured meshes, 55x for point clouds, and 619x for radiance fields while\nmaintaining visual quality comparable to many existing methods. Squeeze3D only\nincurs a small compression and decompression latency since it does not involve\ntraining object-specific networks to compress an object.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07932.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60796959c59d9e1697fa2324",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60796959c59d9e1697fa2324/wxnDm-p3YgB95NV2p4LGF.png",
            "fullname": "Rishit Dagli",
            "name": "rishitdagli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08500",
            "authors": [
                {
                    "_id": "6849358142e4f9106973f453",
                    "name": "Arie Cattan",
                    "hidden": false
                },
                {
                    "_id": "6849358142e4f9106973f454",
                    "name": "Alon Jacovi",
                    "hidden": false
                },
                {
                    "_id": "6849358142e4f9106973f455",
                    "name": "Ori Ram",
                    "hidden": false
                },
                {
                    "_id": "6849358142e4f9106973f456",
                    "name": "Jonathan Herzig",
                    "hidden": false
                },
                {
                    "_id": "6849358142e4f9106973f457",
                    "name": "Roee Aharoni",
                    "hidden": false
                },
                {
                    "_id": "6849358142e4f9106973f458",
                    "name": "Sasha Goldshtein",
                    "hidden": false
                },
                {
                    "_id": "6849358142e4f9106973f459",
                    "name": "Eran Ofek",
                    "hidden": false
                },
                {
                    "_id": "6849358142e4f9106973f45a",
                    "name": "Idan Szpektor",
                    "hidden": false
                },
                {
                    "_id": "6849358142e4f9106973f45b",
                    "name": "Avi Caciularu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T06:52:57.000Z",
            "submittedOnDailyAt": "2025-06-11T13:13:34.474Z",
            "title": "DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in\n  Search-Augmented LLMs",
            "submittedOnDailyBy": {
                "_id": "607d9ade362be7ad6dfdecca",
                "avatarUrl": "/avatars/39cb819fd9c479b300c2a44d8b067e62.svg",
                "isPro": false,
                "fullname": "Arie Cattan",
                "user": "cattana",
                "type": "user"
            },
            "summary": "Retrieval Augmented Generation (RAG) is a commonly used approach for\nenhancing large language models (LLMs) with relevant and up-to-date\ninformation. However, the retrieved sources can often contain conflicting\ninformation and it remains unclear how models should address such\ndiscrepancies. In this work, we first propose a novel taxonomy of knowledge\nconflict types in RAG, along with the desired model behavior for each type. We\nthen introduce CONFLICTS, a high-quality benchmark with expert annotations of\nconflict types in a realistic RAG setting. CONFLICTS is the first benchmark\nthat enables tracking progress on how models address a wide range of knowledge\nconflicts. We conduct extensive experiments on this benchmark, showing that\nLLMs often struggle to appropriately resolve conflicts between sources. While\nprompting LLMs to explicitly reason about the potential conflict in the\nretrieved documents significantly improves the quality and appropriateness of\ntheir responses, substantial room for improvement in future research remains.",
            "upvotes": 5,
            "discussionId": "6849358142e4f9106973f45c",
            "ai_summary": "CONFLICTS, a benchmark for evaluating how LLMs handle knowledge conflicts in RAG, reveals significant challenges in conflict resolution but shows improvement with explicit prompting.",
            "ai_keywords": [
                "Retrieval Augmented Generation",
                "large language models",
                "knowledge conflict types",
                "benchmark",
                "expert annotations",
                "prompt reasoning"
            ]
        },
        "publishedAt": "2025-06-10T02:52:57.000Z",
        "title": "DRAGged into Conflicts: Detecting and Addressing Conflicting Sources in\n  Search-Augmented LLMs",
        "summary": "Retrieval Augmented Generation (RAG) is a commonly used approach for\nenhancing large language models (LLMs) with relevant and up-to-date\ninformation. However, the retrieved sources can often contain conflicting\ninformation and it remains unclear how models should address such\ndiscrepancies. In this work, we first propose a novel taxonomy of knowledge\nconflict types in RAG, along with the desired model behavior for each type. We\nthen introduce CONFLICTS, a high-quality benchmark with expert annotations of\nconflict types in a realistic RAG setting. CONFLICTS is the first benchmark\nthat enables tracking progress on how models address a wide range of knowledge\nconflicts. We conduct extensive experiments on this benchmark, showing that\nLLMs often struggle to appropriately resolve conflicts between sources. While\nprompting LLMs to explicitly reason about the potential conflict in the\nretrieved documents significantly improves the quality and appropriateness of\ntheir responses, substantial room for improvement in future research remains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08500.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "607d9ade362be7ad6dfdecca",
            "avatarUrl": "/avatars/39cb819fd9c479b300c2a44d8b067e62.svg",
            "fullname": "Arie Cattan",
            "name": "cattana",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.08887",
            "authors": [
                {
                    "_id": "6848ec1542e4f9106973f2ac",
                    "user": {
                        "_id": "6364b81b3e248b1e28a68b26",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
                        "isPro": false,
                        "fullname": "LeqiShen",
                        "user": "lunar677",
                        "type": "user"
                    },
                    "name": "Leqi Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:35:05.097Z",
                    "hidden": false
                },
                {
                    "_id": "6848ec1542e4f9106973f2ad",
                    "name": "Guoqiang Gong",
                    "hidden": false
                },
                {
                    "_id": "6848ec1542e4f9106973f2ae",
                    "name": "Tianxiang Hao",
                    "hidden": false
                },
                {
                    "_id": "6848ec1542e4f9106973f2af",
                    "name": "Tao He",
                    "hidden": false
                },
                {
                    "_id": "6848ec1542e4f9106973f2b0",
                    "name": "Yifeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6848ec1542e4f9106973f2b1",
                    "name": "Pengzhang Liu",
                    "hidden": false
                },
                {
                    "_id": "6848ec1542e4f9106973f2b2",
                    "name": "Sicheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "6848ec1542e4f9106973f2b3",
                    "name": "Jungong Han",
                    "hidden": false
                },
                {
                    "_id": "6848ec1542e4f9106973f2b4",
                    "name": "Guiguang Ding",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T15:16:40.000Z",
            "submittedOnDailyAt": "2025-06-11T01:17:30.703Z",
            "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
            "submittedOnDailyBy": {
                "_id": "6364b81b3e248b1e28a68b26",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
                "isPro": false,
                "fullname": "LeqiShen",
                "user": "lunar677",
                "type": "user"
            },
            "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.",
            "upvotes": 4,
            "discussionId": "6848ec1642e4f9106973f2b5",
            "githubRepo": "https://github.com/LunarShen/DsicoVLA",
            "ai_summary": "The paper proposes DiscoVLA to improve video-text retrieval using CLIP by addressing vision, language, and alignment discrepancies, achieving superior performance.",
            "ai_keywords": [
                "parameter-efficient adaptation",
                "image-text pretraining model",
                "CLIP",
                "video-text retrieval",
                "vision",
                "language",
                "alignment",
                "Image-Video Features Fusion",
                "pseudo image captions",
                "Image-to-Video Alignment Distillation",
                "MSRVTT",
                "R@1"
            ]
        },
        "publishedAt": "2025-06-10T11:16:40.000Z",
        "title": "DiscoVLA: Discrepancy Reduction in Vision, Language, and Alignment for\n  Parameter-Efficient Video-Text Retrieval",
        "summary": "The parameter-efficient adaptation of the image-text pretraining model CLIP\nfor video-text retrieval is a prominent area of research. While CLIP is focused\non image-level vision-language matching, video-text retrieval demands\ncomprehensive understanding at the video level. Three key discrepancies emerge\nin the transfer from image-level to video-level: vision, language, and\nalignment. However, existing methods mainly focus on vision while neglecting\nlanguage and alignment. In this paper, we propose Discrepancy Reduction in\nVision, Language, and Alignment (DiscoVLA), which simultaneously mitigates all\nthree discrepancies. Specifically, we introduce Image-Video Features Fusion to\nintegrate image-level and video-level features, effectively tackling both\nvision and language discrepancies. Additionally, we generate pseudo image\ncaptions to learn fine-grained image-level alignment. To mitigate alignment\ndiscrepancies, we propose Image-to-Video Alignment Distillation, which\nleverages image-level alignment knowledge to enhance video-level alignment.\nExtensive experiments demonstrate the superiority of our DiscoVLA. In\nparticular, on MSRVTT with CLIP (ViT-B/16), DiscoVLA outperforms previous\nmethods by 1.5% in R@1, reaching a final score of 50.5% R@1. The code is\navailable at https://github.com/LunarShen/DsicoVLA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08887.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6364b81b3e248b1e28a68b26",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6364b81b3e248b1e28a68b26/9pd6zfH3HGx1gQkYuL3pR.png",
            "fullname": "LeqiShen",
            "name": "lunar677",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08300",
            "authors": [
                {
                    "_id": "684954493614057188acbf5a",
                    "name": "Matteo Cargnelutti",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf5b",
                    "name": "Catherine Brobston",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf5c",
                    "name": "John Hess",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf5d",
                    "name": "Jack Cushman",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf5e",
                    "name": "Kristi Mukk",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf5f",
                    "name": "Aristana Scourtas",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf60",
                    "name": "Kyle Courtney",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf61",
                    "name": "Greg Leppert",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf62",
                    "name": "Amanda Watson",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf63",
                    "name": "Martha Whitehead",
                    "hidden": false
                },
                {
                    "_id": "684954493614057188acbf64",
                    "name": "Jonathan Zittrain",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T00:11:30.000Z",
            "submittedOnDailyAt": "2025-06-11T08:37:11.451Z",
            "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability",
            "submittedOnDailyBy": {
                "_id": "5e6a3d4ea9afd5125d9ec064",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                "isPro": true,
                "fullname": "Stefan Schweter",
                "user": "stefan-it",
                "type": "user"
            },
            "summary": "Large language models (LLMs) use data to learn about the world in order to\nproduce meaningful correlations and predictions. As such, the nature, scale,\nquality, and diversity of the datasets used to train these models, or to\nsupport their work at inference time, have a direct impact on their quality.\nThe rapid development and adoption of LLMs of varying quality has brought into\nfocus the scarcity of publicly available, high-quality training data and\nrevealed an urgent need to ground the stewardship of these datasets in\nsustainable practices with clear provenance chains. To that end, this technical\nreport introduces Institutional Books 1.0, a large collection of public domain\nbooks originally digitized through Harvard Library's participation in the\nGoogle Books project, beginning in 2006. Working with Harvard Library, we\nextracted, analyzed, and processed these volumes into an extensively-documented\ndataset of historic texts. This analysis covers the entirety of Harvard\nLibrary's collection scanned as part of that project, originally spanning\n1,075,899 volumes written in over 250 different languages for a total of\napproximately 250 billion tokens. As part of this initial release, the\nOCR-extracted text (original and post-processed) as well as the metadata\n(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,\nidentified as being in the public domain have been made available. This report\ndescribes this project's goals and methods as well as the results of the\nanalyses we performed, all in service of making this historical collection more\naccessible and easier for humans and machines alike to filter, read and use.",
            "upvotes": 4,
            "discussionId": "684954493614057188acbf65",
            "ai_summary": "Institutional Books 1.0 is a large dataset of public domain books, totaling approximately 242 billion tokens, made available through OCR and metadata processing to facilitate accessibility and usage.",
            "ai_keywords": [
                "Large language models",
                "Institutional Books 1.0",
                "Harvard Library",
                "Google Books project",
                "OCR-extracted text",
                "metadata",
                "bibliographic",
                "source",
                "generated",
                "public domain"
            ]
        },
        "publishedAt": "2025-06-09T20:11:30.000Z",
        "title": "Institutional Books 1.0: A 242B token dataset from Harvard Library's\n  collections, refined for accuracy and usability",
        "summary": "Large language models (LLMs) use data to learn about the world in order to\nproduce meaningful correlations and predictions. As such, the nature, scale,\nquality, and diversity of the datasets used to train these models, or to\nsupport their work at inference time, have a direct impact on their quality.\nThe rapid development and adoption of LLMs of varying quality has brought into\nfocus the scarcity of publicly available, high-quality training data and\nrevealed an urgent need to ground the stewardship of these datasets in\nsustainable practices with clear provenance chains. To that end, this technical\nreport introduces Institutional Books 1.0, a large collection of public domain\nbooks originally digitized through Harvard Library's participation in the\nGoogle Books project, beginning in 2006. Working with Harvard Library, we\nextracted, analyzed, and processed these volumes into an extensively-documented\ndataset of historic texts. This analysis covers the entirety of Harvard\nLibrary's collection scanned as part of that project, originally spanning\n1,075,899 volumes written in over 250 different languages for a total of\napproximately 250 billion tokens. As part of this initial release, the\nOCR-extracted text (original and post-processed) as well as the metadata\n(bibliographic, source, and generated) of the 983,004 volumes, or 242B tokens,\nidentified as being in the public domain have been made available. This report\ndescribes this project's goals and methods as well as the results of the\nanalyses we performed, all in service of making this historical collection more\naccessible and easier for humans and machines alike to filter, read and use.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08300.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "5e6a3d4ea9afd5125d9ec064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2743
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.07976",
            "authors": [
                {
                    "_id": "68487bd642e4f9106973f16d",
                    "name": "Junhong Shen",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f16e",
                    "user": {
                        "_id": "62927c2e56fedc76e396b3ca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
                        "isPro": false,
                        "fullname": "HAO BAI",
                        "user": "JackBAI",
                        "type": "user"
                    },
                    "name": "Hao Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T10:08:42.049Z",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f16f",
                    "name": "Lunjun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f170",
                    "name": "Yifei Zhou",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f171",
                    "name": "Amrith Setlur",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f172",
                    "name": "Shengbang Tong",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f173",
                    "name": "Diego Caples",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f174",
                    "name": "Nan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f175",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f176",
                    "name": "Ameet Talwalkar",
                    "hidden": false
                },
                {
                    "_id": "68487bd642e4f9106973f177",
                    "name": "Aviral Kumar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T17:50:02.000Z",
            "submittedOnDailyAt": "2025-06-11T08:38:04.852Z",
            "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
            "submittedOnDailyBy": {
                "_id": "62927c2e56fedc76e396b3ca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
                "isPro": false,
                "fullname": "HAO BAI",
                "user": "JackBAI",
                "type": "user"
            },
            "summary": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.",
            "upvotes": 4,
            "discussionId": "68487bd742e4f9106973f178",
            "ai_summary": "Test-Time Interaction (TTI) improves web agent performance by scaling interaction, enabling adaptive behavior and balancing exploration and exploitation without adding per-step compute.",
            "ai_keywords": [
                "test-time scaling",
                "thinking traces",
                "agent interaction",
                "interaction horizon",
                "exploration",
                "backtracking",
                "dynamic re-planning",
                "rollout",
                "curriculum-based online reinforcement learning (RL)",
                "Gemma 3 12B model",
                "WebVoyager",
                "WebArena",
                "adaptive agents"
            ]
        },
        "publishedAt": "2025-06-09T13:50:02.000Z",
        "title": "Thinking vs. Doing: Agents that Reason by Scaling Test-Time Interaction",
        "summary": "The current paradigm of test-time scaling relies on generating long reasoning\ntraces (\"thinking\" more) before producing a response. In agent problems that\nrequire interaction, this can be done by generating thinking traces before\nacting in the world. However, this process does not allow agents to acquire new\ninformation from the environment or adapt their behavior over time. In this\nwork, we propose to scale test-time interaction, an untapped dimension of\ntest-time scaling that increases the agent's interaction horizon to enable\nrunning rich behaviors such as exploration, backtracking, and dynamic\nre-planning within a single rollout. To demonstrate the promise of this scaling\ndimension, we study the domain of web agents. We first show that even\nprompting-based interaction scaling without any training can improve task\nsuccess on web benchmarks non-trivially. Building on this, we introduce TTI\n(Test-Time Interaction), a curriculum-based online reinforcement learning (RL)\napproach that trains agents by adaptively adjusting their rollout lengths.\nUsing a Gemma 3 12B model, TTI produces state-of-the-art open-source, open-data\nweb agents on WebVoyager and WebArena benchmarks. We further show that TTI\nenables agents to balance exploration and exploitation adaptively. Our results\nestablish interaction scaling as a powerful, complementary axis to scaling\nper-step compute, offering new avenues for training adaptive agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07976.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62927c2e56fedc76e396b3ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1678105603200-62927c2e56fedc76e396b3ca.jpeg",
            "fullname": "HAO BAI",
            "name": "JackBAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05928",
            "authors": [
                {
                    "_id": "6847b3393ec10bdd8ab4df20",
                    "user": {
                        "_id": "65ea90741b0d7e029a3a1fb0",
                        "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
                        "isPro": false,
                        "fullname": "cj",
                        "user": "cajie",
                        "type": "user"
                    },
                    "name": "Jie Cao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T08:42:50.209Z",
                    "hidden": false
                },
                {
                    "_id": "6847b3393ec10bdd8ab4df21",
                    "name": "Tianwei Lin",
                    "hidden": false
                },
                {
                    "_id": "6847b3393ec10bdd8ab4df22",
                    "name": "Hongyang He",
                    "hidden": false
                },
                {
                    "_id": "6847b3393ec10bdd8ab4df23",
                    "name": "Rolan Yan",
                    "hidden": false
                },
                {
                    "_id": "6847b3393ec10bdd8ab4df24",
                    "name": "Wenqiao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6847b3393ec10bdd8ab4df25",
                    "name": "Juncheng Li",
                    "hidden": false
                },
                {
                    "_id": "6847b3393ec10bdd8ab4df26",
                    "name": "Dongping Zhang",
                    "hidden": false
                },
                {
                    "_id": "6847b3393ec10bdd8ab4df27",
                    "name": "Siliang Tang",
                    "hidden": false
                },
                {
                    "_id": "6847b3393ec10bdd8ab4df28",
                    "name": "Yueting Zhuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T09:54:19.000Z",
            "submittedOnDailyAt": "2025-06-11T01:28:46.209Z",
            "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "65ea90741b0d7e029a3a1fb0",
                "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
                "isPro": false,
                "fullname": "cj",
                "user": "cajie",
                "type": "user"
            },
            "summary": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) to further enhance the performance of parameter-efficient fine-tuning\n(PEFT) methods in Large Language Model (LLM) applications. Existing methods\nemploy homogeneous MoE-LoRA architectures composed of LoRA experts with\neither similar or identical structures and capacities. However, these\napproaches often suffer from representation collapse and expert load imbalance,\nwhich negatively impact the potential of LLMs. To address these challenges, we\npropose a heterogeneous Mixture-of-Adapters (MoA) approach.\nThis method dynamically integrates PEFT adapter experts with diverse\nstructures, leveraging their complementary representational capabilities to\nfoster expert specialization, thereby enhancing the effective transfer of\npre-trained knowledge to downstream tasks. MoA supports two variants:\n(i) Soft MoA achieves fine-grained integration by performing\na weighted fusion of all expert outputs; (ii) Sparse MoA\nactivates adapter experts sparsely based on their contribution, achieving this\nwith negligible performance degradation. Experimental results demonstrate that\nheterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance\nand parameter efficiency. Our project is available at\nhttps://github.com/DCDmllm/MoA.",
            "upvotes": 4,
            "discussionId": "6847b3393ec10bdd8ab4df29",
            "githubRepo": "https://github.com/DCDmllm/MoA",
            "ai_summary": "A heterogeneous Mixture-of-Adapters (MoA) approach enhances parameter-efficient fine-tuning in LLMs by integrating diverse adapter experts, outperforming homogeneous MoE-LoRA methods.",
            "ai_keywords": [
                "Low-Rank Adaptation",
                "Mixture-of-Experts",
                "parameter-efficient fine-tuning",
                "Large Language Model",
                "homogeneous",
                "representation collapse",
                "expert load imbalance",
                "heterogeneous",
                "Mixture-of-Adapters",
                "soft MoA",
                "sparse MoA"
            ]
        },
        "publishedAt": "2025-06-06T05:54:19.000Z",
        "title": "MoA: Heterogeneous Mixture of Adapters for Parameter-Efficient\n  Fine-Tuning of Large Language Models",
        "summary": "Recent studies integrate Low-Rank Adaptation (LoRA) and Mixture-of-Experts\n(MoE) to further enhance the performance of parameter-efficient fine-tuning\n(PEFT) methods in Large Language Model (LLM) applications. Existing methods\nemploy homogeneous MoE-LoRA architectures composed of LoRA experts with\neither similar or identical structures and capacities. However, these\napproaches often suffer from representation collapse and expert load imbalance,\nwhich negatively impact the potential of LLMs. To address these challenges, we\npropose a heterogeneous Mixture-of-Adapters (MoA) approach.\nThis method dynamically integrates PEFT adapter experts with diverse\nstructures, leveraging their complementary representational capabilities to\nfoster expert specialization, thereby enhancing the effective transfer of\npre-trained knowledge to downstream tasks. MoA supports two variants:\n(i) Soft MoA achieves fine-grained integration by performing\na weighted fusion of all expert outputs; (ii) Sparse MoA\nactivates adapter experts sparsely based on their contribution, achieving this\nwith negligible performance degradation. Experimental results demonstrate that\nheterogeneous MoA outperforms homogeneous MoE-LoRA methods in both performance\nand parameter efficiency. Our project is available at\nhttps://github.com/DCDmllm/MoA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05928.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ea90741b0d7e029a3a1fb0",
            "avatarUrl": "/avatars/7a4f861d4ead080996bb28e2b6cb8ac5.svg",
            "fullname": "cj",
            "name": "cajie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05700",
            "authors": [
                {
                    "_id": "6848de6e42e4f9106973f273",
                    "user": {
                        "_id": "65d76cc5b9b7b8bf88faa916",
                        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                        "isPro": true,
                        "fullname": "Yan Wang",
                        "user": "YanAdjeNole",
                        "type": "user"
                    },
                    "name": "Yan Wang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-11T01:39:59.328Z",
                    "hidden": false
                },
                {
                    "_id": "6848de6e42e4f9106973f274",
                    "name": "Yueru He",
                    "hidden": false
                },
                {
                    "_id": "6848de6e42e4f9106973f275",
                    "name": "Ruoyu Xiang",
                    "hidden": false
                },
                {
                    "_id": "6848de6e42e4f9106973f276",
                    "name": "Jeff Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T03:02:52.000Z",
            "submittedOnDailyAt": "2025-06-11T00:15:04.904Z",
            "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
            "submittedOnDailyBy": {
                "_id": "65d76cc5b9b7b8bf88faa916",
                "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                "isPro": true,
                "fullname": "Yan Wang",
                "user": "YanAdjeNole",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) hold great promise for\nfinancial applications but introduce critical accuracy and compliance\nchallenges in Digital Regulatory Reporting (DRR). To address these issues, we\npropose RKEFino1, a regulation knowledge-enhanced financial reasoning model\nbuilt upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We\nformulate two QA tasks-knowledge-based and mathematical reasoning-and introduce\na novel Numerical NER task covering financial entities in both sentences and\ntables. Experimental results demonstrate the effectiveness and generalization\ncapacity of RKEFino1 in compliance-critical financial tasks. We have released\nour model on Hugging Face.",
            "upvotes": 2,
            "discussionId": "6848de6e42e4f9106973f277",
            "ai_summary": "RKEFino1, a regulation-aware LLM fine-tuned with financial domain knowledge, effectively handles compliance-critical tasks including QA and numerical NER.",
            "ai_keywords": [
                "Large language models",
                "financial reasoning",
                "Digital Regulatory Reporting",
                "regulation knowledge-enhanced",
                "fine-tuning",
                "domain knowledge",
                "XBRL",
                "CDM",
                "MOF",
                "QA tasks",
                "knowledge-based reasoning",
                "mathematical reasoning",
                "Numerical NER",
                "financial entities"
            ]
        },
        "publishedAt": "2025-06-05T23:02:52.000Z",
        "title": "RKEFino1: A Regulation Knowledge-Enhanced Large Language Model",
        "summary": "Recent advances in large language models (LLMs) hold great promise for\nfinancial applications but introduce critical accuracy and compliance\nchallenges in Digital Regulatory Reporting (DRR). To address these issues, we\npropose RKEFino1, a regulation knowledge-enhanced financial reasoning model\nbuilt upon Fino1, fine-tuned with domain knowledge from XBRL, CDM, and MOF. We\nformulate two QA tasks-knowledge-based and mathematical reasoning-and introduce\na novel Numerical NER task covering financial entities in both sentences and\ntables. Experimental results demonstrate the effectiveness and generalization\ncapacity of RKEFino1 in compliance-critical financial tasks. We have released\nour model on Hugging Face.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05700.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d76cc5b9b7b8bf88faa916",
            "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
            "fullname": "Yan Wang",
            "name": "YanAdjeNole",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.07047",
            "authors": [
                {
                    "_id": "68492bf142e4f9106973f411",
                    "name": "Yu Xuejun",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f412",
                    "user": {
                        "_id": "6608fa4f5baec84322ec85ea",
                        "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
                        "isPro": false,
                        "fullname": "Zhong",
                        "user": "Jianyuan1",
                        "type": "user"
                    },
                    "name": "Jianyuan Zhong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:34:08.719Z",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f413",
                    "name": "Zijin Feng",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f414",
                    "name": "Pengyi Zhai",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f415",
                    "name": "Roozbeh Yousefzadeh",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f416",
                    "name": "Wei Chong Ng",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f417",
                    "name": "Haoxiong Liu",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f418",
                    "name": "Ziyi Shou",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f419",
                    "name": "Jing Xiong",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f41a",
                    "name": "Yudong Zhou",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f41b",
                    "name": "Claudia Beth Ong",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f41c",
                    "name": "Austen Jeremy Sugiarto",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f41d",
                    "name": "Yaoxi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f41e",
                    "name": "Wai Ming Tai",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f41f",
                    "name": "Huan Cao",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f420",
                    "name": "Dongcai Lu",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f421",
                    "name": "Jiacheng Sun",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f422",
                    "name": "Qiang Xu",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f423",
                    "name": "Shen Xin",
                    "hidden": false
                },
                {
                    "_id": "68492bf142e4f9106973f424",
                    "name": "Zhenguo Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-08T09:04:14.000Z",
            "submittedOnDailyAt": "2025-06-11T05:45:24.452Z",
            "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
            "submittedOnDailyBy": {
                "_id": "6608fa4f5baec84322ec85ea",
                "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
                "isPro": false,
                "fullname": "Zhong",
                "user": "Jianyuan1",
                "type": "user"
            },
            "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.",
            "upvotes": 1,
            "discussionId": "68492bf142e4f9106973f425",
            "githubRepo": "https://github.com/Huawei-AI4Math/Mathesis"
        },
        "publishedAt": "2025-06-08T05:04:14.000Z",
        "title": "Mathesis: Towards Formal Theorem Proving from Natural Languages",
        "summary": "Recent advances in large language models show strong promise for formal\nreasoning. However, most LLM-based theorem provers have long been constrained\nby the need for expert-written formal statements as inputs, limiting their\napplicability to real-world problems expressed in natural language. We tackle\nthis gap with Mathesis, the first end-to-end theorem proving pipeline\nprocessing informal problem statements. It contributes Mathesis-Autoformalizer,\nthe first autoformalizer using reinforcement learning to enhance the\nformalization ability of natural language problems, aided by our novel\nLeanScorer framework for nuanced formalization quality assessment. It also\nproposes a Mathesis-Prover, which generates formal proofs from the formalized\nstatements. To evaluate the real-world applicability of end-to-end formal\ntheorem proving, we introduce Gaokao-Formal, a benchmark of 488 complex\nproblems from China's national college entrance exam. Our approach is carefully\ndesigned, with a thorough study of each component. Experiments demonstrate\nMathesis's effectiveness, with the autoformalizer outperforming the best\nbaseline by 22% in pass-rate on Gaokao-Formal. The full system surpasses other\nmodel combinations, achieving 64% accuracy on MiniF2F with pass@32 and a\nstate-of-the-art 18% on Gaokao-Formal.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07047.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6608fa4f5baec84322ec85ea",
            "avatarUrl": "/avatars/13bdaff931676b065fa1efef06fef922.svg",
            "fullname": "Zhong",
            "name": "Jianyuan1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04688",
            "authors": [
                {
                    "_id": "6843d95d3ec10bdd8ab4d81c",
                    "user": {
                        "_id": "66976a7fa7fd582ae754850e",
                        "avatarUrl": "/avatars/699af1fccb1588edbe876a8fd505926d.svg",
                        "isPro": false,
                        "fullname": "Gio Paik",
                        "user": "skyil7",
                        "type": "user"
                    },
                    "name": "Gio Paik",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-07T13:33:52.701Z",
                    "hidden": false
                },
                {
                    "_id": "6843d95d3ec10bdd8ab4d81d",
                    "name": "Geewook Kim",
                    "hidden": false
                },
                {
                    "_id": "6843d95d3ec10bdd8ab4d81e",
                    "name": "Jinbae Im",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T07:11:36.000Z",
            "submittedOnDailyAt": "2025-06-11T12:47:08.481Z",
            "title": "MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "66976a7fa7fd582ae754850e",
                "avatarUrl": "/avatars/699af1fccb1588edbe876a8fd505926d.svg",
                "isPro": false,
                "fullname": "Gio Paik",
                "user": "skyil7",
                "type": "user"
            },
            "summary": "This paper introduces MMRefine, a MultiModal Refinement benchmark designed to\nevaluate the error refinement capabilities of Multimodal Large Language Models\n(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,\nMMRefine provides a framework that evaluates MLLMs' abilities to detect and\ncorrect errors across six distinct scenarios beyond just comparing final\naccuracy before and after refinement. Furthermore, the benchmark analyzes the\nrefinement performance by categorizing errors into six error types. Experiments\nwith various open and closed MLLMs reveal bottlenecks and factors impeding\nrefinement performance, highlighting areas for improvement in effective\nreasoning enhancement. Our code and dataset are publicly available at\nhttps://github.com/naver-ai/MMRefine.",
            "upvotes": 1,
            "discussionId": "6843d95d3ec10bdd8ab4d81f",
            "projectPage": "https://mmrefine.github.io",
            "githubRepo": "https://github.com/naver-ai/MMRefine",
            "ai_summary": "MMRefine evaluates the error refinement capabilities of Multimodal Large Language Models through a benchmark that categorizes errors and identifies performance bottlenecks.",
            "ai_keywords": [
                "MultiModal Refinement",
                "Multimodal Large Language Models",
                "error refinement",
                "error types",
                "bottleneck analysis"
            ]
        },
        "publishedAt": "2025-06-05T03:11:36.000Z",
        "title": "MMRefine: Unveiling the Obstacles to Robust Refinement in Multimodal\n  Large Language Models",
        "summary": "This paper introduces MMRefine, a MultiModal Refinement benchmark designed to\nevaluate the error refinement capabilities of Multimodal Large Language Models\n(MLLMs). As the emphasis shifts toward enhancing reasoning during inference,\nMMRefine provides a framework that evaluates MLLMs' abilities to detect and\ncorrect errors across six distinct scenarios beyond just comparing final\naccuracy before and after refinement. Furthermore, the benchmark analyzes the\nrefinement performance by categorizing errors into six error types. Experiments\nwith various open and closed MLLMs reveal bottlenecks and factors impeding\nrefinement performance, highlighting areas for improvement in effective\nreasoning enhancement. Our code and dataset are publicly available at\nhttps://github.com/naver-ai/MMRefine.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04688.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66976a7fa7fd582ae754850e",
            "avatarUrl": "/avatars/699af1fccb1588edbe876a8fd505926d.svg",
            "fullname": "Gio Paik",
            "name": "skyil7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.04020",
            "authors": [
                {
                    "_id": "684962453614057188acbfb8",
                    "name": "An Quang Tang",
                    "hidden": false
                },
                {
                    "_id": "684962453614057188acbfb9",
                    "name": "Xiuzhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "684962453614057188acbfba",
                    "name": "Minh Ngoc Dinh",
                    "hidden": false
                },
                {
                    "_id": "684962453614057188acbfbb",
                    "name": "Zhuang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T14:50:32.000Z",
            "submittedOnDailyAt": "2025-06-11T09:35:07.800Z",
            "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused\n  Summarization for Review-based Product Question Answering",
            "submittedOnDailyBy": {
                "_id": "63d159132036e44c44f87a91",
                "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
                "isPro": false,
                "fullname": "Zhuang Li",
                "user": "lizhuang144",
                "type": "user"
            },
            "summary": "Review-based Product Question Answering (PQA) allows e-commerce platforms to\nautomatically address customer queries by leveraging insights from user\nreviews. However, existing PQA systems generate answers with only a single\nperspective, failing to capture the diversity of customer opinions. In this\npaper we introduce a novel task Quantitative Query-Focused Summarization\n(QQSUM), which aims to summarize diverse customer opinions into representative\nKey Points (KPs) and quantify their prevalence to effectively answer user\nqueries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its\ngenerated answers still fall short of capturing the full diversity of\nviewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,\nemploys few-shot learning to jointly train a KP-oriented retriever and a KP\nsummary generator, enabling KP-based summaries that capture diverse and\nrepresentative opinions. Experimental results demonstrate that QQSUM-RAG\nachieves superior performance compared to state-of-the-art RAG baselines in\nboth textual quality and quantification accuracy of opinions. Our source code\nis available at: https://github.com/antangrocket1312/QQSUMM",
            "upvotes": 1,
            "discussionId": "684962453614057188acbfbc",
            "ai_summary": "QQSUM-RAG extends RAG to generate diverse and representative summaries of customer opinions for product question answering by integrating few-shot learning.",
            "ai_keywords": [
                "Quantitative Query-Focused Summarization",
                "QQSUM",
                "Retrieval-Augmented Generation",
                "RAG",
                "few-shot learning",
                "Key Points",
                "KP-based summaries"
            ]
        },
        "publishedAt": "2025-06-04T10:50:32.000Z",
        "title": "QQSUM: A Novel Task and Model of Quantitative Query-Focused\n  Summarization for Review-based Product Question Answering",
        "summary": "Review-based Product Question Answering (PQA) allows e-commerce platforms to\nautomatically address customer queries by leveraging insights from user\nreviews. However, existing PQA systems generate answers with only a single\nperspective, failing to capture the diversity of customer opinions. In this\npaper we introduce a novel task Quantitative Query-Focused Summarization\n(QQSUM), which aims to summarize diverse customer opinions into representative\nKey Points (KPs) and quantify their prevalence to effectively answer user\nqueries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its\ngenerated answers still fall short of capturing the full diversity of\nviewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG,\nemploys few-shot learning to jointly train a KP-oriented retriever and a KP\nsummary generator, enabling KP-based summaries that capture diverse and\nrepresentative opinions. Experimental results demonstrate that QQSUM-RAG\nachieves superior performance compared to state-of-the-art RAG baselines in\nboth textual quality and quantification accuracy of opinions. Our source code\nis available at: https://github.com/antangrocket1312/QQSUMM",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.04020.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d159132036e44c44f87a91",
            "avatarUrl": "/avatars/6a2c9e5b3b25cf1949277d8c40c0070b.svg",
            "fullname": "Zhuang Li",
            "name": "lizhuang144",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    }
]