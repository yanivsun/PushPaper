[
    {
        "paper": {
            "id": "2506.06395",
            "authors": [
                {
                    "_id": "68492dcf42e4f9106973f437",
                    "user": {
                        "_id": "6734e315c1aadce903f73aea",
                        "avatarUrl": "/avatars/95d95c49419372debc201cb63c354b86.svg",
                        "isPro": false,
                        "fullname": "Li Pengyi",
                        "user": "LiPengyi29",
                        "type": "user"
                    },
                    "name": "Pengyi Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-11T07:18:40.287Z",
                    "hidden": false
                },
                {
                    "_id": "68492dcf42e4f9106973f438",
                    "user": {
                        "_id": "6626c5d0a329de26e7eb16fa",
                        "avatarUrl": "/avatars/124f389f768fb666efd8b5a9b54c3b3c.svg",
                        "isPro": false,
                        "fullname": "Matvey Skripkin",
                        "user": "barracuda049",
                        "type": "user"
                    },
                    "name": "Matvey Skripkin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:44.217Z",
                    "hidden": false
                },
                {
                    "_id": "68492dcf42e4f9106973f439",
                    "name": "Alexander Zubrey",
                    "hidden": false
                },
                {
                    "_id": "68492dcf42e4f9106973f43a",
                    "user": {
                        "_id": "643984dceb7c5616ef3f5d54",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
                        "isPro": false,
                        "fullname": "Andrey Kuznetsov",
                        "user": "kuznetsoffandrey",
                        "type": "user"
                    },
                    "name": "Andrey Kuznetsov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:42.242Z",
                    "hidden": false
                },
                {
                    "_id": "68492dcf42e4f9106973f43b",
                    "user": {
                        "_id": "6169a581d05945bfd8718dfa",
                        "avatarUrl": "/avatars/1892ab06a7ddb557232777de3cbec470.svg",
                        "isPro": false,
                        "fullname": "Ivan Oseledets",
                        "user": "oseledets",
                        "type": "user"
                    },
                    "name": "Ivan Oseledets",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:40.519Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
            ],
            "publishedAt": "2025-06-05T19:55:15.000Z",
            "submittedOnDailyAt": "2025-06-12T07:02:06.762Z",
            "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
            "submittedOnDailyBy": {
                "_id": "643984dceb7c5616ef3f5d54",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
                "isPro": false,
                "fullname": "Andrey Kuznetsov",
                "user": "kuznetsoffandrey",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.",
            "upvotes": 76,
            "discussionId": "68492dd042e4f9106973f43c",
            "ai_summary": "Reinforcement Learning via Self-Confidence (RLSC) improves large language model accuracy using the model's confidence as a reward signal, eliminating the need for human labels or reward engineering.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Large language models",
                "self-confidence",
                "RLSC"
            ]
        },
        "publishedAt": "2025-06-05T15:55:15.000Z",
        "title": "Confidence Is All You Need: Few-Shot RL Fine-Tuning of Language Models",
        "summary": "Large language models (LLMs) excel at reasoning, yet post-training remains\ncritical for aligning their behavior with task goals. Existing reinforcement\nlearning (RL) methods often depend on costly human annotations or external\nreward models. We propose Reinforcement Learning via Self-Confidence (RLSC),\nwhich uses the model's own confidence as reward signals-eliminating the need\nfor labels, preference models, or reward engineering. Applied to\nQwen2.5-Math-7B with only 16 samples per question and 10 or 20 training steps,\nRLSC improves accuracy by +13.4% on AIME2024, +21.2% on MATH500, +21.7% on\nMinerva Math, +20.8% on Olympiadbench, and +9.7% on AMC23. RLSC provides a\nsimple, scalable post-training method for inference models, requiring only a\nsmall number of samples and unlabelled supervision.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643984dceb7c5616ef3f5d54/5kHQrpj1ivFhnzHL36xhr.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06395.png",
        "numComments": 8,
        "submittedBy": {
            "_id": "643984dceb7c5616ef3f5d54",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
            "fullname": "Andrey Kuznetsov",
            "name": "kuznetsoffandrey",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09113",
            "authors": [
                {
                    "_id": "684a3b0a9b38e1e5a33a683f",
                    "user": {
                        "_id": "6614f2dca37a503c2320b44e",
                        "avatarUrl": "/avatars/4bee18c49eff253d6eeb9a1f1509b68b.svg",
                        "isPro": false,
                        "fullname": "gaoyu",
                        "user": "gaooyu1520",
                        "type": "user"
                    },
                    "name": "Yu Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:01.191Z",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6840",
                    "name": "Haoyuan Guo",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6841",
                    "name": "Tuyen Hoang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6842",
                    "name": "Weilin Huang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6843",
                    "name": "Lu Jiang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6844",
                    "name": "Fangyuan Kong",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6845",
                    "name": "Huixia Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6846",
                    "name": "Jiashi Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6847",
                    "name": "Liang Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6848",
                    "name": "Xiaojie Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6849",
                    "name": "Xunsong Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684a",
                    "name": "Yifu Li",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684b",
                    "name": "Shanchuan Lin",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684c",
                    "name": "Zhijie Lin",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684d",
                    "user": {
                        "_id": "63049b95dae2eb7d083f1bf3",
                        "avatarUrl": "/avatars/5eaeadc1318724b54ea59873da11275f.svg",
                        "isPro": false,
                        "fullname": "Jiawei Liu",
                        "user": "jwliu-cc",
                        "type": "user"
                    },
                    "name": "Jiawei Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:41:55.061Z",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684e",
                    "name": "Shu Liu",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a684f",
                    "name": "Xiaonan Nie",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6850",
                    "name": "Zhiwu Qing",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6851",
                    "name": "Yuxi Ren",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6852",
                    "name": "Li Sun",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6853",
                    "name": "Zhi Tian",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6854",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6855",
                    "name": "Sen Wang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6856",
                    "name": "Guoqiang Wei",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6857",
                    "name": "Guohong Wu",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6858",
                    "user": {
                        "_id": "6381c5d63680a7cf34e08ca9",
                        "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
                        "isPro": false,
                        "fullname": "wujie10558@gmail.com",
                        "user": "wujie10",
                        "type": "user"
                    },
                    "name": "Jie Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:05.891Z",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6859",
                    "name": "Ruiqi Xia",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685a",
                    "name": "Fei Xiao",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685b",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685c",
                    "name": "Jiangqiao Yan",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685d",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685e",
                    "name": "Jianchao Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a685f",
                    "name": "Runkai Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6860",
                    "name": "Tao Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6861",
                    "name": "Yihang Yang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6862",
                    "user": {
                        "_id": "65b62ab582d3845134ef0aab",
                        "avatarUrl": "/avatars/fb1c22b2937e86f668b06fedb48e57e0.svg",
                        "isPro": false,
                        "fullname": "Zilyu Ye",
                        "user": "YeLuoSuiYou",
                        "type": "user"
                    },
                    "name": "Zilyu Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:41:58.552Z",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6863",
                    "name": "Xuejiao Zeng",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6864",
                    "name": "Yan Zeng",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6865",
                    "name": "Heng Zhang",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6866",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6867",
                    "name": "Xiaozheng Zheng",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6868",
                    "name": "Peihao Zhu",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a6869",
                    "name": "Jiaxin Zou",
                    "hidden": false
                },
                {
                    "_id": "684a3b0a9b38e1e5a33a686a",
                    "name": "Feilong Zuo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:56:11.000Z",
            "submittedOnDailyAt": "2025-06-12T01:08:56.090Z",
            "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
            "submittedOnDailyBy": {
                "_id": "6381c5d63680a7cf34e08ca9",
                "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
                "isPro": false,
                "fullname": "wujie10558@gmail.com",
                "user": "wujie10",
                "type": "user"
            },
            "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.",
            "upvotes": 49,
            "discussionId": "684a3b0b9b38e1e5a33a686b",
            "projectPage": "https://seed.bytedance.com/seedance",
            "ai_summary": "Seedance 1.0 offers high-performance video generation by integrating advanced data curation, efficient architecture, post-training optimization, and model acceleration, resulting in superior quality and speed.",
            "ai_keywords": [
                "diffusion modeling",
                "multi-source data curation",
                "precision and meaningful video captioning",
                "efficient architecture",
                "training paradigm",
                "multi-shot generation",
                "text-to-video",
                "image-to-video",
                "fine-grained supervised fine-tuning",
                "video-specific RLHF",
                "multi-dimensional reward mechanisms",
                "multi-stage distillation strategies",
                "model acceleration",
                "spatiotemporal fluidity",
                "structural stability",
                "instruction adherence",
                "multi-shot narrative coherence",
                "consistent subject representation"
            ]
        },
        "publishedAt": "2025-06-10T13:56:11.000Z",
        "title": "Seedance 1.0: Exploring the Boundaries of Video Generation Models",
        "summary": "Notable breakthroughs in diffusion modeling have propelled rapid improvements\nin video generation, yet current foundational model still face critical\nchallenges in simultaneously balancing prompt following, motion plausibility,\nand visual quality. In this report, we introduce Seedance 1.0, a\nhigh-performance and inference-efficient video foundation generation model that\nintegrates several core technical improvements: (i) multi-source data curation\naugmented with precision and meaningful video captioning, enabling\ncomprehensive learning across diverse scenarios; (ii) an efficient architecture\ndesign with proposed training paradigm, which allows for natively supporting\nmulti-shot generation and jointly learning of both text-to-video and\nimage-to-video tasks. (iii) carefully-optimized post-training approaches\nleveraging fine-grained supervised fine-tuning, and video-specific RLHF with\nmulti-dimensional reward mechanisms for comprehensive performance improvements;\n(iv) excellent model acceleration achieving ~10x inference speedup through\nmulti-stage distillation strategies and system-level optimizations. Seedance\n1.0 can generate a 5-second video at 1080p resolution only with 41.4 seconds\n(NVIDIA-L20). Compared to state-of-the-art video generation models, Seedance\n1.0 stands out with high-quality and fast video generation having superior\nspatiotemporal fluidity with structural stability, precise instruction\nadherence in complex multi-subject contexts, native multi-shot narrative\ncoherence with consistent subject representation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09113.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6381c5d63680a7cf34e08ca9",
            "avatarUrl": "/avatars/731467e2d80d0ae163c4a00a9e3ff9e5.svg",
            "fullname": "wujie10558@gmail.com",
            "name": "wujie10",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09350",
            "authors": [
                {
                    "_id": "684a79ca9b38e1e5a33a68bf",
                    "user": {
                        "_id": "645863f7dc18eb1a9b5d29df",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645863f7dc18eb1a9b5d29df/t49Nnyl4tbkUn7CmQqKZh.jpeg",
                        "isPro": false,
                        "fullname": "Peter Lin",
                        "user": "PeterL1n",
                        "type": "user"
                    },
                    "name": "Shanchuan Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:40:47.762Z",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c0",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c1",
                    "name": "Hao He",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c2",
                    "name": "Jianwen Jiang",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c3",
                    "name": "Yuxi Ren",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c4",
                    "name": "Xin Xia",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c5",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c6",
                    "name": "Xuefeng Xiao",
                    "hidden": false
                },
                {
                    "_id": "684a79ca9b38e1e5a33a68c7",
                    "name": "Lu Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T03:04:23.000Z",
            "submittedOnDailyAt": "2025-06-12T05:25:53.654Z",
            "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
            "upvotes": 36,
            "discussionId": "684a79ca9b38e1e5a33a68c8",
            "projectPage": "https://seaweed-apt.com/2",
            "ai_summary": "Autoregressive adversarial post-training transforms a pre-trained latent video diffusion model into a real-time, interactive video generator with efficient one-step generation and reduced error accumulation.",
            "ai_keywords": [
                "autoregressive adversarial post-training",
                "latent video diffusion model",
                "single neural function evaluation",
                "KV cache",
                "student-forcing",
                "real-time video generation",
                "24fps",
                "736x416 resolution",
                "1280x720 resolution",
                "H100 GPUs"
            ]
        },
        "publishedAt": "2025-06-10T23:04:23.000Z",
        "title": "Autoregressive Adversarial Post-Training for Real-Time Interactive Video\n  Generation",
        "summary": "Existing large-scale video generation models are computationally intensive,\npreventing adoption in real-time and interactive applications. In this work, we\npropose autoregressive adversarial post-training (AAPT) to transform a\npre-trained latent video diffusion model into a real-time, interactive video\ngenerator. Our model autoregressively generates a latent frame at a time using\na single neural function evaluation (1NFE). The model can stream the result to\nthe user in real time and receive interactive responses as controls to generate\nthe next latent frame. Unlike existing approaches, our method explores\nadversarial training as an effective paradigm for autoregressive generation.\nThis not only allows us to design an architecture that is more efficient for\none-step generation while fully utilizing the KV cache, but also enables\ntraining the model in a student-forcing manner that proves to be effective in\nreducing error accumulation during long video generation. Our experiments\ndemonstrate that our 8B model achieves real-time, 24fps, streaming video\ngeneration at 736x416 resolution on a single H100, or 1280x720 on 8xH100 up to\na minute long (1440 frames). Visit our research website at\nhttps://seaweed-apt.com/2",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09350.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7103
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09991",
            "authors": [
                {
                    "_id": "684adf25dbd21a9cc27b0ec8",
                    "name": "Xinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "684adf25dbd21a9cc27b0ec9",
                    "name": "Yuwei An",
                    "hidden": false
                },
                {
                    "_id": "684adf25dbd21a9cc27b0eca",
                    "name": "Hongyi Liu",
                    "hidden": false
                },
                {
                    "_id": "684adf25dbd21a9cc27b0ecb",
                    "name": "Tianqi Chen",
                    "hidden": false
                },
                {
                    "_id": "684adf25dbd21a9cc27b0ecc",
                    "name": "Beidi Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:59:23.000Z",
            "submittedOnDailyAt": "2025-06-12T12:38:22.483Z",
            "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation",
            "submittedOnDailyBy": {
                "_id": "64f58b970b24e548a85522bc",
                "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
                "isPro": false,
                "fullname": "Xinyu Yang",
                "user": "Hanyuezhuohua",
                "type": "user"
            },
            "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.",
            "upvotes": 35,
            "discussionId": "684adf26dbd21a9cc27b0ecd",
            "projectPage": "https://multiverse4fm.github.io/",
            "ai_summary": "Multiverse, a generative model enabling native parallel text generation, achieves performance comparable to autoregressive models through a three-stage MapReduce paradigm and efficient attention mechanisms.",
            "ai_keywords": [
                "Autoregressive Large Language Models",
                "MapReduce paradigm",
                "parallel generation",
                "adaptive task decomposition",
                "parallel subtask execution",
                "lossless result synthesis",
                "Multiverse Attention",
                "causal attention",
                "parallel inference",
                "Multiverse Engine",
                "AIME24",
                "AIME25",
                "superior scaling",
                "efficiency gain",
                "speedup",
                "batch sizes"
            ]
        },
        "publishedAt": "2025-06-11T13:59:23.000Z",
        "title": "Multiverse: Your Language Models Secretly Decide How to Parallelize and\n  Merge Generation",
        "summary": "Autoregressive Large Language Models (AR-LLMs) frequently exhibit implicit\nparallelism in sequential generation. Inspired by this, we introduce\nMultiverse, a new generative model that enables natively parallel generation.\nMultiverse internalizes a MapReduce paradigm, generating automatically through\nthree stages: (i) a Map stage for adaptive task decomposition, (ii) a Process\nstage for parallel subtask execution, and (iii) a Reduce stage for lossless\nresult synthesis. Next, we build a real-world Multiverse reasoning model with\nco-design of data, algorithm, and system, enabling rapid and seamless transfer\nfrom frontier AR-LLMs. Starting from sequential reasoning chains, we create\nMultiverse 1K by converting them into structured training data using an\nautomated LLM-assisted pipeline, avoiding costly human annotations.\nAlgorithmically, we design Multiverse Attention to separate parallel reasoning\nsteps while keeping compatibility with causal attention for efficient training.\nSystematically, we implement Multiverse Engine to enable parallel inference. It\nfeatures a dedicated scheduler that dynamically switches between sequential and\nparallel generation, triggered directly by the model. After a 3-hour\nfine-tuning with 1K examples, our Multiverse-32B stands as the only\nopen-sourced non-AR model achieving performance on par with leading AR-LLMs of\nthe same scale, evidenced by AIME24 & 25 scores of 54% and 46%, respectively.\nMoreover, our budget control experiments show that Multiverse-32B exhibits\nsuperior scaling, outperforming AR-LLMs by 1.87% on average using the same\ncontext length. Such scaling further leads to practical efficiency gain,\nachieving up to 2x speedup across varying batch sizes. We have open-sourced the\nentire Multiverse ecosystem, including data, model weights, engine, supporting\ntools, as well as complete data curation prompts and detailed training and\nevaluation recipes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09991.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f58b970b24e548a85522bc",
            "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
            "fullname": "Xinyu Yang",
            "name": "Hanyuezhuohua",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09790",
            "authors": [
                {
                    "_id": "684a33989b38e1e5a33a6804",
                    "user": {
                        "_id": "639c379cdb7c5f35004066cb",
                        "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
                        "isPro": false,
                        "fullname": "Zhenran Xu",
                        "user": "imryanxu",
                        "type": "user"
                    },
                    "name": "Zhenran Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:13.515Z",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6805",
                    "user": {
                        "_id": "6309995efa440d8b5bd5ddc2",
                        "avatarUrl": "/avatars/e7350783a8edc5660afd5173818f02f2.svg",
                        "isPro": false,
                        "fullname": "Yiyu Wang",
                        "user": "Curya",
                        "type": "user"
                    },
                    "name": "Yiyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:11.706Z",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6806",
                    "name": "Xue Yang",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6807",
                    "name": "Longyue Wang",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6808",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a6809",
                    "name": "Kaifu Zhang",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a680a",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "684a33989b38e1e5a33a680b",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T14:35:15.000Z",
            "submittedOnDailyAt": "2025-06-12T00:38:07.422Z",
            "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
            "submittedOnDailyBy": {
                "_id": "639c379cdb7c5f35004066cb",
                "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
                "isPro": false,
                "fullname": "Zhenran Xu",
                "user": "imryanxu",
                "type": "user"
            },
            "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.",
            "upvotes": 30,
            "discussionId": "684a33989b38e1e5a33a680c",
            "projectPage": "https://github.com/AIDC-AI/ComfyUI-Copilot",
            "githubRepo": "https://github.com/AIDC-AI/ComfyUI-Copilot",
            "ai_summary": "ComfyUI-R1, a large reasoning model for automated workflow generation, demonstrates superior performance in creating AI art workflows through long chain-of-thought reasoning and reinforcement learning.",
            "ai_keywords": [
                "modular workflows",
                "ComfyUI",
                "large reasoning model",
                "automated workflow generation",
                "chain-of-thought (CoT) reasoning",
                "node selection",
                "workflow planning",
                "code-level workflow representation",
                "CoT fine-tuning",
                "reinforcement learning",
                "fine-grained rule-metric hybrid reward",
                "format validity",
                "structural integrity",
                "node-level fidelity",
                "GPT-4o",
                "Claude series",
                "pass rate",
                "node-level F1 scores",
                "graph-level F1 scores",
                "intricate workflows",
                "diverse nodes",
                "qualitative comparison",
                "AI art creation"
            ]
        },
        "publishedAt": "2025-06-11T10:35:15.000Z",
        "title": "ComfyUI-R1: Exploring Reasoning Models for Workflow Generation",
        "summary": "AI-generated content has evolved from monolithic models to modular workflows,\nparticularly on platforms like ComfyUI, enabling customization in creative\npipelines. However, crafting effective workflows requires great expertise to\norchestrate numerous specialized components, presenting a steep learning curve\nfor users. To address this challenge, we introduce ComfyUI-R1, the first large\nreasoning model for automated workflow generation. Starting with our curated\ndataset of 4K workflows, we construct long chain-of-thought (CoT) reasoning\ndata, including node selection, workflow planning, and code-level workflow\nrepresentation. ComfyUI-R1 is trained through a two-stage framework: (1) CoT\nfine-tuning for cold start, adapting models to the ComfyUI domain; (2)\nreinforcement learning for incentivizing reasoning capability, guided by a\nfine-grained rule-metric hybrid reward, ensuring format validity, structural\nintegrity, and node-level fidelity. Experiments show that our 7B-parameter\nmodel achieves a 97\\% format validity rate, along with high pass rate,\nnode-level and graph-level F1 scores, significantly surpassing prior\nstate-of-the-art methods that employ leading closed-source models such as\nGPT-4o and Claude series. Further analysis highlights the critical role of the\nreasoning process and the advantage of transforming workflows into code.\nQualitative comparison reveals our strength in synthesizing intricate workflows\nwith diverse nodes, underscoring the potential of long CoT reasoning in AI art\ncreation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09790.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "639c379cdb7c5f35004066cb",
            "avatarUrl": "/avatars/3e435506ee85aa7d2d0ec2174a07462f.svg",
            "fullname": "Zhenran Xu",
            "name": "imryanxu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09995",
            "authors": [
                {
                    "_id": "684a39639b38e1e5a33a6837",
                    "name": "Yuanpeng Tu",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a6838",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a6839",
                    "user": {
                        "_id": "644a1b6401e18bf93a6f45c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
                        "isPro": false,
                        "fullname": "xichen",
                        "user": "xichenhku",
                        "type": "user"
                    },
                    "name": "Xi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:07.608Z",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a683a",
                    "name": "Xiang Bai",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a683b",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "684a39639b38e1e5a33a683c",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:59:53.000Z",
            "submittedOnDailyAt": "2025-06-12T00:50:19.796Z",
            "title": "PlayerOne: Egocentric World Simulator",
            "submittedOnDailyBy": {
                "_id": "644a1b6401e18bf93a6f45c1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
                "isPro": false,
                "fullname": "xichen",
                "user": "xichenhku",
                "type": "user"
            },
            "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.",
            "upvotes": 25,
            "discussionId": "684a39639b38e1e5a33a683d",
            "projectPage": "https://playerone-hku.github.io/",
            "ai_summary": "PlayerOne is an egocentric realistic world simulator that constructs and generates videos from user-captured images, using a coarse-to-fine training pipeline and advanced motion injection and reconstruction frameworks.",
            "ai_keywords": [
                "egocentric realistic world simulator",
                "coarse-to-fine pipeline",
                "pretraining",
                "finetuning",
                "synchronous motion-video data",
                "automatic construction pipeline",
                "part-disentangled motion injection",
                "joint reconstruction framework",
                "4D scene",
                "video frames",
                "scene consistency",
                "long-form video generation",
                "worldconsistent modeling"
            ]
        },
        "publishedAt": "2025-06-11T13:59:53.000Z",
        "title": "PlayerOne: Egocentric World Simulator",
        "summary": "We introduce PlayerOne, the first egocentric realistic world simulator,\nfacilitating immersive and unrestricted exploration within vividly dynamic\nenvironments. Given an egocentric scene image from the user, PlayerOne can\naccurately construct the corresponding world and generate egocentric videos\nthat are strictly aligned with the real scene human motion of the user captured\nby an exocentric camera. PlayerOne is trained in a coarse-to-fine pipeline that\nfirst performs pretraining on large-scale egocentric text-video pairs for\ncoarse-level egocentric understanding, followed by finetuning on synchronous\nmotion-video data extracted from egocentric-exocentric video datasets with our\nautomatic construction pipeline. Besides, considering the varying importance of\ndifferent components, we design a part-disentangled motion injection scheme,\nenabling precise control of part-level movements. In addition, we devise a\njoint reconstruction framework that progressively models both the 4D scene and\nvideo frames, ensuring scene consistency in the long-form video generation.\nExperimental results demonstrate its great generalization ability in precise\ncontrol of varying human movements and worldconsistent modeling of diverse\nscenarios. It marks the first endeavor into egocentric real-world simulation\nand can pave the way for the community to delve into fresh frontiers of world\nmodeling and its diverse applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09995.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a1b6401e18bf93a6f45c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
            "fullname": "xichen",
            "name": "xichenhku",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 43
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08570",
            "authors": [
                {
                    "_id": "684982b5d546b83e67deb341",
                    "user": {
                        "_id": "64de1ffa016e232d2eed1f23",
                        "avatarUrl": "/avatars/6e6aa0984302254d28a659529f874740.svg",
                        "isPro": false,
                        "fullname": "Or Tal",
                        "user": "ortal1602",
                        "type": "user"
                    },
                    "name": "Or Tal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:30.762Z",
                    "hidden": false
                },
                {
                    "_id": "684982b5d546b83e67deb342",
                    "user": {
                        "_id": "66f526d04970b1adc2651d7e",
                        "avatarUrl": "/avatars/2dd952cba07867a9e9ab443b37052a95.svg",
                        "isPro": false,
                        "fullname": "Felix Kreuk",
                        "user": "felixkreuk1",
                        "type": "user"
                    },
                    "name": "Felix Kreuk",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-11T13:20:53.749Z",
                    "hidden": false
                },
                {
                    "_id": "684982b5d546b83e67deb343",
                    "name": "Yossi Adi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T08:37:45.000Z",
            "submittedOnDailyAt": "2025-06-12T11:15:01.307Z",
            "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation",
            "submittedOnDailyBy": {
                "_id": "6547411a9295970f878aa52e",
                "avatarUrl": "/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg",
                "isPro": false,
                "fullname": "Michael Hassid",
                "user": "hassid",
                "type": "user"
            },
            "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly across many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and pinpoint which design choices most\ninfluence performance. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\nFlow-Matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM",
            "upvotes": 25,
            "discussionId": "684982b5d546b83e67deb344",
            "ai_summary": "A systematic comparison of Auto-Regressive decoding and Conditional Flow-Matching in text-to-music generation highlights distinct strengths and limitations of each modeling paradigm.",
            "ai_keywords": [
                "Auto-Regressive decoding",
                "Conditional Flow-Matching",
                "generation quality",
                "robustness",
                "scalability",
                "textual conditioning",
                "temporal alignment",
                "audio inpainting"
            ]
        },
        "publishedAt": "2025-06-10T04:37:45.000Z",
        "title": "Auto-Regressive vs Flow-Matching: a Comparative Study of Modeling\n  Paradigms for Text-to-Music Generation",
        "summary": "Recent progress in text-to-music generation has enabled models to synthesize\nhigh-quality musical segments, full compositions, and even respond to\nfine-grained control signals, e.g. chord progressions. State-of-the-art (SOTA)\nsystems differ significantly across many dimensions, such as training datasets,\nmodeling paradigms, and architectural choices. This diversity complicates\nefforts to evaluate models fairly and pinpoint which design choices most\ninfluence performance. While factors like data and architecture are important,\nin this study we focus exclusively on the modeling paradigm. We conduct a\nsystematic empirical analysis to isolate its effects, offering insights into\nassociated trade-offs and emergent behaviors that can guide future\ntext-to-music generation systems. Specifically, we compare the two arguably\nmost common modeling paradigms: Auto-Regressive decoding and Conditional\nFlow-Matching. We conduct a controlled comparison by training all models from\nscratch using identical datasets, training configurations, and similar backbone\narchitectures. Performance is evaluated across multiple axes, including\ngeneration quality, robustness to inference configurations, scalability,\nadherence to both textual and temporally aligned conditioning, and editing\ncapabilities in the form of audio inpainting. This comparative study sheds\nlight on distinct strengths and limitations of each paradigm, providing\nactionable insights that can inform future architectural and training decisions\nin the evolving landscape of text-to-music generation. Audio sampled examples\nare available at: https://huggingface.co/spaces/ortal1602/ARvsFM",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08570.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6547411a9295970f878aa52e",
            "avatarUrl": "/avatars/6e240f0add27bf1a6c04a9618eccdf83.svg",
            "fullname": "Michael Hassid",
            "name": "hassid",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.08889",
            "authors": [
                {
                    "_id": "684a39599b38e1e5a33a6822",
                    "name": "Yizhao Gao",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a6823",
                    "name": "Shuming Guo",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a6824",
                    "name": "Shijie Cao",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a6825",
                    "name": "Yuqing Xia",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a6826",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a6827",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a6828",
                    "name": "Lingxiao Ma",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a6829",
                    "name": "Yutao Sun",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a682a",
                    "name": "Tianzhu Ye",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a682b",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a682c",
                    "name": "Hayden Kwok-Hay So",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a682d",
                    "name": "Yu Hua",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a682e",
                    "name": "Ting Cao",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a682f",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "684a39599b38e1e5a33a6830",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T15:17:26.000Z",
            "submittedOnDailyAt": "2025-06-12T00:54:43.454Z",
            "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
            "submittedOnDailyBy": {
                "_id": "661c96f48921f03a9dae04c3",
                "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
                "isPro": false,
                "fullname": "Yizhao Gao",
                "user": "LongMountain",
                "type": "user"
            },
            "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.",
            "upvotes": 18,
            "discussionId": "684a39599b38e1e5a33a6833",
            "githubRepo": "https://github.com/microsoft/SeerAttention",
            "ai_summary": "SeerAttention-R is a sparse attention framework for reasoning models that maintains high accuracy and achieves significant speedups through optimized sparse decoding kernels.",
            "ai_keywords": [
                "sparse attention",
                "reasoning models",
                "self-distilled gating mechanism",
                "query pooling",
                "lightweight plug-in gating",
                "AIME benchmark",
                "TileLang",
                "sparse decoding kernel",
                "FlashAttention-3",
                "H100 GPU"
            ]
        },
        "publishedAt": "2025-06-10T11:17:26.000Z",
        "title": "SeerAttention-R: Sparse Attention Adaptation for Long Reasoning",
        "summary": "We introduce SeerAttention-R, a sparse attention framework specifically\ntailored for the long decoding of reasoning models. Extended from\nSeerAttention, SeerAttention-R retains the design of learning attention\nsparsity through a self-distilled gating mechanism, while removing query\npooling to accommodate auto-regressive decoding. With a lightweight plug-in\ngating, SeerAttention-R is flexible and can be easily integrated into existing\npretrained model without modifying the original parameters. We demonstrate that\nSeerAttention-R, trained on just 0.4B tokens, maintains near-lossless reasoning\naccuracy with 4K token budget in AIME benchmark under large sparse attention\nblock sizes (64/128). Using TileLang, we develop a highly optimized sparse\ndecoding kernel that achieves near-theoretical speedups of up to 9x over\nFlashAttention-3 on H100 GPU at 90% sparsity. Code is available at:\nhttps://github.com/microsoft/SeerAttention.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08889.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661c96f48921f03a9dae04c3",
            "avatarUrl": "/avatars/c486a45ffea7d1a8c72bd8512014b07e.svg",
            "fullname": "Yizhao Gao",
            "name": "LongMountain",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09003",
            "authors": [
                {
                    "_id": "6848eed742e4f9106973f2cf",
                    "user": {
                        "_id": "64c38871f9cd765462fa1a17",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
                        "isPro": false,
                        "fullname": "Lei Zhang",
                        "user": "Lemoncoke",
                        "type": "user"
                    },
                    "name": "Lei Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:42:47.797Z",
                    "hidden": false
                },
                {
                    "_id": "6848eed742e4f9106973f2d0",
                    "name": "Jiaxi Yang",
                    "hidden": false
                },
                {
                    "_id": "6848eed742e4f9106973f2d1",
                    "name": "Min Yang",
                    "hidden": false
                },
                {
                    "_id": "6848eed742e4f9106973f2d2",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6848eed742e4f9106973f2d3",
                    "name": "Mouxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "6848eed742e4f9106973f2d4",
                    "name": "Jiajun Zhang",
                    "hidden": false
                },
                {
                    "_id": "6848eed742e4f9106973f2d5",
                    "name": "Zeyu Cui",
                    "hidden": false
                },
                {
                    "_id": "6848eed742e4f9106973f2d6",
                    "name": "Binyuan Hui",
                    "hidden": false
                },
                {
                    "_id": "6848eed742e4f9106973f2d7",
                    "name": "Junyang Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:23:33.000Z",
            "submittedOnDailyAt": "2025-06-12T00:17:18.390Z",
            "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
            "submittedOnDailyBy": {
                "_id": "64c38871f9cd765462fa1a17",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
                "isPro": false,
                "fullname": "Lei Zhang",
                "user": "Lemoncoke",
                "type": "user"
            },
            "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).",
            "upvotes": 14,
            "discussionId": "6848eed842e4f9106973f2d8",
            "githubRepo": "https://github.com/Hambaobao/SWE-Flow",
            "ai_summary": "A novel data synthesis framework, SWE-Flow, uses unit tests to automatically infer development steps and generate a structured schedule for Test-Driven Development (TDD), significantly improving the performance of open models fine-tuned on real-world projects.",
            "ai_keywords": [
                "Test-Driven Development (TDD)",
                "Runtime Dependency Graph (RDG)",
                "SWE-Flow",
                "unit tests",
                "development schedule",
                "SWE-Flow-Eval",
                "fine-tuning",
                "open model"
            ]
        },
        "publishedAt": "2025-06-10T13:23:33.000Z",
        "title": "SWE-Flow: Synthesizing Software Engineering Data in a Test-Driven Manner",
        "summary": "We introduce **SWE-Flow**, a novel data synthesis framework grounded in\nTest-Driven Development (TDD). Unlike existing software engineering data that\nrely on human-submitted issues, **SWE-Flow** automatically infers incremental\ndevelopment steps directly from unit tests, which inherently encapsulate\nhigh-level requirements. The core of **SWE-Flow** is the construction of a\nRuntime Dependency Graph (RDG), which precisely captures function interactions,\nenabling the generation of a structured, step-by-step *development schedule*.\nAt each step, **SWE-Flow** produces a partial codebase, the corresponding unit\ntests, and the necessary code modifications, resulting in fully verifiable TDD\ntasks. With this approach, we generated 16,061 training instances and 2,020\ntest instances from real-world GitHub projects, creating the **SWE-Flow-Eval**\nbenchmark. Our experiments show that fine-tuning open model on this dataset\nsignificantly improves performance in TDD-based coding. To facilitate further\nresearch, we release all code, datasets, models, and Docker images at\n[Github](https://github.com/Hambaobao/SWE-Flow).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09003.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64c38871f9cd765462fa1a17",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c38871f9cd765462fa1a17/yuIlVcqeDlQVKsUF8uEl3.jpeg",
            "fullname": "Lei Zhang",
            "name": "Lemoncoke",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09984",
            "authors": [
                {
                    "_id": "684a49fa9b38e1e5a33a6884",
                    "name": "Zhenzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "684a49fa9b38e1e5a33a6885",
                    "name": "Jiaqi Yang",
                    "hidden": false
                },
                {
                    "_id": "684a49fa9b38e1e5a33a6886",
                    "name": "Jianwen Jiang",
                    "hidden": false
                },
                {
                    "_id": "684a49fa9b38e1e5a33a6887",
                    "name": "Chao Liang",
                    "hidden": false
                },
                {
                    "_id": "684a49fa9b38e1e5a33a6888",
                    "user": {
                        "_id": "64802fcdcc9e514b3b031244",
                        "avatarUrl": "/avatars/cc5979008bdb21a2be9575865dce909b.svg",
                        "isPro": false,
                        "fullname": "Gaojie Lin",
                        "user": "lingaojie",
                        "type": "user"
                    },
                    "name": "Gaojie Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:41:52.916Z",
                    "hidden": false
                },
                {
                    "_id": "684a49fa9b38e1e5a33a6889",
                    "name": "Zerong Zheng",
                    "hidden": false
                },
                {
                    "_id": "684a49fa9b38e1e5a33a688a",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "684a49fa9b38e1e5a33a688b",
                    "name": "Dahua Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:57:09.000Z",
            "submittedOnDailyAt": "2025-06-12T02:02:32.983Z",
            "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
            "submittedOnDailyBy": {
                "_id": "6519346a186bc3b6997c1aaf",
                "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
                "isPro": false,
                "fullname": "Zhenzhi Wang",
                "user": "zhenzhiwang",
                "type": "user"
            },
            "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
            "upvotes": 11,
            "discussionId": "684a49fa9b38e1e5a33a688c",
            "ai_summary": "A new framework enables precise, per-identity control of multiple concepts in end-to-end human animation by enforcing region-specific binding of multi-modal conditions.",
            "ai_keywords": [
                "human animation",
                "multi-modal conditions",
                "mask predictor",
                "denoised video",
                "layout information",
                "local audio condition",
                "controllable multi-concept videos",
                "explicit layout control"
            ]
        },
        "publishedAt": "2025-06-11T13:57:09.000Z",
        "title": "InterActHuman: Multi-Concept Human Animation with Layout-Aligned Audio\n  Conditions",
        "summary": "End-to-end human animation with rich multi-modal conditions, e.g., text,\nimage and audio has achieved remarkable advancements in recent years. However,\nmost existing methods could only animate a single subject and inject conditions\nin a global manner, ignoring scenarios that multiple concepts could appears in\nthe same video with rich human-human interactions and human-object\ninteractions. Such global assumption prevents precise and per-identity control\nof multiple concepts including humans and objects, therefore hinders\napplications. In this work, we discard the single-entity assumption and\nintroduce a novel framework that enforces strong, region-specific binding of\nconditions from modalities to each identity's spatiotemporal footprint. Given\nreference images of multiple concepts, our method could automatically infer\nlayout information by leveraging a mask predictor to match appearance cues\nbetween the denoised video and each reference appearance. Furthermore, we\ninject local audio condition into its corresponding region to ensure\nlayout-aligned modality matching in a iterative manner. This design enables the\nhigh-quality generation of controllable multi-concept human-centric videos.\nEmpirical results and ablation studies validate the effectiveness of our\nexplicit layout control for multi-modal conditions compared to implicit\ncounterparts and other existing methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09984.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6519346a186bc3b6997c1aaf",
            "avatarUrl": "/avatars/8981bd278962da50f9bbfb92c2abe2bf.svg",
            "fullname": "Zhenzhi Wang",
            "name": "zhenzhiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09501",
            "authors": [
                {
                    "_id": "684a6e2f9b38e1e5a33a68ae",
                    "name": "Jiayi Yuan",
                    "hidden": false
                },
                {
                    "_id": "684a6e2f9b38e1e5a33a68af",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "684a6e2f9b38e1e5a33a68b0",
                    "name": "Xinheng Ding",
                    "hidden": false
                },
                {
                    "_id": "684a6e2f9b38e1e5a33a68b1",
                    "name": "Wenya Xie",
                    "hidden": false
                },
                {
                    "_id": "684a6e2f9b38e1e5a33a68b2",
                    "name": "Yu-Jhe Li",
                    "hidden": false
                },
                {
                    "_id": "684a6e2f9b38e1e5a33a68b3",
                    "name": "Wentian Zhao",
                    "hidden": false
                },
                {
                    "_id": "684a6e2f9b38e1e5a33a68b4",
                    "name": "Kun Wan",
                    "hidden": false
                },
                {
                    "_id": "684a6e2f9b38e1e5a33a68b5",
                    "name": "Jing Shi",
                    "hidden": false
                },
                {
                    "_id": "684a6e2f9b38e1e5a33a68b6",
                    "name": "Xia Hu",
                    "hidden": false
                },
                {
                    "_id": "684a6e2f9b38e1e5a33a68b7",
                    "name": "Zirui Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T08:23:53.000Z",
            "submittedOnDailyAt": "2025-06-12T14:51:07.370Z",
            "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "64b7879a6ab5d14ca7f9bdf4",
                "avatarUrl": "/avatars/48990fe5b18ab17a3dedacdcc6ee3f3a.svg",
                "isPro": false,
                "fullname": "Jiayi Yuan",
                "user": "jy-yuan",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility.",
            "upvotes": 10,
            "discussionId": "684a6e309b38e1e5a33a68b8",
            "githubRepo": "https://github.com/nanomaoli/llm_reproducibility",
            "ai_summary": "The study investigates reproducibility issues in Large Language Models (LLMs) arising from hardware and precision variations, proposing a lightweight inference pipeline to enhance numerical stability while maintaining memory efficiency.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "reasoning models",
                "DeepSeek-R1-Distill-Qwen-7B",
                "floating-point arithmetic",
                "floating-point precision",
                "greedy decoding",
                "LayerCast",
                "FP32"
            ]
        },
        "publishedAt": "2025-06-11T04:23:53.000Z",
        "title": "Give Me FP32 or Give Me Death? Challenges and Solutions for Reproducible\n  Reasoning",
        "summary": "Large Language Models (LLMs) are now integral across various domains and have\ndemonstrated impressive performance. Progress, however, rests on the premise\nthat benchmark scores are both accurate and reproducible. We demonstrate that\nthe reproducibility of LLM performance is fragile: changing system\nconfiguration such as evaluation batch size, GPU count, and GPU version can\nintroduce significant difference in the generated responses. This issue is\nespecially pronounced in reasoning models, where minor rounding differences in\nearly tokens can cascade into divergent chains of thought, ultimately affecting\naccuracy. For instance, under bfloat16 precision with greedy decoding, a\nreasoning model like DeepSeek-R1-Distill-Qwen-7B can exhibit up to 9% variation\nin accuracy and 9,000 tokens difference in response length due to differences\nin GPU count, type, and evaluation batch size. We trace the root cause of this\nvariability to the non-associative nature of floating-point arithmetic under\nlimited numerical precision. This work presents the first systematic\ninvestigation into how numerical precision affects reproducibility in LLM\ninference. Through carefully controlled experiments across various hardware,\nsoftware, and precision settings, we quantify when and how model outputs\ndiverge. Our analysis reveals that floating-point precision -- while critical\nfor reproducibility -- is often neglected in evaluation practices. Inspired by\nthis, we develop a lightweight inference pipeline, dubbed LayerCast, that\nstores weights in 16-bit precision but performs all computations in FP32,\nbalancing memory efficiency with numerical stability. Code is available at\nhttps://github.com/nanomaoli/llm_reproducibility.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09501.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b7879a6ab5d14ca7f9bdf4",
            "avatarUrl": "/avatars/48990fe5b18ab17a3dedacdcc6ee3f3a.svg",
            "fullname": "Jiayi Yuan",
            "name": "jy-yuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.05309",
            "authors": [
                {
                    "_id": "68483359308cb7e626e80e88",
                    "user": {
                        "_id": "67fa8526e1bb5094301f17c9",
                        "avatarUrl": "/avatars/b0ef0f77dc14a498919957f1a6d8688b.svg",
                        "isPro": false,
                        "fullname": "Niv Eckhaus",
                        "user": "niveck",
                        "type": "user"
                    },
                    "name": "Niv Eckhaus",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-10T15:20:12.219Z",
                    "hidden": false
                },
                {
                    "_id": "68483359308cb7e626e80e89",
                    "name": "Uri Berger",
                    "hidden": false
                },
                {
                    "_id": "68483359308cb7e626e80e8a",
                    "name": "Gabriel Stanovsky",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/DwNbnBFcX3hOkBzFNkygd.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/L5pUeRdEp529o5w9O4ec8.gif",
                "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/2Dj-HZN0Zw7R8iditxIR-.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/DVIpPGCmA1yqc7BIuaaZc.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/CLZZ513nPDllOTvwAs_Kl.png"
            ],
            "publishedAt": "2025-06-05T17:53:44.000Z",
            "submittedOnDailyAt": "2025-06-12T11:48:56.909Z",
            "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games",
            "submittedOnDailyBy": {
                "_id": "67fa8526e1bb5094301f17c9",
                "avatarUrl": "/avatars/b0ef0f77dc14a498919957f1a6d8688b.svg",
                "isPro": false,
                "fullname": "Niv Eckhaus",
                "user": "niveck",
                "type": "user"
            },
            "summary": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated.",
            "upvotes": 10,
            "discussionId": "68483359308cb7e626e80e8b",
            "projectPage": "https://niveck.github.io/Time-to-Talk/",
            "githubRepo": "https://github.com/niveck/LLMafia",
            "ai_summary": "An adaptive asynchronous LLM-agent performs similarly to human players in online Mafia games, demonstrating the potential for integrating LLMs into realistic group settings with complex social dynamics.",
            "ai_keywords": [
                "asynchronous communication",
                "LLM-agent",
                "online Mafia games",
                "social dynamics"
            ]
        },
        "publishedAt": "2025-06-05T13:53:44.000Z",
        "title": "Time to Talk: LLM Agents for Asynchronous Group Communication in Mafia\n  Games",
        "summary": "LLMs are used predominantly in synchronous communication, where a human user\nand a model communicate in alternating turns. In contrast, many real-world\nsettings are inherently asynchronous. For example, in group chats, online team\nmeetings, or social games, there is no inherent notion of turns; therefore, the\ndecision of when to speak forms a crucial part of the participant's decision\nmaking. In this work, we develop an adaptive asynchronous LLM-agent which, in\naddition to determining what to say, also decides when to say it. To evaluate\nour agent, we collect a unique dataset of online Mafia games, including both\nhuman participants, as well as our asynchronous agent. Overall, our agent\nperforms on par with human players, both in game performance, as well as in its\nability to blend in with the other human players. Our analysis shows that the\nagent's behavior in deciding when to speak closely mirrors human patterns,\nalthough differences emerge in message content. We release all our data and\ncode to support and encourage further research for more realistic asynchronous\ncommunication between LLM agents. This work paves the way for integration of\nLLMs into realistic human group settings, from assistance in team discussions\nto educational and professional environments where complex social dynamics must\nbe navigated.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/DwNbnBFcX3hOkBzFNkygd.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/L5pUeRdEp529o5w9O4ec8.gif",
            "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/2Dj-HZN0Zw7R8iditxIR-.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/DVIpPGCmA1yqc7BIuaaZc.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67fa8526e1bb5094301f17c9/CLZZ513nPDllOTvwAs_Kl.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05309.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67fa8526e1bb5094301f17c9",
            "avatarUrl": "/avatars/b0ef0f77dc14a498919957f1a6d8688b.svg",
            "fullname": "Niv Eckhaus",
            "name": "niveck",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09937",
            "authors": [
                {
                    "_id": "684a3d639b38e1e5a33a686d",
                    "name": "Qiao Gu",
                    "hidden": false
                },
                {
                    "_id": "684a3d639b38e1e5a33a686e",
                    "name": "Yuanliang Ju",
                    "hidden": false
                },
                {
                    "_id": "684a3d639b38e1e5a33a686f",
                    "name": "Shengxiang Sun",
                    "hidden": false
                },
                {
                    "_id": "684a3d639b38e1e5a33a6870",
                    "name": "Igor Gilitschenski",
                    "hidden": false
                },
                {
                    "_id": "684a3d639b38e1e5a33a6871",
                    "name": "Haruki Nishimura",
                    "hidden": false
                },
                {
                    "_id": "684a3d639b38e1e5a33a6872",
                    "name": "Masha Itkina",
                    "hidden": false
                },
                {
                    "_id": "684a3d639b38e1e5a33a6873",
                    "name": "Florian Shkurti",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T16:59:13.000Z",
            "submittedOnDailyAt": "2025-06-12T01:08:05.216Z",
            "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "63d1df92f7f31a66a2d7292c",
                "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
                "isPro": false,
                "fullname": "Qiao Gu",
                "user": "guqiao",
                "type": "user"
            },
            "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\nand pi_0-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.",
            "upvotes": 7,
            "discussionId": "684a3d639b38e1e5a33a6874",
            "ai_summary": "SAFE is a failure detector for vision-language-action models that generalizes to unseen tasks by learning from high-level internal features of the models.",
            "ai_keywords": [
                "vision-language-action models",
                "VLAs",
                "multitask failure detection",
                "failure detector",
                "feature space",
                "high-level knowledge",
                "scalar prediction",
                "rollout",
                "conformal prediction"
            ]
        },
        "publishedAt": "2025-06-11T12:59:13.000Z",
        "title": "SAFE: Multitask Failure Detection for Vision-Language-Action Models",
        "summary": "While vision-language-action models (VLAs) have shown promising robotic\nbehaviors across a diverse set of manipulation tasks, they achieve limited\nsuccess rates when deployed on novel tasks out-of-the-box. To allow these\npolicies to safely interact with their environments, we need a failure detector\nthat gives a timely alert such that the robot can stop, backtrack, or ask for\nhelp. However, existing failure detectors are trained and tested only on one or\na few specific tasks, while VLAs require the detector to generalize and detect\nfailures also in unseen tasks and novel environments. In this paper, we\nintroduce the multitask failure detection problem and propose SAFE, a failure\ndetector for generalist robot policies such as VLAs. We analyze the VLA feature\nspace and find that VLAs have sufficient high-level knowledge about task\nsuccess and failure, which is generic across different tasks. Based on this\ninsight, we design SAFE to learn from VLA internal features and predict a\nsingle scalar indicating the likelihood of task failure. SAFE is trained on\nboth successful and failed rollouts, and is evaluated on unseen tasks. SAFE is\ncompatible with different policy architectures. We test it on OpenVLA, pi_0,\nand pi_0-FAST in both simulated and real-world environments extensively. We\ncompare SAFE with diverse baselines and show that SAFE achieves\nstate-of-the-art failure detection performance and the best trade-off between\naccuracy and detection time using conformal prediction. More qualitative\nresults can be found at https://vla-safe.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09937.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d1df92f7f31a66a2d7292c",
            "avatarUrl": "/avatars/eb3339c1f2c82742b518b8a7f142e99a.svg",
            "fullname": "Qiao Gu",
            "name": "guqiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09736",
            "authors": [
                {
                    "_id": "684ae230dbd21a9cc27b10ba",
                    "name": "Yuting Li",
                    "hidden": false
                },
                {
                    "_id": "684ae230dbd21a9cc27b10bb",
                    "name": "Lai Wei",
                    "hidden": false
                },
                {
                    "_id": "684ae230dbd21a9cc27b10bc",
                    "name": "Kaipeng Zheng",
                    "hidden": false
                },
                {
                    "_id": "684ae230dbd21a9cc27b10bd",
                    "name": "Jingyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "684ae230dbd21a9cc27b10be",
                    "name": "Linghe Kong",
                    "hidden": false
                },
                {
                    "_id": "684ae230dbd21a9cc27b10bf",
                    "name": "Lichao Sun",
                    "hidden": false
                },
                {
                    "_id": "684ae230dbd21a9cc27b10c0",
                    "name": "Weiran Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T13:39:46.000Z",
            "submittedOnDailyAt": "2025-06-12T12:53:09.040Z",
            "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "64a16b1aeacb4b50ba1c889d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
                "isPro": false,
                "fullname": "Lai Wei",
                "user": "WaltonFuture",
                "type": "user"
            },
            "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.",
            "upvotes": 6,
            "discussionId": "684ae231dbd21a9cc27b10c1",
            "projectPage": "https://github.com/YutingLi0606/Vision-Matters",
            "githubRepo": "https://github.com/YutingLi0606/Vision-Matters",
            "ai_summary": "Visual perturbation enhances multimodal large language models' mathematical reasoning without algorithmic modifications or additional data.",
            "ai_keywords": [
                "multimodal large language models",
                "visual processing",
                "image captions",
                "perceptual robustness",
                "distractor concatenation",
                "dominance-preserving mixup",
                "random rotation",
                "post-training pipelines",
                "mathematical reasoning",
                "open-source RL-tuned models",
                "Qwen2.5-VL-7B",
                "ablation studies",
                "visual reasoning"
            ]
        },
        "publishedAt": "2025-06-11T09:39:46.000Z",
        "title": "Vision Matters: Simple Visual Perturbations Can Boost Multimodal Math\n  Reasoning",
        "summary": "Despite the rapid progress of multimodal large language models (MLLMs), they\nhave largely overlooked the importance of visual processing. In a simple yet\nrevealing experiment, we interestingly find that language-only models, when\nprovided with image captions, can achieve comparable or even better performance\nthan MLLMs that consume raw visual inputs. This suggests that current MLLMs may\ngenerate accurate visual descriptions but fail to effectively integrate them\nduring reasoning. Motivated by this, we propose a simple visual perturbation\nframework that enhances perceptual robustness without requiring algorithmic\nmodifications or additional training data. Our approach introduces three\ntargeted perturbations: distractor concatenation, dominance-preserving mixup,\nand random rotation, that can be easily integrated into existing post-training\npipelines including SFT, DPO, and GRPO. Through extensive experiments across\nmultiple datasets, we demonstrate consistent improvements in mathematical\nreasoning performance, with gains comparable to those achieved through\nalgorithmic changes. Additionally, we achieve competitive performance among\nopen-source 7B RL-tuned models by training Qwen2.5-VL-7B with visual\nperturbation. Through comprehensive ablation studies, we analyze the\neffectiveness of different perturbation strategies, revealing that each\nperturbation type contributes uniquely to different aspects of visual\nreasoning. Our findings highlight the critical role of visual perturbation in\nmultimodal mathematical reasoning: better reasoning begins with better seeing.\nOur code is available at https://github.com/YutingLi0606/Vision-Matters.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09736.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "fullname": "Lai Wei",
            "name": "WaltonFuture",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.08008",
            "authors": [
                {
                    "_id": "684ae06cdbd21a9cc27b0ecf",
                    "name": "Stephanie Fu",
                    "hidden": false
                },
                {
                    "_id": "684ae06cdbd21a9cc27b0ed0",
                    "name": "Tyler Bonnen",
                    "hidden": false
                },
                {
                    "_id": "684ae06cdbd21a9cc27b0ed1",
                    "name": "Devin Guillory",
                    "hidden": false
                },
                {
                    "_id": "684ae06cdbd21a9cc27b0ed2",
                    "name": "Trevor Darrell",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T17:59:54.000Z",
            "submittedOnDailyAt": "2025-06-12T12:52:59.641Z",
            "title": "Hidden in plain sight: VLMs overlook their visual representations",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.",
            "upvotes": 5,
            "discussionId": "684ae06ddbd21a9cc27b0ed3",
            "ai_summary": "Vision language models perform worse than their visual encoders on vision-centric tasks due to ineffective use of visual information and language priors.",
            "ai_keywords": [
                "vision language models",
                "visual encoders",
                "depth estimation",
                "correspondence",
                "vision representations",
                "task prompt",
                "language model",
                "open-source VLMs",
                "visual understanding"
            ]
        },
        "publishedAt": "2025-06-09T13:59:54.000Z",
        "title": "Hidden in plain sight: VLMs overlook their visual representations",
        "summary": "Language provides a natural interface to specify and evaluate performance on\nvisual tasks. To realize this possibility, vision language models (VLMs) must\nsuccessfully integrate visual and linguistic information. Our work compares\nVLMs to a direct readout of their visual encoders to understand their ability\nto integrate across these modalities. Across a series of vision-centric\nbenchmarks (e.g., depth estimation, correspondence), we find that VLMs perform\nsubstantially worse than their visual encoders, dropping to near-chance\nperformance. We investigate these results through a series of analyses across\nthe entire VLM: namely 1) the degradation of vision representations, 2)\nbrittleness to task prompt, and 3) the language model's role in solving the\ntask. We find that the bottleneck in performing these vision-centric tasks lies\nin this third category; VLMs are not effectively using visual information\neasily accessible throughout the entire model, and they inherit the language\npriors present in the LLM. Our work helps diagnose the failure modes of\nopen-source VLMs, and presents a series of evaluations useful for future\ninvestigations into visual understanding within VLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08008.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 85
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09278",
            "authors": [
                {
                    "_id": "684ae1efdbd21a9cc27b0f19",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f1a",
                    "name": "Nikhil Keetha",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f1b",
                    "name": "Chenwei Lyu",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f1c",
                    "name": "Bhuvan Jhamb",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f1d",
                    "name": "Yutian Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f1e",
                    "name": "Yuheng Qiu",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f1f",
                    "name": "Jay Karhade",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f20",
                    "name": "Shreyas Jha",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f21",
                    "name": "Yaoyu Hu",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f22",
                    "name": "Deva Ramanan",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f23",
                    "name": "Sebastian Scherer",
                    "hidden": false
                },
                {
                    "_id": "684ae1efdbd21a9cc27b0f24",
                    "name": "Wenshan Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64dbacab1855ce11cd61be0f/tnTzVv-MSmz-z1FtUy8m2.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/64dbacab1855ce11cd61be0f/gvHg8x6B_SkfgskBDd_8A.mp4"
            ],
            "publishedAt": "2025-06-10T22:32:13.000Z",
            "submittedOnDailyAt": "2025-06-12T14:44:36.194Z",
            "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow",
            "submittedOnDailyBy": {
                "_id": "64dbacab1855ce11cd61be0f",
                "avatarUrl": "/avatars/22f53e8e520067b1053f8f7e21f5fd25.svg",
                "isPro": false,
                "fullname": "Nikhil Keetha",
                "user": "NikV09",
                "type": "user"
            },
            "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.",
            "upvotes": 4,
            "discussionId": "684ae1efdbd21a9cc27b0f25",
            "projectPage": "https://uniflowmatch.github.io/",
            "githubRepo": "https://github.com/UniFlowMatch/UFM",
            "ai_summary": "A Unified Flow & Matching model (UFM) improves dense image correspondence accuracy and speed by using a transformer architecture for unified data training, outperforming specialized methods for both optical flow and wide-baseline scenarios.",
            "ai_keywords": [
                "transformer architecture",
                "dense image correspondence",
                "visual odometry",
                "3D reconstruction",
                "object association",
                "re-identification",
                "wide-baseline scenarios",
                "optical flow estimation",
                "co-visible pixels",
                "(u",
                "v) flow",
                "coarse-to-fine cost volumes",
                "Unimatch",
                "RoMa"
            ]
        },
        "publishedAt": "2025-06-10T18:32:13.000Z",
        "title": "UFM: A Simple Path towards Unified Dense Correspondence with Flow",
        "summary": "Dense image correspondence is central to many applications, such as visual\nodometry, 3D reconstruction, object association, and re-identification.\nHistorically, dense correspondence has been tackled separately for\nwide-baseline scenarios and optical flow estimation, despite the common goal of\nmatching content between two images. In this paper, we develop a Unified Flow &\nMatching model (UFM), which is trained on unified data for pixels that are\nco-visible in both source and target images. UFM uses a simple, generic\ntransformer architecture that directly regresses the (u,v) flow. It is easier\nto train and more accurate for large flows compared to the typical\ncoarse-to-fine cost volumes in prior work. UFM is 28% more accurate than\nstate-of-the-art flow methods (Unimatch), while also having 62% less error and\n6.7x faster than dense wide-baseline matchers (RoMa). UFM is the first to\ndemonstrate that unified training can outperform specialized approaches across\nboth domains. This result enables fast, general-purpose correspondence and\nopens new directions for multi-modal, long-range, and real-time correspondence\ntasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64dbacab1855ce11cd61be0f/tnTzVv-MSmz-z1FtUy8m2.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/64dbacab1855ce11cd61be0f/gvHg8x6B_SkfgskBDd_8A.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09278.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64dbacab1855ce11cd61be0f",
            "avatarUrl": "/avatars/22f53e8e520067b1053f8f7e21f5fd25.svg",
            "fullname": "Nikhil Keetha",
            "name": "NikV09",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09229",
            "authors": [
                {
                    "_id": "684ae250dbd21a9cc27b114c",
                    "name": "Sungwon Hwang",
                    "hidden": false
                },
                {
                    "_id": "684ae250dbd21a9cc27b114d",
                    "name": "Hyojin Jang",
                    "hidden": false
                },
                {
                    "_id": "684ae250dbd21a9cc27b114e",
                    "name": "Kinam Kim",
                    "hidden": false
                },
                {
                    "_id": "684ae250dbd21a9cc27b114f",
                    "name": "Minho Park",
                    "hidden": false
                },
                {
                    "_id": "684ae250dbd21a9cc27b1150",
                    "name": "Jaegul choo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T20:34:47.000Z",
            "submittedOnDailyAt": "2025-06-12T12:54:25.858Z",
            "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion\n  Models",
            "submittedOnDailyBy": {
                "_id": "642fcfc0a043f0ac7deeaae0",
                "avatarUrl": "/avatars/6cc46dd480cdc0d86c7a509e22782a13.svg",
                "isPro": false,
                "fullname": "Sungwon Hwang",
                "user": "sungwon95",
                "type": "user"
            },
            "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io",
            "upvotes": 3,
            "discussionId": "684ae250dbd21a9cc27b1151",
            "projectPage": "https://crepavideo.github.io",
            "githubRepo": "https://github.com/deepshwang/crepa",
            "ai_summary": "Cross-frame Representation Alignment (CREPA) enhances video diffusion model fine-tuning by improving visual fidelity and semantic coherence across frames using parameter-efficient methods.",
            "ai_keywords": [
                "fine-tuning",
                "Video Diffusion Models (VDMs)",
                "Representation Alignment (REPA)",
                "DiT-based image diffusion models",
                "Cross-frame Representation Alignment (CREPA)",
                "hidden states",
                "external features",
                "CogVideoX-5B",
                "Hunyuan Video",
                "visual fidelity",
                "cross-frame semantic coherence",
                "LoRA"
            ]
        },
        "publishedAt": "2025-06-10T16:34:47.000Z",
        "title": "Cross-Frame Representation Alignment for Fine-Tuning Video Diffusion\n  Models",
        "summary": "Fine-tuning Video Diffusion Models (VDMs) at the user level to generate\nvideos that reflect specific attributes of training data presents notable\nchallenges, yet remains underexplored despite its practical importance.\nMeanwhile, recent work such as Representation Alignment (REPA) has shown\npromise in improving the convergence and quality of DiT-based image diffusion\nmodels by aligning, or assimilating, its internal hidden states with external\npretrained visual features, suggesting its potential for VDM fine-tuning. In\nthis work, we first propose a straightforward adaptation of REPA for VDMs and\nempirically show that, while effective for convergence, it is suboptimal in\npreserving semantic consistency across frames. To address this limitation, we\nintroduce Cross-frame Representation Alignment (CREPA), a novel regularization\ntechnique that aligns hidden states of a frame with external features from\nneighboring frames. Empirical evaluations on large-scale VDMs, including\nCogVideoX-5B and Hunyuan Video, demonstrate that CREPA improves both visual\nfidelity and cross-frame semantic coherence when fine-tuned with\nparameter-efficient methods such as LoRA. We further validate CREPA across\ndiverse datasets with varying attributes, confirming its broad applicability.\nProject page: https://crepavideo.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09229.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642fcfc0a043f0ac7deeaae0",
            "avatarUrl": "/avatars/6cc46dd480cdc0d86c7a509e22782a13.svg",
            "fullname": "Sungwon Hwang",
            "name": "sungwon95",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09669",
            "authors": [
                {
                    "_id": "684ae246dbd21a9cc27b1126",
                    "name": "Lihu Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae246dbd21a9cc27b1127",
                    "name": "Gal Varoquaux",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T12:39:48.000Z",
            "submittedOnDailyAt": "2025-06-12T13:26:30.505Z",
            "title": "Query-Level Uncertainty in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "63d1954e8eaa4831006600ff",
                "avatarUrl": "/avatars/a0380156e0773bf0a6ccc2df6b9ea478.svg",
                "isPro": false,
                "fullname": "Lihu Chen",
                "user": "Lihuchen",
                "type": "user"
            },
            "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called Internal Confidence, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.",
            "upvotes": 2,
            "discussionId": "684ae246dbd21a9cc27b1128",
            "ai_summary": "A method called Internal Confidence is proposed to detect knowledge boundaries in Large Language Models by leveraging layer and token-level self-evaluations, improving adaptive inference and model cascading efficiency.",
            "ai_keywords": [
                "Large Language Models",
                "Query-Level Uncertainty",
                "Internal Confidence",
                "adaptive inference",
                "RAG",
                "slow and deep thinking",
                "abstention mechanism",
                "factual QA",
                "mathematical reasoning",
                "model cascading"
            ]
        },
        "publishedAt": "2025-06-11T08:39:48.000Z",
        "title": "Query-Level Uncertainty in Large Language Models",
        "summary": "It is important for Large Language Models to be aware of the boundary of\ntheir knowledge, the mechanism of identifying known and unknown queries. This\ntype of awareness can help models perform adaptive inference, such as invoking\nRAG, engaging in slow and deep thinking, or adopting the abstention mechanism,\nwhich is beneficial to the development of efficient and trustworthy AI. In this\nwork, we propose a method to detect knowledge boundaries via Query-Level\nUncertainty, which aims to determine if the model is able to address a given\nquery without generating any tokens. To this end, we introduce a novel and\ntraining-free method called Internal Confidence, which leverages\nself-evaluations across layers and tokens. Empirical results on both factual QA\nand mathematical reasoning tasks demonstrate that our internal confidence can\noutperform several baselines. Furthermore, we showcase that our proposed method\ncan be used for efficient RAG and model cascading, which is able to reduce\ninference costs while maintaining performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09669.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d1954e8eaa4831006600ff",
            "avatarUrl": "/avatars/a0380156e0773bf0a6ccc2df6b9ea478.svg",
            "fullname": "Lihu Chen",
            "name": "Lihuchen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.08900",
            "authors": [
                {
                    "_id": "684a96ab9aebf043cf7bdb2e",
                    "user": {
                        "_id": "655b3383ed8df831286969f0",
                        "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
                        "isPro": false,
                        "fullname": "Jos Morano",
                        "user": "j-morano",
                        "type": "user"
                    },
                    "name": "Jos Morano",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-12T12:40:45.632Z",
                    "hidden": false
                },
                {
                    "_id": "684a96ab9aebf043cf7bdb2f",
                    "name": "Botond Fazekas",
                    "hidden": false
                },
                {
                    "_id": "684a96ab9aebf043cf7bdb30",
                    "name": "Emese Skei",
                    "hidden": false
                },
                {
                    "_id": "684a96ab9aebf043cf7bdb31",
                    "name": "Ronald Fecso",
                    "hidden": false
                },
                {
                    "_id": "684a96ab9aebf043cf7bdb32",
                    "name": "Taha Emre",
                    "hidden": false
                },
                {
                    "_id": "684a96ab9aebf043cf7bdb33",
                    "name": "Markus Gumpinger",
                    "hidden": false
                },
                {
                    "_id": "684a96ab9aebf043cf7bdb34",
                    "name": "Georg Faustmann",
                    "hidden": false
                },
                {
                    "_id": "684a96ab9aebf043cf7bdb35",
                    "name": "Marzieh Oghbaie",
                    "hidden": false
                },
                {
                    "_id": "684a96ab9aebf043cf7bdb36",
                    "name": "Ursula Schmidt-Erfurth",
                    "hidden": false
                },
                {
                    "_id": "684a96ab9aebf043cf7bdb37",
                    "name": "Hrvoje Bogunovi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
            ],
            "publishedAt": "2025-06-10T15:25:55.000Z",
            "submittedOnDailyAt": "2025-06-12T07:31:36.181Z",
            "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis",
            "submittedOnDailyBy": {
                "_id": "655b3383ed8df831286969f0",
                "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
                "isPro": false,
                "fullname": "Jos Morano",
                "user": "j-morano",
                "type": "user"
            },
            "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.",
            "upvotes": 2,
            "discussionId": "684a96ab9aebf043cf7bdb38",
            "githubRepo": "https://github.com/j-morano/MIRAGE",
            "ai_summary": "MIRAGE, a multimodal foundation model, excels in OCT and SLO image classification and segmentation, outperforming existing general and specialized models.",
            "ai_keywords": [
                "foundation models",
                "multimodal models",
                "OCT",
                "SLO",
                "segmentation tasks",
                "evaluation benchmark"
            ]
        },
        "publishedAt": "2025-06-10T11:25:55.000Z",
        "title": "MIRAGE: Multimodal foundation model and benchmark for comprehensive\n  retinal OCT image analysis",
        "summary": "Artificial intelligence (AI) has become a fundamental tool for assisting\nclinicians in analyzing ophthalmic images, such as optical coherence tomography\n(OCT). However, developing AI models often requires extensive annotation, and\nexisting models tend to underperform on independent, unseen data. Foundation\nmodels (FMs), large AI models trained on vast unlabeled datasets, have shown\npromise in overcoming these challenges. Nonetheless, available FMs for\nophthalmology lack extensive validation, especially for segmentation tasks, and\nfocus on a single imaging modality. In this context, we propose MIRAGE, a novel\nmultimodal FM for the analysis of OCT and scanning laser ophthalmoscopy (SLO)\nimages. Additionally, we propose a new evaluation benchmark with OCT/SLO\nclassification and segmentation tasks. The comparison with general and\nspecialized FMs and segmentation methods shows the superiority of MIRAGE in\nboth types of tasks, highlighting its suitability as a basis for the\ndevelopment of robust AI systems for retinal OCT image analysis. Both MIRAGE\nand the evaluation benchmark are publicly available:\nhttps://github.com/j-morano/MIRAGE.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/655b3383ed8df831286969f0/FuwagzdL2fBNhZyfwg22J.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08900.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655b3383ed8df831286969f0",
            "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
            "fullname": "Jos Morano",
            "name": "j-morano",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08001",
            "authors": [
                {
                    "_id": "684a649f9b38e1e5a33a6895",
                    "name": "Zeju Qiu",
                    "hidden": false
                },
                {
                    "_id": "684a649f9b38e1e5a33a6896",
                    "name": "Simon Buchholz",
                    "hidden": false
                },
                {
                    "_id": "684a649f9b38e1e5a33a6897",
                    "name": "Tim Z. Xiao",
                    "hidden": false
                },
                {
                    "_id": "684a649f9b38e1e5a33a6898",
                    "name": "Maximilian Dax",
                    "hidden": false
                },
                {
                    "_id": "684a649f9b38e1e5a33a6899",
                    "name": "Bernhard Schlkopf",
                    "hidden": false
                },
                {
                    "_id": "684a649f9b38e1e5a33a689a",
                    "name": "Weiyang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T17:59:34.000Z",
            "submittedOnDailyAt": "2025-06-12T03:55:47.829Z",
            "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
            "submittedOnDailyBy": {
                "_id": "648905d1a15c43c791d4381f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
                "isPro": false,
                "fullname": "Weiyang Liu",
                "user": "wy1iu",
                "type": "user"
            },
            "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.",
            "upvotes": 2,
            "discussionId": "684a649f9b38e1e5a33a689b",
            "projectPage": "https://spherelab.ai/poet/",
            "githubRepo": "https://github.com/Sphere-AI-Lab/poet",
            "ai_summary": "POET is a reParameterized training algorithm using Orthogonal Equivalence Transformation to optimize neurons in large language models, ensuring stable training and improved generalization.",
            "ai_keywords": [
                "POET",
                "reParameterized training algorithm",
                "Orthogonal Equivalence Transformation",
                "orthogonal matrices",
                "spectral properties",
                "weight matrices",
                "large language models",
                "generalization",
                "efficient approximations",
                "training",
                "scalability"
            ]
        },
        "publishedAt": "2025-06-09T13:59:34.000Z",
        "title": "Reparameterized LLM Training via Orthogonal Equivalence Transformation",
        "summary": "While large language models (LLMs) are driving the rapid advancement of\nartificial intelligence, effectively and reliably training these large models\nremains one of the field's most significant challenges. To address this\nchallenge, we propose POET, a novel reParameterized training algorithm that\nuses Orthogonal Equivalence Transformation to optimize neurons. Specifically,\nPOET reparameterizes each neuron with two learnable orthogonal matrices and a\nfixed random weight matrix. Because of its provable preservation of spectral\nproperties of weight matrices, POET can stably optimize the objective function\nwith improved generalization. We further develop efficient approximations that\nmake POET flexible and scalable for training large-scale neural networks.\nExtensive experiments validate the effectiveness and scalability of POET in\ntraining LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08001.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648905d1a15c43c791d4381f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
            "fullname": "Weiyang Liu",
            "name": "wy1iu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09980",
            "authors": [
                {
                    "_id": "684b7c563b733ba333686f7e",
                    "name": "Jiaxiang Tang",
                    "hidden": false
                },
                {
                    "_id": "684b7c563b733ba333686f7f",
                    "name": "Ruijie Lu",
                    "hidden": false
                },
                {
                    "_id": "684b7c563b733ba333686f80",
                    "name": "Zhaoshuo Li",
                    "hidden": false
                },
                {
                    "_id": "684b7c563b733ba333686f81",
                    "name": "Zekun Hao",
                    "hidden": false
                },
                {
                    "_id": "684b7c563b733ba333686f82",
                    "name": "Xuan Li",
                    "hidden": false
                },
                {
                    "_id": "684b7c563b733ba333686f83",
                    "name": "Fangyin Wei",
                    "hidden": false
                },
                {
                    "_id": "684b7c563b733ba333686f84",
                    "name": "Shuran Song",
                    "hidden": false
                },
                {
                    "_id": "684b7c563b733ba333686f85",
                    "name": "Gang Zeng",
                    "hidden": false
                },
                {
                    "_id": "684b7c563b733ba333686f86",
                    "name": "Ming-Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "684b7c563b733ba333686f87",
                    "name": "Tsung-Yi Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:55:03.000Z",
            "submittedOnDailyAt": "2025-06-12T23:50:02.390Z",
            "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing",
            "submittedOnDailyBy": {
                "_id": "63367f9a9895307563659be6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63367f9a9895307563659be6/2GW6lrsWcbQjfv-YPWKnm.jpeg",
                "isPro": false,
                "fullname": "kiui",
                "user": "ashawkey",
                "type": "user"
            },
            "summary": "Recent progress in 3D object generation has greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce a dual volume packing strategy that organizes\nall parts into two complementary volumes, allowing for the creation of complete\nand interleaved parts that assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-based part-level generation methods.",
            "upvotes": 1,
            "discussionId": "684b7c563b733ba333686f88",
            "projectPage": "https://research.nvidia.com/labs/dir/partpacker/",
            "githubRepo": "https://github.com/NVlabs/PartPacker",
            "ai_summary": "A new framework for generating part-level 3D objects from images achieves better quality, diversity, and generalization using a dual volume packing strategy.",
            "ai_keywords": [
                "3D object generation",
                "mesh",
                "end-to-end framework",
                "part-level generation",
                "single input image",
                "high-quality 3D objects",
                "semantic parts",
                "dual volume packing strategy",
                "complete parts",
                "interleaved parts",
                "final object",
                "image-based generation"
            ]
        },
        "publishedAt": "2025-06-11T13:55:03.000Z",
        "title": "Efficient Part-level 3D Object Generation via Dual Volume Packing",
        "summary": "Recent progress in 3D object generation has greatly improved both the quality\nand efficiency. However, most existing methods generate a single mesh with all\nparts fused together, which limits the ability to edit or manipulate individual\nparts. A key challenge is that different objects may have a varying number of\nparts. To address this, we propose a new end-to-end framework for part-level 3D\nobject generation. Given a single input image, our method generates\nhigh-quality 3D objects with an arbitrary number of complete and semantically\nmeaningful parts. We introduce a dual volume packing strategy that organizes\nall parts into two complementary volumes, allowing for the creation of complete\nand interleaved parts that assemble into the final object. Experiments show\nthat our model achieves better quality, diversity, and generalization than\nprevious image-based part-level generation methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09980.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63367f9a9895307563659be6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63367f9a9895307563659be6/2GW6lrsWcbQjfv-YPWKnm.jpeg",
            "fullname": "kiui",
            "name": "ashawkey",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 47
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09958",
            "authors": [
                {
                    "_id": "684aef993b733ba333686e4d",
                    "name": "Sushant Gautam",
                    "hidden": false
                },
                {
                    "_id": "684aef993b733ba333686e4e",
                    "name": "Michael A. Riegler",
                    "hidden": false
                },
                {
                    "_id": "684aef993b733ba333686e4f",
                    "name": "Pl Halvorsen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:31:38.000Z",
            "submittedOnDailyAt": "2025-06-12T13:48:36.638Z",
            "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust\n  MedVQA in Gastrointestinal Endoscopy",
            "submittedOnDailyBy": {
                "_id": "62c4779e0a06c039315fb57d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c4779e0a06c039315fb57d/LWdhrmJBGoyCqTFGS5Ap3.jpeg",
                "isPro": false,
                "fullname": "Sushant Gautam",
                "user": "SushantGautam",
                "type": "user"
            },
            "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1",
            "upvotes": 1,
            "discussionId": "684aef9a3b733ba333686e50",
            "projectPage": "https://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1",
            "githubRepo": "https://github.com/Simula/Kvasir-VQA-x1",
            "ai_summary": "Kvasir-VQA-x1, an expanded dataset for gastrointestinal endoscopy, addresses clinical complexity and visual diversity with large-scale question-answer pairs and visual augmentations to enhance multimodal AI system reliability in clinical settings.",
            "ai_keywords": [
                "Kvasir-VQA-x1",
                "large language models",
                "clinical reasoning",
                "visual augmentations",
                "evaluation tracks",
                "multimodal AI systems"
            ]
        },
        "publishedAt": "2025-06-11T13:31:38.000Z",
        "title": "Kvasir-VQA-x1: A Multimodal Dataset for Medical Reasoning and Robust\n  MedVQA in Gastrointestinal Endoscopy",
        "summary": "Medical Visual Question Answering (MedVQA) is a promising field for\ndeveloping clinical decision support systems, yet progress is often limited by\nthe available datasets, which can lack clinical complexity and visual\ndiversity. To address these gaps, we introduce Kvasir-VQA-x1, a new,\nlarge-scale dataset for gastrointestinal (GI) endoscopy. Our work significantly\nexpands upon the original Kvasir-VQA by incorporating 159,549 new\nquestion-answer pairs that are designed to test deeper clinical reasoning. We\ndeveloped a systematic method using large language models to generate these\nquestions, which are stratified by complexity to better assess a model's\ninference capabilities. To ensure our dataset prepares models for real-world\nclinical scenarios, we have also introduced a variety of visual augmentations\nthat mimic common imaging artifacts. The dataset is structured to support two\nmain evaluation tracks: one for standard VQA performance and another to test\nmodel robustness against these visual perturbations. By providing a more\nchallenging and clinically relevant benchmark, Kvasir-VQA-x1 aims to accelerate\nthe development of more reliable and effective multimodal AI systems for use in\nclinical settings. The dataset is fully accessible and adheres to FAIR data\nprinciples, making it a valuable resource for the wider research community.\nCode and data: https://github.com/Simula/Kvasir-VQA-x1 and\nhttps://huggingface.co/datasets/SimulaMet/Kvasir-VQA-x1",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09958.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c4779e0a06c039315fb57d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c4779e0a06c039315fb57d/LWdhrmJBGoyCqTFGS5Ap3.jpeg",
            "fullname": "Sushant Gautam",
            "name": "SushantGautam",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09007",
            "authors": [
                {
                    "_id": "6849516c42e4f9106973f4d1",
                    "name": "Sophia Tang",
                    "hidden": false
                },
                {
                    "_id": "6849516c42e4f9106973f4d2",
                    "name": "Yinuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6849516c42e4f9106973f4d3",
                    "name": "Alexander Tong",
                    "hidden": false
                },
                {
                    "_id": "6849516c42e4f9106973f4d4",
                    "name": "Pranam Chatterjee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:29:48.000Z",
            "submittedOnDailyAt": "2025-06-12T01:32:31.298Z",
            "title": "Branched Schrdinger Bridge Matching",
            "submittedOnDailyBy": {
                "_id": "64cd5b3f0494187a9e8b7c69",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
                "isPro": false,
                "fullname": "Pranam Chatterjee",
                "user": "pranamanam",
                "type": "user"
            },
            "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.",
            "upvotes": 1,
            "discussionId": "6849516d42e4f9106973f4d5",
            "ai_summary": "BranchSBM, a novel generative modeling framework, extends Schr\\\"odinger Bridge Matching to model branched stochastic paths and multi-path evolution from a single initial distribution to multiple outcomes.",
            "ai_keywords": [
                "flow matching",
                "Schr\\\"odinger Bridge Matching",
                "Branched Schr\\\"odinger Bridge Matching",
                "BranchSBM",
                "time-dependent velocity fields",
                "growth processes",
                "multi-path surface navigation",
                "cell fate bifurcations",
                "cellular responses to perturbations"
            ]
        },
        "publishedAt": "2025-06-10T13:29:48.000Z",
        "title": "Branched Schrdinger Bridge Matching",
        "summary": "Predicting the intermediate trajectories between an initial and target\ndistribution is a central problem in generative modeling. Existing approaches,\nsuch as flow matching and Schr\\\"odinger Bridge Matching, effectively learn\nmappings between two distributions by modeling a single stochastic path.\nHowever, these methods are inherently limited to unimodal transitions and\ncannot capture branched or divergent evolution from a common origin to multiple\ndistinct outcomes. To address this, we introduce Branched Schr\\\"odinger Bridge\nMatching (BranchSBM), a novel framework that learns branched Schr\\\"odinger\nbridges. BranchSBM parameterizes multiple time-dependent velocity fields and\ngrowth processes, enabling the representation of population-level divergence\ninto multiple terminal distributions. We show that BranchSBM is not only more\nexpressive but also essential for tasks involving multi-path surface\nnavigation, modeling cell fate bifurcations from homogeneous progenitor states,\nand simulating diverging cellular responses to perturbations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09007.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cd5b3f0494187a9e8b7c69",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eo9HiGbvCxHQZ-QJB1Y_o.jpeg",
            "fullname": "Pranam Chatterjee",
            "name": "pranamanam",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.06020",
            "authors": [
                {
                    "_id": "684aff253b733ba333686e6e",
                    "name": "Zeqi Zhou",
                    "hidden": false
                },
                {
                    "_id": "684aff253b733ba333686e6f",
                    "user": {
                        "_id": "675e0d5cdd3e9eeed6954f5a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
                        "isPro": false,
                        "fullname": "Fang Wu",
                        "user": "fangwu97",
                        "type": "user"
                    },
                    "name": "Fang Wu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-12T16:24:05.966Z",
                    "hidden": false
                },
                {
                    "_id": "684aff253b733ba333686e70",
                    "name": "Shayan Talaei",
                    "hidden": false
                },
                {
                    "_id": "684aff253b733ba333686e71",
                    "name": "Haokai Zhao",
                    "hidden": false
                },
                {
                    "_id": "684aff253b733ba333686e72",
                    "name": "Cheng Meixin",
                    "hidden": false
                },
                {
                    "_id": "684aff253b733ba333686e73",
                    "name": "Tinson Xu",
                    "hidden": false
                },
                {
                    "_id": "684aff253b733ba333686e74",
                    "name": "Amin Saberi",
                    "hidden": false
                },
                {
                    "_id": "684aff253b733ba333686e75",
                    "name": "Yejin Choi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T12:09:34.000Z",
            "submittedOnDailyAt": "2025-06-12T14:57:40.299Z",
            "title": "When to Trust Context: Self-Reflective Debates for Context Reliability",
            "submittedOnDailyBy": {
                "_id": "675e0d5cdd3e9eeed6954f5a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
                "isPro": false,
                "fullname": "Fang Wu",
                "user": "fangwu97",
                "type": "user"
            },
            "summary": "Large language models frequently encounter conflicts between their parametric\nknowledge and contextual input, often resulting in factual inconsistencies or\nhallucinations. We propose Self-Reflective Debate for Contextual Reliability\n(SR-DCR), a lightweight framework that integrates token-level self-confidence\nwith an asymmetric multi-agent debate to adjudicate such conflicts. A critic,\ndeprived of context, challenges a defender who argues from the given passage; a\njudge model evaluates the debate and determines the context's reliability. The\nfinal answer is selected by combining the verdict with model confidence.\nExperiments on the ClashEval benchmark demonstrate that SR-DCR consistently\nenhances robustness to misleading context while maintaining accuracy on\ntrustworthy inputs, outperforming both classical debate and confidence-only\nbaselines with minimal computational overhead. The code is available at\nhttps://github.com/smiles724/Self-Reflective-Debates.",
            "upvotes": 1,
            "discussionId": "684aff253b733ba333686e76",
            "githubRepo": "https://github.com/smiles724/Self-Reflective-Debates",
            "ai_summary": "A lightweight framework integrating token-level self-confidence and an asymmetric debate between agents enhances the robustness of large language models to contextual inconsistencies with minimal computational cost.",
            "ai_keywords": [
                "token-level self-confidence",
                "asymmetric multi-agent debate",
                "critic",
                "defender",
                "judge model",
                "context reliability",
                "factual inconsistencies",
                "hallucinations",
                "ClashEval benchmark"
            ]
        },
        "publishedAt": "2025-06-06T08:09:34.000Z",
        "title": "When to Trust Context: Self-Reflective Debates for Context Reliability",
        "summary": "Large language models frequently encounter conflicts between their parametric\nknowledge and contextual input, often resulting in factual inconsistencies or\nhallucinations. We propose Self-Reflective Debate for Contextual Reliability\n(SR-DCR), a lightweight framework that integrates token-level self-confidence\nwith an asymmetric multi-agent debate to adjudicate such conflicts. A critic,\ndeprived of context, challenges a defender who argues from the given passage; a\njudge model evaluates the debate and determines the context's reliability. The\nfinal answer is selected by combining the verdict with model confidence.\nExperiments on the ClashEval benchmark demonstrate that SR-DCR consistently\nenhances robustness to misleading context while maintaining accuracy on\ntrustworthy inputs, outperforming both classical debate and confidence-only\nbaselines with minimal computational overhead. The code is available at\nhttps://github.com/smiles724/Self-Reflective-Debates.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06020.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "675e0d5cdd3e9eeed6954f5a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/7oMEoBmaFiCR9K2q9Z_7q.png",
            "fullname": "Fang Wu",
            "name": "fangwu97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05412",
            "authors": [
                {
                    "_id": "684ae322dbd21a9cc27b11e0",
                    "name": "Zory Zhang",
                    "hidden": false
                },
                {
                    "_id": "684ae322dbd21a9cc27b11e1",
                    "name": "Pinyuan Feng",
                    "hidden": false
                },
                {
                    "_id": "684ae322dbd21a9cc27b11e2",
                    "name": "Bingyang Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae322dbd21a9cc27b11e3",
                    "name": "Tianwei Zhao",
                    "hidden": false
                },
                {
                    "_id": "684ae322dbd21a9cc27b11e4",
                    "name": "Suyang Yu",
                    "hidden": false
                },
                {
                    "_id": "684ae322dbd21a9cc27b11e5",
                    "name": "Qingying Gao",
                    "hidden": false
                },
                {
                    "_id": "684ae322dbd21a9cc27b11e6",
                    "name": "Hokin Deng",
                    "hidden": false
                },
                {
                    "_id": "684ae322dbd21a9cc27b11e7",
                    "name": "Ziqiao Ma",
                    "hidden": false
                },
                {
                    "_id": "684ae322dbd21a9cc27b11e8",
                    "name": "Yijiang Li",
                    "hidden": false
                },
                {
                    "_id": "684ae322dbd21a9cc27b11e9",
                    "name": "Dezhi Luo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6595f19f2ae03e58169be1b1/HfcLQCA-Nom-Yec_ZqcUg.mp4"
            ],
            "publishedAt": "2025-06-04T17:59:25.000Z",
            "submittedOnDailyAt": "2025-06-12T23:49:40.655Z",
            "title": "Can Vision Language Models Infer Human Gaze Direction? A Controlled\n  Study",
            "submittedOnDailyBy": {
                "_id": "6595f19f2ae03e58169be1b1",
                "avatarUrl": "/avatars/97c583dbe497b5974c52cc35bcd63e20.svg",
                "isPro": false,
                "fullname": "Zory Zhang",
                "user": "Zory",
                "type": "user"
            },
            "summary": "Gaze-referential inference--the ability to infer what others are looking\nat--is a critical component of a theory of mind that underpins natural human-AI\ninteraction. In a controlled study, we evaluated this skill across 111 Vision\nLanguage Models (VLMs) using photos taken with manipulated difficulty and\nvariability, comparing performance with that of human participants (N = 65),\nand analyzed behaviors using mixed-effects models. We found that 94 of the 111\nVLMs failed to do better than random guessing, while humans achieved\nnear-ceiling accuracy. VLMs even respond with each choice almost equally\nfrequently. Are they randomly guessing? Although most VLMs struggle, when we\nzoom in on five of the top-tier VLMs with above-chance performance, we find\nthat their performance declined with increasing task difficulty but varied only\nslightly across different prompts and scene objects. These behavioral features\ncannot be explained by considering them as random guessers. Instead, they\nlikely use a combination of heuristics and guessing such that their performance\nis subject to the task difficulty but robust to perceptual variations. This\nsuggests that VLMs, lacking gaze inference capability, have yet to become\ntechnologies that can naturally interact with humans, but the potential\nremains.",
            "upvotes": 1,
            "discussionId": "684ae322dbd21a9cc27b11ea",
            "projectPage": "https://grow-ai-like-a-child.github.io/gaze/",
            "githubRepo": "https://github.com/grow-ai-like-a-child/referential-gaze",
            "ai_summary": "Vision Language Models struggle with gaze-referential inference compared to humans, showing near-random guessing behavior, while top-tier models perform above chance but are still affected by task difficulty.",
            "ai_keywords": [
                "Vision Language Models",
                "gaze-referential inference",
                "mixed-effects models",
                "task difficulty"
            ]
        },
        "publishedAt": "2025-06-04T13:59:25.000Z",
        "title": "Can Vision Language Models Infer Human Gaze Direction? A Controlled\n  Study",
        "summary": "Gaze-referential inference--the ability to infer what others are looking\nat--is a critical component of a theory of mind that underpins natural human-AI\ninteraction. In a controlled study, we evaluated this skill across 111 Vision\nLanguage Models (VLMs) using photos taken with manipulated difficulty and\nvariability, comparing performance with that of human participants (N = 65),\nand analyzed behaviors using mixed-effects models. We found that 94 of the 111\nVLMs failed to do better than random guessing, while humans achieved\nnear-ceiling accuracy. VLMs even respond with each choice almost equally\nfrequently. Are they randomly guessing? Although most VLMs struggle, when we\nzoom in on five of the top-tier VLMs with above-chance performance, we find\nthat their performance declined with increasing task difficulty but varied only\nslightly across different prompts and scene objects. These behavioral features\ncannot be explained by considering them as random guessers. Instead, they\nlikely use a combination of heuristics and guessing such that their performance\nis subject to the task difficulty but robust to perceptual variations. This\nsuggests that VLMs, lacking gaze inference capability, have yet to become\ntechnologies that can naturally interact with humans, but the potential\nremains.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6595f19f2ae03e58169be1b1/HfcLQCA-Nom-Yec_ZqcUg.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05412.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6595f19f2ae03e58169be1b1",
            "avatarUrl": "/avatars/97c583dbe497b5974c52cc35bcd63e20.svg",
            "fullname": "Zory Zhang",
            "name": "Zory",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10209",
            "authors": [
                {
                    "_id": "684b73173b733ba333686f75",
                    "user": {
                        "_id": "65bb2a8400e143d672b170c9",
                        "avatarUrl": "/avatars/e1d745d12f0a4c8dae95d2f6983d5201.svg",
                        "isPro": false,
                        "fullname": "Prakamya Mishra",
                        "user": "Prakamya",
                        "type": "user"
                    },
                    "name": "Prakamya Mishra",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-13T00:53:00.119Z",
                    "hidden": false
                },
                {
                    "_id": "684b73173b733ba333686f76",
                    "name": "Jiang Liu",
                    "hidden": false
                },
                {
                    "_id": "684b73173b733ba333686f77",
                    "name": "Jialian Wu",
                    "hidden": false
                },
                {
                    "_id": "684b73173b733ba333686f78",
                    "name": "Xiaodong Yu",
                    "hidden": false
                },
                {
                    "_id": "684b73173b733ba333686f79",
                    "name": "Zicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "684b73173b733ba333686f7a",
                    "name": "Emad Barsoum",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65bb2a8400e143d672b170c9/wTosGVAW3ab-BTEF2Z-KX.png"
            ],
            "publishedAt": "2025-06-11T22:03:19.000Z",
            "submittedOnDailyAt": "2025-06-12T23:26:15.849Z",
            "title": "TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and\n  Novel Tic-Tac-Toe-style Games",
            "submittedOnDailyBy": {
                "_id": "65bb2a8400e143d672b170c9",
                "avatarUrl": "/avatars/e1d745d12f0a4c8dae95d2f6983d5201.svg",
                "isPro": false,
                "fullname": "Prakamya Mishra",
                "user": "Prakamya",
                "type": "user"
            },
            "summary": "Large reasoning models (LRMs) have demonstrated impressive reasoning\ncapabilities across a broad range of tasks including Olympiad-level\nmathematical problems, indicating evidence of their complex reasoning\nabilities. While many reasoning benchmarks focus on the STEM domain, the\nability of LRMs to reason correctly in broader task domains remains\nunderexplored. In this work, we introduce TTT-Bench, a new benchmark\nthat is designed to evaluate basic strategic, spatial, and logical reasoning\nabilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games\nthat humans can effortlessly solve from a young age. We propose a simple yet\nscalable programmatic approach for generating verifiable two-player game\nproblems for TTT-Bench. Although these games are trivial for humans, they\nrequire reasoning about the intentions of the opponent, as well as the game\nboard's spatial configurations, to ensure a win. We evaluate a diverse set of\nstate-of-the-art LRMs, and discover that the models that excel at hard\nmath problems frequently fail at these simple reasoning games. Further testing\nreveals that our evaluated reasoning models score on average downarrow 41\\%\n\\& downarrow 5\\% lower on TTT-Bench compared to MATH 500 \\& AIME 2024\nrespectively, with larger models achieving higher performance using shorter\nreasoning traces, where most of the models struggle on long-term strategic\nreasoning situations on simple and new TTT-Bench tasks.",
            "upvotes": 0,
            "discussionId": "684b73173b733ba333686f7b",
            "projectPage": "https://prakamya-mishra.github.io/TTTBench/",
            "ai_summary": "TTT-Bench evaluates strategic, spatial, and logical reasoning in large reasoning models through Tic-Tac-Toe-style games, revealing that models proficient in advanced math often struggle with these basic tasks.",
            "ai_keywords": [
                "large reasoning models",
                "reasoning benchmarks",
                "TTT-Bench",
                "two-player Tic-Tac-Toe-style games",
                "strategic reasoning",
                "spatial reasoning",
                "logical reasoning",
                "reasoning traces"
            ]
        },
        "publishedAt": "2025-06-11T18:03:19.000Z",
        "title": "TTT-Bench: A Benchmark for Evaluating Reasoning Ability with Simple and\n  Novel Tic-Tac-Toe-style Games",
        "summary": "Large reasoning models (LRMs) have demonstrated impressive reasoning\ncapabilities across a broad range of tasks including Olympiad-level\nmathematical problems, indicating evidence of their complex reasoning\nabilities. While many reasoning benchmarks focus on the STEM domain, the\nability of LRMs to reason correctly in broader task domains remains\nunderexplored. In this work, we introduce TTT-Bench, a new benchmark\nthat is designed to evaluate basic strategic, spatial, and logical reasoning\nabilities in LRMs through a suite of four two-player Tic-Tac-Toe-style games\nthat humans can effortlessly solve from a young age. We propose a simple yet\nscalable programmatic approach for generating verifiable two-player game\nproblems for TTT-Bench. Although these games are trivial for humans, they\nrequire reasoning about the intentions of the opponent, as well as the game\nboard's spatial configurations, to ensure a win. We evaluate a diverse set of\nstate-of-the-art LRMs, and discover that the models that excel at hard\nmath problems frequently fail at these simple reasoning games. Further testing\nreveals that our evaluated reasoning models score on average downarrow 41\\%\n\\& downarrow 5\\% lower on TTT-Bench compared to MATH 500 \\& AIME 2024\nrespectively, with larger models achieving higher performance using shorter\nreasoning traces, where most of the models struggle on long-term strategic\nreasoning situations on simple and new TTT-Bench tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65bb2a8400e143d672b170c9/wTosGVAW3ab-BTEF2Z-KX.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10209.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65bb2a8400e143d672b170c9",
            "avatarUrl": "/avatars/e1d745d12f0a4c8dae95d2f6983d5201.svg",
            "fullname": "Prakamya Mishra",
            "name": "Prakamya",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09820",
            "authors": [
                {
                    "_id": "684ae213dbd21a9cc27b100b",
                    "name": "Chengpeng Li",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b100c",
                    "name": "Zhengyang Tang",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b100d",
                    "name": "Ziniu Li",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b100e",
                    "name": "Mingfeng Xue",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b100f",
                    "name": "Keqin Bao",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b1010",
                    "name": "Tian Ding",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b1011",
                    "name": "Ruoyu Sun",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b1012",
                    "name": "Benyou Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b1013",
                    "name": "Xiang Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b1014",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "684ae213dbd21a9cc27b1015",
                    "name": "Dayiheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T14:59:02.000Z",
            "submittedOnDailyAt": "2025-06-12T23:58:27.470Z",
            "title": "CoRT: Code-integrated Reasoning within Thinking",
            "submittedOnDailyBy": {
                "_id": "65294b334d7cf551ac50d6a6",
                "avatarUrl": "/avatars/75d21e20b711b871616ef3850bb900b7.svg",
                "isPro": false,
                "fullname": "ChengpengLi",
                "user": "ChengpengLi",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.",
            "upvotes": 0,
            "discussionId": "684ae213dbd21a9cc27b1016",
            "ai_summary": "CoRT enhances Large Reasoning Models with Code Interpreter using Hint-Engineering, improving mathematical reasoning efficiency and reducing token usage.",
            "ai_keywords": [
                "Large Reasoning Models",
                "LRMs",
                "DeepSeek-R1",
                "chain-of-thought",
                "CoT",
                "complex mathematical operations",
                "Code Interpreter",
                "CI",
                "Hint-Engineering",
                "supervised fine-tuning",
                "rejection fine-tuning",
                "reinforcement learning",
                "mathematical reasoning datasets"
            ]
        },
        "publishedAt": "2025-06-11T10:59:02.000Z",
        "title": "CoRT: Code-integrated Reasoning within Thinking",
        "summary": "Large Reasoning Models (LRMs) like o1 and DeepSeek-R1 have shown remarkable\nprogress in natural language reasoning with long chain-of-thought (CoT), yet\nthey remain inefficient or inaccurate when handling complex mathematical\noperations. Addressing these limitations through computational tools (e.g.,\ncomputation libraries and symbolic solvers) is promising, but it introduces a\ntechnical challenge: Code Interpreter (CI) brings external knowledge beyond the\nmodel's internal text representations, thus the direct combination is not\nefficient. This paper introduces CoRT, a post-training framework for teaching\nLRMs to leverage CI effectively and efficiently. As a first step, we address\nthe data scarcity issue by synthesizing code-integrated reasoning data through\nHint-Engineering, which strategically inserts different hints at appropriate\npositions to optimize LRM-CI interaction. We manually create 30 high-quality\nsamples, upon which we post-train models ranging from 1.5B to 32B parameters,\nwith supervised fine-tuning, rejection fine-tuning and reinforcement learning.\nOur experimental results demonstrate that Hint-Engineering models achieve 4\\%\nand 8\\% absolute improvements on DeepSeek-R1-Distill-Qwen-32B and\nDeepSeek-R1-Distill-Qwen-1.5B respectively, across five challenging\nmathematical reasoning datasets. Furthermore, Hint-Engineering models use about\n30\\% fewer tokens for the 32B model and 50\\% fewer tokens for the 1.5B model\ncompared with the natural language models. The models and code are available at\nhttps://github.com/ChengpengLi1003/CoRT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09820.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65294b334d7cf551ac50d6a6",
            "avatarUrl": "/avatars/75d21e20b711b871616ef3850bb900b7.svg",
            "fullname": "ChengpengLi",
            "name": "ChengpengLi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09420",
            "authors": [
                {
                    "_id": "684ae20fdbd21a9cc27b0fed",
                    "name": "Henry Peng Zou",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0fee",
                    "name": "Wei-Chieh Huang",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0fef",
                    "name": "Yaozu Wu",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff0",
                    "name": "Chunyu Miao",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff1",
                    "name": "Dongyuan Li",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff2",
                    "name": "Aiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff3",
                    "name": "Yue Zhou",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff4",
                    "name": "Yankai Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff5",
                    "name": "Weizhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff6",
                    "name": "Yangning Li",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff7",
                    "name": "Liancheng Fang",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff8",
                    "name": "Renhe Jiang",
                    "hidden": false
                },
                {
                    "_id": "684ae20fdbd21a9cc27b0ff9",
                    "name": "Philip S. Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T06:08:13.000Z",
            "submittedOnDailyAt": "2025-06-12T21:19:15.094Z",
            "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should\n  Precede AI Autonomy",
            "submittedOnDailyBy": {
                "_id": "633f112013e836a0fc4fa567",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665077519962-noauth.jpeg",
                "isPro": false,
                "fullname": "Henry Peng Zou",
                "user": "TreeForest",
                "type": "user"
            },
            "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.",
            "upvotes": 0,
            "discussionId": "684ae20fdbd21a9cc27b0ffa",
            "githubRepo": "https://github.com/HenryPengZou/Awesome-LLM-Based-Human-Agent-Systems",
            "ai_summary": "The paper advocates for LLM-based Human-Agent Systems over fully autonomous AI agents due to improved reliability, transparency, and understanding of human needs through collaboration in various domains.",
            "ai_keywords": [
                "large language models",
                "LLM-based Human-Agent Systems",
                "human-agent teamwork",
                "healthcare",
                "finance",
                "software development"
            ]
        },
        "publishedAt": "2025-06-11T02:08:13.000Z",
        "title": "A Call for Collaborative Intelligence: Why Human-Agent Systems Should\n  Precede AI Autonomy",
        "summary": "Recent improvements in large language models (LLMs) have led many researchers\nto focus on building fully autonomous AI agents. This position paper questions\nwhether this approach is the right path forward, as these autonomous systems\nstill have problems with reliability, transparency, and understanding the\nactual requirements of human. We suggest a different approach: LLM-based\nHuman-Agent Systems (LLM-HAS), where AI works with humans rather than replacing\nthem. By keeping human involved to provide guidance, answer questions, and\nmaintain control, these systems can be more trustworthy and adaptable. Looking\nat examples from healthcare, finance, and software development, we show how\nhuman-AI teamwork can handle complex tasks better than AI working alone. We\nalso discuss the challenges of building these collaborative systems and offer\npractical solutions. This paper argues that progress in AI should not be\nmeasured by how independent systems become, but by how well they can work with\nhumans. The most promising future for AI is not in systems that take over human\nroles, but in those that enhance human capabilities through meaningful\npartnership.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09420.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633f112013e836a0fc4fa567",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665077519962-noauth.jpeg",
            "fullname": "Henry Peng Zou",
            "name": "TreeForest",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]