[
    {
        "paper": {
            "id": "2506.09513",
            "authors": [
                {
                    "_id": "684b8dbd3b733ba33368701b",
                    "user": {
                        "_id": "6723079ad1306fe9c76a1d29",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
                        "isPro": false,
                        "fullname": "Yu Sun",
                        "user": "YuSun-AI",
                        "type": "user"
                    },
                    "name": "Yu Sun",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-13T02:32:30.652Z",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba33368701c",
                    "name": "Xingyu Qian",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba33368701d",
                    "name": "Weiwen Xu",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba33368701e",
                    "user": {
                        "_id": "64b7cd74ff6d81ae297feded",
                        "avatarUrl": "/avatars/880fbc96cc093f5e901ce84f32a1d21d.svg",
                        "isPro": false,
                        "fullname": "ZHANG HAO",
                        "user": "26hzhang",
                        "type": "user"
                    },
                    "name": "Hao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:43.056Z",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba33368701f",
                    "name": "Chenghao Xiao",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687020",
                    "name": "Long Li",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687021",
                    "user": {
                        "_id": "642eecbf9b2484d7d8526781",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
                        "isPro": false,
                        "fullname": "Yu Rong",
                        "user": "Swrooy",
                        "type": "user"
                    },
                    "name": "Yu Rong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:40.908Z",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687022",
                    "name": "Wenbing Huang",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687023",
                    "name": "Qifeng Bai",
                    "hidden": false
                },
                {
                    "_id": "684b8dbd3b733ba333687024",
                    "name": "Tingyang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T08:36:55.000Z",
            "submittedOnDailyAt": "2025-06-13T01:06:46.741Z",
            "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "6723079ad1306fe9c76a1d29",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
                "isPro": false,
                "fullname": "Yu Sun",
                "user": "YuSun-AI",
                "type": "user"
            },
            "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
            "upvotes": 61,
            "discussionId": "684b8dbe3b733ba333687025",
            "githubRepo": "https://github.com/YuSun-Work/ReasonMed",
            "ai_summary": "ReasonMed, a large medical reasoning dataset, enhances the accuracy of medical question answering models by combining detailed reasoning paths with concise summaries, setting new benchmarks for model performance.",
            "ai_keywords": [
                "reasoning-based large language models",
                "LLMs",
                "medical question answering",
                "ReasonMed",
                "multi-agent verification",
                "Error Refiner",
                "Chain-of-Thought",
                "CoT reasoning",
                "ReasonMed-7B",
                "PubMedQA"
            ]
        },
        "publishedAt": "2025-06-11T04:36:55.000Z",
        "title": "ReasonMed: A 370K Multi-Agent Generated Dataset for Advancing Medical\n  Reasoning",
        "summary": "Though reasoning-based large language models (LLMs) have excelled in\nmathematics and programming, their capabilities in knowledge-intensive medical\nquestion answering remain underexplored. To address this, we introduce\nReasonMed, the largest medical reasoning dataset, comprising 370k high-quality\nexamples distilled from 1.7 million initial reasoning paths generated by\nvarious LLMs. ReasonMed is constructed through a multi-agent\nverification and refinement process, where we design an Error Refiner\nto enhance the reasoning paths by identifying and correcting error-prone steps\nflagged by a verifier. Leveraging ReasonMed, we systematically investigate best\npractices for training medical reasoning models and find that combining\ndetailed Chain-of-Thought (CoT) reasoning with concise answer summaries yields\nthe most effective fine-tuning strategy. Based on this strategy, we train\nReasonMed-7B, which sets a new benchmark for sub-10B models, outperforming the\nprior best by 4.17\\% and even exceeding LLaMA3.1-70B on PubMedQA by 4.60\\%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09513.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6723079ad1306fe9c76a1d29",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b4BNPCeZs59MKxo1qmT6r.png",
            "fullname": "Yu Sun",
            "name": "YuSun-AI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10954",
            "authors": [
                {
                    "_id": "684b7ea83b733ba333686f8a",
                    "name": "Lianghong Guo",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8b",
                    "name": "Yanlin Wang",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8c",
                    "name": "Caihua Li",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8d",
                    "name": "Pengyu Yang",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8e",
                    "name": "Jiachi Chen",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f8f",
                    "user": {
                        "_id": "6355473d525beaee688b7ba1",
                        "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
                        "isPro": false,
                        "fullname": "Wei Tao",
                        "user": "itaowe",
                        "type": "user"
                    },
                    "name": "Wei Tao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:58.091Z",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f90",
                    "name": "Yingtian Zou",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f91",
                    "name": "Duyu Tang",
                    "hidden": false
                },
                {
                    "_id": "684b7ea83b733ba333686f92",
                    "name": "Zibin Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:54:17.000Z",
            "submittedOnDailyAt": "2025-06-13T00:07:20.052Z",
            "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
            "submittedOnDailyBy": {
                "_id": "6355473d525beaee688b7ba1",
                "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
                "isPro": false,
                "fullname": "Wei Tao",
                "user": "itaowe",
                "type": "user"
            },
            "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of 0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
            "upvotes": 36,
            "discussionId": "684b7ea83b733ba333686f93",
            "githubRepo": "https://github.com/DeepSoftwareAnalytics/swe-factory",
            "ai_summary": "A pipeline named SWE-Factory automates the creation and validation of GitHub issue resolution datasets for training and evaluating Large Language Models, using SWE-Builder for environment setup, exit-code-based grading, and automated fail2pass validation.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "SWE-Factory",
                "SWE-Builder",
                "multi-agent system",
                "environment memory pool",
                "exit-code-based grading",
                "automated fail2pass validation",
                "GPT-4.1-mini",
                "Gemini-2.5-flash",
                "precision",
                "recall"
            ]
        },
        "publishedAt": "2025-06-12T13:54:17.000Z",
        "title": "SWE-Factory: Your Automated Factory for Issue Resolution Training Data\n  and Evaluation Benchmarks",
        "summary": "Constructing large-scale datasets for the GitHub issue resolution task is\ncrucial for both training and evaluating the software engineering capabilities\nof Large Language Models (LLMs). However, the traditional process for creating\nsuch benchmarks is notoriously challenging and labor-intensive, particularly in\nthe stages of setting up evaluation environments, grading test outcomes, and\nvalidating task instances. In this paper, we propose SWE-Factory, an automated\npipeline designed to address these challenges. To tackle these issues, our\npipeline integrates three core automated components. First, we introduce\nSWE-Builder, a multi-agent system that automates evaluation environment\nconstruction, which employs four specialized agents that work in a\ncollaborative, iterative loop and leverages an environment memory pool to\nenhance efficiency. Second, we introduce a standardized, exit-code-based\ngrading method that eliminates the need for manually writing custom parsers.\nFinally, we automate the fail2pass validation process using these reliable exit\ncode signals. Experiments on 671 issues across four programming languages show\nthat our pipeline can effectively construct valid task instances; for example,\nwith GPT-4.1-mini, our SWE-Builder constructs 269 valid instances at 0.045 per\ninstance, while with Gemini-2.5-flash, it achieves comparable performance at\nthe lowest cost of 0.024 per instance. We also demonstrate that our\nexit-code-based grading achieves 100% accuracy compared to manual inspection,\nand our automated fail2pass validation reaches a precision of 0.92 and a recall\nof 1.00. We hope our automated pipeline will accelerate the collection of\nlarge-scale, high-quality GitHub issue resolution datasets for both training\nand evaluation. Our code and datasets are released at\nhttps://github.com/DeepSoftwareAnalytics/swe-factory.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10954.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6355473d525beaee688b7ba1",
            "avatarUrl": "/avatars/0a0f0acc65829c6d864440444c580698.svg",
            "fullname": "Wei Tao",
            "name": "itaowe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09993",
            "authors": [
                {
                    "_id": "684ae204dbd21a9cc27b0fba",
                    "user": {
                        "_id": "66012e9c9e1cf5eb41ee0c4c",
                        "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
                        "isPro": false,
                        "fullname": "Jaewon Min",
                        "user": "Min-Jaewon",
                        "type": "user"
                    },
                    "name": "Jaewon Min",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:25.024Z",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbb",
                    "user": {
                        "_id": "65ec3449a69aaabb431db0da",
                        "avatarUrl": "/avatars/d7b507be0175a61a8fc21176eea45001.svg",
                        "isPro": false,
                        "fullname": "Jin Hyeon Kim",
                        "user": "jinlovespho",
                        "type": "user"
                    },
                    "name": "Jin Hyeon Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:22.776Z",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbc",
                    "user": {
                        "_id": "6752b6315281c3cae4b0783f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xmcyVEl2xBhk3G5_7dmpz.png",
                        "isPro": false,
                        "fullname": "Paul Hyunbin Cho",
                        "user": "paulcho98",
                        "type": "user"
                    },
                    "name": "Paul Hyunbin Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:20.327Z",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbd",
                    "user": {
                        "_id": "644be3e922d211df644416e9",
                        "avatarUrl": "/avatars/6bebbda7a7c16992eba64dc489eaeca5.svg",
                        "isPro": false,
                        "fullname": "Jaeeun Lee",
                        "user": "alicia10",
                        "type": "user"
                    },
                    "name": "Jaeeun Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:38.559Z",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbe",
                    "name": "Jihye Park",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fbf",
                    "name": "Minkyu Park",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fc0",
                    "name": "Sangpil Kim",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fc1",
                    "name": "Hyunhee Park",
                    "hidden": false
                },
                {
                    "_id": "684ae204dbd21a9cc27b0fc2",
                    "name": "Seungryong Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:59:46.000Z",
            "submittedOnDailyAt": "2025-06-13T00:32:01.285Z",
            "title": "Text-Aware Image Restoration with Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "66012e9c9e1cf5eb41ee0c4c",
                "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
                "isPro": false,
                "fullname": "Jaewon Min",
                "user": "Min-Jaewon",
                "type": "user"
            },
            "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
            "upvotes": 32,
            "discussionId": "684ae204dbd21a9cc27b0fc5",
            "projectPage": "https://cvlab-kaist.github.io/TAIR/",
            "githubRepo": "https://github.com/cvlab-kaist/TAIR",
            "ai_summary": "The proposed Text-Aware Image Restoration (TAIR) system integrates a multi-task diffusion framework with a text-spotting module to enhance both image recovery and textual fidelity, outperforming existing diffusion-based methods.",
            "ai_keywords": [
                "diffusion-based restoration",
                "text-image hallucination",
                "Text-Aware Image Restoration (TAIR)",
                "SA-Text",
                "multi-task diffusion framework",
                "TeReDiff",
                "text-spotting module",
                "text recognition accuracy"
            ]
        },
        "publishedAt": "2025-06-11T13:59:46.000Z",
        "title": "Text-Aware Image Restoration with Diffusion Models",
        "summary": "Image restoration aims to recover degraded images. However, existing\ndiffusion-based restoration methods, despite great success in natural image\nrestoration, often struggle to faithfully reconstruct textual regions in\ndegraded images. Those methods frequently generate plausible but incorrect\ntext-like patterns, a phenomenon we refer to as text-image hallucination. In\nthis paper, we introduce Text-Aware Image Restoration (TAIR), a novel\nrestoration task that requires the simultaneous recovery of visual contents and\ntextual fidelity. To tackle this task, we present SA-Text, a large-scale\nbenchmark of 100K high-quality scene images densely annotated with diverse and\ncomplex text instances. Furthermore, we propose a multi-task diffusion\nframework, called TeReDiff, that integrates internal features from diffusion\nmodels into a text-spotting module, enabling both components to benefit from\njoint training. This allows for the extraction of rich text representations,\nwhich are utilized as prompts in subsequent denoising steps. Extensive\nexperiments demonstrate that our approach consistently outperforms\nstate-of-the-art restoration methods, achieving significant gains in text\nrecognition accuracy. See our project page: https://cvlab-kaist.github.io/TAIR/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09993.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66012e9c9e1cf5eb41ee0c4c",
            "avatarUrl": "/avatars/b07240cb86315b9e33d14677e02e4024.svg",
            "fullname": "Jaewon Min",
            "name": "Min-Jaewon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10857",
            "authors": [
                {
                    "_id": "684b817e3b733ba333686f95",
                    "user": {
                        "_id": "64b89a14cf14c2fabe96664c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
                        "isPro": false,
                        "fullname": "Jiashuo Yu",
                        "user": "awojustin",
                        "type": "user"
                    },
                    "name": "Jiashuo Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:55.618Z",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f96",
                    "name": "Yue Wu",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f97",
                    "name": "Meng Chu",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f98",
                    "name": "Zhifei Ren",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f99",
                    "name": "Zizheng Huang",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9a",
                    "user": {
                        "_id": "64c9beb2904317f42de06dd8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64c9beb2904317f42de06dd8/he3rxfyzfwEd1vLuK6_o2.jpeg",
                        "isPro": false,
                        "fullname": "Pei Chu",
                        "user": "chupei",
                        "type": "user"
                    },
                    "name": "Pei Chu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:51.884Z",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9b",
                    "name": "Ruijie Zhang",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9c",
                    "name": "Yinan He",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9d",
                    "name": "Qirui Li",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9e",
                    "user": {
                        "_id": "64acbbd51aee69ece03c6c0c",
                        "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
                        "isPro": false,
                        "fullname": "Songze Li",
                        "user": "LarryLee",
                        "type": "user"
                    },
                    "name": "Songze Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:53.776Z",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686f9f",
                    "name": "Zhenxiang Li",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa0",
                    "name": "Zhongying Tu",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa1",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa2",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa3",
                    "name": "Yali Wang",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa4",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "684b817e3b733ba333686fa5",
                    "name": "Limin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T16:17:17.000Z",
            "submittedOnDailyAt": "2025-06-13T00:10:47.082Z",
            "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
            "submittedOnDailyBy": {
                "_id": "64b89a14cf14c2fabe96664c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
                "isPro": false,
                "fullname": "Jiashuo Yu",
                "user": "awojustin",
                "type": "user"
            },
            "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
            "upvotes": 28,
            "discussionId": "684b817e3b733ba333686fa6",
            "projectPage": "https://vrbench.github.io/",
            "githubRepo": "https://github.com/OpenGVLab/VRBench",
            "ai_summary": "VRBench is a long narrative video benchmark designed to evaluate models' multi-step reasoning and procedural validity through human-labeled question-answering pairs and a human-AI collaborative framework with a multi-phase evaluation pipeline.",
            "ai_keywords": [
                "VRBench",
                "multi-step reasoning",
                "temporal reasoning",
                "procedural validity",
                "long videos",
                "human-labeled",
                "multi-step question-answering",
                "expert inter-rater reviewing",
                "coherent reasoning chains",
                "event attribution",
                "implicit inference",
                "multi-phase evaluation",
                "progress-level LLM-guided scoring metric",
                "LLMs",
                "VLMs"
            ]
        },
        "publishedAt": "2025-06-12T12:17:17.000Z",
        "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
        "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 1,010 long videos (with an average duration\nof 1.6 hours), along with 9,468 human-labeled multi-step question-answering\npairs and 30,292 reasoning steps with timestamps. These videos are curated via\na multi-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 16 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10857.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b89a14cf14c2fabe96664c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tEd3fBjMcEubF4plqzcUz.jpeg",
            "fullname": "Jiashuo Yu",
            "name": "awojustin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10540",
            "authors": [
                {
                    "_id": "684bad683b733ba3336870b6",
                    "user": {
                        "_id": "652fb8bcc9dd2692a25ef2e3",
                        "avatarUrl": "/avatars/461e6cc1c3441cde18192b080b0b8576.svg",
                        "isPro": false,
                        "fullname": "Haoyuan Shi",
                        "user": "MrSunshy",
                        "type": "user"
                    },
                    "name": "Haoyuan Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:38:52.713Z",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870b7",
                    "user": {
                        "_id": "62fdb01bc1588e1d4c6c1a7c",
                        "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
                        "isPro": false,
                        "fullname": "Yunxin Li",
                        "user": "YunxinLi",
                        "type": "user"
                    },
                    "name": "Yunxin Li",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-13T04:47:39.539Z",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870b8",
                    "name": "Xinyu Chen",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870b9",
                    "name": "Longyue Wang",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870ba",
                    "name": "Baotian Hu",
                    "hidden": false
                },
                {
                    "_id": "684bad683b733ba3336870bb",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T10:06:21.000Z",
            "submittedOnDailyAt": "2025-06-13T03:26:17.710Z",
            "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
            "submittedOnDailyBy": {
                "_id": "62fdb01bc1588e1d4c6c1a7c",
                "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
                "isPro": false,
                "fullname": "Yunxin Li",
                "user": "YunxinLi",
                "type": "user"
            },
            "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
            "upvotes": 27,
            "discussionId": "684bad683b733ba3336870bc",
            "ai_summary": "AniMaker, a multi-agent framework using MCTS-Gen and AniEval, generates coherent storytelling videos from text input, outperforming existing models with better quality and efficiency.",
            "ai_keywords": [
                "multi-agent framework",
                "Director Agent",
                "Photography Agent",
                "Reviewer Agent",
                "Post-Production Agent",
                "Monte Carlo Tree Search (MCTS)",
                "AniEval",
                "VBench",
                "action completion",
                "story-level consistency",
                "animation-specific features"
            ]
        },
        "publishedAt": "2025-06-12T06:06:21.000Z",
        "title": "AniMaker: Automated Multi-Agent Animated Storytelling with MCTS-Driven\n  Clip Generation",
        "summary": "Despite rapid advancements in video generation models, generating coherent\nstorytelling videos that span multiple scenes and characters remains\nchallenging. Current methods often rigidly convert pre-generated keyframes into\nfixed-length clips, resulting in disjointed narratives and pacing issues.\nFurthermore, the inherent instability of video generation models means that\neven a single low-quality clip can significantly degrade the entire output\nanimation's logical coherence and visual continuity. To overcome these\nobstacles, we introduce AniMaker, a multi-agent framework enabling efficient\nmulti-candidate clip generation and storytelling-aware clip selection, thus\ncreating globally consistent and story-coherent animation solely from text\ninput. The framework is structured around specialized agents, including the\nDirector Agent for storyboard generation, the Photography Agent for video clip\ngeneration, the Reviewer Agent for evaluation, and the Post-Production Agent\nfor editing and voiceover. Central to AniMaker's approach are two key technical\ncomponents: MCTS-Gen in Photography Agent, an efficient Monte Carlo Tree Search\n(MCTS)-inspired strategy that intelligently navigates the candidate space to\ngenerate high-potential clips while optimizing resource usage; and AniEval in\nReviewer Agent, the first framework specifically designed for multi-shot\nanimation evaluation, which assesses critical aspects such as story-level\nconsistency, action completion, and animation-specific features by considering\neach clip in the context of its preceding and succeeding clips. Experiments\ndemonstrate that AniMaker achieves superior quality as measured by popular\nmetrics including VBench and our proposed AniEval framework, while\nsignificantly improving the efficiency of multi-candidate generation, pushing\nAI-generated storytelling animation closer to production standards.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10540.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "62fdb01bc1588e1d4c6c1a7c",
            "avatarUrl": "/avatars/bd03085995b1c34e0ac8a845cf2c4e83.svg",
            "fullname": "Yunxin Li",
            "name": "YunxinLi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10274",
            "authors": [
                {
                    "_id": "684bd94c3b733ba3336871a2",
                    "name": "Pooneh Mousavi",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871a3",
                    "user": {
                        "_id": "66b9bc2dacdbc1d0b39c3b50",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
                        "isPro": false,
                        "fullname": "Gallil Maimon",
                        "user": "gallilmaimon",
                        "type": "user"
                    },
                    "name": "Gallil Maimon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:26.386Z",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871a4",
                    "name": "Adel Moumen",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871a5",
                    "name": "Darius Petermann",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871a6",
                    "name": "Jiatong Shi",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871a7",
                    "name": "Haibin Wu",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871a8",
                    "name": "Haici Yang",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871a9",
                    "name": "Anastasia Kuznetsova",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871aa",
                    "name": "Artem Ploujnikov",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871ab",
                    "name": "Ricard Marxer",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871ac",
                    "name": "Bhuvana Ramabhadran",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871ad",
                    "name": "Benjamin Elizalde",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871ae",
                    "name": "Loren Lugosch",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871af",
                    "name": "Jinyu Li",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871b0",
                    "name": "Cem Subakan",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871b1",
                    "name": "Phil Woodland",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871b2",
                    "name": "Minje Kim",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871b3",
                    "name": "Hung-yi Lee",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871b4",
                    "name": "Shinji Watanabe",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871b5",
                    "name": "Yossi Adi",
                    "hidden": false
                },
                {
                    "_id": "684bd94c3b733ba3336871b6",
                    "name": "Mirco Ravanelli",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/Nn0Fw0aQ5evXmdebmWXKX.png"
            ],
            "publishedAt": "2025-06-12T01:35:43.000Z",
            "submittedOnDailyAt": "2025-06-13T11:19:02.871Z",
            "title": "Discrete Audio Tokens: More Than a Survey!",
            "submittedOnDailyBy": {
                "_id": "66b9bc2dacdbc1d0b39c3b50",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
                "isPro": false,
                "fullname": "Gallil Maimon",
                "user": "gallilmaimon",
                "type": "user"
            },
            "summary": "Discrete audio tokens are compact representations that aim to preserve\nperceptual quality, phonetic content, and speaker characteristics while\nenabling efficient storage and inference, as well as competitive performance\nacross diverse downstream tasks.They provide a practical alternative to\ncontinuous features, enabling the integration of speech and audio into modern\nlarge language models (LLMs). As interest in token-based audio processing\ngrows, various tokenization methods have emerged, and several surveys have\nreviewed the latest progress in the field. However, existing studies often\nfocus on specific domains or tasks and lack a unified comparison across various\nbenchmarks. This paper presents a systematic review and benchmark of discrete\naudio tokenizers, covering three domains: speech, music, and general audio. We\npropose a taxonomy of tokenization approaches based on encoder-decoder,\nquantization techniques, training paradigm, streamability, and application\ndomains. We evaluate tokenizers on multiple benchmarks for reconstruction,\ndownstream performance, and acoustic language modeling, and analyze trade-offs\nthrough controlled ablation studies. Our findings highlight key limitations,\npractical considerations, and open challenges, providing insight and guidance\nfor future research in this rapidly evolving area. For more information,\nincluding our main results and tokenizer database, please refer to our website:\nhttps://poonehmousavi.github.io/dates-website/.",
            "upvotes": 21,
            "discussionId": "684bd94c3b733ba3336871b7",
            "projectPage": "https://poonehmousavi.github.io/dates-website/",
            "ai_summary": "A systematic review and benchmark of discrete audio tokenizers across speech, music, and general audio domains is presented, covering their taxonomy, evaluation metrics, and limitations.",
            "ai_keywords": [
                "discrete audio tokens",
                "perceptual quality",
                "phonetic content",
                "speaker characteristics",
                "efficient storage",
                "inference",
                "large language models",
                "tokenization methods",
                "encoder-decoder",
                "quantization techniques",
                "training paradigm",
                "streamability",
                "application domains",
                "reconstruction",
                "acoustic language modeling",
                "ablation studies"
            ]
        },
        "publishedAt": "2025-06-11T21:35:43.000Z",
        "title": "Discrete Audio Tokens: More Than a Survey!",
        "summary": "Discrete audio tokens are compact representations that aim to preserve\nperceptual quality, phonetic content, and speaker characteristics while\nenabling efficient storage and inference, as well as competitive performance\nacross diverse downstream tasks.They provide a practical alternative to\ncontinuous features, enabling the integration of speech and audio into modern\nlarge language models (LLMs). As interest in token-based audio processing\ngrows, various tokenization methods have emerged, and several surveys have\nreviewed the latest progress in the field. However, existing studies often\nfocus on specific domains or tasks and lack a unified comparison across various\nbenchmarks. This paper presents a systematic review and benchmark of discrete\naudio tokenizers, covering three domains: speech, music, and general audio. We\npropose a taxonomy of tokenization approaches based on encoder-decoder,\nquantization techniques, training paradigm, streamability, and application\ndomains. We evaluate tokenizers on multiple benchmarks for reconstruction,\ndownstream performance, and acoustic language modeling, and analyze trade-offs\nthrough controlled ablation studies. Our findings highlight key limitations,\npractical considerations, and open challenges, providing insight and guidance\nfor future research in this rapidly evolving area. For more information,\nincluding our main results and tokenizer database, please refer to our website:\nhttps://poonehmousavi.github.io/dates-website/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66b9bc2dacdbc1d0b39c3b50/Nn0Fw0aQ5evXmdebmWXKX.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10274.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b9bc2dacdbc1d0b39c3b50",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/hwR0pVfP_E8XjimXIxDOU.jpeg",
            "fullname": "Gallil Maimon",
            "name": "gallilmaimon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10910",
            "authors": [
                {
                    "_id": "684bbe273b733ba3336870ed",
                    "name": "Mistral-AI",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870ef",
                    "name": "Abhinav Rastogi",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f0",
                    "name": "Albert Q. Jiang",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f1",
                    "name": "Andy Lo",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f2",
                    "name": "Gabrielle Berrada",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f3",
                    "name": "Guillaume Lample",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f4",
                    "name": "Jason Rute",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f5",
                    "name": "Joep Barmentlo",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f6",
                    "name": "Karmesh Yadav",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f7",
                    "name": "Kartik Khandelwal",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f8",
                    "name": "Khyathi Raghavi Chandu",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870f9",
                    "name": "Lonard Blier",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870fa",
                    "name": "Lucile Saulnier",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870fb",
                    "name": "Matthieu Dinot",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870fc",
                    "name": "Maxime Darrin",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870fd",
                    "name": "Neha Gupta",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870fe",
                    "name": "Roman Soletskyi",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba3336870ff",
                    "name": "Sagar Vaze",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687100",
                    "name": "Teven Le Scao",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687101",
                    "name": "Yihan Wang",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687102",
                    "name": "Adam Yang",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687103",
                    "name": "Alexander H. Liu",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687104",
                    "name": "Alexandre Sablayrolles",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687105",
                    "name": "Amlie Hliou",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687106",
                    "name": "Amlie Martin",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687107",
                    "name": "Andy Ehrenberg",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687108",
                    "name": "Anmol Agarwal",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687109",
                    "name": "Antoine Roux",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368710a",
                    "name": "Arthur Darcet",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368710b",
                    "name": "Arthur Mensch",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368710c",
                    "name": "Baptiste Bout",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368710d",
                    "name": "Baptiste Rozire",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368710e",
                    "name": "Baudouin De Monicault",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368710f",
                    "name": "Chris Bamford",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687110",
                    "name": "Christian Wallenwein",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687111",
                    "name": "Christophe Renaudin",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687112",
                    "name": "Clmence Lanfranchi",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687113",
                    "name": "Darius Dabert",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687114",
                    "name": "Devon Mizelle",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687115",
                    "name": "Diego de las Casas",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687116",
                    "name": "Elliot Chane-Sane",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687117",
                    "name": "Emilien Fugier",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687118",
                    "name": "Emma Bou Hanna",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687119",
                    "name": "Gauthier Delerce",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368711a",
                    "name": "Gauthier Guinet",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368711b",
                    "name": "Georgii Novikov",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368711c",
                    "name": "Guillaume Martin",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368711d",
                    "name": "Himanshu Jaju",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368711e",
                    "name": "Jan Ludziejewski",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368711f",
                    "name": "Jean-Hadrien Chabran",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687120",
                    "name": "Jean-Malo Delignon",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687121",
                    "name": "Joachim Studnia",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687122",
                    "name": "Jonas Amar",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687123",
                    "name": "Josselin Somerville Roberts",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687124",
                    "name": "Julien Denize",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687125",
                    "name": "Karan Saxena",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687126",
                    "name": "Kush Jain",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687127",
                    "name": "Lingxiao Zhao",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687128",
                    "name": "Louis Martin",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687129",
                    "name": "Luyu Gao",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368712a",
                    "name": "Llio Renard Lavaud",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368712b",
                    "name": "Marie Pellat",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368712c",
                    "name": "Mathilde Guillaumin",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368712d",
                    "name": "Mathis Felardos",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368712e",
                    "name": "Maximilian Augustin",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368712f",
                    "name": "Mickal Seznec",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687130",
                    "name": "Nikhil Raghuraman",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687131",
                    "name": "Olivier Duchenne",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687132",
                    "name": "Patricia Wang",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687133",
                    "name": "Patrick von Platen",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687134",
                    "name": "Patryk Saffer",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687135",
                    "name": "Paul Jacob",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687136",
                    "name": "Paul Wambergue",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687137",
                    "name": "Paula Kurylowicz",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687138",
                    "name": "Pavankumar Reddy Muddireddy",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687139",
                    "name": "Philomne Chagniot",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368713a",
                    "name": "Pierre Stock",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368713b",
                    "name": "Pravesh Agrawal",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368713c",
                    "name": "Romain Sauvestre",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368713d",
                    "name": "Rmi Delacourt",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368713e",
                    "name": "Sanchit Gandhi",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368713f",
                    "name": "Sandeep Subramanian",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687140",
                    "name": "Shashwat Dalal",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687141",
                    "name": "Siddharth Gandhi",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687142",
                    "name": "Soham Ghosh",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687143",
                    "name": "Srijan Mishra",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687144",
                    "name": "Sumukh Aithal",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687145",
                    "name": "Szymon Antoniak",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687146",
                    "name": "Thibault Schueller",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687147",
                    "name": "Thibaut Lavril",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687148",
                    "name": "Thomas Robert",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687149",
                    "name": "Thomas Wang",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368714a",
                    "name": "Timothe Lacroix",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368714b",
                    "name": "Valeriia Nemychnikova",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368714c",
                    "name": "Victor Paltz",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368714d",
                    "name": "Virgile Richard",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368714e",
                    "name": "Wen-Ding Li",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba33368714f",
                    "name": "William Marshall",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687150",
                    "name": "Xuanyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "684bbe273b733ba333687151",
                    "name": "Yunhao Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:22:37.000Z",
            "submittedOnDailyAt": "2025-06-13T04:29:39.974Z",
            "title": "Magistral",
            "submittedOnDailyBy": {
                "_id": "5e6a3d4ea9afd5125d9ec064",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                "isPro": true,
                "fullname": "Stefan Schweter",
                "user": "stefan-it",
                "type": "user"
            },
            "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.",
            "upvotes": 20,
            "discussionId": "684bbe283b733ba333687152",
            "ai_summary": "Magistral, a scalable reinforcement learning pipeline, demonstrates that RL can enhance multimodal understanding and instruction following in large language models without requiring existing RL traces.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "LLMs",
                "multimodal understanding",
                "instruction following",
                "function calling",
                "cold-start data"
            ]
        },
        "publishedAt": "2025-06-12T13:22:37.000Z",
        "title": "Magistral",
        "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable\nreinforcement learning (RL) pipeline. Instead of relying on existing\nimplementations and RL traces distilled from prior models, we follow a ground\nup approach, relying solely on our own models and infrastructure. Notably, we\ndemonstrate a stack that enabled us to explore the limits of pure RL training\nof LLMs, present a simple method to force the reasoning language of the model,\nand show that RL on text data alone maintains most of the initial checkpoint's\ncapabilities. We find that RL on text maintains or improves multimodal\nunderstanding, instruction following and function calling. We present Magistral\nMedium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we\nopen-source Magistral Small (Apache 2.0) which further includes cold-start data\nfrom Magistral Medium.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10910.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e6a3d4ea9afd5125d9ec064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2745
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10952",
            "authors": [
                {
                    "_id": "684b96403b733ba33368703a",
                    "user": {
                        "_id": "65e808ed7c10574cc3f8e363",
                        "avatarUrl": "/avatars/ed10759d354e271bfc15afd946b66b4a.svg",
                        "isPro": false,
                        "fullname": "zhangmozhi",
                        "user": "mzzhang",
                        "type": "user"
                    },
                    "name": "Mozhi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:33.791Z",
                    "hidden": false
                },
                {
                    "_id": "684b96403b733ba33368703b",
                    "user": {
                        "_id": "6718fc605e14ff6b94a7109f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
                        "isPro": false,
                        "fullname": "Howe Tissue",
                        "user": "Howe77",
                        "type": "user"
                    },
                    "name": "Howe Tissue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:31.965Z",
                    "hidden": false
                },
                {
                    "_id": "684b96403b733ba33368703c",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "684b96403b733ba33368703d",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:53:51.000Z",
            "submittedOnDailyAt": "2025-06-13T01:43:47.223Z",
            "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
            "submittedOnDailyBy": {
                "_id": "6718fc605e14ff6b94a7109f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
                "isPro": false,
                "fullname": "Howe Tissue",
                "user": "Howe77",
                "type": "user"
            },
            "summary": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\textbf{Distribution\nAlignment Assumption} (DA^{2}), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, Domain2vec can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\nDomain2Vec helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\nDomain2Vec achieves the same validation loss on Pile-CC using only\n51.5% of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, Domain2Vec improves\ndownstream performance by an average of 2.83%.",
            "upvotes": 18,
            "discussionId": "684b96413b733ba33368703e",
            "ai_summary": "Domain2Vec decomposes datasets into meta-domains to optimize language model pretraining and downstream performance with reduced computational cost.",
            "ai_keywords": [
                "Domain2Vec",
                "meta-domains",
                "domain vector",
                "distribution alignment assumption",
                "DA",
                "language model",
                "pretraining",
                "downstream task performance",
                "Pile-CC",
                "The Pile dataset"
            ]
        },
        "publishedAt": "2025-06-12T13:53:51.000Z",
        "title": "Domain2Vec: Vectorizing Datasets to Find the Optimal Data Mixture\n  without Training",
        "summary": "We introduce~Domain2Vec, a novel approach that decomposes any\ndataset into a linear combination of several meta-domains, a new concept\ndesigned to capture the key underlying features of datasets.\nDomain2Vec maintains a vocabulary of meta-domains and uses a\nclassifier to decompose any given dataset into a domain vector that corresponds\nto a distribution over this vocabulary. These domain vectors enable the\nidentification of the optimal data mixture for language model (LM) pretraining\nin a training-free manner under the \\textbf{Distribution\nAlignment Assumption} (DA^{2}), which suggests that when\nthe data distributions of the training set and the validation set are better\naligned, a lower validation loss is achieved. Moreover, Domain2vec can\nbe seamlessly integrated into previous works to model the relationship between\ndomain vectors and LM performance, greatly enhancing the efficiency and\nscalability of previous methods. Extensive experiments demonstrate that\nDomain2Vec helps find the data mixture that enhances downstream task\nperformance with minimal computational overhead. Specifically,\nDomain2Vec achieves the same validation loss on Pile-CC using only\n51.5% of the computation required when training on the original mixture of\nThe Pile dataset. Under equivalent compute budget, Domain2Vec improves\ndownstream performance by an average of 2.83%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10952.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6718fc605e14ff6b94a7109f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6718fc605e14ff6b94a7109f/uz2O2F_qbY3wefpY548qS.jpeg",
            "fullname": "Howe Tissue",
            "name": "Howe77",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10357",
            "authors": [
                {
                    "_id": "684b86bf3b733ba333686fbe",
                    "name": "Zaijing Li",
                    "hidden": false
                },
                {
                    "_id": "684b86bf3b733ba333686fbf",
                    "name": "Yuquan Xie",
                    "hidden": false
                },
                {
                    "_id": "684b86bf3b733ba333686fc0",
                    "name": "Rui Shao",
                    "hidden": false
                },
                {
                    "_id": "684b86bf3b733ba333686fc1",
                    "name": "Gongwei Chen",
                    "hidden": false
                },
                {
                    "_id": "684b86bf3b733ba333686fc2",
                    "name": "Weili Guan",
                    "hidden": false
                },
                {
                    "_id": "684b86bf3b733ba333686fc3",
                    "name": "Dongmei Jiang",
                    "hidden": false
                },
                {
                    "_id": "684b86bf3b733ba333686fc4",
                    "name": "Liqiang Nie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T05:29:40.000Z",
            "submittedOnDailyAt": "2025-06-13T00:37:48.793Z",
            "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
            "submittedOnDailyBy": {
                "_id": "66b45fe75d0ac130d7d82764",
                "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
                "isPro": false,
                "fullname": "Zaijing Li",
                "user": "dawn0815",
                "type": "user"
            },
            "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/",
            "upvotes": 17,
            "discussionId": "684b86bf3b733ba333686fc5",
            "projectPage": "https://cybertronagent.github.io/Optimus-3.github.io/",
            "githubRepo": "https://github.com/JiuTian-VL/Optimus-3",
            "ai_summary": "Optimus-3, a multimodal large language model agent, uses knowledge-enhanced data generation, a Mixture-of-Experts architecture, and multimodal reasoning-augmented reinforcement learning to achieve superior performance across various tasks in Minecraft.",
            "ai_keywords": [
                "multimodal large language models",
                "knowledge-enhanced data generation",
                "Mixture-of-Experts",
                "task-level routing",
                "multimodal reasoning-augmented reinforcement learning"
            ]
        },
        "publishedAt": "2025-06-12T01:29:40.000Z",
        "title": "Optimus-3: Towards Generalist Multimodal Minecraft Agents with Scalable\n  Task Experts",
        "summary": "Recently, agents based on multimodal large language models (MLLMs) have\nachieved remarkable progress across various domains. However, building a\ngeneralist agent with capabilities such as perception, planning, action,\ngrounding, and reflection in open-world environments like Minecraft remains\nchallenges: insufficient domain-specific data, interference among heterogeneous\ntasks, and visual diversity in open-world settings. In this paper, we address\nthese challenges through three key contributions. 1) We propose a\nknowledge-enhanced data generation pipeline to provide scalable and\nhigh-quality training data for agent development. 2) To mitigate interference\namong heterogeneous tasks, we introduce a Mixture-of-Experts (MoE) architecture\nwith task-level routing. 3) We develop a Multimodal Reasoning-Augmented\nReinforcement Learning approach to enhance the agent's reasoning ability for\nvisual diversity in Minecraft. Built upon these innovations, we present\nOptimus-3, a general-purpose agent for Minecraft. Extensive experimental\nresults demonstrate that Optimus-3 surpasses both generalist multimodal large\nlanguage models and existing state-of-the-art agents across a wide range of\ntasks in the Minecraft environment. Project page:\nhttps://cybertronagent.github.io/Optimus-3.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10357.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b45fe75d0ac130d7d82764",
            "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
            "fullname": "Zaijing Li",
            "name": "dawn0815",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10741",
            "authors": [
                {
                    "_id": "684b881f3b733ba333686fd4",
                    "user": {
                        "_id": "64966691990b342dcc9fccb5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64966691990b342dcc9fccb5/tQSrE3MkBeakk5QYfgHSo.jpeg",
                        "isPro": true,
                        "fullname": "sixiang chen",
                        "user": "Ephemeral182",
                        "type": "user"
                    },
                    "name": "SiXiang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:47.414Z",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fd5",
                    "name": "Jianyu Lai",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fd6",
                    "name": "Jialin Gao",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fd7",
                    "name": "Tian Ye",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fd8",
                    "name": "Haoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fd9",
                    "name": "Hengyu Shi",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fda",
                    "name": "Shitong Shao",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fdb",
                    "name": "Yunlong Lin",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fdc",
                    "name": "Song Fei",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fdd",
                    "name": "Zhaohu Xing",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fde",
                    "name": "Yeying Jin",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fdf",
                    "name": "Junfeng Luo",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fe0",
                    "name": "Xiaoming Wei",
                    "hidden": false
                },
                {
                    "_id": "684b881f3b733ba333686fe1",
                    "name": "Lei Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T14:28:12.000Z",
            "submittedOnDailyAt": "2025-06-13T04:30:38.214Z",
            "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
            "submittedOnDailyBy": {
                "_id": "66015e8aa4d296af07de538e",
                "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
                "isPro": false,
                "fullname": "Ye",
                "user": "Owen777",
                "type": "user"
            },
            "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft",
            "upvotes": 15,
            "discussionId": "684b881f3b733ba333686fe2",
            "projectPage": "https://ephemeral182.github.io/PosterCraft/",
            "githubRepo": "https://github.com/Ephemeral182/PosterCraft",
            "ai_summary": "PosterCraft improves aesthetic poster generation through a unified, modular pipeline with enhanced text rendering, region-aware fine-tuning, aesthetic reinforcement learning, and joint vision-language refinement.",
            "ai_keywords": [
                "text-rendering optimization",
                "Text-Render-2M",
                "region-aware supervised fine-tuning",
                "HQ-Poster100K",
                "aesthetic-text-reinforcement learning",
                "best-of-n preference optimization",
                "joint vision-language feedback refinement"
            ]
        },
        "publishedAt": "2025-06-12T10:28:12.000Z",
        "title": "PosterCraft: Rethinking High-Quality Aesthetic Poster Generation in a\n  Unified Framework",
        "summary": "Generating aesthetic posters is more challenging than simple design images:\nit requires not only precise text rendering but also the seamless integration\nof abstract artistic content, striking layouts, and overall stylistic harmony.\nTo address this, we propose PosterCraft, a unified framework that abandons\nprior modular pipelines and rigid, predefined layouts, allowing the model to\nfreely explore coherent, visually compelling compositions. PosterCraft employs\na carefully designed, cascaded workflow to optimize the generation of\nhigh-aesthetic posters: (i) large-scale text-rendering optimization on our\nnewly introduced Text-Render-2M dataset; (ii) region-aware supervised\nfine-tuning on HQ-Poster100K; (iii) aesthetic-text-reinforcement learning via\nbest-of-n preference optimization; and (iv) joint vision-language feedback\nrefinement. Each stage is supported by a fully automated data-construction\npipeline tailored to its specific needs, enabling robust training without\ncomplex architectural modifications. Evaluated on multiple experiments,\nPosterCraft significantly outperforms open-source baselines in rendering\naccuracy, layout coherence, and overall visual appeal-approaching the quality\nof SOTA commercial systems. Our code, models, and datasets can be found in the\nProject page: https://ephemeral182.github.io/PosterCraft",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10741.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66015e8aa4d296af07de538e",
            "avatarUrl": "/avatars/a1295c631cc2646282c545859975ce4c.svg",
            "fullname": "Ye",
            "name": "Owen777",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09967",
            "authors": [
                {
                    "_id": "684ae1eedbd21a9cc27b0f10",
                    "user": {
                        "_id": "67469d6a8407f929491dce06",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
                        "isPro": true,
                        "fullname": "Shangshang Wang",
                        "user": "upup-ashton-wang",
                        "type": "user"
                    },
                    "name": "Shangshang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:31.479Z",
                    "hidden": false
                },
                {
                    "_id": "684ae1eedbd21a9cc27b0f11",
                    "name": "Julian Asilis",
                    "hidden": false
                },
                {
                    "_id": "684ae1eedbd21a9cc27b0f12",
                    "name": "mer Faruk Akgl",
                    "hidden": false
                },
                {
                    "_id": "684ae1eedbd21a9cc27b0f13",
                    "name": "Enes Burak Bilgin",
                    "hidden": false
                },
                {
                    "_id": "684ae1eedbd21a9cc27b0f14",
                    "name": "Ollie Liu",
                    "hidden": false
                },
                {
                    "_id": "684ae1eedbd21a9cc27b0f15",
                    "user": {
                        "_id": "63c8454e46421a2efe82709d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c8454e46421a2efe82709d/3BcSk4KOwAgWHEPVtsAV3.png",
                        "isPro": true,
                        "fullname": "Deqing Fu",
                        "user": "deqing",
                        "type": "user"
                    },
                    "name": "Deqing Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:29.284Z",
                    "hidden": false
                },
                {
                    "_id": "684ae1eedbd21a9cc27b0f16",
                    "user": {
                        "_id": "644bf65522d211df6444a7f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644bf65522d211df6444a7f4/k_ddZdQDg2fzhwjI1EXyx.jpeg",
                        "isPro": false,
                        "fullname": "Willie Neiswanger",
                        "user": "willieneis",
                        "type": "user"
                    },
                    "name": "Willie Neiswanger",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:27.301Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:44:01.000Z",
            "submittedOnDailyAt": "2025-06-13T02:39:37.215Z",
            "title": "Resa: Transparent Reasoning Models via SAEs",
            "submittedOnDailyBy": {
                "_id": "67469d6a8407f929491dce06",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
                "isPro": true,
                "fullname": "Shangshang Wang",
                "user": "upup-ashton-wang",
                "type": "user"
            },
            "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround 1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.",
            "upvotes": 13,
            "discussionId": "684ae1eedbd21a9cc27b0f17",
            "projectPage": "https://shangshangwang.notion.site/resa",
            "githubRepo": "https://github.com/shangshang-wang/Resa",
            "ai_summary": "SAE-Tuning efficiently elicits strong reasoning in language models by leveraging sparse autoencoders, enabling cost-effective performance gains without extensive retraining.",
            "ai_keywords": [
                "sparse autoencoder tuning",
                "SAE-Tuning",
                "reasoning models",
                "verification",
                "sparse autoencoders",
                "supervised fine-tuning",
                "RL post-training",
                "Pass@1",
                "AIME24",
                "AMC23",
                "generality",
                "modularity",
                "R1-Distill",
                "Qwen",
                "Qwen-Math"
            ]
        },
        "publishedAt": "2025-06-11T13:44:01.000Z",
        "title": "Resa: Transparent Reasoning Models via SAEs",
        "summary": "How cost-effectively can we elicit strong reasoning in language models by\nleveraging their underlying representations? We answer this question with Resa,\na family of 1.5B reasoning models trained via a novel and efficient sparse\nautoencoder tuning (SAE-Tuning) procedure. This method first trains an SAE to\ncapture reasoning abilities from a source model, and then uses the trained SAE\nto guide a standard supervised fine-tuning process to elicit such abilities in\na target model, all using verified question-answer data without any reasoning\ntraces. Notably, when applied to certain base models before further RL\npost-training, SAE-Tuning retains >97% of its RL-trained counterpart's\nreasoning performance while reducing training costs by >2000x to roughly \\1\nand training time by >450x to around 20 minutes. Furthermore, when applied to\nlightly RL-trained models (e.g., within 1 hour on 2 GPUs), it enables reasoning\nperformance such as 43.33% Pass@1 on AIME24 and 90% Pass@1 on AMC23 for only\naround 1 additional cost. Surprisingly, the reasoning abilities extracted via\nSAEs are potentially both generalizable and modular. Generality means abilities\nextracted from one dataset still elevate performance on a larger and\noverlapping corpus. Modularity means abilities extracted from Qwen or Qwen-Math\ncan be attached to the R1-Distill model at test time, without any retraining,\nand yield comparable gains. Extensive ablations validate these findings and all\nartifacts are fully open-sourced.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09967.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67469d6a8407f929491dce06",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67469d6a8407f929491dce06/7vd7zyApmT4rXe58gqmG1.png",
            "fullname": "Shangshang Wang",
            "name": "upup-ashton-wang",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10974",
            "authors": [
                {
                    "_id": "684b8e193b733ba333687028",
                    "user": {
                        "_id": "6241749cf80bd930bd99f3dd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669210243382-6241749cf80bd930bd99f3dd.jpeg",
                        "isPro": false,
                        "fullname": "Ou Yixin",
                        "user": "OE-Heart",
                        "type": "user"
                    },
                    "name": "Yixin Ou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:36.148Z",
                    "hidden": false
                },
                {
                    "_id": "684b8e193b733ba333687029",
                    "name": "Yujie Luo",
                    "hidden": false
                },
                {
                    "_id": "684b8e193b733ba33368702a",
                    "name": "Jingsheng Zheng",
                    "hidden": false
                },
                {
                    "_id": "684b8e193b733ba33368702b",
                    "name": "Lanning Wei",
                    "hidden": false
                },
                {
                    "_id": "684b8e193b733ba33368702c",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "684b8e193b733ba33368702d",
                    "name": "Jintian Zhang",
                    "hidden": false
                },
                {
                    "_id": "684b8e193b733ba33368702e",
                    "name": "Da Zheng",
                    "hidden": false
                },
                {
                    "_id": "684b8e193b733ba33368702f",
                    "name": "Huajun Chen",
                    "hidden": false
                },
                {
                    "_id": "684b8e193b733ba333687030",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:37.984Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:59:32.000Z",
            "submittedOnDailyAt": "2025-06-13T03:44:21.173Z",
            "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.",
            "upvotes": 11,
            "discussionId": "684b8e193b733ba333687031",
            "ai_summary": "AutoMind, a flexible and knowledgeable LLM-agent framework, improves automated data science through expert knowledge integration, strategic solution exploration, and adaptive coding, outperforming existing systems.",
            "ai_keywords": [
                "LLM",
                "data science agents",
                "machine learning pipeline",
                "expert knowledge base",
                "agentic knowledgeable tree search",
                "self-adaptive coding strategy"
            ]
        },
        "publishedAt": "2025-06-12T13:59:32.000Z",
        "title": "AutoMind: Adaptive Knowledgeable Agent for Automated Data Science",
        "summary": "Large Language Model (LLM) agents have shown great potential in addressing\nreal-world data science problems. LLM-driven data science agents promise to\nautomate the entire machine learning pipeline, yet their real-world\neffectiveness remains limited. Existing frameworks depend on rigid, pre-defined\nworkflows and inflexible coding strategies; consequently, they excel only on\nrelatively simple, classical problems and fail to capture the empirical\nexpertise that human practitioners bring to complex, innovative tasks. In this\nwork, we introduce AutoMind, an adaptive, knowledgeable LLM-agent framework\nthat overcomes these deficiencies through three key advances: (1) a curated\nexpert knowledge base that grounds the agent in domain expert knowledge, (2) an\nagentic knowledgeable tree search algorithm that strategically explores\npossible solutions, and (3) a self-adaptive coding strategy that dynamically\ntailors code generation to task complexity. Evaluations on two automated data\nscience benchmarks demonstrate that AutoMind delivers superior performance\nversus state-of-the-art baselines. Additional analyses confirm favorable\neffectiveness, efficiency, and qualitative solution quality, highlighting\nAutoMind as an efficient and robust step toward fully automated data science.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10974.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10821",
            "authors": [
                {
                    "_id": "684b91c73b733ba333687033",
                    "name": "Huaying Yuan",
                    "hidden": false
                },
                {
                    "_id": "684b91c73b733ba333687034",
                    "name": "Zheng Liu",
                    "hidden": false
                },
                {
                    "_id": "684b91c73b733ba333687035",
                    "name": "Junjie Zhou",
                    "hidden": false
                },
                {
                    "_id": "684b91c73b733ba333687036",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                },
                {
                    "_id": "684b91c73b733ba333687037",
                    "name": "Zhicheng Dou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T15:39:10.000Z",
            "submittedOnDailyAt": "2025-06-13T01:20:51.837Z",
            "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
            "submittedOnDailyBy": {
                "_id": "66d916a7b86f0d569aa19b60",
                "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
                "isPro": false,
                "fullname": "huaying Yuan",
                "user": "avery00",
                "type": "user"
            },
            "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.",
            "upvotes": 11,
            "discussionId": "684b91c73b733ba333687038",
            "ai_summary": "VideoDeepResearch, a text-only reasoning model with modular tools, surpasses existing baselines in long video understanding tasks without extending context windows or enhancing visual perception capabilities.",
            "ai_keywords": [
                "long video understanding",
                "multi-modal large language models",
                "VideoDeepResearch",
                "text-only large reasoning model",
                "multimodal retrievers",
                "visual perceivers",
                "MLVU",
                "Video-MME",
                "LVBench",
                "LongVideoBench",
                "agentic systems"
            ]
        },
        "publishedAt": "2025-06-12T11:39:10.000Z",
        "title": "VideoDeepResearch: Long Video Understanding With Agentic Tool Using",
        "summary": "Long video understanding (LVU) presents a significant challenge for current\nmulti-modal large language models (MLLMs) due to the task's inherent complexity\nand context window constraint. It is widely assumed that addressing LVU tasks\nrequires foundation MLLMs with extended context windows, strong visual\nperception capabilities, and proficient domain expertise. In this work, we\nchallenge this common belief by introducing VideoDeepResearch, a novel agentic\nframework for long video understanding. Our approach relies solely on a\ntext-only large reasoning model (LRM) combined with a modular multi-modal\ntoolkit, including multimodal retrievers and visual perceivers, all of which\nare readily available in practice. For each LVU task, the system formulates a\nproblem-solving strategy through reasoning, while selectively accessing and\nutilizing essential video content via tool using. We conduct extensive\nexperiments on popular LVU benchmarks, including MLVU, Video-MME, and LVBench.\nOur results demonstrate that VideoDeepResearch achieves substantial\nimprovements over existing MLLM baselines, surpassing the previous\nstate-of-the-art by 9.6%, 6.6%, and 3.9% on MLVU (test), LVBench, and\nLongVideoBench, respectively. These findings highlight the promise of agentic\nsystems in overcoming key challenges in LVU problems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10821.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d916a7b86f0d569aa19b60",
            "avatarUrl": "/avatars/2537cee66afecc2d999e05b01c78d319.svg",
            "fullname": "huaying Yuan",
            "name": "avery00",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09344",
            "authors": [
                {
                    "_id": "684ae277dbd21a9cc27b118d",
                    "name": "Inclusion AI",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b118e",
                    "name": "Biao Gong",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b118f",
                    "name": "Cheng Zou",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1190",
                    "name": "Chuanyang Zheng",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1191",
                    "name": "Chunluan Zhou",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1192",
                    "name": "Canxiang Yan",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1193",
                    "name": "Chunxiang Jin",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1194",
                    "name": "Chunjie Shen",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1195",
                    "name": "Dandan Zheng",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1196",
                    "name": "Fudong Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1197",
                    "name": "Furong Xu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1198",
                    "name": "GuangMing Yao",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b1199",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b119a",
                    "name": "Jingdong Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b119b",
                    "name": "Jianxin Sun",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b119c",
                    "name": "Jiajia Liu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b119d",
                    "name": "Jianjiang Zhu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b119e",
                    "name": "Jun Peng",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b119f",
                    "name": "Kaixiang Ji",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a0",
                    "name": "Kaiyou Song",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a1",
                    "name": "Kaimeng Ren",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a2",
                    "name": "Libin Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a3",
                    "name": "Lixiang Ru",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a4",
                    "name": "Lele Xie",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a5",
                    "name": "Longhua Tan",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a6",
                    "name": "Lyuxin Xue",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a7",
                    "name": "Lan Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a8",
                    "name": "Mochen Bai",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11a9",
                    "name": "Ning Gao",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11aa",
                    "name": "Pei Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11ab",
                    "name": "Qingpei Guo",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11ac",
                    "name": "Qinglong Zhang",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11ad",
                    "name": "Qiang Xu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11ae",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11af",
                    "name": "Ruijie Xiong",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b0",
                    "name": "Sirui Gao",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b1",
                    "name": "Tinghao Liu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b2",
                    "name": "Taisong Li",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b3",
                    "name": "Weilong Chai",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b4",
                    "name": "Xinyu Xiao",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b5",
                    "name": "Xiaomei Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b6",
                    "name": "Xiaoxue Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b7",
                    "name": "Xiao Lu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b8",
                    "name": "Xiaoyu Li",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11b9",
                    "name": "Xingning Dong",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11ba",
                    "name": "Xuzheng Yu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11bb",
                    "name": "Yi Yuan",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11bc",
                    "name": "Yuting Gao",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11bd",
                    "name": "Yunxiao Sun",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11be",
                    "name": "Yipeng Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11bf",
                    "name": "Yifei Wu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11c0",
                    "name": "Yongjie Lyu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11c1",
                    "name": "Ziping Ma",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11c2",
                    "name": "Zipeng Feng",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11c3",
                    "name": "Zhijiang Fang",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11c4",
                    "name": "Zhihao Qiu",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11c5",
                    "name": "Ziyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "684ae277dbd21a9cc27b11c6",
                    "name": "Zhengyu He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T02:50:49.000Z",
            "submittedOnDailyAt": "2025-06-13T01:53:15.172Z",
            "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
            "submittedOnDailyBy": {
                "_id": "644fcbea4f7316588267dc80",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
                "isPro": false,
                "fullname": "Biao Gong",
                "user": "BiaoGong",
                "type": "user"
            },
            "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
            "upvotes": 11,
            "discussionId": "684ae277dbd21a9cc27b11c7",
            "ai_summary": "Ming-Omni is a unified multimodal model with dedicated encoders and modality-specific routers that can process images, text, audio, and video, and performs tasks like speech and image generation, context-aware chatting, and versatile image editing.",
            "ai_keywords": [
                "multimodal model",
                "encoders",
                "tokens",
                "MoE architecture",
                "modality-specific routers",
                "audio decoder",
                "Ming-Lite-Uni",
                "context-aware chatting",
                "text-to-speech conversion",
                "image editing",
                "unified perception",
                "generation",
                "open-source"
            ]
        },
        "publishedAt": "2025-06-10T22:50:49.000Z",
        "title": "Ming-Omni: A Unified Multimodal Model for Perception and Generation",
        "summary": "We propose Ming-Omni, a unified multimodal model capable of processing\nimages, text, audio, and video, while demonstrating strong proficiency in both\nspeech and image generation. Ming-Omni employs dedicated encoders to extract\ntokens from different modalities, which are then processed by Ling, an MoE\narchitecture equipped with newly proposed modality-specific routers. This\ndesign enables a single model to efficiently process and fuse multimodal inputs\nwithin a unified framework, thereby facilitating diverse tasks without\nrequiring separate models, task-specific fine-tuning, or structural redesign.\nImportantly, Ming-Omni extends beyond conventional multimodal models by\nsupporting audio and image generation. This is achieved through the integration\nof an advanced audio decoder for natural-sounding speech and Ming-Lite-Uni for\nhigh-quality image generation, which also allow the model to engage in\ncontext-aware chatting, perform text-to-speech conversion, and conduct\nversatile image editing. Our experimental results showcase Ming-Omni offers a\npowerful solution for unified perception and generation across all modalities.\nNotably, our proposed Ming-Omni is the first open-source model we are aware of\nto match GPT-4o in modality support, and we release all code and model weights\nto encourage further research and development in the community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09344.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644fcbea4f7316588267dc80",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644fcbea4f7316588267dc80/w8-2Gkaw9BN9VzppNXrTP.jpeg",
            "fullname": "Biao Gong",
            "name": "BiaoGong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10890",
            "authors": [
                {
                    "_id": "684b8b533b733ba333686fe4",
                    "user": {
                        "_id": "62bc1adacaf01b9bec398547",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
                        "isPro": false,
                        "fullname": "Zhao Zhang",
                        "user": "zbrl",
                        "type": "user"
                    },
                    "name": "Zhao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:45.153Z",
                    "hidden": false
                },
                {
                    "_id": "684b8b533b733ba333686fe5",
                    "name": "Yutao Cheng",
                    "hidden": false
                },
                {
                    "_id": "684b8b533b733ba333686fe6",
                    "user": {
                        "_id": "6669a0cc9f28880b31d7c4ef",
                        "avatarUrl": "/avatars/bd66a6f68a9af2bf7ee40510579e57fe.svg",
                        "isPro": false,
                        "fullname": "dexiang hong",
                        "user": "hxxxl",
                        "type": "user"
                    },
                    "name": "Dexiang Hong",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
                    "hidden": false
                },
                {
                    "_id": "684b8b533b733ba333686fe7",
                    "user": {
                        "_id": "63fd7279ed9eead590fd02ed",
                        "avatarUrl": "/avatars/4cf6f005069412ee87ed07cd81500f1e.svg",
                        "isPro": false,
                        "fullname": "YangMaoke",
                        "user": "YangMaoke",
                        "type": "user"
                    },
                    "name": "Maoke Yang",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-13T02:22:14.202Z",
                    "hidden": false
                },
                {
                    "_id": "684b8b533b733ba333686fe8",
                    "user": {
                        "_id": "6436619ead9b9147de287a24",
                        "avatarUrl": "/avatars/180c43c79e552dd345636a47db80e3e9.svg",
                        "isPro": false,
                        "fullname": "ShiLayne",
                        "user": "ShiLayne",
                        "type": "user"
                    },
                    "name": "Gonglei Shi",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-13T02:23:52.345Z",
                    "hidden": true
                },
                {
                    "_id": "684b8b533b733ba333686fe9",
                    "name": "Lei Ma",
                    "hidden": false
                },
                {
                    "_id": "684b8b533b733ba333686fea",
                    "name": "Hui Zhang",
                    "hidden": false
                },
                {
                    "_id": "684b8b533b733ba333686feb",
                    "name": "Jie Shao",
                    "hidden": false
                },
                {
                    "_id": "684b8b533b733ba333686fec",
                    "name": "Xinglong Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T16:54:39.000Z",
            "submittedOnDailyAt": "2025-06-13T00:55:02.473Z",
            "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
            "submittedOnDailyBy": {
                "_id": "62bc1adacaf01b9bec398547",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
                "isPro": false,
                "fullname": "Zhao Zhang",
                "user": "zbrl",
                "type": "user"
            },
            "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter",
            "upvotes": 10,
            "discussionId": "684b8b533b733ba333686fed",
            "githubRepo": "https://github.com/graphic-design-ai/creatiposter",
            "ai_summary": "CreatiPoster generates high-quality, editable, and customizable graphic compositions from text or assets, outperforming existing tools and templates.",
            "ai_keywords": [
                "RGBA large multimodal model",
                "JSON specification",
                "conditional background model",
                "automated metrics",
                "graphic-design generation",
                "multi-layer designs",
                "AI-assisted graphic design"
            ]
        },
        "publishedAt": "2025-06-12T12:54:39.000Z",
        "title": "CreatiPoster: Towards Editable and Controllable Multi-Layer Graphic\n  Design Generation",
        "summary": "Graphic design plays a crucial role in both commercial and personal contexts,\nyet creating high-quality, editable, and aesthetically pleasing graphic\ncompositions remains a time-consuming and skill-intensive task, especially for\nbeginners. Current AI tools automate parts of the workflow, but struggle to\naccurately incorporate user-supplied assets, maintain editability, and achieve\nprofessional visual appeal. Commercial systems, like Canva Magic Design, rely\non vast template libraries, which are impractical for replicate. In this paper,\nwe introduce CreatiPoster, a framework that generates editable, multi-layer\ncompositions from optional natural-language instructions or assets. A protocol\nmodel, an RGBA large multimodal model, first produces a JSON specification\ndetailing every layer (text or asset) with precise layout, hierarchy, content\nand style, plus a concise background prompt. A conditional background model\nthen synthesizes a coherent background conditioned on this rendered foreground\nlayers. We construct a benchmark with automated metrics for graphic-design\ngeneration and show that CreatiPoster surpasses leading open-source approaches\nand proprietary commercial systems. To catalyze further research, we release a\ncopyright-free corpus of 100,000 multi-layer designs. CreatiPoster supports\ndiverse applications such as canvas editing, text overlay, responsive resizing,\nmultilingual adaptation, and animated posters, advancing the democratization of\nAI-assisted graphic design. Project homepage:\nhttps://github.com/graphic-design-ai/creatiposter",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10890.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62bc1adacaf01b9bec398547",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656494729797-noauth.png",
            "fullname": "Zhao Zhang",
            "name": "zbrl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10960",
            "authors": [
                {
                    "_id": "684bb33a3b733ba3336870c5",
                    "name": "Kangwei Liu",
                    "hidden": false
                },
                {
                    "_id": "684bb33a3b733ba3336870c6",
                    "name": "Siyuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "684bb33a3b733ba3336870c7",
                    "name": "Bozhong Tian",
                    "hidden": false
                },
                {
                    "_id": "684bb33a3b733ba3336870c8",
                    "name": "Xiaozhuan Liang",
                    "hidden": false
                },
                {
                    "_id": "684bb33a3b733ba3336870c9",
                    "name": "Yuyang Yin",
                    "hidden": false
                },
                {
                    "_id": "684bb33a3b733ba3336870ca",
                    "name": "Meng Han",
                    "hidden": false
                },
                {
                    "_id": "684bb33a3b733ba3336870cb",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:38:50.549Z",
                    "hidden": false
                },
                {
                    "_id": "684bb33a3b733ba3336870cc",
                    "name": "Bryan Hooi",
                    "hidden": false
                },
                {
                    "_id": "684bb33a3b733ba3336870cd",
                    "user": {
                        "_id": "635113fdcba4ff2e81cb236e",
                        "avatarUrl": "/avatars/f80df906b722b4901debce9baa867073.svg",
                        "isPro": false,
                        "fullname": "chen",
                        "user": "Jasonchen123",
                        "type": "user"
                    },
                    "name": "Xi Chen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-13T05:12:27.526Z",
                    "hidden": false
                },
                {
                    "_id": "684bb33a3b733ba3336870ce",
                    "name": "Shumin Deng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:57:05.000Z",
            "submittedOnDailyAt": "2025-06-13T03:43:12.055Z",
            "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.",
            "upvotes": 9,
            "discussionId": "684bb33a3b733ba3336870cf",
            "ai_summary": "A benchmark for Chinese harmful content detection is introduced, along with a knowledge-augmented model that enhances efficiency and accuracy using human-annotated rules and LLMs.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "harmful content detection",
                "knowledge-augmented baseline",
                "annotation process",
                "knowledge rule base",
                "Chinese datasets"
            ]
        },
        "publishedAt": "2025-06-12T13:57:05.000Z",
        "title": "ChineseHarm-Bench: A Chinese Harmful Content Detection Benchmark",
        "summary": "Large language models (LLMs) have been increasingly applied to automated\nharmful content detection tasks, assisting moderators in identifying policy\nviolations and improving the overall efficiency and accuracy of content review.\nHowever, existing resources for harmful content detection are predominantly\nfocused on English, with Chinese datasets remaining scarce and often limited in\nscope. We present a comprehensive, professionally annotated benchmark for\nChinese content harm detection, which covers six representative categories and\nis constructed entirely from real-world data. Our annotation process further\nyields a knowledge rule base that provides explicit expert knowledge to assist\nLLMs in Chinese harmful content detection. In addition, we propose a\nknowledge-augmented baseline that integrates both human-annotated knowledge\nrules and implicit knowledge from large language models, enabling smaller\nmodels to achieve performance comparable to state-of-the-art LLMs. Code and\ndata are available at https://github.com/zjunlp/ChineseHarm-bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10960.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10953",
            "authors": [
                {
                    "_id": "684b9fa13b733ba333687066",
                    "user": {
                        "_id": "5fa9ff3ea13e063b8b2b60cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
                        "isPro": false,
                        "fullname": "Xing Han L",
                        "user": "xhluca",
                        "type": "user"
                    },
                    "name": "Xing Han L",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:26.665Z",
                    "hidden": false
                },
                {
                    "_id": "684b9fa13b733ba333687067",
                    "name": "Gaurav Kamath",
                    "hidden": false
                },
                {
                    "_id": "684b9fa13b733ba333687068",
                    "name": "Marius Mosbach",
                    "hidden": false
                },
                {
                    "_id": "684b9fa13b733ba333687069",
                    "name": "Siva Reddy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:53:58.000Z",
            "submittedOnDailyAt": "2025-06-13T02:22:59.640Z",
            "title": "Build the web for agents, not agents for the web",
            "submittedOnDailyBy": {
                "_id": "5fa9ff3ea13e063b8b2b60cb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
                "isPro": false,
                "fullname": "Xing Han L",
                "user": "xhluca",
                "type": "user"
            },
            "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.",
            "upvotes": 9,
            "discussionId": "684b9fa13b733ba33368706a",
            "ai_summary": "A new Agentic Web Interface (AWI) design paradigm is proposed to optimize web agents for navigating websites, focusing on safety, efficiency, and standardization to address fundamental interface mismatches.",
            "ai_keywords": [
                "Large Language Models",
                "multimodal",
                "web agents",
                "Agentic Web Interface",
                "AWI",
                "DOM trees",
                "screenshots",
                "API interactions"
            ]
        },
        "publishedAt": "2025-06-12T13:53:58.000Z",
        "title": "Build the web for agents, not agents for the web",
        "summary": "Recent advancements in Large Language Models (LLMs) and multimodal\ncounterparts have spurred significant interest in developing web agents -- AI\nsystems capable of autonomously navigating and completing tasks within web\nenvironments. While holding tremendous promise for automating complex web\ninteractions, current approaches face substantial challenges due to the\nfundamental mismatch between human-designed interfaces and LLM capabilities.\nCurrent methods struggle with the inherent complexity of web inputs, whether\nprocessing massive DOM trees, relying on screenshots augmented with additional\ninformation, or bypassing the user interface entirely through API interactions.\nThis position paper advocates for a paradigm shift in web agent research:\nrather than forcing web agents to adapt to interfaces designed for humans, we\nshould develop a new interaction paradigm specifically optimized for agentic\ncapabilities. To this end, we introduce the concept of an Agentic Web Interface\n(AWI), an interface specifically designed for agents to navigate a website. We\nestablish six guiding principles for AWI design, emphasizing safety,\nefficiency, and standardization, to account for the interests of all primary\nstakeholders. This reframing aims to overcome fundamental limitations of\nexisting interfaces, paving the way for more efficient, reliable, and\ntransparent web agent design, which will be a collaborative effort involving\nthe broader ML community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10953.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "fullname": "Xing Han L",
            "name": "xhluca",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.06952",
            "authors": [
                {
                    "_id": "684b98083b733ba333687040",
                    "name": "Ying Shen",
                    "hidden": false
                },
                {
                    "_id": "684b98083b733ba333687041",
                    "user": {
                        "_id": "665a443ce0bd04136ed3f163",
                        "avatarUrl": "/avatars/ffd910be7722713377e57b059268d2a4.svg",
                        "isPro": false,
                        "fullname": "Zhiyang Xu",
                        "user": "zhiyang1",
                        "type": "user"
                    },
                    "name": "Zhiyang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:29.999Z",
                    "hidden": false
                },
                {
                    "_id": "684b98083b733ba333687042",
                    "name": "Jiuhai Chen",
                    "hidden": false
                },
                {
                    "_id": "684b98083b733ba333687043",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "684b98083b733ba333687044",
                    "name": "Jiaxin Zhang",
                    "hidden": false
                },
                {
                    "_id": "684b98083b733ba333687045",
                    "name": "Yuguang Yao",
                    "hidden": false
                },
                {
                    "_id": "684b98083b733ba333687046",
                    "name": "Joy Rimchala",
                    "hidden": false
                },
                {
                    "_id": "684b98083b733ba333687047",
                    "name": "Ismini Lourentzou",
                    "hidden": false
                },
                {
                    "_id": "684b98083b733ba333687048",
                    "name": "Lifu Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-08T00:15:32.000Z",
            "submittedOnDailyAt": "2025-06-13T16:57:36.945Z",
            "title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer",
            "submittedOnDailyBy": {
                "_id": "665a443ce0bd04136ed3f163",
                "avatarUrl": "/avatars/ffd910be7722713377e57b059268d2a4.svg",
                "isPro": false,
                "fullname": "Zhiyang Xu",
                "user": "zhiyang1",
                "type": "user"
            },
            "summary": "Recent advances in multimodal foundation models unifying image understanding\nand generation have opened exciting avenues for tackling a wide range of\nvision-language tasks within a single framework. Despite progress, existing\nunified models typically require extensive pretraining and struggle to achieve\nthe same level of performance compared to models dedicated to each task.\nAdditionally, many of these models suffer from slow image generation speeds,\nlimiting their practical deployment in real-time or resource-constrained\nsettings. In this work, we propose Layerwise Timestep-Expert Flow-based\nTransformer (LaTtE-Flow), a novel and efficient architecture that unifies image\nunderstanding and generation within a single multimodal model. LaTtE-Flow\nbuilds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong\nmultimodal understanding capabilities, and extends them with a novel Layerwise\nTimestep Experts flow-based architecture for efficient image generation.\nLaTtE-Flow distributes the flow-matching process across specialized groups of\nTransformer layers, each responsible for a distinct subset of timesteps. This\ndesign significantly improves sampling efficiency by activating only a small\nsubset of layers at each sampling timestep. To further enhance performance, we\npropose a Timestep-Conditioned Residual Attention mechanism for efficient\ninformation reuse across layers. Experiments demonstrate that LaTtE-Flow\nachieves strong performance on multimodal understanding tasks, while achieving\ncompetitive image generation quality with around 6x faster inference speed\ncompared to recent unified multimodal models.",
            "upvotes": 9,
            "discussionId": "684b98093b733ba333687049",
            "ai_summary": "LaTtE-Flow, a new architecture, unifies image understanding and generation with high performance and faster inference by using a Layerwise Timestep Experts flow-based Transformer and Timestep-Conditioned Residual Attention mechanism.",
            "ai_keywords": [
                "Vision-Language Models",
                "Layerwise Timestep Experts",
                "flow-based architecture",
                "Transformer layers",
                "flow-matching process",
                "Timestep-Conditioned Residual Attention mechanism"
            ]
        },
        "publishedAt": "2025-06-07T20:15:32.000Z",
        "title": "LaTtE-Flow: Layerwise Timestep-Expert Flow-based Transformer",
        "summary": "Recent advances in multimodal foundation models unifying image understanding\nand generation have opened exciting avenues for tackling a wide range of\nvision-language tasks within a single framework. Despite progress, existing\nunified models typically require extensive pretraining and struggle to achieve\nthe same level of performance compared to models dedicated to each task.\nAdditionally, many of these models suffer from slow image generation speeds,\nlimiting their practical deployment in real-time or resource-constrained\nsettings. In this work, we propose Layerwise Timestep-Expert Flow-based\nTransformer (LaTtE-Flow), a novel and efficient architecture that unifies image\nunderstanding and generation within a single multimodal model. LaTtE-Flow\nbuilds upon powerful pretrained Vision-Language Models (VLMs) to inherit strong\nmultimodal understanding capabilities, and extends them with a novel Layerwise\nTimestep Experts flow-based architecture for efficient image generation.\nLaTtE-Flow distributes the flow-matching process across specialized groups of\nTransformer layers, each responsible for a distinct subset of timesteps. This\ndesign significantly improves sampling efficiency by activating only a small\nsubset of layers at each sampling timestep. To further enhance performance, we\npropose a Timestep-Conditioned Residual Attention mechanism for efficient\ninformation reuse across layers. Experiments demonstrate that LaTtE-Flow\nachieves strong performance on multimodal understanding tasks, while achieving\ncompetitive image generation quality with around 6x faster inference speed\ncompared to recent unified multimodal models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06952.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "665a443ce0bd04136ed3f163",
            "avatarUrl": "/avatars/ffd910be7722713377e57b059268d2a4.svg",
            "fullname": "Zhiyang Xu",
            "name": "zhiyang1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10178",
            "authors": [
                {
                    "_id": "684bfd5d3b733ba3336871f1",
                    "user": {
                        "_id": "626a9b5205fe1cb65720e00e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626a9b5205fe1cb65720e00e/hyWcWn_8jVZsu1Yc5Z0R8.png",
                        "isPro": false,
                        "fullname": "Bill Psomas",
                        "user": "billpsomas",
                        "type": "user"
                    },
                    "name": "Bill Psomas",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:18.359Z",
                    "hidden": false
                },
                {
                    "_id": "684bfd5d3b733ba3336871f2",
                    "name": "Dionysis Christopoulos",
                    "hidden": false
                },
                {
                    "_id": "684bfd5d3b733ba3336871f3",
                    "name": "Eirini Baltzi",
                    "hidden": false
                },
                {
                    "_id": "684bfd5d3b733ba3336871f4",
                    "name": "Ioannis Kakogeorgiou",
                    "hidden": false
                },
                {
                    "_id": "684bfd5d3b733ba3336871f5",
                    "name": "Tilemachos Aravanis",
                    "hidden": false
                },
                {
                    "_id": "684bfd5d3b733ba3336871f6",
                    "name": "Nikos Komodakis",
                    "hidden": false
                },
                {
                    "_id": "684bfd5d3b733ba3336871f7",
                    "name": "Konstantinos Karantzalos",
                    "hidden": false
                },
                {
                    "_id": "684bfd5d3b733ba3336871f8",
                    "name": "Yannis Avrithis",
                    "hidden": false
                },
                {
                    "_id": "684bfd5d3b733ba3336871f9",
                    "name": "Giorgos Tolias",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T21:10:26.000Z",
            "submittedOnDailyAt": "2025-06-13T09:12:31.259Z",
            "title": "Attention, Please! Revisiting Attentive Probing for Masked Image\n  Modeling",
            "submittedOnDailyBy": {
                "_id": "626a9b5205fe1cb65720e00e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626a9b5205fe1cb65720e00e/hyWcWn_8jVZsu1Yc5Z0R8.png",
                "isPro": false,
                "fullname": "Bill Psomas",
                "user": "billpsomas",
                "type": "user"
            },
            "summary": "As fine-tuning (FT) becomes increasingly impractical at scale, probing is\nemerging as the preferred evaluation protocol for self-supervised learning\n(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the\npotential of models trained with Masked Image Modeling (MIM), due to the\ndistributed nature of patch tokens. This motivates the need for attentive\nprobing, an alternative that uses attention to selectively aggregate\npatch-level features. Despite its growing adoption, attentive probing remains\nunder-explored, with existing methods suffering from excessive parameterization\nand poor computational efficiency.\n  In this work, we revisit attentive probing through the lens of the\naccuracy-efficiency trade-off. We conduct a systematic study of existing\nmethods, analyzing their mechanisms and benchmarking their performance. We\nintroduce efficient probing (EP), a multi-query cross-attention mechanism that\neliminates redundant projections, reduces the number of trainable parameters,\nand achieves up to a 10times speed-up over conventional multi-head\nattention. Despite its simplicity, EP outperforms LP and prior attentive\nprobing approaches across seven benchmarks, generalizes well beyond MIM to\ndiverse pre-training paradigms, produces interpretable attention maps, and\nachieves strong gains in low-shot and layer-wise settings. Code available at\nhttps://github.com/billpsomas/efficient-probing.",
            "upvotes": 6,
            "discussionId": "684bfd5d3b733ba3336871fa",
            "githubRepo": "https://github.com/billpsomas/efficient-probing",
            "ai_summary": "Efficient probing using a multi-query cross-attention mechanism enhances performance in self-supervised learning while reducing parameterization and computational cost compared to traditional methods.",
            "ai_keywords": [
                "fine-tuning",
                "self-supervised learning",
                "Masked Image Modeling",
                "linear probing",
                "attentive probing",
                "patch tokens",
                "attention",
                "patch-level features",
                "multi-query cross-attention",
                "parameterization",
                "computational efficiency",
                "accuracy-efficiency trade-off",
                "benchmarks",
                "multi-head attention",
                "low-shot",
                "layer-wise"
            ]
        },
        "publishedAt": "2025-06-11T17:10:26.000Z",
        "title": "Attention, Please! Revisiting Attentive Probing for Masked Image\n  Modeling",
        "summary": "As fine-tuning (FT) becomes increasingly impractical at scale, probing is\nemerging as the preferred evaluation protocol for self-supervised learning\n(SSL). Yet, the standard linear probing (LP) fails to adequately reflect the\npotential of models trained with Masked Image Modeling (MIM), due to the\ndistributed nature of patch tokens. This motivates the need for attentive\nprobing, an alternative that uses attention to selectively aggregate\npatch-level features. Despite its growing adoption, attentive probing remains\nunder-explored, with existing methods suffering from excessive parameterization\nand poor computational efficiency.\n  In this work, we revisit attentive probing through the lens of the\naccuracy-efficiency trade-off. We conduct a systematic study of existing\nmethods, analyzing their mechanisms and benchmarking their performance. We\nintroduce efficient probing (EP), a multi-query cross-attention mechanism that\neliminates redundant projections, reduces the number of trainable parameters,\nand achieves up to a 10times speed-up over conventional multi-head\nattention. Despite its simplicity, EP outperforms LP and prior attentive\nprobing approaches across seven benchmarks, generalizes well beyond MIM to\ndiverse pre-training paradigms, produces interpretable attention maps, and\nachieves strong gains in low-shot and layer-wise settings. Code available at\nhttps://github.com/billpsomas/efficient-probing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10178.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626a9b5205fe1cb65720e00e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/626a9b5205fe1cb65720e00e/hyWcWn_8jVZsu1Yc5Z0R8.png",
            "fullname": "Bill Psomas",
            "name": "billpsomas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09942",
            "authors": [
                {
                    "_id": "684ae26adbd21a9cc27b1177",
                    "user": {
                        "_id": "625a5446f1063e7085d5178a",
                        "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
                        "isPro": false,
                        "fullname": "Hao Peng",
                        "user": "Wesleythu",
                        "type": "user"
                    },
                    "name": "Hao Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:40:06.456Z",
                    "hidden": false
                },
                {
                    "_id": "684ae26adbd21a9cc27b1178",
                    "name": "Yunjia Qi",
                    "hidden": false
                },
                {
                    "_id": "684ae26adbd21a9cc27b1179",
                    "name": "Xiaozhi Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae26adbd21a9cc27b117a",
                    "name": "Bin Xu",
                    "hidden": false
                },
                {
                    "_id": "684ae26adbd21a9cc27b117b",
                    "name": "Lei Hou",
                    "hidden": false
                },
                {
                    "_id": "684ae26adbd21a9cc27b117c",
                    "name": "Juanzi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:10:36.000Z",
            "submittedOnDailyAt": "2025-06-13T00:15:19.828Z",
            "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
            "submittedOnDailyBy": {
                "_id": "625a5446f1063e7085d5178a",
                "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
                "isPro": false,
                "fullname": "Hao Peng",
                "user": "Wesleythu",
                "type": "user"
            },
            "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
            "upvotes": 5,
            "discussionId": "684ae26adbd21a9cc27b117d",
            "githubRepo": "https://github.com/THU-KEG/VerIF",
            "ai_summary": "VerIF, a hybrid verification method combining rule-based and LLM-based approaches, enhances instruction-following RL with significant performance improvements and generalization.",
            "ai_keywords": [
                "reinforcement learning",
                "verifiable rewards",
                "RLVR",
                "large language models",
                "LLMs",
                "rule-based code verification",
                "QwQ-32B",
                "instruction-following",
                "VerInstruct",
                "RL training",
                "instruction-following benchmarks",
                "state-of-the-art performance",
                "existing RL recipes"
            ]
        },
        "publishedAt": "2025-06-11T13:10:36.000Z",
        "title": "VerIF: Verification Engineering for Reinforcement Learning in\n  Instruction Following",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has become a key\ntechnique for enhancing large language models (LLMs), with verification\nengineering playing a central role. However, best practices for RL in\ninstruction following remain underexplored. In this work, we explore the\nverification challenge in RL for instruction following and propose VerIF, a\nverification method that combines rule-based code verification with LLM-based\nverification from a large reasoning model (e.g., QwQ-32B). To support this\napproach, we construct a high-quality instruction-following dataset,\nVerInstruct, containing approximately 22,000 instances with associated\nverification signals. We apply RL training with VerIF to two models, achieving\nsignificant improvements across several representative instruction-following\nbenchmarks. The trained models reach state-of-the-art performance among models\nof comparable size and generalize well to unseen constraints. We further\nobserve that their general capabilities remain unaffected, suggesting that RL\nwith VerIF can be integrated into existing RL recipes to enhance overall model\nperformance. We have released our datasets, codes, and models to facilitate\nfuture research at https://github.com/THU-KEG/VerIF.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09942.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "625a5446f1063e7085d5178a",
            "avatarUrl": "/avatars/5e78186f13f74b14e01583e06ff6c4dc.svg",
            "fullname": "Hao Peng",
            "name": "Wesleythu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09250",
            "authors": [
                {
                    "_id": "684c94ae3b733ba3336872f1",
                    "name": "C. Opus",
                    "hidden": false
                },
                {
                    "_id": "684c94ae3b733ba3336872f2",
                    "name": "A. Lawsen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T21:16:53.000Z",
            "submittedOnDailyAt": "2025-06-13T19:44:50.559Z",
            "title": "Comment on The Illusion of Thinking: Understanding the Strengths and\n  Limitations of Reasoning Models via the Lens of Problem Complexity",
            "submittedOnDailyBy": {
                "_id": "61b85ce86eb1f2c5e6233736",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655385361868-61b85ce86eb1f2c5e6233736.jpeg",
                "isPro": true,
                "fullname": "Vaibhav Srivastav",
                "user": "reach-vb",
                "type": "user"
            },
            "summary": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit\n\"accuracy collapse\" on planning puzzles beyond certain complexity thresholds.\nWe demonstrate that their findings primarily reflect experimental design\nlimitations rather than fundamental reasoning failures. Our analysis reveals\nthree critical issues: (1) Tower of Hanoi experiments systematically exceed\nmodel output token limits at reported failure points, with models explicitly\nacknowledging these constraints in their outputs; (2) The authors' automated\nevaluation framework fails to distinguish between reasoning failures and\npractical constraints, leading to misclassification of model capabilities; (3)\nMost concerningly, their River Crossing benchmarks include mathematically\nimpossible instances for N > 5 due to insufficient boat capacity, yet models\nare scored as failures for not solving these unsolvable problems. When we\ncontrol for these experimental artifacts, by requesting generating functions\ninstead of exhaustive move lists, preliminary experiments across multiple\nmodels indicate high accuracy on Tower of Hanoi instances previously reported\nas complete failures. These findings highlight the importance of careful\nexperimental design when evaluating AI reasoning capabilities.",
            "upvotes": 5,
            "discussionId": "684c94af3b733ba3336872f3",
            "ai_summary": "Evaluation artifacts, particularly token limits and impractical instances in benchmarks, lead to misreported failures in Large Reasoning Models on planning puzzles.",
            "ai_keywords": [
                "Large Reasoning Models",
                "accuracy collapse",
                "Tower of Hanoi",
                "token limits",
                "automated evaluation framework",
                "River Crossing benchmarks",
                "generating functions",
                "exhaustive move lists"
            ]
        },
        "publishedAt": "2025-06-10T17:16:53.000Z",
        "title": "Comment on The Illusion of Thinking: Understanding the Strengths and\n  Limitations of Reasoning Models via the Lens of Problem Complexity",
        "summary": "Shojaee et al. (2025) report that Large Reasoning Models (LRMs) exhibit\n\"accuracy collapse\" on planning puzzles beyond certain complexity thresholds.\nWe demonstrate that their findings primarily reflect experimental design\nlimitations rather than fundamental reasoning failures. Our analysis reveals\nthree critical issues: (1) Tower of Hanoi experiments systematically exceed\nmodel output token limits at reported failure points, with models explicitly\nacknowledging these constraints in their outputs; (2) The authors' automated\nevaluation framework fails to distinguish between reasoning failures and\npractical constraints, leading to misclassification of model capabilities; (3)\nMost concerningly, their River Crossing benchmarks include mathematically\nimpossible instances for N > 5 due to insufficient boat capacity, yet models\nare scored as failures for not solving these unsolvable problems. When we\ncontrol for these experimental artifacts, by requesting generating functions\ninstead of exhaustive move lists, preliminary experiments across multiple\nmodels indicate high accuracy on Tower of Hanoi instances previously reported\nas complete failures. These findings highlight the importance of careful\nexperimental design when evaluating AI reasoning capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09250.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61b85ce86eb1f2c5e6233736",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655385361868-61b85ce86eb1f2c5e6233736.jpeg",
            "fullname": "Vaibhav Srivastav",
            "name": "reach-vb",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 841
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10568",
            "authors": [
                {
                    "_id": "684c06a83b733ba3336871fc",
                    "name": "Lizhen Wang",
                    "hidden": false
                },
                {
                    "_id": "684c06a83b733ba3336871fd",
                    "name": "Zhurong Xia",
                    "hidden": false
                },
                {
                    "_id": "684c06a83b733ba3336871fe",
                    "name": "Tianshu Hu",
                    "hidden": false
                },
                {
                    "_id": "684c06a83b733ba3336871ff",
                    "name": "Pengrui Wang",
                    "hidden": false
                },
                {
                    "_id": "684c06a83b733ba333687200",
                    "name": "Pengfei Wang",
                    "hidden": false
                },
                {
                    "_id": "684c06a83b733ba333687201",
                    "name": "Zerong Zheng",
                    "hidden": false
                },
                {
                    "_id": "684c06a83b733ba333687202",
                    "name": "Ming Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T10:58:23.000Z",
            "submittedOnDailyAt": "2025-06-13T09:41:22.217Z",
            "title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video\n  Generation via Motion-designed Diffusion Transformers",
            "submittedOnDailyBy": {
                "_id": "63451d2dfeba4bdba59cf9b1",
                "avatarUrl": "/avatars/ff5dd2f3b502c54e7c3e2512c3e98b28.svg",
                "isPro": false,
                "fullname": "Lizhen Wang",
                "user": "wanglz14",
                "type": "user"
            },
            "summary": "In e-commerce and digital marketing, generating high-fidelity human-product\ndemonstration videos is important for effective product presentation. However,\nmost existing frameworks either fail to preserve the identities of both humans\nand products or lack an understanding of human-product spatial relationships,\nleading to unrealistic representations and unnatural interactions. To address\nthese challenges, we propose a Diffusion Transformer (DiT)-based framework. Our\nmethod simultaneously preserves human identities and product-specific details,\nsuch as logos and textures, by injecting paired human-product reference\ninformation and utilizing an additional masked cross-attention mechanism. We\nemploy a 3D body mesh template and product bounding boxes to provide precise\nmotion guidance, enabling intuitive alignment of hand gestures with product\nplacements. Additionally, structured text encoding is used to incorporate\ncategory-level semantics, enhancing 3D consistency during small rotational\nchanges across frames. Trained on a hybrid dataset with extensive data\naugmentation strategies, our approach outperforms state-of-the-art techniques\nin maintaining the identity integrity of both humans and products and\ngenerating realistic demonstration motions. Project page:\nhttps://submit2025-dream.github.io/DreamActor-H1/.",
            "upvotes": 4,
            "discussionId": "684c06a83b733ba333687203",
            "ai_summary": "A Diffusion Transformer-based framework generates high-fidelity human-product demonstration videos by preserving identities and spatial relationships, using masked cross-attention and structured text encoding.",
            "ai_keywords": [
                "Diffusion Transformer (DiT)",
                "masked cross-attention",
                "3D body mesh template",
                "product bounding boxes",
                "structured text encoding",
                "hybrid dataset",
                "data augmentation strategies",
                "identity integrity",
                "realistic demonstration motions"
            ]
        },
        "publishedAt": "2025-06-12T06:58:23.000Z",
        "title": "DreamActor-H1: High-Fidelity Human-Product Demonstration Video\n  Generation via Motion-designed Diffusion Transformers",
        "summary": "In e-commerce and digital marketing, generating high-fidelity human-product\ndemonstration videos is important for effective product presentation. However,\nmost existing frameworks either fail to preserve the identities of both humans\nand products or lack an understanding of human-product spatial relationships,\nleading to unrealistic representations and unnatural interactions. To address\nthese challenges, we propose a Diffusion Transformer (DiT)-based framework. Our\nmethod simultaneously preserves human identities and product-specific details,\nsuch as logos and textures, by injecting paired human-product reference\ninformation and utilizing an additional masked cross-attention mechanism. We\nemploy a 3D body mesh template and product bounding boxes to provide precise\nmotion guidance, enabling intuitive alignment of hand gestures with product\nplacements. Additionally, structured text encoding is used to incorporate\ncategory-level semantics, enhancing 3D consistency during small rotational\nchanges across frames. Trained on a hybrid dataset with extensive data\naugmentation strategies, our approach outperforms state-of-the-art techniques\nin maintaining the identity integrity of both humans and products and\ngenerating realistic demonstration motions. Project page:\nhttps://submit2025-dream.github.io/DreamActor-H1/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10568.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63451d2dfeba4bdba59cf9b1",
            "avatarUrl": "/avatars/ff5dd2f3b502c54e7c3e2512c3e98b28.svg",
            "fullname": "Lizhen Wang",
            "name": "wanglz14",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09952",
            "authors": [
                {
                    "_id": "684ae226dbd21a9cc27b107a",
                    "user": {
                        "_id": "63579b21a8e247a69d4e13de",
                        "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
                        "isPro": false,
                        "fullname": "Ziyi Wang",
                        "user": "LavenderLA",
                        "type": "user"
                    },
                    "name": "Ziyi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:40:18.063Z",
                    "hidden": false
                },
                {
                    "_id": "684ae226dbd21a9cc27b107b",
                    "user": {
                        "_id": "661cfae9a853782abad2a495",
                        "avatarUrl": "/avatars/39723a07bf9efed8278e009fe966d044.svg",
                        "isPro": false,
                        "fullname": "Yanran Zhang",
                        "user": "Yanran21",
                        "type": "user"
                    },
                    "name": "Yanran Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:09.847Z",
                    "hidden": false
                },
                {
                    "_id": "684ae226dbd21a9cc27b107c",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "684ae226dbd21a9cc27b107d",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:23:21.000Z",
            "submittedOnDailyAt": "2025-06-13T06:56:09.722Z",
            "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
            "submittedOnDailyBy": {
                "_id": "63579b21a8e247a69d4e13de",
                "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
                "isPro": false,
                "fullname": "Ziyi Wang",
                "user": "LavenderLA",
                "type": "user"
            },
            "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
            "upvotes": 4,
            "discussionId": "684ae226dbd21a9cc27b107e",
            "ai_summary": "UniPre3D is a unified pre-training method for 3D point clouds and models of any scale, using Gaussian primitives and 2D feature integration for effective performance across object and scene tasks.",
            "ai_keywords": [
                "point cloud",
                "3D vision",
                "representation learning",
                "UniPre3D",
                "Gaussian primitives",
                "differentiable Gaussian splatting",
                "pixel-level supervision",
                "end-to-end optimization",
                "2D features",
                "pre-trained image models",
                "geometric structures"
            ]
        },
        "publishedAt": "2025-06-11T13:23:21.000Z",
        "title": "UniPre3D: Unified Pre-training of 3D Point Cloud Models with Cross-Modal\n  Gaussian Splatting",
        "summary": "The scale diversity of point cloud data presents significant challenges in\ndeveloping unified representation learning techniques for 3D vision. Currently,\nthere are few unified 3D models, and no existing pre-training method is equally\neffective for both object- and scene-level point clouds. In this paper, we\nintroduce UniPre3D, the first unified pre-training method that can be\nseamlessly applied to point clouds of any scale and 3D models of any\narchitecture. Our approach predicts Gaussian primitives as the pre-training\ntask and employs differentiable Gaussian splatting to render images, enabling\nprecise pixel-level supervision and end-to-end optimization. To further\nregulate the complexity of the pre-training task and direct the model's focus\ntoward geometric structures, we integrate 2D features from pre-trained image\nmodels to incorporate well-established texture knowledge. We validate the\nuniversal effectiveness of our proposed method through extensive experiments\nacross a variety of object- and scene-level tasks, using diverse point cloud\nmodels as backbones. Code is available at https://github.com/wangzy22/UniPre3D.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09952.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63579b21a8e247a69d4e13de",
            "avatarUrl": "/avatars/7892fbc842eaf616228dfade9f13c712.svg",
            "fullname": "Ziyi Wang",
            "name": "LavenderLA",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08234",
            "authors": [
                {
                    "_id": "684ae1dddbd21a9cc27b0edc",
                    "user": {
                        "_id": "6615752da15c52fa7ab3e2f7",
                        "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
                        "isPro": false,
                        "fullname": "Lee",
                        "user": "Speeeed",
                        "type": "user"
                    },
                    "name": "Yu-Ang Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:40.713Z",
                    "hidden": false
                },
                {
                    "_id": "684ae1dddbd21a9cc27b0edd",
                    "name": "Guan-Ting Yi",
                    "hidden": false
                },
                {
                    "_id": "684ae1dddbd21a9cc27b0ede",
                    "name": "Mei-Yi Liu",
                    "hidden": false
                },
                {
                    "_id": "684ae1dddbd21a9cc27b0edf",
                    "name": "Jui-Chao Lu",
                    "hidden": false
                },
                {
                    "_id": "684ae1dddbd21a9cc27b0ee0",
                    "name": "Guan-Bo Yang",
                    "hidden": false
                },
                {
                    "_id": "684ae1dddbd21a9cc27b0ee1",
                    "name": "Yun-Nung Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T21:04:14.000Z",
            "submittedOnDailyAt": "2025-06-13T07:22:29.081Z",
            "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions",
            "submittedOnDailyBy": {
                "_id": "6615752da15c52fa7ab3e2f7",
                "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
                "isPro": false,
                "fullname": "Lee",
                "user": "Speeeed",
                "type": "user"
            },
            "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.",
            "upvotes": 4,
            "discussionId": "684ae1dedbd21a9cc27b0ee2",
            "ai_summary": "Recent advancements in optimizing compound AI systems highlight challenges in integrating various components, with an emphasis on natural language feedback methods for non-differentiable systems.",
            "ai_keywords": [
                "large language models",
                "AI systems",
                "compound AI systems",
                "supervised fine-tuning",
                "reinforcement learning",
                "natural language feedback",
                "non-differentiable systems"
            ]
        },
        "publishedAt": "2025-06-09T17:04:14.000Z",
        "title": "Compound AI Systems Optimization: A Survey of Methods, Challenges, and\n  Future Directions",
        "summary": "Recent advancements in large language models (LLMs) and AI systems have led\nto a paradigm shift in the design and optimization of complex AI workflows. By\nintegrating multiple components, compound AI systems have become increasingly\nadept at performing sophisticated tasks. However, as these systems grow in\ncomplexity, new challenges arise in optimizing not only individual components\nbut also their interactions. While traditional optimization methods such as\nsupervised fine-tuning (SFT) and reinforcement learning (RL) remain\nfoundational, the rise of natural language feedback introduces promising new\napproaches, especially for optimizing non-differentiable systems. This paper\nprovides a systematic review of recent progress in optimizing compound AI\nsystems, encompassing both numerical and language-based techniques. We\nformalize the notion of compound AI system optimization, classify existing\nmethods along several key dimensions, and highlight open research challenges\nand future directions in this rapidly evolving field. A list of surveyed papers\nis publicly available at https://github.com/MiuLab/AISysOpt-Survey.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08234.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6615752da15c52fa7ab3e2f7",
            "avatarUrl": "/avatars/37e72cfb829a42630d229080ad8d60f3.svg",
            "fullname": "Lee",
            "name": "Speeeed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08060",
            "authors": [
                {
                    "_id": "6848e0b042e4f9106973f280",
                    "user": {
                        "_id": "62f32eab52ad88c930bb3f3b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
                        "isPro": true,
                        "fullname": "Asankhaya Sharma",
                        "user": "codelion",
                        "type": "user"
                    },
                    "name": "Asankhaya Sharma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:35:08.045Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T08:37:19.000Z",
            "submittedOnDailyAt": "2025-06-13T00:31:17.814Z",
            "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
            "submittedOnDailyBy": {
                "_id": "62f32eab52ad88c930bb3f3b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
                "isPro": true,
                "fullname": "Asankhaya Sharma",
                "user": "codelion",
                "type": "user"
            },
            "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength l, datasets of size Oleft( m V{varepsilon^2} log\nm{delta} right) or, with bounded context, Oleft( l\nlog V{varepsilon^2} log 1{delta} right) suffice to approximate\nfine-tuned behavior across m contexts within error varepsilon, where V\nis the vocabulary size and delta is the failure probability. For linear\nclassification, datasets of size Oleft( d{varepsilon}\nright) or, with fixed context, Oleft( 1{varepsilon^2} log\n1{delta} right) are sufficient, where d is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.",
            "upvotes": 4,
            "discussionId": "6848e0b042e4f9106973f281",
            "githubRepo": "https://github.com/codelion/optillm",
            "ai_summary": "Transformers can approximate supervised fine-tuning capabilities through in-context learning without altering model parameters, supported by theoretical bounds and practical techniques.",
            "ai_keywords": [
                "supervised fine-tuning",
                "in-context learning",
                "base transformer model",
                "Turing completeness",
                "retrieval-augmented generation",
                "text generation",
                "linear classification"
            ]
        },
        "publishedAt": "2025-06-09T04:37:19.000Z",
        "title": "Eliciting Fine-Tuned Transformer Capabilities via Inference-Time\n  Techniques",
        "summary": "Large language models have transformed natural language processing, yet\nsupervised fine-tuning (SFT) remains computationally intensive. This paper\nformally proves that capabilities acquired through SFT can be approximated by a\nbase transformer model using inference-time techniques, specifically in-context\nlearning (ICL), without altering model parameters, under idealized assumptions\nincluding unbounded computational resources and access to the fine-tuning\ndataset. We extend these results to practical scenarios with finite context\nlengths and partial dataset access. For text generation tasks with fixed output\nlength l, datasets of size Oleft( m V{varepsilon^2} log\nm{delta} right) or, with bounded context, Oleft( l\nlog V{varepsilon^2} log 1{delta} right) suffice to approximate\nfine-tuned behavior across m contexts within error varepsilon, where V\nis the vocabulary size and delta is the failure probability. For linear\nclassification, datasets of size Oleft( d{varepsilon}\nright) or, with fixed context, Oleft( 1{varepsilon^2} log\n1{delta} right) are sufficient, where d is the input dimension.\nGrounded in the Turing completeness of transformers, these results provide a\ntheoretical foundation for resource-efficient deployment of large language\nmodels, with practical techniques like retrieval-augmented generation bridging\ntheory to real-world applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08060.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f32eab52ad88c930bb3f3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
            "fullname": "Asankhaya Sharma",
            "name": "codelion",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 93
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.06950",
            "authors": [
                {
                    "_id": "684ae1fbdbd21a9cc27b0f51",
                    "name": "Do Xuan Long",
                    "hidden": false
                },
                {
                    "_id": "684ae1fbdbd21a9cc27b0f52",
                    "name": "Duy Dinh",
                    "hidden": false
                },
                {
                    "_id": "684ae1fbdbd21a9cc27b0f53",
                    "name": "Ngoc-Hai Nguyen",
                    "hidden": false
                },
                {
                    "_id": "684ae1fbdbd21a9cc27b0f54",
                    "name": "Kenji Kawaguchi",
                    "hidden": false
                },
                {
                    "_id": "684ae1fbdbd21a9cc27b0f55",
                    "name": "Nancy F. Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae1fbdbd21a9cc27b0f56",
                    "name": "Shafiq Joty",
                    "hidden": false
                },
                {
                    "_id": "684ae1fbdbd21a9cc27b0f57",
                    "name": "Min-Yen Kan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-07T23:19:27.000Z",
            "submittedOnDailyAt": "2025-06-13T03:14:57.042Z",
            "title": "What Makes a Good Natural Language Prompt?",
            "submittedOnDailyBy": {
                "_id": "63a9a0d13453852ef53c0b37",
                "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
                "isPro": false,
                "fullname": "Do Xuan Long",
                "user": "dxlong2000",
                "type": "user"
            },
            "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.",
            "upvotes": 4,
            "discussionId": "684ae1fbdbd21a9cc27b0f58",
            "ai_summary": "A framework for evaluating and optimizing natural language prompts in large language models is proposed, revealing correlations between prompt properties and their impact on reasoning tasks.",
            "ai_keywords": [
                "large language models",
                "prompting",
                "meta-analysis",
                "property-centric framework",
                "instruction-tuning",
                "reasoning tasks"
            ]
        },
        "publishedAt": "2025-06-07T19:19:27.000Z",
        "title": "What Makes a Good Natural Language Prompt?",
        "summary": "As large language models (LLMs) have progressed towards more human-like and\nhuman--AI communications have become prevalent, prompting has emerged as a\ndecisive component. However, there is limited conceptual consensus on what\nexactly quantifies natural language prompts. We attempt to address this\nquestion by conducting a meta-analysis surveying more than 150\nprompting-related papers from leading NLP and AI conferences from 2022 to 2025\nand blogs. We propose a property- and human-centric framework for evaluating\nprompt quality, encompassing 21 properties categorized into six dimensions. We\nthen examine how existing studies assess their impact on LLMs, revealing their\nimbalanced support across models and tasks, and substantial research gaps.\nFurther, we analyze correlations among properties in high-quality natural\nlanguage prompts, deriving prompting recommendations. We then empirically\nexplore multi-property prompt enhancements in reasoning tasks, observing that\nsingle-property enhancements often have the greatest impact. Finally, we\ndiscover that instruction-tuning on property-enhanced prompts can result in\nbetter reasoning models. Our findings establish a foundation for\nproperty-centric prompt evaluation and optimization, bridging the gaps between\nhuman--AI communication and opening new prompting research directions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06950.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a9a0d13453852ef53c0b37",
            "avatarUrl": "/avatars/411c4ffc2a3e89047a23c6e7442f6ed5.svg",
            "fullname": "Do Xuan Long",
            "name": "dxlong2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10674",
            "authors": [
                {
                    "_id": "684bdf0f3b733ba3336871c3",
                    "user": {
                        "_id": "66e81005f42f13eb43120bb3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/B_PUi4PjBNnfpQ09Lvtn0.png",
                        "isPro": false,
                        "fullname": "Vincenzo Colle",
                        "user": "vincolle",
                        "type": "user"
                    },
                    "name": "Vincenzo Colle",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:22.494Z",
                    "hidden": false
                },
                {
                    "_id": "684bdf0f3b733ba3336871c4",
                    "name": "Mohamed Sana",
                    "hidden": false
                },
                {
                    "_id": "684bdf0f3b733ba3336871c5",
                    "name": "Nicola Piovesan",
                    "hidden": false
                },
                {
                    "_id": "684bdf0f3b733ba3336871c6",
                    "name": "Antonio De Domenico",
                    "hidden": false
                },
                {
                    "_id": "684bdf0f3b733ba3336871c7",
                    "name": "Fadhel Ayed",
                    "hidden": false
                },
                {
                    "_id": "684bdf0f3b733ba3336871c8",
                    "name": "Merouane Debbah",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T13:04:18.000Z",
            "submittedOnDailyAt": "2025-06-13T13:49:25.885Z",
            "title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical\n  Problem Solving",
            "submittedOnDailyBy": {
                "_id": "66e81005f42f13eb43120bb3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/B_PUi4PjBNnfpQ09Lvtn0.png",
                "isPro": false,
                "fullname": "Vincenzo Colle",
                "user": "vincolle",
                "type": "user"
            },
            "summary": "The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath, the first benchmark dataset specifically\ndesigned to evaluate LLM performance in solving mathematical problems with\nnumerical solutions in the telecommunications domain. Comprising 500\nquestion-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the\ntelecommunications field. This paper outlines the proposed QnAs generation\npipeline, starting from a selected seed of problems crafted by Subject Matter\nExperts. The evaluation of a wide range of open-source LLMs reveals that best\nperformance on TeleMath is achieved by recent models explicitly designed for\nmathematical or logical reasoning. In contrast, general-purpose models, even\nthose with a large number of parameters, often struggle with these challenges.\nWe have released the dataset and the evaluation code to ease result\nreproducibility and support future research.",
            "upvotes": 3,
            "discussionId": "684bdf0f3b733ba3336871c9",
            "ai_summary": "A benchmark dataset called TeleMath evaluates Large Language Models in domain-specific mathematical problems within telecommunications, showing that models designed for mathematical reasoning perform better than general-purpose models.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "mathematical reasoning",
                "signal processing",
                "network optimization",
                "performance analysis",
                "benchmark dataset",
                "QnA pairs",
                "mathematical problems",
                "numerical solutions",
                "evaluation pipeline",
                "open-source models",
                "parameter-efficient fine-tuning",
                "result reproducibility"
            ]
        },
        "publishedAt": "2025-06-12T09:04:18.000Z",
        "title": "TeleMath: A Benchmark for Large Language Models in Telecom Mathematical\n  Problem Solving",
        "summary": "The increasing adoption of artificial intelligence in telecommunications has\nraised interest in the capability of Large Language Models (LLMs) to address\ndomain-specific, mathematically intensive tasks. Although recent advancements\nhave improved the performance of LLMs in general mathematical reasoning, their\neffectiveness within specialized domains, such as signal processing, network\noptimization, and performance analysis, remains largely unexplored. To address\nthis gap, we introduce TeleMath, the first benchmark dataset specifically\ndesigned to evaluate LLM performance in solving mathematical problems with\nnumerical solutions in the telecommunications domain. Comprising 500\nquestion-answer (QnA) pairs, TeleMath covers a wide spectrum of topics in the\ntelecommunications field. This paper outlines the proposed QnAs generation\npipeline, starting from a selected seed of problems crafted by Subject Matter\nExperts. The evaluation of a wide range of open-source LLMs reveals that best\nperformance on TeleMath is achieved by recent models explicitly designed for\nmathematical or logical reasoning. In contrast, general-purpose models, even\nthose with a large number of parameters, often struggle with these challenges.\nWe have released the dataset and the evaluation code to ease result\nreproducibility and support future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10674.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66e81005f42f13eb43120bb3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/B_PUi4PjBNnfpQ09Lvtn0.png",
            "fullname": "Vincenzo Colle",
            "name": "vincolle",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.07795",
            "authors": [
                {
                    "_id": "6848dca942e4f9106973f25c",
                    "user": {
                        "_id": "6659b410a69183808d04b22f",
                        "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
                        "isPro": false,
                        "fullname": "Xiaotian Ye",
                        "user": "Acruxos",
                        "type": "user"
                    },
                    "name": "Xiaotian Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:35:10.086Z",
                    "hidden": false
                },
                {
                    "_id": "6848dca942e4f9106973f25d",
                    "name": "Mengqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6848dca942e4f9106973f25e",
                    "name": "Shu Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T14:21:25.000Z",
            "submittedOnDailyAt": "2025-06-13T05:51:30.395Z",
            "title": "LLM Unlearning Should Be Form-Independent",
            "submittedOnDailyBy": {
                "_id": "6659b410a69183808d04b22f",
                "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
                "isPro": false,
                "fullname": "Xiaotian Ye",
                "user": "Acruxos",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.",
            "upvotes": 3,
            "discussionId": "6848dca942e4f9106973f25f",
            "ai_summary": "Form-Dependent Bias limits the effectiveness of LLM unlearning across different knowledge expressions, and Rank-one Concept Redirection (ROCR) is proposed as a form-independent solution that enhances unlearning efficacy.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "unlearning",
                "Form-Dependent Bias",
                "ORT",
                "Rank-one Concept Redirection (ROCR)",
                "downstream tasks",
                "unlearning methods",
                "concept redirection",
                "model parameters",
                "activated dangerous concepts"
            ]
        },
        "publishedAt": "2025-06-09T10:21:25.000Z",
        "title": "LLM Unlearning Should Be Form-Independent",
        "summary": "Large Language Model (LLM) unlearning aims to erase or suppress undesirable\nknowledge within the model, offering promise for controlling harmful or private\ninformation to prevent misuse. However, recent studies highlight its limited\nefficacy in real-world scenarios, hindering practical adoption. In this study,\nwe identify a pervasive issue underlying many downstream failures: the\neffectiveness of existing unlearning methods heavily depends on the form of\ntraining samples and frequently fails to generalize to alternate expressions of\nthe same knowledge. We formally characterize this problem as Form-Dependent\nBias and systematically investigate its specific manifestation patterns across\nvarious downstream tasks. To quantify its prevalence and support future\nresearch, we introduce ORT, a novel benchmark designed to evaluate the\nrobustness of unlearning methods against variations in knowledge expression.\nResults reveal that Form-Dependent Bias is both widespread and severe among\ncurrent techniques.\n  We argue that LLM unlearning should be form-independent to address the\nendless forms of downstream tasks encountered in real-world security-critical\nscenarios. Towards this goal, we introduce Rank-one Concept Redirection (ROCR),\na novel training-free method, as a promising solution path. ROCR performs\nunlearning by targeting the invariants in downstream tasks, specifically the\nactivated dangerous concepts. It is capable of modifying model parameters\nwithin seconds to redirect the model's perception of a specific unlearning\ntarget concept to another harmless concept. Extensive experiments demonstrate\nthat ROCR significantly improves unlearning effectiveness compared to\ntraditional methods while generating highly natural outputs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07795.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6659b410a69183808d04b22f",
            "avatarUrl": "/avatars/a16d1fed9ef87163fe458b10c477140b.svg",
            "fullname": "Xiaotian Ye",
            "name": "Acruxos",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10978",
            "authors": [
                {
                    "_id": "684c09ee3b733ba333687205",
                    "name": "Donghoon Ahn",
                    "hidden": false
                },
                {
                    "_id": "684c09ee3b733ba333687206",
                    "name": "Jiwon Kang",
                    "hidden": false
                },
                {
                    "_id": "684c09ee3b733ba333687207",
                    "name": "Sanghyun Lee",
                    "hidden": false
                },
                {
                    "_id": "684c09ee3b733ba333687208",
                    "name": "Minjae Kim",
                    "hidden": false
                },
                {
                    "_id": "684c09ee3b733ba333687209",
                    "name": "Jaewon Min",
                    "hidden": false
                },
                {
                    "_id": "684c09ee3b733ba33368720a",
                    "name": "Wooseok Jang",
                    "hidden": false
                },
                {
                    "_id": "684c09ee3b733ba33368720b",
                    "name": "Saungwu Lee",
                    "hidden": false
                },
                {
                    "_id": "684c09ee3b733ba33368720c",
                    "name": "Sayak Paul",
                    "hidden": false
                },
                {
                    "_id": "684c09ee3b733ba33368720d",
                    "name": "Susung Hong",
                    "hidden": false
                },
                {
                    "_id": "684c09ee3b733ba33368720e",
                    "name": "Seungryong Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:59:51.000Z",
            "submittedOnDailyAt": "2025-06-13T09:52:56.305Z",
            "title": "Fine-Grained Perturbation Guidance via Attention Head Selection",
            "submittedOnDailyBy": {
                "_id": "5f7fbd813e94f16a85448745",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
                "isPro": false,
                "fullname": "Sayak Paul",
                "user": "sayakpaul",
                "type": "user"
            },
            "summary": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer (DiT) architectures where\nquality-relevant computations are distributed across layers. In this paper, we\ninvestigate the granularity of attention perturbations, ranging from the layer\nlevel down to individual attention heads, and discover that specific heads\ngovern distinct visual concepts such as structure, style, and texture quality.\nBuilding on this insight, we propose \"HeadHunter\", a systematic framework for\niteratively selecting attention heads that align with user-centric objectives,\nenabling fine-grained control over generation quality and visual attributes. In\naddition, we introduce SoftPAG, which linearly interpolates each selected\nhead's attention map toward an identity matrix, providing a continuous knob to\ntune perturbation strength and suppress artifacts. Our approach not only\nmitigates the oversmoothing issues of existing layer-level perturbation but\nalso enables targeted manipulation of specific visual styles through\ncompositional head selection. We validate our method on modern large-scale\nDiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,\ndemonstrating superior performance in both general quality enhancement and\nstyle-specific guidance. Our work provides the first head-level analysis of\nattention perturbation in diffusion models, uncovering interpretable\nspecialization within attention layers and enabling practical design of\neffective perturbation strategies.",
            "upvotes": 2,
            "discussionId": "684c09ee3b733ba33368720f",
            "ai_summary": "The paper proposes HeadHunter, a systematic framework for selecting attention heads in Diffusion Transformer architectures to enable precise control over image generation quality and style, outperforming existing methods.",
            "ai_keywords": [
                "diffusion models",
                "attention perturbation",
                "Diffusion Transformer",
                "DiT",
                "attention heads",
                "visual concepts",
                "HeadHunter",
                "SoftPAG",
                "identity matrix",
                "targeted manipulation",
                "visual styles",
                "compositional head selection",
                "text-to-image models",
                "Stable Diffusion 3",
                "FLUX"
            ]
        },
        "publishedAt": "2025-06-12T13:59:51.000Z",
        "title": "Fine-Grained Perturbation Guidance via Attention Head Selection",
        "summary": "Recent guidance methods in diffusion models steer reverse sampling by\nperturbing the model to construct an implicit weak model and guide generation\naway from it. Among these approaches, attention perturbation has demonstrated\nstrong empirical performance in unconditional scenarios where classifier-free\nguidance is not applicable. However, existing attention perturbation methods\nlack principled approaches for determining where perturbations should be\napplied, particularly in Diffusion Transformer (DiT) architectures where\nquality-relevant computations are distributed across layers. In this paper, we\ninvestigate the granularity of attention perturbations, ranging from the layer\nlevel down to individual attention heads, and discover that specific heads\ngovern distinct visual concepts such as structure, style, and texture quality.\nBuilding on this insight, we propose \"HeadHunter\", a systematic framework for\niteratively selecting attention heads that align with user-centric objectives,\nenabling fine-grained control over generation quality and visual attributes. In\naddition, we introduce SoftPAG, which linearly interpolates each selected\nhead's attention map toward an identity matrix, providing a continuous knob to\ntune perturbation strength and suppress artifacts. Our approach not only\nmitigates the oversmoothing issues of existing layer-level perturbation but\nalso enables targeted manipulation of specific visual styles through\ncompositional head selection. We validate our method on modern large-scale\nDiT-based text-to-image models including Stable Diffusion 3 and FLUX.1,\ndemonstrating superior performance in both general quality enhancement and\nstyle-specific guidance. Our work provides the first head-level analysis of\nattention perturbation in diffusion models, uncovering interpretable\nspecialization within attention layers and enabling practical design of\neffective perturbation strategies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10978.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f7fbd813e94f16a85448745",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649681653581-5f7fbd813e94f16a85448745.jpeg",
            "fullname": "Sayak Paul",
            "name": "sayakpaul",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 640
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10920",
            "authors": [
                {
                    "_id": "684c39883b733ba333687278",
                    "user": {
                        "_id": "67b738790d11464dcbc8adf5",
                        "avatarUrl": "/avatars/f59109ff562d7b037be2f81bb11df270.svg",
                        "isPro": false,
                        "fullname": "Or Shafran",
                        "user": "ordavids1",
                        "type": "user"
                    },
                    "name": "Or Shafran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:09.236Z",
                    "hidden": false
                },
                {
                    "_id": "684c39883b733ba333687279",
                    "user": {
                        "_id": "627b2d0527dc4650b62eef42",
                        "avatarUrl": "/avatars/e70381850f5657b54e90f5539f3d74eb.svg",
                        "isPro": false,
                        "fullname": "Atticus Geiger",
                        "user": "atticusg",
                        "type": "user"
                    },
                    "name": "Atticus Geiger",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-13T15:21:27.694Z",
                    "hidden": false
                },
                {
                    "_id": "684c39883b733ba33368727a",
                    "name": "Mor Geva",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:33:29.000Z",
            "submittedOnDailyAt": "2025-06-13T14:10:58.129Z",
            "title": "Decomposing MLP Activations into Interpretable Features via\n  Semi-Nonnegative Matrix Factorization",
            "submittedOnDailyBy": {
                "_id": "67b738790d11464dcbc8adf5",
                "avatarUrl": "/avatars/f59109ff562d7b037be2f81bb11df270.svg",
                "isPro": false,
                "fullname": "Or Shafran",
                "user": "ordavids1",
                "type": "user"
            },
            "summary": "A central goal for mechanistic interpretability has been to identify the\nright units of analysis in large language models (LLMs) that causally explain\ntheir outputs. While early work focused on individual neurons, evidence that\nneurons often encode multiple concepts has motivated a shift toward analyzing\ndirections in activation space. A key question is how to find directions that\ncapture interpretable features in an unsupervised manner. Current methods rely\non dictionary learning with sparse autoencoders (SAEs), commonly trained over\nresidual stream activations to learn directions from scratch. However, SAEs\noften struggle in causal evaluations and lack intrinsic interpretability, as\ntheir learning is not explicitly tied to the computations of the model. Here,\nwe tackle these limitations by directly decomposing MLP activations with\nsemi-nonnegative matrix factorization (SNMF), such that the learned features\nare (a) sparse linear combinations of co-activated neurons, and (b) mapped to\ntheir activating inputs, making them directly interpretable. Experiments on\nLlama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs\nand a strong supervised baseline (difference-in-means) on causal steering,\nwhile aligning with human-interpretable concepts. Further analysis reveals that\nspecific neuron combinations are reused across semantically-related features,\nexposing a hierarchical structure in the MLP's activation space. Together,\nthese results position SNMF as a simple and effective tool for identifying\ninterpretable features and dissecting concept representations in LLMs.",
            "upvotes": 2,
            "discussionId": "684c39883b733ba33368727b",
            "ai_summary": "SNMF is used to identify interpretable features in LLMs by directly decomposing MLP activations, outperforming SAEs and supervised methods in causal evaluations and aligning with human-interpretable concepts.",
            "ai_keywords": [
                "mechanistic interpretability",
                "large language models (LLMs)",
                "individual neurons",
                "activation space",
                "dictionary learning",
                "sparse autoencoders (SAEs)",
                "semi-nonnegative matrix factorization (SNMF)",
                "MLP activations",
                "causal steering",
                "concept representations"
            ]
        },
        "publishedAt": "2025-06-12T13:33:29.000Z",
        "title": "Decomposing MLP Activations into Interpretable Features via\n  Semi-Nonnegative Matrix Factorization",
        "summary": "A central goal for mechanistic interpretability has been to identify the\nright units of analysis in large language models (LLMs) that causally explain\ntheir outputs. While early work focused on individual neurons, evidence that\nneurons often encode multiple concepts has motivated a shift toward analyzing\ndirections in activation space. A key question is how to find directions that\ncapture interpretable features in an unsupervised manner. Current methods rely\non dictionary learning with sparse autoencoders (SAEs), commonly trained over\nresidual stream activations to learn directions from scratch. However, SAEs\noften struggle in causal evaluations and lack intrinsic interpretability, as\ntheir learning is not explicitly tied to the computations of the model. Here,\nwe tackle these limitations by directly decomposing MLP activations with\nsemi-nonnegative matrix factorization (SNMF), such that the learned features\nare (a) sparse linear combinations of co-activated neurons, and (b) mapped to\ntheir activating inputs, making them directly interpretable. Experiments on\nLlama 3.1, Gemma 2 and GPT-2 show that SNMF derived features outperform SAEs\nand a strong supervised baseline (difference-in-means) on causal steering,\nwhile aligning with human-interpretable concepts. Further analysis reveals that\nspecific neuron combinations are reused across semantically-related features,\nexposing a hierarchical structure in the MLP's activation space. Together,\nthese results position SNMF as a simple and effective tool for identifying\ninterpretable features and dissecting concept representations in LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10920.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67b738790d11464dcbc8adf5",
            "avatarUrl": "/avatars/f59109ff562d7b037be2f81bb11df270.svg",
            "fullname": "Or Shafran",
            "name": "ordavids1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10911",
            "authors": [
                {
                    "_id": "684c72743b733ba3336872cb",
                    "user": {
                        "_id": "6760ac9b53b5d7148556d5ef",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/kkodSQK4l5zkh-xAXe-K3.png",
                        "isPro": false,
                        "fullname": "Jari Kolehmainen",
                        "user": "jkolehm",
                        "type": "user"
                    },
                    "name": "Jari Kolehmainen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-13T18:48:21.254Z",
                    "hidden": false
                },
                {
                    "_id": "684c72743b733ba3336872cc",
                    "name": "Nikolay Blagoev",
                    "hidden": false
                },
                {
                    "_id": "684c72743b733ba3336872cd",
                    "name": "John Donaghy",
                    "hidden": false
                },
                {
                    "_id": "684c72743b733ba3336872ce",
                    "name": "Ouzhan Ersoy",
                    "hidden": false
                },
                {
                    "_id": "684c72743b733ba3336872cf",
                    "name": "Christopher Nies",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66d252ec8a438492b0d6e4ce/a0ObpsEzLCuavWPLZfb1n.jpeg"
            ],
            "publishedAt": "2025-06-12T17:23:23.000Z",
            "submittedOnDailyAt": "2025-06-13T17:26:09.514Z",
            "title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models",
            "submittedOnDailyBy": {
                "_id": "66d252ec8a438492b0d6e4ce",
                "avatarUrl": "/avatars/4f0fc7fbe8afae463741f786ea774e95.svg",
                "isPro": true,
                "fullname": "Ben",
                "user": "benfielding",
                "type": "user"
            },
            "summary": "Training large language models is generally done via optimization methods on\nclusters containing tens of thousands of accelerators, communicating over a\nhigh-bandwidth interconnect. Scaling up these clusters is expensive and can\nbecome impractical, imposing limits on the size of models that can be trained.\nSeveral recent studies have proposed training methods that are less\ncommunication intensive, avoiding the need for a highly connected compute\ncluster. These state-of-the-art low communication training methods still employ\na synchronization step for model parameters, which, when performed over all\nmodel replicas, can become costly on a low-bandwidth network.\n  In this work, we propose a novel optimization method, NoLoCo, that does not\nexplicitly synchronize all model parameters during training and, as a result,\ndoes not require any collective communication. NoLoCo implicitly synchronizes\nmodel weights via a novel variant of the Nesterov momentum optimizer by\npartially averaging model weights with a randomly selected other one. We\nprovide both a theoretical convergence analysis for our proposed optimizer as\nwell as empirical results from language model training.\n  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,\nbetween 125M to 6.8B parameters. Our method requires significantly less\ncommunication overhead than fully sharded data parallel training or even widely\nused low communication training method, DiLoCo. The synchronization step itself\nis estimated to be one magnitude faster than the all-reduce used in DiLoCo for\nfew hundred accelerators training over the internet. We also do not have any\nglobal blocking communication that reduces accelerator idling time. Compared to\nDiLoCo, we also observe up to 4% faster convergence rate with wide range of\nmodel sizes and accelerator counts.",
            "upvotes": 2,
            "discussionId": "684c72743b733ba3336872d0",
            "projectPage": "https://www.gensyn.ai/articles/noloco",
            "githubRepo": "https://github.com/gensyn-ai/noloco",
            "ai_summary": "NoLoCo is a novel optimization method that eliminates explicit parameter synchronization and reduces communication overhead during the training of large language models, achieving faster convergence rates and reduced idling time compared to existing methods.",
            "ai_keywords": [
                "NoLoCo",
                "Nesterov momentum optimizer",
                "parameter synchronization",
                "collective communication",
                "communication overhead",
                "empirical results",
                "language model training",
                "theoretical convergence analysis",
                "fully sharded data parallel training",
                "DiLoCo",
                "convergence rate",
                "accelerator idling time"
            ]
        },
        "publishedAt": "2025-06-12T13:23:23.000Z",
        "title": "NoLoCo: No-all-reduce Low Communication Training Method for Large Models",
        "summary": "Training large language models is generally done via optimization methods on\nclusters containing tens of thousands of accelerators, communicating over a\nhigh-bandwidth interconnect. Scaling up these clusters is expensive and can\nbecome impractical, imposing limits on the size of models that can be trained.\nSeveral recent studies have proposed training methods that are less\ncommunication intensive, avoiding the need for a highly connected compute\ncluster. These state-of-the-art low communication training methods still employ\na synchronization step for model parameters, which, when performed over all\nmodel replicas, can become costly on a low-bandwidth network.\n  In this work, we propose a novel optimization method, NoLoCo, that does not\nexplicitly synchronize all model parameters during training and, as a result,\ndoes not require any collective communication. NoLoCo implicitly synchronizes\nmodel weights via a novel variant of the Nesterov momentum optimizer by\npartially averaging model weights with a randomly selected other one. We\nprovide both a theoretical convergence analysis for our proposed optimizer as\nwell as empirical results from language model training.\n  We benchmark NoLoCo on a wide range of accelerator counts and model sizes,\nbetween 125M to 6.8B parameters. Our method requires significantly less\ncommunication overhead than fully sharded data parallel training or even widely\nused low communication training method, DiLoCo. The synchronization step itself\nis estimated to be one magnitude faster than the all-reduce used in DiLoCo for\nfew hundred accelerators training over the internet. We also do not have any\nglobal blocking communication that reduces accelerator idling time. Compared to\nDiLoCo, we also observe up to 4% faster convergence rate with wide range of\nmodel sizes and accelerator counts.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66d252ec8a438492b0d6e4ce/a0ObpsEzLCuavWPLZfb1n.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10911.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d252ec8a438492b0d6e4ce",
            "avatarUrl": "/avatars/4f0fc7fbe8afae463741f786ea774e95.svg",
            "fullname": "Ben",
            "name": "benfielding",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10737",
            "authors": [
                {
                    "_id": "684c2c6c3b733ba333687248",
                    "user": {
                        "_id": "6476ae4083d4fdaedddf405f",
                        "avatarUrl": "/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg",
                        "isPro": false,
                        "fullname": "Priyanka Kargupta",
                        "user": "pkargupta",
                        "type": "user"
                    },
                    "name": "Priyanka Kargupta",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:14.479Z",
                    "hidden": false
                },
                {
                    "_id": "684c2c6c3b733ba333687249",
                    "name": "Nan Zhang",
                    "hidden": false
                },
                {
                    "_id": "684c2c6c3b733ba33368724a",
                    "name": "Yunyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "684c2c6c3b733ba33368724b",
                    "name": "Rui Zhang",
                    "hidden": false
                },
                {
                    "_id": "684c2c6c3b733ba33368724c",
                    "name": "Prasenjit Mitra",
                    "hidden": false
                },
                {
                    "_id": "684c2c6c3b733ba33368724d",
                    "name": "Jiawei Han",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/74HPyELcUejedIKkwVDfl.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/jhWsY6N7gO921TvBcwXFh.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/jtpBSKwYIzYRkceWALmXE.png"
            ],
            "publishedAt": "2025-06-12T14:26:28.000Z",
            "submittedOnDailyAt": "2025-06-13T12:22:31.509Z",
            "title": "TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to\n  Evolving Research Corpora",
            "submittedOnDailyBy": {
                "_id": "6476ae4083d4fdaedddf405f",
                "avatarUrl": "/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg",
                "isPro": false,
                "fullname": "Priyanka Kargupta",
                "user": "pkargupta",
                "type": "user"
            },
            "summary": "The rapid evolution of scientific fields introduces challenges in organizing\nand retrieving scientific literature. While expert-curated taxonomies have\ntraditionally addressed this need, the process is time-consuming and expensive.\nFurthermore, recent automatic taxonomy construction methods either (1)\nover-rely on a specific corpus, sacrificing generalizability, or (2) depend\nheavily on the general knowledge of large language models (LLMs) contained\nwithin their pre-training datasets, often overlooking the dynamic nature of\nevolving scientific domains. Additionally, these approaches fail to account for\nthe multi-faceted nature of scientific literature, where a single research\npaper may contribute to multiple dimensions (e.g., methodology, new tasks,\nevaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a\nframework that dynamically adapts an LLM-generated taxonomy to a given corpus\nacross multiple dimensions. TaxoAdapt performs iterative hierarchical\nclassification, expanding both the taxonomy width and depth based on corpus'\ntopical distribution. We demonstrate its state-of-the-art performance across a\ndiverse set of computer science conferences over the years to showcase its\nability to structure and capture the evolution of scientific fields. As a\nmultidimensional method, TaxoAdapt generates taxonomies that are 26.51% more\ngranularity-preserving and 50.41% more coherent than the most competitive\nbaselines judged by LLMs.",
            "upvotes": 2,
            "discussionId": "684c2c6d3b733ba33368724e",
            "githubRepo": "https://github.com/pkargupta/taxoadapt",
            "ai_summary": "TaxoAdapt dynamically adapts an LLM-generated taxonomy for scientific literature across multiple dimensions, improving granularity and coherence compared to existing methods.",
            "ai_keywords": [
                "LLM-generated taxonomy",
                "iterative hierarchical classification",
                "computer science conferences",
                "granularity-preserving",
                "coherence"
            ]
        },
        "publishedAt": "2025-06-12T10:26:28.000Z",
        "title": "TaxoAdapt: Aligning LLM-Based Multidimensional Taxonomy Construction to\n  Evolving Research Corpora",
        "summary": "The rapid evolution of scientific fields introduces challenges in organizing\nand retrieving scientific literature. While expert-curated taxonomies have\ntraditionally addressed this need, the process is time-consuming and expensive.\nFurthermore, recent automatic taxonomy construction methods either (1)\nover-rely on a specific corpus, sacrificing generalizability, or (2) depend\nheavily on the general knowledge of large language models (LLMs) contained\nwithin their pre-training datasets, often overlooking the dynamic nature of\nevolving scientific domains. Additionally, these approaches fail to account for\nthe multi-faceted nature of scientific literature, where a single research\npaper may contribute to multiple dimensions (e.g., methodology, new tasks,\nevaluation metrics, benchmarks). To address these gaps, we propose TaxoAdapt, a\nframework that dynamically adapts an LLM-generated taxonomy to a given corpus\nacross multiple dimensions. TaxoAdapt performs iterative hierarchical\nclassification, expanding both the taxonomy width and depth based on corpus'\ntopical distribution. We demonstrate its state-of-the-art performance across a\ndiverse set of computer science conferences over the years to showcase its\nability to structure and capture the evolution of scientific fields. As a\nmultidimensional method, TaxoAdapt generates taxonomies that are 26.51% more\ngranularity-preserving and 50.41% more coherent than the most competitive\nbaselines judged by LLMs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/74HPyELcUejedIKkwVDfl.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/jhWsY6N7gO921TvBcwXFh.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/jtpBSKwYIzYRkceWALmXE.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10737.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6476ae4083d4fdaedddf405f",
            "avatarUrl": "/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg",
            "fullname": "Priyanka Kargupta",
            "name": "pkargupta",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10728",
            "authors": [
                {
                    "_id": "684c2a593b733ba333687243",
                    "user": {
                        "_id": "6476ae4083d4fdaedddf405f",
                        "avatarUrl": "/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg",
                        "isPro": false,
                        "fullname": "Priyanka Kargupta",
                        "user": "pkargupta",
                        "type": "user"
                    },
                    "name": "Priyanka Kargupta",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:16.568Z",
                    "hidden": false
                },
                {
                    "_id": "684c2a593b733ba333687244",
                    "name": "Runchu Tian",
                    "hidden": false
                },
                {
                    "_id": "684c2a593b733ba333687245",
                    "name": "Jiawei Han",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/12ONADwgtRd32k6YbuJPc.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/2mu5UktVGJyBCyEObDK41.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/BKFoQPJuqjVaQ4_1d_uYL.png"
            ],
            "publishedAt": "2025-06-12T14:17:45.000Z",
            "submittedOnDailyAt": "2025-06-13T12:18:58.104Z",
            "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of\n  Nuanced Claims",
            "submittedOnDailyBy": {
                "_id": "6476ae4083d4fdaedddf405f",
                "avatarUrl": "/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg",
                "isPro": false,
                "fullname": "Priyanka Kargupta",
                "user": "pkargupta",
                "type": "user"
            },
            "summary": "Claims made by individuals or entities are oftentimes nuanced and cannot be\nclearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with\nscientific and political claims. However, a claim (e.g., \"vaccine A is better\nthan vaccine B\") can be dissected into its integral aspects and sub-aspects\n(e.g., efficacy, safety, distribution), which are individually easier to\nvalidate. This enables a more comprehensive, structured response that provides\na well-rounded perspective on a given problem while also allowing the reader to\nprioritize specific angles of interest within the claim (e.g., safety towards\nchildren). Thus, we propose ClaimSpect, a retrieval-augmented generation-based\nframework for automatically constructing a hierarchy of aspects typically\nconsidered when addressing a claim and enriching them with corpus-specific\nperspectives. This structure hierarchically partitions an input corpus to\nretrieve relevant segments, which assist in discovering new sub-aspects.\nMoreover, these segments enable the discovery of varying perspectives towards\nan aspect of the claim (e.g., support, neutral, or oppose) and their respective\nprevalence (e.g., \"how many biomedical papers believe vaccine A is more\ntransportable than B?\"). We apply ClaimSpect to a wide variety of real-world\nscientific and political claims featured in our constructed dataset, showcasing\nits robustness and accuracy in deconstructing a nuanced claim and representing\nperspectives within a corpus. Through real-world case studies and human\nevaluation, we validate its effectiveness over multiple baselines.",
            "upvotes": 2,
            "discussionId": "684c2a593b733ba333687246",
            "githubRepo": "https://github.com/pkargupta/claimspect",
            "ai_summary": "ClaimSpect is a retrieval-augmented generation-based framework that constructs a hierarchical structure of aspects for claims, enriching them with diverse perspectives from a corpus.",
            "ai_keywords": [
                ""
            ]
        },
        "publishedAt": "2025-06-12T10:17:45.000Z",
        "title": "Beyond True or False: Retrieval-Augmented Hierarchical Analysis of\n  Nuanced Claims",
        "summary": "Claims made by individuals or entities are oftentimes nuanced and cannot be\nclearly labeled as entirely \"true\" or \"false\" -- as is frequently the case with\nscientific and political claims. However, a claim (e.g., \"vaccine A is better\nthan vaccine B\") can be dissected into its integral aspects and sub-aspects\n(e.g., efficacy, safety, distribution), which are individually easier to\nvalidate. This enables a more comprehensive, structured response that provides\na well-rounded perspective on a given problem while also allowing the reader to\nprioritize specific angles of interest within the claim (e.g., safety towards\nchildren). Thus, we propose ClaimSpect, a retrieval-augmented generation-based\nframework for automatically constructing a hierarchy of aspects typically\nconsidered when addressing a claim and enriching them with corpus-specific\nperspectives. This structure hierarchically partitions an input corpus to\nretrieve relevant segments, which assist in discovering new sub-aspects.\nMoreover, these segments enable the discovery of varying perspectives towards\nan aspect of the claim (e.g., support, neutral, or oppose) and their respective\nprevalence (e.g., \"how many biomedical papers believe vaccine A is more\ntransportable than B?\"). We apply ClaimSpect to a wide variety of real-world\nscientific and political claims featured in our constructed dataset, showcasing\nits robustness and accuracy in deconstructing a nuanced claim and representing\nperspectives within a corpus. Through real-world case studies and human\nevaluation, we validate its effectiveness over multiple baselines.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/12ONADwgtRd32k6YbuJPc.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/2mu5UktVGJyBCyEObDK41.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6476ae4083d4fdaedddf405f/BKFoQPJuqjVaQ4_1d_uYL.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10728.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6476ae4083d4fdaedddf405f",
            "avatarUrl": "/avatars/08b23ccfa1f3bede6ade5a1aef06931d.svg",
            "fullname": "Priyanka Kargupta",
            "name": "pkargupta",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10036",
            "authors": [
                {
                    "_id": "684bc8db3b733ba33368718c",
                    "name": "Javad Rajabi",
                    "hidden": false
                },
                {
                    "_id": "684bc8db3b733ba33368718d",
                    "name": "Soroush Mehraban",
                    "hidden": false
                },
                {
                    "_id": "684bc8db3b733ba33368718e",
                    "user": {
                        "_id": "63b4b02a103617b0a5b0ee2e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
                        "isPro": false,
                        "fullname": "Seyedmorteza Sadat",
                        "user": "msadat97",
                        "type": "user"
                    },
                    "name": "Seyedmorteza Sadat",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:38:32.584Z",
                    "hidden": false
                },
                {
                    "_id": "684bc8db3b733ba33368718f",
                    "name": "Babak Taati",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T21:25:46.000Z",
            "submittedOnDailyAt": "2025-06-13T05:21:38.595Z",
            "title": "Token Perturbation Guidance for Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "63b4b02a103617b0a5b0ee2e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
                "isPro": false,
                "fullname": "Seyedmorteza Sadat",
                "user": "msadat97",
                "type": "user"
            },
            "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2times improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
            "upvotes": 2,
            "discussionId": "684bc8db3b733ba333687190",
            "githubRepo": "https://github.com/TaatiTeam/Token-Perturbation-Guidance",
            "ai_summary": "Token Perturbation Guidance (TPG) enhances diffusion models with condition-agnostic, training-free guidance, similar to classifier-free guidance (CFG), without requiring architectural changes.",
            "ai_keywords": [
                "classifier-free guidance (CFG)",
                "Token Perturbation Guidance (TPG)",
                "perturbation matrices",
                "intermediate token representations",
                "norm-preserving shuffling",
                "FID",
                "prompt alignment",
                "SDXL",
                "Stable Diffusion 2.1"
            ]
        },
        "publishedAt": "2025-06-10T17:25:46.000Z",
        "title": "Token Perturbation Guidance for Diffusion Models",
        "summary": "Classifier-free guidance (CFG) has become an essential component of modern\ndiffusion models to enhance both generation quality and alignment with input\nconditions. However, CFG requires specific training procedures and is limited\nto conditional generation. To address these limitations, we propose Token\nPerturbation Guidance (TPG), a novel method that applies perturbation matrices\ndirectly to intermediate token representations within the diffusion network.\nTPG employs a norm-preserving shuffling operation to provide effective and\nstable guidance signals that improve generation quality without architectural\nchanges. As a result, TPG is training-free and agnostic to input conditions,\nmaking it readily applicable to both conditional and unconditional generation.\nWe further analyze the guidance term provided by TPG and show that its effect\non sampling more closely resembles CFG compared to existing training-free\nguidance techniques. Extensive experiments on SDXL and Stable Diffusion 2.1\nshow that TPG achieves nearly a 2times improvement in FID for unconditional\ngeneration over the SDXL baseline, while closely matching CFG in prompt\nalignment. These results establish TPG as a general, condition-agnostic\nguidance method that brings CFG-like benefits to a broader class of diffusion\nmodels. The code is available at\nhttps://github.com/TaatiTeam/Token-Perturbation-Guidance",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10036.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "fullname": "Seyedmorteza Sadat",
            "name": "msadat97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08373",
            "authors": [
                {
                    "_id": "684ae1f3dbd21a9cc27b0f32",
                    "name": "Kevin Galim",
                    "hidden": false
                },
                {
                    "_id": "684ae1f3dbd21a9cc27b0f33",
                    "name": "Ethan Ewer",
                    "hidden": false
                },
                {
                    "_id": "684ae1f3dbd21a9cc27b0f34",
                    "name": "Wonjun Kang",
                    "hidden": false
                },
                {
                    "_id": "684ae1f3dbd21a9cc27b0f35",
                    "name": "Minjae Lee",
                    "hidden": false
                },
                {
                    "_id": "684ae1f3dbd21a9cc27b0f36",
                    "name": "Hyung Il Koo",
                    "hidden": false
                },
                {
                    "_id": "684ae1f3dbd21a9cc27b0f37",
                    "name": "Kangwook Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T02:37:46.000Z",
            "submittedOnDailyAt": "2025-06-13T00:35:36.456Z",
            "title": "Draft-based Approximate Inference for LLMs",
            "submittedOnDailyBy": {
                "_id": "630c90123dc31beba6e8f406",
                "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
                "isPro": false,
                "fullname": "Kevin Galim",
                "user": "kev95",
                "type": "user"
            },
            "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
            "upvotes": 2,
            "discussionId": "684ae1f3dbd21a9cc27b0f38",
            "ai_summary": "A new framework using draft models enhances approximate inference for long-context LLMs by better predicting token and key-value pair importance, improving accuracy while maintaining memory and compute efficiency.",
            "ai_keywords": [
                "Large Language Models",
                "Transformers",
                "key-value cache dropping",
                "sparse attention",
                "prompt compression",
                "draft models",
                "SpecKV",
                "SpecPC",
                "attention activations",
                "long-context benchmarks"
            ]
        },
        "publishedAt": "2025-06-09T22:37:46.000Z",
        "title": "Draft-based Approximate Inference for LLMs",
        "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, which\nleverages a draft output to accurately assess the importance of each KV pair\nfor more effective KV cache dropping, and (ii) SpecPC, which uses the draft\nmodel's attention activations to identify and discard unimportant prompt\ntokens. To the best of our knowledge, this is the first work to use draft\nmodels for approximate LLM inference acceleration, extending their utility\nbeyond traditional lossless speculative decoding. We motivate our methods with\ntheoretical and empirical analyses, and show a strong correlation between the\nattention patterns of draft and target models. Extensive experiments on\nlong-context benchmarks show that our methods consistently achieve higher\naccuracy than existing baselines, while preserving the same improvements in\nmemory usage, latency, and throughput. Our code is available at\nhttps://github.com/furiosa-ai/draft-based-approx-llm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08373.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630c90123dc31beba6e8f406",
            "avatarUrl": "/avatars/2188b41fff122d4f5683b46c529ed79d.svg",
            "fullname": "Kevin Galim",
            "name": "kev95",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.06694",
            "authors": [
                {
                    "_id": "684ba6bc3b733ba333687093",
                    "name": "Yuan Yuan",
                    "hidden": false
                },
                {
                    "_id": "684ba6bc3b733ba333687094",
                    "name": "Yukun Liu",
                    "hidden": false
                },
                {
                    "_id": "684ba6bc3b733ba333687095",
                    "name": "Chonghua Han",
                    "hidden": false
                },
                {
                    "_id": "684ba6bc3b733ba333687096",
                    "user": {
                        "_id": "6465d3bd63e7e09dd02e95c3",
                        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                        "isPro": false,
                        "fullname": "Jie Feng",
                        "user": "JJ-TMT",
                        "type": "user"
                    },
                    "name": "Jie Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:13.023Z",
                    "hidden": false
                },
                {
                    "_id": "684ba6bc3b733ba333687097",
                    "name": "Yong Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
            ],
            "publishedAt": "2025-06-07T07:19:11.000Z",
            "submittedOnDailyAt": "2025-06-13T02:53:49.430Z",
            "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
            "submittedOnDailyBy": {
                "_id": "6465d3bd63e7e09dd02e95c3",
                "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                "isPro": false,
                "fullname": "Jie Feng",
                "user": "JJ-TMT",
                "type": "user"
            },
            "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.",
            "upvotes": 2,
            "discussionId": "684ba6bc3b733ba333687098",
            "githubRepo": "https://github.com/ScottLiu2003/MoveGCL",
            "ai_summary": "MoveGCL is a privacy-preserving framework using generative continual learning and a Mixture-of-Experts Transformer for training mobility foundation models without sharing raw data.",
            "ai_keywords": [
                "generative continual learning",
                "privacy-preserving",
                "Mixture-of-Experts Transformer",
                "mobility-aware expert routing mechanism",
                "layer-wise progressive adaptation",
                "catastrophic forgetting",
                "federated learning"
            ]
        },
        "publishedAt": "2025-06-07T03:19:11.000Z",
        "title": "Breaking Data Silos: Towards Open and Scalable Mobility Foundation\n  Models via Generative Continual Learning",
        "summary": "Foundation models have revolutionized fields such as natural language\nprocessing and computer vision by enabling general-purpose learning across\ndiverse tasks and datasets. However, building analogous models for human\nmobility remains challenging due to the privacy-sensitive nature of mobility\ndata and the resulting data silos across institutions. To bridge this gap, we\npropose MoveGCL, a scalable and privacy-preserving framework for training\nmobility foundation models via generative continual learning. Without sharing\nraw data, MoveGCL enables decentralized and progressive model evolution by\nreplaying synthetic trajectories generated from a frozen teacher model, and\nreinforces knowledge retention through a tailored distillation strategy that\nmitigates catastrophic forgetting. To address the heterogeneity of mobility\npatterns, MoveGCL incorporates a Mixture-of-Experts Transformer with a\nmobility-aware expert routing mechanism, and employs a layer-wise progressive\nadaptation strategy to stabilize continual updates. Experiments on six\nreal-world urban datasets demonstrate that MoveGCL achieves performance\ncomparable to joint training and significantly outperforms federated learning\nbaselines, while offering strong privacy protection. MoveGCL marks a crucial\nstep toward unlocking foundation models for mobility, offering a practical\nblueprint for open, scalable, and privacy-preserving model development in the\nera of foundation models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/LfNEmKBa7cYZhNNRnZRIx.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06694.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "fullname": "Jie Feng",
            "name": "JJ-TMT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10378",
            "authors": [
                {
                    "_id": "684bb08e3b733ba3336870bf",
                    "name": "Jikai Jin",
                    "hidden": false
                },
                {
                    "_id": "684bb08e3b733ba3336870c0",
                    "name": "Vasilis Syrgkanis",
                    "hidden": false
                },
                {
                    "_id": "684bb08e3b733ba3336870c1",
                    "name": "Sham Kakade",
                    "hidden": false
                },
                {
                    "_id": "684bb08e3b733ba3336870c2",
                    "name": "Hanlin Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T06:07:42.000Z",
            "submittedOnDailyAt": "2025-06-13T03:36:14.455Z",
            "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
            "submittedOnDailyBy": {
                "_id": "624054bcc2c17da6a63eb539",
                "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
                "isPro": false,
                "fullname": "hlzhang109",
                "user": "hlzhang109",
                "type": "user"
            },
            "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.",
            "upvotes": 1,
            "discussionId": "684bb08e3b733ba3336870c3",
            "ai_summary": "A causal representation learning framework identifies a concise causal structure to explain performance variations in language models across benchmarks by controlling for base model variations.",
            "ai_keywords": [
                "causal representation learning",
                "latent capability factors",
                "causal interrelated",
                "base model confounder",
                "Open LLM Leaderboard",
                "linear causal structure",
                "general problem-solving capabilities",
                "instruction-following proficiency",
                "mathematical reasoning ability"
            ]
        },
        "publishedAt": "2025-06-12T02:07:42.000Z",
        "title": "Discovering Hierarchical Latent Capabilities of Language Models via\n  Causal Representation Learning",
        "summary": "Faithful evaluation of language model capabilities is crucial for deriving\nactionable insights that can inform model development. However, rigorous causal\nevaluations in this domain face significant methodological challenges,\nincluding complex confounding effects and prohibitive computational costs\nassociated with extensive retraining. To tackle these challenges, we propose a\ncausal representation learning framework wherein observed benchmark performance\nis modeled as a linear transformation of a few latent capability factors.\nCrucially, these latent factors are identified as causally interrelated after\nappropriately controlling for the base model as a common confounder. Applying\nthis approach to a comprehensive dataset encompassing over 1500 models\nevaluated across six benchmarks from the Open LLM Leaderboard, we identify a\nconcise three-node linear causal structure that reliably explains the observed\nperformance variations. Further interpretation of this causal structure\nprovides substantial scientific insights beyond simple numerical rankings:\nspecifically, we reveal a clear causal direction starting from general\nproblem-solving capabilities, advancing through instruction-following\nproficiency, and culminating in mathematical reasoning ability. Our results\nunderscore the essential role of carefully controlling base model variations\nduring evaluation, a step critical to accurately uncovering the underlying\ncausal relationships among latent model capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10378.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "624054bcc2c17da6a63eb539",
            "avatarUrl": "/avatars/bf52dc0683b4100733f8696a97696d0e.svg",
            "fullname": "hlzhang109",
            "name": "hlzhang109",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.06561",
            "authors": [
                {
                    "_id": "684b88113b733ba333686fc7",
                    "name": "Ho Yin 'Sam' Ng",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fc8",
                    "name": "Ting-Yao Hsu",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fc9",
                    "name": "Aashish Anantha Ramakrishnan",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fca",
                    "name": "Branislav Kveton",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fcb",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fcc",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:39:49.692Z",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fcd",
                    "name": "Dongwon Lee",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fce",
                    "name": "Tong Yu",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fcf",
                    "name": "Sungchul Kim",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fd0",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "684b88113b733ba333686fd1",
                    "name": "Ting-Hao 'Kenneth' Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T22:16:16.000Z",
            "submittedOnDailyAt": "2025-06-13T00:38:33.715Z",
            "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.",
            "upvotes": 1,
            "discussionId": "684b88123b733ba333686fd2",
            "ai_summary": "LaMP-Cap introduces a dataset for personalized figure caption generation using multimodal profiles to improve the quality of AI-generated captions.",
            "ai_keywords": [
                "LaMP-Cap",
                "personalized figure caption generation",
                "multimodal figures",
                "figure profiles",
                "ablation studies"
            ]
        },
        "publishedAt": "2025-06-06T18:16:16.000Z",
        "title": "LaMP-Cap: Personalized Figure Caption Generation With Multimodal Figure\n  Profiles",
        "summary": "Figure captions are crucial for helping readers understand and remember a\nfigure's key message. Many models have been developed to generate these\ncaptions, helping authors compose better quality captions more easily. Yet,\nauthors almost always need to revise generic AI-generated captions to match\ntheir writing style and the domain's style, highlighting the need for\npersonalization. Despite language models' personalization (LaMP) advances,\nthese technologies often focus on text-only settings and rarely address\nscenarios where both inputs and profiles are multimodal. This paper introduces\nLaMP-Cap, a dataset for personalized figure caption generation with multimodal\nfigure profiles. For each target figure, LaMP-Cap provides not only the needed\ninputs, such as figure images, but also up to three other figures from the same\ndocument--each with its image, caption, and figure-mentioning paragraphs--as a\nprofile to characterize the context. Experiments with four LLMs show that using\nprofile information consistently helps generate captions closer to the original\nauthor-written ones. Ablation studies reveal that images in the profile are\nmore helpful than figure-mentioning paragraphs, highlighting the advantage of\nusing multimodal profiles over text-only ones.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06561.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.05982",
            "authors": [
                {
                    "_id": "684b86913b733ba333686fb8",
                    "name": "Zonglin Wu",
                    "hidden": false
                },
                {
                    "_id": "684b86913b733ba333686fb9",
                    "name": "Yule Xue",
                    "hidden": false
                },
                {
                    "_id": "684b86913b733ba333686fba",
                    "name": "Xin Wei",
                    "hidden": false
                },
                {
                    "_id": "684b86913b733ba333686fbb",
                    "name": "Yiren Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T11:02:01.000Z",
            "submittedOnDailyAt": "2025-06-13T00:33:34.648Z",
            "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
            "submittedOnDailyBy": {
                "_id": "64311a95034ecbefddd141ef",
                "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
                "isPro": true,
                "fullname": "Yiren Song",
                "user": "yiren98",
                "type": "user"
            },
            "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.",
            "upvotes": 1,
            "discussionId": "684b86923b733ba333686fbc",
            "ai_summary": "MCA-Bench provides a unified benchmark for evaluating CAPTCHA security using a shared vision-language model and attackers specialized for each type of CAPTCHA.",
            "ai_keywords": [
                "vision-language model",
                "CAPTCHA",
                "benchmark",
                "evaluation protocol",
                "cracking agents",
                "vulnerability spectrum",
                "challenge complexity",
                "interaction depth",
                "model solvability"
            ]
        },
        "publishedAt": "2025-06-06T07:02:01.000Z",
        "title": "MCA-Bench: A Multimodal Benchmark for Evaluating CAPTCHA Robustness\n  Against VLM-based Attacks",
        "summary": "As automated attack techniques rapidly advance, CAPTCHAs remain a critical\ndefense mechanism against malicious bots. However, existing CAPTCHA schemes\nencompass a diverse range of modalities -- from static distorted text and\nobfuscated images to interactive clicks, sliding puzzles, and logic-based\nquestions -- yet the community still lacks a unified, large-scale, multimodal\nbenchmark to rigorously evaluate their security robustness. To address this\ngap, we introduce MCA-Bench, a comprehensive and reproducible benchmarking\nsuite that integrates heterogeneous CAPTCHA types into a single evaluation\nprotocol. Leveraging a shared vision-language model backbone, we fine-tune\nspecialized cracking agents for each CAPTCHA category, enabling consistent,\ncross-modal assessments. Extensive experiments reveal that MCA-Bench\neffectively maps the vulnerability spectrum of modern CAPTCHA designs under\nvaried attack settings, and crucially offers the first quantitative analysis of\nhow challenge complexity, interaction depth, and model solvability interrelate.\nBased on these findings, we propose three actionable design principles and\nidentify key open challenges, laying the groundwork for systematic CAPTCHA\nhardening, fair benchmarking, and broader community collaboration. Datasets and\ncode are available online.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05982.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64311a95034ecbefddd141ef",
            "avatarUrl": "/avatars/b6dc5ca373bedbaa368208517954c375.svg",
            "fullname": "Yiren Song",
            "name": "yiren98",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10600",
            "authors": [
                {
                    "_id": "684b9aa43b733ba33368704c",
                    "user": {
                        "_id": "659b9ee689010f9c7ad674ed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659b9ee689010f9c7ad674ed/4vFsBx0FEzri5W9WUzI6p.png",
                        "isPro": true,
                        "fullname": "xinjjj",
                        "user": "xinjjj",
                        "type": "user"
                    },
                    "name": "Wang Xinjie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:28.296Z",
                    "hidden": false
                },
                {
                    "_id": "684b9aa43b733ba33368704d",
                    "name": "Liu Liu",
                    "hidden": false
                },
                {
                    "_id": "684b9aa43b733ba33368704e",
                    "name": "Cao Yu",
                    "hidden": false
                },
                {
                    "_id": "684b9aa43b733ba33368704f",
                    "name": "Wu Ruiqi",
                    "hidden": false
                },
                {
                    "_id": "684b9aa43b733ba333687050",
                    "name": "Qin Wenkang",
                    "hidden": false
                },
                {
                    "_id": "684b9aa43b733ba333687051",
                    "name": "Wang Dehui",
                    "hidden": false
                },
                {
                    "_id": "684b9aa43b733ba333687052",
                    "name": "Sui Wei",
                    "hidden": false
                },
                {
                    "_id": "684b9aa43b733ba333687053",
                    "name": "Su Zhizhong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T11:43:50.000Z",
            "submittedOnDailyAt": "2025-06-13T13:50:01.243Z",
            "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied\n  Intelligence",
            "submittedOnDailyBy": {
                "_id": "659b9ee689010f9c7ad674ed",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659b9ee689010f9c7ad674ed/4vFsBx0FEzri5W9WUzI6p.png",
                "isPro": true,
                "fullname": "xinjjj",
                "user": "xinjjj",
                "type": "user"
            },
            "summary": "Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.",
            "upvotes": 0,
            "discussionId": "684b9aa53b733ba333687054",
            "projectPage": "https://horizonrobotics.github.io/robot_lab/embodied_gen/index.html",
            "githubRepo": "https://github.com/HorizonRobotics/EmbodiedGen",
            "ai_summary": "EmbodiedGen is a platform that generates high-quality, photorealistic 3D assets at low cost, enabling scalable and realistic embodied AI research through generative AI techniques.",
            "ai_keywords": [
                "3D assets",
                "photo-realistic",
                "high-quality",
                "generative AI",
                "Image-to-3D",
                "Text-to-3D",
                "Texture Generation",
                "Articulated Object Generation",
                "Scene Generation",
                "Layout Generation",
                "physics simulation engines",
                "Unified Robotics Description Format (URDF)",
                "embodied intelligence"
            ]
        },
        "publishedAt": "2025-06-12T07:43:50.000Z",
        "title": "EmbodiedGen: Towards a Generative 3D World Engine for Embodied\n  Intelligence",
        "summary": "Constructing a physically realistic and accurately scaled simulated 3D world\nis crucial for the training and evaluation of embodied intelligence tasks. The\ndiversity, realism, low cost accessibility and affordability of 3D data assets\nare critical for achieving generalization and scalability in embodied AI.\nHowever, most current embodied intelligence tasks still rely heavily on\ntraditional 3D computer graphics assets manually created and annotated, which\nsuffer from high production costs and limited realism. These limitations\nsignificantly hinder the scalability of data driven approaches. We present\nEmbodiedGen, a foundational platform for interactive 3D world generation. It\nenables the scalable generation of high-quality, controllable and\nphotorealistic 3D assets with accurate physical properties and real-world scale\nin the Unified Robotics Description Format (URDF) at low cost. These assets can\nbe directly imported into various physics simulation engines for fine-grained\nphysical control, supporting downstream tasks in training and evaluation.\nEmbodiedGen is an easy-to-use, full-featured toolkit composed of six key\nmodules: Image-to-3D, Text-to-3D, Texture Generation, Articulated Object\nGeneration, Scene Generation and Layout Generation. EmbodiedGen generates\ndiverse and interactive 3D worlds composed of generative 3D assets, leveraging\ngenerative AI to address the challenges of generalization and evaluation to the\nneeds of embodied intelligence related research. Code is available at\nhttps://horizonrobotics.github.io/robot_lab/embodied_gen/index.html.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10600.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "659b9ee689010f9c7ad674ed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659b9ee689010f9c7ad674ed/4vFsBx0FEzri5W9WUzI6p.png",
            "fullname": "xinjjj",
            "name": "xinjjj",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08862",
            "authors": [
                {
                    "_id": "684ae26ddbd21a9cc27b117f",
                    "user": {
                        "_id": "648058ff8c6a3b8f11f77893",
                        "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
                        "isPro": false,
                        "fullname": "Wu Zike",
                        "user": "Nickwzk",
                        "type": "user"
                    },
                    "name": "Zike Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T14:59:30.377Z",
                    "hidden": false
                },
                {
                    "_id": "684ae26ddbd21a9cc27b1180",
                    "name": "Qi Yan",
                    "hidden": false
                },
                {
                    "_id": "684ae26ddbd21a9cc27b1181",
                    "name": "Xuanyu Yi",
                    "hidden": false
                },
                {
                    "_id": "684ae26ddbd21a9cc27b1182",
                    "name": "Lele Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae26ddbd21a9cc27b1183",
                    "name": "Renjie Liao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T14:52:36.000Z",
            "submittedOnDailyAt": "2025-06-13T07:33:07.278Z",
            "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
            "submittedOnDailyBy": {
                "_id": "648058ff8c6a3b8f11f77893",
                "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
                "isPro": false,
                "fullname": "Wu Zike",
                "user": "Nickwzk",
                "type": "user"
            },
            "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat.",
            "upvotes": 0,
            "discussionId": "684ae26ddbd21a9cc27b1184",
            "githubRepo": "https://github.com/nickwzk/StreamSplat",
            "ai_summary": "StreamSplat, a fully feed-forward framework, addresses real-time 3D scene reconstruction from uncalibrated video with accurate dynamics and long-term stability.",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "3DGS",
                "probabilistic sampling mechanism",
                "bidirectional deformation field",
                "online reconstruction"
            ]
        },
        "publishedAt": "2025-06-10T10:52:36.000Z",
        "title": "StreamSplat: Towards Online Dynamic 3D Reconstruction from Uncalibrated\n  Video Streams",
        "summary": "Real-time reconstruction of dynamic 3D scenes from uncalibrated video streams\nis crucial for numerous real-world applications. However, existing methods\nstruggle to jointly address three key challenges: 1) processing uncalibrated\ninputs in real time, 2) accurately modeling dynamic scene evolution, and 3)\nmaintaining long-term stability and computational efficiency. To this end, we\nintroduce StreamSplat, the first fully feed-forward framework that transforms\nuncalibrated video streams of arbitrary length into dynamic 3D Gaussian\nSplatting (3DGS) representations in an online manner, capable of recovering\nscene dynamics from temporally local observations. We propose two key technical\ninnovations: a probabilistic sampling mechanism in the static encoder for 3DGS\nposition prediction, and a bidirectional deformation field in the dynamic\ndecoder that enables robust and efficient dynamic modeling. Extensive\nexperiments on static and dynamic benchmarks demonstrate that StreamSplat\nconsistently outperforms prior works in both reconstruction quality and dynamic\nscene modeling, while uniquely supporting online reconstruction of arbitrarily\nlong video streams. Code and models are available at\nhttps://github.com/nickwzk/StreamSplat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08862.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648058ff8c6a3b8f11f77893",
            "avatarUrl": "/avatars/043c832314a8d6713af90d7c255fc2f2.svg",
            "fullname": "Wu Zike",
            "name": "Nickwzk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    }
]