[
    {
        "paper": {
            "id": "2506.09600",
            "authors": [
                {
                    "_id": "684fca8160b4a34dbe007b4f",
                    "name": "Itay Nakash",
                    "hidden": false
                },
                {
                    "_id": "684fca8160b4a34dbe007b50",
                    "name": "George Kour",
                    "hidden": false
                },
                {
                    "_id": "684fca8160b4a34dbe007b51",
                    "name": "Koren Lazar",
                    "hidden": false
                },
                {
                    "_id": "684fca8160b4a34dbe007b52",
                    "user": {
                        "_id": "6465fae8e9906a259f328032",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6465fae8e9906a259f328032/mOf1PEA_GhCY5N7v_vfb6.jpeg",
                        "isPro": false,
                        "fullname": "Matan Vetzler",
                        "user": "matanvetzler",
                        "type": "user"
                    },
                    "name": "Matan Vetzler",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T12:56:52.767Z",
                    "hidden": false
                },
                {
                    "_id": "684fca8160b4a34dbe007b53",
                    "name": "Guy Uziel",
                    "hidden": false
                },
                {
                    "_id": "684fca8160b4a34dbe007b54",
                    "name": "Ateret Anaby-Tavor",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/99oCW2IrMaCeLyuyfgvbG.png"
            ],
            "publishedAt": "2025-06-11T10:59:47.000Z",
            "submittedOnDailyAt": "2025-06-16T06:16:02.507Z",
            "title": "Effective Red-Teaming of Policy-Adherent Agents",
            "submittedOnDailyBy": {
                "_id": "671f8106d677d3a764a6f9a5",
                "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
                "isPro": false,
                "fullname": "itay nakash",
                "user": "itaynakash",
                "type": "user"
            },
            "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks",
            "upvotes": 33,
            "discussionId": "684fca8160b4a34dbe007b55",
            "ai_summary": "CRAFT, a multi-agent system using policy-aware persuasive strategies, challenges policy-adherent LLM-based agents in customer service to assess and improve their robustness against adversarial attacks.",
            "ai_keywords": [
                "LLM-based agents",
                "policy-adherence",
                "adversarial users",
                "CRAFT",
                "multi-agent red-teaming",
                "policy-aware persuasive strategies",
                "DAN prompts",
                "emotional manipulation",
                "coercive",
                "tau-break",
                "defense strategies",
                "adversarial attacks"
            ]
        },
        "publishedAt": "2025-06-11T06:59:47.000Z",
        "title": "Effective Red-Teaming of Policy-Adherent Agents",
        "summary": "Task-oriented LLM-based agents are increasingly used in domains with strict\npolicies, such as refund eligibility or cancellation rules. The challenge lies\nin ensuring that the agent consistently adheres to these rules and policies,\nappropriately refusing any request that would violate them, while still\nmaintaining a helpful and natural interaction. This calls for the development\nof tailored design and evaluation methodologies to ensure agent resilience\nagainst malicious user behavior. We propose a novel threat model that focuses\non adversarial users aiming to exploit policy-adherent agents for personal\nbenefit. To address this, we present CRAFT, a multi-agent red-teaming system\nthat leverages policy-aware persuasive strategies to undermine a\npolicy-adherent agent in a customer-service scenario, outperforming\nconventional jailbreak methods such as DAN prompts, emotional manipulation, and\ncoercive. Building upon the existing tau-bench benchmark, we introduce\ntau-break, a complementary benchmark designed to rigorously assess the agent's\nrobustness against manipulative user behavior. Finally, we evaluate several\nstraightforward yet effective defense strategies. While these measures provide\nsome protection, they fall short, highlighting the need for stronger,\nresearch-driven safeguards to protect policy-adherent agents from adversarial\nattacks",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/671f8106d677d3a764a6f9a5/99oCW2IrMaCeLyuyfgvbG.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09600.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "671f8106d677d3a764a6f9a5",
            "avatarUrl": "/avatars/90b4b00058aac30c060c5eac8debb1c7.svg",
            "fullname": "itay nakash",
            "name": "itaynakash",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.11924",
            "authors": [
                {
                    "_id": "684faeba60b4a34dbe007ae2",
                    "name": "Min-Seop Kwak",
                    "hidden": false
                },
                {
                    "_id": "684faeba60b4a34dbe007ae3",
                    "name": "Junho Kim",
                    "hidden": false
                },
                {
                    "_id": "684faeba60b4a34dbe007ae4",
                    "name": "Sangdoo Yun",
                    "hidden": false
                },
                {
                    "_id": "684faeba60b4a34dbe007ae5",
                    "name": "Dongyoon Han",
                    "hidden": false
                },
                {
                    "_id": "684faeba60b4a34dbe007ae6",
                    "name": "Taekyoung Kim",
                    "hidden": false
                },
                {
                    "_id": "684faeba60b4a34dbe007ae7",
                    "name": "Seungryong Kim",
                    "hidden": false
                },
                {
                    "_id": "684faeba60b4a34dbe007ae8",
                    "name": "Jin-Hwa Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T16:19:00.000Z",
            "submittedOnDailyAt": "2025-06-16T04:13:42.201Z",
            "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation",
            "submittedOnDailyBy": {
                "_id": "642673f185f26ab94af4b422",
                "avatarUrl": "/avatars/289d611e0907f02f72d4e489468e039c.svg",
                "isPro": false,
                "fullname": "Bracio",
                "user": "bracio9623",
                "type": "user"
            },
            "summary": "We introduce a diffusion-based framework that performs aligned novel view\nimage and geometry generation via a warping-and-inpainting methodology. Unlike\nprior methods that require dense posed images or pose-embedded generative\nmodels limited to in-domain views, our method leverages off-the-shelf geometry\npredictors to predict partial geometries viewed from reference images, and\nformulates novel-view synthesis as an inpainting task for both image and\ngeometry. To ensure accurate alignment between generated images and geometry,\nwe propose cross-modal attention distillation, where attention maps from the\nimage diffusion branch are injected into a parallel geometry diffusion branch\nduring both training and inference. This multi-task approach achieves\nsynergistic effects, facilitating geometrically robust image synthesis as well\nas well-defined geometry prediction. We further introduce proximity-based mesh\nconditioning to integrate depth and normal cues, interpolating between point\ncloud and filtering erroneously predicted geometry from influencing the\ngeneration process. Empirically, our method achieves high-fidelity\nextrapolative view synthesis on both image and geometry across a range of\nunseen scenes, delivers competitive reconstruction quality under interpolation\nsettings, and produces geometrically aligned colored point clouds for\ncomprehensive 3D completion. Project page is available at\nhttps://cvlab-kaist.github.io/MoAI.",
            "upvotes": 28,
            "discussionId": "684faebb60b4a34dbe007ae9",
            "projectPage": "https://cvlab-kaist.github.io/MoAI/",
            "githubRepo": "https://github.com/cvlab-kaist/MoAI",
            "ai_summary": "A diffusion-based framework generates aligned novel views of images and geometry using warping-and-inpainting with cross-modal attention distillation and proximity-based mesh conditioning, achieving high-fidelity synthesis and 3D completion.",
            "ai_keywords": [
                "diffusion-based framework",
                "warping-and-inpainting",
                "off-the-shelf geometry predictors",
                "cross-modal attention distillation",
                "proximity-based mesh conditioning",
                "novel-view synthesis",
                "multi-task approach",
                "geometrically robust image synthesis",
                "well-defined geometry prediction",
                "extrapolative view synthesis",
                "3D completion"
            ]
        },
        "publishedAt": "2025-06-13T12:19:00.000Z",
        "title": "Aligned Novel View Image and Geometry Synthesis via Cross-modal\n  Attention Instillation",
        "summary": "We introduce a diffusion-based framework that performs aligned novel view\nimage and geometry generation via a warping-and-inpainting methodology. Unlike\nprior methods that require dense posed images or pose-embedded generative\nmodels limited to in-domain views, our method leverages off-the-shelf geometry\npredictors to predict partial geometries viewed from reference images, and\nformulates novel-view synthesis as an inpainting task for both image and\ngeometry. To ensure accurate alignment between generated images and geometry,\nwe propose cross-modal attention distillation, where attention maps from the\nimage diffusion branch are injected into a parallel geometry diffusion branch\nduring both training and inference. This multi-task approach achieves\nsynergistic effects, facilitating geometrically robust image synthesis as well\nas well-defined geometry prediction. We further introduce proximity-based mesh\nconditioning to integrate depth and normal cues, interpolating between point\ncloud and filtering erroneously predicted geometry from influencing the\ngeneration process. Empirically, our method achieves high-fidelity\nextrapolative view synthesis on both image and geometry across a range of\nunseen scenes, delivers competitive reconstruction quality under interpolation\nsettings, and produces geometrically aligned colored point clouds for\ncomprehensive 3D completion. Project page is available at\nhttps://cvlab-kaist.github.io/MoAI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11924.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642673f185f26ab94af4b422",
            "avatarUrl": "/avatars/289d611e0907f02f72d4e489468e039c.svg",
            "fullname": "Bracio",
            "name": "bracio9623",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10892",
            "authors": [
                {
                    "_id": "684fb2f060b4a34dbe007aeb",
                    "name": "Subham Sekhar Sahoo",
                    "hidden": false
                },
                {
                    "_id": "684fb2f060b4a34dbe007aec",
                    "name": "Justin Deschenaux",
                    "hidden": false
                },
                {
                    "_id": "684fb2f060b4a34dbe007aed",
                    "name": "Aaron Gokaslan",
                    "hidden": false
                },
                {
                    "_id": "684fb2f060b4a34dbe007aee",
                    "name": "Guanghan Wang",
                    "hidden": false
                },
                {
                    "_id": "684fb2f060b4a34dbe007aef",
                    "name": "Justin Chiu",
                    "hidden": false
                },
                {
                    "_id": "684fb2f060b4a34dbe007af0",
                    "name": "Volodymyr Kuleshov",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/GmIlLMVIuyWjydykQPOt2.png",
                "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/TjIhoD3hxygzenitTi75x.qt"
            ],
            "publishedAt": "2025-06-12T16:55:35.000Z",
            "submittedOnDailyAt": "2025-06-16T04:40:29.065Z",
            "title": "The Diffusion Duality",
            "submittedOnDailyBy": {
                "_id": "661839d73b412cdc851299c1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661839d73b412cdc851299c1/xicwANPQPTFdWfblisL2-.png",
                "isPro": false,
                "fullname": "Subham Sekhar Sahoo",
                "user": "s-sahoo",
                "type": "user"
            },
            "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo",
            "upvotes": 28,
            "discussionId": "684fb2f060b4a34dbe007af1",
            "projectPage": "https://s-sahoo.com/duo/",
            "githubRepo": "https://github.com/s-sahoo/duo",
            "ai_summary": "Duo improves uniform-state discrete diffusion models by transferring techniques from Gaussian diffusion, enhancing training speed and enabling fast few-step text generation.",
            "ai_keywords": [
                "discrete diffusion models",
                "Gaussian diffusion",
                "curriculum learning",
                "Discrete Consistency Distillation",
                "zero-shot perplexity",
                "few-step generation"
            ]
        },
        "publishedAt": "2025-06-12T12:55:35.000Z",
        "title": "The Diffusion Duality",
        "summary": "Uniform-state discrete diffusion models hold the promise of fast text\ngeneration due to their inherent ability to self-correct. However, they are\ntypically outperformed by autoregressive models and masked diffusion models. In\nthis work, we narrow this performance gap by leveraging a key insight:\nUniform-state diffusion processes naturally emerge from an underlying Gaussian\ndiffusion. Our method, Duo, transfers powerful techniques from Gaussian\ndiffusion to improve both training and sampling. First, we introduce a\ncurriculum learning strategy guided by the Gaussian process, doubling training\nspeed by reducing variance. Models trained with curriculum learning surpass\nautoregressive models in zero-shot perplexity on 3 of 7 benchmarks. Second, we\npresent Discrete Consistency Distillation, which adapts consistency\ndistillation from the continuous to the discrete setting. This algorithm\nunlocks few-step generation in diffusion language models by accelerating\nsampling by two orders of magnitude. We provide the code and model checkpoints\non the project page: http://s-sahoo.github.io/duo",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/GmIlLMVIuyWjydykQPOt2.png",
            "https://cdn-uploads.huggingface.co/production/uploads/661839d73b412cdc851299c1/TjIhoD3hxygzenitTi75x.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10892.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661839d73b412cdc851299c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661839d73b412cdc851299c1/xicwANPQPTFdWfblisL2-.png",
            "fullname": "Subham Sekhar Sahoo",
            "name": "s-sahoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.11930",
            "authors": [
                {
                    "_id": "684fbab760b4a34dbe007b14",
                    "name": "Dongwei Jiang",
                    "hidden": false
                },
                {
                    "_id": "684fbab760b4a34dbe007b15",
                    "user": {
                        "_id": "65ed3f714515999256c1f9c9",
                        "avatarUrl": "/avatars/615b2cd9f227226cb9d1219706556567.svg",
                        "isPro": false,
                        "fullname": "Alvin Zhang",
                        "user": "AlvinBZ04",
                        "type": "user"
                    },
                    "name": "Alvin Zhang",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-16T14:28:56.348Z",
                    "hidden": false
                },
                {
                    "_id": "684fbab760b4a34dbe007b16",
                    "name": "Andrew Wang",
                    "hidden": false
                },
                {
                    "_id": "684fbab760b4a34dbe007b17",
                    "name": "Nicholas Andrews",
                    "hidden": false
                },
                {
                    "_id": "684fbab760b4a34dbe007b18",
                    "name": "Daniel Khashabi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T16:31:51.000Z",
            "submittedOnDailyAt": "2025-06-16T13:34:44.550Z",
            "title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback",
            "submittedOnDailyBy": {
                "_id": "63eba2774dcaaf087638e3d6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676386919413-noauth.jpeg",
                "isPro": false,
                "fullname": "Jiang",
                "user": "Dongwei",
                "type": "user"
            },
            "summary": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement.",
            "upvotes": 24,
            "discussionId": "684fbab760b4a34dbe007b19",
            "ai_summary": "LLMs show resistance to feedback, termed feedback friction, even under ideal conditions, and sampling-based strategies only partially mitigate this issue.",
            "ai_keywords": [
                "LLMs",
                "feedback generation",
                "solver models",
                "math reasoning",
                "knowledge reasoning",
                "scientific reasoning",
                "state-of-the-art language models",
                "Claude 3.7",
                "extended thinking",
                "feedback friction",
                "progressive temperature increases",
                "explicit rejection"
            ]
        },
        "publishedAt": "2025-06-13T12:31:51.000Z",
        "title": "Feedback Friction: LLMs Struggle to Fully Incorporate External Feedback",
        "summary": "Recent studies have shown LLMs possess some ability to improve their\nresponses when given external feedback. However, it remains unclear how\neffectively and thoroughly these models can incorporate extrinsic feedback. In\nan ideal scenario, if LLMs receive near-perfect and complete feedback, we would\nexpect them to fully integrate the feedback and change their incorrect answers\nto correct ones. In this paper, we systematically investigate LLMs' ability to\nincorporate feedback by designing a controlled experimental environment. For\neach problem, a solver model attempts a solution, then a feedback generator\nwith access to near-complete ground-truth answers produces targeted feedback,\nafter which the solver tries again. We evaluate this pipeline across a diverse\nrange of tasks, including math reasoning, knowledge reasoning, scientific\nreasoning, and general multi-domain evaluations with state-of-the-art language\nmodels including Claude 3.7 (with and without extended thinking). Surprisingly,\neven under these near-ideal conditions, solver models consistently show\nresistance to feedback, a limitation that we term FEEDBACK FRICTION. To\nmitigate this limitation, we experiment with sampling-based strategies like\nprogressive temperature increases and explicit rejection of previously\nattempted incorrect answers, which yield improvements but still fail to help\nmodels achieve target performance. We also perform a rigorous exploration of\npotential causes of FEEDBACK FRICTION, ruling out factors such as model\noverconfidence and data familiarity. We hope that highlighting this issue in\nLLMs and ruling out several apparent causes will help future research in\nself-improvement.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11930.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63eba2774dcaaf087638e3d6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676386919413-noauth.jpeg",
            "fullname": "Jiang",
            "name": "Dongwei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10128",
            "authors": [
                {
                    "_id": "684fed081d9b438aa3957a50",
                    "user": {
                        "_id": "655fed9fdef5905d38b84af3",
                        "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
                        "isPro": false,
                        "fullname": "Xiyao Wang",
                        "user": "russwang",
                        "type": "user"
                    },
                    "name": "Xiyao Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:57:28.955Z",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a51",
                    "user": {
                        "_id": "630713411801ecc7d2592a7c",
                        "avatarUrl": "/avatars/fb36f69f03421c3a2a7f72ba0858fa60.svg",
                        "isPro": false,
                        "fullname": "Zhengyuan Yang",
                        "user": "zyang39",
                        "type": "user"
                    },
                    "name": "Zhengyuan Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:57:36.188Z",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a52",
                    "name": "Chao Feng",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a53",
                    "user": {
                        "_id": "6646d5819bb34d2b6b7455d3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JFH3ZTPvlaVSg4RJJBb6L.jpeg",
                        "isPro": false,
                        "fullname": "Yongyuan Liang",
                        "user": "cheryyunl",
                        "type": "user"
                    },
                    "name": "Yongyuan Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:58:44.352Z",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a54",
                    "name": "Yuhang Zhou",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a55",
                    "name": "Xiaoyu Liu",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a56",
                    "user": {
                        "_id": "67c336977ab1d6e701795009",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/r7Sm5Pk6ZdONW5nDyfdgF.png",
                        "isPro": false,
                        "fullname": "Ziyi Zang",
                        "user": "ZYii",
                        "type": "user"
                    },
                    "name": "Ziyi Zang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:58:34.481Z",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a57",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a58",
                    "name": "Chung-Ching Lin",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a59",
                    "name": "Kevin Lin",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a5a",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a5b",
                    "user": {
                        "_id": "64cbc3e2a257a3212c00a115",
                        "avatarUrl": "/avatars/836e61be4aeda2080ddf2db9f2626cc6.svg",
                        "isPro": false,
                        "fullname": "Furong Huang Lab at UMD",
                        "user": "furongh-lab",
                        "type": "user"
                    },
                    "name": "Furong Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:58:00.984Z",
                    "hidden": false
                },
                {
                    "_id": "684fed081d9b438aa3957a5c",
                    "user": {
                        "_id": "6413521d4e5305c14f22e110",
                        "avatarUrl": "/avatars/a6f8d0573e678f79bc3c0b7897b818ce.svg",
                        "isPro": false,
                        "fullname": "Lijuan Wang",
                        "user": "Lijuan",
                        "type": "user"
                    },
                    "name": "Lijuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:57:54.886Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T19:16:54.000Z",
            "submittedOnDailyAt": "2025-06-16T08:39:17.071Z",
            "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual\n  Perception in VLMs",
            "submittedOnDailyBy": {
                "_id": "655fed9fdef5905d38b84af3",
                "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
                "isPro": false,
                "fullname": "Xiyao Wang",
                "user": "russwang",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning\nlarge language models (LLMs) using tasks that are challenging yet easily\nverifiable, such as math reasoning or code generation. However, extending this\nsuccess to visual perception in vision-language models (VLMs) has been impeded\nby the scarcity of vision-centric tasks that are simultaneously challenging and\nunambiguously verifiable. To this end, we introduce ViCrit (Visual Caption\nHallucination Critic), an RL proxy task that trains VLMs to localize a subtle,\nsynthetic visual hallucination injected into paragraphs of human-written image\ncaptions. Starting from a 200-word captions, we inject a single, subtle visual\ndescription error-altering a few words on objects, attributes, counts, or\nspatial relations-and task the model to pinpoint the corrupted span given the\nimage and the modified caption. This formulation preserves the full perceptual\ndifficulty while providing a binary, exact-match reward that is easy to compute\nand unambiguous. Models trained with the ViCrit Task exhibit substantial gains\nacross a variety of VL benchmarks. Crucially, the improvements transfer beyond\nnatural-image training data to abstract image reasoning and visual math,\nshowing promises of learning to perceive rather than barely memorizing seen\nobjects. To facilitate evaluation, we further introduce ViCrit-Bench, a\ncategory-balanced diagnostic benchmark that systematically probes perception\nerrors across diverse image domains and error types. Together, our results\ndemonstrate that fine-grained hallucination criticism is an effective and\ngeneralizable objective for enhancing visual perception in VLMs.",
            "upvotes": 15,
            "discussionId": "684fed081d9b438aa3957a5d",
            "githubRepo": "https://github.com/si0wang/ViCrit",
            "ai_summary": "ViCrit, an RL task for fine-tuning VLMs, improves visual perception by training models to detect subtle hallucinations in image captions, with gains transferable to various visual domains.",
            "ai_keywords": [
                "reinforcement learning (RL)",
                "large language models (LLMs)",
                "vision-language models (VLMs)",
                "visual caption hallucination critic",
                "perceptual difficulty",
                "binary reward",
                "exact-match reward",
                "ViCrit-Bench",
                "abstract image reasoning",
                "visual math"
            ]
        },
        "publishedAt": "2025-06-11T15:16:54.000Z",
        "title": "ViCrit: A Verifiable Reinforcement Learning Proxy Task for Visual\n  Perception in VLMs",
        "summary": "Reinforcement learning (RL) has shown great effectiveness for fine-tuning\nlarge language models (LLMs) using tasks that are challenging yet easily\nverifiable, such as math reasoning or code generation. However, extending this\nsuccess to visual perception in vision-language models (VLMs) has been impeded\nby the scarcity of vision-centric tasks that are simultaneously challenging and\nunambiguously verifiable. To this end, we introduce ViCrit (Visual Caption\nHallucination Critic), an RL proxy task that trains VLMs to localize a subtle,\nsynthetic visual hallucination injected into paragraphs of human-written image\ncaptions. Starting from a 200-word captions, we inject a single, subtle visual\ndescription error-altering a few words on objects, attributes, counts, or\nspatial relations-and task the model to pinpoint the corrupted span given the\nimage and the modified caption. This formulation preserves the full perceptual\ndifficulty while providing a binary, exact-match reward that is easy to compute\nand unambiguous. Models trained with the ViCrit Task exhibit substantial gains\nacross a variety of VL benchmarks. Crucially, the improvements transfer beyond\nnatural-image training data to abstract image reasoning and visual math,\nshowing promises of learning to perceive rather than barely memorizing seen\nobjects. To facilitate evaluation, we further introduce ViCrit-Bench, a\ncategory-balanced diagnostic benchmark that systematically probes perception\nerrors across diverse image domains and error types. Together, our results\ndemonstrate that fine-grained hallucination criticism is an effective and\ngeneralizable objective for enhancing visual perception in VLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10128.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655fed9fdef5905d38b84af3",
            "avatarUrl": "/avatars/2cda4182dfd11a1e94743639e62328ea.svg",
            "fullname": "Xiyao Wang",
            "name": "russwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.11928",
            "authors": [
                {
                    "_id": "684fae8d60b4a34dbe007acd",
                    "name": "Zihan Zheng",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ace",
                    "user": {
                        "_id": "67d8e5f041fc44bb16e5468c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jUCe0rf7Z-cF51OC5Iczn.png",
                        "isPro": false,
                        "fullname": "Zerui Cheng",
                        "user": "ZeruiCheng",
                        "type": "user"
                    },
                    "name": "Zerui Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T15:50:50.311Z",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007acf",
                    "user": {
                        "_id": "672857ff73515943ca416973",
                        "avatarUrl": "/avatars/5f630a6b867c46fdd0fc1996186fef4d.svg",
                        "isPro": true,
                        "fullname": "Zeyu Shen",
                        "user": "zeyush",
                        "type": "user"
                    },
                    "name": "Zeyu Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T15:50:52.696Z",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad0",
                    "name": "Shang Zhou",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad1",
                    "name": "Kaiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad2",
                    "name": "Hansen He",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad3",
                    "name": "Dongruixuan Li",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad4",
                    "name": "Stanley Wei",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad5",
                    "name": "Hangyi Hao",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad6",
                    "name": "Jianzhu Yao",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad7",
                    "user": {
                        "_id": "681e8c219afd4ecd3e2c19f8",
                        "avatarUrl": "/avatars/2ddd4e0b5de7ffa060141243cbc2410a.svg",
                        "isPro": false,
                        "fullname": "Peiyao Sheng",
                        "user": "peiyao-sentient",
                        "type": "user"
                    },
                    "name": "Peiyao Sheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:00:36.911Z",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad8",
                    "name": "Zixuan Wang",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ad9",
                    "user": {
                        "_id": "637c7503fe115289cfecbe6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
                        "isPro": false,
                        "fullname": "Wenhao Chai",
                        "user": "wchai",
                        "type": "user"
                    },
                    "name": "Wenhao Chai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T15:50:54.781Z",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ada",
                    "user": {
                        "_id": "66aff5f4f0bb52918956631a",
                        "avatarUrl": "/avatars/12dfc9b534e2dc5d0e98c3723af2ca97.svg",
                        "isPro": false,
                        "fullname": "Aleksandra Korolova",
                        "user": "korolova",
                        "type": "user"
                    },
                    "name": "Aleksandra Korolova",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:00:20.576Z",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007adb",
                    "name": "Peter Henderson",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007adc",
                    "name": "Sanjeev Arora",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007add",
                    "name": "Pramod Viswanath",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007ade",
                    "user": {
                        "_id": "660655119e3555d648f6c6b5",
                        "avatarUrl": "/avatars/ae1e2c97a08be39b77a9f1a5c2a718ef.svg",
                        "isPro": false,
                        "fullname": "Jingbo Shang",
                        "user": "shangjingbo",
                        "type": "user"
                    },
                    "name": "Jingbo Shang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:00:07.437Z",
                    "hidden": false
                },
                {
                    "_id": "684fae8d60b4a34dbe007adf",
                    "user": {
                        "_id": "6596422646624a86ff3b3bda",
                        "avatarUrl": "/avatars/216e12b77e45ac5f1fa20932f5745411.svg",
                        "isPro": false,
                        "fullname": "Saining Xie",
                        "user": "sainx",
                        "type": "user"
                    },
                    "name": "Saining Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T13:00:01.691Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T16:29:09.000Z",
            "submittedOnDailyAt": "2025-06-16T04:13:30.111Z",
            "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
            "submittedOnDailyBy": {
                "_id": "637c7503fe115289cfecbe6b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
                "isPro": false,
                "fullname": "Wenhao Chai",
                "user": "wchai",
                "type": "user"
            },
            "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.",
            "upvotes": 11,
            "discussionId": "684fae8d60b4a34dbe007ae0",
            "ai_summary": "LLMs perform well on implementation-heavy competitive programming problems but struggle with nuanced algorithmic reasoning, as highlighted by LiveCodeBench Pro.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "competitive programming",
                "LiveCodeBench Pro",
                "Codeforces",
                "ICPC",
                "IOI",
                "algorithmic categories",
                "algorithmic reasoning",
                "case analysis"
            ]
        },
        "publishedAt": "2025-06-13T12:29:09.000Z",
        "title": "LiveCodeBench Pro: How Do Olympiad Medalists Judge LLMs in Competitive\n  Programming?",
        "summary": "Recent reports claim that large language models (LLMs) now outperform elite\nhumans in competitive programming. Drawing on knowledge from a group of\nmedalists in international algorithmic contests, we revisit this claim,\nexamining how LLMs differ from human experts and where limitations still\nremain. We introduce LiveCodeBench Pro, a benchmark composed of problems from\nCodeforces, ICPC, and IOI that are continuously updated to reduce the\nlikelihood of data contamination. A team of Olympiad medalists annotates every\nproblem for algorithmic categories and conducts a line-by-line analysis of\nfailed model-generated submissions. Using this new data and benchmark, we find\nthat frontier models still have significant limitations: without external\ntools, the best model achieves only 53% pass@1 on medium-difficulty problems\nand 0% on hard problems, domains where expert humans still excel. We also find\nthat LLMs succeed at implementation-heavy problems but struggle with nuanced\nalgorithmic reasoning and complex case analysis, often generating confidently\nincorrect justifications. High performance appears largely driven by\nimplementation precision and tool augmentation, not superior reasoning.\nLiveCodeBench Pro thus highlights the significant gap to human grandmaster\nlevels, while offering fine-grained diagnostics to steer future improvements in\ncode-centric LLM reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11928.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637c7503fe115289cfecbe6b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676361945047-637c7503fe115289cfecbe6b.jpeg",
            "fullname": "Wenhao Chai",
            "name": "wchai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 34
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08989",
            "authors": [
                {
                    "_id": "684d3eb33b733ba333687407",
                    "user": {
                        "_id": "6560763e152b659e623865ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liang",
                        "user": "MasterVito",
                        "type": "user"
                    },
                    "name": "Xiao Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T12:56:19.750Z",
                    "hidden": false
                },
                {
                    "_id": "684d3eb33b733ba333687408",
                    "user": {
                        "_id": "6545f575650796837446afca",
                        "avatarUrl": "/avatars/df9fab381e5d43af753b8fc2c7d35865.svg",
                        "isPro": false,
                        "fullname": "Zhongzhi Li",
                        "user": "lizhongzhi2022",
                        "type": "user"
                    },
                    "name": "Zhong-Zhi Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T12:56:21.547Z",
                    "hidden": false
                },
                {
                    "_id": "684d3eb33b733ba333687409",
                    "name": "Yeyun Gong",
                    "hidden": false
                },
                {
                    "_id": "684d3eb33b733ba33368740a",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "684d3eb33b733ba33368740b",
                    "name": "Hengyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "684d3eb33b733ba33368740c",
                    "user": {
                        "_id": "643f615aa16cd6d1f4c581de",
                        "avatarUrl": "/avatars/47753a3e82b44f81881600c52e1e8495.svg",
                        "isPro": false,
                        "fullname": "Yeyun Gong",
                        "user": "yegong",
                        "type": "user"
                    },
                    "name": "Yelong Shen",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-14T09:19:48.976Z",
                    "hidden": false
                },
                {
                    "_id": "684d3eb33b733ba33368740d",
                    "name": "Ying Nian Wu",
                    "hidden": false
                },
                {
                    "_id": "684d3eb33b733ba33368740e",
                    "name": "Weizhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:02:00.000Z",
            "submittedOnDailyAt": "2025-06-16T10:06:05.425Z",
            "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement\n  Learning for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "65407ba7a38390065750233f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
                "isPro": false,
                "fullname": "Zirui Song",
                "user": "Ziruibest",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective\nfor training large language models (LLMs) on complex reasoning tasks, such as\nmathematical problem solving. A prerequisite for the scalability of RLVR is a\nhigh-quality problem set with precise and verifiable answers. However, the\nscarcity of well-crafted human-labeled math problems and limited-verification\nanswers in existing distillation-oriented synthetic datasets limit their\neffectiveness in RL. Additionally, most problem synthesis strategies\nindiscriminately expand the problem set without considering the model's\ncapabilities, leading to low efficiency in generating useful questions. To\nmitigate this issue, we introduce a Self-aware Weakness-driven problem\nSynthesis framework (SwS) that systematically identifies model deficiencies and\nleverages them for problem augmentation. Specifically, we define weaknesses as\nquestions that the model consistently fails to learn through its iterative\nsampling during RL training. We then extract the core concepts from these\nfailure cases and synthesize new problems to strengthen the model's weak areas\nin subsequent augmented training, enabling it to focus on and gradually\novercome its weaknesses. Without relying on external knowledge distillation,\nour framework enables robust generalization byempowering the model to\nself-identify and address its weaknesses in RL, yielding average performance\ngains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning\nbenchmarks.",
            "upvotes": 8,
            "discussionId": "684d3eb43b733ba33368740f",
            "projectPage": "https://mastervito.github.io/MasterVito.SwS.github.io/",
            "githubRepo": "https://github.com/MasterVito/SwS",
            "ai_summary": "A self-aware problem synthesis framework that leverages model weaknesses enhances reinforcement learning with verifiable rewards, improving large language model performance on reasoning tasks.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "large language models",
                "LLMs",
                "complex reasoning tasks",
                "mathematical problem solving",
                "problem set",
                "verifiable answers",
                "human-labeled math problems",
                "synthetic datasets",
                "problem synthesis",
                "model capabilities",
                "self-aware",
                "weakness-driven",
                "problem augmentation",
                "iterative sampling",
                "core concepts",
                "synthesizing new problems",
                "robust generalization",
                "reasoning benchmarks"
            ]
        },
        "publishedAt": "2025-06-10T13:02:00.000Z",
        "title": "SwS: Self-aware Weakness-driven Problem Synthesis in Reinforcement\n  Learning for LLM Reasoning",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven effective\nfor training large language models (LLMs) on complex reasoning tasks, such as\nmathematical problem solving. A prerequisite for the scalability of RLVR is a\nhigh-quality problem set with precise and verifiable answers. However, the\nscarcity of well-crafted human-labeled math problems and limited-verification\nanswers in existing distillation-oriented synthetic datasets limit their\neffectiveness in RL. Additionally, most problem synthesis strategies\nindiscriminately expand the problem set without considering the model's\ncapabilities, leading to low efficiency in generating useful questions. To\nmitigate this issue, we introduce a Self-aware Weakness-driven problem\nSynthesis framework (SwS) that systematically identifies model deficiencies and\nleverages them for problem augmentation. Specifically, we define weaknesses as\nquestions that the model consistently fails to learn through its iterative\nsampling during RL training. We then extract the core concepts from these\nfailure cases and synthesize new problems to strengthen the model's weak areas\nin subsequent augmented training, enabling it to focus on and gradually\novercome its weaknesses. Without relying on external knowledge distillation,\nour framework enables robust generalization byempowering the model to\nself-identify and address its weaknesses in RL, yielding average performance\ngains of 10.0% and 7.7% on 7B and 32B models across eight mainstream reasoning\nbenchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08989.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65407ba7a38390065750233f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65407ba7a38390065750233f/1_IPMZbk-S9u2t18PQgMp.jpeg",
            "fullname": "Zirui Song",
            "name": "Ziruibest",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.11886",
            "authors": [
                {
                    "_id": "684fedfa1d9b438aa3957a6d",
                    "user": {
                        "_id": "64f033ef82c6eea604c4da8b",
                        "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
                        "isPro": false,
                        "fullname": "Liu Xiaoran",
                        "user": "LiuXR",
                        "type": "user"
                    },
                    "name": "Xiaoran Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T12:56:14.162Z",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a6e",
                    "name": "Siyang He",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a6f",
                    "name": "Qiqi Wang",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a70",
                    "name": "Ruixiao Li",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a71",
                    "name": "Yuerong Song",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a72",
                    "name": "Zhigeng Liu",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a73",
                    "name": "Linlin Li",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a74",
                    "name": "Qun Liu",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a75",
                    "name": "Zengfeng Huang",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a76",
                    "name": "Qipeng Guo",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a77",
                    "name": "Ziwei He",
                    "hidden": false
                },
                {
                    "_id": "684fedfa1d9b438aa3957a78",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T15:35:54.000Z",
            "submittedOnDailyAt": "2025-06-16T09:25:52.482Z",
            "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
            "submittedOnDailyBy": {
                "_id": "64f033ef82c6eea604c4da8b",
                "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
                "isPro": false,
                "fullname": "Liu Xiaoran",
                "user": "LiuXR",
                "type": "user"
            },
            "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
            "upvotes": 7,
            "discussionId": "684fedfa1d9b438aa3957a79",
            "ai_summary": "FourierAttention is a training-free framework that enhances memory efficiency in Large Language Models by compressing long-context-insensitive transformer head dimensions using orthogonal Fourier bases, while maintaining accuracy.",
            "ai_keywords": [
                "FourierAttention",
                "transformer head dimensions",
                "LongBench",
                "Needle-In-A-Haystack",
                "FlashFourierAttention",
                "orthogonal Fourier bases",
                "long-context accuracy"
            ]
        },
        "publishedAt": "2025-06-13T11:35:54.000Z",
        "title": "Beyond Homogeneous Attention: Memory-Efficient LLMs via\n  Fourier-Approximated KV Cache",
        "summary": "Large Language Models struggle with memory demands from the growing Key-Value\n(KV) cache as context lengths increase. Existing compression methods homogenize\nhead dimensions or rely on attention-guided token pruning, often sacrificing\naccuracy or introducing computational overhead. We propose FourierAttention, a\ntraining-free framework that exploits the heterogeneous roles of transformer\nhead dimensions: lower dimensions prioritize local context, while upper ones\ncapture long-range dependencies. By projecting the long-context-insensitive\ndimensions onto orthogonal Fourier bases, FourierAttention approximates their\ntemporal evolution with fixed-length spectral coefficients. Evaluations on\nLLaMA models show that FourierAttention achieves the best long-context accuracy\non LongBench and Needle-In-A-Haystack (NIAH). Besides, a custom Triton kernel,\nFlashFourierAttention, is designed to optimize memory via streamlined\nread-write operations, enabling efficient deployment without performance\ncompromise.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11886.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "fullname": "Liu Xiaoran",
            "name": "LiuXR",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.11997",
            "authors": [
                {
                    "_id": "684fd0cb60b4a34dbe007b70",
                    "user": {
                        "_id": "6333650673c07e8aebb2e941",
                        "avatarUrl": "/avatars/bfcc236641671e88c2fe5426740071d3.svg",
                        "isPro": false,
                        "fullname": "Korbinian Poeppel",
                        "user": "korbip",
                        "type": "user"
                    },
                    "name": "Korbinian Pppel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T08:35:40.461Z",
                    "hidden": false
                },
                {
                    "_id": "684fd0cb60b4a34dbe007b71",
                    "name": "Richard Freinschlag",
                    "hidden": false
                },
                {
                    "_id": "684fd0cb60b4a34dbe007b72",
                    "name": "Thomas Schmied",
                    "hidden": false
                },
                {
                    "_id": "684fd0cb60b4a34dbe007b73",
                    "name": "Wei Lin",
                    "hidden": false
                },
                {
                    "_id": "684fd0cb60b4a34dbe007b74",
                    "name": "Sepp Hochreiter",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T17:51:37.000Z",
            "submittedOnDailyAt": "2025-06-16T06:38:46.139Z",
            "title": "pLSTM: parallelizable Linear Source Transition Mark networks",
            "submittedOnDailyBy": {
                "_id": "64c3849269b1a6796052eac7",
                "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
                "isPro": false,
                "fullname": "Thomas Schmied",
                "user": "thomasschmied",
                "type": "user"
            },
            "summary": "Modern recurrent architectures, such as xLSTM and Mamba, have recently\nchallenged the Transformer in language modeling. However, their structure\nconstrains their applicability to sequences only or requires processing\nmulti-dimensional data structures, such as images or molecular graphs, in a\npre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are\nwell suited for data with a higher level structure, like 2D grids, trees, and\ndirected acyclic graphs (DAGs). In this work, we extend the notion of\nmulti-dimensionality to linear RNNs. We introduce parallelizable Linear Source\nTransition Mark networks (pLSTMs) using Source, Transition, and Mark gates that\nact on the line graph of a general DAG. This enables parallelization in analogy\nto parallel associative scans and the chunkwise-recurrent form of sequential\nlinear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this\nscheme can be efficiently implemented using einsum operations, concatenations,\nand padding in logarithmic time. pLSTMs tackle the vanishing/exploding\nactivation/gradient problem for long distances in DAGs via two distinct modes:\na directed propagation mode (P-mode) and a diffusive distribution mode\n(D-mode). To showcase the long-range capabilities of pLSTM, we introduce\narrow-pointing extrapolation as a synthetic computer vision task that contains\nlong-distance directional information. We demonstrate that pLSTMs generalize\nwell to larger image sizes, whereas Transformers struggle to extrapolate. On\nestablished molecular graph and computer vision benchmarks, pLSTMs also show\nstrong performance. Code and Datasets are available at:\nhttps://github.com/ml-jku/plstm_experiments.",
            "upvotes": 6,
            "discussionId": "684fd0cb60b4a34dbe007b75",
            "ai_summary": "pLSTMs are parallelizable linear RNNs designed for DAGs, demonstrating superior performance on long-range tasks and benchmarks compared to Transformers.",
            "ai_keywords": [
                "xLSTM",
                "Mamba",
                "Transformer",
                "Multi-Dimensional RNNs",
                "MDRNNs",
                "parallelizable Linear Source Transition Mark networks",
                "pLSTMs",
                "Source gates",
                "Transition gates",
                "Mark gates",
                "line graph",
                "DAGs",
                "parallel associative scans",
                "chunkwise-recurrent",
                "einsum operations",
                "concatenations",
                "padding",
                "vanishing/exploding activation/gradient problem",
                "directed propagation mode",
                "diffusive distribution mode",
                "arrow-pointing extrapolation",
                "computer vision task",
                "molecular graph",
                "performance benchmarks"
            ]
        },
        "publishedAt": "2025-06-13T13:51:37.000Z",
        "title": "pLSTM: parallelizable Linear Source Transition Mark networks",
        "summary": "Modern recurrent architectures, such as xLSTM and Mamba, have recently\nchallenged the Transformer in language modeling. However, their structure\nconstrains their applicability to sequences only or requires processing\nmulti-dimensional data structures, such as images or molecular graphs, in a\npre-defined sequential order. In contrast, Multi-Dimensional RNNs (MDRNNs) are\nwell suited for data with a higher level structure, like 2D grids, trees, and\ndirected acyclic graphs (DAGs). In this work, we extend the notion of\nmulti-dimensionality to linear RNNs. We introduce parallelizable Linear Source\nTransition Mark networks (pLSTMs) using Source, Transition, and Mark gates that\nact on the line graph of a general DAG. This enables parallelization in analogy\nto parallel associative scans and the chunkwise-recurrent form of sequential\nlinear RNNs, but for DAGs. For regular grids (1D and 2D), like images, this\nscheme can be efficiently implemented using einsum operations, concatenations,\nand padding in logarithmic time. pLSTMs tackle the vanishing/exploding\nactivation/gradient problem for long distances in DAGs via two distinct modes:\na directed propagation mode (P-mode) and a diffusive distribution mode\n(D-mode). To showcase the long-range capabilities of pLSTM, we introduce\narrow-pointing extrapolation as a synthetic computer vision task that contains\nlong-distance directional information. We demonstrate that pLSTMs generalize\nwell to larger image sizes, whereas Transformers struggle to extrapolate. On\nestablished molecular graph and computer vision benchmarks, pLSTMs also show\nstrong performance. Code and Datasets are available at:\nhttps://github.com/ml-jku/plstm_experiments.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11997.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c3849269b1a6796052eac7",
            "avatarUrl": "/avatars/9f0c832d5b51b659c7bb83074f02a648.svg",
            "fullname": "Thomas Schmied",
            "name": "thomasschmied",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.07464",
            "authors": [
                {
                    "_id": "684fd9a160b4a34dbe007b93",
                    "name": "Jinyoung Park",
                    "hidden": false
                },
                {
                    "_id": "684fd9a160b4a34dbe007b94",
                    "name": "Jeehye Na",
                    "hidden": false
                },
                {
                    "_id": "684fd9a160b4a34dbe007b95",
                    "name": "Jinyoung Kim",
                    "hidden": false
                },
                {
                    "_id": "684fd9a160b4a34dbe007b96",
                    "user": {
                        "_id": "63183a30a760ff8cff318be5",
                        "avatarUrl": "/avatars/1b314afd8d3319423915ffc2ae56d567.svg",
                        "isPro": false,
                        "fullname": "Hyunwoo Kim",
                        "user": "hkimcvml",
                        "type": "user"
                    },
                    "name": "Hyunwoo J. Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T15:50:47.946Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T06:15:54.000Z",
            "submittedOnDailyAt": "2025-06-16T07:17:50.892Z",
            "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO",
            "submittedOnDailyBy": {
                "_id": "64b6eae88ba7d6c922c0434a",
                "avatarUrl": "/avatars/6adac8242106ab12abeaa3584346c0cd.svg",
                "isPro": false,
                "fullname": "Jinyoung Park",
                "user": "jinypark",
                "type": "user"
            },
            "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training in enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success by employing a PPO-style reinforcement\nalgorithm with group-based normalized rewards. However, the application of GRPO\nto Video Large Language Models (Video LLMs) has been less studied. In this\npaper, we explore GRPO for video LLMs and identify two primary issues that\nimpede its effective learning: (1) reliance on safeguards, and (2) the\nvanishing advantage problem. To mitigate these challenges, we propose\nDeepVideo-R1, a video large language model trained with our proposed Reg-GRPO\n(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO\nreformulates the GRPO objective as a regression task, directly predicting the\nadvantage in GRPO. This design eliminates the need for safeguards like clipping\nand min functions, thereby facilitating more direct policy guidance by aligning\nthe model with the advantage values. We also design the difficulty-aware data\naugmentation strategy that dynamically augments training samples at solvable\ndifficulty levels, fostering diverse and informative reward signals. Our\ncomprehensive experiments show that DeepVideo-R1 significantly improves video\nreasoning performance across multiple video reasoning benchmarks.",
            "upvotes": 6,
            "discussionId": "684fd9a160b4a34dbe007b97",
            "ai_summary": "DeepVideo-R1 enhances video reasoning performance using Reg-GRPO, a regression-based GRPO approach, and difficulty-aware data augmentation for video large language models.",
            "ai_keywords": [
                "reinforcement learning",
                "Group Relative Policy Optimization",
                "GRPO",
                "Policy Optimization",
                "PPO",
                "Video Large Language Models",
                "Video LLMs",
                "DeepVideo-R1",
                "Reg-GRPO",
                "regression task",
                "advantage values",
                "difficulty-aware data augmentation",
                "video reasoning benchmarks"
            ]
        },
        "publishedAt": "2025-06-09T02:15:54.000Z",
        "title": "DeepVideo-R1: Video Reinforcement Fine-Tuning via Difficulty-aware\n  Regressive GRPO",
        "summary": "Recent works have demonstrated the effectiveness of reinforcement learning\n(RL)-based post-training in enhancing the reasoning capabilities of large\nlanguage models (LLMs). In particular, Group Relative Policy Optimization\n(GRPO) has shown impressive success by employing a PPO-style reinforcement\nalgorithm with group-based normalized rewards. However, the application of GRPO\nto Video Large Language Models (Video LLMs) has been less studied. In this\npaper, we explore GRPO for video LLMs and identify two primary issues that\nimpede its effective learning: (1) reliance on safeguards, and (2) the\nvanishing advantage problem. To mitigate these challenges, we propose\nDeepVideo-R1, a video large language model trained with our proposed Reg-GRPO\n(Regressive GRPO) and difficulty-aware data augmentation strategy. Reg-GRPO\nreformulates the GRPO objective as a regression task, directly predicting the\nadvantage in GRPO. This design eliminates the need for safeguards like clipping\nand min functions, thereby facilitating more direct policy guidance by aligning\nthe model with the advantage values. We also design the difficulty-aware data\naugmentation strategy that dynamically augments training samples at solvable\ndifficulty levels, fostering diverse and informative reward signals. Our\ncomprehensive experiments show that DeepVideo-R1 significantly improves video\nreasoning performance across multiple video reasoning benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07464.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b6eae88ba7d6c922c0434a",
            "avatarUrl": "/avatars/6adac8242106ab12abeaa3584346c0cd.svg",
            "fullname": "Jinyoung Park",
            "name": "jinypark",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.11474",
            "authors": [
                {
                    "_id": "684ffd121d9b438aa3957aa4",
                    "name": "Jaehoon Yun",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aa5",
                    "user": {
                        "_id": "64660e9ecf550af36eb2b774",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64660e9ecf550af36eb2b774/tUySYpqg7FQQ_evLOWA70.jpeg",
                        "isPro": false,
                        "fullname": "Jiwoong Sohn",
                        "user": "aheatnow",
                        "type": "user"
                    },
                    "name": "Jiwoong Sohn",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T12:56:03.283Z",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aa6",
                    "name": "Jungwoo Park",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aa7",
                    "name": "Hyunjae Kim",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aa8",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aa9",
                    "name": "Yanjun Shao",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aaa",
                    "name": "Yonghoe Koo",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aab",
                    "name": "Minhyeok Ko",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aac",
                    "name": "Qingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aad",
                    "name": "Mark Gerstein",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aae",
                    "name": "Michael Moor",
                    "hidden": false
                },
                {
                    "_id": "684ffd121d9b438aa3957aaf",
                    "name": "Jaewoo Kang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66ba71a4447411b9c0e19d71/WQaXye8b9fxDBQ63Y9Db8.jpeg"
            ],
            "publishedAt": "2025-06-13T05:36:30.000Z",
            "submittedOnDailyAt": "2025-06-16T13:11:05.007Z",
            "title": "Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified\n  Process Rewards",
            "submittedOnDailyBy": {
                "_id": "66ba71a4447411b9c0e19d71",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4f93ZrYdaKfK3F53IB51x.jpeg",
                "isPro": true,
                "fullname": "Cyril",
                "user": "cyrilzakka",
                "type": "user"
            },
            "summary": "Large language models have shown promise in clinical decision making, but\ncurrent approaches struggle to localize and correct errors at specific steps of\nthe reasoning process. This limitation is critical in medicine, where\nidentifying and addressing reasoning errors is essential for accurate diagnosis\nand effective patient care. We introduce Med-PRM, a process reward modeling\nframework that leverages retrieval-augmented generation to verify each\nreasoning step against established medical knowledge bases. By verifying\nintermediate reasoning steps with evidence retrieved from clinical guidelines\nand literature, our model can precisely assess the reasoning quality in a\nfine-grained manner. Evaluations on five medical QA benchmarks and two\nopen-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art\nperformance, with improving the performance of base models by up to 13.50%\nusing Med-PRM. Moreover, we demonstrate the generality of Med-PRM by\nintegrating it in a plug-and-play fashion with strong policy models such as\nMeerkat, achieving over 80\\% accuracy on MedQA for the first time using\nsmall-scale models of 8 billion parameters. Our code and data are available at:\nhttps://med-prm.github.io/",
            "upvotes": 5,
            "discussionId": "684ffd121d9b438aa3957ab0",
            "ai_summary": "Med-PRM enhances clinical decision making by verifying reasoning steps against medical knowledge bases, achieving state-of-the-art performance in medical QA benchmarks with improved accuracy.",
            "ai_keywords": [
                "retrieval-augmented generation",
                "process reward modeling",
                "Med-PRM",
                "intermediate reasoning steps",
                "clinical guidelines",
                "literature",
                "reasoning quality",
                "fine-grained assessment",
                "MedQA",
                "Meerkat",
                "accuracy"
            ]
        },
        "publishedAt": "2025-06-13T01:36:30.000Z",
        "title": "Med-PRM: Medical Reasoning Models with Stepwise, Guideline-verified\n  Process Rewards",
        "summary": "Large language models have shown promise in clinical decision making, but\ncurrent approaches struggle to localize and correct errors at specific steps of\nthe reasoning process. This limitation is critical in medicine, where\nidentifying and addressing reasoning errors is essential for accurate diagnosis\nand effective patient care. We introduce Med-PRM, a process reward modeling\nframework that leverages retrieval-augmented generation to verify each\nreasoning step against established medical knowledge bases. By verifying\nintermediate reasoning steps with evidence retrieved from clinical guidelines\nand literature, our model can precisely assess the reasoning quality in a\nfine-grained manner. Evaluations on five medical QA benchmarks and two\nopen-ended diagnostic tasks demonstrate that Med-PRM achieves state-of-the-art\nperformance, with improving the performance of base models by up to 13.50%\nusing Med-PRM. Moreover, we demonstrate the generality of Med-PRM by\nintegrating it in a plug-and-play fashion with strong policy models such as\nMeerkat, achieving over 80\\% accuracy on MedQA for the first time using\nsmall-scale models of 8 billion parameters. Our code and data are available at:\nhttps://med-prm.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66ba71a4447411b9c0e19d71/WQaXye8b9fxDBQ63Y9Db8.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11474.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66ba71a4447411b9c0e19d71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/4f93ZrYdaKfK3F53IB51x.jpeg",
            "fullname": "Cyril",
            "name": "cyrilzakka",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 77
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09366",
            "authors": [
                {
                    "_id": "684ae246dbd21a9cc27b111c",
                    "user": {
                        "_id": "62359088a17d7271859c88f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647677549197-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Yuxuan Kuang",
                        "user": "yxK",
                        "type": "user"
                    },
                    "name": "Yuxuan Kuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T07:17:03.322Z",
                    "hidden": false
                },
                {
                    "_id": "684ae246dbd21a9cc27b111d",
                    "name": "Haoran Geng",
                    "hidden": false
                },
                {
                    "_id": "684ae246dbd21a9cc27b111e",
                    "user": {
                        "_id": "64da71311d19239f50483005",
                        "avatarUrl": "/avatars/d97a7177adca180d795bf0f9ec66c65c.svg",
                        "isPro": false,
                        "fullname": "Amine Elhafsi",
                        "user": "AmineElhafsi",
                        "type": "user"
                    },
                    "name": "Amine Elhafsi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T07:50:59.675Z",
                    "hidden": false
                },
                {
                    "_id": "684ae246dbd21a9cc27b111f",
                    "name": "Tan-Dzung Do",
                    "hidden": false
                },
                {
                    "_id": "684ae246dbd21a9cc27b1120",
                    "name": "Pieter Abbeel",
                    "hidden": false
                },
                {
                    "_id": "684ae246dbd21a9cc27b1121",
                    "user": {
                        "_id": "65369a95605a07338de78ab0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sGFjOjLT2akN-sn5beVWL.jpeg",
                        "isPro": false,
                        "fullname": "Jitendra Malik ",
                        "user": "jitendra1995",
                        "type": "user"
                    },
                    "name": "Jitendra Malik",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T07:51:18.391Z",
                    "hidden": false
                },
                {
                    "_id": "684ae246dbd21a9cc27b1122",
                    "name": "Marco Pavone",
                    "hidden": false
                },
                {
                    "_id": "684ae246dbd21a9cc27b1123",
                    "name": "Yue Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T03:24:26.000Z",
            "submittedOnDailyAt": "2025-06-16T04:37:55.714Z",
            "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation\n  via Skill Blending",
            "submittedOnDailyBy": {
                "_id": "62359088a17d7271859c88f4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647677549197-noauth.jpeg",
                "isPro": false,
                "fullname": "Yuxuan Kuang",
                "user": "yxK",
                "type": "user"
            },
            "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.",
            "upvotes": 5,
            "discussionId": "684ae246dbd21a9cc27b1124",
            "projectPage": "https://usc-gvl.github.io/SkillBlender-web/",
            "githubRepo": "https://github.com/Humanoid-SkillBlender/SkillBlender",
            "ai_summary": "SkillBlender is a hierarchical reinforcement learning framework that uses pretrained primitive skills to efficiently solve diverse loco-manipulation tasks for humanoid robots.",
            "ai_keywords": [
                "reinforcement learning",
                "SkillBlender",
                "goal-conditioned",
                "task-agnostic primitive skills",
                "hierarchical reinforcement learning",
                "SkillBench",
                "cross-embodiment",
                "simulated benchmark",
                "loco-manipulation tasks",
                "reward engineering",
                "reward hacking"
            ]
        },
        "publishedAt": "2025-06-10T23:24:26.000Z",
        "title": "SkillBlender: Towards Versatile Humanoid Whole-Body Loco-Manipulation\n  via Skill Blending",
        "summary": "Humanoid robots hold significant potential in accomplishing daily tasks\nacross diverse environments thanks to their flexibility and human-like\nmorphology. Recent works have made significant progress in humanoid whole-body\ncontrol and loco-manipulation leveraging optimal control or reinforcement\nlearning. However, these methods require tedious task-specific tuning for each\ntask to achieve satisfactory behaviors, limiting their versatility and\nscalability to diverse tasks in daily scenarios. To that end, we introduce\nSkillBlender, a novel hierarchical reinforcement learning framework for\nversatile humanoid loco-manipulation. SkillBlender first pretrains\ngoal-conditioned task-agnostic primitive skills, and then dynamically blends\nthese skills to accomplish complex loco-manipulation tasks with minimal\ntask-specific reward engineering. We also introduce SkillBench, a parallel,\ncross-embodiment, and diverse simulated benchmark containing three embodiments,\nfour primitive skills, and eight challenging loco-manipulation tasks,\naccompanied by a set of scientific evaluation metrics balancing accuracy and\nfeasibility. Extensive simulated experiments show that our method significantly\noutperforms all baselines, while naturally regularizing behaviors to avoid\nreward hacking, resulting in more accurate and feasible movements for diverse\nloco-manipulation tasks in our daily scenarios. Our code and benchmark will be\nopen-sourced to the community to facilitate future research. Project page:\nhttps://usc-gvl.github.io/SkillBlender-web/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09366.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62359088a17d7271859c88f4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647677549197-noauth.jpeg",
            "fullname": "Yuxuan Kuang",
            "name": "yxK",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09427",
            "authors": [
                {
                    "_id": "684fa6d060b4a34dbe007aa7",
                    "user": {
                        "_id": "66d94f2a36aa5055694dfe04",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/grAN83brH0E4_S0__yLdv.jpeg",
                        "isPro": false,
                        "fullname": "fengyukang",
                        "user": "finyorko",
                        "type": "user"
                    },
                    "name": "Yukang Feng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T07:48:14.381Z",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007aa8",
                    "name": "Jianwen Sun",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007aa9",
                    "user": {
                        "_id": "6533f7ecb3852ed1ceb48e47",
                        "avatarUrl": "/avatars/5d767c093e73f06a89f625c3a5903902.svg",
                        "isPro": false,
                        "fullname": "Chuanhao Li",
                        "user": "cyrilli",
                        "type": "user"
                    },
                    "name": "Chuanhao Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T07:48:30.156Z",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007aaa",
                    "name": "Zizhen Li",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007aab",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007aac",
                    "user": {
                        "_id": "665305eff0c8c891cae7fe01",
                        "avatarUrl": "/avatars/1f372e3bc6a4eb19ef702ec96a391c96.svg",
                        "isPro": false,
                        "fullname": "Fanrui Zhang",
                        "user": "fanrui00",
                        "type": "user"
                    },
                    "name": "Fanrui Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T07:49:04.386Z",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007aad",
                    "name": "Yifan Chang",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007aae",
                    "name": "Sizhuo Zhou",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007aaf",
                    "user": {
                        "_id": "6674d02914e2aebef893779e",
                        "avatarUrl": "/avatars/acdbe3820462b87126c8f1e14f0d1a60.svg",
                        "isPro": false,
                        "fullname": "ZhangShenglin",
                        "user": "ZhangShenglin",
                        "type": "user"
                    },
                    "name": "Shenglin Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T07:49:28.741Z",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007ab0",
                    "name": "Yu Dai",
                    "hidden": false
                },
                {
                    "_id": "684fa6d060b4a34dbe007ab1",
                    "user": {
                        "_id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T07:49:35.126Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/1FHKfzv4w4VzV4nhqKCJ7.png"
            ],
            "publishedAt": "2025-06-11T06:21:20.000Z",
            "submittedOnDailyAt": "2025-06-16T03:49:15.860Z",
            "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved\n  Image-Text Generation",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.",
            "upvotes": 4,
            "discussionId": "684fa6d060b4a34dbe007ab2",
            "ai_summary": "InterSyn, a large-scale dataset with tightly interleaved image-text outputs and automated quality refinement, improves multimodal understanding and generation through the SEIR method and SynJudge, an automatic evaluation tool.",
            "ai_keywords": [
                "Large Multimodal Models (LMMs)",
                "multimodal understanding",
                "multimodal generation",
                "Self-Evaluation with Iterative Refinement (SEIR)",
                "InterSyn",
                "image-text outputs",
                "SynJudge",
                "text content",
                "image content",
                "image quality",
                "image-text synergy"
            ]
        },
        "publishedAt": "2025-06-11T02:21:20.000Z",
        "title": "A High-Quality Dataset and Reliable Evaluation for Interleaved\n  Image-Text Generation",
        "summary": "Recent advancements in Large Multimodal Models (LMMs) have significantly\nimproved multimodal understanding and generation. However, these models still\nstruggle to generate tightly interleaved image-text outputs, primarily due to\nthe limited scale, quality and instructional richness of current training\ndatasets. To address this, we introduce InterSyn, a large-scale multimodal\ndataset constructed using our Self-Evaluation with Iterative Refinement (SEIR)\nmethod. InterSyn features multi-turn, instruction-driven dialogues with tightly\ninterleaved imagetext responses, providing rich object diversity and rigorous\nautomated quality refinement, making it well-suited for training\nnext-generation instruction-following LMMs. Furthermore, to address the lack of\nreliable evaluation tools capable of assessing interleaved multimodal outputs,\nwe introduce SynJudge, an automatic evaluation model designed to quantitatively\nassess multimodal outputs along four dimensions: text content, image content,\nimage quality, and image-text synergy.\n  Experimental studies show that the SEIR method leads to substantially higher\ndataset quality compared to an otherwise identical process without refinement.\n  Moreover, LMMs trained on InterSyn achieve uniform performance gains across\nall evaluation metrics, confirming InterSyn's utility for advancing multimodal\nsystems.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/1FHKfzv4w4VzV4nhqKCJ7.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09427.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.11136",
            "authors": [
                {
                    "_id": "685003c61d9b438aa3957adb",
                    "user": {
                        "_id": "653b7e9a7c01c693a19cf0ca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653b7e9a7c01c693a19cf0ca/J1w-Iu3qo6Ljko2rPjRMC.jpeg",
                        "isPro": false,
                        "fullname": "Paul Couairon",
                        "user": "paulcouairon",
                        "type": "user"
                    },
                    "name": "Paul Couairon",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T12:57:21.672Z",
                    "hidden": false
                },
                {
                    "_id": "685003c61d9b438aa3957adc",
                    "name": "Loick Chambon",
                    "hidden": false
                },
                {
                    "_id": "685003c61d9b438aa3957add",
                    "user": {
                        "_id": "6565d4d4d4ef4fe85fb50388",
                        "avatarUrl": "/avatars/7b8752b30aac8346ade2165a0ba09ca2.svg",
                        "isPro": false,
                        "fullname": "Louis Serrano",
                        "user": "sogeeking",
                        "type": "user"
                    },
                    "name": "Louis Serrano",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T12:57:35.319Z",
                    "hidden": false
                },
                {
                    "_id": "685003c61d9b438aa3957ade",
                    "name": "Jean-Emmanuel Haugeard",
                    "hidden": false
                },
                {
                    "_id": "685003c61d9b438aa3957adf",
                    "name": "Matthieu Cord",
                    "hidden": false
                },
                {
                    "_id": "685003c61d9b438aa3957ae0",
                    "user": {
                        "_id": "67c30ca63b3d10fab6202e99",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lY9SbMEgkmo8TQ_SNneol.png",
                        "isPro": false,
                        "fullname": "Nicolas Thom",
                        "user": "nicolasthome",
                        "type": "user"
                    },
                    "name": "Nicolas Thome",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T12:57:55.446Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T20:53:12.000Z",
            "submittedOnDailyAt": "2025-06-16T10:17:06.458Z",
            "title": "JAFAR: Jack up Any Feature at Any Resolution",
            "submittedOnDailyBy": {
                "_id": "653b7e9a7c01c693a19cf0ca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653b7e9a7c01c693a19cf0ca/J1w-Iu3qo6Ljko2rPjRMC.jpeg",
                "isPro": false,
                "fullname": "Paul Couairon",
                "user": "paulcouairon",
                "type": "user"
            },
            "summary": "Foundation Vision Encoders have become essential for a wide range of dense\nvision tasks. However, their low-resolution spatial feature outputs necessitate\nfeature upsampling to produce the high-resolution modalities required for\ndownstream tasks. In this work, we introduce JAFAR, a lightweight and flexible\nfeature upsampler that enhances the spatial resolution of visual features from\nany Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs\nan attention-based module designed to promote semantic alignment between\nhigh-resolution queries, derived from low-level image features, and\nsemantically enriched low-resolution keys, using Spatial Feature Transform\n(SFT) modulation. Notably, despite the absence of high-resolution supervision,\nwe demonstrate that learning at low upsampling ratios and resolutions\ngeneralizes remarkably well to significantly higher output scales. Extensive\nexperiments show that JAFAR effectively recovers fine-grained spatial details\nand consistently outperforms existing feature upsampling methods across a\ndiverse set of downstream tasks. Project page at\nhttps://jafar-upsampler.github.io",
            "upvotes": 4,
            "discussionId": "685003c71d9b438aa3957ae1",
            "projectPage": "https://jafar-upsampler.github.io/",
            "githubRepo": "https://github.com/PaulCouairon/JAFAR",
            "ai_summary": "JAFAR is a lightweight feature upsampler using an attention-based module with Spatial Feature Transform modulation, enabling high-resolution features from Foundation Vision Encoders without high-resolution supervision.",
            "ai_keywords": [
                "Foundation Vision Encoders",
                "feature upsampling",
                "attention-based module",
                "Spatial Feature Transform",
                "SFT modulation",
                "high-resolution queries",
                "low-resolution keys",
                "semantic alignment",
                "fine-grained spatial details"
            ]
        },
        "publishedAt": "2025-06-10T16:53:12.000Z",
        "title": "JAFAR: Jack up Any Feature at Any Resolution",
        "summary": "Foundation Vision Encoders have become essential for a wide range of dense\nvision tasks. However, their low-resolution spatial feature outputs necessitate\nfeature upsampling to produce the high-resolution modalities required for\ndownstream tasks. In this work, we introduce JAFAR, a lightweight and flexible\nfeature upsampler that enhances the spatial resolution of visual features from\nany Foundation Vision Encoder to an arbitrary target resolution. JAFAR employs\nan attention-based module designed to promote semantic alignment between\nhigh-resolution queries, derived from low-level image features, and\nsemantically enriched low-resolution keys, using Spatial Feature Transform\n(SFT) modulation. Notably, despite the absence of high-resolution supervision,\nwe demonstrate that learning at low upsampling ratios and resolutions\ngeneralizes remarkably well to significantly higher output scales. Extensive\nexperiments show that JAFAR effectively recovers fine-grained spatial details\nand consistently outperforms existing feature upsampling methods across a\ndiverse set of downstream tasks. Project page at\nhttps://jafar-upsampler.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11136.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653b7e9a7c01c693a19cf0ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653b7e9a7c01c693a19cf0ca/J1w-Iu3qo6Ljko2rPjRMC.jpeg",
            "fullname": "Paul Couairon",
            "name": "paulcouairon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.11274",
            "authors": [
                {
                    "_id": "68500fd9a81b87ad9c3862a2",
                    "user": {
                        "_id": "63a4a8f0769ff94bc94b343f",
                        "avatarUrl": "/avatars/422545ce2b822389316340c8f0ef8cc4.svg",
                        "isPro": false,
                        "fullname": "Liran Ringel",
                        "user": "liranringel",
                        "type": "user"
                    },
                    "name": "Liran Ringel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T12:56:01.319Z",
                    "hidden": false
                },
                {
                    "_id": "68500fd9a81b87ad9c3862a3",
                    "user": {
                        "_id": "651bcd5d5c81ea4d1b721b94",
                        "avatarUrl": "/avatars/0a1a5f33e81204eeada03875713ca76c.svg",
                        "isPro": false,
                        "fullname": "Elad Tolochinsky",
                        "user": "eladto",
                        "type": "user"
                    },
                    "name": "Elad Tolochinsky",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-16T13:02:52.713Z",
                    "hidden": false
                },
                {
                    "_id": "68500fd9a81b87ad9c3862a4",
                    "name": "Yaniv Romano",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63a4a8f0769ff94bc94b343f/4kVfk5TmchBXVFVlE4JtY.png"
            ],
            "publishedAt": "2025-06-12T20:28:54.000Z",
            "submittedOnDailyAt": "2025-06-16T11:42:42.304Z",
            "title": "Learning a Continue-Thinking Token for Enhanced Test-Time Scaling",
            "submittedOnDailyBy": {
                "_id": "63a4a8f0769ff94bc94b343f",
                "avatarUrl": "/avatars/422545ce2b822389316340c8f0ef8cc4.svg",
                "isPro": false,
                "fullname": "Liran Ringel",
                "user": "liranringel",
                "type": "user"
            },
            "summary": "Test-time scaling has emerged as an effective approach for improving language\nmodel performance by utilizing additional compute at inference time. Recent\nstudies have shown that overriding end-of-thinking tokens (e.g., replacing\n\"</think>\" with \"Wait\") can extend reasoning steps and improve accuracy. In\nthis work, we explore whether a dedicated continue-thinking token can be\nlearned to trigger extended reasoning. We augment a distilled version of\nDeepSeek-R1 with a single learned \"<|continue-thinking|>\" token, training only\nits embedding via reinforcement learning while keeping the model weights\nfrozen. Our experiments show that this learned token achieves improved accuracy\non standard math benchmarks compared to both the baseline model and a test-time\nscaling approach that uses a fixed token (e.g., \"Wait\") for budget forcing. In\nparticular, we observe that in cases where the fixed-token approach enhances\nthe base model's accuracy, our method achieves a markedly greater improvement.\nFor example, on the GSM8K benchmark, the fixed-token approach yields a 1.3%\nabsolute improvement in accuracy, whereas our learned-token method achieves a\n4.2% improvement over the base model that does not use budget forcing.",
            "upvotes": 3,
            "discussionId": "68500fdaa81b87ad9c3862a5",
            "githubRepo": "https://github.com/liranringel/learning-continue-thinking-token",
            "ai_summary": "A continuous thinking token learned via reinforcement learning improves language model accuracy more effectively than a fixed token during inference.",
            "ai_keywords": [
                "test-time scaling",
                "language model",
                "end-of-thinking tokens",
                "continue-thinking token",
                "reinforcement learning",
                "distilled version",
                "GSM8K benchmark"
            ]
        },
        "publishedAt": "2025-06-12T16:28:54.000Z",
        "title": "Learning a Continue-Thinking Token for Enhanced Test-Time Scaling",
        "summary": "Test-time scaling has emerged as an effective approach for improving language\nmodel performance by utilizing additional compute at inference time. Recent\nstudies have shown that overriding end-of-thinking tokens (e.g., replacing\n\"</think>\" with \"Wait\") can extend reasoning steps and improve accuracy. In\nthis work, we explore whether a dedicated continue-thinking token can be\nlearned to trigger extended reasoning. We augment a distilled version of\nDeepSeek-R1 with a single learned \"<|continue-thinking|>\" token, training only\nits embedding via reinforcement learning while keeping the model weights\nfrozen. Our experiments show that this learned token achieves improved accuracy\non standard math benchmarks compared to both the baseline model and a test-time\nscaling approach that uses a fixed token (e.g., \"Wait\") for budget forcing. In\nparticular, we observe that in cases where the fixed-token approach enhances\nthe base model's accuracy, our method achieves a markedly greater improvement.\nFor example, on the GSM8K benchmark, the fixed-token approach yields a 1.3%\nabsolute improvement in accuracy, whereas our learned-token method achieves a\n4.2% improvement over the base model that does not use budget forcing.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a4a8f0769ff94bc94b343f/4kVfk5TmchBXVFVlE4JtY.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11274.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4a8f0769ff94bc94b343f",
            "avatarUrl": "/avatars/422545ce2b822389316340c8f0ef8cc4.svg",
            "fullname": "Liran Ringel",
            "name": "liranringel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08592",
            "authors": [
                {
                    "_id": "684cfefc3b733ba3336873a6",
                    "user": {
                        "_id": "650f0fac11f3210cf7a8a849",
                        "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
                        "isPro": false,
                        "fullname": "Leon Xu",
                        "user": "lxucs",
                        "type": "user"
                    },
                    "name": "Liyan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T07:50:16.178Z",
                    "hidden": false
                },
                {
                    "_id": "684cfefc3b733ba3336873a7",
                    "name": "Zhenlin Su",
                    "hidden": false
                },
                {
                    "_id": "684cfefc3b733ba3336873a8",
                    "name": "Mo Yu",
                    "hidden": false
                },
                {
                    "_id": "684cfefc3b733ba3336873a9",
                    "name": "Jiangnan Li",
                    "hidden": false
                },
                {
                    "_id": "684cfefc3b733ba3336873aa",
                    "name": "Fandong Meng",
                    "hidden": false
                },
                {
                    "_id": "684cfefc3b733ba3336873ab",
                    "name": "Jie Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T09:00:33.000Z",
            "submittedOnDailyAt": "2025-06-16T06:09:56.672Z",
            "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity\n  Dilemma of Embeddings",
            "submittedOnDailyBy": {
                "_id": "650f0fac11f3210cf7a8a849",
                "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
                "isPro": false,
                "fullname": "Leon Xu",
                "user": "lxucs",
                "type": "user"
            },
            "summary": "This work focuses on an observed limitation of text encoders: embeddings may\nnot be able to recognize fine-grained entities or events within the semantics,\nresulting in failed dense retrieval on even simple cases. To examine such\nbehaviors, we first introduce a new evaluation dataset in Chinese, named\nCapRetrieval, whose passages are image captions, and queries are phrases\ninquiring entities or events in various forms. Zero-shot evaluation suggests\nthat encoders may fail on these fine-grained matching, regardless of training\nsources or model sizes. Aiming for enhancement, we proceed to finetune encoders\nwith our proposed data generation strategies, which obtains the best\nperformance on CapRetrieval. Within this process, we further identify an issue\nof granularity dilemma, a challenge for embeddings to express fine-grained\nsalience while aligning with overall semantics. Our dataset, code and models in\nthis work are publicly released at https://github.com/lxucs/CapRetrieval.",
            "upvotes": 3,
            "discussionId": "684cfefc3b733ba3336873ac",
            "ai_summary": "A new dataset named CapRetrieval is introduced to evaluate the ability of text encoders to recognize fine-grained entities and events, highlighting challenges in dense retrieval tasks.",
            "ai_keywords": [
                "text encoders",
                "embeddings",
                "fine-grained entities",
                "events",
                "dense retrieval",
                "zero-shot evaluation",
                "data generation strategies",
                "granularity dilemma"
            ]
        },
        "publishedAt": "2025-06-10T05:00:33.000Z",
        "title": "Dense Retrievers Can Fail on Simple Queries: Revealing The Granularity\n  Dilemma of Embeddings",
        "summary": "This work focuses on an observed limitation of text encoders: embeddings may\nnot be able to recognize fine-grained entities or events within the semantics,\nresulting in failed dense retrieval on even simple cases. To examine such\nbehaviors, we first introduce a new evaluation dataset in Chinese, named\nCapRetrieval, whose passages are image captions, and queries are phrases\ninquiring entities or events in various forms. Zero-shot evaluation suggests\nthat encoders may fail on these fine-grained matching, regardless of training\nsources or model sizes. Aiming for enhancement, we proceed to finetune encoders\nwith our proposed data generation strategies, which obtains the best\nperformance on CapRetrieval. Within this process, we further identify an issue\nof granularity dilemma, a challenge for embeddings to express fine-grained\nsalience while aligning with overall semantics. Our dataset, code and models in\nthis work are publicly released at https://github.com/lxucs/CapRetrieval.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08592.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650f0fac11f3210cf7a8a849",
            "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
            "fullname": "Leon Xu",
            "name": "lxucs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08477",
            "authors": [
                {
                    "_id": "684fb8cb60b4a34dbe007b05",
                    "name": "Fengjun Pan",
                    "hidden": false
                },
                {
                    "_id": "684fb8cb60b4a34dbe007b06",
                    "name": "Anh Tuan Luu",
                    "hidden": false
                },
                {
                    "_id": "684fb8cb60b4a34dbe007b07",
                    "user": {
                        "_id": "64cb02869e30a46f7b80b355",
                        "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
                        "isPro": false,
                        "fullname": "Xiaobao Wu",
                        "user": "bobxwu",
                        "type": "user"
                    },
                    "name": "Xiaobao Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T07:16:04.938Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T06:10:45.000Z",
            "submittedOnDailyAt": "2025-06-16T04:59:02.024Z",
            "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "64cb02869e30a46f7b80b355",
                "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
                "isPro": false,
                "fullname": "Xiaobao Wu",
                "user": "bobxwu",
                "type": "user"
            },
            "summary": "Detecting harmful memes is essential for maintaining the integrity of online\nenvironments. However, current approaches often struggle with resource\nefficiency, flexibility, or explainability, limiting their practical deployment\nin content moderation systems. To address these challenges, we introduce\nU-CoT+, a novel framework for harmful meme detection. Instead of relying solely\non prompting or fine-tuning multimodal models, we first develop a high-fidelity\nmeme-to-text pipeline that converts visual memes into detail-preserving textual\ndescriptions. This design decouples meme interpretation from meme\nclassification, thus avoiding immediate reasoning over complex raw visual\ncontent and enabling resource-efficient harmful meme detection with general\nlarge language models (LLMs). Building on these textual descriptions, we\nfurther incorporate targeted, interpretable human-crafted guidelines to guide\nmodels' reasoning under zero-shot CoT prompting. As such, this framework allows\nfor easy adaptation to different harmfulness detection criteria across\nplatforms, regions, and over time, offering high flexibility and\nexplainability. Extensive experiments on seven benchmark datasets validate the\neffectiveness of our framework, highlighting its potential for explainable and\nlow-resource harmful meme detection using small-scale LLMs. Codes and data are\navailable at: https://anonymous.4open.science/r/HMC-AF2B/README.md.",
            "upvotes": 3,
            "discussionId": "684fb8cb60b4a34dbe007b08",
            "ai_summary": "U-CoT+ is a novel framework for detecting harmful memes by converting them into textual descriptions and using human-crafted guidelines with zero-shot CoT prompting to achieve high flexibility and explainability with small-scale LLMs.",
            "ai_keywords": [
                "U-CoT+",
                "meme-to-text pipeline",
                "high-fidelity",
                "zero-shot CoT prompting",
                "human-crafted guidelines",
                "large language models (LLMs)",
                "harmful meme detection",
                "explainability",
                "flexibility",
                "benchmark datasets"
            ]
        },
        "publishedAt": "2025-06-10T02:10:45.000Z",
        "title": "Detecting Harmful Memes with Decoupled Understanding and Guided CoT\n  Reasoning",
        "summary": "Detecting harmful memes is essential for maintaining the integrity of online\nenvironments. However, current approaches often struggle with resource\nefficiency, flexibility, or explainability, limiting their practical deployment\nin content moderation systems. To address these challenges, we introduce\nU-CoT+, a novel framework for harmful meme detection. Instead of relying solely\non prompting or fine-tuning multimodal models, we first develop a high-fidelity\nmeme-to-text pipeline that converts visual memes into detail-preserving textual\ndescriptions. This design decouples meme interpretation from meme\nclassification, thus avoiding immediate reasoning over complex raw visual\ncontent and enabling resource-efficient harmful meme detection with general\nlarge language models (LLMs). Building on these textual descriptions, we\nfurther incorporate targeted, interpretable human-crafted guidelines to guide\nmodels' reasoning under zero-shot CoT prompting. As such, this framework allows\nfor easy adaptation to different harmfulness detection criteria across\nplatforms, regions, and over time, offering high flexibility and\nexplainability. Extensive experiments on seven benchmark datasets validate the\neffectiveness of our framework, highlighting its potential for explainable and\nlow-resource harmful meme detection using small-scale LLMs. Codes and data are\navailable at: https://anonymous.4open.science/r/HMC-AF2B/README.md.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08477.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64cb02869e30a46f7b80b355",
            "avatarUrl": "/avatars/81ce4ba78826b54f0e1b53eeaff87ee6.svg",
            "fullname": "Xiaobao Wu",
            "name": "bobxwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.11305",
            "authors": [
                {
                    "_id": "68500ddfa81b87ad9c38629c",
                    "user": {
                        "_id": "68473f12c0d029ddabed529d",
                        "avatarUrl": "/avatars/2588b702884092abc7436f9df26bdfda.svg",
                        "isPro": false,
                        "fullname": "Mohammad Hammoud",
                        "user": "mhhamoud",
                        "type": "user"
                    },
                    "name": "Mohammad Hammoud",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-16T13:37:25.460Z",
                    "hidden": false
                },
                {
                    "_id": "68500ddfa81b87ad9c38629d",
                    "user": {
                        "_id": "683c849152d744770fe82ade",
                        "avatarUrl": "/avatars/6117767025cd743c19eb1f0b3b4d3ac5.svg",
                        "isPro": false,
                        "fullname": "Devang Acharya",
                        "user": "dacharya-avey",
                        "type": "user"
                    },
                    "name": "Devang Acharya",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-16T12:28:16.482Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/683c849152d744770fe82ade/0kSb-LAJtrHBdTSMPHlTC.png",
                "https://cdn-uploads.huggingface.co/production/uploads/683c849152d744770fe82ade/LReojSRVlJChEByk5sYaT.png"
            ],
            "publishedAt": "2025-06-12T21:11:06.000Z",
            "submittedOnDailyAt": "2025-06-16T22:39:01.232Z",
            "title": "Don't Pay Attention",
            "submittedOnDailyBy": {
                "_id": "683c849152d744770fe82ade",
                "avatarUrl": "/avatars/6117767025cd743c19eb1f0b3b4d3ac5.svg",
                "isPro": false,
                "fullname": "Devang Acharya",
                "user": "dacharya-avey",
                "type": "user"
            },
            "summary": "The Transformer has become the de facto standard for large language models\nand a wide range of downstream tasks across various domains. Despite its\nnumerous advantages like inherent training parallelism, the Transformer still\nfaces key challenges due to its inability to effectively process sequences\nbeyond a fixed context window and the quadratic complexity of its attention\nmechanism. These challenges have renewed interest in RNN-like architectures,\nwhich offer linear scaling with sequence length and improved handling of\nlong-range dependencies, albeit with limited parallelism due to their\ninherently recurrent nature. In this paper, we propose Avey, a new neural\nfoundational architecture that breaks away from both attention and recurrence.\nAvey comprises a ranker and an autoregressive neural processor, which\ncollaboratively identify and contextualize only the most relevant tokens for\nany given token, regardless of their positions in the sequence. Specifically,\nAvey decouples sequence length from context width, thus enabling effective\nprocessing of arbitrarily long sequences. Experimental results show that Avey\ncompares favorably to the Transformer across a variety of standard short-range\nNLP benchmarks, while notably excelling at capturing long-range dependencies.",
            "upvotes": 2,
            "discussionId": "68500de0a81b87ad9c38629e",
            "githubRepo": "https://github.com/rimads/avey-dpa",
            "ai_summary": "Avey, a new neural architecture combining a ranker and an autoregressive processor, demonstrates superior performance over the Transformer in processing long-range dependencies and has competitive short-range capabilities.",
            "ai_keywords": [
                "Transformer",
                "RNN-like architectures",
                "attention mechanism",
                "autoregressive neural processor",
                "ranker",
                "long-range dependencies",
                "short-range NLP benchmarks",
                "context width",
                "sequence length"
            ]
        },
        "publishedAt": "2025-06-12T17:11:06.000Z",
        "title": "Don't Pay Attention",
        "summary": "The Transformer has become the de facto standard for large language models\nand a wide range of downstream tasks across various domains. Despite its\nnumerous advantages like inherent training parallelism, the Transformer still\nfaces key challenges due to its inability to effectively process sequences\nbeyond a fixed context window and the quadratic complexity of its attention\nmechanism. These challenges have renewed interest in RNN-like architectures,\nwhich offer linear scaling with sequence length and improved handling of\nlong-range dependencies, albeit with limited parallelism due to their\ninherently recurrent nature. In this paper, we propose Avey, a new neural\nfoundational architecture that breaks away from both attention and recurrence.\nAvey comprises a ranker and an autoregressive neural processor, which\ncollaboratively identify and contextualize only the most relevant tokens for\nany given token, regardless of their positions in the sequence. Specifically,\nAvey decouples sequence length from context width, thus enabling effective\nprocessing of arbitrarily long sequences. Experimental results show that Avey\ncompares favorably to the Transformer across a variety of standard short-range\nNLP benchmarks, while notably excelling at capturing long-range dependencies.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/683c849152d744770fe82ade/0kSb-LAJtrHBdTSMPHlTC.png",
            "https://cdn-uploads.huggingface.co/production/uploads/683c849152d744770fe82ade/LReojSRVlJChEByk5sYaT.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11305.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "683c849152d744770fe82ade",
            "avatarUrl": "/avatars/6117767025cd743c19eb1f0b3b4d3ac5.svg",
            "fullname": "Devang Acharya",
            "name": "dacharya-avey",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10387",
            "authors": [
                {
                    "_id": "6850415f2932e11c891b5848",
                    "name": "Yuquan Xie",
                    "hidden": false
                },
                {
                    "_id": "6850415f2932e11c891b5849",
                    "name": "Zaijing Li",
                    "hidden": false
                },
                {
                    "_id": "6850415f2932e11c891b584a",
                    "name": "Rui Shao",
                    "hidden": false
                },
                {
                    "_id": "6850415f2932e11c891b584b",
                    "name": "Gongwei Chen",
                    "hidden": false
                },
                {
                    "_id": "6850415f2932e11c891b584c",
                    "name": "Kaiwen Zhou",
                    "hidden": false
                },
                {
                    "_id": "6850415f2932e11c891b584d",
                    "name": "Yinchuan Li",
                    "hidden": false
                },
                {
                    "_id": "6850415f2932e11c891b584e",
                    "name": "Dongmei Jiang",
                    "hidden": false
                },
                {
                    "_id": "6850415f2932e11c891b584f",
                    "name": "Liqiang Nie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T06:21:19.000Z",
            "submittedOnDailyAt": "2025-06-16T14:39:56.216Z",
            "title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal\n  Skills",
            "submittedOnDailyBy": {
                "_id": "66b45fe75d0ac130d7d82764",
                "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
                "isPro": false,
                "fullname": "Zaijing Li",
                "user": "dawn0815",
                "type": "user"
            },
            "summary": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI\nagents have yielded promising outcomes. However, these agents still struggle\nwith long-horizon tasks in online environments, primarily due to insufficient\nknowledge and the inherent gap between offline and online domains. In this\npaper, inspired by how humans generalize knowledge in open-ended environments,\nwe propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of\ninsufficient knowledge. It progressively abstracts trajectories into execution\nskills, core skills, and ultimately meta-skills, providing a hierarchical\nknowledge structure for long-horizon task planning. To bridge the domain gap,\nwe propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,\nwhich efficiently leverages skills acquired in offline environments to reduce\nthe action search space during online tree exploration. Building on HMS, we\npropose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To\nvalidate the performance of Mirage-1 in real-world long-horizon scenarios, we\nconstructed a new benchmark, AndroidLH. Experimental results show that Mirage-1\noutperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld,\nMobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:\nhttps://cybertronagent.github.io/Mirage-1.github.io/",
            "upvotes": 2,
            "discussionId": "6850415f2932e11c891b5850",
            "ai_summary": "Hierarchical Multimodal Skills and Skill-Augmented Monte Carlo Tree Search improve multimodal GUI agent performance in long-horizon tasks by abstracting knowledge and bridging the offline-online domain gap.",
            "ai_keywords": [
                "Hierarchical Multimodal Skills",
                "Skill-Augmented Monte Carlo Tree Search",
                "GUI agents",
                "long-horizon tasks",
                "offline environments",
                "online environments",
                "execution skills",
                "core skills",
                "meta-skills",
                "action search space",
                "knowledge structure",
                "task planning",
                "AndroidLH benchmark",
                "Mirage-1",
                "AndroidWorld",
                "MobileMiniWob++",
                "Mind2Web-Live"
            ]
        },
        "publishedAt": "2025-06-12T02:21:19.000Z",
        "title": "Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal\n  Skills",
        "summary": "Recent efforts to leverage the Multi-modal Large Language Model (MLLM) as GUI\nagents have yielded promising outcomes. However, these agents still struggle\nwith long-horizon tasks in online environments, primarily due to insufficient\nknowledge and the inherent gap between offline and online domains. In this\npaper, inspired by how humans generalize knowledge in open-ended environments,\nwe propose a Hierarchical Multimodal Skills (HMS) module to tackle the issue of\ninsufficient knowledge. It progressively abstracts trajectories into execution\nskills, core skills, and ultimately meta-skills, providing a hierarchical\nknowledge structure for long-horizon task planning. To bridge the domain gap,\nwe propose the Skill-Augmented Monte Carlo Tree Search (SA-MCTS) algorithm,\nwhich efficiently leverages skills acquired in offline environments to reduce\nthe action search space during online tree exploration. Building on HMS, we\npropose Mirage-1, a multimodal, cross-platform, plug-and-play GUI agent. To\nvalidate the performance of Mirage-1 in real-world long-horizon scenarios, we\nconstructed a new benchmark, AndroidLH. Experimental results show that Mirage-1\noutperforms previous agents by 32\\%, 19\\%, 15\\%, and 79\\% on AndroidWorld,\nMobileMiniWob++, Mind2Web-Live, and AndroidLH, respectively. Project page:\nhttps://cybertronagent.github.io/Mirage-1.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10387.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b45fe75d0ac130d7d82764",
            "avatarUrl": "/avatars/09253f41f82c533b36199f82620cd075.svg",
            "fullname": "Zaijing Li",
            "name": "dawn0815",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09038",
            "authors": [
                {
                    "_id": "68509e6c5e07650ecce88f89",
                    "name": "Polina Kirichenko",
                    "hidden": false
                },
                {
                    "_id": "68509e6c5e07650ecce88f8a",
                    "name": "Mark Ibrahim",
                    "hidden": false
                },
                {
                    "_id": "68509e6c5e07650ecce88f8b",
                    "name": "Kamalika Chaudhuri",
                    "hidden": false
                },
                {
                    "_id": "68509e6c5e07650ecce88f8c",
                    "name": "Samuel J. Bell",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:57:30.000Z",
            "submittedOnDailyAt": "2025-06-16T21:16:13.859Z",
            "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions",
            "submittedOnDailyBy": {
                "_id": "64c17345e82e55936cf971bc",
                "avatarUrl": "/avatars/b39c5637e00291410620afb7c770d587.svg",
                "isPro": false,
                "fullname": "Mark Ibrahim",
                "user": "marksibrahim",
                "type": "user"
            },
            "summary": "For Large Language Models (LLMs) to be reliably deployed in both everyday and\nhigh-stakes domains, knowing when not to answer is equally critical as\nanswering correctly. Real-world user queries, which can be underspecified,\nill-posed, or fundamentally unanswerable, require LLMs to reason about\nuncertainty and selectively abstain -- i.e., refuse to answer definitively.\nHowever, abstention remains understudied, without a systematic evaluation\nframework for modern LLMs. In this work, we introduce AbstentionBench, a\nlarge-scale benchmark for holistically evaluating abstention across 20 diverse\ndatasets, including questions with unknown answers, underspecification, false\npremises, subjective interpretations, and outdated information. Evaluating 20\nfrontier LLMs reveals abstention is an unsolved problem, and one where scaling\nmodels is of little use. While recent reasoning LLMs have shown impressive\nresults in complex problem solving, surprisingly, we find that reasoning\nfine-tuning degrades abstention (by 24% on average), even for math and\nscience domains on which reasoning models are explicitly trained. We find that\nwhile a carefully crafted system prompt can boost abstention in practice, it\ndoes not resolve models' fundamental inability to reason about uncertainty. We\nrelease AbstentionBench to foster research into advancing LLM reliability.",
            "upvotes": 2,
            "discussionId": "68509e6c5e07650ecce88f8d",
            "ai_summary": "AbstentionBench evaluates the ability of LLMs to abstain from answering uncertain or unanswerable questions, revealing that reasoning fine-tuning often degrades this capability.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "abstention",
                "uncertainty reasoning",
                "AbstentionBench",
                "frontier LLMs",
                "undetermined answers",
                "underspecification",
                "false premises",
                "subjective interpretations",
                "outdated information",
                "parameter-efficient fine-tuning",
                "reasoning fine-tuning"
            ]
        },
        "publishedAt": "2025-06-10T13:57:30.000Z",
        "title": "AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions",
        "summary": "For Large Language Models (LLMs) to be reliably deployed in both everyday and\nhigh-stakes domains, knowing when not to answer is equally critical as\nanswering correctly. Real-world user queries, which can be underspecified,\nill-posed, or fundamentally unanswerable, require LLMs to reason about\nuncertainty and selectively abstain -- i.e., refuse to answer definitively.\nHowever, abstention remains understudied, without a systematic evaluation\nframework for modern LLMs. In this work, we introduce AbstentionBench, a\nlarge-scale benchmark for holistically evaluating abstention across 20 diverse\ndatasets, including questions with unknown answers, underspecification, false\npremises, subjective interpretations, and outdated information. Evaluating 20\nfrontier LLMs reveals abstention is an unsolved problem, and one where scaling\nmodels is of little use. While recent reasoning LLMs have shown impressive\nresults in complex problem solving, surprisingly, we find that reasoning\nfine-tuning degrades abstention (by 24% on average), even for math and\nscience domains on which reasoning models are explicitly trained. We find that\nwhile a carefully crafted system prompt can boost abstention in practice, it\ndoes not resolve models' fundamental inability to reason about uncertainty. We\nrelease AbstentionBench to foster research into advancing LLM reliability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09038.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c17345e82e55936cf971bc",
            "avatarUrl": "/avatars/b39c5637e00291410620afb7c770d587.svg",
            "fullname": "Mark Ibrahim",
            "name": "marksibrahim",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.11702",
            "authors": [
                {
                    "_id": "684fae8360b4a34dbe007ac9",
                    "user": {
                        "_id": "5fad8602b8423e1d80b8a965",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
                        "isPro": false,
                        "fullname": "Victor Gallego",
                        "user": "vicgalle",
                        "type": "user"
                    },
                    "name": "Vctor Gallego",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-16T05:42:55.745Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T12:17:38.000Z",
            "submittedOnDailyAt": "2025-06-16T04:12:09.638Z",
            "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
            "submittedOnDailyBy": {
                "_id": "5fad8602b8423e1d80b8a965",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
                "isPro": false,
                "fullname": "Victor Gallego",
                "user": "vicgalle",
                "type": "user"
            },
            "summary": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning",
            "upvotes": 1,
            "discussionId": "684fae8460b4a34dbe007aca",
            "ai_summary": "Configurable Preference Tuning enables language models to dynamically adjust their behavior based on human-interprettable directives, using rubric-guided preference data for fine-tuning and inference-time modulation.",
            "ai_keywords": [
                "Configurable Preference Tuning",
                "Direct Preference Optimization",
                "language models",
                "fine-grained control",
                "rubric-guided preferences",
                "inference-time modulation"
            ]
        },
        "publishedAt": "2025-06-13T08:17:38.000Z",
        "title": "Configurable Preference Tuning with Rubric-Guided Synthetic Data",
        "summary": "Models of human feedback for AI alignment, such as those underpinning Direct\nPreference Optimization (DPO), often bake in a singular, static set of\npreferences, limiting adaptability. This paper challenges the assumption of\nmonolithic preferences by introducing Configurable Preference Tuning (CPT), a\nnovel framework for endowing language models with the ability to dynamically\nadjust their behavior based on explicit, human-interpretable directives. CPT\nleverages synthetically generated preference data, conditioned on system\nprompts derived from structured, fine-grained rubrics that define desired\nattributes like writing style. By fine-tuning with these rubric-guided\npreferences, the LLM learns to modulate its outputs at inference time in\nresponse to the system prompt, without retraining. This approach not only\noffers fine-grained control but also provides a mechanism for modeling more\nnuanced and context-dependent human feedback. Several experimental artifacts,\nsuch as training code, generated datasets and fine-tuned models are released at\nhttps://github.com/vicgalle/configurable-preference-tuning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11702.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "fullname": "Victor Gallego",
            "name": "vicgalle",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 129
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10082",
            "authors": [
                {
                    "_id": "685018392932e11c891b57f8",
                    "name": "Chenjian Gao",
                    "hidden": false
                },
                {
                    "_id": "685018392932e11c891b57f9",
                    "name": "Lihe Ding",
                    "hidden": false
                },
                {
                    "_id": "685018392932e11c891b57fa",
                    "name": "Xin Cai",
                    "hidden": false
                },
                {
                    "_id": "685018392932e11c891b57fb",
                    "name": "Zhanpeng Huang",
                    "hidden": false
                },
                {
                    "_id": "685018392932e11c891b57fc",
                    "name": "Zibin Wang",
                    "hidden": false
                },
                {
                    "_id": "685018392932e11c891b57fd",
                    "name": "Tianfan Xue",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/659d5f6f499009b2590c0442/sMte_F1B9VVC3g5R9CoyD.mp4"
            ],
            "publishedAt": "2025-06-11T18:03:55.000Z",
            "submittedOnDailyAt": "2025-06-16T11:46:24.596Z",
            "title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware\n  LoRA Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "659d5f6f499009b2590c0442",
                "avatarUrl": "/avatars/85e3c1d3f10d691b01b0f3085e7dd441.svg",
                "isPro": false,
                "fullname": "cjeen",
                "user": "cjeen",
                "type": "user"
            },
            "summary": "Video editing using diffusion models has achieved remarkable results in\ngenerating high-quality edits for videos. However, current methods often rely\non large-scale pretraining, limiting flexibility for specific edits.\nFirst-frame-guided editing provides control over the first frame, but lacks\nflexibility over subsequent frames. To address this, we propose a mask-based\nLoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video\n(I2V) models for flexible video editing. Our approach preserves background\nregions while enabling controllable edits propagation. This solution offers\nefficient and adaptable video editing without altering the model architecture.\nTo better steer this process, we incorporate additional references, such as\nalternate viewpoints or representative scene states, which serve as visual\nanchors for how content should unfold. We address the control challenge using a\nmask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model\nto the editing context. The model must learn from two distinct sources: the\ninput video provides spatial structure and motion cues, while reference images\noffer appearance guidance. A spatial mask enables region-specific learning by\ndynamically modulating what the model attends to, ensuring that each area draws\nfrom the appropriate source. Experimental results show our method achieves\nsuperior video editing performance compared to state-of-the-art methods.",
            "upvotes": 1,
            "discussionId": "685018392932e11c891b57fe",
            "projectPage": "https://cjeen.github.io/LoraEditPaper/",
            "githubRepo": "https://github.com/cjeen/LoRAEdit",
            "ai_summary": "A mask-based LoRA tuning method for video editing adapts pretrained Image-to-Video models for flexible and high-quality video editing, using spatial masks and reference images for context-specific adaptation.",
            "ai_keywords": [
                "diffusion models",
                "mask-basedLoRA tuning",
                "Image-to-Video (I2V) models",
                "first-frame-guided editing",
                "spatial structure",
                "motion cues",
                "appearance guidance",
                "spatial mask"
            ]
        },
        "publishedAt": "2025-06-11T14:03:55.000Z",
        "title": "LoRA-Edit: Controllable First-Frame-Guided Video Editing via Mask-Aware\n  LoRA Fine-Tuning",
        "summary": "Video editing using diffusion models has achieved remarkable results in\ngenerating high-quality edits for videos. However, current methods often rely\non large-scale pretraining, limiting flexibility for specific edits.\nFirst-frame-guided editing provides control over the first frame, but lacks\nflexibility over subsequent frames. To address this, we propose a mask-based\nLoRA (Low-Rank Adaptation) tuning method that adapts pretrained Image-to-Video\n(I2V) models for flexible video editing. Our approach preserves background\nregions while enabling controllable edits propagation. This solution offers\nefficient and adaptable video editing without altering the model architecture.\nTo better steer this process, we incorporate additional references, such as\nalternate viewpoints or representative scene states, which serve as visual\nanchors for how content should unfold. We address the control challenge using a\nmask-driven LoRA tuning strategy that adapts a pre-trained image-to-video model\nto the editing context. The model must learn from two distinct sources: the\ninput video provides spatial structure and motion cues, while reference images\noffer appearance guidance. A spatial mask enables region-specific learning by\ndynamically modulating what the model attends to, ensuring that each area draws\nfrom the appropriate source. Experimental results show our method achieves\nsuperior video editing performance compared to state-of-the-art methods.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/659d5f6f499009b2590c0442/sMte_F1B9VVC3g5R9CoyD.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10082.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "659d5f6f499009b2590c0442",
            "avatarUrl": "/avatars/85e3c1d3f10d691b01b0f3085e7dd441.svg",
            "fullname": "cjeen",
            "name": "cjeen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10056",
            "authors": [
                {
                    "_id": "6850651a2932e11c891b5892",
                    "name": "Gabriel Orlanski",
                    "hidden": false
                },
                {
                    "_id": "6850651a2932e11c891b5893",
                    "name": "Nicholas Roberts",
                    "hidden": false
                },
                {
                    "_id": "6850651a2932e11c891b5894",
                    "name": "Aws Albarghouthi",
                    "hidden": false
                },
                {
                    "_id": "6850651a2932e11c891b5895",
                    "name": "Frederic Sala",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:58:21.000Z",
            "submittedOnDailyAt": "2025-06-16T17:10:34.135Z",
            "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for\n  Throughput",
            "submittedOnDailyBy": {
                "_id": "603ac96345ca069a4c0817e3",
                "avatarUrl": "/avatars/54c62f1ab3bfeded8a1e73b0a1b38768.svg",
                "isPro": false,
                "fullname": "Gabe Orlanski",
                "user": "gabeorlanski",
                "type": "user"
            },
            "summary": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems.",
            "upvotes": 1,
            "discussionId": "6850651b2932e11c891b5896"
        },
        "publishedAt": "2025-06-11T13:58:21.000Z",
        "title": "Reward Models Enable Scalable Code Verification by Trading Accuracy for\n  Throughput",
        "summary": "The standard paradigm for solving coding tasks via large language models\n(LLMs) is to generate-then-rank programs, where the latter step uses a verifier\nin the ranking process. The growing consensus is that a comprehensive verifier\n(e.g., a full test suite) should be prioritized over an outcome reward model\n(ORM) whenever possible, with little consideration given to the trade-offs\ninvolved. We aim to challenge this assumption by systematically exploring the\ntradeoff between speed and accuracy. We find that ORMs play a crucial role in\nscaling verification through trading accuracy for speed, even when a\ncomprehensive verifier is available. Their value becomes especially apparent\nwhen used in a generate-prune-then-rank approach, where a faster but less\naccurate verifier removes incorrect solutions prior to ranking -- leading to a\nsystem that is 11.65x faster while only being 8.33% less accurate than the full\ntest suite. We analyze the generate-prune-then-rank approach and show that it\nworks by filtering out incorrect but highly ranked solutions. These findings\nenable the design of scalable and accurate program ranking systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10056.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "603ac96345ca069a4c0817e3",
            "avatarUrl": "/avatars/54c62f1ab3bfeded8a1e73b0a1b38768.svg",
            "fullname": "Gabe Orlanski",
            "name": "gabeorlanski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.11130",
            "authors": [
                {
                    "_id": "684fdd8e1d9b438aa39579c6",
                    "name": "Cheng Kang Chou",
                    "hidden": false
                },
                {
                    "_id": "684fdd8e1d9b438aa39579c7",
                    "name": "Chan-Jan Hsu",
                    "hidden": false
                },
                {
                    "_id": "684fdd8e1d9b438aa39579c8",
                    "name": "Ho-Lam Chung",
                    "hidden": false
                },
                {
                    "_id": "684fdd8e1d9b438aa39579c9",
                    "user": {
                        "_id": "637b996e086af1cb122ead59",
                        "avatarUrl": "/avatars/c39d4c46d413b83cabf85940bd2423d1.svg",
                        "isPro": false,
                        "fullname": "Liang-Hsuan Tseng",
                        "user": "andybi7676",
                        "type": "user"
                    },
                    "name": "Liang-Hsuan Tseng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T12:58:44.206Z",
                    "hidden": false
                },
                {
                    "_id": "684fdd8e1d9b438aa39579ca",
                    "name": "Hsi-Chun Cheng",
                    "hidden": false
                },
                {
                    "_id": "684fdd8e1d9b438aa39579cb",
                    "user": {
                        "_id": "64f21f1a73461a8c3abe43a5",
                        "avatarUrl": "/avatars/4b7ff46ae12476f5b6727bd5cc303f38.svg",
                        "isPro": false,
                        "fullname": "Yu-Kuan Fu",
                        "user": "YuKuanFu",
                        "type": "user"
                    },
                    "name": "Yu-Kuan Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T12:58:50.206Z",
                    "hidden": false
                },
                {
                    "_id": "684fdd8e1d9b438aa39579cc",
                    "name": "Kuan Po Huang",
                    "hidden": false
                },
                {
                    "_id": "684fdd8e1d9b438aa39579cd",
                    "user": {
                        "_id": "629b6eb3ad498ab19282ae6f",
                        "avatarUrl": "/avatars/adade8c512122120b06e5c50724a178d.svg",
                        "isPro": false,
                        "fullname": "Hung-yi Lee",
                        "user": "hungyilee",
                        "type": "user"
                    },
                    "name": "Hung-Yi Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-16T12:59:12.434Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:30:32.000Z",
            "submittedOnDailyAt": "2025-06-16T07:32:50.759Z",
            "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data",
            "submittedOnDailyBy": {
                "_id": "6213410828005421265b27d3",
                "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
                "isPro": false,
                "fullname": "",
                "user": "Splend1dchan",
                "type": "user"
            },
            "summary": "We propose a self-refining framework that enhances ASR performance with only\nunlabeled datasets. The process starts with an existing ASR model generating\npseudo-labels on unannotated speech, which are then used to train a\nhigh-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs\nare bootstrapped into the original ASR system, completing the closed-loop\nself-improvement cycle. We demonstrated the effectiveness of the framework on\nTaiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a\nmoderate amount of text data, and synthetic content from the AI models, we\nadapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error\nrates by up to 20% on Mandarin and 50% on Mandarin-English code-switching\nbenchmarks compared to Whisper. Results highlight the framework as a compelling\nalternative to pseudo-labeling self-distillation approaches and provides a\npractical pathway for improving ASR performance in low-resource or\ndomain-specific settings.",
            "upvotes": 1,
            "discussionId": "684fdd8f1d9b438aa39579ce",
            "ai_summary": "A self-refining framework enhances ASR performance using unlabeled datasets by integrating pseudo-labeling, TTS, and synthesized speech to create a specialized model.",
            "ai_keywords": [
                "self-refining framework",
                "ASR",
                "pseudo-labels",
                "TTS",
                "synthesized speech",
                "Whisper-large-v2",
                "Twister",
                "error rates",
                "Mandarin",
                "Mandarin-English code-switching"
            ]
        },
        "publishedAt": "2025-06-10T13:30:32.000Z",
        "title": "A Self-Refining Framework for Enhancing ASR Using TTS-Synthesized Data",
        "summary": "We propose a self-refining framework that enhances ASR performance with only\nunlabeled datasets. The process starts with an existing ASR model generating\npseudo-labels on unannotated speech, which are then used to train a\nhigh-fidelity text-to-speech (TTS) system. Then, synthesized speech text pairs\nare bootstrapped into the original ASR system, completing the closed-loop\nself-improvement cycle. We demonstrated the effectiveness of the framework on\nTaiwanese Mandarin speech. Leveraging 6,000 hours of unlabeled speech, a\nmoderate amount of text data, and synthetic content from the AI models, we\nadapt Whisper-large-v2 into a specialized model, Twister. Twister reduces error\nrates by up to 20% on Mandarin and 50% on Mandarin-English code-switching\nbenchmarks compared to Whisper. Results highlight the framework as a compelling\nalternative to pseudo-labeling self-distillation approaches and provides a\npractical pathway for improving ASR performance in low-resource or\ndomain-specific settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11130.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6213410828005421265b27d3",
            "avatarUrl": "/avatars/930ac20daf640ca31fab713bf00c3268.svg",
            "fullname": "",
            "name": "Splend1dchan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.08915",
            "authors": [
                {
                    "_id": "684fe3711d9b438aa39579da",
                    "user": {
                        "_id": "6508647f0c87331947c4a46d",
                        "avatarUrl": "/avatars/9ddaf4ec53729cb69f65b314a7f4a9a0.svg",
                        "isPro": false,
                        "fullname": "Ananthu Aniraj",
                        "user": "ananthu-aniraj",
                        "type": "user"
                    },
                    "name": "Ananthu Aniraj",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T09:52:38.316Z",
                    "hidden": false
                },
                {
                    "_id": "684fe3711d9b438aa39579db",
                    "user": {
                        "_id": "6642254468b89fe3453439f1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6642254468b89fe3453439f1/9lgQXPykZ5XYiwCQsE8ia.jpeg",
                        "isPro": false,
                        "fullname": "Cassio F. Dantas",
                        "user": "CassioFraga",
                        "type": "user"
                    },
                    "name": "Cassio F. Dantas",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T12:56:17.943Z",
                    "hidden": false
                },
                {
                    "_id": "684fe3711d9b438aa39579dc",
                    "name": "Dino Ienco",
                    "hidden": false
                },
                {
                    "_id": "684fe3711d9b438aa39579dd",
                    "name": "Diego Marcos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T15:41:22.000Z",
            "submittedOnDailyAt": "2025-06-16T08:04:59.344Z",
            "title": "Inherently Faithful Attention Maps for Vision Transformers",
            "submittedOnDailyBy": {
                "_id": "6508647f0c87331947c4a46d",
                "avatarUrl": "/avatars/9ddaf4ec53729cb69f65b314a7f4a9a0.svg",
                "isPro": false,
                "fullname": "Ananthu Aniraj",
                "user": "ananthu-aniraj",
                "type": "user"
            },
            "summary": "We introduce an attention-based method that uses learned binary attention\nmasks to ensure that only attended image regions influence the prediction.\nContext can strongly affect object perception, sometimes leading to biased\nrepresentations, particularly when objects appear in out-of-distribution\nbackgrounds. At the same time, many image-level object-centric tasks require\nidentifying relevant regions, often requiring context. To address this\nconundrum, we propose a two-stage framework: stage 1 processes the full image\nto discover object parts and identify task-relevant regions, while stage 2\nleverages input attention masking to restrict its receptive field to these\nregions, enabling a focused analysis while filtering out potentially spurious\ninformation. Both stages are trained jointly, allowing stage 2 to refine stage\n1. Extensive experiments across diverse benchmarks demonstrate that our\napproach significantly improves robustness against spurious correlations and\nout-of-distribution backgrounds.",
            "upvotes": 1,
            "discussionId": "684fe3711d9b438aa39579de",
            "githubRepo": "https://github.com/ananthu-aniraj/ifam",
            "ai_summary": "An attention-based method using learned binary masks improves robustness in object perception by focusing on relevant image regions while filtering out spurious information.",
            "ai_keywords": [
                "attention-based method",
                "learned binary attention masks",
                "object perception",
                "context",
                "out-of-distribution backgrounds",
                "image-level object-centric tasks",
                "task-relevant regions",
                "two-stage framework",
                "receptive field",
                "joint training",
                "robustness",
                "spurious correlations"
            ]
        },
        "publishedAt": "2025-06-10T11:41:22.000Z",
        "title": "Inherently Faithful Attention Maps for Vision Transformers",
        "summary": "We introduce an attention-based method that uses learned binary attention\nmasks to ensure that only attended image regions influence the prediction.\nContext can strongly affect object perception, sometimes leading to biased\nrepresentations, particularly when objects appear in out-of-distribution\nbackgrounds. At the same time, many image-level object-centric tasks require\nidentifying relevant regions, often requiring context. To address this\nconundrum, we propose a two-stage framework: stage 1 processes the full image\nto discover object parts and identify task-relevant regions, while stage 2\nleverages input attention masking to restrict its receptive field to these\nregions, enabling a focused analysis while filtering out potentially spurious\ninformation. Both stages are trained jointly, allowing stage 2 to refine stage\n1. Extensive experiments across diverse benchmarks demonstrate that our\napproach significantly improves robustness against spurious correlations and\nout-of-distribution backgrounds.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08915.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6508647f0c87331947c4a46d",
            "avatarUrl": "/avatars/9ddaf4ec53729cb69f65b314a7f4a9a0.svg",
            "fullname": "Ananthu Aniraj",
            "name": "ananthu-aniraj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.11116",
            "authors": [
                {
                    "_id": "685021682932e11c891b582e",
                    "name": "Jijie Li",
                    "hidden": false
                },
                {
                    "_id": "685021682932e11c891b582f",
                    "name": "Li Du",
                    "hidden": false
                },
                {
                    "_id": "685021682932e11c891b5830",
                    "name": "Hanyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "685021682932e11c891b5831",
                    "name": "Bo-wen Zhang",
                    "hidden": false
                },
                {
                    "_id": "685021682932e11c891b5832",
                    "user": {
                        "_id": "63a11ce02fabbbb899a01d58",
                        "avatarUrl": "/avatars/ee3d4088b6d32b2c18b8be91913e90dd.svg",
                        "isPro": false,
                        "fullname": "ldwang",
                        "user": "ldwang",
                        "type": "user"
                    },
                    "name": "Liangdong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T15:50:45.345Z",
                    "hidden": false
                },
                {
                    "_id": "685021682932e11c891b5833",
                    "name": "Boyan Gao",
                    "hidden": false
                },
                {
                    "_id": "685021682932e11c891b5834",
                    "user": {
                        "_id": "632c234f42c386ebd2710434",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
                        "isPro": false,
                        "fullname": "Guang Liu",
                        "user": "ZacLiu",
                        "type": "user"
                    },
                    "name": "Guang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T13:58:12.267Z",
                    "hidden": false
                },
                {
                    "_id": "685021682932e11c891b5835",
                    "name": "Yonghua Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T06:37:15.000Z",
            "submittedOnDailyAt": "2025-06-16T12:24:56.668Z",
            "title": "Infinity Instruct: Scaling Instruction Selection and Synthesis to\n  Enhance Language Models",
            "submittedOnDailyBy": {
                "_id": "632c234f42c386ebd2710434",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
                "isPro": false,
                "fullname": "Guang Liu",
                "user": "ZacLiu",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) demonstrate strong performance in real-world\napplications, yet existing open-source instruction datasets often concentrate\non narrow domains, such as mathematics or coding, limiting generalization and\nwidening the gap with proprietary models. To bridge this gap, we introduce\nInfinity-Instruct, a high-quality instruction dataset designed to enhance both\nfoundational and chat capabilities of LLMs through a two-phase pipeline. In\nPhase 1, we curate 7.4M high-quality foundational instructions\n(InfInstruct-F-7.4M) from over 100M samples using hybrid data selection\ntechniques. In Phase 2, we synthesize 1.5M high-quality chat instructions\n(InfInstruct-G-1.5M) through a two-stage process involving instruction\nselection, evolution, and diagnostic filtering. We empirically evaluate\nInfinity-Instruct by fine-tuning several open-source models, including Mistral,\nLLaMA, Qwen, and Yi, and observe substantial performance gains across both\nfoundational and instruction following benchmarks, consistently surpassing\nofficial instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B\noutperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving\ncomparable foundational performance. These results underscore the synergy\nbetween foundational and chat training and offer new insights into holistic LLM\ndevelopment. Our\ndatasethttps://huggingface.co/datasets/BAAI/Infinity-Instruct and\ncodeshttps://gitee.com/li-touch/infinity-instruct have been publicly\nreleased.",
            "upvotes": 1,
            "discussionId": "685021682932e11c891b5836",
            "ai_summary": "Infinity-Instruct, a comprehensive instruction dataset, enhances both foundational and chat capabilities of large language models through curation and synthesis, achieving superior performance compared to existing datasets.",
            "ai_keywords": [
                "large language models",
                "instruction datasets",
                "foundational instructions",
                "chat instructions",
                "hybrid data selection",
                "instruction selection",
                "evolution",
                "diagnostic filtering",
                "fine-tuning",
                "instruction following benchmarks"
            ]
        },
        "publishedAt": "2025-06-09T02:37:15.000Z",
        "title": "Infinity Instruct: Scaling Instruction Selection and Synthesis to\n  Enhance Language Models",
        "summary": "Large Language Models (LLMs) demonstrate strong performance in real-world\napplications, yet existing open-source instruction datasets often concentrate\non narrow domains, such as mathematics or coding, limiting generalization and\nwidening the gap with proprietary models. To bridge this gap, we introduce\nInfinity-Instruct, a high-quality instruction dataset designed to enhance both\nfoundational and chat capabilities of LLMs through a two-phase pipeline. In\nPhase 1, we curate 7.4M high-quality foundational instructions\n(InfInstruct-F-7.4M) from over 100M samples using hybrid data selection\ntechniques. In Phase 2, we synthesize 1.5M high-quality chat instructions\n(InfInstruct-G-1.5M) through a two-stage process involving instruction\nselection, evolution, and diagnostic filtering. We empirically evaluate\nInfinity-Instruct by fine-tuning several open-source models, including Mistral,\nLLaMA, Qwen, and Yi, and observe substantial performance gains across both\nfoundational and instruction following benchmarks, consistently surpassing\nofficial instruction-tuned counterparts. Notably, InfInstruct-LLaMA3.1-70B\noutperforms GPT-4-0314 by 8.6\\% on instruction following tasks while achieving\ncomparable foundational performance. These results underscore the synergy\nbetween foundational and chat training and offer new insights into holistic LLM\ndevelopment. Our\ndatasethttps://huggingface.co/datasets/BAAI/Infinity-Instruct and\ncodeshttps://gitee.com/li-touch/infinity-instruct have been publicly\nreleased.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11116.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "632c234f42c386ebd2710434",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/632c234f42c386ebd2710434/HyWRWi063S69JTy_IMjoe.jpeg",
            "fullname": "Guang Liu",
            "name": "ZacLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.03857",
            "authors": [
                {
                    "_id": "684ff7e31d9b438aa3957a9b",
                    "user": {
                        "_id": "65dc040952eca001fd0bb142",
                        "avatarUrl": "/avatars/cd8e54ceef7c9e4a3bb4b0900c47a8b6.svg",
                        "isPro": false,
                        "fullname": "Mingxuan Xia",
                        "user": "MingxuanXia",
                        "type": "user"
                    },
                    "name": "Mingxuan Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T12:56:05.905Z",
                    "hidden": false
                },
                {
                    "_id": "684ff7e31d9b438aa3957a9c",
                    "name": "Haobo Wang",
                    "hidden": false
                },
                {
                    "_id": "684ff7e31d9b438aa3957a9d",
                    "name": "Yixuan Li",
                    "hidden": false
                },
                {
                    "_id": "684ff7e31d9b438aa3957a9e",
                    "name": "Zewei Yu",
                    "hidden": false
                },
                {
                    "_id": "684ff7e31d9b438aa3957a9f",
                    "name": "Jindong Wang",
                    "hidden": false
                },
                {
                    "_id": "684ff7e31d9b438aa3957aa0",
                    "name": "Junbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "684ff7e31d9b438aa3957aa1",
                    "name": "Runze Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T11:42:37.000Z",
            "submittedOnDailyAt": "2025-06-16T12:47:07.333Z",
            "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for\n  LLM-driven Data Annotation",
            "submittedOnDailyBy": {
                "_id": "65dc040952eca001fd0bb142",
                "avatarUrl": "/avatars/cd8e54ceef7c9e4a3bb4b0900c47a8b6.svg",
                "isPro": false,
                "fullname": "Mingxuan Xia",
                "user": "MingxuanXia",
                "type": "user"
            },
            "summary": "Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist.",
            "upvotes": 1,
            "discussionId": "684ff7e41d9b438aa3957aa2",
            "githubRepo": "https://github.com/MingxuanXia/CanDist",
            "ai_summary": "A novel candidate annotation paradigm using a teacher-student framework improves data quality for applications by encouraging large language models to output multiple labels when uncertain.",
            "ai_keywords": [
                "Large Language Models",
                "candidate annotation paradigm",
                "teacher-student framework",
                "small language model",
                "text classification"
            ]
        },
        "publishedAt": "2025-06-04T07:42:37.000Z",
        "title": "Prompt Candidates, then Distill: A Teacher-Student Framework for\n  LLM-driven Data Annotation",
        "summary": "Recently, Large Language Models (LLMs) have demonstrated significant\npotential for data annotation, markedly reducing the labor costs associated\nwith downstream applications. However, existing methods mostly adopt an\naggressive strategy by prompting LLM to determine a single gold label for each\nunlabeled sample. Due to the inherent uncertainty within LLMs, they often\nproduce incorrect labels for difficult samples, severely compromising the data\nquality for downstream applications. Motivated by ambiguity aversion in human\nbehaviors, we propose a novel candidate annotation paradigm wherein large\nlanguage models are encouraged to output all possible labels when incurring\nuncertainty. To ensure unique labels are provided for downstream tasks, we\ndevelop a teacher-student framework CanDist that distills candidate annotations\nwith a Small Language Model (SLM). We further provide a rigorous justification\ndemonstrating that distilling candidate annotations from the teacher LLM offers\nsuperior theoretical guarantees compared to directly using single annotations.\nExtensive experiments across six text classification tasks validate the\neffectiveness of our proposed method. The source code is available at\nhttps://github.com/MingxuanXia/CanDist.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03857.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65dc040952eca001fd0bb142",
            "avatarUrl": "/avatars/cd8e54ceef7c9e4a3bb4b0900c47a8b6.svg",
            "fullname": "Mingxuan Xia",
            "name": "MingxuanXia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]