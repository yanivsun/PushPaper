[
    {
        "paper": {
            "id": "2506.13585",
            "authors": [
                {
                    "_id": "6850d0105e07650ecce89009",
                    "name": "MiniMax",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8900b",
                    "user": {
                        "_id": "63f86b099f87cc3e645b51d9",
                        "avatarUrl": "/avatars/27ca5ba425640bf67474cee871e8e53a.svg",
                        "isPro": false,
                        "fullname": "Ellie Chen",
                        "user": "sheep33333",
                        "type": "user"
                    },
                    "name": "Aili Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:27.223Z",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8900c",
                    "name": "Aonian Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8900d",
                    "name": "Bangwei Gong",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8900e",
                    "name": "Binyang Jiang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8900f",
                    "name": "Bo Fei",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89010",
                    "name": "Bo Yang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89011",
                    "name": "Boji Shan",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89012",
                    "name": "Changqing Yu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89013",
                    "name": "Chao Wang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89014",
                    "name": "Cheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89015",
                    "name": "Chengjun Xiao",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89016",
                    "name": "Chengyu Du",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89017",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89018",
                    "name": "Chu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89019",
                    "user": {
                        "_id": "642662fa22bddcea3d289f0a",
                        "avatarUrl": "/avatars/9b28e1325d866a24d33fdfafcaa85c4b.svg",
                        "isPro": false,
                        "fullname": "Enoch Zhang",
                        "user": "enochzhang",
                        "type": "user"
                    },
                    "name": "Chunhao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:43.093Z",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8901a",
                    "name": "Chunhui Du",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8901b",
                    "name": "Congchao Guo",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8901c",
                    "name": "Da Chen",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8901d",
                    "name": "Deming Ding",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8901e",
                    "name": "Dianjun Sun",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8901f",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89020",
                    "name": "Enwei Jiao",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89021",
                    "name": "Haigang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89022",
                    "name": "Haimo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89023",
                    "name": "Han Ding",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89024",
                    "name": "Haohai Sun",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89025",
                    "name": "Haoyu Feng",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89026",
                    "name": "Huaiguang Cai",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89027",
                    "name": "Haichao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89028",
                    "name": "Jian Sun",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89029",
                    "name": "Jiaqi Zhuang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8902a",
                    "name": "Jiaren Cai",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8902b",
                    "name": "Jiayuan Song",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8902c",
                    "user": {
                        "_id": "665d5c955491b1e10dfc4097",
                        "avatarUrl": "/avatars/1d577113e407068d29e61f670e662f81.svg",
                        "isPro": false,
                        "fullname": "Jin Zhu",
                        "user": "GinZhu",
                        "type": "user"
                    },
                    "name": "Jin Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T13:06:28.403Z",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8902d",
                    "name": "Jingyang Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8902e",
                    "name": "Jinhao Tian",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8902f",
                    "name": "Jinli Liu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89030",
                    "name": "Junhao Xu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89031",
                    "name": "Junjie Yan",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89032",
                    "name": "Junteng Liu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89033",
                    "name": "Junxian He",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89034",
                    "name": "Kaiyi Feng",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89035",
                    "name": "Ke Yang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89036",
                    "name": "Kecheng Xiao",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89037",
                    "name": "Le Han",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89038",
                    "name": "Leyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89039",
                    "name": "Lianfei Yu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8903a",
                    "name": "Liheng Feng",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8903b",
                    "name": "Lin Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8903c",
                    "name": "Lin Zheng",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8903d",
                    "name": "Linge Du",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8903e",
                    "name": "Lingyu Yang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8903f",
                    "name": "Lunbin Zeng",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89040",
                    "name": "Minghui Yu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89041",
                    "name": "Mingliang Tao",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89042",
                    "name": "Mingyuan Chi",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89043",
                    "name": "Mozhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89044",
                    "user": {
                        "_id": "67ac4d69a122ac29aed98f3c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/5NBmaUAoutU4RA85qH8mw.png",
                        "isPro": false,
                        "fullname": "LINMUJIE",
                        "user": "LINMUJIE-judy",
                        "type": "user"
                    },
                    "name": "Mujie Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:37.448Z",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89045",
                    "name": "Nan Hu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89046",
                    "name": "Nongyu Di",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89047",
                    "name": "Peng Gao",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89048",
                    "name": "Pengfei Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89049",
                    "name": "Pengyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8904a",
                    "name": "Qibing Ren",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8904b",
                    "name": "Qidi Xu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8904c",
                    "name": "Qile Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8904d",
                    "name": "Qin Wang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8904e",
                    "name": "Rong Tian",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8904f",
                    "name": "Ruitao Leng",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89050",
                    "name": "Shaoxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89051",
                    "name": "Shaoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89052",
                    "name": "Shengmin Shi",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89053",
                    "name": "Shitong Weng",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89054",
                    "name": "Shuchang Guan",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89055",
                    "name": "Shuqi Yu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89056",
                    "name": "Sichen Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89057",
                    "name": "Songquan Zhu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89058",
                    "name": "Tengfei Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89059",
                    "name": "Tianchi Cai",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8905a",
                    "name": "Tianrun Liang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8905b",
                    "name": "Weiyu Cheng",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8905c",
                    "name": "Weize Kong",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8905d",
                    "name": "Wenkai Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8905e",
                    "name": "Xiancai Chen",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8905f",
                    "name": "Xiangjun Song",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89060",
                    "user": {
                        "_id": "612741a391de43c1101df014",
                        "avatarUrl": "/avatars/1461b1c7d3cedd91cea6cf3b0ecb14ae.svg",
                        "isPro": false,
                        "fullname": "Rock Luo",
                        "user": "windlx",
                        "type": "user"
                    },
                    "name": "Xiao Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:57.356Z",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89061",
                    "name": "Xiao Su",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89062",
                    "user": {
                        "_id": "63dc802e270a8bb46ad31783",
                        "avatarUrl": "/avatars/c0584f86663e64d64d276a32fde9be49.svg",
                        "isPro": false,
                        "fullname": "晓波",
                        "user": "lixiabo12",
                        "type": "user"
                    },
                    "name": "Xiaobo Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T13:06:42.746Z",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89063",
                    "name": "Xiaodong Han",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89064",
                    "name": "Xinzhu Hou",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89065",
                    "name": "Xuan Lu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89066",
                    "name": "Xun Zou",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89067",
                    "name": "Xuyang Shen",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89068",
                    "name": "Yan Gong",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89069",
                    "user": {
                        "_id": "633fc70529b5a95f6e15a6b7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633fc70529b5a95f6e15a6b7/Fzh7wWuqU-fBbzdupOUtF.jpeg",
                        "isPro": false,
                        "fullname": "Yan Ma",
                        "user": "ManTle",
                        "type": "user"
                    },
                    "name": "Yan Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:39.279Z",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8906a",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8906b",
                    "name": "Yiqi Shi",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8906c",
                    "name": "Yiran Zhong",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8906d",
                    "name": "Yonghong Duan",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8906e",
                    "name": "Yongxiang Fu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8906f",
                    "name": "Yongyi Hu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89070",
                    "name": "Yu Gao",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89071",
                    "name": "Yuanxiang Fan",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89072",
                    "name": "Yufeng Yang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89073",
                    "name": "Yuhao Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89074",
                    "name": "Yulin Hu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89075",
                    "name": "Yunan Huang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89076",
                    "name": "Yunji Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89077",
                    "name": "Yunzhi Xu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89078",
                    "name": "Yuxin Mao",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89079",
                    "name": "Yuxuan Shi",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8907a",
                    "name": "Yuze Wenren",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8907b",
                    "name": "Zehan Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8907c",
                    "name": "Zelin Li",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8907d",
                    "name": "Zhanxu Tian",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8907e",
                    "name": "Zhengmao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce8907f",
                    "name": "Zhenhua Fan",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89080",
                    "name": "Zhenzhen Wu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89081",
                    "name": "Zhichao Xu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89082",
                    "name": "Zhihang Yu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89083",
                    "name": "Zhiheng Lyu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89084",
                    "name": "Zhuo Jiang",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89085",
                    "user": {
                        "_id": "6690a4b6a4d0df7e51b93392",
                        "avatarUrl": "/avatars/6d3694c39344854221f6ca0ed3cf0557.svg",
                        "isPro": false,
                        "fullname": "gao zibo",
                        "user": "afhhl",
                        "type": "user"
                    },
                    "name": "Zibo Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:41.298Z",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89086",
                    "name": "Zijia Wu",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89087",
                    "name": "Zijian Song",
                    "hidden": false
                },
                {
                    "_id": "6850d0105e07650ecce89088",
                    "name": "Zijun Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T15:08:02.000Z",
            "submittedOnDailyAt": "2025-06-17T00:48:14.831Z",
            "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention",
            "submittedOnDailyBy": {
                "_id": "676e38ad04af5bec20bc9faf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/676e38ad04af5bec20bc9faf/AG8Q9wAUzGtPWyjd5QO2l.jpeg",
                "isPro": false,
                "fullname": "MiniMax",
                "user": "MiniMax-AI",
                "type": "user"
            },
            "summary": "We introduce MiniMax-M1, the world's first open-weight, large-scale\nhybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid\nMixture-of-Experts (MoE) architecture combined with a lightning attention\nmechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters\nactivated per token. The M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning\nattention mechanism in MiniMax-M1 enables efficient scaling of test-time\ncompute. These properties make M1 particularly suitable for complex tasks that\nrequire processing long inputs and thinking extensively. MiniMax-M1 is trained\nusing large-scale reinforcement learning (RL) on diverse problems including\nsandbox-based, real-world software engineering environments. In addition to\nM1's inherent efficiency advantage for RL training, we propose CISPO, a novel\nRL algorithm to further enhance RL efficiency. CISPO clips importance sampling\nweights rather than token updates, outperforming other competitive RL variants.\nCombining hybrid-attention and CISPO enables MiniMax-M1's full RL training on\n512 H800 GPUs to complete in only three weeks, with a rental cost of just\n$534,700. We release two versions of MiniMax-M1 models with 40K and 80K\nthinking budgets respectively, where the 40K model represents an intermediate\nphase of the 80K training. Experiments on standard benchmarks show that our\nmodels are comparable or superior to strong open-weight models such as the\noriginal DeepSeek-R1 and Qwen3-235B, with particular strengths in complex\nsoftware engineering, tool utilization, and long-context tasks. We publicly\nrelease MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.",
            "upvotes": 189,
            "discussionId": "6850d0105e07650ecce89089",
            "projectPage": "https://huggingface.co/MiniMaxAI/MiniMax-M1-80k",
            "githubRepo": "https://github.com/MiniMax-AI/MiniMax-M1",
            "ai_summary": "A hybrid-attention reasoning model called MiniMax-M1, featuring a Mixture-of-Experts architecture and lightning attention mechanism, is introduced for efficient long-input processing and reinforcement learning.",
            "ai_keywords": [
                "Mixture-of-Experts (MoE)",
                "lightning attention mechanism",
                "reinforcement learning (RL)",
                "CISPO",
                "importance sampling weights",
                "token updates"
            ]
        },
        "publishedAt": "2025-06-16T11:08:02.000Z",
        "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning\n  Attention",
        "summary": "We introduce MiniMax-M1, the world's first open-weight, large-scale\nhybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid\nMixture-of-Experts (MoE) architecture combined with a lightning attention\nmechanism. The model is developed based on our previous MiniMax-Text-01 model,\nwhich contains a total of 456 billion parameters with 45.9 billion parameters\nactivated per token. The M1 model natively supports a context length of 1\nmillion tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning\nattention mechanism in MiniMax-M1 enables efficient scaling of test-time\ncompute. These properties make M1 particularly suitable for complex tasks that\nrequire processing long inputs and thinking extensively. MiniMax-M1 is trained\nusing large-scale reinforcement learning (RL) on diverse problems including\nsandbox-based, real-world software engineering environments. In addition to\nM1's inherent efficiency advantage for RL training, we propose CISPO, a novel\nRL algorithm to further enhance RL efficiency. CISPO clips importance sampling\nweights rather than token updates, outperforming other competitive RL variants.\nCombining hybrid-attention and CISPO enables MiniMax-M1's full RL training on\n512 H800 GPUs to complete in only three weeks, with a rental cost of just\n$534,700. We release two versions of MiniMax-M1 models with 40K and 80K\nthinking budgets respectively, where the 40K model represents an intermediate\nphase of the 80K training. Experiments on standard benchmarks show that our\nmodels are comparable or superior to strong open-weight models such as the\noriginal DeepSeek-R1 and Qwen3-235B, with particular strengths in complex\nsoftware engineering, tool utilization, and long-context tasks. We publicly\nrelease MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13585.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "676e38ad04af5bec20bc9faf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/676e38ad04af5bec20bc9faf/AG8Q9wAUzGtPWyjd5QO2l.jpeg",
            "fullname": "MiniMax",
            "name": "MiniMax-AI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 162
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10521",
            "authors": [
                {
                    "_id": "684b8c603b733ba333686ffe",
                    "name": "Yuhao Zhou",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333686fff",
                    "name": "Yiheng Wang",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687000",
                    "name": "Xuming He",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687001",
                    "name": "Ruoyao Xiao",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687002",
                    "name": "Zhiwei Li",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687003",
                    "name": "Qiantai Feng",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687004",
                    "name": "Zijie Guo",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687005",
                    "name": "Yuejin Yang",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687006",
                    "name": "Hao Wu",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687007",
                    "user": {
                        "_id": "675118b088a927f8898f81b4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/qVVgc2kuR37QqO5-Lu8Xq.png",
                        "isPro": false,
                        "fullname": "Wilson Huang",
                        "user": "WilsonHwang",
                        "type": "user"
                    },
                    "name": "Wenxuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:22:33.270Z",
                    "hidden": true
                },
                {
                    "_id": "684b8c603b733ba333687008",
                    "name": "Jiaqi Wei",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687009",
                    "name": "Dan Si",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba33368700a",
                    "name": "Xiuqi Yao",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba33368700b",
                    "name": "Jia Bu",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba33368700c",
                    "name": "Haiwen Huang",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba33368700d",
                    "name": "Tianfan Fu",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba33368700e",
                    "name": "Shixiang Tang",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba33368700f",
                    "name": "Ben Fei",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687010",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687011",
                    "name": "Fenghua Ling",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687012",
                    "name": "Yan Lu",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687013",
                    "user": {
                        "_id": "67d3a1a5943a965360fcae51",
                        "avatarUrl": "/avatars/165ed684b0750e7f57b9f2babfb47a8c.svg",
                        "isPro": false,
                        "fullname": "Siqi Sun",
                        "user": "siqisun",
                        "type": "user"
                    },
                    "name": "Siqi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T13:06:51.338Z",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687014",
                    "name": "Chenhui Li",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687015",
                    "name": "Guanjie Zheng",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687016",
                    "name": "Jiancheng Lv",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687017",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "684b8c603b733ba333687018",
                    "name": "Lei Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T09:29:16.000Z",
            "submittedOnDailyAt": "2025-06-17T02:12:14.955Z",
            "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning",
            "submittedOnDailyBy": {
                "_id": "6538b861613fe158bd581e35",
                "avatarUrl": "/avatars/6817dbfe903675721fd227058b0a91ac.svg",
                "isPro": false,
                "fullname": "Dongzhan Zhou",
                "user": "schrodingers-tiger",
                "type": "user"
            },
            "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries.",
            "upvotes": 61,
            "discussionId": "684b8c603b733ba333687019",
            "ai_summary": "Scientists' First Exam (SFE) benchmark assesses scientific cognitive capacities of Multimodal Large Language Models through perception, understanding, and comparative reasoning.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "SFE benchmark",
                "scientific signal perception",
                "scientific attribute understanding",
                "scientific comparative reasoning",
                "expert-verified VQA",
                "GPT-o3",
                "InternVL-3"
            ]
        },
        "publishedAt": "2025-06-12T05:29:16.000Z",
        "title": "Scientists' First Exam: Probing Cognitive Abilities of MLLM via\n  Perception, Understanding, and Reasoning",
        "summary": "Scientific discoveries increasingly rely on complex multimodal reasoning\nbased on information-intensive scientific data and domain-specific expertise.\nEmpowered by expert-level scientific benchmarks, scientific Multimodal Large\nLanguage Models (MLLMs) hold the potential to significantly enhance this\ndiscovery process in realistic workflows. However, current scientific\nbenchmarks mostly focus on evaluating the knowledge understanding capabilities\nof MLLMs, leading to an inadequate assessment of their perception and reasoning\nabilities. To address this gap, we present the Scientists' First Exam (SFE)\nbenchmark, designed to evaluate the scientific cognitive capacities of MLLMs\nthrough three interconnected levels: scientific signal perception, scientific\nattribute understanding, scientific comparative reasoning. Specifically, SFE\ncomprises 830 expert-verified VQA pairs across three question types, spanning\n66 multimodal tasks across five high-value disciplines. Extensive experiments\nreveal that current state-of-the-art GPT-o3 and InternVL-3 achieve only 34.08%\nand 26.52% on SFE, highlighting significant room for MLLMs to improve in\nscientific realms. We hope the insights obtained in SFE will facilitate further\ndevelopments in AI-enhanced scientific discoveries.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10521.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6538b861613fe158bd581e35",
            "avatarUrl": "/avatars/6817dbfe903675721fd227058b0a91ac.svg",
            "fullname": "Dongzhan Zhou",
            "name": "schrodingers-tiger",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.11763",
            "authors": [
                {
                    "_id": "684ff5051d9b438aa3957a7f",
                    "user": {
                        "_id": "646dbba74ad7f907279dd486",
                        "avatarUrl": "/avatars/fe2b95e9a55711164e9624e1d15e0af2.svg",
                        "isPro": false,
                        "fullname": "Mingxuan Du",
                        "user": "Ayanami0730",
                        "type": "user"
                    },
                    "name": "Mingxuan Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T12:56:12.158Z",
                    "hidden": false
                },
                {
                    "_id": "684ff5051d9b438aa3957a80",
                    "name": "Benfeng Xu",
                    "hidden": false
                },
                {
                    "_id": "684ff5051d9b438aa3957a81",
                    "user": {
                        "_id": "663b22a80966eef8686aadaf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663b22a80966eef8686aadaf/iBzyQTyGZKf33RPVIFh9a.jpeg",
                        "isPro": false,
                        "fullname": "Chiwei Zhu",
                        "user": "IgnoraZ",
                        "type": "user"
                    },
                    "name": "Chiwei Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T12:56:09.702Z",
                    "hidden": false
                },
                {
                    "_id": "684ff5051d9b438aa3957a82",
                    "name": "Xiaorui Wang",
                    "hidden": false
                },
                {
                    "_id": "684ff5051d9b438aa3957a83",
                    "name": "Zhendong Mao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T13:17:32.000Z",
            "submittedOnDailyAt": "2025-06-17T00:31:26.473Z",
            "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
            "submittedOnDailyBy": {
                "_id": "646dbba74ad7f907279dd486",
                "avatarUrl": "/avatars/fe2b95e9a55711164e9624e1d15e0af2.svg",
                "isPro": false,
                "fullname": "Mingxuan Du",
                "user": "Ayanami0730",
                "type": "user"
            },
            "summary": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.",
            "upvotes": 44,
            "discussionId": "684ff5051d9b438aa3957a84",
            "projectPage": "https://deepresearch-bench.github.io",
            "githubRepo": "https://github.com/Ayanami0730/deep_research_bench",
            "ai_summary": "DeepResearch Bench offers a benchmark framework to evaluate the capabilities of Deep Research Agents in terms of research quality and information retrieval accuracy across multiple fields.",
            "ai_keywords": [
                "Deep Research Agents",
                "LLM-based agents",
                "multistep web exploration",
                "targeted retrieval",
                "higher-order synthesis",
                "PhD-level research tasks",
                "reference-based method",
                "effective citation count",
                "citation accuracy"
            ]
        },
        "publishedAt": "2025-06-13T09:17:32.000Z",
        "title": "DeepResearch Bench: A Comprehensive Benchmark for Deep Research Agents",
        "summary": "Deep Research Agents are a prominent category of LLM-based agents. By\nautonomously orchestrating multistep web exploration, targeted retrieval, and\nhigher-order synthesis, they transform vast amounts of online information into\nanalyst-grade, citation-rich reports--compressing hours of manual desk research\ninto minutes. However, a comprehensive benchmark for systematically evaluating\nthe capabilities of these agents remains absent. To bridge this gap, we present\nDeepResearch Bench, a benchmark consisting of 100 PhD-level research tasks,\neach meticulously crafted by domain experts across 22 distinct fields.\nEvaluating DRAs is inherently complex and labor-intensive. We therefore propose\ntwo novel methodologies that achieve strong alignment with human judgment. The\nfirst is a reference-based method with adaptive criteria to assess the quality\nof generated research reports. The other framework is introduced to evaluate\nDRA's information retrieval and collection capabilities by assessing its\neffective citation count and overall citation accuracy. We have open-sourced\nDeepResearch Bench and key components of these frameworks at\nhttps://github.com/Ayanami0730/deep_research_bench to accelerate the\ndevelopment of practical LLM-based agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11763.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "646dbba74ad7f907279dd486",
            "avatarUrl": "/avatars/fe2b95e9a55711164e9624e1d15e0af2.svg",
            "fullname": "Mingxuan Du",
            "name": "Ayanami0730",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.12571",
            "authors": [
                {
                    "_id": "6850cba15e07650ecce88fce",
                    "user": {
                        "_id": "62243664af5df9d9e5582f67",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62243664af5df9d9e5582f67/nAntUd0NVDcMYtwiunCU8.jpeg",
                        "isPro": false,
                        "fullname": "Saksorn Ruangtanusak",
                        "user": "saksornr",
                        "type": "user"
                    },
                    "name": "Saksorn Ruangtanusak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:59.261Z",
                    "hidden": false
                },
                {
                    "_id": "6850cba15e07650ecce88fcf",
                    "name": "Natthapath Rungseesiripak",
                    "hidden": false
                },
                {
                    "_id": "6850cba15e07650ecce88fd0",
                    "user": {
                        "_id": "647d9e689bb822b5cd3cc752",
                        "avatarUrl": "/avatars/c97ae9fce3ff06f9ba48feb0ea95296e.svg",
                        "isPro": false,
                        "fullname": "Peerawat",
                        "user": "boat1603",
                        "type": "user"
                    },
                    "name": "Peerawat Rojratchadakorn",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T15:13:12.119Z",
                    "hidden": false
                },
                {
                    "_id": "6850cba15e07650ecce88fd1",
                    "user": {
                        "_id": "66c5a51e82dec44cc50bc23f",
                        "avatarUrl": "/avatars/afcdb138fbe40f55283b6fd7912d7097.svg",
                        "isPro": false,
                        "fullname": "Monthol Charattrakool",
                        "user": "montholscbx",
                        "type": "user"
                    },
                    "name": "Monthol Charattrakool",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-17T01:57:53.939Z",
                    "hidden": false
                },
                {
                    "_id": "6850cba15e07650ecce88fd2",
                    "user": {
                        "_id": "64705d3890482b0e0f6591ed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64705d3890482b0e0f6591ed/HqOaaRjzkXrC8POGtZYwh.jpeg",
                        "isPro": false,
                        "fullname": "Natapong Nitarach (Schwyter)",
                        "user": "natnitaract",
                        "type": "user"
                    },
                    "name": "Natapong Nitarach",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-17T01:57:53.939Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/CEw32qHWVcm9CaXt5778s.png",
                "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/0uJ9QrBOx5WgVxj2UC27j.png"
            ],
            "publishedAt": "2025-06-14T16:56:00.000Z",
            "submittedOnDailyAt": "2025-06-17T08:31:40.191Z",
            "title": "DoTA-RAG: Dynamic of Thought Aggregation RAG",
            "submittedOnDailyBy": {
                "_id": "62243664af5df9d9e5582f67",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62243664af5df9d9e5582f67/nAntUd0NVDcMYtwiunCU8.jpeg",
                "isPro": false,
                "fullname": "Saksorn Ruangtanusak",
                "user": "saksornr",
                "type": "user"
            },
            "summary": "In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a\nretrieval-augmented generation system optimized for high-throughput,\nlarge-scale web knowledge indexes. Traditional RAG pipelines often suffer from\nhigh latency and limited accuracy over massive, diverse datasets. DoTA-RAG\naddresses these challenges with a three-stage pipeline: query rewriting,\ndynamic routing to specialized sub-indexes, and multi-stage retrieval and\nranking. We further enhance retrieval by evaluating and selecting a superior\nembedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we\ncreate a diverse Q&A dataset of 500 questions generated via the DataMorgana\nsetup across a broad range of WebOrganizer topics and formats. DoTA-RAG\nimproves the answer correctness score from 0.752 (baseline, using LiveRAG\npre-built vector store) to 1.478 while maintaining low latency, and it achieves\na 0.929 correctness score on the Live Challenge Day. These results highlight\nDoTA-RAG's potential for practical deployment in domains requiring fast,\nreliable access to large and evolving knowledge sources.",
            "upvotes": 35,
            "discussionId": "6850cba15e07650ecce88fd3",
            "githubRepo": "https://github.com/DoTA-RAG/dota-rag",
            "ai_summary": "DoTA-RAG improves retrieval and generation accuracy over massive web datasets using a dynamic routing pipeline and optimized embedding models, achieving high correctness scores while maintaining low latency.",
            "ai_keywords": [
                "RAG",
                "DoTA-RAG",
                "query rewriting",
                "dynamic routing",
                "specialized sub-indexes",
                "multi-stage retrieval",
                "ranking",
                "embedding models",
                "re-embedding",
                "FineWeb-10BT",
                "Q&A dataset",
                "DataMorgana",
                "LiveRAG",
                "Live Challenge Day"
            ]
        },
        "publishedAt": "2025-06-14T12:56:00.000Z",
        "title": "DoTA-RAG: Dynamic of Thought Aggregation RAG",
        "summary": "In this paper, we introduce DoTA-RAG (Dynamic-of-Thought Aggregation RAG), a\nretrieval-augmented generation system optimized for high-throughput,\nlarge-scale web knowledge indexes. Traditional RAG pipelines often suffer from\nhigh latency and limited accuracy over massive, diverse datasets. DoTA-RAG\naddresses these challenges with a three-stage pipeline: query rewriting,\ndynamic routing to specialized sub-indexes, and multi-stage retrieval and\nranking. We further enhance retrieval by evaluating and selecting a superior\nembedding model, re-embedding the large FineWeb-10BT corpus. Moreover, we\ncreate a diverse Q&A dataset of 500 questions generated via the DataMorgana\nsetup across a broad range of WebOrganizer topics and formats. DoTA-RAG\nimproves the answer correctness score from 0.752 (baseline, using LiveRAG\npre-built vector store) to 1.478 while maintaining low latency, and it achieves\na 0.929 correctness score on the Live Challenge Day. These results highlight\nDoTA-RAG's potential for practical deployment in domains requiring fast,\nreliable access to large and evolving knowledge sources.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/CEw32qHWVcm9CaXt5778s.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62243664af5df9d9e5582f67/0uJ9QrBOx5WgVxj2UC27j.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12571.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62243664af5df9d9e5582f67",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62243664af5df9d9e5582f67/nAntUd0NVDcMYtwiunCU8.jpeg",
            "fullname": "Saksorn Ruangtanusak",
            "name": "saksornr",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13654",
            "authors": [
                {
                    "_id": "6850e2a05e07650ecce89106",
                    "user": {
                        "_id": "6658d01c6f1a71ba56d6c273",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tc4nZrMuZQLfgt5aVxtH4.jpeg",
                        "isPro": false,
                        "fullname": "Tian Shulin",
                        "user": "shulin16",
                        "type": "user"
                    },
                    "name": "Shulin Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:00.696Z",
                    "hidden": false
                },
                {
                    "_id": "6850e2a05e07650ecce89107",
                    "user": {
                        "_id": "6303e551d14428368d194477",
                        "avatarUrl": "/avatars/b3c583e4525747b314379a7613e3b115.svg",
                        "isPro": false,
                        "fullname": "Ruiqi Wang",
                        "user": "ruiqiw",
                        "type": "user"
                    },
                    "name": "Ruiqi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:03.098Z",
                    "hidden": false
                },
                {
                    "_id": "6850e2a05e07650ecce89108",
                    "name": "Hongming Guo",
                    "hidden": false
                },
                {
                    "_id": "6850e2a05e07650ecce89109",
                    "name": "Penghao Wu",
                    "hidden": false
                },
                {
                    "_id": "6850e2a05e07650ecce8910a",
                    "name": "Yuhao Dong",
                    "hidden": false
                },
                {
                    "_id": "6850e2a05e07650ecce8910b",
                    "name": "Xiuying Wang",
                    "hidden": false
                },
                {
                    "_id": "6850e2a05e07650ecce8910c",
                    "name": "Jingkang Yang",
                    "hidden": false
                },
                {
                    "_id": "6850e2a05e07650ecce8910d",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850e2a05e07650ecce8910e",
                    "name": "Hongyuan Zhu",
                    "hidden": false
                },
                {
                    "_id": "6850e2a05e07650ecce8910f",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T16:17:08.000Z",
            "submittedOnDailyAt": "2025-06-17T02:14:02.144Z",
            "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning",
            "submittedOnDailyBy": {
                "_id": "6658d01c6f1a71ba56d6c273",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tc4nZrMuZQLfgt5aVxtH4.jpeg",
                "isPro": false,
                "fullname": "Tian Shulin",
                "user": "shulin16",
                "type": "user"
            },
            "summary": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,\nin days and weeks) egocentric videos, which leverages a structured\nChain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained\nvia reinforcement learning (RL). Inspired by human problem-solving strategies,\nCoTT decomposes complex reasoning into modular steps, with the RL agent\ninvoking specific tools, one per step, to iteratively and collaboratively\nanswer sub-questions tackling such tasks as temporal retrieval and multi-modal\nunderstanding. We design a two-stage training paradigm involving supervised\nfinetuning (SFT) of a pretrained language model using CoTT data and RL to\nenable our agent to dynamically propose step-by-step tools for long-range\nreasoning. To facilitate training, we construct a dataset called Ego-R1 Data,\nwhich consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our\nEgo-R1 agent is evaluated on a newly curated week-long video QA benchmark,\nEgo-R1 Bench, which contains human-verified QA pairs from hybrid sources.\nExtensive results demonstrate that the dynamic, tool-augmented chain-of-thought\nreasoning by our Ego-R1 Agent can effectively tackle the unique challenges of\nunderstanding ultra-long egocentric videos, significantly extending the time\ncoverage from few hours to a week.",
            "upvotes": 33,
            "discussionId": "6850e2a05e07650ecce89110",
            "ai_summary": "Ego-R1, a reinforcement learning-based framework, uses a structured tool-augmented chain-of-thought process to reason over ultra-long egocentric videos, achieving better performance than existing methods by extending time coverage to a week.",
            "ai_keywords": [
                "Chain-of-Tool-Thought",
                "CoTT",
                "reinforcement learning",
                "RL",
                "pretrained language model",
                "supervised finetuning",
                "SFT",
                "Ego-CoTT-25K",
                "Ego-QA-4.4K",
                "Ego-R1 Bench",
                "video QA",
                "temporal retrieval",
                "multi-modal understanding"
            ]
        },
        "publishedAt": "2025-06-16T12:17:08.000Z",
        "title": "Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning",
        "summary": "We introduce Ego-R1, a novel framework for reasoning over ultra-long (i.e.,\nin days and weeks) egocentric videos, which leverages a structured\nChain-of-Tool-Thought (CoTT) process, orchestrated by an Ego-R1 Agent trained\nvia reinforcement learning (RL). Inspired by human problem-solving strategies,\nCoTT decomposes complex reasoning into modular steps, with the RL agent\ninvoking specific tools, one per step, to iteratively and collaboratively\nanswer sub-questions tackling such tasks as temporal retrieval and multi-modal\nunderstanding. We design a two-stage training paradigm involving supervised\nfinetuning (SFT) of a pretrained language model using CoTT data and RL to\nenable our agent to dynamically propose step-by-step tools for long-range\nreasoning. To facilitate training, we construct a dataset called Ego-R1 Data,\nwhich consists of Ego-CoTT-25K for SFT and Ego-QA-4.4K for RL. Furthermore, our\nEgo-R1 agent is evaluated on a newly curated week-long video QA benchmark,\nEgo-R1 Bench, which contains human-verified QA pairs from hybrid sources.\nExtensive results demonstrate that the dynamic, tool-augmented chain-of-thought\nreasoning by our Ego-R1 Agent can effectively tackle the unique challenges of\nunderstanding ultra-long egocentric videos, significantly extending the time\ncoverage from few hours to a week.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13654.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6658d01c6f1a71ba56d6c273",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/tc4nZrMuZQLfgt5aVxtH4.jpeg",
            "fullname": "Tian Shulin",
            "name": "shulin16",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.08343",
            "authors": [
                {
                    "_id": "684ae1f5dbd21a9cc27b0f3a",
                    "name": "Chenlong Wang",
                    "hidden": false
                },
                {
                    "_id": "684ae1f5dbd21a9cc27b0f3b",
                    "name": "Yuanning Feng",
                    "hidden": false
                },
                {
                    "_id": "684ae1f5dbd21a9cc27b0f3c",
                    "name": "Dongping Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae1f5dbd21a9cc27b0f3d",
                    "name": "Zhaoyang Chu",
                    "hidden": false
                },
                {
                    "_id": "684ae1f5dbd21a9cc27b0f3e",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "684ae1f5dbd21a9cc27b0f3f",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T21:13:27.668Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T01:54:04.000Z",
            "submittedOnDailyAt": "2025-06-17T00:33:58.377Z",
            "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves\n  Reasoning Efficiency",
            "submittedOnDailyBy": {
                "_id": "643be8879f5d314db2d9ed23",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
                "isPro": false,
                "fullname": "Chen Dongping",
                "user": "shuaishuaicdp",
                "type": "user"
            },
            "summary": "Recent advances in large reasoning models have enabled complex, step-by-step\nreasoning but often introduce significant overthinking, resulting in verbose\nand redundant outputs that hinder efficiency. In this study, we examine whether\nexplicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is\nnecessary for advanced reasoning. We propose NoWait, a simple yet effective\napproach that disables explicit self-reflection by suppressing these tokens\nduring inference. Extensive experiments on ten benchmarks across textual,\nvisual, and video reasoning tasks show that NoWait reduces chain-of-thought\ntrajectory length by up to 27%-51% in five R1-style model series, without\ncompromising model utility. NoWait thus offers a plug-and-play solution for\nefficient and utility-preserving multimodal reasoning.",
            "upvotes": 31,
            "discussionId": "684ae1f5dbd21a9cc27b0f40",
            "ai_summary": "NoWait suppresses explicit self-reflection tokens during inference to enhance efficiency in multimodal reasoning without reducing model utility.",
            "ai_keywords": [
                "reasoning models",
                "self-reflection",
                "tokens",
                "NoWait",
                "chain-of-thought trajectory length",
                "R1-style model series",
                "multimodal reasoning"
            ]
        },
        "publishedAt": "2025-06-09T21:54:04.000Z",
        "title": "Wait, We Don't Need to \"Wait\"! Removing Thinking Tokens Improves\n  Reasoning Efficiency",
        "summary": "Recent advances in large reasoning models have enabled complex, step-by-step\nreasoning but often introduce significant overthinking, resulting in verbose\nand redundant outputs that hinder efficiency. In this study, we examine whether\nexplicit self-reflection, signaled by tokens such as \"Wait\" and \"Hmm\", is\nnecessary for advanced reasoning. We propose NoWait, a simple yet effective\napproach that disables explicit self-reflection by suppressing these tokens\nduring inference. Extensive experiments on ten benchmarks across textual,\nvisual, and video reasoning tasks show that NoWait reduces chain-of-thought\ntrajectory length by up to 27%-51% in five R1-style model series, without\ncompromising model utility. NoWait thus offers a plug-and-play solution for\nefficient and utility-preserving multimodal reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.08343.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
            "fullname": "Chen Dongping",
            "name": "shuaishuaicdp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10055",
            "authors": [
                {
                    "_id": "6850ca685e07650ecce88fb6",
                    "name": "Dingfeng Shi",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fb7",
                    "name": "Jingyi Cao",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fb8",
                    "name": "Qianben Chen",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fb9",
                    "name": "Weichen Sun",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fba",
                    "name": "Weizhen Li",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fbb",
                    "name": "Hongxuan Lu",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fbc",
                    "name": "Fangchen Dong",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fbd",
                    "name": "Tianrui Qin",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fbe",
                    "name": "King Zhu",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fbf",
                    "name": "Minghao Yang",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fc0",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fc1",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fc2",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fc3",
                    "name": "Changwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fc4",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fc5",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "6850ca685e07650ecce88fc6",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:58:14.000Z",
            "submittedOnDailyAt": "2025-06-17T05:04:25.899Z",
            "title": "TaskCraft: Automated Generation of Agentic Tasks",
            "submittedOnDailyBy": {
                "_id": "628c8598ef14f971b698107f",
                "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
                "isPro": false,
                "fullname": "Zhou",
                "user": "Wangchunshu",
                "type": "user"
            },
            "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce TaskCraft, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.",
            "upvotes": 24,
            "discussionId": "6850ca695e07650ecce88fc7",
            "ai_summary": "TaskCraft automates the generation of scalable, multi-tool, and complex agentic tasks to enhance prompt optimization and fine-tuning of agentic models.",
            "ai_keywords": [
                "agentic tasks",
                "multi-step problem solving",
                "tool use",
                "adaptive reasoning",
                "instruction data",
                "human annotation",
                "difficulty-scalable",
                "multi-tool",
                "verifiable tasks",
                "execution trajectories",
                "depth-based extensions",
                "width-based extensions",
                "hierarchical complexity",
                "prompt optimization",
                "supervised fine-tuning",
                "agentic foundation models",
                "large-scale synthetic dataset"
            ]
        },
        "publishedAt": "2025-06-11T13:58:14.000Z",
        "title": "TaskCraft: Automated Generation of Agentic Tasks",
        "summary": "Agentic tasks, which require multi-step problem solving with autonomy, tool\nuse, and adaptive reasoning, are becoming increasingly central to the\nadvancement of NLP and AI. However, existing instruction data lacks tool\ninteraction, and current agentic benchmarks rely on costly human annotation,\nlimiting their scalability. We introduce TaskCraft, an automated\nworkflow for generating difficulty-scalable, multi-tool, and verifiable agentic\ntasks with execution trajectories. TaskCraft expands atomic tasks using\ndepth-based and width-based extensions to create structurally and\nhierarchically complex challenges. Empirical results show that these tasks\nimprove prompt optimization in the generation workflow and enhance supervised\nfine-tuning of agentic foundation models. We present a large-scale synthetic\ndataset of approximately 36,000 tasks with varying difficulty to support future\nresearch on agent tuning and evaluation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10055.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "628c8598ef14f971b698107f",
            "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
            "fullname": "Zhou",
            "name": "Wangchunshu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09482",
            "authors": [
                {
                    "_id": "684eb86a60b4a34dbe007a0a",
                    "user": {
                        "_id": "6398433a1054c4954270182c",
                        "avatarUrl": "/avatars/5451d1d9bc79b6c6356c93fb82d4861c.svg",
                        "isPro": false,
                        "fullname": "zhen dc",
                        "user": "zhendch",
                        "type": "user"
                    },
                    "name": "Dingcheng Zhen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T13:06:49.402Z",
                    "hidden": false
                },
                {
                    "_id": "684eb86a60b4a34dbe007a0b",
                    "name": "Qian Qiao",
                    "hidden": false
                },
                {
                    "_id": "684eb86a60b4a34dbe007a0c",
                    "name": "Tan Yu",
                    "hidden": false
                },
                {
                    "_id": "684eb86a60b4a34dbe007a0d",
                    "name": "Kangxi Wu",
                    "hidden": false
                },
                {
                    "_id": "684eb86a60b4a34dbe007a0e",
                    "name": "Ziwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "684eb86a60b4a34dbe007a0f",
                    "name": "Siyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "684eb86a60b4a34dbe007a10",
                    "name": "Shunshun Yin",
                    "hidden": false
                },
                {
                    "_id": "684eb86a60b4a34dbe007a11",
                    "name": "Ming Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T07:50:31.000Z",
            "submittedOnDailyAt": "2025-06-17T12:06:19.127Z",
            "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference\n  Autoregression",
            "submittedOnDailyBy": {
                "_id": "6398433a1054c4954270182c",
                "avatarUrl": "/avatars/5451d1d9bc79b6c6356c93fb82d4861c.svg",
                "isPro": false,
                "fullname": "zhen dc",
                "user": "zhendch",
                "type": "user"
            },
            "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.",
            "upvotes": 24,
            "discussionId": "684eb86b60b4a34dbe007a12",
            "githubRepo": "https://github.com/TransDiff/TransDiff",
            "ai_summary": "TransDiff, combining an Autoregressive Transformer and diffusion models, achieves superior image generation performance and speed, while Multi-Reference Autoregression further enhances its quality and diversity.",
            "ai_keywords": [
                "Autoregressive (AR) Transformer",
                "diffusion models",
                "high-level semantic features",
                "Fr\\'echet Inception Distance (FID)",
                "Inception Score (IS)",
                "Multi-Reference Autoregression (MRAR)"
            ]
        },
        "publishedAt": "2025-06-11T03:50:31.000Z",
        "title": "Marrying Autoregressive Transformer and Diffusion with Multi-Reference\n  Autoregression",
        "summary": "We introduce TransDiff, the first image generation model that marries\nAutoregressive (AR) Transformer with diffusion models. In this joint modeling\nframework, TransDiff encodes labels and images into high-level semantic\nfeatures and employs a diffusion model to estimate the distribution of image\nsamples. On the ImageNet 256x256 benchmark, TransDiff significantly outperforms\nother image generation models based on standalone AR Transformer or diffusion\nmodels. Specifically, TransDiff achieves a Fr\\'echet Inception Distance (FID)\nof 1.61 and an Inception Score (IS) of 293.4, and further provides x2 faster\ninference latency compared to state-of-the-art methods based on AR Transformer\nand x112 faster inference compared to diffusion-only models. Furthermore,\nbuilding on the TransDiff model, we introduce a novel image generation paradigm\ncalled Multi-Reference Autoregression (MRAR), which performs autoregressive\ngeneration by predicting the next image. MRAR enables the model to reference\nmultiple previously generated images, thereby facilitating the learning of more\ndiverse representations and improving the quality of generated images in\nsubsequent iterations. By applying MRAR, the performance of TransDiff is\nimproved, with the FID reduced from 1.61 to 1.42. We expect TransDiff to open\nup a new frontier in the field of image generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09482.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6398433a1054c4954270182c",
            "avatarUrl": "/avatars/5451d1d9bc79b6c6356c93fb82d4861c.svg",
            "fullname": "zhen dc",
            "name": "zhendch",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13759",
            "authors": [
                {
                    "_id": "6850ccab5e07650ecce88fd7",
                    "name": "Runpeng Yu",
                    "hidden": false
                },
                {
                    "_id": "6850ccab5e07650ecce88fd8",
                    "name": "Qi Li",
                    "hidden": false
                },
                {
                    "_id": "6850ccab5e07650ecce88fd9",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T17:59:08.000Z",
            "submittedOnDailyAt": "2025-06-17T00:51:24.409Z",
            "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
            "submittedOnDailyBy": {
                "_id": "635364b3c41f548fe39db945",
                "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
                "isPro": false,
                "fullname": "Runpeng Yu",
                "user": "rp-yu",
                "type": "user"
            },
            "summary": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
            "upvotes": 22,
            "discussionId": "6850ccab5e07650ecce88fda",
            "githubRepo": "https://github.com/LiQiiiii/DLLM-Survey",
            "ai_summary": "Discrete Diffusion Language Models (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs) enable parallel generation and faster inference compared to autoregressive models through denoising-based strategies and full attention mechanisms.",
            "ai_keywords": [
                "Discrete Diffusion Language Models",
                "Discrete Diffusion Multimodal Language Models",
                "autoregressive models",
                "multi-token",
                "parallel decoding",
                "full attention",
                "denoising-based generation",
                "response-aware perception",
                "inference speed",
                "autoregressive LLMs",
                "autoregressive MLLMs",
                "mathematical models",
                "historical development",
                "training",
                "inference",
                "language applications",
                "vision-language applications",
                "biological applications",
                "future research directions",
                "deployment"
            ]
        },
        "publishedAt": "2025-06-16T13:59:08.000Z",
        "title": "Discrete Diffusion in Large Language and Multimodal Models: A Survey",
        "summary": "In this work, we provide a systematic survey of Discrete Diffusion Language\nModels (dLLMs) and Discrete Diffusion Multimodal Language Models (dMLLMs).\nUnlike autoregressive (AR) models, dLLMs and dMLLMs adopt a multi-token,\nparallel decoding paradigm using full attention and a denoising-based\ngeneration strategy. This paradigm naturally enables parallel generation,\nfine-grained output controllability, and dynamic, response-aware perception.\nThese capabilities are previously difficult to achieve with AR models.\nRecently, a growing number of industrial-scale proprietary d(M)LLMs, as well as\na large number of open-source academic d(M)LLMs, have demonstrated performance\ncomparable to their autoregressive counterparts, while achieving up to 10x\nacceleration in inference speed.\n  The advancement of discrete diffusion LLMs and MLLMs has been largely driven\nby progress in two domains. The first is the development of autoregressive LLMs\nand MLLMs, which has accumulated vast amounts of data, benchmarks, and\nfoundational infrastructure for training and inference. The second contributing\ndomain is the evolution of the mathematical models underlying discrete\ndiffusion. Together, these advancements have catalyzed a surge in dLLMs and\ndMLLMs research in early 2025.\n  In this work, we present a comprehensive overview of the research in the dLLM\nand dMLLM domains. We trace the historical development of dLLMs and dMLLMs,\nformalize the underlying mathematical frameworks, and categorize representative\nmodels. We further analyze key techniques for training and inference, and\nsummarize emerging applications across language, vision-language, and\nbiological domains. We conclude by discussing future directions for research\nand deployment.\n  Paper collection: https://github.com/LiQiiiii/DLLM-Survey",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13759.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635364b3c41f548fe39db945",
            "avatarUrl": "/avatars/ad1916bbfabca0b6651c8eabacc5eba8.svg",
            "fullname": "Runpeng Yu",
            "name": "rp-yu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.06962",
            "authors": [
                {
                    "_id": "685192c60164cd1316710312",
                    "user": {
                        "_id": "632b597ab2dd35f13560f006",
                        "avatarUrl": "/avatars/4b37dce118566ab40b159f5598ef9fb2.svg",
                        "isPro": true,
                        "fullname": "Jingyuan Qi",
                        "user": "jingyq1",
                        "type": "user"
                    },
                    "name": "Jingyuan Qi",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-17T18:24:30.825Z",
                    "hidden": false
                },
                {
                    "_id": "685192c60164cd1316710313",
                    "name": "Zhiyang Xu",
                    "hidden": false
                },
                {
                    "_id": "685192c60164cd1316710314",
                    "name": "Qifan Wang",
                    "hidden": false
                },
                {
                    "_id": "685192c60164cd1316710315",
                    "name": "Lifu Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-08T01:33:05.000Z",
            "submittedOnDailyAt": "2025-06-17T14:41:30.287Z",
            "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation",
            "submittedOnDailyBy": {
                "_id": "632b597ab2dd35f13560f006",
                "avatarUrl": "/avatars/4b37dce118566ab40b159f5598ef9fb2.svg",
                "isPro": true,
                "fullname": "Jingyuan Qi",
                "user": "jingyq1",
                "type": "user"
            },
            "summary": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm\nthat enhances image generation by autoregressively incorporating knearest\nneighbor retrievals at the patch level. Unlike prior methods that perform a\nsingle, static retrieval before generation and condition the entire generation\non fixed reference images, AR-RAG performs context-aware retrievals at each\ngeneration step, using prior-generated patches as queries to retrieve and\nincorporate the most relevant patch-level visual references, enabling the model\nto respond to evolving generation needs while avoiding limitations (e.g.,\nover-copying, stylistic bias, etc.) prevalent in existing methods. To realize\nAR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in\nDecoding (DAiD), a training-free plug-and-use decoding strategy that directly\nmerges the distribution of model-predicted patches with the distribution of\nretrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a\nparameter-efficient fine-tuning method that progressively smooths the features\nof retrieved patches via multi-scale convolution operations and leverages them\nto augment the image generation process. We validate the effectiveness of\nAR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and\nDPG-Bench, demonstrating significant performance gains over state-of-the-art\nimage generation models.",
            "upvotes": 19,
            "discussionId": "685192c60164cd1316710316",
            "ai_summary": "Autoregressive Retrieval Augmentation enhances image generation through context-aware patch-level retrievals, improving performance over existing methods.",
            "ai_keywords": [
                "Autoregressive Retrieval Augmentation",
                "AR-RAG",
                "Patch level",
                "Distribution-Augmentation in Decoding",
                "DAiD",
                "Feature-Augmentation in Decoding",
                "FAiD",
                "parameter-efficient fine-tuning",
                "multi-scale convolution operations",
                "Midjourney-30K",
                "GenEval",
                "DPG-Bench"
            ]
        },
        "publishedAt": "2025-06-07T21:33:05.000Z",
        "title": "AR-RAG: Autoregressive Retrieval Augmentation for Image Generation",
        "summary": "We introduce Autoregressive Retrieval Augmentation (AR-RAG), a novel paradigm\nthat enhances image generation by autoregressively incorporating knearest\nneighbor retrievals at the patch level. Unlike prior methods that perform a\nsingle, static retrieval before generation and condition the entire generation\non fixed reference images, AR-RAG performs context-aware retrievals at each\ngeneration step, using prior-generated patches as queries to retrieve and\nincorporate the most relevant patch-level visual references, enabling the model\nto respond to evolving generation needs while avoiding limitations (e.g.,\nover-copying, stylistic bias, etc.) prevalent in existing methods. To realize\nAR-RAG, we propose two parallel frameworks: (1) Distribution-Augmentation in\nDecoding (DAiD), a training-free plug-and-use decoding strategy that directly\nmerges the distribution of model-predicted patches with the distribution of\nretrieved patches, and (2) Feature-Augmentation in Decoding (FAiD), a\nparameter-efficient fine-tuning method that progressively smooths the features\nof retrieved patches via multi-scale convolution operations and leverages them\nto augment the image generation process. We validate the effectiveness of\nAR-RAG on widely adopted benchmarks, including Midjourney-30K, GenEval and\nDPG-Bench, demonstrating significant performance gains over state-of-the-art\nimage generation models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06962.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632b597ab2dd35f13560f006",
            "avatarUrl": "/avatars/4b37dce118566ab40b159f5598ef9fb2.svg",
            "fullname": "Jingyuan Qi",
            "name": "jingyq1",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13750",
            "authors": [
                {
                    "_id": "6850d08d5e07650ecce8908b",
                    "user": {
                        "_id": "67ac56c90f861b63617d153d",
                        "avatarUrl": "/avatars/96f5e60031fe4fe677159dccfecaa65c.svg",
                        "isPro": false,
                        "fullname": "Yuan Yuheng",
                        "user": "nopyyh",
                        "type": "user"
                    },
                    "name": "Yuheng Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T15:13:10.277Z",
                    "hidden": false
                },
                {
                    "_id": "6850d08d5e07650ecce8908c",
                    "user": {
                        "_id": "643a6e89a856622f9788bf67",
                        "avatarUrl": "/avatars/419c0379f072295b27d4bfe2f8fb946d.svg",
                        "isPro": false,
                        "fullname": "qiuhong shen",
                        "user": "florinshum",
                        "type": "user"
                    },
                    "name": "Qiuhong Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:24.996Z",
                    "hidden": false
                },
                {
                    "_id": "6850d08d5e07650ecce8908d",
                    "name": "Shizun Wang",
                    "hidden": false
                },
                {
                    "_id": "6850d08d5e07650ecce8908e",
                    "name": "Xingyi Yang",
                    "hidden": false
                },
                {
                    "_id": "6850d08d5e07650ecce8908f",
                    "name": "Xinchao Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67ac56c90f861b63617d153d/RxoJjrMeG2lKENvLl_bR0.mp4"
            ],
            "publishedAt": "2025-06-16T17:56:22.000Z",
            "submittedOnDailyAt": "2025-06-17T00:54:48.920Z",
            "title": "Test3R: Learning to Reconstruct 3D at Test Time",
            "submittedOnDailyBy": {
                "_id": "67ac56c90f861b63617d153d",
                "avatarUrl": "/avatars/96f5e60031fe4fe677159dccfecaa65c.svg",
                "isPro": false,
                "fullname": "Yuan Yuheng",
                "user": "nopyyh",
                "type": "user"
            },
            "summary": "Dense matching methods like DUSt3R regress pairwise pointmaps for 3D\nreconstruction. However, the reliance on pairwise prediction and the limited\ngeneralization capability inherently restrict the global geometric consistency.\nIn this work, we introduce Test3R, a surprisingly simple test-time learning\ntechnique that significantly boosts geometric accuracy. Using image triplets\n(I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and\n(I_1,I_3). The core idea is to optimize the network at test time via a\nself-supervised objective: maximizing the geometric consistency between these\ntwo reconstructions relative to the common image I_1. This ensures the model\nproduces cross-pair consistent outputs, regardless of the inputs. Extensive\nexperiments demonstrate that our technique significantly outperforms previous\nstate-of-the-art methods on the 3D reconstruction and multi-view depth\nestimation tasks. Moreover, it is universally applicable and nearly cost-free,\nmaking it easily applied to other models and implemented with minimal test-time\ntraining overhead and parameter footprint. Code is available at\nhttps://github.com/nopQAQ/Test3R.",
            "upvotes": 18,
            "discussionId": "6850d08e5e07650ecce89090",
            "ai_summary": "Test3R, a test-time learning technique for 3D reconstruction, enhances geometric accuracy by optimizing network consistency using self-supervised learning on image triplets.",
            "ai_keywords": [
                "DUSt3R",
                "dense matching",
                "3D reconstruction",
                "geometric accuracy",
                "test-time learning",
                "image triplets",
                "self-supervised objective",
                "cross-pair consistency"
            ]
        },
        "publishedAt": "2025-06-16T13:56:22.000Z",
        "title": "Test3R: Learning to Reconstruct 3D at Test Time",
        "summary": "Dense matching methods like DUSt3R regress pairwise pointmaps for 3D\nreconstruction. However, the reliance on pairwise prediction and the limited\ngeneralization capability inherently restrict the global geometric consistency.\nIn this work, we introduce Test3R, a surprisingly simple test-time learning\ntechnique that significantly boosts geometric accuracy. Using image triplets\n(I_1,I_2,I_3), Test3R generates reconstructions from pairs (I_1,I_2) and\n(I_1,I_3). The core idea is to optimize the network at test time via a\nself-supervised objective: maximizing the geometric consistency between these\ntwo reconstructions relative to the common image I_1. This ensures the model\nproduces cross-pair consistent outputs, regardless of the inputs. Extensive\nexperiments demonstrate that our technique significantly outperforms previous\nstate-of-the-art methods on the 3D reconstruction and multi-view depth\nestimation tasks. Moreover, it is universally applicable and nearly cost-free,\nmaking it easily applied to other models and implemented with minimal test-time\ntraining overhead and parameter footprint. Code is available at\nhttps://github.com/nopQAQ/Test3R.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67ac56c90f861b63617d153d/RxoJjrMeG2lKENvLl_bR0.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13750.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67ac56c90f861b63617d153d",
            "avatarUrl": "/avatars/96f5e60031fe4fe677159dccfecaa65c.svg",
            "fullname": "Yuan Yuheng",
            "name": "nopyyh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14111",
            "authors": [
                {
                    "_id": "6852072a0164cd1316710407",
                    "name": "Essential AI",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710409",
                    "name": "Andrew Hojel",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671040a",
                    "name": "Michael Pust",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671040b",
                    "name": "Tim Romanski",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671040c",
                    "name": "Yash Vanjani",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671040d",
                    "name": "Ritvik Kapila",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671040e",
                    "name": "Mohit Parmar",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671040f",
                    "name": "Adarsh Chaluvaraju",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710410",
                    "name": "Alok Tripathy",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710411",
                    "name": "Anil Thomas",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710412",
                    "name": "Ashish Tanwer",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710413",
                    "name": "Darsh J Shah",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710414",
                    "name": "Ishaan Shah",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710415",
                    "name": "Karl Stratos",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710416",
                    "name": "Khoi Nguyen",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710417",
                    "name": "Kurt Smith",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710418",
                    "name": "Michael Callahan",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd1316710419",
                    "name": "Peter Rushton",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671041a",
                    "name": "Philip Monk",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671041b",
                    "name": "Platon Mazarakis",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671041c",
                    "name": "Saad Jamal",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671041d",
                    "name": "Saurabh Srivastava",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671041e",
                    "name": "Somanshu Singla",
                    "hidden": false
                },
                {
                    "_id": "6852072a0164cd131671041f",
                    "name": "Ashish Vaswani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T02:03:36.000Z",
            "submittedOnDailyAt": "2025-06-17T22:56:08.098Z",
            "title": "Essential-Web v1.0: 24T tokens of organized web data",
            "submittedOnDailyBy": {
                "_id": "67f40efb64c87965b9063655",
                "avatarUrl": "/avatars/a25abd6f3318036c8d589633221c2010.svg",
                "isPro": false,
                "fullname": "Research at Essential AI",
                "user": "Research-EAI",
                "type": "user"
            },
            "summary": "Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreement within 3% of\nQwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain\ncompetitive web-curated datasets in math (-8.0% relative to SOTA), web code\n(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on\nHuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0",
            "upvotes": 17,
            "discussionId": "6852072b0164cd1316710420",
            "ai_summary": "A large, 24-trillion-token Essential-Web v1.0 dataset annotated with a multi-category taxonomy outperforms or is competitive with existing datasets in various domains using simple filtering techniques.",
            "ai_keywords": [
                "Essential-Web",
                "EAI-Distill-0.5b",
                "Qwen2.5-32B-Instruct"
            ]
        },
        "publishedAt": "2025-06-16T22:03:36.000Z",
        "title": "Essential-Web v1.0: 24T tokens of organized web data",
        "summary": "Data plays the most prominent role in how language models acquire skills and\nknowledge. The lack of massive, well-organized pre-training datasets results in\ncostly and inaccessible data pipelines. We present Essential-Web v1.0, a\n24-trillion-token dataset in which every document is annotated with a\ntwelve-category taxonomy covering topic, format, content complexity, and\nquality. Taxonomy labels are produced by EAI-Distill-0.5b, a fine-tuned\n0.5b-parameter model that achieves an annotator agreement within 3% of\nQwen2.5-32B-Instruct. With nothing more than SQL-style filters, we obtain\ncompetitive web-curated datasets in math (-8.0% relative to SOTA), web code\n(+14.3%), STEM (+24.5%) and medical (+8.6%). Essential-Web v1.0 is available on\nHuggingFace: https://huggingface.co/datasets/EssentialAI/essential-web-v1.0",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14111.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67f40efb64c87965b9063655",
            "avatarUrl": "/avatars/a25abd6f3318036c8d589633221c2010.svg",
            "fullname": "Research at Essential AI",
            "name": "Research-EAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.11991",
            "authors": [
                {
                    "_id": "684fc67360b4a34dbe007b3c",
                    "user": {
                        "_id": "64d201b1c2bd235422fb1d14",
                        "avatarUrl": "/avatars/e50581aa66391cedae94e116e759b9ec.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "stormthunder",
                        "type": "user"
                    },
                    "name": "Jiacong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:22:25.360Z",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b3d",
                    "name": "Zijiang Kang",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b3e",
                    "name": "Haochen Wang",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b3f",
                    "name": "Haiyong Jiang",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b40",
                    "name": "Jiawen Li",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b41",
                    "user": {
                        "_id": "64722a616facfb01d8ae8349",
                        "avatarUrl": "/avatars/1dce23ae5ebd9996770cf5efe910b857.svg",
                        "isPro": false,
                        "fullname": "Wu Bohong",
                        "user": "bongbohong",
                        "type": "user"
                    },
                    "name": "Bohong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T09:52:40.461Z",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b42",
                    "name": "Ya Wang",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b43",
                    "name": "Jiao Ran",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b44",
                    "name": "Xiao Liang",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b45",
                    "name": "Chao Feng",
                    "hidden": false
                },
                {
                    "_id": "684fc67360b4a34dbe007b46",
                    "name": "Jun Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T17:47:43.000Z",
            "submittedOnDailyAt": "2025-06-17T05:58:53.901Z",
            "title": "VGR: Visual Grounded Reasoning",
            "submittedOnDailyBy": {
                "_id": "64d201b1c2bd235422fb1d14",
                "avatarUrl": "/avatars/e50581aa66391cedae94e116e759b9ec.svg",
                "isPro": false,
                "fullname": "wang",
                "user": "stormthunder",
                "type": "user"
            },
            "summary": "In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.",
            "upvotes": 16,
            "discussionId": "684fc67460b4a34dbe007b47",
            "projectPage": "https://huggingface.co/BytedanceDouyinContent/VGR",
            "ai_summary": "VGR, a novel multimodal large language model, improves visual reasoning by detecting relevant image regions and integrating them into the reasoning process, outperforming existing models on multimodal benchmarks with reduced resource usage.",
            "ai_keywords": [
                "multimodal chain-of-thought reasoning",
                "MLLM",
                "VGR",
                "enhanced fine-grained visual perception",
                "SFT dataset",
                "bounding boxes",
                "replay stage",
                "multimodal comprehension",
                "LLaVA-NeXT-7B",
                "MMStar",
                "AI2D",
                "ChartQA"
            ]
        },
        "publishedAt": "2025-06-13T13:47:43.000Z",
        "title": "VGR: Visual Grounded Reasoning",
        "summary": "In the field of multimodal chain-of-thought (CoT) reasoning, existing\napproaches predominantly rely on reasoning on pure language space, which\ninherently suffers from language bias and is largely confined to math or\nscience domains. This narrow focus limits their ability to handle complex\nvisual reasoning tasks that demand comprehensive understanding of image\ndetails. To address these limitations, this paper introduces VGR, a novel\nreasoning multimodal large language model (MLLM) with enhanced fine-grained\nvisual perception capabilities. Unlike traditional MLLMs that answer the\nquestion or reasoning solely on the language space, our VGR first detects\nrelevant regions that may help to solve problems, and then provides precise\nanswers based on replayed image regions. To achieve this, we conduct a\nlarge-scale SFT dataset called VGR -SFT that contains reasoning data with mixed\nvision grounding and language deduction. The inference pipeline of VGR allows\nthe model to choose bounding boxes for visual reference and a replay stage is\nintroduced to integrates the corresponding regions into the reasoning process,\nenhancing multimodel comprehension. Experiments on the LLaVA-NeXT-7B baseline\nshow that VGR achieves superior performance on multi-modal benchmarks requiring\ncomprehensive image detail understanding. Compared to the baseline, VGR uses\nonly 30\\% of the image token count while delivering scores of +4.1 on MMStar,\n+7.1 on AI2D, and a +12.9 improvement on ChartQA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11991.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d201b1c2bd235422fb1d14",
            "avatarUrl": "/avatars/e50581aa66391cedae94e116e759b9ec.svg",
            "fullname": "wang",
            "name": "stormthunder",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.12915",
            "authors": [
                {
                    "_id": "6850da255e07650ecce890d3",
                    "name": "Meiling Tao",
                    "hidden": false
                },
                {
                    "_id": "6850da255e07650ecce890d4",
                    "name": "Chenghao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6850da255e07650ecce890d5",
                    "name": "Dongyi Ding",
                    "hidden": false
                },
                {
                    "_id": "6850da255e07650ecce890d6",
                    "name": "Tiannan Wang",
                    "hidden": false
                },
                {
                    "_id": "6850da255e07650ecce890d7",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "6850da255e07650ecce890d8",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-15T17:19:19.000Z",
            "submittedOnDailyAt": "2025-06-17T01:37:16.408Z",
            "title": "PersonaFeedback: A Large-scale Human-annotated Benchmark For\n  Personalization",
            "submittedOnDailyBy": {
                "_id": "632bfaebea6e62428ab0e9c2",
                "avatarUrl": "/avatars/344aaf371bbba9aea091b12741c451e5.svg",
                "isPro": false,
                "fullname": "Tiannan Wang",
                "user": "WTNswaggy",
                "type": "user"
            },
            "summary": "With the rapid improvement in the general capabilities of LLMs, LLM\npersonalization, i.e., how to build LLM systems that can generate personalized\nresponses or services that are tailored to distinct user personas, has become\nan increasingly important research and engineering problem. However, unlike\nmany new challenging benchmarks being released for evaluating the\ngeneral/reasoning capabilities, the lack of high-quality benchmarks for\nevaluating LLM personalization greatly hinders progress in this field. To\naddress this, we introduce PersonaFeedback, a new benchmark that directly\nevaluates LLMs' ability to provide personalized responses given pre-defined\nuser personas and queries. Unlike existing benchmarks that require models to\ninfer implicit user personas from historical interactions, PersonaFeedback\ndecouples persona inference from personalization, focusing on evaluating the\nmodel's ability to generate responses tailored to explicit personas.\nPersonaFeedback consists of 8298 human-annotated test cases, which are\ncategorized into easy, medium, and hard tiers based on the contextual\ncomplexity of the user personas and the difficulty in distinguishing subtle\ndifferences between two personalized responses. We conduct comprehensive\nevaluations across a wide range of models. The empirical results reveal that\neven state-of-the-art LLMs that can solve complex real-world reasoning tasks\ncould fall short on the hard tier of PersonaFeedback where even human\nevaluators may find the distinctions challenging. Furthermore, we conduct an\nin-depth analysis of failure modes across various types of systems,\ndemonstrating that the current retrieval-augmented framework should not be seen\nas a de facto solution for personalization tasks. All benchmark data,\nannotation protocols, and the evaluation pipeline will be publicly available to\nfacilitate future research on LLM personalization.",
            "upvotes": 14,
            "discussionId": "6850da255e07650ecce890d9",
            "ai_summary": "A new benchmark, PersonaFeedback, evaluates Large Language Models' ability to generate personalized responses given explicit user personas, revealing limitations in current systems.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "LLM personalization",
                "PersonaFeedback",
                "user personas",
                "personalized responses",
                "contextual complexity",
                "human-annotated test cases",
                "retrieval-augmented framework"
            ]
        },
        "publishedAt": "2025-06-15T13:19:19.000Z",
        "title": "PersonaFeedback: A Large-scale Human-annotated Benchmark For\n  Personalization",
        "summary": "With the rapid improvement in the general capabilities of LLMs, LLM\npersonalization, i.e., how to build LLM systems that can generate personalized\nresponses or services that are tailored to distinct user personas, has become\nan increasingly important research and engineering problem. However, unlike\nmany new challenging benchmarks being released for evaluating the\ngeneral/reasoning capabilities, the lack of high-quality benchmarks for\nevaluating LLM personalization greatly hinders progress in this field. To\naddress this, we introduce PersonaFeedback, a new benchmark that directly\nevaluates LLMs' ability to provide personalized responses given pre-defined\nuser personas and queries. Unlike existing benchmarks that require models to\ninfer implicit user personas from historical interactions, PersonaFeedback\ndecouples persona inference from personalization, focusing on evaluating the\nmodel's ability to generate responses tailored to explicit personas.\nPersonaFeedback consists of 8298 human-annotated test cases, which are\ncategorized into easy, medium, and hard tiers based on the contextual\ncomplexity of the user personas and the difficulty in distinguishing subtle\ndifferences between two personalized responses. We conduct comprehensive\nevaluations across a wide range of models. The empirical results reveal that\neven state-of-the-art LLMs that can solve complex real-world reasoning tasks\ncould fall short on the hard tier of PersonaFeedback where even human\nevaluators may find the distinctions challenging. Furthermore, we conduct an\nin-depth analysis of failure modes across various types of systems,\ndemonstrating that the current retrieval-augmented framework should not be seen\nas a de facto solution for personalization tasks. All benchmark data,\nannotation protocols, and the evaluation pipeline will be publicly available to\nfacilitate future research on LLM personalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12915.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632bfaebea6e62428ab0e9c2",
            "avatarUrl": "/avatars/344aaf371bbba9aea091b12741c451e5.svg",
            "fullname": "Tiannan Wang",
            "name": "WTNswaggy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03968",
            "authors": [
                {
                    "_id": "684169b041d567923aa6c5be",
                    "user": {
                        "_id": "663b22a80966eef8686aadaf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663b22a80966eef8686aadaf/iBzyQTyGZKf33RPVIFh9a.jpeg",
                        "isPro": false,
                        "fullname": "Chiwei Zhu",
                        "user": "IgnoraZ",
                        "type": "user"
                    },
                    "name": "Chiwei Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-05T09:59:41.997Z",
                    "hidden": false
                },
                {
                    "_id": "684169b041d567923aa6c5bf",
                    "name": "Benfeng Xu",
                    "hidden": false
                },
                {
                    "_id": "684169b041d567923aa6c5c0",
                    "name": "Xiaorui Wang",
                    "hidden": false
                },
                {
                    "_id": "684169b041d567923aa6c5c1",
                    "name": "Zhendong Mao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T14:00:47.000Z",
            "submittedOnDailyAt": "2025-06-17T00:33:16.832Z",
            "title": "From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding",
            "submittedOnDailyBy": {
                "_id": "663b22a80966eef8686aadaf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663b22a80966eef8686aadaf/iBzyQTyGZKf33RPVIFh9a.jpeg",
                "isPro": false,
                "fullname": "Chiwei Zhu",
                "user": "IgnoraZ",
                "type": "user"
            },
            "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.",
            "upvotes": 13,
            "discussionId": "684169b141d567923aa6c603",
            "githubRepo": "https://github.com/Ignoramus0817/SynthQuestions",
            "ai_summary": "The paper presents a method for generating diverse and complex instruction data for large language models using attributed grounding, achieving top performance on benchmarks with a large synthesized dataset.",
            "ai_keywords": [
                "acknowledged grounding",
                "top-down attribution process",
                "bottom-up synthesis process",
                "web documents",
                "large language models",
                "SynthQuestions"
            ]
        },
        "publishedAt": "2025-06-04T10:00:47.000Z",
        "title": "From Real to Synthetic: Synthesizing Millions of Diversified and\n  Complicated User Instructions with Attributed Grounding",
        "summary": "The pursuit of diverse, complex, and large-scale instruction data is crucial\nfor automatically aligning large language models (LLMs). While there are\nmethods capable of generating synthetic instructions at scale, they either\nsuffer from limited grounding sources, leading to a narrow distribution, or\nrely on trivial extensions that fail to produce meaningful trajectories in\nterms of complexity. In contrast, instructions that benefit efficient alignment\nare typically crafted with cognitive insights and grounded in real-world use\ncases. In this paper, we synthesize such instructions using attributed\ngrounding, which involves 1) a top-down attribution process that grounds a\nselective set of real instructions to situated users, and 2) a bottom-up\nsynthesis process that leverages web documents to first generate a situation,\nthen a meaningful instruction. This framework allows us to harvest diverse and\ncomplex instructions at scale, utilizing the vast range of web documents.\nSpecifically, we construct a dataset of 1 million instructions, called\nSynthQuestions, and demonstrate that models trained on it achieve leading\nperformance on several common benchmarks, with improvements that continually\nscale with more web corpora. Data, models and codes will be available at\nhttps://github.com/Ignoramus0817/SynthQuestions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03968.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "663b22a80966eef8686aadaf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663b22a80966eef8686aadaf/iBzyQTyGZKf33RPVIFh9a.jpeg",
            "fullname": "Chiwei Zhu",
            "name": "IgnoraZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13284",
            "authors": [
                {
                    "_id": "6850d4585e07650ecce890a8",
                    "user": {
                        "_id": "65f33b1c9f7970ccc0234cbf",
                        "avatarUrl": "/avatars/99fbab303912e3674663251c04279907.svg",
                        "isPro": false,
                        "fullname": "Zihan Liu",
                        "user": "zihanliu",
                        "type": "user"
                    },
                    "name": "Zihan Liu",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-17T02:35:05.073Z",
                    "hidden": false
                },
                {
                    "_id": "6850d4585e07650ecce890a9",
                    "user": {
                        "_id": "67d75b0117c2acac528f47b6",
                        "avatarUrl": "/avatars/619aacd1a619aab64de3499ac3ee2229.svg",
                        "isPro": false,
                        "fullname": "Zhuolin Yang",
                        "user": "zhuoliny",
                        "type": "user"
                    },
                    "name": "Zhuolin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T21:13:22.533Z",
                    "hidden": false
                },
                {
                    "_id": "6850d4585e07650ecce890aa",
                    "user": {
                        "_id": "62bc9d90e81dfd65cced9316",
                        "avatarUrl": "/avatars/05df14cd1fdbc7d6a80d2960a05a94f0.svg",
                        "isPro": false,
                        "fullname": "Yang Chen",
                        "user": "ychenNLP",
                        "type": "user"
                    },
                    "name": "Yang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T21:13:20.767Z",
                    "hidden": false
                },
                {
                    "_id": "6850d4585e07650ecce890ab",
                    "name": "Chankyu Lee",
                    "hidden": false
                },
                {
                    "_id": "6850d4585e07650ecce890ac",
                    "name": "Mohammad Shoeybi",
                    "hidden": false
                },
                {
                    "_id": "6850d4585e07650ecce890ad",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                },
                {
                    "_id": "6850d4585e07650ecce890ae",
                    "user": {
                        "_id": "663ee43bfeeb49803537da98",
                        "avatarUrl": "/avatars/17c3e9c435cc36fb04b4589e6176a243.svg",
                        "isPro": false,
                        "fullname": "Wei Ping",
                        "user": "wping",
                        "type": "user"
                    },
                    "name": "Wei Ping",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-17T02:35:05.073Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T09:27:48.000Z",
            "submittedOnDailyAt": "2025-06-17T16:13:25.654Z",
            "title": "AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL Synergy",
            "submittedOnDailyBy": {
                "_id": "65f33b1c9f7970ccc0234cbf",
                "avatarUrl": "/avatars/99fbab303912e3674663251c04279907.svg",
                "isPro": false,
                "fullname": "Zihan Liu",
                "user": "zihanliu",
                "type": "user"
            },
            "summary": "In this work, we investigate the synergy between supervised fine-tuning (SFT)\nand reinforcement learning (RL) in developing strong reasoning models. We begin\nby curating the SFT training data through two scaling strategies: increasing\nthe number of collected prompts and the number of generated responses per\nprompt. Both approaches yield notable improvements in reasoning performance,\nwith scaling the number of prompts resulting in more substantial gains. We then\nexplore the following questions regarding the synergy between SFT and RL: (i)\nDoes a stronger SFT model consistently lead to better final performance after\nlarge-scale RL training? (ii) How can we determine an appropriate sampling\ntemperature during RL training to effectively balance exploration and\nexploitation for a given SFT initialization? Our findings suggest that (i)\nholds true, provided effective RL training is conducted, particularly when the\nsampling temperature is carefully chosen to maintain the temperature-adjusted\nentropy around 0.3, a setting that strikes a good balance between exploration\nand exploitation. Notably, the performance gap between initial SFT models\nnarrows significantly throughout the RL process. Leveraging a strong SFT\nfoundation and insights into the synergistic interplay between SFT and RL, our\nAceReason-Nemotron-1.1 7B model significantly outperforms\nAceReason-Nemotron-1.0 and achieves new state-of-the-art performance among\nQwen2.5-7B-based reasoning models on challenging math and code benchmarks,\nthereby demonstrating the effectiveness of our post-training recipe. We release\nthe model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B",
            "upvotes": 12,
            "discussionId": "6850d4585e07650ecce890af",
            "ai_summary": "Combining supervised fine-tuning and reinforcement learning enhances reasoning models, especially when optimizing sampling temperature and leveraging strong initial fine-tuning, as demonstrated by the improved AceReason-Nemotron-1.1 model.",
            "ai_keywords": [
                "supervised fine-tuning",
                "reinforcement learning",
                "reasoning models",
                "sampling temperature",
                "entropy"
            ]
        },
        "publishedAt": "2025-06-16T05:27:48.000Z",
        "title": "AceReason-Nemotron 1.1: Advancing Math and Code Reasoning through SFT\n  and RL Synergy",
        "summary": "In this work, we investigate the synergy between supervised fine-tuning (SFT)\nand reinforcement learning (RL) in developing strong reasoning models. We begin\nby curating the SFT training data through two scaling strategies: increasing\nthe number of collected prompts and the number of generated responses per\nprompt. Both approaches yield notable improvements in reasoning performance,\nwith scaling the number of prompts resulting in more substantial gains. We then\nexplore the following questions regarding the synergy between SFT and RL: (i)\nDoes a stronger SFT model consistently lead to better final performance after\nlarge-scale RL training? (ii) How can we determine an appropriate sampling\ntemperature during RL training to effectively balance exploration and\nexploitation for a given SFT initialization? Our findings suggest that (i)\nholds true, provided effective RL training is conducted, particularly when the\nsampling temperature is carefully chosen to maintain the temperature-adjusted\nentropy around 0.3, a setting that strikes a good balance between exploration\nand exploitation. Notably, the performance gap between initial SFT models\nnarrows significantly throughout the RL process. Leveraging a strong SFT\nfoundation and insights into the synergistic interplay between SFT and RL, our\nAceReason-Nemotron-1.1 7B model significantly outperforms\nAceReason-Nemotron-1.0 and achieves new state-of-the-art performance among\nQwen2.5-7B-based reasoning models on challenging math and code benchmarks,\nthereby demonstrating the effectiveness of our post-training recipe. We release\nthe model and data at: https://huggingface.co/nvidia/AceReason-Nemotron-1.1-7B",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13284.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "65f33b1c9f7970ccc0234cbf",
            "avatarUrl": "/avatars/99fbab303912e3674663251c04279907.svg",
            "fullname": "Zihan Liu",
            "name": "zihanliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.12450",
            "authors": [
                {
                    "_id": "6850dfc85e07650ecce890e7",
                    "user": {
                        "_id": "61728a033edf4cc38a81237a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652231681579-61728a033edf4cc38a81237a.jpeg",
                        "isPro": false,
                        "fullname": "Joanito Agili Lopo",
                        "user": "joanitolopo",
                        "type": "user"
                    },
                    "name": "Joanito Agili Lopo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:08.064Z",
                    "hidden": false
                },
                {
                    "_id": "6850dfc85e07650ecce890e8",
                    "user": {
                        "_id": "63ddfced5ea8577c8d5fb421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677144169806-63ddfced5ea8577c8d5fb421.jpeg",
                        "isPro": false,
                        "fullname": "Muhammad Ravi Shulthan Habibi",
                        "user": "muhammadravi251001",
                        "type": "user"
                    },
                    "name": "Muhammad Ravi Shulthan Habibi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:10.071Z",
                    "hidden": false
                },
                {
                    "_id": "6850dfc85e07650ecce890e9",
                    "user": {
                        "_id": "65a378339b0ac6aafca9bb9c",
                        "avatarUrl": "/avatars/d5e8c2714f025adfe1487384664ddff6.svg",
                        "isPro": false,
                        "fullname": "wong tack hwa",
                        "user": "tackhwa",
                        "type": "user"
                    },
                    "name": "Tack Hwa Wong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:05.829Z",
                    "hidden": false
                },
                {
                    "_id": "6850dfc85e07650ecce890ea",
                    "name": "Muhammad Ilham Ghozali",
                    "hidden": false
                },
                {
                    "_id": "6850dfc85e07650ecce890eb",
                    "name": "Fajri Koto",
                    "hidden": false
                },
                {
                    "_id": "6850dfc85e07650ecce890ec",
                    "name": "Genta Indra Winata",
                    "hidden": false
                },
                {
                    "_id": "6850dfc85e07650ecce890ed",
                    "name": "Peerat Limkonchotiwat",
                    "hidden": false
                },
                {
                    "_id": "6850dfc85e07650ecce890ee",
                    "name": "Alham Fikri Aji",
                    "hidden": false
                },
                {
                    "_id": "6850dfc85e07650ecce890ef",
                    "user": {
                        "_id": "66f1af390ae00cd951861005",
                        "avatarUrl": "/avatars/eeab3bf515e911c3250f99a1a73d43d3.svg",
                        "isPro": false,
                        "fullname": "Samuel Cahyawijaya",
                        "user": "samuel-cahyawijaya",
                        "type": "user"
                    },
                    "name": "Samuel Cahyawijaya",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-17T03:23:52.870Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-14T11:09:50.000Z",
            "submittedOnDailyAt": "2025-06-17T01:58:16.211Z",
            "title": "Language Surgery in Multilingual Large Language Models",
            "submittedOnDailyBy": {
                "_id": "61728a033edf4cc38a81237a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652231681579-61728a033edf4cc38a81237a.jpeg",
                "isPro": false,
                "fullname": "Joanito Agili Lopo",
                "user": "joanitolopo",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across tasks and languages, revolutionizing natural language\nprocessing. This paper investigates the naturally emerging representation\nalignment in LLMs, particularly in the middle layers, and its implications for\ndisentangling language-specific and language-agnostic information. We\nempirically confirm the existence of this alignment, analyze its behavior in\ncomparison to explicitly designed alignment models, and demonstrate its\npotential for language-specific manipulation without semantic degradation.\nBuilding on these findings, we propose Inference-Time Language Control (ITLC),\na novel method that leverages latent injection to enable precise cross-lingual\nlanguage control and mitigate language confusion in LLMs. Our experiments\nhighlight ITLC's strong cross-lingual control capabilities while preserving\nsemantic integrity in target languages. Furthermore, we demonstrate its\neffectiveness in alleviating the cross-lingual language confusion problem,\nwhich persists even in current large-scale LLMs, leading to inconsistent\nlanguage generation. This work advances our understanding of representation\nalignment in LLMs and introduces a practical solution for enhancing their\ncross-lingual performance.",
            "upvotes": 10,
            "discussionId": "6850dfc85e07650ecce890f0",
            "githubRepo": "https://github.com/SEACrowd/itlc",
            "ai_summary": "Research confirms natural representation alignment in large language models and introduces Inference-Time Language Control to enhance cross-lingual performance.",
            "ai_keywords": [
                "Large Language Models",
                "representation alignment",
                "middle layers",
                "language-specific information",
                "language-agnostic information",
                "explicitly designed alignment models",
                "latent injection",
                "cross-lingual language control",
                "semantic integrity",
                "language confusion"
            ]
        },
        "publishedAt": "2025-06-14T07:09:50.000Z",
        "title": "Language Surgery in Multilingual Large Language Models",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable generalization\ncapabilities across tasks and languages, revolutionizing natural language\nprocessing. This paper investigates the naturally emerging representation\nalignment in LLMs, particularly in the middle layers, and its implications for\ndisentangling language-specific and language-agnostic information. We\nempirically confirm the existence of this alignment, analyze its behavior in\ncomparison to explicitly designed alignment models, and demonstrate its\npotential for language-specific manipulation without semantic degradation.\nBuilding on these findings, we propose Inference-Time Language Control (ITLC),\na novel method that leverages latent injection to enable precise cross-lingual\nlanguage control and mitigate language confusion in LLMs. Our experiments\nhighlight ITLC's strong cross-lingual control capabilities while preserving\nsemantic integrity in target languages. Furthermore, we demonstrate its\neffectiveness in alleviating the cross-lingual language confusion problem,\nwhich persists even in current large-scale LLMs, leading to inconsistent\nlanguage generation. This work advances our understanding of representation\nalignment in LLMs and introduces a practical solution for enhancing their\ncross-lingual performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12450.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61728a033edf4cc38a81237a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1652231681579-61728a033edf4cc38a81237a.jpeg",
            "fullname": "Joanito Agili Lopo",
            "name": "joanitolopo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.07961",
            "authors": [
                {
                    "_id": "684eae5e60b4a34dbe0079ea",
                    "user": {
                        "_id": "6337e04b171879571956212f",
                        "avatarUrl": "/avatars/aa69142bf92e4c50b192463490251ed9.svg",
                        "isPro": false,
                        "fullname": "Li Peiyan",
                        "user": "LPY",
                        "type": "user"
                    },
                    "name": "Peiyan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-16T07:16:27.694Z",
                    "hidden": false
                },
                {
                    "_id": "684eae5e60b4a34dbe0079eb",
                    "name": "Yixiang Chen",
                    "hidden": false
                },
                {
                    "_id": "684eae5e60b4a34dbe0079ec",
                    "name": "Hongtao Wu",
                    "hidden": false
                },
                {
                    "_id": "684eae5e60b4a34dbe0079ed",
                    "name": "Xiao Ma",
                    "hidden": false
                },
                {
                    "_id": "684eae5e60b4a34dbe0079ee",
                    "name": "Xiangnan Wu",
                    "hidden": false
                },
                {
                    "_id": "684eae5e60b4a34dbe0079ef",
                    "name": "Yan Huang",
                    "hidden": false
                },
                {
                    "_id": "684eae5e60b4a34dbe0079f0",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "684eae5e60b4a34dbe0079f1",
                    "name": "Tao Kong",
                    "hidden": false
                },
                {
                    "_id": "684eae5e60b4a34dbe0079f2",
                    "name": "Tieniu Tan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6337e04b171879571956212f/-CQCtJRF01UYHt7QmTwPl.mp4"
            ],
            "publishedAt": "2025-06-09T17:36:34.000Z",
            "submittedOnDailyAt": "2025-06-17T00:45:03.925Z",
            "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "6337e04b171879571956212f",
                "avatarUrl": "/avatars/aa69142bf92e4c50b192463490251ed9.svg",
                "isPro": false,
                "fullname": "Li Peiyan",
                "user": "LPY",
                "type": "user"
            },
            "summary": "Recently, leveraging pre-trained vision-language models (VLMs) for building\nvision-language-action (VLA) models has emerged as a promising approach to\neffective robot manipulation learning. However, only few methods incorporate 3D\nsignals into VLMs for action prediction, and they do not fully leverage the\nspatial structure inherent in 3D data, leading to low sample efficiency. In\nthis paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D\ninputs to multiple 2D images, ensuring input alignment with the VLM backbone,\nand (2) utilizes 2D heatmaps for action prediction, unifying the input and\noutput spaces within a consistent 2D image space. In addition, we propose a\nscalable pre-training method that equips the VLM backbone with the capability\nto predict 2D heatmaps before downstream policy learning. Extensive experiments\nshow the proposed method is able to learn 3D manipulation efficiently and\neffectively. BridgeVLA outperforms state-of-the-art baseline methods across\nthree simulation benchmarks. In RLBench, it improves the average success rate\nfrom 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better\nperformance in challenging generalization settings, boosting the average\nsuccess rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing\nbaseline methods in terms of average success rate. In real-robot experiments,\nBridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It\ngeneralizes robustly in multiple out-of-distribution settings, including visual\ndisturbances and unseen instructions. Remarkably, it is able to achieve a\nsuccess rate of 96.8% on 10+ tasks with only 3 trajectories per task,\nhighlighting its extraordinary sample efficiency. Project\nWebsite:https://bridgevla.github.io/",
            "upvotes": 10,
            "discussionId": "684eae5e60b4a34dbe0079f3",
            "projectPage": "https://bridgevla.github.io/",
            "githubRepo": "https://github.com/BridgeVLA/BridgeVLA",
            "ai_summary": "BridgeVLA is a 3D vision-language-action model that projects 3D inputs to 2D images and uses 2D heatmaps for efficient and effective action prediction, outperforming baselines in various benchmarks.",
            "ai_keywords": [
                "pre-trained vision-language models",
                "3D VLA model",
                "3D signals",
                "2D images",
                "VLM backbone",
                "2D heatmaps",
                "scalable pre-training method"
            ]
        },
        "publishedAt": "2025-06-09T13:36:34.000Z",
        "title": "BridgeVLA: Input-Output Alignment for Efficient 3D Manipulation Learning\n  with Vision-Language Models",
        "summary": "Recently, leveraging pre-trained vision-language models (VLMs) for building\nvision-language-action (VLA) models has emerged as a promising approach to\neffective robot manipulation learning. However, only few methods incorporate 3D\nsignals into VLMs for action prediction, and they do not fully leverage the\nspatial structure inherent in 3D data, leading to low sample efficiency. In\nthis paper, we introduce BridgeVLA, a novel 3D VLA model that (1) projects 3D\ninputs to multiple 2D images, ensuring input alignment with the VLM backbone,\nand (2) utilizes 2D heatmaps for action prediction, unifying the input and\noutput spaces within a consistent 2D image space. In addition, we propose a\nscalable pre-training method that equips the VLM backbone with the capability\nto predict 2D heatmaps before downstream policy learning. Extensive experiments\nshow the proposed method is able to learn 3D manipulation efficiently and\neffectively. BridgeVLA outperforms state-of-the-art baseline methods across\nthree simulation benchmarks. In RLBench, it improves the average success rate\nfrom 81.4% to 88.2%. In COLOSSEUM, it demonstrates significantly better\nperformance in challenging generalization settings, boosting the average\nsuccess rate from 56.7% to 64.0%. In GemBench, it surpasses all the comparing\nbaseline methods in terms of average success rate. In real-robot experiments,\nBridgeVLA outperforms a state-of-the-art baseline method by 32% on average. It\ngeneralizes robustly in multiple out-of-distribution settings, including visual\ndisturbances and unseen instructions. Remarkably, it is able to achieve a\nsuccess rate of 96.8% on 10+ tasks with only 3 trajectories per task,\nhighlighting its extraordinary sample efficiency. Project\nWebsite:https://bridgevla.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6337e04b171879571956212f/-CQCtJRF01UYHt7QmTwPl.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.07961.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6337e04b171879571956212f",
            "avatarUrl": "/avatars/aa69142bf92e4c50b192463490251ed9.svg",
            "fullname": "Li Peiyan",
            "name": "LPY",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.06366",
            "authors": [
                {
                    "_id": "684ae229dbd21a9cc27b1099",
                    "name": "Lin Chen",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b109a",
                    "name": "Yunke Zhang",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b109b",
                    "user": {
                        "_id": "6465d3bd63e7e09dd02e95c3",
                        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                        "isPro": false,
                        "fullname": "Jie Feng",
                        "user": "JJ-TMT",
                        "type": "user"
                    },
                    "name": "Jie Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-15T07:03:58.220Z",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b109c",
                    "name": "Haoye Chai",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b109d",
                    "name": "Honglin Zhang",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b109e",
                    "name": "Bingbing Fan",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b109f",
                    "name": "Yibo Ma",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b10a0",
                    "name": "Shiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b10a1",
                    "name": "Nian Li",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b10a2",
                    "name": "Tianhui Liu",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b10a3",
                    "name": "Nicholas Sukiennik",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b10a4",
                    "name": "Keyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b10a5",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b10a6",
                    "name": "Ziyi Liu",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b10a7",
                    "name": "Fengli Xu",
                    "hidden": false
                },
                {
                    "_id": "684ae229dbd21a9cc27b10a8",
                    "name": "Yong Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/Q7lRl1w4-YqKvGsijYryV.jpeg"
            ],
            "publishedAt": "2025-06-04T08:12:32.000Z",
            "submittedOnDailyAt": "2025-06-17T02:23:25.961Z",
            "title": "AI Agent Behavioral Science",
            "submittedOnDailyBy": {
                "_id": "6465d3bd63e7e09dd02e95c3",
                "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                "isPro": false,
                "fullname": "Jie Feng",
                "user": "JJ-TMT",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have enabled the development\nof AI agents that exhibit increasingly human-like behaviors, including\nplanning, adaptation, and social dynamics across diverse, interactive, and\nopen-ended scenarios. These behaviors are not solely the product of the\ninternal architectures of the underlying models, but emerge from their\nintegration into agentic systems operating within specific contexts, where\nenvironmental factors, social cues, and interaction feedbacks shape behavior\nover time. This evolution necessitates a new scientific perspective: AI Agent\nBehavioral Science. Rather than focusing only on internal mechanisms, this\nperspective emphasizes the systematic observation of behavior, design of\ninterventions to test hypotheses, and theory-guided interpretation of how AI\nagents act, adapt, and interact over time. We systematize a growing body of\nresearch across individual agent, multi-agent, and human-agent interaction\nsettings, and further demonstrate how this perspective informs responsible AI\nby treating fairness, safety, interpretability, accountability, and privacy as\nbehavioral properties. By unifying recent findings and laying out future\ndirections, we position AI Agent Behavioral Science as a necessary complement\nto traditional model-centric approaches, providing essential tools for\nunderstanding, evaluating, and governing the real-world behavior of\nincreasingly autonomous AI systems.",
            "upvotes": 7,
            "discussionId": "684ae229dbd21a9cc27b10a9",
            "ai_summary": "A new field, AI Agent Behavioral Science, is proposed to systematically study the behaviors of AI agents in diverse contexts, emphasizing external factors and their interactions, and addressing responsible AI aspects.",
            "ai_keywords": [
                "large language models",
                "AI agents",
                "planning",
                "adaptation",
                "social dynamics",
                "internal architectures",
                "agentic systems",
                "AI Agent Behavioral Science",
                "individual agent",
                "multi-agent",
                "human-agent interaction",
                "fairness",
                "safety",
                "interpretability",
                "accountability",
                "privacy"
            ]
        },
        "publishedAt": "2025-06-04T04:12:32.000Z",
        "title": "AI Agent Behavioral Science",
        "summary": "Recent advances in large language models (LLMs) have enabled the development\nof AI agents that exhibit increasingly human-like behaviors, including\nplanning, adaptation, and social dynamics across diverse, interactive, and\nopen-ended scenarios. These behaviors are not solely the product of the\ninternal architectures of the underlying models, but emerge from their\nintegration into agentic systems operating within specific contexts, where\nenvironmental factors, social cues, and interaction feedbacks shape behavior\nover time. This evolution necessitates a new scientific perspective: AI Agent\nBehavioral Science. Rather than focusing only on internal mechanisms, this\nperspective emphasizes the systematic observation of behavior, design of\ninterventions to test hypotheses, and theory-guided interpretation of how AI\nagents act, adapt, and interact over time. We systematize a growing body of\nresearch across individual agent, multi-agent, and human-agent interaction\nsettings, and further demonstrate how this perspective informs responsible AI\nby treating fairness, safety, interpretability, accountability, and privacy as\nbehavioral properties. By unifying recent findings and laying out future\ndirections, we position AI Agent Behavioral Science as a necessary complement\nto traditional model-centric approaches, providing essential tools for\nunderstanding, evaluating, and governing the real-world behavior of\nincreasingly autonomous AI systems.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/Q7lRl1w4-YqKvGsijYryV.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06366.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "fullname": "Jie Feng",
            "name": "JJ-TMT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13404",
            "authors": [
                {
                    "_id": "68516d328a68fee7f6ba4ccc",
                    "user": {
                        "_id": "6575a625b951d40e7a4d8685",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
                        "isPro": false,
                        "fullname": "zhuangxialie",
                        "user": "ZhuangXialie",
                        "type": "user"
                    },
                    "name": "Xialie Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T15:13:08.213Z",
                    "hidden": false
                },
                {
                    "_id": "68516d328a68fee7f6ba4ccd",
                    "user": {
                        "_id": "663e29ddc0ce43e121c80f92",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663e29ddc0ce43e121c80f92/jEAQa2oLCRUpiFEhTy0nK.jpeg",
                        "isPro": false,
                        "fullname": "Peixian Ma",
                        "user": "MPX0222forHF",
                        "type": "user"
                    },
                    "name": "Peixian Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T15:13:06.442Z",
                    "hidden": false
                },
                {
                    "_id": "68516d328a68fee7f6ba4cce",
                    "name": "Zhikai Jia",
                    "hidden": false
                },
                {
                    "_id": "68516d328a68fee7f6ba4ccf",
                    "name": "Zheng Cao",
                    "hidden": false
                },
                {
                    "_id": "68516d328a68fee7f6ba4cd0",
                    "name": "Shiwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T12:18:11.000Z",
            "submittedOnDailyAt": "2025-06-17T11:58:19.511Z",
            "title": "A Technical Study into Small Reasoning Language Models",
            "submittedOnDailyBy": {
                "_id": "6575a625b951d40e7a4d8685",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
                "isPro": false,
                "fullname": "zhuangxialie",
                "user": "ZhuangXialie",
                "type": "user"
            },
            "summary": "The ongoing evolution of language models has led to the development of\nlarge-scale architectures that demonstrate exceptional performance across a\nwide range of tasks. However, these models come with significant computational\nand energy demands, as well as potential privacy implications. In this context,\nSmall Reasoning Language Models (SRLMs) with approximately 0.5 billion\nparameters present a compelling alternative due to their remarkable\ncomputational efficiency and cost effectiveness, particularly in\nresource-constrained environments. Despite these advantages, the limited\ncapacity of 0.5 billion parameter models poses challenges in handling complex\ntasks such as mathematical reasoning and code generation. This research\ninvestigates various training strategies, including supervised fine-tuning\n(SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as\ntheir hybrid implementations, to enhance the performance of 0.5B SRLMs. We\nanalyze effective methodologies to bridge the performance gap between SRLMS and\nlarger models and present insights into optimal training pipelines tailored for\nthese smaller architectures. Through extensive experimental validation and\nanalysis, our work aims to provide actionable recommendations for maximizing\nthe reasoning capabilities of 0.5B models.",
            "upvotes": 6,
            "discussionId": "68516d338a68fee7f6ba4cd1",
            "ai_summary": "The research explores training strategies such as supervised fine-tuning, knowledge distillation, and reinforcement learning to enhance the performance of resource-efficient Small Reasoning Language Models with limited capacity.",
            "ai_keywords": [
                "supervised fine-tuning",
                "knowledge distillation",
                "reinforcement learning"
            ]
        },
        "publishedAt": "2025-06-16T08:18:11.000Z",
        "title": "A Technical Study into Small Reasoning Language Models",
        "summary": "The ongoing evolution of language models has led to the development of\nlarge-scale architectures that demonstrate exceptional performance across a\nwide range of tasks. However, these models come with significant computational\nand energy demands, as well as potential privacy implications. In this context,\nSmall Reasoning Language Models (SRLMs) with approximately 0.5 billion\nparameters present a compelling alternative due to their remarkable\ncomputational efficiency and cost effectiveness, particularly in\nresource-constrained environments. Despite these advantages, the limited\ncapacity of 0.5 billion parameter models poses challenges in handling complex\ntasks such as mathematical reasoning and code generation. This research\ninvestigates various training strategies, including supervised fine-tuning\n(SFT), knowledge distillation (KD), and reinforcement learning (RL), as well as\ntheir hybrid implementations, to enhance the performance of 0.5B SRLMs. We\nanalyze effective methodologies to bridge the performance gap between SRLMS and\nlarger models and present insights into optimal training pipelines tailored for\nthese smaller architectures. Through extensive experimental validation and\nanalysis, our work aims to provide actionable recommendations for maximizing\nthe reasoning capabilities of 0.5B models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13404.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6575a625b951d40e7a4d8685",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6575a625b951d40e7a4d8685/mcnyqU--r-vi11aXN02cZ.jpeg",
            "fullname": "zhuangxialie",
            "name": "ZhuangXialie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09050",
            "authors": [
                {
                    "_id": "684909e942e4f9106973f386",
                    "name": "Yuki Imajuku",
                    "hidden": false
                },
                {
                    "_id": "684909e942e4f9106973f387",
                    "name": "Kohki Horie",
                    "hidden": false
                },
                {
                    "_id": "684909e942e4f9106973f388",
                    "name": "Yoichi Iwata",
                    "hidden": false
                },
                {
                    "_id": "684909e942e4f9106973f389",
                    "name": "Kensho Aoki",
                    "hidden": false
                },
                {
                    "_id": "684909e942e4f9106973f38a",
                    "name": "Naohiro Takahashi",
                    "hidden": false
                },
                {
                    "_id": "684909e942e4f9106973f38b",
                    "user": {
                        "_id": "6482810dba6c556892f6f257",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6482810dba6c556892f6f257/c7-wiVKenXiRtwnRpnjZN.jpeg",
                        "isPro": false,
                        "fullname": "Takuya Akiba",
                        "user": "iwiwi",
                        "type": "user"
                    },
                    "name": "Takuya Akiba",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:23:16.335Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6482810dba6c556892f6f257/J5fdxZ7P_40qJ4qJPqPcI.png"
            ],
            "publishedAt": "2025-06-10T17:59:56.000Z",
            "submittedOnDailyAt": "2025-06-17T04:32:07.528Z",
            "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering",
            "submittedOnDailyBy": {
                "_id": "6482810dba6c556892f6f257",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6482810dba6c556892f6f257/c7-wiVKenXiRtwnRpnjZN.jpeg",
                "isPro": false,
                "fullname": "Takuya Akiba",
                "user": "iwiwi",
                "type": "user"
            },
            "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.",
            "upvotes": 6,
            "discussionId": "684909ea42e4f9106973f38c",
            "githubRepo": "https://github.com/SakanaAI/ALE-Bench",
            "ai_summary": "ALE-Bench evaluates AI systems on score-based algorithmic programming contests drawn from AtCoder, focusing on long-term iterative problem-solving in domains like package-delivery routing, crew scheduling, factory production, and power-grid balancing.",
            "ai_keywords": [
                "algorithm engineering",
                "optimization problems",
                "ALE-Bench",
                "AtCoder Heuristic Contests",
                "interactive agent architectures",
                "long-horizon problem-solving",
                "frontier LLMs"
            ]
        },
        "publishedAt": "2025-06-10T13:59:56.000Z",
        "title": "ALE-Bench: A Benchmark for Long-Horizon Objective-Driven Algorithm\n  Engineering",
        "summary": "How well do AI systems perform in algorithm engineering for hard optimization\nproblems in domains such as package-delivery routing, crew scheduling, factory\nproduction planning, and power-grid balancing? We introduce ALE-Bench, a new\nbenchmark for evaluating AI systems on score-based algorithmic programming\ncontests. Drawing on real tasks from the AtCoder Heuristic Contests, ALE-Bench\npresents optimization problems that are computationally hard and admit no known\nexact solution. Unlike short-duration, pass/fail coding benchmarks, ALE-Bench\nencourages iterative solution refinement over long time horizons. Our software\nframework supports interactive agent architectures that leverage test-run\nfeedback and visualizations. Our evaluation of frontier LLMs revealed that\nwhile they demonstrate high performance on specific problems, a notable gap\nremains compared to humans in terms of consistency across problems and\nlong-horizon problem-solving capabilities. This highlights the need for this\nbenchmark to foster future AI advancements.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6482810dba6c556892f6f257/J5fdxZ7P_40qJ4qJPqPcI.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09050.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6482810dba6c556892f6f257",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6482810dba6c556892f6f257/c7-wiVKenXiRtwnRpnjZN.jpeg",
            "fullname": "Takuya Akiba",
            "name": "iwiwi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10341",
            "authors": [
                {
                    "_id": "684c9be23b733ba3336872f5",
                    "name": "Wanqiao Xu",
                    "hidden": false
                },
                {
                    "_id": "684c9be23b733ba3336872f6",
                    "name": "Allen Nie",
                    "hidden": false
                },
                {
                    "_id": "684c9be23b733ba3336872f7",
                    "name": "Ruijie Zheng",
                    "hidden": false
                },
                {
                    "_id": "684c9be23b733ba3336872f8",
                    "name": "Aditya Modi",
                    "hidden": false
                },
                {
                    "_id": "684c9be23b733ba3336872f9",
                    "name": "Adith Swaminathan",
                    "hidden": false
                },
                {
                    "_id": "684c9be23b733ba3336872fa",
                    "name": "Ching-An Cheng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/649466d90560480110a72247/rLRH1sns6x30CkPHRdTWP.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/649466d90560480110a72247/uqZJHK1E6OiM3f-5eICnM.jpeg"
            ],
            "publishedAt": "2025-06-12T04:35:02.000Z",
            "submittedOnDailyAt": "2025-06-17T19:22:55.293Z",
            "title": "Provably Learning from Language Feedback",
            "submittedOnDailyBy": {
                "_id": "649466d90560480110a72247",
                "avatarUrl": "/avatars/79fc060693328f4345c01adef4aba17c.svg",
                "isPro": false,
                "fullname": "Allen Nie",
                "user": "allenanie",
                "type": "user"
            },
            "summary": "Interactively learning from observation and language feedback is an\nincreasingly studied area driven by the emergence of large language model (LLM)\nagents. While impressive empirical demonstrations have been shown, so far a\nprincipled framing of these decision problems remains lacking. In this paper,\nwe formalize the Learning from Language Feedback (LLF) problem, assert\nsufficient assumptions to enable learning despite latent rewards, and introduce\ntransfer eluder dimension as a complexity measure to characterize\nthe hardness of LLF problems. We show that transfer eluder dimension captures\nthe intuition that information in the feedback changes the learning complexity\nof the LLF problem. We demonstrate cases where learning from rich language\nfeedback can be exponentially faster than learning from reward. We develop a\nno-regret algorithm, called HELiX, that provably solves LLF problems\nthrough sequential interactions, with performance guarantees that scale with\nthe transfer eluder dimension of the problem. Across several empirical domains,\nwe show that HELiX performs well even when repeatedly prompting LLMs\ndoes not work reliably. Our contributions mark a first step towards designing\nprincipled interactive learning algorithms from generic language feedback.",
            "upvotes": 5,
            "discussionId": "684c9be33b733ba3336872fb",
            "ai_summary": "A formal framework and no-regret algorithm are introduced for learning from language feedback, addressing challenges in interactive learning with large language models.",
            "ai_keywords": [
                "Learning from Language Feedback (LLF)",
                "large language model (LLM) agents",
                "latent rewards",
                "transfer eluder dimension",
                "no-regret algorithm",
                "HELiX"
            ]
        },
        "publishedAt": "2025-06-12T00:35:02.000Z",
        "title": "Provably Learning from Language Feedback",
        "summary": "Interactively learning from observation and language feedback is an\nincreasingly studied area driven by the emergence of large language model (LLM)\nagents. While impressive empirical demonstrations have been shown, so far a\nprincipled framing of these decision problems remains lacking. In this paper,\nwe formalize the Learning from Language Feedback (LLF) problem, assert\nsufficient assumptions to enable learning despite latent rewards, and introduce\ntransfer eluder dimension as a complexity measure to characterize\nthe hardness of LLF problems. We show that transfer eluder dimension captures\nthe intuition that information in the feedback changes the learning complexity\nof the LLF problem. We demonstrate cases where learning from rich language\nfeedback can be exponentially faster than learning from reward. We develop a\nno-regret algorithm, called HELiX, that provably solves LLF problems\nthrough sequential interactions, with performance guarantees that scale with\nthe transfer eluder dimension of the problem. Across several empirical domains,\nwe show that HELiX performs well even when repeatedly prompting LLMs\ndoes not work reliably. Our contributions mark a first step towards designing\nprincipled interactive learning algorithms from generic language feedback.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/649466d90560480110a72247/rLRH1sns6x30CkPHRdTWP.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/649466d90560480110a72247/uqZJHK1E6OiM3f-5eICnM.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10341.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649466d90560480110a72247",
            "avatarUrl": "/avatars/79fc060693328f4345c01adef4aba17c.svg",
            "fullname": "Allen Nie",
            "name": "allenanie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.06454",
            "authors": [
                {
                    "_id": "6848ed6142e4f9106973f2b7",
                    "user": {
                        "_id": "647e61c2e4d52fe0e0205d94",
                        "avatarUrl": "/avatars/d7a2327ab10494fb471496e85eed8ff0.svg",
                        "isPro": false,
                        "fullname": "Alpha Omega",
                        "user": "alphaomeaga",
                        "type": "user"
                    },
                    "name": "Abrar Majeedi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-13T07:41:53.817Z",
                    "hidden": false
                },
                {
                    "_id": "6848ed6142e4f9106973f2b8",
                    "user": {
                        "_id": "658708e06b17c068728f436a",
                        "avatarUrl": "/avatars/b51f36d1bce76c195350ff6e523bb036.svg",
                        "isPro": false,
                        "fullname": "Viswa",
                        "user": "viswa-98",
                        "type": "user"
                    },
                    "name": "Viswanatha Reddy Gajjala",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-11T08:35:02.871Z",
                    "hidden": false
                },
                {
                    "_id": "6848ed6142e4f9106973f2b9",
                    "name": "Satya Sai Srinath Namburi GNVV",
                    "hidden": false
                },
                {
                    "_id": "6848ed6142e4f9106973f2ba",
                    "name": "Nada Magdi Elkordi",
                    "hidden": false
                },
                {
                    "_id": "6848ed6142e4f9106973f2bb",
                    "name": "Yin Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/658708e06b17c068728f436a/CeZkFqf8ZE6bTIgIAyoFS.png"
            ],
            "publishedAt": "2025-06-06T18:24:12.000Z",
            "submittedOnDailyAt": "2025-06-17T05:37:39.544Z",
            "title": "LETS Forecast: Learning Embedology for Time Series Forecasting",
            "submittedOnDailyBy": {
                "_id": "658708e06b17c068728f436a",
                "avatarUrl": "/avatars/b51f36d1bce76c195350ff6e523bb036.svg",
                "isPro": false,
                "fullname": "Viswa",
                "user": "viswa-98",
                "type": "user"
            },
            "summary": "Real-world time series are often governed by complex nonlinear dynamics.\nUnderstanding these underlying dynamics is crucial for precise future\nprediction. While deep learning has achieved major success in time series\nforecasting, many existing approaches do not explicitly model the dynamics. To\nbridge this gap, we introduce DeepEDM, a framework that integrates nonlinear\ndynamical systems modeling with deep neural networks. Inspired by empirical\ndynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel\ndeep model that learns a latent space from time-delayed embeddings, and employs\nkernel regression to approximate the underlying dynamics, while leveraging\nefficient implementation of softmax attention and allowing for accurate\nprediction of future time steps. To evaluate our method, we conduct\ncomprehensive experiments on synthetic data of nonlinear dynamical systems as\nwell as real-world time series across domains. Our results show that DeepEDM is\nrobust to input noise, and outperforms state-of-the-art methods in forecasting\naccuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.",
            "upvotes": 4,
            "discussionId": "6848ed6242e4f9106973f2bc",
            "projectPage": "https://abrarmajeedi.github.io/deep_edm/",
            "githubRepo": "https://github.com/abrarmajeedi/DeepEDM",
            "ai_summary": "DeepEDM integrates empirical dynamic modeling with deep neural networks to learn latent spaces and approximate complex nonlinear dynamics for improved time series forecasting.",
            "ai_keywords": [
                "nonlinear dynamical systems",
                "empirical dynamic modeling",
                "EDM",
                "Takens' theorem",
                "latent space",
                "time-delayed embeddings",
                "kernel regression",
                "softmax attention",
                "time series forecasting"
            ]
        },
        "publishedAt": "2025-06-06T14:24:12.000Z",
        "title": "LETS Forecast: Learning Embedology for Time Series Forecasting",
        "summary": "Real-world time series are often governed by complex nonlinear dynamics.\nUnderstanding these underlying dynamics is crucial for precise future\nprediction. While deep learning has achieved major success in time series\nforecasting, many existing approaches do not explicitly model the dynamics. To\nbridge this gap, we introduce DeepEDM, a framework that integrates nonlinear\ndynamical systems modeling with deep neural networks. Inspired by empirical\ndynamic modeling (EDM) and rooted in Takens' theorem, DeepEDM presents a novel\ndeep model that learns a latent space from time-delayed embeddings, and employs\nkernel regression to approximate the underlying dynamics, while leveraging\nefficient implementation of softmax attention and allowing for accurate\nprediction of future time steps. To evaluate our method, we conduct\ncomprehensive experiments on synthetic data of nonlinear dynamical systems as\nwell as real-world time series across domains. Our results show that DeepEDM is\nrobust to input noise, and outperforms state-of-the-art methods in forecasting\naccuracy. Our code is available at: https://abrarmajeedi.github.io/deep_edm.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/658708e06b17c068728f436a/CeZkFqf8ZE6bTIgIAyoFS.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06454.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "658708e06b17c068728f436a",
            "avatarUrl": "/avatars/b51f36d1bce76c195350ff6e523bb036.svg",
            "fullname": "Viswa",
            "name": "viswa-98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.12189",
            "authors": [
                {
                    "_id": "6850cb005e07650ecce88fc9",
                    "user": {
                        "_id": "64b89f096c57038f205a7751",
                        "avatarUrl": "/avatars/c268578685cf5b9f0e37ffbdcf239827.svg",
                        "isPro": false,
                        "fullname": "Pranav Agarwal",
                        "user": "pranavAL2109",
                        "type": "user"
                    },
                    "name": "Pranav Agarwal",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-17T01:55:41.273Z",
                    "hidden": false
                },
                {
                    "_id": "6850cb005e07650ecce88fca",
                    "name": "Ioana Ciucă",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T19:31:52.000Z",
            "submittedOnDailyAt": "2025-06-17T00:35:05.037Z",
            "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis",
            "submittedOnDailyBy": {
                "_id": "64b89f096c57038f205a7751",
                "avatarUrl": "/avatars/c268578685cf5b9f0e37ffbdcf239827.svg",
                "isPro": false,
                "fullname": "Pranav Agarwal",
                "user": "pranavAL2109",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications. As their influence grows, understanding their decision making and\nunderlying personality becomes essential. In this work, we interpret model\npersonality using our proposed Supernova Event Dataset, a novel dataset with\ndiverse articles spanning biographies, historical events, news, and scientific\ndiscoveries. We use this dataset to benchmark LLMs on extracting and ranking\nkey events from text, a subjective and complex challenge that requires\nreasoning over long-range context and modeling causal chains. We evaluate small\nmodels like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as\nClaude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another\nLLM acts as a judge to infer each model's personality based on its selection\nand classification of events. Our analysis shows distinct personality traits:\nfor instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal\ndynamics, while Qwen 2.5 displays a more strategic, analytical style. When\nanalyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual\nframing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors\nstep-by-step causal reasoning. This analysis improves model interpretability,\nmaking them user-friendly for a wide range of diverse applications.",
            "upvotes": 3,
            "discussionId": "6850cb005e07650ecce88fcb",
            "projectPage": "https://supernova-event.ai/",
            "githubRepo": "https://github.com/pranavAL/Supernova-Event-Dataset",
            "ai_summary": "The study evaluates various LLMs on diverse text tasks using a new dataset, revealing distinct personality traits and improving model interpretability.",
            "ai_keywords": [
                "Supernova Event Dataset",
                "LLMs",
                "key event extraction",
                "reasoning",
                "long-range context",
                "causal chains",
                "model personality",
                "emotional reasoning",
                "strategic",
                "analytical style",
                "conceptual framing",
                "empirical validation",
                "step-by-step causal reasoning"
            ]
        },
        "publishedAt": "2025-06-13T15:31:52.000Z",
        "title": "Supernova Event Dataset: Interpreting Large Language Model's Personality\n  through Critical Event Analysis",
        "summary": "Large Language Models (LLMs) are increasingly integrated into everyday\napplications. As their influence grows, understanding their decision making and\nunderlying personality becomes essential. In this work, we interpret model\npersonality using our proposed Supernova Event Dataset, a novel dataset with\ndiverse articles spanning biographies, historical events, news, and scientific\ndiscoveries. We use this dataset to benchmark LLMs on extracting and ranking\nkey events from text, a subjective and complex challenge that requires\nreasoning over long-range context and modeling causal chains. We evaluate small\nmodels like Phi-4, Orca 2, and Qwen 2.5, and large, stronger models such as\nClaude 3.7, Gemini 2.5, and OpenAI o3, and propose a framework where another\nLLM acts as a judge to infer each model's personality based on its selection\nand classification of events. Our analysis shows distinct personality traits:\nfor instance, Orca 2 demonstrates emotional reasoning focusing on interpersonal\ndynamics, while Qwen 2.5 displays a more strategic, analytical style. When\nanalyzing scientific discovery events, Claude Sonnet 3.7 emphasizes conceptual\nframing, Gemini 2.5 Pro prioritizes empirical validation, and o3 favors\nstep-by-step causal reasoning. This analysis improves model interpretability,\nmaking them user-friendly for a wide range of diverse applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12189.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b89f096c57038f205a7751",
            "avatarUrl": "/avatars/c268578685cf5b9f0e37ffbdcf239827.svg",
            "fullname": "Pranav Agarwal",
            "name": "pranavAL2109",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14202",
            "authors": [
                {
                    "_id": "685208ce0164cd131671042d",
                    "user": {
                        "_id": "60c2e7747a42b2edc5d2ccf7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c2e7747a42b2edc5d2ccf7/5GdUBW1HYEy17orItOqfV.png",
                        "isPro": false,
                        "fullname": "Makoto Shing",
                        "user": "mkshing",
                        "type": "user"
                    },
                    "name": "Makoto Shing",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-18T00:32:08.632Z",
                    "hidden": false
                },
                {
                    "_id": "685208ce0164cd131671042e",
                    "user": {
                        "_id": "6482810dba6c556892f6f257",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6482810dba6c556892f6f257/c7-wiVKenXiRtwnRpnjZN.jpeg",
                        "isPro": false,
                        "fullname": "Takuya Akiba",
                        "user": "iwiwi",
                        "type": "user"
                    },
                    "name": "Takuya Akiba",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-18T00:44:22.068Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/60c2e7747a42b2edc5d2ccf7/cIBi6gXEbFsQYAW04Mxo4.png"
            ],
            "publishedAt": "2025-06-17T05:44:18.000Z",
            "submittedOnDailyAt": "2025-06-17T23:03:29.051Z",
            "title": "DiffusionBlocks: Blockwise Training for Generative Models via\n  Score-Based Diffusion",
            "submittedOnDailyBy": {
                "_id": "60c2e7747a42b2edc5d2ccf7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c2e7747a42b2edc5d2ccf7/5GdUBW1HYEy17orItOqfV.png",
                "isPro": false,
                "fullname": "Makoto Shing",
                "user": "mkshing",
                "type": "user"
            },
            "summary": "Training large neural networks with end-to-end backpropagation creates\nsignificant memory bottlenecks, limiting accessibility to state-of-the-art AI\nresearch. We propose DiffusionBlocks, a novel training framework\nthat interprets neural network blocks as performing denoising operations in a\ncontinuous-time diffusion process. By partitioning the network into\nindependently trainable blocks and optimizing noise level assignments based on\nequal cumulative probability mass, our approach achieves significant memory\nefficiency while maintaining competitive performance compared to traditional\nbackpropagation in generative tasks. Experiments on image generation and\nlanguage modeling tasks demonstrate memory reduction proportional to the number\nof blocks while achieving superior performance. DiffusionBlocks provides a\npromising pathway for democratizing access to large-scale neural network\ntraining with limited computational resources.",
            "upvotes": 2,
            "discussionId": "685208ce0164cd131671042f",
            "ai_summary": "A novel training framework called DiffusionBlocks optimizes neural network blocks as denoising operations in a diffusion process, achieving memory efficiency and competitive performance in generative tasks.",
            "ai_keywords": [
                "DiffusionBlocks",
                "denoising operations",
                "continuous-time diffusion process",
                "independently trainable blocks",
                "equal cumulative probability mass",
                "memory efficiency",
                "generative tasks",
                "image generation",
                "language modeling"
            ]
        },
        "publishedAt": "2025-06-17T01:44:18.000Z",
        "title": "DiffusionBlocks: Blockwise Training for Generative Models via\n  Score-Based Diffusion",
        "summary": "Training large neural networks with end-to-end backpropagation creates\nsignificant memory bottlenecks, limiting accessibility to state-of-the-art AI\nresearch. We propose DiffusionBlocks, a novel training framework\nthat interprets neural network blocks as performing denoising operations in a\ncontinuous-time diffusion process. By partitioning the network into\nindependently trainable blocks and optimizing noise level assignments based on\nequal cumulative probability mass, our approach achieves significant memory\nefficiency while maintaining competitive performance compared to traditional\nbackpropagation in generative tasks. Experiments on image generation and\nlanguage modeling tasks demonstrate memory reduction proportional to the number\nof blocks while achieving superior performance. DiffusionBlocks provides a\npromising pathway for democratizing access to large-scale neural network\ntraining with limited computational resources.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60c2e7747a42b2edc5d2ccf7/cIBi6gXEbFsQYAW04Mxo4.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14202.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60c2e7747a42b2edc5d2ccf7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60c2e7747a42b2edc5d2ccf7/5GdUBW1HYEy17orItOqfV.png",
            "fullname": "Makoto Shing",
            "name": "mkshing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13752",
            "authors": [
                {
                    "_id": "6850e1645e07650ecce890fa",
                    "name": "Junyan Li",
                    "hidden": false
                },
                {
                    "_id": "6850e1645e07650ecce890fb",
                    "name": "Wenshuo Zhao",
                    "hidden": false
                },
                {
                    "_id": "6850e1645e07650ecce890fc",
                    "name": "Yang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850e1645e07650ecce890fd",
                    "name": "Chuang Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T17:57:05.000Z",
            "submittedOnDailyAt": "2025-06-17T02:01:57.062Z",
            "title": "Steering LLM Thinking with Budget Guidance",
            "submittedOnDailyBy": {
                "_id": "62d09eb86a61a88ea0d83918",
                "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
                "isPro": false,
                "fullname": "Junyan Li",
                "user": "senfu",
                "type": "user"
            },
            "summary": "Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget without requiring any LLM\nfine-tuning. Our approach introduces a lightweight predictor that models a\nGamma distribution over the remaining thinking length during next-token\ngeneration. This signal is then used to guide generation in a soft, token-level\nmanner, ensuring that the overall reasoning trace adheres to the specified\nthinking budget. Budget guidance enables natural control of the thinking\nlength, along with significant token efficiency improvements over baseline\nmethods on challenging math benchmarks. For instance, it achieves up to a 26%\naccuracy gain on the MATH-500 benchmark under tight budgets compared to\nbaseline methods, while maintaining competitive accuracy with only 63% of the\nthinking tokens used by the full-thinking model. Budget guidance also\ngeneralizes to broader task domains and exhibits emergent capabilities, such as\nestimating question difficulty. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/BudgetGuidance.",
            "upvotes": 2,
            "discussionId": "6850e1655e07650ecce890fe",
            "ai_summary": "Budget guidance is a method that steers LLM reasoning within a targeted budget without fine-tuning and achieves improved efficiency and performance on math benchmarks.",
            "ai_keywords": [
                "deep-thinking large language models",
                "next-token generation",
                "Gamma distribution",
                "budget guidance",
                "thinking budget",
                "token efficiency",
                "natural control",
                "question difficulty estimation"
            ]
        },
        "publishedAt": "2025-06-16T13:57:05.000Z",
        "title": "Steering LLM Thinking with Budget Guidance",
        "summary": "Recent deep-thinking large language models often reason extensively to\nimprove performance, but such lengthy reasoning is not always desirable, as it\nincurs excessive inference costs with disproportionate performance gains.\nControlling reasoning length without sacrificing performance is therefore\nimportant, but remains challenging, especially under tight thinking budgets. We\npropose budget guidance, a simple yet effective method for steering the\nreasoning process of LLMs toward a target budget without requiring any LLM\nfine-tuning. Our approach introduces a lightweight predictor that models a\nGamma distribution over the remaining thinking length during next-token\ngeneration. This signal is then used to guide generation in a soft, token-level\nmanner, ensuring that the overall reasoning trace adheres to the specified\nthinking budget. Budget guidance enables natural control of the thinking\nlength, along with significant token efficiency improvements over baseline\nmethods on challenging math benchmarks. For instance, it achieves up to a 26%\naccuracy gain on the MATH-500 benchmark under tight budgets compared to\nbaseline methods, while maintaining competitive accuracy with only 63% of the\nthinking tokens used by the full-thinking model. Budget guidance also\ngeneralizes to broader task domains and exhibits emergent capabilities, such as\nestimating question difficulty. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/BudgetGuidance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13752.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62d09eb86a61a88ea0d83918",
            "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
            "fullname": "Junyan Li",
            "name": "senfu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.12953",
            "authors": [
                {
                    "_id": "6850d2345e07650ecce89092",
                    "name": "Mayank Bumb",
                    "hidden": false
                },
                {
                    "_id": "6850d2345e07650ecce89093",
                    "name": "Anshul Vemulapalli",
                    "hidden": false
                },
                {
                    "_id": "6850d2345e07650ecce89094",
                    "name": "Sri Harsha Vardhan Prasad Jella",
                    "hidden": false
                },
                {
                    "_id": "6850d2345e07650ecce89095",
                    "name": "Anish Gupta",
                    "hidden": false
                },
                {
                    "_id": "6850d2345e07650ecce89096",
                    "name": "An La",
                    "hidden": false
                },
                {
                    "_id": "6850d2345e07650ecce89097",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "6850d2345e07650ecce89098",
                    "name": "Hongjie Chen",
                    "hidden": false
                },
                {
                    "_id": "6850d2345e07650ecce89099",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:22.624Z",
                    "hidden": false
                },
                {
                    "_id": "6850d2345e07650ecce8909a",
                    "name": "Nesreen K. Ahmed",
                    "hidden": false
                },
                {
                    "_id": "6850d2345e07650ecce8909b",
                    "name": "Yu Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-15T19:42:58.000Z",
            "submittedOnDailyAt": "2025-06-17T00:55:59.045Z",
            "title": "Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) have demonstrated new\npossibilities for accurate and efficient time series analysis, but prior work\noften required heavy fine-tuning and/or ignored inter-series correlations. In\nthis work, we explore simple and flexible prompt-based strategies that enable\nLLMs to perform time series forecasting without extensive retraining or the use\nof a complex external architecture. Through the exploration of specialized\nprompting methods that leverage time series decomposition, patch-based\ntokenization, and similarity-based neighbor augmentation, we find that it is\npossible to enhance LLM forecasting quality while maintaining simplicity and\nrequiring minimal preprocessing of data. To this end, we propose our own\nmethod, PatchInstruct, which enables LLMs to make precise and effective\npredictions.",
            "upvotes": 2,
            "discussionId": "6850d2355e07650ecce8909c",
            "ai_summary": "PatchInstruct enhances LLM forecasting quality through specialized prompting methods that include time series decomposition, patch-based tokenization, and similarity-based neighbor augmentation.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "time series forecasting",
                "prompt-based strategies",
                "time series decomposition",
                "patch-based tokenization",
                "similarity-based neighbor augmentation",
                "PatchInstruct"
            ]
        },
        "publishedAt": "2025-06-15T15:42:58.000Z",
        "title": "Forecasting Time Series with LLMs via Patch-Based Prompting and\n  Decomposition",
        "summary": "Recent advances in Large Language Models (LLMs) have demonstrated new\npossibilities for accurate and efficient time series analysis, but prior work\noften required heavy fine-tuning and/or ignored inter-series correlations. In\nthis work, we explore simple and flexible prompt-based strategies that enable\nLLMs to perform time series forecasting without extensive retraining or the use\nof a complex external architecture. Through the exploration of specialized\nprompting methods that leverage time series decomposition, patch-based\ntokenization, and similarity-based neighbor augmentation, we find that it is\npossible to enhance LLM forecasting quality while maintaining simplicity and\nrequiring minimal preprocessing of data. To this end, we propose our own\nmethod, PatchInstruct, which enables LLMs to make precise and effective\npredictions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12953.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.12623",
            "authors": [
                {
                    "_id": "6850d25d5e07650ecce8909e",
                    "name": "Yuan Zang",
                    "hidden": false
                },
                {
                    "_id": "6850d25d5e07650ecce8909f",
                    "name": "Hao Tan",
                    "hidden": false
                },
                {
                    "_id": "6850d25d5e07650ecce890a0",
                    "name": "Seunghyun Yoon",
                    "hidden": false
                },
                {
                    "_id": "6850d25d5e07650ecce890a1",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:21:15.407Z",
                    "hidden": false
                },
                {
                    "_id": "6850d25d5e07650ecce890a2",
                    "name": "Jiuxiang Gu",
                    "hidden": false
                },
                {
                    "_id": "6850d25d5e07650ecce890a3",
                    "name": "Kushal Kafle",
                    "hidden": false
                },
                {
                    "_id": "6850d25d5e07650ecce890a4",
                    "name": "Chen Sun",
                    "hidden": false
                },
                {
                    "_id": "6850d25d5e07650ecce890a5",
                    "name": "Trung Bui",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-14T20:39:32.000Z",
            "submittedOnDailyAt": "2025-06-17T00:56:38.375Z",
            "title": "MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "We study multi-modal summarization for instructional videos, whose goal is to\nprovide users an efficient way to learn skills in the form of text instructions\nand key video frames. We observe that existing benchmarks focus on generic\nsemantic-level video summarization, and are not suitable for providing\nstep-by-step executable instructions and illustrations, both of which are\ncrucial for instructional videos. We propose a novel benchmark for user\ninterface (UI) instructional video summarization to fill the gap. We collect a\ndataset of 2,413 UI instructional videos, which spans over 167 hours. These\nvideos are manually annotated for video segmentation, text summarization, and\nvideo summarization, which enable the comprehensive evaluations for concise and\nexecutable video summarization. We conduct extensive experiments on our\ncollected MS4UI dataset, which suggest that state-of-the-art multi-modal\nsummarization methods struggle on UI video summarization, and highlight the\nimportance of new methods for UI instructional video summarization.",
            "upvotes": 2,
            "discussionId": "6850d25d5e07650ecce890a6",
            "ai_summary": "A novel benchmark and dataset are proposed for multi-modal summarization of UI instructional videos, addressing the need for step-by-step executable instructions and key video frames.",
            "ai_keywords": [
                "multi-modal summarization",
                "instructional videos",
                "video segmentation",
                "text summarization",
                "video summarization",
                "MS4UI dataset"
            ]
        },
        "publishedAt": "2025-06-14T16:39:32.000Z",
        "title": "MS4UI: A Dataset for Multi-modal Summarization of User Interface\n  Instructional Videos",
        "summary": "We study multi-modal summarization for instructional videos, whose goal is to\nprovide users an efficient way to learn skills in the form of text instructions\nand key video frames. We observe that existing benchmarks focus on generic\nsemantic-level video summarization, and are not suitable for providing\nstep-by-step executable instructions and illustrations, both of which are\ncrucial for instructional videos. We propose a novel benchmark for user\ninterface (UI) instructional video summarization to fill the gap. We collect a\ndataset of 2,413 UI instructional videos, which spans over 167 hours. These\nvideos are manually annotated for video segmentation, text summarization, and\nvideo summarization, which enable the comprehensive evaluations for concise and\nexecutable video summarization. We conduct extensive experiments on our\ncollected MS4UI dataset, which suggest that state-of-the-art multi-modal\nsummarization methods struggle on UI video summarization, and highlight the\nimportance of new methods for UI instructional video summarization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12623.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.12552",
            "authors": [
                {
                    "_id": "6851336a8a68fee7f6ba4c0b",
                    "user": {
                        "_id": "637e8b1b66ee00bcb2468ed0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669240174964-637e8b1b66ee00bcb2468ed0.jpeg",
                        "isPro": false,
                        "fullname": "Zain",
                        "user": "zainmujahid",
                        "type": "user"
                    },
                    "name": "Zain Muhammad Mujahid",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T13:06:26.481Z",
                    "hidden": false
                },
                {
                    "_id": "6851336a8a68fee7f6ba4c0c",
                    "name": "Dilshod Azizov",
                    "hidden": false
                },
                {
                    "_id": "6851336a8a68fee7f6ba4c0d",
                    "name": "Maha Tufail Agro",
                    "hidden": false
                },
                {
                    "_id": "6851336a8a68fee7f6ba4c0e",
                    "name": "Preslav Nakov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-14T15:49:20.000Z",
            "submittedOnDailyAt": "2025-06-17T08:20:53.007Z",
            "title": "Profiling News Media for Factuality and Bias Using LLMs and the\n  Fact-Checking Methodology of Human Experts",
            "submittedOnDailyBy": {
                "_id": "637e8b1b66ee00bcb2468ed0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669240174964-637e8b1b66ee00bcb2468ed0.jpeg",
                "isPro": false,
                "fullname": "Zain",
                "user": "zainmujahid",
                "type": "user"
            },
            "summary": "In an age characterized by the proliferation of mis- and disinformation\nonline, it is critical to empower readers to understand the content they are\nreading. Important efforts in this direction rely on manual or automatic\nfact-checking, which can be challenging for emerging claims with limited\ninformation. Such scenarios can be handled by assessing the reliability and the\npolitical bias of the source of the claim, i.e., characterizing entire news\noutlets rather than individual claims or articles. This is an important but\nunderstudied research direction. While prior work has looked into linguistic\nand social contexts, we do not analyze individual articles or information in\nsocial media. Instead, we propose a novel methodology that emulates the\ncriteria that professional fact-checkers use to assess the factuality and\npolitical bias of an entire outlet. Specifically, we design a variety of\nprompts based on these criteria and elicit responses from large language models\n(LLMs), which we aggregate to make predictions. In addition to demonstrating\nsizable improvements over strong baselines via extensive experiments with\nmultiple LLMs, we provide an in-depth error analysis of the effect of media\npopularity and region on model performance. Further, we conduct an ablation\nstudy to highlight the key components of our dataset that contribute to these\nimprovements. To facilitate future research, we released our dataset and code\nat https://github.com/mbzuai-nlp/llm-media-profiling.",
            "upvotes": 2,
            "discussionId": "6851336a8a68fee7f6ba4c0f",
            "githubRepo": "https://github.com/mbzuai-nlp/llm-media-profiling",
            "ai_summary": "A novel methodology using large language models with curated prompts improves predictions of media outlet factuality and political bias, validated through experiments and error analysis.",
            "ai_keywords": [
                "large language models",
                "LLMs"
            ]
        },
        "publishedAt": "2025-06-14T11:49:20.000Z",
        "title": "Profiling News Media for Factuality and Bias Using LLMs and the\n  Fact-Checking Methodology of Human Experts",
        "summary": "In an age characterized by the proliferation of mis- and disinformation\nonline, it is critical to empower readers to understand the content they are\nreading. Important efforts in this direction rely on manual or automatic\nfact-checking, which can be challenging for emerging claims with limited\ninformation. Such scenarios can be handled by assessing the reliability and the\npolitical bias of the source of the claim, i.e., characterizing entire news\noutlets rather than individual claims or articles. This is an important but\nunderstudied research direction. While prior work has looked into linguistic\nand social contexts, we do not analyze individual articles or information in\nsocial media. Instead, we propose a novel methodology that emulates the\ncriteria that professional fact-checkers use to assess the factuality and\npolitical bias of an entire outlet. Specifically, we design a variety of\nprompts based on these criteria and elicit responses from large language models\n(LLMs), which we aggregate to make predictions. In addition to demonstrating\nsizable improvements over strong baselines via extensive experiments with\nmultiple LLMs, we provide an in-depth error analysis of the effect of media\npopularity and region on model performance. Further, we conduct an ablation\nstudy to highlight the key components of our dataset that contribute to these\nimprovements. To facilitate future research, we released our dataset and code\nat https://github.com/mbzuai-nlp/llm-media-profiling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12552.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637e8b1b66ee00bcb2468ed0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669240174964-637e8b1b66ee00bcb2468ed0.jpeg",
            "fullname": "Zain",
            "name": "zainmujahid",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09968",
            "authors": [
                {
                    "_id": "68504ea12932e11c891b5871",
                    "user": {
                        "_id": "6465995994327a238f5a4f03",
                        "avatarUrl": "/avatars/e8e29603977b8ac2ddf62b20bd97a8f2.svg",
                        "isPro": false,
                        "fullname": "Ge Wentao",
                        "user": "Owenngt",
                        "type": "user"
                    },
                    "name": "Wentao Ge",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:22:12.913Z",
                    "hidden": false
                },
                {
                    "_id": "68504ea12932e11c891b5872",
                    "name": "Yuqing Sun",
                    "hidden": false
                },
                {
                    "_id": "68504ea12932e11c891b5873",
                    "name": "Ziyan Wang",
                    "hidden": false
                },
                {
                    "_id": "68504ea12932e11c891b5874",
                    "name": "Haoyue Zheng",
                    "hidden": false
                },
                {
                    "_id": "68504ea12932e11c891b5875",
                    "name": "Weiyang He",
                    "hidden": false
                },
                {
                    "_id": "68504ea12932e11c891b5876",
                    "name": "Piaohong Wang",
                    "hidden": false
                },
                {
                    "_id": "68504ea12932e11c891b5877",
                    "name": "Qianyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "68504ea12932e11c891b5878",
                    "name": "Benyou Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:45:03.000Z",
            "submittedOnDailyAt": "2025-06-17T05:54:34.208Z",
            "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance",
            "submittedOnDailyBy": {
                "_id": "6465995994327a238f5a4f03",
                "avatarUrl": "/avatars/e8e29603977b8ac2ddf62b20bd97a8f2.svg",
                "isPro": false,
                "fullname": "Ge Wentao",
                "user": "Owenngt",
                "type": "user"
            },
            "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.",
            "upvotes": 2,
            "discussionId": "68504ea12932e11c891b5879",
            "ai_summary": "A gamified LLM-assisted system, SRLAgent, significantly improves self-regulated learning skills in college students through interactive, goal-setting, and real-time AI feedback.",
            "ai_keywords": [
                "LLM-assisted system",
                "gamification",
                "adaptive support",
                "large language models",
                "SRL (Self-regulated learning)",
                "Zimmermans three-phase SRL framework",
                "goal-setting",
                "strategy execution",
                "self-reflection",
                "between-subjects design",
                "baseline system",
                "traditional multimedia learning condition"
            ]
        },
        "publishedAt": "2025-06-11T13:45:03.000Z",
        "title": "SRLAgent: Enhancing Self-Regulated Learning Skills through Gamification\n  and LLM Assistance",
        "summary": "Self-regulated learning (SRL) is crucial for college students navigating\nincreased academic demands and independence. Insufficient SRL skills can lead\nto disorganized study habits, low motivation, and poor time management,\nundermining learners ability to thrive in challenging environments. Through a\nformative study involving 59 college students, we identified key challenges\nstudents face in developing SRL skills, including difficulties with\ngoal-setting, time management, and reflective learning. To address these\nchallenges, we introduce SRLAgent, an LLM-assisted system that fosters SRL\nskills through gamification and adaptive support from large language models\n(LLMs). Grounded in Zimmermans three-phase SRL framework, SRLAgent enables\nstudents to engage in goal-setting, strategy execution, and self-reflection\nwithin an interactive game-based environment. The system offers real-time\nfeedback and scaffolding powered by LLMs to support students independent study\nefforts. We evaluated SRLAgent using a between-subjects design, comparing it to\na baseline system (SRL without Agent features) and a traditional multimedia\nlearning condition. Results showed significant improvements in SRL skills\nwithin the SRLAgent group (p < .001, Cohens d = 0.234) and higher engagement\ncompared to the baselines. This work highlights the value of embedding SRL\nscaffolding and real-time AI support within gamified environments, offering\ndesign implications for educational technologies that aim to promote deeper\nlearning and metacognitive skill development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09968.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6465995994327a238f5a4f03",
            "avatarUrl": "/avatars/e8e29603977b8ac2ddf62b20bd97a8f2.svg",
            "fullname": "Ge Wentao",
            "name": "Owenngt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.11115",
            "authors": [
                {
                    "_id": "685107725e07650ecce891d6",
                    "user": {
                        "_id": "65e746079cf349af294e1f10",
                        "avatarUrl": "/avatars/0a3f13e03b1f9249595b387001203908.svg",
                        "isPro": false,
                        "fullname": "yerim Oh",
                        "user": "yerim0210",
                        "type": "user"
                    },
                    "name": "Yerim Oh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:20:35.446Z",
                    "hidden": false
                },
                {
                    "_id": "685107725e07650ecce891d7",
                    "name": "Jun-Hyung Park",
                    "hidden": false
                },
                {
                    "_id": "685107725e07650ecce891d8",
                    "name": "Junho Kim",
                    "hidden": false
                },
                {
                    "_id": "685107725e07650ecce891d9",
                    "name": "SungHo Kim",
                    "hidden": false
                },
                {
                    "_id": "685107725e07650ecce891da",
                    "name": "SangKeun Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-09T04:59:13.000Z",
            "submittedOnDailyAt": "2025-06-17T05:52:24.545Z",
            "title": "Incorporating Domain Knowledge into Materials Tokenization",
            "submittedOnDailyBy": {
                "_id": "65e746079cf349af294e1f10",
                "avatarUrl": "/avatars/0a3f13e03b1f9249595b387001203908.svg",
                "isPro": false,
                "fullname": "yerim Oh",
                "user": "yerim0210",
                "type": "user"
            },
            "summary": "While language models are increasingly utilized in materials science, typical\nmodels rely on frequency-centric tokenization methods originally developed for\nnatural language processing. However, these methods frequently produce\nexcessive fragmentation and semantic loss, failing to maintain the structural\nand semantic integrity of material concepts. To address this issue, we propose\nMATTER, a novel tokenization approach that integrates material knowledge into\ntokenization. Based on MatDetector trained on our materials knowledge base and\na re-ranking method prioritizing material concepts in token merging, MATTER\nmaintains the structural integrity of identified material concepts and prevents\nfragmentation during tokenization, ensuring their semantic meaning remains\nintact. The experimental results demonstrate that MATTER outperforms existing\ntokenization methods, achieving an average performance gain of 4% and 2%\nin the generation and classification tasks, respectively. These results\nunderscore the importance of domain knowledge for tokenization strategies in\nscientific text processing. Our code is available at\nhttps://github.com/yerimoh/MATTER",
            "upvotes": 2,
            "discussionId": "685107735e07650ecce891db",
            "ai_summary": "MATTER, a novel tokenization approach incorporating material knowledge, improves performance in scientific text processing tasks by maintaining structural and semantic material integrity.",
            "ai_keywords": [
                "MATTER",
                "MatDetector",
                "tokenization",
                "material knowledge",
                "token merging",
                "semantic integrity",
                "generation tasks",
                "classification tasks"
            ]
        },
        "publishedAt": "2025-06-09T00:59:13.000Z",
        "title": "Incorporating Domain Knowledge into Materials Tokenization",
        "summary": "While language models are increasingly utilized in materials science, typical\nmodels rely on frequency-centric tokenization methods originally developed for\nnatural language processing. However, these methods frequently produce\nexcessive fragmentation and semantic loss, failing to maintain the structural\nand semantic integrity of material concepts. To address this issue, we propose\nMATTER, a novel tokenization approach that integrates material knowledge into\ntokenization. Based on MatDetector trained on our materials knowledge base and\na re-ranking method prioritizing material concepts in token merging, MATTER\nmaintains the structural integrity of identified material concepts and prevents\nfragmentation during tokenization, ensuring their semantic meaning remains\nintact. The experimental results demonstrate that MATTER outperforms existing\ntokenization methods, achieving an average performance gain of 4% and 2%\nin the generation and classification tasks, respectively. These results\nunderscore the importance of domain knowledge for tokenization strategies in\nscientific text processing. Our code is available at\nhttps://github.com/yerimoh/MATTER",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11115.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e746079cf349af294e1f10",
            "avatarUrl": "/avatars/0a3f13e03b1f9249595b387001203908.svg",
            "fullname": "yerim Oh",
            "name": "yerim0210",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13502",
            "authors": [
                {
                    "_id": "6851ece80164cd13167103f1",
                    "name": "Ming Shen",
                    "hidden": false
                },
                {
                    "_id": "6851ece80164cd13167103f2",
                    "name": "Zhikun Xu",
                    "hidden": false
                },
                {
                    "_id": "6851ece80164cd13167103f3",
                    "name": "Xiao Ye",
                    "hidden": false
                },
                {
                    "_id": "6851ece80164cd13167103f4",
                    "name": "Jacob Dineen",
                    "hidden": false
                },
                {
                    "_id": "6851ece80164cd13167103f5",
                    "name": "Ben Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T13:58:54.000Z",
            "submittedOnDailyAt": "2025-06-17T21:06:34.444Z",
            "title": "BOW: Bottlenecked Next Word Exploration",
            "submittedOnDailyBy": {
                "_id": "65685a95677a71b8ab1406cf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65685a95677a71b8ab1406cf/6tl-GiLHlYk7M17dPZjTr.jpeg",
                "isPro": false,
                "fullname": "Zhikun Xu",
                "user": "JerrrrryKun",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are typically trained via next-word prediction\n(NWP), which provides strong surface-level fluency but often lacks support for\nrobust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel\nRL framework that rethinks NWP by introducing a reasoning bottleneck where a\npolicy model first generates a reasoning path rather than predicting the next\ntoken directly, after which a frozen judge model predicts the next token\ndistribution based solely on this reasoning path. We train the policy model\nusing GRPO with rewards that quantify how effectively the reasoning path\nfacilitates next-word recovery. Compared with other continual pretraining\nbaselines, we show that BOW improves both the general and next-word reasoning\ncapabilities of the base model, evaluated on various benchmarks. Our findings\nshow that BOW can serve as an effective and scalable alternative to vanilla\nNWP.",
            "upvotes": 1,
            "discussionId": "6851ece90164cd13167103f6",
            "ai_summary": "BOW, a reinforcement learning framework, enhances language model reasoning by introducing a reasoning bottleneck between the policy and judge models, improving general and next-word reasoning.",
            "ai_keywords": [
                "LLMs",
                "next-word prediction",
                "NWP",
                "BOttlenecked next Word exploration",
                "BOW",
                "RL",
                "policy model",
                "frozen judge model",
                "reasoning path",
                "GRPO",
                "continual pretraining",
                "general reasoning",
                "next-word reasoning"
            ]
        },
        "publishedAt": "2025-06-16T09:58:54.000Z",
        "title": "BOW: Bottlenecked Next Word Exploration",
        "summary": "Large language models (LLMs) are typically trained via next-word prediction\n(NWP), which provides strong surface-level fluency but often lacks support for\nrobust reasoning. We propose BOttlenecked next Word exploration (BOW), a novel\nRL framework that rethinks NWP by introducing a reasoning bottleneck where a\npolicy model first generates a reasoning path rather than predicting the next\ntoken directly, after which a frozen judge model predicts the next token\ndistribution based solely on this reasoning path. We train the policy model\nusing GRPO with rewards that quantify how effectively the reasoning path\nfacilitates next-word recovery. Compared with other continual pretraining\nbaselines, we show that BOW improves both the general and next-word reasoning\ncapabilities of the base model, evaluated on various benchmarks. Our findings\nshow that BOW can serve as an effective and scalable alternative to vanilla\nNWP.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13502.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65685a95677a71b8ab1406cf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65685a95677a71b8ab1406cf/6tl-GiLHlYk7M17dPZjTr.jpeg",
            "fullname": "Zhikun Xu",
            "name": "JerrrrryKun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.13277",
            "authors": [
                {
                    "_id": "68516d668a68fee7f6ba4cd3",
                    "name": "Huyang Li",
                    "hidden": false
                },
                {
                    "_id": "68516d668a68fee7f6ba4cd4",
                    "name": "Yahui Liu",
                    "hidden": false
                },
                {
                    "_id": "68516d668a68fee7f6ba4cd5",
                    "name": "Hongyu Sun",
                    "hidden": false
                },
                {
                    "_id": "68516d668a68fee7f6ba4cd6",
                    "name": "Deng Cai",
                    "hidden": false
                },
                {
                    "_id": "68516d668a68fee7f6ba4cd7",
                    "name": "Leyang Cui",
                    "hidden": false
                },
                {
                    "_id": "68516d668a68fee7f6ba4cd8",
                    "name": "Wei Bi",
                    "hidden": false
                },
                {
                    "_id": "68516d668a68fee7f6ba4cd9",
                    "name": "Peilin Zhao",
                    "hidden": false
                },
                {
                    "_id": "68516d668a68fee7f6ba4cda",
                    "name": "Taro Watanabe",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T09:16:40.000Z",
            "submittedOnDailyAt": "2025-06-17T11:59:01.655Z",
            "title": "SeqPE: Transformer with Sequential Position Encoding",
            "submittedOnDailyBy": {
                "_id": "64672e286192d3914221d633",
                "avatarUrl": "/avatars/a26908ebcbb74feaf44fe83a44fef8ac.svg",
                "isPro": false,
                "fullname": "Huayang",
                "user": "huayangli",
                "type": "user"
            },
            "summary": "Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each n-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.",
            "upvotes": 1,
            "discussionId": "68516d668a68fee7f6ba4cdb",
            "githubRepo": "https://github.com/ghrua/seqpe",
            "ai_summary": "SeqPE, a fully learnable position encoding framework, enhances the adaptability and scalability of positional encodings in Transformers, improving performance in various tasks and seamless multi-dimensional generalization.",
            "ai_keywords": [
                "self-attention layers",
                "Transformers",
                "permutation invariant",
                "positional encodings",
                "learnable position embeddings",
                "ALiBi",
                "RoPE",
                "symbolic sequence",
                "sequential position encoder",
                "contrastive objective",
                "knowledge distillation loss",
                "language modeling",
                "long-context question answering",
                "2D image classification",
                "perplexity",
                "exact match",
                "accuracy",
                "context length extrapolation"
            ]
        },
        "publishedAt": "2025-06-16T05:16:40.000Z",
        "title": "SeqPE: Transformer with Sequential Position Encoding",
        "summary": "Since self-attention layers in Transformers are permutation invariant by\ndesign, positional encodings must be explicitly incorporated to enable spatial\nunderstanding. However, fixed-size lookup tables used in traditional learnable\nposition embeddings (PEs) limit extrapolation capabilities beyond pre-trained\nsequence lengths. Expert-designed methods such as ALiBi and RoPE, mitigate this\nlimitation but demand extensive modifications for adapting to new modalities,\nunderscoring fundamental challenges in adaptability and scalability. In this\nwork, we present SeqPE, a unified and fully learnable position encoding\nframework that represents each n-dimensional position index as a symbolic\nsequence and employs a lightweight sequential position encoder to learn their\nembeddings in an end-to-end manner. To regularize SeqPE's embedding space, we\nintroduce two complementary objectives: a contrastive objective that aligns\nembedding distances with a predefined position-distance function, and a\nknowledge distillation loss that anchors out-of-distribution position\nembeddings to in-distribution teacher representations, further enhancing\nextrapolation performance. Experiments across language modeling, long-context\nquestion answering, and 2D image classification demonstrate that SeqPE not only\nsurpasses strong baselines in perplexity, exact match (EM), and\naccuracy--particularly under context length extrapolation--but also enables\nseamless generalization to multi-dimensional inputs without requiring manual\narchitectural redesign. We release our code, data, and checkpoints at\nhttps://github.com/ghrua/seqpe.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13277.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64672e286192d3914221d633",
            "avatarUrl": "/avatars/a26908ebcbb74feaf44fe83a44fef8ac.svg",
            "fullname": "Huayang",
            "name": "huayangli",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.12299",
            "authors": [
                {
                    "_id": "6850e32c5e07650ecce89112",
                    "name": "Taegyeong Lee",
                    "hidden": false
                },
                {
                    "_id": "6850e32c5e07650ecce89113",
                    "name": "Jeonghwa Yoo",
                    "hidden": false
                },
                {
                    "_id": "6850e32c5e07650ecce89114",
                    "name": "Hyoungseo Cho",
                    "hidden": false
                },
                {
                    "_id": "6850e32c5e07650ecce89115",
                    "name": "Soo Yong Kim",
                    "hidden": false
                },
                {
                    "_id": "6850e32c5e07650ecce89116",
                    "name": "Yunho Maeng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-14T01:23:50.000Z",
            "submittedOnDailyAt": "2025-06-17T05:43:52.729Z",
            "title": "QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety",
            "submittedOnDailyBy": {
                "_id": "65019c5420cfd12a88a74078",
                "avatarUrl": "/avatars/fdcfe7be656e9c5aa7236e3b076c7897.svg",
                "isPro": false,
                "fullname": "Taegyeong Lee",
                "user": "Taegyeonglee",
                "type": "user"
            },
            "summary": "The recent advancements in Large Language Models(LLMs) have had a significant\nimpact on a wide range of fields, from general domains to specialized areas.\nHowever, these advancements have also significantly increased the potential for\nmalicious users to exploit harmful and jailbreak prompts for malicious attacks.\nAlthough there have been many efforts to prevent harmful prompts and jailbreak\nprompts, protecting LLMs from such malicious attacks remains an important and\nchallenging task. In this paper, we propose QGuard, a simple yet effective\nsafety guard method, that utilizes question prompting to block harmful prompts\nin a zero-shot manner. Our method can defend LLMs not only from text-based\nharmful prompts but also from multi-modal harmful prompt attacks. Moreover, by\ndiversifying and modifying guard questions, our approach remains robust against\nthe latest harmful prompts without fine-tuning. Experimental results show that\nour model performs competitively on both text-only and multi-modal harmful\ndatasets. Additionally, by providing an analysis of question prompting, we\nenable a white-box analysis of user inputs. We believe our method provides\nvaluable insights for real-world LLM services in mitigating security risks\nassociated with harmful prompts.",
            "upvotes": 1,
            "discussionId": "6850e32c5e07650ecce89117",
            "ai_summary": "QGuard, a safety guard method using question prompting, effectively defends LLMs against harmful and multi-modal malicious prompts without fine-tuning.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "harmful prompts",
                "jailbreak prompts",
                "safety guard",
                "question prompting",
                "multi-modal harmful prompts"
            ]
        },
        "publishedAt": "2025-06-13T21:23:50.000Z",
        "title": "QGuard:Question-based Zero-shot Guard for Multi-modal LLM Safety",
        "summary": "The recent advancements in Large Language Models(LLMs) have had a significant\nimpact on a wide range of fields, from general domains to specialized areas.\nHowever, these advancements have also significantly increased the potential for\nmalicious users to exploit harmful and jailbreak prompts for malicious attacks.\nAlthough there have been many efforts to prevent harmful prompts and jailbreak\nprompts, protecting LLMs from such malicious attacks remains an important and\nchallenging task. In this paper, we propose QGuard, a simple yet effective\nsafety guard method, that utilizes question prompting to block harmful prompts\nin a zero-shot manner. Our method can defend LLMs not only from text-based\nharmful prompts but also from multi-modal harmful prompt attacks. Moreover, by\ndiversifying and modifying guard questions, our approach remains robust against\nthe latest harmful prompts without fine-tuning. Experimental results show that\nour model performs competitively on both text-only and multi-modal harmful\ndatasets. Additionally, by providing an analysis of question prompting, we\nenable a white-box analysis of user inputs. We believe our method provides\nvaluable insights for real-world LLM services in mitigating security risks\nassociated with harmful prompts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12299.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65019c5420cfd12a88a74078",
            "avatarUrl": "/avatars/fdcfe7be656e9c5aa7236e3b076c7897.svg",
            "fullname": "Taegyeong Lee",
            "name": "Taegyeonglee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.12258",
            "authors": [
                {
                    "_id": "6851360c8a68fee7f6ba4c1a",
                    "name": "Yijiang Li",
                    "hidden": false
                },
                {
                    "_id": "6851360c8a68fee7f6ba4c1b",
                    "name": "Genpei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6851360c8a68fee7f6ba4c1c",
                    "name": "Jiacheng Cheng",
                    "hidden": false
                },
                {
                    "_id": "6851360c8a68fee7f6ba4c1d",
                    "name": "Yi Li",
                    "hidden": false
                },
                {
                    "_id": "6851360c8a68fee7f6ba4c1e",
                    "name": "Xiaojun Shan",
                    "hidden": false
                },
                {
                    "_id": "6851360c8a68fee7f6ba4c1f",
                    "name": "Dashan Gao",
                    "hidden": false
                },
                {
                    "_id": "6851360c8a68fee7f6ba4c20",
                    "name": "Jiancheng Lyu",
                    "hidden": false
                },
                {
                    "_id": "6851360c8a68fee7f6ba4c21",
                    "name": "Yuan Li",
                    "hidden": false
                },
                {
                    "_id": "6851360c8a68fee7f6ba4c22",
                    "name": "Ning Bi",
                    "hidden": false
                },
                {
                    "_id": "6851360c8a68fee7f6ba4c23",
                    "name": "Nuno Vasconcelos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T22:19:54.000Z",
            "submittedOnDailyAt": "2025-06-17T08:02:23.997Z",
            "title": "EgoPrivacy: What Your First-Person Camera Says About You?",
            "submittedOnDailyBy": {
                "_id": "6419309f22270b3ccf177c77",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg",
                "isPro": false,
                "fullname": "William Li",
                "user": "williamium",
                "type": "user"
            },
            "summary": "While the rapid proliferation of wearable cameras has raised significant\nconcerns about egocentric video privacy, prior work has largely overlooked the\nunique privacy threats posed to the camera wearer. This work investigates the\ncore question: How much privacy information about the camera wearer can be\ninferred from their first-person view videos? We introduce EgoPrivacy, the\nfirst large-scale benchmark for the comprehensive evaluation of privacy risks\nin egocentric vision. EgoPrivacy covers three types of privacy (demographic,\nindividual, and situational), defining seven tasks that aim to recover private\ninformation ranging from fine-grained (e.g., wearer's identity) to\ncoarse-grained (e.g., age group). To further emphasize the privacy threats\ninherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel\nattack strategy that leverages ego-to-exo retrieval from an external pool of\nexocentric videos to boost the effectiveness of demographic privacy attacks. An\nextensive comparison of the different attacks possible under all threat models\nis presented, showing that private information of the wearer is highly\nsusceptible to leakage. For instance, our findings indicate that foundation\nmodels can effectively compromise wearer privacy even in zero-shot settings by\nrecovering attributes such as identity, scene, gender, and race with 70-80%\naccuracy. Our code and data are available at\nhttps://github.com/williamium3000/ego-privacy.",
            "upvotes": 1,
            "discussionId": "6851360c8a68fee7f6ba4c24",
            "ai_summary": "EgoPrivacy evaluates privacy risks in egocentric vision through a large-scale benchmark, revealing that foundation models can infer private information about camera wearers with high accuracy in zero-shot settings.",
            "ai_keywords": [
                "EgoPrivacy",
                "egocentric vision",
                "privacy threats",
                "ego-to-exo retrieval",
                "demographic privacy",
                "exocentric videos",
                "foundation models",
                "zero-shot settings"
            ]
        },
        "publishedAt": "2025-06-13T18:19:54.000Z",
        "title": "EgoPrivacy: What Your First-Person Camera Says About You?",
        "summary": "While the rapid proliferation of wearable cameras has raised significant\nconcerns about egocentric video privacy, prior work has largely overlooked the\nunique privacy threats posed to the camera wearer. This work investigates the\ncore question: How much privacy information about the camera wearer can be\ninferred from their first-person view videos? We introduce EgoPrivacy, the\nfirst large-scale benchmark for the comprehensive evaluation of privacy risks\nin egocentric vision. EgoPrivacy covers three types of privacy (demographic,\nindividual, and situational), defining seven tasks that aim to recover private\ninformation ranging from fine-grained (e.g., wearer's identity) to\ncoarse-grained (e.g., age group). To further emphasize the privacy threats\ninherent to egocentric vision, we propose Retrieval-Augmented Attack, a novel\nattack strategy that leverages ego-to-exo retrieval from an external pool of\nexocentric videos to boost the effectiveness of demographic privacy attacks. An\nextensive comparison of the different attacks possible under all threat models\nis presented, showing that private information of the wearer is highly\nsusceptible to leakage. For instance, our findings indicate that foundation\nmodels can effectively compromise wearer privacy even in zero-shot settings by\nrecovering attributes such as identity, scene, gender, and race with 70-80%\naccuracy. Our code and data are available at\nhttps://github.com/williamium3000/ego-privacy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12258.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6419309f22270b3ccf177c77",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6419309f22270b3ccf177c77/KQa1586iBBKqucUlfpuPp.jpeg",
            "fullname": "William Li",
            "name": "williamium",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.12229",
            "authors": [
                {
                    "_id": "6850e8735e07650ecce89119",
                    "name": "Hao Xu",
                    "hidden": false
                },
                {
                    "_id": "6850e8735e07650ecce8911a",
                    "name": "Jiacheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6850e8735e07650ecce8911b",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "6850e8735e07650ecce8911c",
                    "name": "Noah A. Smith",
                    "hidden": false
                },
                {
                    "_id": "6850e8735e07650ecce8911d",
                    "name": "Hannaneh Hajishirzi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T21:13:57.000Z",
            "submittedOnDailyAt": "2025-06-17T23:51:52.077Z",
            "title": "Infini-gram mini: Exact n-gram Search at the Internet Scale with\n  FM-Index",
            "submittedOnDailyBy": {
                "_id": "635f46d1928a42bc95cfcf7c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
                "isPro": false,
                "fullname": "Jiacheng Liu",
                "user": "liujch1998",
                "type": "user"
            },
            "summary": "Language models are trained mainly on massive text data from the Internet,\nand it becomes increasingly important to understand this data source.\nExact-match search engines enable searching in large text corpora -- counting\nstring appearances and retrieving the enclosing documents -- yet the high\nstorage overhead hinders their application on Internet-scale data. We present\nInfini-gram mini, an efficient and scalable system that can make petabyte-level\ntext corpora searchable. Based on the FM-index data structure (Ferragina and\nManzini, 2000), which simultaneously indexes and compresses text, our system\ncreates indexes with size only 44% of the corpus. Infini-gram mini greatly\nimproves upon the best existing implementation of FM-index in terms of indexing\nspeed (18times) and memory use during both indexing (3.2times reduction)\nand querying (down to a negligible amount). We index 46TB of Internet text in\n50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes).\nWe show one important use case of Infini-gram mini in a large-scale analysis of\nbenchmark contamination. We find several core LM evaluation benchmarks to be\nheavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead\nto overestimating the capabilities of language models if trained on such data.\nWe host a benchmark contamination bulletin to share the contamination rate of\nmany core and community-contributed benchmarks. We also release a web interface\nand an API endpoint to serve general search queries on Infini-gram mini\nindexes.",
            "upvotes": 1,
            "discussionId": "6850e8735e07650ecce8911e"
        },
        "publishedAt": "2025-06-13T17:13:57.000Z",
        "title": "Infini-gram mini: Exact n-gram Search at the Internet Scale with\n  FM-Index",
        "summary": "Language models are trained mainly on massive text data from the Internet,\nand it becomes increasingly important to understand this data source.\nExact-match search engines enable searching in large text corpora -- counting\nstring appearances and retrieving the enclosing documents -- yet the high\nstorage overhead hinders their application on Internet-scale data. We present\nInfini-gram mini, an efficient and scalable system that can make petabyte-level\ntext corpora searchable. Based on the FM-index data structure (Ferragina and\nManzini, 2000), which simultaneously indexes and compresses text, our system\ncreates indexes with size only 44% of the corpus. Infini-gram mini greatly\nimproves upon the best existing implementation of FM-index in terms of indexing\nspeed (18times) and memory use during both indexing (3.2times reduction)\nand querying (down to a negligible amount). We index 46TB of Internet text in\n50 days with a single 128-core CPU node (or 19 hours if using 75 such nodes).\nWe show one important use case of Infini-gram mini in a large-scale analysis of\nbenchmark contamination. We find several core LM evaluation benchmarks to be\nheavily contaminated in Internet crawls (up to 40% in SQuAD), which could lead\nto overestimating the capabilities of language models if trained on such data.\nWe host a benchmark contamination bulletin to share the contamination rate of\nmany core and community-contributed benchmarks. We also release a web interface\nand an API endpoint to serve general search queries on Infini-gram mini\nindexes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12229.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635f46d1928a42bc95cfcf7c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635f46d1928a42bc95cfcf7c/5KF8aLiDCJdl7B1SdJ-7V.png",
            "fullname": "Jiacheng Liu",
            "name": "liujch1998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.13430",
            "authors": [
                {
                    "_id": "685111805e07650ecce891dd",
                    "user": {
                        "_id": "6663028e73399db40074a357",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ITH5YMg1N0OghXRd0xzwY.jpeg",
                        "isPro": false,
                        "fullname": "Tristan Kenneweg",
                        "user": "TristanKe",
                        "type": "user"
                    },
                    "name": "Tristan Kenneweg",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:20:33.250Z",
                    "hidden": false
                },
                {
                    "_id": "685111805e07650ecce891de",
                    "name": "Philip Kenneweg",
                    "hidden": false
                },
                {
                    "_id": "685111805e07650ecce891df",
                    "name": "Barbara Hammer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T12:47:37.000Z",
            "submittedOnDailyAt": "2025-06-17T07:21:42.800Z",
            "title": "Uncertainty-Aware Remaining Lifespan Prediction from Images",
            "submittedOnDailyBy": {
                "_id": "6663028e73399db40074a357",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ITH5YMg1N0OghXRd0xzwY.jpeg",
                "isPro": false,
                "fullname": "Tristan Kenneweg",
                "user": "TristanKe",
                "type": "user"
            },
            "summary": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on\nan established Dataset, and further improves to 4.79 and 5.07 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide well-calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.62 years. While not intended for\nclinical deployment, these results highlight the potential of extracting\nmedically relevant signals from images. We make all code and datasets available\nto facilitate further research.",
            "upvotes": 0,
            "discussionId": "685111815e07650ecce891e0",
            "ai_summary": "Vision transformer models predict remaining lifespan from images with high accuracy and well-calibrated uncertainty estimates.",
            "ai_keywords": [
                "vision transformer",
                "Gaussian distribution",
                "mean absolute error",
                "bucketed expected calibration error"
            ]
        },
        "publishedAt": "2025-06-16T08:47:37.000Z",
        "title": "Uncertainty-Aware Remaining Lifespan Prediction from Images",
        "summary": "Predicting mortality-related outcomes from images offers the prospect of\naccessible, noninvasive, and scalable health screening. We present a method\nthat leverages pretrained vision transformer foundation models to estimate\nremaining lifespan from facial and whole-body images, alongside robust\nuncertainty quantification. We show that predictive uncertainty varies\nsystematically with the true remaining lifespan, and that this uncertainty can\nbe effectively modeled by learning a Gaussian distribution for each sample. Our\napproach achieves state-of-the-art mean absolute error (MAE) of 7.48 years on\nan established Dataset, and further improves to 4.79 and 5.07 years MAE on two\nnew, higher-quality datasets curated and published in this work. Importantly,\nour models provide well-calibrated uncertainty estimates, as demonstrated by a\nbucketed expected calibration error of 0.62 years. While not intended for\nclinical deployment, these results highlight the potential of extracting\nmedically relevant signals from images. We make all code and datasets available\nto facilitate further research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13430.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6663028e73399db40074a357",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/ITH5YMg1N0OghXRd0xzwY.jpeg",
            "fullname": "Tristan Kenneweg",
            "name": "TristanKe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13172",
            "authors": [
                {
                    "_id": "6850fc485e07650ecce8918b",
                    "user": {
                        "_id": "68264aa0e6a0ae8670403081",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68264aa0e6a0ae8670403081/a6V9yE1cf6-lFf7G8Ih-H.png",
                        "isPro": false,
                        "fullname": "Evgeny Markhasin",
                        "user": "PChemGuy",
                        "type": "user"
                    },
                    "name": "Evgeny Markhasin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T07:20:37.574Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T07:34:31.000Z",
            "submittedOnDailyAt": "2025-06-17T04:00:54.011Z",
            "title": "Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging\n  Unsubstantiated Claims and Ambiguous Pronouns",
            "submittedOnDailyBy": {
                "_id": "68264aa0e6a0ae8670403081",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68264aa0e6a0ae8670403081/a6V9yE1cf6-lFf7G8Ih-H.png",
                "isPro": false,
                "fullname": "Evgeny Markhasin",
                "user": "PChemGuy",
                "type": "user"
            },
            "summary": "We present and evaluate a suite of proof-of-concept (PoC), structured\nworkflow prompts designed to elicit human-like hierarchical reasoning while\nguiding Large Language Models (LLMs) in high-level semantic and linguistic\nanalysis of scholarly manuscripts. The prompts target two non-trivial\nanalytical tasks: identifying unsubstantiated claims in summaries\n(informational integrity) and flagging ambiguous pronoun references (linguistic\nclarity). We conducted a systematic, multi-run evaluation on two frontier\nmodels (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context\nconditions. Our results for the informational integrity task reveal a\nsignificant divergence in model performance: while both models successfully\nidentified an unsubstantiated head of a noun phrase (95% success), ChatGPT\nconsistently failed (0% success) to identify an unsubstantiated adjectival\nmodifier that Gemini correctly flagged (95% success), raising a question\nregarding potential influence of the target's syntactic role. For the\nlinguistic analysis task, both models performed well (80-90% success) with full\nmanuscript context. In a summary-only setting, however, ChatGPT achieved a\nperfect (100%) success rate, while Gemini's performance was substantially\ndegraded. Our findings suggest that structured prompting is a viable\nmethodology for complex textual analysis but show that prompt performance may\nbe highly dependent on the interplay between the model, task type, and context,\nhighlighting the need for rigorous, model-specific testing.",
            "upvotes": 0,
            "discussionId": "6850fc495e07650ecce8918c",
            "ai_summary": "Structured workflow prompts improve hierarchical reasoning in LLMs for scholarly manuscript analysis, but their effectiveness varies with the model, task type, and context.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "Gemini Pro 2.5 Pro",
                "ChatGPT Plus o3",
                "unsubstantiated claims",
                "informational integrity",
                "ambiguous pronoun references",
                "linguistic clarity",
                "hierarchical reasoning",
                "textual analysis"
            ]
        },
        "publishedAt": "2025-06-16T03:34:31.000Z",
        "title": "Ai-Facilitated Analysis of Abstracts and Conclusions: Flagging\n  Unsubstantiated Claims and Ambiguous Pronouns",
        "summary": "We present and evaluate a suite of proof-of-concept (PoC), structured\nworkflow prompts designed to elicit human-like hierarchical reasoning while\nguiding Large Language Models (LLMs) in high-level semantic and linguistic\nanalysis of scholarly manuscripts. The prompts target two non-trivial\nanalytical tasks: identifying unsubstantiated claims in summaries\n(informational integrity) and flagging ambiguous pronoun references (linguistic\nclarity). We conducted a systematic, multi-run evaluation on two frontier\nmodels (Gemini Pro 2.5 Pro and ChatGPT Plus o3) under varied context\nconditions. Our results for the informational integrity task reveal a\nsignificant divergence in model performance: while both models successfully\nidentified an unsubstantiated head of a noun phrase (95% success), ChatGPT\nconsistently failed (0% success) to identify an unsubstantiated adjectival\nmodifier that Gemini correctly flagged (95% success), raising a question\nregarding potential influence of the target's syntactic role. For the\nlinguistic analysis task, both models performed well (80-90% success) with full\nmanuscript context. In a summary-only setting, however, ChatGPT achieved a\nperfect (100%) success rate, while Gemini's performance was substantially\ndegraded. Our findings suggest that structured prompting is a viable\nmethodology for complex textual analysis but show that prompt performance may\nbe highly dependent on the interplay between the model, task type, and context,\nhighlighting the need for rigorous, model-specific testing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13172.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68264aa0e6a0ae8670403081",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68264aa0e6a0ae8670403081/a6V9yE1cf6-lFf7G8Ih-H.png",
            "fullname": "Evgeny Markhasin",
            "name": "PChemGuy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13001",
            "authors": [
                {
                    "_id": "685209ec0164cd1316710433",
                    "user": {
                        "_id": "6584f042b378d311dccea501",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vkq1AQhcuZwIjpFDkdgPQ.png",
                        "isPro": false,
                        "fullname": "Christian Zhou-Zheng",
                        "user": "ChristianAzinn",
                        "type": "user"
                    },
                    "name": "Christian Zhou-Zheng",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-18T00:35:57.447Z",
                    "hidden": false
                },
                {
                    "_id": "685209ec0164cd1316710434",
                    "user": {
                        "_id": "66342b6b0797618ea4e0bd54",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/msWLNWjX6XIlYRi5iG8Ys.jpeg",
                        "isPro": false,
                        "fullname": "Philippe  Pasquier",
                        "user": "MetacreationLab",
                        "type": "user"
                    },
                    "name": "Philippe Pasquier",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-18T00:35:57.447Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T00:04:01.000Z",
            "submittedOnDailyAt": "2025-06-17T23:09:02.137Z",
            "title": "Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV",
            "submittedOnDailyBy": {
                "_id": "6584f042b378d311dccea501",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vkq1AQhcuZwIjpFDkdgPQ.png",
                "isPro": false,
                "fullname": "Christian Zhou-Zheng",
                "user": "ChristianAzinn",
                "type": "user"
            },
            "summary": "Existing work in automatic music generation has primarily focused on\nend-to-end systems that produce complete compositions or continuations.\nHowever, because musical composition is typically an iterative process, such\nsystems make it difficult to engage in the back-and-forth between human and\nmachine that is essential to computer-assisted creativity. In this study, we\naddress the task of personalizable, multi-track, long-context, and controllable\nsymbolic music infilling to enhance the process of computer-assisted\ncomposition. We present MIDI-RWKV, a novel model based on the RWKV-7 linear\narchitecture, to enable efficient and coherent musical cocreation on edge\ndevices. We also demonstrate that MIDI-RWKV admits an effective method of\nfinetuning its initial state for personalization in the very-low-sample regime.\nWe evaluate MIDI-RWKV and its state tuning on several quantitative and\nqualitative metrics, and release model weights and code at\nhttps://github.com/christianazinn/MIDI-RWKV.",
            "upvotes": 0,
            "discussionId": "685209ed0164cd1316710435",
            "ai_summary": "MIDI-RWKV, a novel RWKV-7 based model, enables efficient and coherent musical infilling on edge devices with personalizable initial states, enhancing the computer-assisted composition process.",
            "ai_keywords": [
                "symbolic music infilling",
                "RWKV-7",
                "MIDI-RWKV",
                "musical cocreation",
                "edge devices",
                "fine-tuning",
                "initial state personalization",
                "very-low-sample regime"
            ]
        },
        "publishedAt": "2025-06-15T20:04:01.000Z",
        "title": "Personalizable Long-Context Symbolic Music Infilling with MIDI-RWKV",
        "summary": "Existing work in automatic music generation has primarily focused on\nend-to-end systems that produce complete compositions or continuations.\nHowever, because musical composition is typically an iterative process, such\nsystems make it difficult to engage in the back-and-forth between human and\nmachine that is essential to computer-assisted creativity. In this study, we\naddress the task of personalizable, multi-track, long-context, and controllable\nsymbolic music infilling to enhance the process of computer-assisted\ncomposition. We present MIDI-RWKV, a novel model based on the RWKV-7 linear\narchitecture, to enable efficient and coherent musical cocreation on edge\ndevices. We also demonstrate that MIDI-RWKV admits an effective method of\nfinetuning its initial state for personalization in the very-low-sample regime.\nWe evaluate MIDI-RWKV and its state tuning on several quantitative and\nqualitative metrics, and release model weights and code at\nhttps://github.com/christianazinn/MIDI-RWKV.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13001.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6584f042b378d311dccea501",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/vkq1AQhcuZwIjpFDkdgPQ.png",
            "fullname": "Christian Zhou-Zheng",
            "name": "ChristianAzinn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.12148",
            "authors": [
                {
                    "_id": "68513a7d8a68fee7f6ba4c2e",
                    "user": {
                        "_id": "62de70fb86220b5cb895e199",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62de70fb86220b5cb895e199/k7w8pTBIvCwkrTksQJ7GM.jpeg",
                        "isPro": false,
                        "fullname": "Chiara Di Bonaventura",
                        "user": "dibo",
                        "type": "user"
                    },
                    "name": "Chiara Di Bonaventura",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T13:06:24.238Z",
                    "hidden": false
                },
                {
                    "_id": "68513a7d8a68fee7f6ba4c2f",
                    "name": "Barbara McGillivray",
                    "hidden": false
                },
                {
                    "_id": "68513a7d8a68fee7f6ba4c30",
                    "name": "Yulan He",
                    "hidden": false
                },
                {
                    "_id": "68513a7d8a68fee7f6ba4c31",
                    "name": "Albert Meroño-Peñuela",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T18:08:19.000Z",
            "submittedOnDailyAt": "2025-06-17T08:46:06.795Z",
            "title": "Hatevolution: What Static Benchmarks Don't Tell Us",
            "submittedOnDailyBy": {
                "_id": "62de70fb86220b5cb895e199",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62de70fb86220b5cb895e199/k7w8pTBIvCwkrTksQJ7GM.jpeg",
                "isPro": false,
                "fullname": "Chiara Di Bonaventura",
                "user": "dibo",
                "type": "user"
            },
            "summary": "Language changes over time, including in the hate speech domain, which\nevolves quickly following social dynamics and cultural shifts. While NLP\nresearch has investigated the impact of language evolution on model training\nand has proposed several solutions for it, its impact on model benchmarking\nremains under-explored. Yet, hate speech benchmarks play a crucial role to\nensure model safety. In this paper, we empirically evaluate the robustness of\n20 language models across two evolving hate speech experiments, and we show the\ntemporal misalignment between static and time-sensitive evaluations. Our\nfindings call for time-sensitive linguistic benchmarks in order to correctly\nand reliably evaluate language models in the hate speech domain.",
            "upvotes": 0,
            "discussionId": "68513a7e8a68fee7f6ba4c32",
            "ai_summary": "Empirical evaluation reveals temporal misalignment in the robustness of language models on evolving hate speech benchmarks, highlighting the need for time-sensitive linguistic assessments.",
            "ai_keywords": [
                "NLP",
                "language models",
                "hate speech benchmarks",
                "temporal misalignment",
                "time-sensitive evaluations",
                "language evolution"
            ]
        },
        "publishedAt": "2025-06-13T14:08:19.000Z",
        "title": "Hatevolution: What Static Benchmarks Don't Tell Us",
        "summary": "Language changes over time, including in the hate speech domain, which\nevolves quickly following social dynamics and cultural shifts. While NLP\nresearch has investigated the impact of language evolution on model training\nand has proposed several solutions for it, its impact on model benchmarking\nremains under-explored. Yet, hate speech benchmarks play a crucial role to\nensure model safety. In this paper, we empirically evaluate the robustness of\n20 language models across two evolving hate speech experiments, and we show the\ntemporal misalignment between static and time-sensitive evaluations. Our\nfindings call for time-sensitive linguistic benchmarks in order to correctly\nand reliably evaluate language models in the hate speech domain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12148.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62de70fb86220b5cb895e199",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62de70fb86220b5cb895e199/k7w8pTBIvCwkrTksQJ7GM.jpeg",
            "fullname": "Chiara Di Bonaventura",
            "name": "dibo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    }
]