[
    {
        "paper": {
            "id": "2506.14028",
            "authors": [
                {
                    "_id": "685240aa0164cd131671056a",
                    "user": {
                        "_id": "63a0c0803c8841cfe2cd1f15",
                        "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
                        "isPro": false,
                        "fullname": "Xueqing Peng",
                        "user": "Xueqing",
                        "type": "user"
                    },
                    "name": "Xueqing Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T16:15:27.517Z",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056b",
                    "name": "Lingfei Qian",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056c",
                    "user": {
                        "_id": "65d76cc5b9b7b8bf88faa916",
                        "avatarUrl": "/avatars/d95232cd0c307efab6197ade1a66190b.svg",
                        "isPro": true,
                        "fullname": "Yan Wang",
                        "user": "YanAdjeNole",
                        "type": "user"
                    },
                    "name": "Yan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T16:15:31.114Z",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056d",
                    "name": "Ruoyu Xiang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056e",
                    "user": {
                        "_id": "65bd14e8ce846f8aa94db1d1",
                        "avatarUrl": "/avatars/76eaad15bf32eba75271f3dc315527c2.svg",
                        "isPro": false,
                        "fullname": "Yueru He",
                        "user": "Yueru1",
                        "type": "user"
                    },
                    "name": "Yueru He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T16:15:33.127Z",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671056f",
                    "name": "Yang Ren",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710570",
                    "user": {
                        "_id": "678ab76d27bb31ad067cbffd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/678ab76d27bb31ad067cbffd/l_5mRG6_BmqmADKs2Kb3W.png",
                        "isPro": false,
                        "fullname": "Mingyang Jiang",
                        "user": "0oJ1mmyo0",
                        "type": "user"
                    },
                    "name": "Mingyang Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T16:15:29.378Z",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710571",
                    "name": "Jeff Zhao",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710572",
                    "name": "Huan He",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710573",
                    "name": "Yi Han",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710574",
                    "name": "Yun Feng",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710575",
                    "name": "Yuechen Jiang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710576",
                    "name": "Yupeng Cao",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710577",
                    "name": "Haohang Li",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710578",
                    "name": "Yangyang Yu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710579",
                    "name": "Xiaoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057a",
                    "name": "Penglei Gao",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057b",
                    "name": "Shengyuan Lin",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057c",
                    "name": "Keyi Wang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057d",
                    "name": "Shanshan Yang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057e",
                    "user": {
                        "_id": "62f662bcc58915315c4eccea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                        "isPro": true,
                        "fullname": "Yilun Zhao",
                        "user": "yilunzhao",
                        "type": "user"
                    },
                    "name": "Yilun Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:58:59.111Z",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671057f",
                    "name": "Zhiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710580",
                    "name": "Peng Lu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710581",
                    "name": "Jerry Huang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710582",
                    "user": {
                        "_id": "62bb1e0f3ff437e49a3088e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62bb1e0f3ff437e49a3088e5/bcUQmH8tKfI6DIWH9IcYp.jpeg",
                        "isPro": true,
                        "fullname": "Suyuchen Wang",
                        "user": "sheryc",
                        "type": "user"
                    },
                    "name": "Suyuchen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T16:15:35.878Z",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710583",
                    "name": "Triantafillos Papadopoulos",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710584",
                    "name": "Polydoros Giannouris",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710585",
                    "name": "Efstathia Soufleri",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710586",
                    "name": "Nuo Chen",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710587",
                    "name": "Guojun Xiong",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710588",
                    "name": "Zhiyang Deng",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710589",
                    "name": "Yijia Zhao",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058a",
                    "name": "Mingquan Lin",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058b",
                    "name": "Meikang Qiu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058c",
                    "name": "Kaleb E Smith",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058d",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058e",
                    "name": "Xiao-Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd131671058f",
                    "name": "Jimin Huang",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710590",
                    "name": "Alejandro Lopez-Lira",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710591",
                    "name": "Xi Chen",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710592",
                    "name": "Junichi Tsujii",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710593",
                    "name": "Jian-Yun Nie",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710594",
                    "name": "Sophia Ananiadou",
                    "hidden": false
                },
                {
                    "_id": "685240aa0164cd1316710595",
                    "name": "Qianqian Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T22:01:49.000Z",
            "submittedOnDailyAt": "2025-06-18T13:36:29.873Z",
            "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation",
            "submittedOnDailyBy": {
                "_id": "63a0c0803c8841cfe2cd1f15",
                "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
                "isPro": false,
                "fullname": "Xueqing Peng",
                "user": "Xueqing",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have accelerated progress in\nfinancial NLP and applications, yet existing benchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\nto the global financial domain, evaluating LLMs across modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\na compact, balanced benchmark rather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their general multimodal and multilingual\ncapabilities, struggle dramatically when faced with complex cross-lingual and\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\nfoster transparent, reproducible, and inclusive progress in financial studies\nand applications.",
            "upvotes": 67,
            "discussionId": "685240ab0164cd1316710596",
            "ai_summary": "MultiFinBen is a multilingual and multimodal benchmark for financial domain tasks, evaluating LLMs across modalities and linguistic settings, revealing challenges in complex cross-lingual and multimodal financial reasoning.",
            "ai_keywords": [
                "LLMs",
                "financial NLP",
                "multilingual",
                "multimodal",
                "benchmark",
                "domain-specific tasks",
                "PolyFiQA-Easy",
                "PolyFiQA-Expert",
                "EnglishOCR",
                "SpanishOCR",
                "dynamic selection mechanism",
                "difficulty-aware",
                "OCR-embedded",
                "financial QA"
            ]
        },
        "publishedAt": "2025-06-16T18:01:49.000Z",
        "title": "MultiFinBen: A Multilingual, Multimodal, and Difficulty-Aware Benchmark\n  for Financial LLM Evaluation",
        "summary": "Recent advances in large language models (LLMs) have accelerated progress in\nfinancial NLP and applications, yet existing benchmarks remain limited to\nmonolingual and unimodal settings, often over-relying on simple tasks and\nfailing to reflect the complexity of real-world financial communication. We\nintroduce MultiFinBen, the first multilingual and multimodal benchmark tailored\nto the global financial domain, evaluating LLMs across modalities (text,\nvision, audio) and linguistic settings (monolingual, bilingual, multilingual)\non domain-specific tasks. We introduce two novel tasks, including PolyFiQA-Easy\nand PolyFiQA-Expert, the first multilingual financial benchmarks requiring\nmodels to perform complex reasoning over mixed-language inputs; and EnglishOCR\nand SpanishOCR, the first OCR-embedded financial QA tasks challenging models to\nextract and reason over information from visual-text financial documents.\nMoreover, we propose a dynamic, difficulty-aware selection mechanism and curate\na compact, balanced benchmark rather than simple aggregation existing datasets.\nExtensive evaluation of 22 state-of-the-art models reveals that even the\nstrongest models, despite their general multimodal and multilingual\ncapabilities, struggle dramatically when faced with complex cross-lingual and\nmultimodal tasks in financial domain. MultiFinBen is publicly released to\nfoster transparent, reproducible, and inclusive progress in financial studies\nand applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14028.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63a0c0803c8841cfe2cd1f15",
            "avatarUrl": "/avatars/bbe216db7a33612f23d23ce4ed4ba3f9.svg",
            "fullname": "Xueqing Peng",
            "name": "Xueqing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.12928",
            "authors": [
                {
                    "_id": "6851dd060164cd13167103d7",
                    "name": "King Zhu",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103d8",
                    "name": "Hanhao Li",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103d9",
                    "name": "Siwei Wu",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103da",
                    "name": "Tianshun Xing",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103db",
                    "name": "Dehua Ma",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103dc",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103dd",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103de",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103df",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e0",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e1",
                    "name": "Changwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e2",
                    "name": "Chenghua Lin",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e3",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e4",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:44.662Z",
                    "hidden": false
                },
                {
                    "_id": "6851dd060164cd13167103e5",
                    "user": {
                        "_id": "628c8598ef14f971b698107f",
                        "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "Wangchunshu",
                        "type": "user"
                    },
                    "name": "Wangchunshu Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:42.693Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-15T17:59:47.000Z",
            "submittedOnDailyAt": "2025-06-18T04:21:02.464Z",
            "title": "Scaling Test-time Compute for LLM Agents",
            "submittedOnDailyBy": {
                "_id": "638efcf4c67af472d316d424",
                "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                "isPro": false,
                "fullname": "Ge Zhang",
                "user": "zhangysk",
                "type": "user"
            },
            "summary": "Scaling test time compute has shown remarkable success in improving the\nreasoning abilities of large language models (LLMs). In this work, we conduct\nthe first systematic exploration of applying test-time scaling methods to\nlanguage agents and investigate the extent to which it improves their\neffectiveness. Specifically, we explore different test-time scaling strategies,\nincluding: (1) parallel sampling algorithms; (2) sequential revision\nstrategies; (3) verifiers and merging methods; (4)strategies for diversifying\nrollouts.We carefully analyze and ablate the impact of different design\nstrategies on applying test-time scaling on language agents, and have follow\nfindings: 1. Scaling test time compute could improve the performance of agents.\n2. Knowing when to reflect is important for agents. 3. Among different\nverification and result merging approaches, the list-wise method performs best.\n4. Increasing diversified rollouts exerts a positive effect on the agent's task\nperformance.",
            "upvotes": 40,
            "discussionId": "6851dd060164cd13167103e6",
            "ai_summary": "Systematic exploration of test-time scaling methods in large language agents reveals that computational scaling improves performance, especially through parallel sampling, sequential revision, effective verification, and increased rollout diversity.",
            "ai_keywords": [
                "parallel sampling algorithms",
                "sequential revision strategies",
                "verifiers",
                "merging methods",
                "diversified rollouts",
                "test-time scaling",
                "large language models"
            ]
        },
        "publishedAt": "2025-06-15T13:59:47.000Z",
        "title": "Scaling Test-time Compute for LLM Agents",
        "summary": "Scaling test time compute has shown remarkable success in improving the\nreasoning abilities of large language models (LLMs). In this work, we conduct\nthe first systematic exploration of applying test-time scaling methods to\nlanguage agents and investigate the extent to which it improves their\neffectiveness. Specifically, we explore different test-time scaling strategies,\nincluding: (1) parallel sampling algorithms; (2) sequential revision\nstrategies; (3) verifiers and merging methods; (4)strategies for diversifying\nrollouts.We carefully analyze and ablate the impact of different design\nstrategies on applying test-time scaling on language agents, and have follow\nfindings: 1. Scaling test time compute could improve the performance of agents.\n2. Knowing when to reflect is important for agents. 3. Among different\nverification and result merging approaches, the list-wise method performs best.\n4. Increasing diversified rollouts exerts a positive effect on the agent's task\nperformance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12928.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 49
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.12285",
            "authors": [
                {
                    "_id": "6852b1597eb90a35d35de603",
                    "name": "Yinghao Ma",
                    "hidden": false
                },
                {
                    "_id": "6852b1597eb90a35d35de604",
                    "name": "Siyou Li",
                    "hidden": false
                },
                {
                    "_id": "6852b1597eb90a35d35de605",
                    "name": "Juntao Yu",
                    "hidden": false
                },
                {
                    "_id": "6852b1597eb90a35d35de606",
                    "name": "Emmanouil Benetos",
                    "hidden": false
                },
                {
                    "_id": "6852b1597eb90a35d35de607",
                    "name": "Akira Maezawa",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-14T00:18:44.000Z",
            "submittedOnDailyAt": "2025-06-18T13:02:05.187Z",
            "title": "CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\n  Following",
            "submittedOnDailyBy": {
                "_id": "6410665d5364a661bee22524",
                "avatarUrl": "/avatars/f1cb0e07f36933187ceccbd5dcbeff79.svg",
                "isPro": false,
                "fullname": "Yinghao Ma",
                "user": "nicolaus625",
                "type": "user"
            },
            "summary": "Recent advances in audio-text large language models (LLMs) have opened new\npossibilities for music understanding and generation. However, existing\nbenchmarks are limited in scope, often relying on simplified tasks or\nmulti-choice evaluations that fail to reflect the complexity of real-world\nmusic analysis. We reinterpret a broad range of traditional MIR annotations as\ninstruction-following formats and introduce CMI-Bench, a comprehensive music\ninstruction following benchmark designed to evaluate audio-text LLMs on a\ndiverse set of music information retrieval (MIR) tasks. These include genre\nclassification, emotion regression, emotion tagging, instrument classification,\npitch estimation, key detection, lyrics transcription, melody extraction, vocal\ntechnique recognition, instrument performance technique detection, music\ntagging, music captioning, and (down)beat tracking: reflecting core challenges\nin MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized\nevaluation metrics consistent with previous state-of-the-art MIR models,\nensuring direct comparability with supervised approaches. We provide an\nevaluation toolkit supporting all open-source audio-textual LLMs, including\nLTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant\nperformance gaps between LLMs and supervised models, along with their culture,\nchronological and gender bias, highlighting the potential and limitations of\ncurrent models in addressing MIR tasks. CMI-Bench establishes a unified\nfoundation for evaluating music instruction following, driving progress in\nmusic-aware LLMs.",
            "upvotes": 38,
            "discussionId": "6852b15a7eb90a35d35de608",
            "githubRepo": "https://github.com/nicolaus625/CMI-bench",
            "ai_summary": "CMI-Bench introduces a comprehensive instruction-following benchmark for audio-text LLMs to evaluate them on a diverse range of music information retrieval tasks.",
            "ai_keywords": [
                "audio-text large language models",
                "LLMs",
                "music information retrieval",
                "MIR",
                "genre classification",
                "emotion regression",
                "instrument classification",
                "pitch estimation",
                "key detection",
                "lyrics transcription",
                "melody extraction",
                "vocal technique recognition",
                "instrument performance technique detection",
                "music tagging",
                "music captioning",
                "beat tracking",
                "evaluation metrics",
                "cultural bias",
                "chronological bias",
                "gender bias"
            ]
        },
        "publishedAt": "2025-06-13T20:18:44.000Z",
        "title": "CMI-Bench: A Comprehensive Benchmark for Evaluating Music Instruction\n  Following",
        "summary": "Recent advances in audio-text large language models (LLMs) have opened new\npossibilities for music understanding and generation. However, existing\nbenchmarks are limited in scope, often relying on simplified tasks or\nmulti-choice evaluations that fail to reflect the complexity of real-world\nmusic analysis. We reinterpret a broad range of traditional MIR annotations as\ninstruction-following formats and introduce CMI-Bench, a comprehensive music\ninstruction following benchmark designed to evaluate audio-text LLMs on a\ndiverse set of music information retrieval (MIR) tasks. These include genre\nclassification, emotion regression, emotion tagging, instrument classification,\npitch estimation, key detection, lyrics transcription, melody extraction, vocal\ntechnique recognition, instrument performance technique detection, music\ntagging, music captioning, and (down)beat tracking: reflecting core challenges\nin MIR research. Unlike previous benchmarks, CMI-Bench adopts standardized\nevaluation metrics consistent with previous state-of-the-art MIR models,\nensuring direct comparability with supervised approaches. We provide an\nevaluation toolkit supporting all open-source audio-textual LLMs, including\nLTU, Qwen-audio, SALMONN, MusiLingo, etc. Experiment results reveal significant\nperformance gaps between LLMs and supervised models, along with their culture,\nchronological and gender bias, highlighting the potential and limitations of\ncurrent models in addressing MIR tasks. CMI-Bench establishes a unified\nfoundation for evaluating music instruction following, driving progress in\nmusic-aware LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12285.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6410665d5364a661bee22524",
            "avatarUrl": "/avatars/f1cb0e07f36933187ceccbd5dcbeff79.svg",
            "fullname": "Yinghao Ma",
            "name": "nicolaus625",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.14429",
            "authors": [
                {
                    "_id": "68521a9a0164cd131671045c",
                    "user": {
                        "_id": "64f033ef82c6eea604c4da8b",
                        "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
                        "isPro": false,
                        "fullname": "Liu Xiaoran",
                        "user": "LiuXR",
                        "type": "user"
                    },
                    "name": "Xiaoran Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:22.707Z",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd131671045d",
                    "name": "Zhigeng Liu",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd131671045e",
                    "name": "Zengfeng Huang",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd131671045f",
                    "name": "Qipeng Guo",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd1316710460",
                    "name": "Ziwei He",
                    "hidden": false
                },
                {
                    "_id": "68521a9a0164cd1316710461",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T11:45:37.000Z",
            "submittedOnDailyAt": "2025-06-18T00:18:23.135Z",
            "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
            "submittedOnDailyBy": {
                "_id": "64f033ef82c6eea604c4da8b",
                "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
                "isPro": false,
                "fullname": "Liu Xiaoran",
                "user": "LiuXR",
                "type": "user"
            },
            "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textit{stable perplexity} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textit{local perception}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.",
            "upvotes": 34,
            "discussionId": "68521a9a0164cd1316710462",
            "ai_summary": "This study investigates long-context performance of diffusion LLMs compared to auto-regressive LLMs, identifies their unique characteristics, and proposes LongLLaDA, a training-free method for extending context windows.",
            "ai_keywords": [
                "diffusion LLMs",
                "auto-regressive LLMs",
                "stable perplexity",
                "local perception",
                "Rotary Position Embedding (RoPE) scaling theory",
                "LongLLaDA",
                "NTK-based RoPE extrapolation",
                "context extrapolation scaling laws",
                "long-context tasks"
            ]
        },
        "publishedAt": "2025-06-17T07:45:37.000Z",
        "title": "LongLLaDA: Unlocking Long Context Capabilities in Diffusion LLMs",
        "summary": "Large Language Diffusion Models, or diffusion LLMs, have emerged as a\nsignificant focus in NLP research, with substantial effort directed toward\nunderstanding their scalability and downstream task performance. However, their\nlong-context capabilities remain unexplored, lacking systematic analysis or\nmethods for context extension. In this work, we present the first systematic\ninvestigation comparing the long-context performance of diffusion LLMs and\ntraditional auto-regressive LLMs. We first identify a unique characteristic of\ndiffusion LLMs, unlike auto-regressive LLMs, they maintain remarkably\n\\textit{stable perplexity} during direct context extrapolation.\nFurthermore, where auto-regressive models fail outright during the\nNeedle-In-A-Haystack task with context exceeding their pretrained length, we\ndiscover diffusion LLMs exhibit a distinct \\textit{local perception}\nphenomenon, enabling successful retrieval from recent context segments. We\nexplain both phenomena through the lens of Rotary Position Embedding (RoPE)\nscaling theory. Building on these observations, we propose LongLLaDA, a\ntraining-free method that integrates LLaDA with the NTK-based RoPE\nextrapolation. Our results validate that established extrapolation scaling laws\nremain effective for extending the context windows of diffusion LLMs.\nFurthermore, we identify long-context tasks where diffusion LLMs outperform\nauto-regressive LLMs and others where they fall short. Consequently, this study\nestablishes the first context extrapolation method for diffusion LLMs while\nproviding essential theoretical insights and empirical benchmarks critical for\nadvancing future research on long-context diffusion LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14429.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "fullname": "Liu Xiaoran",
            "name": "LiuXR",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14245",
            "authors": [
                {
                    "_id": "68521c2a0164cd131671046b",
                    "user": {
                        "_id": "669f940b3b09946711e20c52",
                        "avatarUrl": "/avatars/27044caec57d8d68d700208fae78b6c6.svg",
                        "isPro": false,
                        "fullname": "XumengWen",
                        "user": "XumengWen",
                        "type": "user"
                    },
                    "name": "Xumeng Wen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:15:48.128Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd131671046c",
                    "name": "Zihan Liu",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd131671046d",
                    "user": {
                        "_id": "64a7a2bad001860e0c34f7f2",
                        "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
                        "isPro": false,
                        "fullname": "Shun Zheng",
                        "user": "shun-zheng",
                        "type": "user"
                    },
                    "name": "Shun Zheng",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-18T02:37:06.379Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd131671046e",
                    "user": {
                        "_id": "67d7c0e0cb3e80af0d13660a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hCmEo__IWO_R_Ps8Q52Os.png",
                        "isPro": false,
                        "fullname": "zhijianxu",
                        "user": "VEWOXIC",
                        "type": "user"
                    },
                    "name": "Zhijian Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:15:46.044Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd131671046f",
                    "name": "Shengyu Ye",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710470",
                    "name": "Zhirong Wu",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710471",
                    "user": {
                        "_id": "6560763e152b659e623865ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liang",
                        "user": "MasterVito",
                        "type": "user"
                    },
                    "name": "Xiao Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:18.590Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710472",
                    "user": {
                        "_id": "604714a0c82d59b7347b55ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/604714a0c82d59b7347b55ae/WZFKDDUPi8JS0BK8t7mIv.jpeg",
                        "isPro": false,
                        "fullname": "YangWang92",
                        "user": "yangwang92",
                        "type": "user"
                    },
                    "name": "Yang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:20.452Z",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710473",
                    "name": "Junjie Li",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710474",
                    "name": "Ziming Miao",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710475",
                    "name": "Jiang Bian",
                    "hidden": false
                },
                {
                    "_id": "68521c2a0164cd1316710476",
                    "name": "Mao Yang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a7a2bad001860e0c34f7f2/zpklFaaRznQyEa0t9Ji70.png"
            ],
            "publishedAt": "2025-06-17T07:06:56.000Z",
            "submittedOnDailyAt": "2025-06-18T01:24:22.712Z",
            "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs",
            "submittedOnDailyBy": {
                "_id": "64a7a2bad001860e0c34f7f2",
                "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
                "isPro": false,
                "fullname": "Shun Zheng",
                "user": "shun-zheng",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npromising paradigm for advancing the reasoning capabilities of Large Language\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\nmodels often underperform their base models on the Pass@K metric for\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\nresolve this contradiction by identifying the source of the problem: the\nPass@K metric itself is a flawed measure of reasoning, as it credits correct\nfinal answers that probably arise from inaccurate or incomplete chains of\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\nCoT-Pass@K, which mandates that both the reasoning path and the final\nanswer be correct. We provide a new theoretical foundation that formalizes how\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\nintegrity. Our empirical results are supportive: using CoT-Pass@K, we\nobserve that RLVR can incentivize the generalization of correct reasoning for\nall values of K. Furthermore, by analyzing the training dynamics, we find\nthat this enhanced reasoning capability emerges early in the training process\nand smoothly generalizes. Our work provides a clear perspective on the role of\nRLVR, offers a more reliable method for its evaluation, and confirms its\npotential to genuinely advance machine reasoning.",
            "upvotes": 25,
            "discussionId": "68521c2a0164cd1316710477",
            "ai_summary": "RLVR advances machine reasoning by incentivizing correct and logical thought chains, addressing limitations identified by a more precise evaluation metric, $CoT$-$Pass@K$.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "LLMS",
                "Pass@K",
                "chains of thought",
                "CoT-Pass@K",
                "logical integrity",
                "machine reasoning",
                "training dynamics"
            ]
        },
        "publishedAt": "2025-06-17T03:06:56.000Z",
        "title": "Reinforcement Learning with Verifiable Rewards Implicitly Incentivizes\n  Correct Reasoning in Base LLMs",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npromising paradigm for advancing the reasoning capabilities of Large Language\nModels (LLMs). However, a critical paradox clouds its efficacy: RLVR-tuned\nmodels often underperform their base models on the Pass@K metric for\nsolution-finding, leading to the hypothesis that RLVR merely re-weights\nexisting reasoning paths at the cost of reasoning diversity. In this work, we\nresolve this contradiction by identifying the source of the problem: the\nPass@K metric itself is a flawed measure of reasoning, as it credits correct\nfinal answers that probably arise from inaccurate or incomplete chains of\nthought (CoTs). To address this, we introduce a more precise evaluation metric,\nCoT-Pass@K, which mandates that both the reasoning path and the final\nanswer be correct. We provide a new theoretical foundation that formalizes how\nRLVR, unlike traditional RL, is uniquely structured to incentivize logical\nintegrity. Our empirical results are supportive: using CoT-Pass@K, we\nobserve that RLVR can incentivize the generalization of correct reasoning for\nall values of K. Furthermore, by analyzing the training dynamics, we find\nthat this enhanced reasoning capability emerges early in the training process\nand smoothly generalizes. Our work provides a clear perspective on the role of\nRLVR, offers a more reliable method for its evaluation, and confirms its\npotential to genuinely advance machine reasoning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a7a2bad001860e0c34f7f2/zpklFaaRznQyEa0t9Ji70.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14245.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64a7a2bad001860e0c34f7f2",
            "avatarUrl": "/avatars/2433104071e4ae1c3e2d755d81d7964b.svg",
            "fullname": "Shun Zheng",
            "name": "shun-zheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14234",
            "authors": [
                {
                    "_id": "68522d7f0164cd13167104ee",
                    "name": "Md Tanzib Hosain",
                    "hidden": false
                },
                {
                    "_id": "68522d7f0164cd13167104ef",
                    "name": "Salman Rahman",
                    "hidden": false
                },
                {
                    "_id": "68522d7f0164cd13167104f0",
                    "name": "Md Kishor Morol",
                    "hidden": false
                },
                {
                    "_id": "68522d7f0164cd13167104f1",
                    "name": "Md Rizwan Parvez",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T06:47:19.000Z",
            "submittedOnDailyAt": "2025-06-18T01:39:00.207Z",
            "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team",
            "submittedOnDailyBy": {
                "_id": "65ae1c4468139e3c42973fe4",
                "avatarUrl": "/avatars/b065a857dd763410caadea37a2dc01c4.svg",
                "isPro": false,
                "fullname": "Md Rizwan Parvez",
                "user": "mparvez",
                "type": "user"
            },
            "summary": "Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/.",
            "upvotes": 24,
            "discussionId": "68522d7f0164cd13167104f2",
            "ai_summary": "Xolver, a multi-agent reasoning framework, enhances large language models with persistent memory and diverse experience modalities, improving performance on complex reasoning tasks by avoiding generating solutions from scratch.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "multi-agent reasoning framework",
                "persistent memory",
                "experience-aware language agents",
                "external and self-retrieval",
                "tool use",
                "collaborative interactions",
                "agent-driven evaluation",
                "iterative refinement",
                "GSM8K",
                "AIME'24",
                "AIME'25",
                "Math-500",
                "LiveCodeBench-V5",
                "generalist agents",
                "expert-level reasoning"
            ]
        },
        "publishedAt": "2025-06-17T02:47:19.000Z",
        "title": "Xolver: Multi-Agent Reasoning with Holistic Experience Learning Just\n  Like an Olympiad Team",
        "summary": "Despite impressive progress on complex reasoning, current large language\nmodels (LLMs) typically operate in isolation - treating each problem as an\nindependent attempt, without accumulating or integrating experiential\nknowledge. In contrast, expert problem solvers - such as Olympiad or\nprogramming contest teams - leverage a rich tapestry of experiences: absorbing\nmentorship from coaches, developing intuition from past problems, leveraging\nknowledge of tool usage and library functionality, adapting strategies based on\nthe expertise and experiences of peers, continuously refining their reasoning\nthrough trial and error, and learning from other related problems even during\ncompetition. We introduce Xolver, a training-free multi-agent reasoning\nframework that equips a black-box LLM with a persistent, evolving memory of\nholistic experience. Xolver integrates diverse experience modalities, including\nexternal and self-retrieval, tool use, collaborative interactions, agent-driven\nevaluation, and iterative refinement. By learning from relevant strategies,\ncode fragments, and abstract reasoning patterns at inference time, Xolver\navoids generating solutions from scratch - marking a transition from isolated\ninference toward experience-aware language agents. Built on both open-weight\nand proprietary models, Xolver consistently outperforms specialized reasoning\nagents. Even with lightweight backbones (e.g., QWQ-32B), it often surpasses\nadvanced models including Qwen3-235B, Gemini 2.5 Pro, o3, and o4-mini-high.\nWith o3-mini-high, it achieves new best results on GSM8K (98.1%), AIME'24\n(94.4%), AIME'25 (93.7%), Math-500 (99.8%), and LiveCodeBench-V5 (91.6%) -\nhighlighting holistic experience learning as a key step toward generalist\nagents capable of expert-level reasoning. Code and data are available at\nhttps://kagnlp.github.io/xolver.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14234.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65ae1c4468139e3c42973fe4",
            "avatarUrl": "/avatars/b065a857dd763410caadea37a2dc01c4.svg",
            "fullname": "Md Rizwan Parvez",
            "name": "mparvez",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.13363",
            "authors": [
                {
                    "_id": "685234100164cd1316710508",
                    "name": "Lijun Liu",
                    "hidden": false
                },
                {
                    "_id": "685234100164cd1316710509",
                    "user": {
                        "_id": "66864a38369b09d564ba2ce4",
                        "avatarUrl": "/avatars/866699ffee8a1d7cf2c9bebe0d3d58fe.svg",
                        "isPro": false,
                        "fullname": "lry",
                        "user": "lryyyy",
                        "type": "user"
                    },
                    "name": "Ruiyang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:59:03.729Z",
                    "hidden": false
                },
                {
                    "_id": "685234100164cd131671050a",
                    "user": {
                        "_id": "633e570be7d5ce7bfe037a53",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg",
                        "isPro": false,
                        "fullname": "Zhaocheng Liu",
                        "user": "zhaocheng",
                        "type": "user"
                    },
                    "name": "Zhaocheng Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:59:00.988Z",
                    "hidden": false
                },
                {
                    "_id": "685234100164cd131671050b",
                    "name": "Chenglin Zhu",
                    "hidden": false
                },
                {
                    "_id": "685234100164cd131671050c",
                    "name": "Chong Li",
                    "hidden": false
                },
                {
                    "_id": "685234100164cd131671050d",
                    "name": "Jiehan Cheng",
                    "hidden": false
                },
                {
                    "_id": "685234100164cd131671050e",
                    "name": "Qiang Ju",
                    "hidden": false
                },
                {
                    "_id": "685234100164cd131671050f",
                    "name": "Jian Xie",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/633e570be7d5ce7bfe037a53/-7W-jpcvZwQ046FQHgVdY.qt"
            ],
            "publishedAt": "2025-06-16T11:10:25.000Z",
            "submittedOnDailyAt": "2025-06-18T02:12:29.534Z",
            "title": "Efficient Medical VIE via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "633e570be7d5ce7bfe037a53",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg",
                "isPro": false,
                "fullname": "Zhaocheng Liu",
                "user": "zhaocheng",
                "type": "user"
            },
            "summary": "Visual Information Extraction (VIE) converts unstructured document images\ninto structured formats like JSON, critical for medical applications such as\nreport analysis and online consultations. Traditional methods rely on OCR and\nlanguage models, while end-to-end multimodal models offer direct JSON\ngeneration. However, domain-specific schemas and high annotation costs limit\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\nfield coverage, and innovative sampling strategies to enhance reasoning\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\nprecision, and recall. While our models excel on tasks similar to medical\ndatasets, performance drops on dissimilar tasks, highlighting the need for\ndomain-specific optimization. Case studies further demonstrate the value of\nreasoning during training and inference for VIE.",
            "upvotes": 22,
            "discussionId": "685234100164cd1316710510",
            "ai_summary": "An RLVR framework using fine-tuned Qwen2.5-VL-7B achieves state-of-the-art performance in medical VIE with limited annotated samples, enhancing reasoning and balance between precision and recall.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "JSON generation",
                "multimodal models",
                "dataset diversity",
                "precision-recall reward mechanism",
                "hallucinations",
                "field coverage",
                "sampling strategies",
                "fine-tuning",
                "Qwen2.5-VL-7B",
                "F1 score",
                "case studies"
            ]
        },
        "publishedAt": "2025-06-16T07:10:25.000Z",
        "title": "Efficient Medical VIE via Reinforcement Learning",
        "summary": "Visual Information Extraction (VIE) converts unstructured document images\ninto structured formats like JSON, critical for medical applications such as\nreport analysis and online consultations. Traditional methods rely on OCR and\nlanguage models, while end-to-end multimodal models offer direct JSON\ngeneration. However, domain-specific schemas and high annotation costs limit\ntheir effectiveness in medical VIE. We base our approach on the Reinforcement\nLearning with Verifiable Rewards (RLVR) framework to address these challenges\nusing only 100 annotated samples. Our approach ensures dataset diversity, a\nbalanced precision-recall reward mechanism to reduce hallucinations and improve\nfield coverage, and innovative sampling strategies to enhance reasoning\ncapabilities. Fine-tuning Qwen2.5-VL-7B with our RLVR method, we achieve\nstate-of-the-art performance on medical VIE tasks, significantly improving F1,\nprecision, and recall. While our models excel on tasks similar to medical\ndatasets, performance drops on dissimilar tasks, highlighting the need for\ndomain-specific optimization. Case studies further demonstrate the value of\nreasoning during training and inference for VIE.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/633e570be7d5ce7bfe037a53/-7W-jpcvZwQ046FQHgVdY.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13363.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633e570be7d5ce7bfe037a53",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633e570be7d5ce7bfe037a53/zV8ULv4Mu7YIGZ8D3JtmK.jpeg",
            "fullname": "Zhaocheng Liu",
            "name": "zhaocheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13642",
            "authors": [
                {
                    "_id": "685129448a68fee7f6ba4c04",
                    "name": "Shaolei Zhang",
                    "hidden": false
                },
                {
                    "_id": "685129448a68fee7f6ba4c05",
                    "name": "Shoutao Guo",
                    "hidden": false
                },
                {
                    "_id": "685129448a68fee7f6ba4c06",
                    "name": "Qingkai Fang",
                    "hidden": false
                },
                {
                    "_id": "685129448a68fee7f6ba4c07",
                    "name": "Yan Zhou",
                    "hidden": false
                },
                {
                    "_id": "685129448a68fee7f6ba4c08",
                    "name": "Yang Feng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4"
            ],
            "publishedAt": "2025-06-16T16:06:45.000Z",
            "submittedOnDailyAt": "2025-06-18T00:16:02.465Z",
            "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
            "submittedOnDailyBy": {
                "_id": "64803e5dc57f629056c601f1",
                "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
                "isPro": false,
                "fullname": "Shaolei Zhang",
                "user": "zhangshaolei",
                "type": "user"
            },
            "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
            "upvotes": 21,
            "discussionId": "685129458a68fee7f6ba4c09",
            "projectPage": "https://github.com/ictnlp/Stream-Omni",
            "githubRepo": "https://github.com/ictnlp/Stream-Omni",
            "ai_summary": "Stream-Omni, a large multimodal model, integrates text, vision, and speech by efficiently aligning modalities using sequence-dimension concatenation for vision and layer-dimension mapping for speech, achieving strong performance with less data.",
            "ai_keywords": [
                "GPT-4o-like",
                "large multimodal models",
                "LLM backbone",
                "modality alignments",
                "sequence-dimension concatenation",
                "CTC-based layer-dimension mapping",
                "visual understanding",
                "speech interaction",
                "vision-grounded speech interaction",
                "ASR transcriptions",
                "model responses"
            ]
        },
        "publishedAt": "2025-06-16T12:06:45.000Z",
        "title": "Stream-Omni: Simultaneous Multimodal Interactions with Large\n  Language-Vision-Speech Model",
        "summary": "The emergence of GPT-4o-like large multimodal models (LMMs) has raised the\nexploration of integrating text, vision, and speech modalities to support more\nflexible multimodal interaction. Existing LMMs typically concatenate\nrepresentation of modalities along the sequence dimension and feed them into a\nlarge language model (LLM) backbone. While sequence-dimension concatenation is\nstraightforward for modality integration, it often relies heavily on\nlarge-scale data to learn modality alignments. In this paper, we aim to model\nthe relationships between modalities more purposefully, thereby achieving more\nefficient and flexible modality alignments. To this end, we propose\nStream-Omni, a large language-vision-speech model with efficient modality\nalignments, which can simultaneously support interactions under various\nmodality combinations. Stream-Omni employs LLM as the backbone and aligns the\nvision and speech to the text based on their relationships. For vision that is\nsemantically complementary to text, Stream-Omni uses sequence-dimension\nconcatenation to achieve vision-text alignment. For speech that is semantically\nconsistent with text, Stream-Omni introduces a CTC-based layer-dimension\nmapping to achieve speech-text alignment. In this way, Stream-Omni can achieve\nmodality alignments with less data (especially speech), enabling the transfer\nof text capabilities to other modalities. Experiments on various benchmarks\ndemonstrate that Stream-Omni achieves strong performance on visual\nunderstanding, speech interaction, and vision-grounded speech interaction\ntasks. Owing to the layer-dimensional mapping, Stream-Omni can simultaneously\nprovide intermediate text outputs (such as ASR transcriptions and model\nresponses) during speech interaction, offering users a comprehensive multimodal\nexperience.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/MBm95m2RAX6iKKBaKTma8.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64803e5dc57f629056c601f1/PoLupV32gI1iLxILZccQS.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13642.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64803e5dc57f629056c601f1",
            "avatarUrl": "/avatars/a9e9c97c70714e3a29bef2cf929ee6b3.svg",
            "fullname": "Shaolei Zhang",
            "name": "zhangshaolei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.14758",
            "authors": [
                {
                    "_id": "685239610164cd1316710553",
                    "user": {
                        "_id": "649e6761f9134a06ed1e0cea",
                        "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
                        "isPro": false,
                        "fullname": "Daixuan Cheng",
                        "user": "daixuancheng",
                        "type": "user"
                    },
                    "name": "Daixuan Cheng",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-18T06:57:21.864Z",
                    "hidden": false
                },
                {
                    "_id": "685239610164cd1316710554",
                    "name": "Shaohan Huang",
                    "hidden": false
                },
                {
                    "_id": "685239610164cd1316710555",
                    "name": "Xuekai Zhu",
                    "hidden": false
                },
                {
                    "_id": "685239610164cd1316710556",
                    "name": "Bo Dai",
                    "hidden": false
                },
                {
                    "_id": "685239610164cd1316710557",
                    "name": "Wayne Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "685239610164cd1316710558",
                    "name": "Zhenliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "685239610164cd1316710559",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UekvaawzSgcb5I120mngD.png",
                "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UbAwRMdcT31OV6ORPZG52.png"
            ],
            "publishedAt": "2025-06-17T17:54:03.000Z",
            "submittedOnDailyAt": "2025-06-18T02:33:04.259Z",
            "title": "Reasoning with Exploration: An Entropy Perspective",
            "submittedOnDailyBy": {
                "_id": "649e6761f9134a06ed1e0cea",
                "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
                "isPro": false,
                "fullname": "Daixuan Cheng",
                "user": "daixuancheng",
                "type": "user"
            },
            "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.",
            "upvotes": 17,
            "discussionId": "685239610164cd131671055a",
            "ai_summary": "Introducing an entropy-based term to the advantage function in reinforcement learning enhances exploratory reasoning in language models, leading to improved performance on complex reasoning tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "entropy",
                "exploratory reasoning",
                "pivotal tokens",
                "reflective actions",
                "rare behaviors",
                "advantage function",
                "Pass@K"
            ]
        },
        "publishedAt": "2025-06-17T13:54:03.000Z",
        "title": "Reasoning with Exploration: An Entropy Perspective",
        "summary": "Balancing exploration and exploitation is a central goal in reinforcement\nlearning (RL). Despite recent advances in enhancing language model (LM)\nreasoning, most methods lean toward exploitation, and increasingly encounter\nperformance plateaus. In this work, we revisit entropy -- a signal of\nexploration in RL -- and examine its relationship to exploratory reasoning in\nLMs. Through empirical analysis, we uncover strong positive correlations\nbetween high-entropy regions and three types of exploratory reasoning actions:\n(1) pivotal tokens that determine or connect logical steps, (2) reflective\nactions such as self-verification and correction, and (3) rare behaviors\nunder-explored by the base LMs. Motivated by this, we introduce a minimal\nmodification to standard RL with only one line of code: augmenting the\nadvantage function with an entropy-based term. Unlike traditional\nmaximum-entropy methods which encourage exploration by promoting uncertainty,\nwe encourage exploration by promoting longer and deeper reasoning chains.\nNotably, our method achieves significant gains on the Pass@K metric -- an\nupper-bound estimator of LM reasoning capabilities -- even when evaluated with\nextremely large K values, pushing the boundaries of LM reasoning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UekvaawzSgcb5I120mngD.png",
            "https://cdn-uploads.huggingface.co/production/uploads/649e6761f9134a06ed1e0cea/UbAwRMdcT31OV6ORPZG52.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14758.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "649e6761f9134a06ed1e0cea",
            "avatarUrl": "/avatars/00b5dcb744c54a4aa18fe08efd70d6ff.svg",
            "fullname": "Daixuan Cheng",
            "name": "daixuancheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14603",
            "authors": [
                {
                    "_id": "68521f8a0164cd1316710481",
                    "name": "Amirmojtaba Sabour",
                    "hidden": false
                },
                {
                    "_id": "68521f8a0164cd1316710482",
                    "name": "Sanja Fidler",
                    "hidden": false
                },
                {
                    "_id": "68521f8a0164cd1316710483",
                    "name": "Karsten Kreis",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg"
            ],
            "publishedAt": "2025-06-17T15:06:07.000Z",
            "submittedOnDailyAt": "2025-06-18T00:42:20.168Z",
            "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
            "submittedOnDailyBy": {
                "_id": "656015f28138827138c70858",
                "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
                "isPro": false,
                "fullname": "Amirmojtaba Sabour",
                "user": "amsabour",
                "type": "user"
            },
            "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.",
            "upvotes": 14,
            "discussionId": "68521f8a0164cd1316710484",
            "projectPage": "https://research.nvidia.com/labs/toronto-ai/AlignYourFlow/",
            "ai_summary": "Flow maps, introduced with new continuous-time objectives and training techniques, achieve state-of-the-art performance in few-step image and text-to-image generation.",
            "ai_keywords": [
                "diffusion models",
                "flow-based models",
                "consistency models",
                "flow maps",
                "noise levels",
                "autoguidance",
                "adversarial finetuning",
                "Align Your Flow",
                "ImageNet",
                "text-to-image synthesis"
            ]
        },
        "publishedAt": "2025-06-17T11:06:07.000Z",
        "title": "Align Your Flow: Scaling Continuous-Time Flow Map Distillation",
        "summary": "Diffusion- and flow-based models have emerged as state-of-the-art generative\nmodeling approaches, but they require many sampling steps. Consistency models\ncan distill these models into efficient one-step generators; however, unlike\nflow- and diffusion-based methods, their performance inevitably degrades when\nincreasing the number of steps, which we show both analytically and\nempirically. Flow maps generalize these approaches by connecting any two noise\nlevels in a single step and remain effective across all step counts. In this\npaper, we introduce two new continuous-time objectives for training flow maps,\nalong with additional novel training techniques, generalizing existing\nconsistency and flow matching objectives. We further demonstrate that\nautoguidance can improve performance, using a low-quality model for guidance\nduring distillation, and an additional boost can be achieved by adversarial\nfinetuning, with minimal loss in sample diversity. We extensively validate our\nflow map models, called Align Your Flow, on challenging image generation\nbenchmarks and achieve state-of-the-art few-step generation performance on both\nImageNet 64x64 and 512x512, using small and efficient neural networks. Finally,\nwe show text-to-image flow map models that outperform all existing\nnon-adversarially trained few-step samplers in text-conditioned synthesis.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/_yuUKzjzngNZzaUJCVwjR.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/2FmUhOtaDCtAzFkNEx6Di.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/MC_ZLBqT81dOpANroMLrm.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/656015f28138827138c70858/degPI6Z5IEf4v_y2_UL9V.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14603.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "656015f28138827138c70858",
            "avatarUrl": "/avatars/b8471ae4d80f078c7c928fc3d8f49126.svg",
            "fullname": "Amirmojtaba Sabour",
            "name": "amsabour",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.12278",
            "authors": [
                {
                    "_id": "685234030164cd1316710502",
                    "name": "Zheyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "685234030164cd1316710503",
                    "name": "Zexi Kuang",
                    "hidden": false
                },
                {
                    "_id": "685234030164cd1316710504",
                    "name": "Xue Xia",
                    "hidden": false
                },
                {
                    "_id": "685234030164cd1316710505",
                    "user": {
                        "_id": "62f662bcc58915315c4eccea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                        "isPro": true,
                        "fullname": "Yilun Zhao",
                        "user": "yilunzhao",
                        "type": "user"
                    },
                    "name": "Yilun Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:59:08.920Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T23:56:17.000Z",
            "submittedOnDailyAt": "2025-06-18T02:08:35.444Z",
            "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure",
            "submittedOnDailyBy": {
                "_id": "62f662bcc58915315c4eccea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                "isPro": true,
                "fullname": "Yilun Zhao",
                "user": "yilunzhao",
                "type": "user"
            },
            "summary": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems.",
            "upvotes": 14,
            "discussionId": "685234030164cd1316710506",
            "githubRepo": "https://github.com/FlowRays/TestCase-Eval",
            "ai_summary": "TestCase-Eval is a benchmark for evaluating LLMs in generating comprehensive and targeted test cases for algorithm problems.",
            "ai_keywords": [
                "test-case generation",
                "Fault Coverage",
                "Fault Exposure",
                "LLMs",
                "algorithm problems",
                "human-crafted solutions",
                "Codeforces",
                "test sets",
                "failure modes",
                "incorrect code implementation"
            ]
        },
        "publishedAt": "2025-06-13T19:56:17.000Z",
        "title": "Can LLMs Generate High-Quality Test Cases for Algorithm Problems?\n  TestCase-Eval: A Systematic Evaluation of Fault Coverage and Exposure",
        "summary": "We introduce TestCase-Eval, a new benchmark for systematic evaluation of LLMs\nin test-case generation. TestCase-Eval includes 500 algorithm problems and\n100,000 human-crafted solutions from the Codeforces platform. It focuses on two\npivotal tasks: (1) Fault Coverage, which measures how well LLM-generated test\nsets probe diverse input scenarios and cover a wide range of potential failure\nmodes. (2) Fault Exposure, which evaluates whether LLMs can craft a tailored\ntest input that reveals a specific incorrect code implementation. We provide a\ncomprehensive assessment of 19 state-of-the-art open-source and proprietary\nLLMs on TestCase-Eval, offering insights into their strengths and limitations\nin generating effective test cases for algorithm problems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12278.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "fullname": "Yilun Zhao",
            "name": "yilunzhao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.12860",
            "authors": [
                {
                    "_id": "685226a40164cd13167104bd",
                    "user": {
                        "_id": "64eb333e6878d90b031fa5c5",
                        "avatarUrl": "/avatars/a0d875b49d1c56be88f34854647306da.svg",
                        "isPro": false,
                        "fullname": "Wanlong Liu",
                        "user": "lwl-uestc",
                        "type": "user"
                    },
                    "name": "Wanlong Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:59:15.053Z",
                    "hidden": false
                },
                {
                    "_id": "685226a40164cd13167104be",
                    "name": "Junxiao Xu",
                    "hidden": false
                },
                {
                    "_id": "685226a40164cd13167104bf",
                    "name": "Fei Yu",
                    "hidden": false
                },
                {
                    "_id": "685226a40164cd13167104c0",
                    "name": "Yukang Lin",
                    "hidden": false
                },
                {
                    "_id": "685226a40164cd13167104c1",
                    "name": "Ke Ji",
                    "hidden": false
                },
                {
                    "_id": "685226a40164cd13167104c2",
                    "name": "Wenyu Chen",
                    "hidden": false
                },
                {
                    "_id": "685226a40164cd13167104c3",
                    "name": "Yan Xu",
                    "hidden": false
                },
                {
                    "_id": "685226a40164cd13167104c4",
                    "name": "Yasheng Wang",
                    "hidden": false
                },
                {
                    "_id": "685226a40164cd13167104c5",
                    "name": "Lifeng Shang",
                    "hidden": false
                },
                {
                    "_id": "685226a40164cd13167104c6",
                    "name": "Benyou Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-15T14:21:28.000Z",
            "submittedOnDailyAt": "2025-06-18T02:06:38.533Z",
            "title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning",
            "submittedOnDailyBy": {
                "_id": "64eb333e6878d90b031fa5c5",
                "avatarUrl": "/avatars/a0d875b49d1c56be88f34854647306da.svg",
                "isPro": false,
                "fullname": "Wanlong Liu",
                "user": "lwl-uestc",
                "type": "user"
            },
            "summary": "Recent advancements in Long Chain-of-Thought (CoT) reasoning models have\nimproved performance on complex tasks, but they suffer from overthinking, which\ngenerates redundant reasoning steps, especially for simple questions. This\npaper revisits the reasoning patterns of Long and Short CoT models, observing\nthat the Short CoT patterns offer concise reasoning efficiently, while the Long\nCoT patterns excel in challenging scenarios where the Short CoT patterns\nstruggle. To enable models to leverage both patterns, we propose Question-Free\nFine-Tuning (QFFT), a fine-tuning approach that removes the input question\nduring training and learns exclusively from Long CoT responses. This approach\nenables the model to adaptively employ both reasoning patterns: it prioritizes\nthe Short CoT patterns and activates the Long CoT patterns only when necessary.\nExperiments on various mathematical datasets demonstrate that QFFT reduces\naverage response length by more than 50\\%, while achieving performance\ncomparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits\nsuperior performance compared to SFT in noisy, out-of-domain, and low-resource\nscenarios.",
            "upvotes": 13,
            "discussionId": "685226a40164cd13167104c7",
            "githubRepo": "https://github.com/LWL-cpu/Question-Free-Fine-Tuning",
            "ai_summary": "Question-Free Fine-Tuning (QFFT) improves efficiency and adaptability in cognitive models by leveraging both short and long chain-of-thought patterns, reducing response length while maintaining performance across various scenarios.",
            "ai_keywords": [
                "Long Chain-of-Thought",
                "Short Chain-of-Thought",
                "Question-Free Fine-Tuning",
                "fine-tuning",
                "Supervised Fine-Tuning"
            ]
        },
        "publishedAt": "2025-06-15T10:21:28.000Z",
        "title": "QFFT, Question-Free Fine-Tuning for Adaptive Reasoning",
        "summary": "Recent advancements in Long Chain-of-Thought (CoT) reasoning models have\nimproved performance on complex tasks, but they suffer from overthinking, which\ngenerates redundant reasoning steps, especially for simple questions. This\npaper revisits the reasoning patterns of Long and Short CoT models, observing\nthat the Short CoT patterns offer concise reasoning efficiently, while the Long\nCoT patterns excel in challenging scenarios where the Short CoT patterns\nstruggle. To enable models to leverage both patterns, we propose Question-Free\nFine-Tuning (QFFT), a fine-tuning approach that removes the input question\nduring training and learns exclusively from Long CoT responses. This approach\nenables the model to adaptively employ both reasoning patterns: it prioritizes\nthe Short CoT patterns and activates the Long CoT patterns only when necessary.\nExperiments on various mathematical datasets demonstrate that QFFT reduces\naverage response length by more than 50\\%, while achieving performance\ncomparable to Supervised Fine-Tuning (SFT). Additionally, QFFT exhibits\nsuperior performance compared to SFT in noisy, out-of-domain, and low-resource\nscenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12860.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64eb333e6878d90b031fa5c5",
            "avatarUrl": "/avatars/a0d875b49d1c56be88f34854647306da.svg",
            "fullname": "Wanlong Liu",
            "name": "lwl-uestc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09985",
            "authors": [
                {
                    "_id": "684b22c63b733ba333686eed",
                    "name": "Mido Assran",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686eee",
                    "name": "Adrien Bardes",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686eef",
                    "name": "David Fan",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef0",
                    "name": "Quentin Garrido",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef1",
                    "name": "Russell Howes",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef2",
                    "name": "Mojtaba",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef3",
                    "name": "Komeili",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef4",
                    "name": "Matthew Muckley",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef5",
                    "name": "Ammar Rizvi",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef6",
                    "name": "Claire Roberts",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef7",
                    "name": "Koustuv Sinha",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef8",
                    "name": "Artem Zholus",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686ef9",
                    "name": "Sergio Arnaud",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686efa",
                    "name": "Abha Gejji",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686efb",
                    "name": "Ada Martin",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686efc",
                    "name": "Francois Robert Hogan",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686efd",
                    "name": "Daniel Dugas",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686efe",
                    "name": "Piotr Bojanowski",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686eff",
                    "name": "Vasil Khalidov",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f00",
                    "name": "Patrick Labatut",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f01",
                    "name": "Francisco Massa",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f02",
                    "name": "Marc Szafraniec",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f03",
                    "name": "Kapil Krishnakumar",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f04",
                    "name": "Yong Li",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f05",
                    "name": "Xiaodong Ma",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f06",
                    "name": "Sarath Chandar",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f07",
                    "name": "Franziska Meier",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f08",
                    "name": "Yann LeCun",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f09",
                    "name": "Michael Rabbat",
                    "hidden": false
                },
                {
                    "_id": "684b22c63b733ba333686f0a",
                    "name": "Nicolas Ballas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:57:09.000Z",
            "submittedOnDailyAt": "2025-06-18T12:40:43.816Z",
            "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\n  and Planning",
            "submittedOnDailyBy": {
                "_id": "622b93067b9143726fbedc37",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647023505150-622b93067b9143726fbedc37.jpeg",
                "isPro": false,
                "fullname": "Koustuv Sinha",
                "user": "koustuvs",
                "type": "user"
            },
            "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.",
            "upvotes": 11,
            "discussionId": "684b22c73b733ba333686f0b",
            "ai_summary": "A self-supervised approach combining internet video data and minimal robot interaction achieves strong performances in motion understanding, action anticipation, video question-answering, and robotic planning without task-specific training or reward.",
            "ai_keywords": [
                "self-supervised learning",
                "joint-embedding-predictive architecture",
                "V-JEPA 2",
                "motion understanding",
                "human action anticipation",
                "video question-answering",
                "latent action-conditioned world model",
                "V-JEPA 2-AC",
                "robotic planning",
                "zero-shot deployment",
                "Franka arms",
                "image goals"
            ]
        },
        "publishedAt": "2025-06-11T13:57:09.000Z",
        "title": "V-JEPA 2: Self-Supervised Video Models Enable Understanding, Prediction\n  and Planning",
        "summary": "A major challenge for modern AI is to learn to understand the world and learn\nto act largely by observation. This paper explores a self-supervised approach\nthat combines internet-scale video data with a small amount of interaction data\n(robot trajectories), to develop models capable of understanding, predicting,\nand planning in the physical world. We first pre-train an action-free\njoint-embedding-predictive architecture, V-JEPA 2, on a video and image dataset\ncomprising over 1 million hours of internet video. V-JEPA 2 achieves strong\nperformance on motion understanding (77.3 top-1 accuracy on Something-Something\nv2) and state-of-the-art performance on human action anticipation (39.7\nrecall-at-5 on Epic-Kitchens-100) surpassing previous task-specific models.\nAdditionally, after aligning V-JEPA 2 with a large language model, we\ndemonstrate state-of-the-art performance on multiple video question-answering\ntasks at the 8 billion parameter scale (e.g., 84.0 on PerceptionTest, 76.9 on\nTempCompass). Finally, we show how self-supervised learning can be applied to\nrobotic planning tasks by post-training a latent action-conditioned world\nmodel, V-JEPA 2-AC, using less than 62 hours of unlabeled robot videos from the\nDroid dataset. We deploy V-JEPA 2-AC zero-shot on Franka arms in two different\nlabs and enable picking and placing of objects using planning with image goals.\nNotably, this is achieved without collecting any data from the robots in these\nenvironments, and without any task-specific training or reward. This work\ndemonstrates how self-supervised learning from web-scale data and a small\namount of robot interaction data can yield a world model capable of planning in\nthe physical world.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09985.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "622b93067b9143726fbedc37",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1647023505150-622b93067b9143726fbedc37.jpeg",
            "fullname": "Koustuv Sinha",
            "name": "koustuvs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.14606",
            "authors": [
                {
                    "_id": "68521f240164cd131671047a",
                    "user": {
                        "_id": "656864e12d73834278a8dea7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                        "isPro": true,
                        "fullname": "Ahmed Heakl",
                        "user": "ahmedheakl",
                        "type": "user"
                    },
                    "name": "Ahmed Heakl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:15:43.091Z",
                    "hidden": false
                },
                {
                    "_id": "68521f240164cd131671047b",
                    "user": {
                        "_id": "62676a94dacab364889bb36c",
                        "avatarUrl": "/avatars/0ead41b44957eb30564ea685ed22781a.svg",
                        "isPro": false,
                        "fullname": "SARIM HASHMI",
                        "user": "Sarim-Hash",
                        "type": "user"
                    },
                    "name": "Sarim Hashmi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:59:21.334Z",
                    "hidden": false
                },
                {
                    "_id": "68521f240164cd131671047c",
                    "name": "Chaimaa Abi",
                    "hidden": false
                },
                {
                    "_id": "68521f240164cd131671047d",
                    "name": "Celine Lee",
                    "hidden": false
                },
                {
                    "_id": "68521f240164cd131671047e",
                    "name": "Abdulrahman Mahmoud",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png",
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png"
            ],
            "publishedAt": "2025-06-17T15:06:54.000Z",
            "submittedOnDailyAt": "2025-06-18T00:38:14.336Z",
            "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
            "submittedOnDailyBy": {
                "_id": "656864e12d73834278a8dea7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                "isPro": true,
                "fullname": "Ahmed Heakl",
                "user": "ahmedheakl",
                "type": "user"
            },
            "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
            "upvotes": 10,
            "discussionId": "68521f240164cd131671047f",
            "projectPage": "https://ahmedheakl.github.io/Guaranteed-Guess/",
            "githubRepo": "https://github.com/ahmedheakl/Guaranteed-Guess",
            "ai_summary": "A novel ISA-centric transpilation pipeline using LLMs and software testing achieves high correctness and efficiency in translating between complex and reduced hardware architectures.",
            "ai_keywords": [
                "pre-trained large language models",
                "software testing constructs",
                "ISA-centric transpilation",
                "complex-instruction set computing (CISC)",
                "reduced-instruction set computing (RISC)",
                "instruction set architecture (ISA)",
                "HumanEval",
                "BringupBench",
                "Rosetta 2 framework",
                "functional/semantic correctness",
                "real-world CISC-to-RISC translation",
                "memory models",
                "execution paradigms",
                "transpilation",
                "hardware ecosystem",
                "low-level programs",
                "code portability",
                "longevity"
            ]
        },
        "publishedAt": "2025-06-17T11:06:54.000Z",
        "title": "Guaranteed Guess: A Language Modeling Approach for CISC-to-RISC\n  Transpilation with Testing Guarantees",
        "summary": "The hardware ecosystem is rapidly evolving, with increasing interest in\ntranslating low-level programs across different instruction set architectures\n(ISAs) in a quick, flexible, and correct way to enhance the portability and\nlongevity of existing code. A particularly challenging class of this\ntranspilation problem is translating between complex- (CISC) and reduced-\n(RISC) hardware architectures, due to fundamental differences in instruction\ncomplexity, memory models, and execution paradigms. In this work, we introduce\nGG (Guaranteed Guess), an ISA-centric transpilation pipeline that combines the\ntranslation power of pre-trained large language models (LLMs) with the rigor of\nestablished software testing constructs. Our method generates candidate\ntranslations using an LLM from one ISA to another, and embeds such translations\nwithin a software-testing framework to build quantifiable confidence in the\ntranslation. We evaluate our GG approach over two diverse datasets, enforce\nhigh code coverage (>98%) across unit tests, and achieve functional/semantic\ncorrectness of 99% on HumanEval programs and 49% on BringupBench programs,\nrespectively. Further, we compare our approach to the state-of-the-art Rosetta\n2 framework on Apple Silicon, showcasing 1.73x faster runtime performance,\n1.47x better energy efficiency, and 2.41x better memory usage for our\ntranspiled code, demonstrating the effectiveness of GG for real-world\nCISC-to-RISC translation tasks. We will open-source our codes, data, models,\nand benchmarks to establish a common foundation for ISA-level code translation\nresearch.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G_EGzMfb1C6fX_o-yLFbl.png",
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/G2UhoU9mbZ8tQ0VzmKLV5.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14606.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "fullname": "Ahmed Heakl",
            "name": "ahmedheakl",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 41
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10100",
            "authors": [
                {
                    "_id": "68522b190164cd13167104d9",
                    "name": "Yantai Yang",
                    "hidden": false
                },
                {
                    "_id": "68522b190164cd13167104da",
                    "name": "Yuhao Wang",
                    "hidden": false
                },
                {
                    "_id": "68522b190164cd13167104db",
                    "name": "Zichen Wen",
                    "hidden": false
                },
                {
                    "_id": "68522b190164cd13167104dc",
                    "name": "Luo Zhongwei",
                    "hidden": false
                },
                {
                    "_id": "68522b190164cd13167104dd",
                    "name": "Chang Zou",
                    "hidden": false
                },
                {
                    "_id": "68522b190164cd13167104de",
                    "name": "Zhipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68522b190164cd13167104df",
                    "name": "Chuan Wen",
                    "hidden": false
                },
                {
                    "_id": "68522b190164cd13167104e0",
                    "name": "Linfeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T18:34:57.000Z",
            "submittedOnDailyAt": "2025-06-18T01:30:42.708Z",
            "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
            "submittedOnDailyBy": {
                "_id": "653b8c3e97a4d71d950e2f20",
                "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
                "isPro": false,
                "fullname": "Zichen Wen",
                "user": "zichenwen",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
            "upvotes": 8,
            "discussionId": "68522b190164cd13167104e1",
            "ai_summary": "EfficientVLA accelerates Vision-Language-Action models by pruning language layers, optimizing visual token selection, and caching intermediate features in the diffusion-based action head.",
            "ai_keywords": [
                "diffusion-based architectures",
                "inference acceleration framework",
                "pruning",
                "inter-layer redundancies",
                "visual tokens",
                "task-aware strategy",
                "iterative diffusion-based action head",
                "caching",
                "FLOPs",
                "SIMPLER benchmark"
            ]
        },
        "publishedAt": "2025-06-11T14:34:57.000Z",
        "title": "EfficientVLA: Training-Free Acceleration and Compression for\n  Vision-Language-Action Models",
        "summary": "Vision-Language-Action (VLA) models, particularly diffusion-based\narchitectures, demonstrate transformative potential for embodied intelligence\nbut are severely hampered by high computational and memory demands stemming\nfrom extensive inherent and inference-time redundancies. While existing\nacceleration efforts often target isolated inefficiencies, such piecemeal\nsolutions typically fail to holistically address the varied computational and\nmemory bottlenecks across the entire VLA pipeline, thereby limiting practical\ndeployability. We introduce EfficientVLA, a structured and training-free\ninference acceleration framework that systematically eliminates these barriers\nby cohesively exploiting multifaceted redundancies. EfficientVLA\nsynergistically integrates three targeted strategies: (1) pruning of\nfunctionally inconsequential layers from the language module, guided by an\nanalysis of inter-layer redundancies; (2) optimizing the visual processing\npathway through a task-aware strategy that selects a compact, diverse set of\nvisual tokens, balancing task-criticality with informational coverage; and (3)\nalleviating temporal computational redundancy within the iterative\ndiffusion-based action head by strategically caching and reusing key\nintermediate features. We apply our method to a standard VLA model CogACT,\nyielding a 1.93X inference speedup and reduces FLOPs to 28.9%, with only a 0.6%\nsuccess rate drop in the SIMPLER benchmark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10100.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653b8c3e97a4d71d950e2f20",
            "avatarUrl": "/avatars/b68880022e14556d0be58c69615db3be.svg",
            "fullname": "Zichen Wen",
            "name": "zichenwen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.13977",
            "authors": [
                {
                    "_id": "68524ff90164cd13167105aa",
                    "name": "Shiting Huang",
                    "hidden": false
                },
                {
                    "_id": "68524ff90164cd13167105ab",
                    "user": {
                        "_id": "64b0a5037a475fba70a7260d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
                        "isPro": false,
                        "fullname": "Zhen Fang",
                        "user": "CostaliyA",
                        "type": "user"
                    },
                    "name": "Zhen Fang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:58:56.933Z",
                    "hidden": false
                },
                {
                    "_id": "68524ff90164cd13167105ac",
                    "name": "Zehui Chen",
                    "hidden": false
                },
                {
                    "_id": "68524ff90164cd13167105ad",
                    "name": "Siyu Yuan",
                    "hidden": false
                },
                {
                    "_id": "68524ff90164cd13167105ae",
                    "name": "Junjie Ye",
                    "hidden": false
                },
                {
                    "_id": "68524ff90164cd13167105af",
                    "name": "Yu Zeng",
                    "hidden": false
                },
                {
                    "_id": "68524ff90164cd13167105b0",
                    "name": "Lin Chen",
                    "hidden": false
                },
                {
                    "_id": "68524ff90164cd13167105b1",
                    "name": "Qi Mao",
                    "hidden": false
                },
                {
                    "_id": "68524ff90164cd13167105b2",
                    "name": "Feng Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-11T17:59:18.000Z",
            "submittedOnDailyAt": "2025-06-18T04:10:42.444Z",
            "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios",
            "submittedOnDailyBy": {
                "_id": "64b0a5037a475fba70a7260d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
                "isPro": false,
                "fullname": "Zhen Fang",
                "user": "CostaliyA",
                "type": "user"
            },
            "summary": "The ability of large language models (LLMs) to utilize external tools has\nenabled them to tackle an increasingly diverse range of tasks. However, as the\ntasks become more complex and long-horizon, the intricate tool utilization\nprocess may trigger various unexpected errors. Therefore, how to effectively\nhandle such errors, including identifying, diagnosing, and recovering from\nthem, has emerged as a key research direction for advancing tool learning. In\nthis work, we first extensively analyze the types of errors encountered during\nthe function-calling process on several competitive tool evaluation benchmarks.\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\nbenchmark specialized for tool learning. Building upon a novel evolutionary\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\nwith varying complexities, which better reflects real-world scenarios. We\nconduct extensive experiments on CRITICTOOL, and validate the generalization\nand effectiveness of our constructed benchmark strategy. We also provide an\nin-depth analysis of the tool reflection ability on various LLMs, offering a\nnew perspective on the field of tool learning in LLMs. The code is available at\nhttps://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.",
            "upvotes": 8,
            "discussionId": "68524ff90164cd13167105b3",
            "githubRepo": "https://github.com/Shellorley0513/CriticTool",
            "ai_summary": "A comprehensive benchmark, CRITICTOOL, evaluates and enhances the robustness of large language models in handling errors during tool usage.",
            "ai_keywords": [
                "large language models",
                "tool learning",
                "function-calling process",
                "error identification",
                "error diagnosis",
                "error recovery",
                "evolutionary strategy",
                "dataset construction",
                "tool reflection ability",
                "critique evaluation benchmark"
            ]
        },
        "publishedAt": "2025-06-11T13:59:18.000Z",
        "title": "CRITICTOOL: Evaluating Self-Critique Capabilities of Large Language\n  Models in Tool-Calling Error Scenarios",
        "summary": "The ability of large language models (LLMs) to utilize external tools has\nenabled them to tackle an increasingly diverse range of tasks. However, as the\ntasks become more complex and long-horizon, the intricate tool utilization\nprocess may trigger various unexpected errors. Therefore, how to effectively\nhandle such errors, including identifying, diagnosing, and recovering from\nthem, has emerged as a key research direction for advancing tool learning. In\nthis work, we first extensively analyze the types of errors encountered during\nthe function-calling process on several competitive tool evaluation benchmarks.\nBased on it, we introduce CRITICTOOL, a comprehensive critique evaluation\nbenchmark specialized for tool learning. Building upon a novel evolutionary\nstrategy for dataset construction, CRITICTOOL holds diverse tool-use errors\nwith varying complexities, which better reflects real-world scenarios. We\nconduct extensive experiments on CRITICTOOL, and validate the generalization\nand effectiveness of our constructed benchmark strategy. We also provide an\nin-depth analysis of the tool reflection ability on various LLMs, offering a\nnew perspective on the field of tool learning in LLMs. The code is available at\nhttps://github.com/Shellorley0513/CriticTool{https://github.com/Shellorley0513/CriticTool}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13977.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b0a5037a475fba70a7260d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64b0a5037a475fba70a7260d/MauBbb6raMA23yrR1Zq21.jpeg",
            "fullname": "Zhen Fang",
            "name": "CostaliyA",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13651",
            "authors": [
                {
                    "_id": "6850cf555e07650ecce88fe2",
                    "name": "Kaiyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fe3",
                    "name": "Yixin Ren",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fe4",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fe5",
                    "name": "Xiaobo Hu",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fe6",
                    "name": "Haotong Tian",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fe7",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fe8",
                    "user": {
                        "_id": "6505a02f9310ce8c400edc63",
                        "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
                        "isPro": false,
                        "fullname": "Fangfu Liu",
                        "user": "Liuff23",
                        "type": "user"
                    },
                    "name": "Fangfu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:52.421Z",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fe9",
                    "name": "Haoye Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fea",
                    "name": "Hongzhang Liu",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88feb",
                    "name": "Yuan Gong",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fec",
                    "name": "Chen Sun",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fed",
                    "name": "Han Hou",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fee",
                    "name": "Hui Yang",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fef",
                    "name": "James Pan",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff0",
                    "name": "Jianan Lou",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff1",
                    "name": "Jiayi Mao",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff2",
                    "name": "Jizheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff3",
                    "name": "Jinpeng Li",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff4",
                    "name": "Kangyi Liu",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff5",
                    "name": "Kenkun Liu",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff6",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff7",
                    "name": "Run Li",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff8",
                    "name": "Tong Niu",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ff9",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ffa",
                    "name": "Wenqi Yan",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ffb",
                    "name": "Xuanzheng Wang",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ffc",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ffd",
                    "name": "Yi-Hsin Hung",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88ffe",
                    "name": "Yuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce88fff",
                    "name": "Zexuan Liu",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce89000",
                    "name": "Zihan Yin",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce89001",
                    "name": "Zijian Ma",
                    "hidden": false
                },
                {
                    "_id": "6850cf555e07650ecce89002",
                    "name": "Zhiwen Mo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T16:16:14.000Z",
            "submittedOnDailyAt": "2025-06-18T04:38:47.336Z",
            "title": "xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations",
            "submittedOnDailyBy": {
                "_id": "6505a02f9310ce8c400edc63",
                "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
                "isPro": false,
                "fullname": "Fangfu Liu",
                "user": "Liuff23",
                "type": "user"
            },
            "summary": "We introduce xbench, a dynamic, profession-aligned evaluation suite designed\nto bridge the gap between AI agent capabilities and real-world productivity.\nWhile existing benchmarks often focus on isolated technical skills, they may\nnot accurately reflect the economic value agents deliver in professional\nsettings. To address this, xbench targets commercially significant domains with\nevaluation tasks defined by industry professionals. Our framework creates\nmetrics that strongly correlate with productivity value, enables prediction of\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\nover time. As our initial implementations, we present two benchmarks:\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\nheadhunting business scenarios to evaluate agents' abilities in company\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\nagents' ability to match influencers with advertiser needs, evaluating their\nperformance across 50 advertiser requirements using a curated pool of 836\ncandidate influencers. We present initial evaluation results for leading\ncontemporary agents, establishing a baseline for these professional domains.\nOur continuously updated evalsets and evaluations are available at\nhttps://xbench.org.",
            "upvotes": 6,
            "discussionId": "6850cf555e07650ecce89003",
            "projectPage": "https://xbench.org/",
            "githubRepo": "https://github.com/xbench-ai/xbench-evals"
        },
        "publishedAt": "2025-06-16T12:16:14.000Z",
        "title": "xbench: Tracking Agents Productivity Scaling with Profession-Aligned\n  Real-World Evaluations",
        "summary": "We introduce xbench, a dynamic, profession-aligned evaluation suite designed\nto bridge the gap between AI agent capabilities and real-world productivity.\nWhile existing benchmarks often focus on isolated technical skills, they may\nnot accurately reflect the economic value agents deliver in professional\nsettings. To address this, xbench targets commercially significant domains with\nevaluation tasks defined by industry professionals. Our framework creates\nmetrics that strongly correlate with productivity value, enables prediction of\nTechnology-Market Fit (TMF), and facilitates tracking of product capabilities\nover time. As our initial implementations, we present two benchmarks:\nRecruitment and Marketing. For Recruitment, we collect 50 tasks from real-world\nheadhunting business scenarios to evaluate agents' abilities in company\nmapping, information retrieval, and talent sourcing. For Marketing, we assess\nagents' ability to match influencers with advertiser needs, evaluating their\nperformance across 50 advertiser requirements using a curated pool of 836\ncandidate influencers. We present initial evaluation results for leading\ncontemporary agents, establishing a baseline for these professional domains.\nOur continuously updated evalsets and evaluations are available at\nhttps://xbench.org.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13651.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6505a02f9310ce8c400edc63",
            "avatarUrl": "/avatars/bbf781594fc8c812316711aa8e2797aa.svg",
            "fullname": "Fangfu Liu",
            "name": "Liuff23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14755",
            "authors": [
                {
                    "_id": "685225250164cd13167104aa",
                    "user": {
                        "_id": "669096da35cddb688a352ca8",
                        "avatarUrl": "/avatars/5dd096cb7360682016d0fca909ab9744.svg",
                        "isPro": false,
                        "fullname": "zxiang",
                        "user": "zx10086",
                        "type": "user"
                    },
                    "name": "Zhengxiang Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T16:15:41.166Z",
                    "hidden": false
                },
                {
                    "_id": "685225250164cd13167104ab",
                    "name": "Dongping Chen",
                    "hidden": false
                },
                {
                    "_id": "685225250164cd13167104ac",
                    "name": "Mingyang Fu",
                    "hidden": false
                },
                {
                    "_id": "685225250164cd13167104ad",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:59:17.278Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T17:50:16.000Z",
            "submittedOnDailyAt": "2025-06-18T01:05:46.554Z",
            "title": "Optimizing Length Compression in Large Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
            "upvotes": 5,
            "discussionId": "685225250164cd13167104ae",
            "githubRepo": "https://github.com/zxiangx/LC-R1",
            "ai_summary": "LC-R1, a post-training method guided by Brevity and Sufficiency principles, reduces unnecessary reasoning in Large Reasoning Models with minimal accuracy loss.",
            "ai_keywords": [
                "Large Reasoning Models",
                "LRM",
                "post-training method",
                "Group Relative Policy Optimization",
                "GRPO",
                "Length Reward",
                "Compress Reward",
                "reasoning benchmarks",
                "Pareto frontier"
            ]
        },
        "publishedAt": "2025-06-17T13:50:16.000Z",
        "title": "Optimizing Length Compression in Large Reasoning Models",
        "summary": "Large Reasoning Models (LRMs) have achieved remarkable success, yet they\noften suffer from producing unnecessary and verbose reasoning chains. We\nidentify a core aspect of this issue as \"invalid thinking\" -- models tend to\nrepeatedly double-check their work after having derived the correct answer. To\naddress this specific inefficiency, we move beyond the general principles of\nEfficacy and Efficiency to propose two new, fine-grained principles: Brevity,\nwhich advocates for eliminating redundancy, and Sufficiency, which ensures\ncritical reasoning steps are preserved. Guided by these principles, we\nintroduce LC-R1, a post-training method based on Group Relative Policy\nOptimization (GRPO). LC-R1 employs a novel combination of a Length Reward for\noverall conciseness and a Compress Reward that is specifically designed to\nremove the invalid portion of the thinking process. Extensive experiments on\nmultiple reasoning benchmarks demonstrate that LC-R1 achieves a significant\nreduction in sequence length (~50%) with only a marginal (~2%) drop in\naccuracy, achieving a favorable trade-off point on the Pareto frontier that\nprioritizes high compression. Our analysis further validates the robustness of\nLC-R1 and provides valuable insights for developing more powerful yet\ncomputationally efficient LRMs. Our code is released at\nhttps://github.com/zxiangx/LC-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14755.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14002",
            "authors": [
                {
                    "_id": "685210eb0164cd131671043e",
                    "name": "Siyu Chen",
                    "hidden": false
                },
                {
                    "_id": "685210eb0164cd131671043f",
                    "name": "Heejune Sheen",
                    "hidden": false
                },
                {
                    "_id": "685210eb0164cd1316710440",
                    "name": "Xuyuan Xiong",
                    "hidden": false
                },
                {
                    "_id": "685210eb0164cd1316710441",
                    "name": "Tianhao Wang",
                    "hidden": false
                },
                {
                    "_id": "685210eb0164cd1316710442",
                    "name": "Zhuoran Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T20:58:05.000Z",
            "submittedOnDailyAt": "2025-06-18T00:09:08.222Z",
            "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
            "submittedOnDailyBy": {
                "_id": "683229900411a9d65cd410c0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png",
                "isPro": false,
                "fullname": "Siyu Chen",
                "user": "Siyuc",
                "type": "user"
            },
            "summary": "We study the challenge of achieving theoretically grounded feature recovery\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\nModels. Existing SAE training algorithms often lack rigorous mathematical\nguarantees and suffer from practical limitations such as hyperparameter\nsensitivity and instability. To address these issues, we first propose a novel\nstatistical framework for the feature recovery problem, which includes a new\nnotion of feature identifiability by modeling polysemantic features as sparse\nmixtures of underlying monosemantic concepts. Building on this framework, we\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\ntechnique that adaptively adjusts neural network bias parameters to ensure\nappropriate activation sparsity. We theoretically prove that this\nalgorithm correctly recovers all monosemantic features when input data is\nsampled from our proposed statistical model. Furthermore, we develop an\nimproved empirical variant, Group Bias Adaptation (GBA), and\ndemonstrate its superior performance against benchmark methods when\napplied to LLMs with up to 1.5 billion parameters. This work represents a\nfoundational step in demystifying SAE training by providing the first SAE\nalgorithm with theoretical recovery guarantees, thereby advancing the\ndevelopment of more transparent and trustworthy AI systems through enhanced\nmechanistic interpretability.",
            "upvotes": 5,
            "discussionId": "685210ec0164cd1316710443",
            "ai_summary": "A new statistical framework and training algorithm, Group Bias Adaptation, enhance Sparse Autoencoders for recovering monosemantic features in Large Language Models, offering theoretical guarantees and superior performance.",
            "ai_keywords": [
                "Sparse Autoencoders",
                "feature recovery",
                "statistical framework",
                "feature identifiability",
                "polysemantic features",
                "monosemantic concepts",
                "bias adaptation",
                "Group Bias Adaptation",
                "Large Language Models",
                "theoretical recovery guarantees",
                "mechnistic interpretability"
            ]
        },
        "publishedAt": "2025-06-16T16:58:05.000Z",
        "title": "Taming Polysemanticity in LLMs: Provable Feature Recovery via Sparse\n  Autoencoders",
        "summary": "We study the challenge of achieving theoretically grounded feature recovery\nusing Sparse Autoencoders (SAEs) for the interpretation of Large Language\nModels. Existing SAE training algorithms often lack rigorous mathematical\nguarantees and suffer from practical limitations such as hyperparameter\nsensitivity and instability. To address these issues, we first propose a novel\nstatistical framework for the feature recovery problem, which includes a new\nnotion of feature identifiability by modeling polysemantic features as sparse\nmixtures of underlying monosemantic concepts. Building on this framework, we\nintroduce a new SAE training algorithm based on ``bias adaptation'', a\ntechnique that adaptively adjusts neural network bias parameters to ensure\nappropriate activation sparsity. We theoretically prove that this\nalgorithm correctly recovers all monosemantic features when input data is\nsampled from our proposed statistical model. Furthermore, we develop an\nimproved empirical variant, Group Bias Adaptation (GBA), and\ndemonstrate its superior performance against benchmark methods when\napplied to LLMs with up to 1.5 billion parameters. This work represents a\nfoundational step in demystifying SAE training by providing the first SAE\nalgorithm with theoretical recovery guarantees, thereby advancing the\ndevelopment of more transparent and trustworthy AI systems through enhanced\nmechanistic interpretability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14002.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "683229900411a9d65cd410c0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/VqwvpUYF8CQAKPHMNfLyw.png",
            "fullname": "Siyu Chen",
            "name": "Siyuc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.10038",
            "authors": [
                {
                    "_id": "68523b4a0164cd131671055d",
                    "name": "Giannis Daras",
                    "hidden": false
                },
                {
                    "_id": "68523b4a0164cd131671055e",
                    "user": {
                        "_id": "67d204c05422de5644126f0b",
                        "avatarUrl": "/avatars/8a9ac73d93785f48e63184d612b9fff1.svg",
                        "isPro": false,
                        "fullname": "Adrian Rodriguez Munoz",
                        "user": "adrianrm",
                        "type": "user"
                    },
                    "name": "Adrian Rodriguez-Munoz",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-18T10:29:49.294Z",
                    "hidden": false
                },
                {
                    "_id": "68523b4a0164cd131671055f",
                    "name": "Adam Klivans",
                    "hidden": false
                },
                {
                    "_id": "68523b4a0164cd1316710560",
                    "name": "Antonio Torralba",
                    "hidden": false
                },
                {
                    "_id": "68523b4a0164cd1316710561",
                    "name": "Constantinos Daskalakis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T22:37:39.000Z",
            "submittedOnDailyAt": "2025-06-18T02:40:58.837Z",
            "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
            "submittedOnDailyBy": {
                "_id": "5f45f44b79c1ba4c353d1035",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f45f44b79c1ba4c353d1035/6piqagYr7RNCy6XwJGWCG.jpeg",
                "isPro": false,
                "fullname": "Giannis Daras",
                "user": "giannisdaras",
                "type": "user"
            },
            "summary": "We show how to use low-quality, synthetic, and out-of-distribution images to\nimprove the quality of a diffusion model. Typically, diffusion models are\ntrained on curated datasets that emerge from highly filtered data pools from\nthe Web and other sources. We show that there is immense value in the\nlower-quality images that are often discarded. We present Ambient Diffusion\nOmni, a simple, principled framework to train diffusion models that can extract\nsignal from all available images during training. Our framework exploits two\nproperties of natural images -- spectral power law decay and locality. We first\nvalidate our framework by successfully training diffusion models with images\nsynthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We\nthen use our framework to achieve state-of-the-art ImageNet FID, and we show\nsignificant improvements in both image quality and diversity for text-to-image\ngenerative modeling. The core insight is that noise dampens the initial skew\nbetween the desired high-quality distribution and the mixed distribution we\nactually observe. We provide rigorous theoretical justification for our\napproach by analyzing the trade-off between learning from biased data versus\nlimited unbiased data across diffusion times.",
            "upvotes": 5,
            "discussionId": "68523b4a0164cd1316710562",
            "projectPage": "https://giannisdaras.github.io/publication/ambient_omni",
            "githubRepo": "https://github.com/giannisdaras/ambient-omni",
            "ai_summary": "Ambient Diffusion Omni framework leverages low-quality images to enhance diffusion models by utilizing properties of natural images and shows improvements in ImageNet FID and text-to-image quality.",
            "ai_keywords": [
                "diffusion models",
                "synthetic images",
                "out-of-distribution images",
                "Ambient Diffusion Omni",
                "spectral power law decay",
                "locality",
                "Gaussian blur",
                "JPEG compression",
                "motion blur",
                "ImageNet FID",
                "text-to-image generative modeling",
                "noise dampening",
                "biased data",
                "limited unbiased data"
            ]
        },
        "publishedAt": "2025-06-10T18:37:39.000Z",
        "title": "Ambient Diffusion Omni: Training Good Models with Bad Data",
        "summary": "We show how to use low-quality, synthetic, and out-of-distribution images to\nimprove the quality of a diffusion model. Typically, diffusion models are\ntrained on curated datasets that emerge from highly filtered data pools from\nthe Web and other sources. We show that there is immense value in the\nlower-quality images that are often discarded. We present Ambient Diffusion\nOmni, a simple, principled framework to train diffusion models that can extract\nsignal from all available images during training. Our framework exploits two\nproperties of natural images -- spectral power law decay and locality. We first\nvalidate our framework by successfully training diffusion models with images\nsynthetically corrupted by Gaussian blur, JPEG compression, and motion blur. We\nthen use our framework to achieve state-of-the-art ImageNet FID, and we show\nsignificant improvements in both image quality and diversity for text-to-image\ngenerative modeling. The core insight is that noise dampens the initial skew\nbetween the desired high-quality distribution and the mixed distribution we\nactually observe. We provide rigorous theoretical justification for our\napproach by analyzing the trade-off between learning from biased data versus\nlimited unbiased data across diffusion times.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10038.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f45f44b79c1ba4c353d1035",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f45f44b79c1ba4c353d1035/6piqagYr7RNCy6XwJGWCG.jpeg",
            "fullname": "Giannis Daras",
            "name": "giannisdaras",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.05336",
            "authors": [
                {
                    "_id": "68425a7ab63271ff41652734",
                    "name": "Ghazi Shazan Ahmad",
                    "hidden": false
                },
                {
                    "_id": "68425a7ab63271ff41652735",
                    "user": {
                        "_id": "656864e12d73834278a8dea7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                        "isPro": true,
                        "fullname": "Ahmed Heakl",
                        "user": "ahmedheakl",
                        "type": "user"
                    },
                    "name": "Ahmed Heakl",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-07T05:48:53.401Z",
                    "hidden": false
                },
                {
                    "_id": "68425a7ab63271ff41652736",
                    "name": "Hanan Gani",
                    "hidden": false
                },
                {
                    "_id": "68425a7ab63271ff41652737",
                    "name": "Abdelrahman Shaker",
                    "hidden": false
                },
                {
                    "_id": "68425a7ab63271ff41652738",
                    "name": "Zhiqiang Shen",
                    "hidden": false
                },
                {
                    "_id": "68425a7ab63271ff41652739",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "68425a7ab63271ff4165273a",
                    "name": "Fahad Shahbaz Khan",
                    "hidden": false
                },
                {
                    "_id": "68425a7ab63271ff4165273b",
                    "name": "Salman Khan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/7r2x8C0UYFCrqk95QpBGr.mp4"
            ],
            "publishedAt": "2025-06-05T17:59:29.000Z",
            "submittedOnDailyAt": "2025-06-18T01:30:20.599Z",
            "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
            "submittedOnDailyBy": {
                "_id": "656864e12d73834278a8dea7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
                "isPro": true,
                "fullname": "Ahmed Heakl",
                "user": "ahmedheakl",
                "type": "user"
            },
            "summary": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo.",
            "upvotes": 5,
            "discussionId": "68425a80b63271ff416528f2",
            "projectPage": "https://mbzuai-oryx.github.io/VideoMolmo/",
            "githubRepo": "https://github.com/mbzuai-oryx/VideoMolmo",
            "ai_summary": "VideoMolmo, a multimodal model incorporating a temporal attention mechanism and SAM2 for mask fusion, enhances spatio-temporal pointing accuracy and reasoning capabilities in diverse real-world scenarios.",
            "ai_keywords": [
                "Molmo",
                "attention mechanism",
                "temporal mask fusion",
                "SAM2",
                "bidirectional point propagation",
                "VideoMolmo",
                "LLM",
                "sequential mask-fusion module",
                "VPoS-Bench",
                "Referring Video Object Segmentation",
                "Reasoning VOS"
            ]
        },
        "publishedAt": "2025-06-05T13:59:29.000Z",
        "title": "VideoMolmo: Spatio-Temporal Grounding Meets Pointing",
        "summary": "Spatio-temporal localization is vital for precise interactions across diverse\ndomains, from biological research to autonomous navigation and interactive\ninterfaces. Current video-based approaches, while proficient in tracking, lack\nthe sophisticated reasoning capabilities of large language models, limiting\ntheir contextual understanding and generalization. We introduce VideoMolmo, a\nlarge multimodal model tailored for fine-grained spatio-temporal pointing\nconditioned on textual descriptions. Building upon the Molmo architecture,\nVideoMolmo incorporates a temporal module utilizing an attention mechanism to\ncondition each frame on preceding frames, ensuring temporal consistency.\nAdditionally, our novel temporal mask fusion pipeline employs SAM2 for\nbidirectional point propagation, significantly enhancing coherence across video\nsequences. This two-step decomposition, i.e., first using the LLM to generate\nprecise pointing coordinates, then relying on a sequential mask-fusion module\nto produce coherent segmentation, not only simplifies the task for the language\nmodel but also enhances interpretability. Due to the lack of suitable datasets,\nwe curate a comprehensive dataset comprising 72k video-caption pairs annotated\nwith 100k object points. To evaluate the generalization of VideoMolmo, we\nintroduce VPoS-Bench, a challenging out-of-distribution benchmark spanning five\nreal-world scenarios: Cell Tracking, Egocentric Vision, Autonomous Driving,\nVideo-GUI Interaction, and Robotics. We also evaluate our model on Referring\nVideo Object Segmentation (Refer-VOS) and Reasoning VOS tasks. In comparison to\nexisting models, VideoMolmo substantially improves spatio-temporal pointing\naccuracy and reasoning capability. Our code and models are publicly available\nat https://github.com/mbzuai-oryx/VideoMolmo.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/656864e12d73834278a8dea7/7r2x8C0UYFCrqk95QpBGr.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05336.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "656864e12d73834278a8dea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656864e12d73834278a8dea7/sfAWS2eyPtFHb_2GZIypp.jpeg",
            "fullname": "Ahmed Heakl",
            "name": "ahmedheakl",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 41
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.09033",
            "authors": [
                {
                    "_id": "6852ac4a7eb90a35d35de5f1",
                    "name": "Haozhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6852ac4a7eb90a35d35de5f2",
                    "name": "Tao Feng",
                    "hidden": false
                },
                {
                    "_id": "6852ac4a7eb90a35d35de5f3",
                    "name": "Jiaxuan You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-10T17:56:45.000Z",
            "submittedOnDailyAt": "2025-06-18T10:41:49.919Z",
            "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64f58f3468047192d6c7f335",
                "avatarUrl": "/avatars/88be16ee80da7d2eaa0feae878375001.svg",
                "isPro": false,
                "fullname": "XaiverZ",
                "user": "XaiverZ",
                "type": "user"
            },
            "summary": "The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (i.e., assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\nRouter-R1, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To guide learning, we employ a lightweight rule-based\nreward comprising format rewards, final outcome rewards, and a novel cost\nreward for performance and cost trade-off optimization, opening a pathway\ntoward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions\nonly on simple model descriptors such as pricing, latency, and example\nperformance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms over several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.Code is available at\nhttps://github.com/ulab-uiuc/Router-R1.",
            "upvotes": 3,
            "discussionId": "6852ac4b7eb90a35d35de5f4",
            "projectPage": "https://ulab-uiuc.github.io/Router-R1/",
            "githubRepo": "https://github.com/ulab-uiuc/Router-R1",
            "ai_summary": "Router-R1, a reinforcement learning-based framework, improves multi-LLM routing by interleaving think and route actions, optimizing performance-cost trade-offs, and generalizing to unseen models.",
            "ai_keywords": [
                "LLMs",
                "reinforcement learning",
                "multi-LLM routing",
                "aggregation",
                "sequential decision process",
                "think actions",
                "route actions",
                "context integration",
                "reward shaping",
                "format rewards",
                "final outcome rewards",
                "cost reward",
                "performance optimization",
                "cost management",
                "generalization"
            ]
        },
        "publishedAt": "2025-06-10T13:56:45.000Z",
        "title": "Router-R1: Teaching LLMs Multi-Round Routing and Aggregation via\n  Reinforcement Learning",
        "summary": "The rapid emergence of diverse large language models (LLMs) has spurred the\ndevelopment of LLM routers that assign user queries to the most suitable model.\nHowever, existing LLM routers typically perform a single-round, one-to-one\nmapping (i.e., assigning each query to a single model in isolation),\nwhich limits their capability to tackle complex tasks that demand the\ncomplementary strengths of multiple LLMs. In this paper, we present\nRouter-R1, a reinforcement learning (RL)-based framework that\nformulates multi-LLM routing and aggregation as a sequential decision process.\nRouter-R1 instantiates the router itself as a capable LLM, leveraging its\nreasoning ability to interleave \"think\" actions (internal deliberation) with\n\"route\" actions (dynamic model invocation), and integrates each response into\nits evolving context. To guide learning, we employ a lightweight rule-based\nreward comprising format rewards, final outcome rewards, and a novel cost\nreward for performance and cost trade-off optimization, opening a pathway\ntoward optimizing performance-cost tradeoffs via RL. Router-R1 also conditions\nonly on simple model descriptors such as pricing, latency, and example\nperformance, enabling strong generalization to unseen model selection.\nExperiments on seven general and multi-hop QA benchmarks show that Router-R1\noutperforms over several strong baselines, achieving superior performance while\nmaintaining robust generalization and cost management.Code is available at\nhttps://github.com/ulab-uiuc/Router-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09033.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f58f3468047192d6c7f335",
            "avatarUrl": "/avatars/88be16ee80da7d2eaa0feae878375001.svg",
            "fullname": "XaiverZ",
            "name": "XaiverZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.14761",
            "authors": [
                {
                    "_id": "685264ca0164cd13167105f2",
                    "name": "Mathurin Videau",
                    "hidden": false
                },
                {
                    "_id": "685264ca0164cd13167105f3",
                    "name": "Badr Youbi Idrissi",
                    "hidden": false
                },
                {
                    "_id": "685264ca0164cd13167105f4",
                    "name": "Alessandro Leite",
                    "hidden": false
                },
                {
                    "_id": "685264ca0164cd13167105f5",
                    "name": "Marc Schoenauer",
                    "hidden": false
                },
                {
                    "_id": "685264ca0164cd13167105f6",
                    "name": "Olivier Teytaud",
                    "hidden": false
                },
                {
                    "_id": "685264ca0164cd13167105f7",
                    "name": "David Lopez-Paz",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T17:55:11.000Z",
            "submittedOnDailyAt": "2025-06-18T15:32:27.211Z",
            "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
            "submittedOnDailyBy": {
                "_id": "632df0e4860318fab98f8804",
                "avatarUrl": "/avatars/94b5810ec4a167d9e2b523c0626a23b4.svg",
                "isPro": false,
                "fullname": "Badr Youbi Idrissi",
                "user": "cetosignis",
                "type": "user"
            },
            "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a\nlanguage model operates on data and how far in the future it predicts. Byte\nPair Encoding (BPE) and similar schemes split text once, build a static\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\nby introducing an autoregressive U-Net that learns to embed its own tokens as\nit trains. The network reads raw bytes, pools them into words, then pairs of\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\ndeeper stages, the model must predict further into the future -- anticipating\nthe next few words rather than the next byte -- so deeper stages focus on\nbroader semantic patterns while earlier stages handle fine details. When\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\ntokenization now lives inside the model, the same system can handle\ncharacter-level tasks and carry knowledge across low-resource languages.",
            "upvotes": 2,
            "discussionId": "685264ca0164cd13167105f8",
            "ai_summary": "An autoregressive U-Net learns to embed its own tokens during training, enabling a multi-scale view of text sequences and improved handling of character-level tasks and low-resource languages.",
            "ai_keywords": [
                "autoregressive U-Net",
                "Byte Pair Encoding (BPE)",
                "multi-scale view",
                "semantic patterns",
                "character-level tasks",
                "low-resource languages"
            ]
        },
        "publishedAt": "2025-06-17T13:55:11.000Z",
        "title": "From Bytes to Ideas: Language Modeling with Autoregressive U-Nets",
        "summary": "Tokenization imposes a fixed granularity on the input text, freezing how a\nlanguage model operates on data and how far in the future it predicts. Byte\nPair Encoding (BPE) and similar schemes split text once, build a static\nvocabulary, and leave the model stuck with that choice. We relax this rigidity\nby introducing an autoregressive U-Net that learns to embed its own tokens as\nit trains. The network reads raw bytes, pools them into words, then pairs of\nwords, then up to 4 words, giving it a multi-scale view of the sequence. At\ndeeper stages, the model must predict further into the future -- anticipating\nthe next few words rather than the next byte -- so deeper stages focus on\nbroader semantic patterns while earlier stages handle fine details. When\ncarefully tuning and controlling pretraining compute, shallow hierarchies tie\nstrong BPE baselines, and deeper hierarchies have a promising trend. Because\ntokenization now lives inside the model, the same system can handle\ncharacter-level tasks and carry knowledge across low-resource languages.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14761.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "632df0e4860318fab98f8804",
            "avatarUrl": "/avatars/94b5810ec4a167d9e2b523c0626a23b4.svg",
            "fullname": "Badr Youbi Idrissi",
            "name": "cetosignis",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.14731",
            "authors": [
                {
                    "_id": "685236ac0164cd1316710512",
                    "name": "Ring Team",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710513",
                    "name": "Bin Hu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710514",
                    "name": "Cai Chen",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710515",
                    "name": "Deng Zhao",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710516",
                    "name": "Ding Liu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710517",
                    "name": "Dingnan Jin",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710518",
                    "name": "Feng Zhu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710519",
                    "name": "Hao Dai",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671051a",
                    "name": "Hongzhi Luan",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671051b",
                    "name": "Jia Guo",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671051c",
                    "name": "Jiaming Liu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671051d",
                    "name": "Jiewei Wu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671051e",
                    "name": "Jun Mei",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671051f",
                    "name": "Jun Zhou",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710520",
                    "name": "Junbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710521",
                    "name": "Junwu Xiong",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710522",
                    "name": "Kaihong Zhang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710523",
                    "name": "Kuan Xu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710524",
                    "name": "Lei Liang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710525",
                    "name": "Liang Jiang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710526",
                    "name": "Liangcheng Fu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710527",
                    "name": "Longfei Zheng",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710528",
                    "name": "Qiang Gao",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710529",
                    "name": "Qing Cui",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671052a",
                    "name": "Quan Wan",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671052b",
                    "name": "Shaomian Zheng",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671052c",
                    "name": "Shuaicheng Li",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671052d",
                    "name": "Tongkai Yang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671052e",
                    "name": "Wang Ren",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671052f",
                    "name": "Xiaodong Yan",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710530",
                    "name": "Xiaopei Wan",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710531",
                    "name": "Xiaoyun Feng",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710532",
                    "name": "Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710533",
                    "name": "Xinxing Yang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710534",
                    "name": "Xinyu Kong",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710535",
                    "name": "Xuemin Yang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710536",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710537",
                    "name": "Yingting Wu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710538",
                    "name": "Yongkang Liu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd1316710539",
                    "name": "Zhankai Xu",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671053a",
                    "name": "Zhenduo Zhang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671053b",
                    "name": "Zhenglei Zhou",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671053c",
                    "name": "Zhenyu Huang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671053d",
                    "name": "Zhiqiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671053e",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "685236ac0164cd131671053f",
                    "name": "Zujie Wen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T17:12:34.000Z",
            "submittedOnDailyAt": "2025-06-18T03:14:38.830Z",
            "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
            "upvotes": 2,
            "discussionId": "685236ad0164cd1316710540",
            "ai_summary": "Ring-lite uses a MoE architecture and reinforcement learning to efficiently match SOTA reasoning models while activating fewer parameters and addressing challenges specific to MoE training.",
            "ai_keywords": [
                "Mixture-of-Experts (MoE)",
                "reinforcement learning (RL)",
                "Ling-lite",
                "AIME",
                "LiveCodeBench",
                "GPQA-Diamond",
                "Constrained Contextual Computation Policy Optimization(C3PO)",
                "entropy loss",
                "two-stage training paradigm"
            ]
        },
        "publishedAt": "2025-06-17T13:12:34.000Z",
        "title": "Ring-lite: Scalable Reasoning via C3PO-Stabilized Reinforcement Learning\n  for LLMs",
        "summary": "We present Ring-lite, a Mixture-of-Experts (MoE)-based large language model\noptimized via reinforcement learning (RL) to achieve efficient and robust\nreasoning capabilities. Built upon the publicly available Ling-lite model, a\n16.8 billion parameter model with 2.75 billion activated parameters, our\napproach matches the performance of state-of-the-art (SOTA) small-scale\nreasoning models on challenging benchmarks (e.g., AIME, LiveCodeBench,\nGPQA-Diamond) while activating only one-third of the parameters required by\ncomparable models. To accomplish this, we introduce a joint training pipeline\nintegrating distillation with RL, revealing undocumented challenges in MoE RL\ntraining. First, we identify optimization instability during RL training, and\nwe propose Constrained Contextual Computation Policy Optimization(C3PO), a\nnovel approach that enhances training stability and improves computational\nthroughput via algorithm-system co-design methodology. Second, we empirically\ndemonstrate that selecting distillation checkpoints based on entropy loss for\nRL training, rather than validation metrics, yields superior\nperformance-efficiency trade-offs in subsequent RL training. Finally, we\ndevelop a two-stage training paradigm to harmonize multi-domain data\nintegration, addressing domain conflicts that arise in training with mixed\ndataset. We will release the model, dataset, and code.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14731.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7135
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.14702",
            "authors": [
                {
                    "_id": "6852331e0164cd13167104fb",
                    "name": "Daniel D'souza",
                    "hidden": false
                },
                {
                    "_id": "6852331e0164cd13167104fc",
                    "name": "Julia Kreutzer",
                    "hidden": false
                },
                {
                    "_id": "6852331e0164cd13167104fd",
                    "name": "Adrien Morisot",
                    "hidden": false
                },
                {
                    "_id": "6852331e0164cd13167104fe",
                    "name": "Ahmet stn",
                    "hidden": false
                },
                {
                    "_id": "6852331e0164cd13167104ff",
                    "name": "Sara Hooker",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6658011eaba105a066e37e1b/RBQEHm8CJA9fKsg4bupgs.png"
            ],
            "publishedAt": "2025-06-17T16:40:42.000Z",
            "submittedOnDailyAt": "2025-06-18T02:55:26.808Z",
            "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers",
            "submittedOnDailyBy": {
                "_id": "6658011eaba105a066e37e1b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg",
                "isPro": false,
                "fullname": "Daniel D'souza",
                "user": "dsouzadaniel",
                "type": "user"
            },
            "summary": "One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations.",
            "upvotes": 2,
            "discussionId": "6852331e0164cd1316710500",
            "ai_summary": "A principled approach to fine-tuning models for better performance and controllability on underrepresented use cases is developed through automatic inference of generation attributes.",
            "ai_keywords": [
                "prompt engineering",
                "few-shot examples",
                "controllability",
                "performance",
                "long-tail",
                "training protocols",
                "inference techniques",
                "taxonomy",
                "data characteristics",
                "task provenance",
                "fine-tuning",
                "generation attributes",
                "markers",
                "underrepresented domains",
                "CodeRepair",
                "length instruction following"
            ]
        },
        "publishedAt": "2025-06-17T12:40:42.000Z",
        "title": "Treasure Hunt: Real-time Targeting of the Long Tail using Training-Time\n  Markers",
        "summary": "One of the most profound challenges of modern machine learning is performing\nwell on the long-tail of rare and underrepresented features. Large\ngeneral-purpose models are trained for many tasks, but work best on\nhigh-frequency use cases. After training, it is hard to adapt a model to\nperform well on specific use cases underrepresented in the training corpus.\nRelying on prompt engineering or few-shot examples to maximize the output\nquality on a particular test case can be frustrating, as models can be highly\nsensitive to small changes, react in unpredicted ways or rely on a fixed system\nprompt for maintaining performance. In this work, we ask: \"Can we optimize our\ntraining protocols to both improve controllability and performance on\nunderrepresented use cases at inference time?\" We revisit the divide between\ntraining and inference techniques to improve long-tail performance while\nproviding users with a set of control levers the model is trained to be\nresponsive to. We create a detailed taxonomy of data characteristics and task\nprovenance to explicitly control generation attributes and implicitly condition\ngenerations at inference time. We fine-tune a base model to infer these markers\nautomatically, which makes them optional at inference time. This principled and\nflexible approach yields pronounced improvements in performance, especially on\nexamples from the long tail of the training distribution. While we observe an\naverage lift of 5.7% win rates in open-ended generation quality with our\nmarkers, we see over 9.1% gains in underrepresented domains. We also observe\nrelative lifts of up to 14.1% on underrepresented tasks like CodeRepair and\nabsolute improvements of 35.3% on length instruction following evaluations.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6658011eaba105a066e37e1b/RBQEHm8CJA9fKsg4bupgs.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14702.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6658011eaba105a066e37e1b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6658011eaba105a066e37e1b/VPwyTv1bnVMQbVMoMQzcf.jpeg",
            "fullname": "Daniel D'souza",
            "name": "dsouzadaniel",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.13599",
            "authors": [
                {
                    "_id": "685220810164cd131671048e",
                    "name": "Yuwei Du",
                    "hidden": false
                },
                {
                    "_id": "685220810164cd131671048f",
                    "user": {
                        "_id": "6465d3bd63e7e09dd02e95c3",
                        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                        "isPro": false,
                        "fullname": "Jie Feng",
                        "user": "JJ-TMT",
                        "type": "user"
                    },
                    "name": "Jie Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:59:19.122Z",
                    "hidden": false
                },
                {
                    "_id": "685220810164cd1316710490",
                    "name": "Jian Yuan",
                    "hidden": false
                },
                {
                    "_id": "685220810164cd1316710491",
                    "name": "Yong Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/t_3QiPaJ54vNcgeYZKMgy.jpeg"
            ],
            "publishedAt": "2025-06-16T15:24:07.000Z",
            "submittedOnDailyAt": "2025-06-18T01:27:33.225Z",
            "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
            "submittedOnDailyBy": {
                "_id": "6465d3bd63e7e09dd02e95c3",
                "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                "isPro": false,
                "fullname": "Jie Feng",
                "user": "JJ-TMT",
                "type": "user"
            },
            "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose CityGPT-Powered\nAgentic framework for Mobility Simulation\n(CAMS), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. CAMS\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that CAMS achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, CAMS generates more realistic and\nplausible trajectories. In general, CAMS establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
            "upvotes": 2,
            "discussionId": "685220820164cd1316710492",
            "ai_summary": "CAMS integrates an agentic framework with urban-knowledgeable large language models to simulate human mobility more realistically by modeling individual and collective patterns.",
            "ai_keywords": [
                "large language models",
                "CityGPT",
                "agentic framework",
                "human mobility simulation",
                "urban spaces",
                "individual mobility patterns",
                "collective mobility distributions",
                "MobExtractor",
                "GeoGenerator",
                "TrajEnhancer",
                "DPO",
                "trajectory preference alignment",
                "real-world datasets"
            ]
        },
        "publishedAt": "2025-06-16T11:24:07.000Z",
        "title": "CAMS: A CityGPT-Powered Agentic Framework for Urban Human Mobility\n  Simulation",
        "summary": "Human mobility simulation plays a crucial role in various real-world\napplications. Recently, to address the limitations of traditional data-driven\napproaches, researchers have explored leveraging the commonsense knowledge and\nreasoning capabilities of large language models (LLMs) to accelerate human\nmobility simulation. However, these methods suffer from several critical\nshortcomings, including inadequate modeling of urban spaces and poor\nintegration with both individual mobility patterns and collective mobility\ndistributions. To address these challenges, we propose CityGPT-Powered\nAgentic framework for Mobility Simulation\n(CAMS), an agentic framework that leverages the language based urban\nfoundation model to simulate human mobility in urban space. CAMS\ncomprises three core modules, including MobExtractor to extract template\nmobility patterns and synthesize new ones based on user profiles, GeoGenerator\nto generate anchor points considering collective knowledge and generate\ncandidate urban geospatial knowledge using an enhanced version of CityGPT,\nTrajEnhancer to retrieve spatial knowledge based on mobility patterns and\ngenerate trajectories with real trajectory preference alignment via DPO.\nExperiments on real-world datasets show that CAMS achieves superior\nperformance without relying on externally provided geospatial information.\nMoreover, by holistically modeling both individual mobility patterns and\ncollective mobility constraints, CAMS generates more realistic and\nplausible trajectories. In general, CAMS establishes a new paradigm\nthat integrates the agentic framework with urban-knowledgeable LLMs for human\nmobility simulation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/t_3QiPaJ54vNcgeYZKMgy.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13599.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "fullname": "Jie Feng",
            "name": "JJ-TMT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.05426",
            "authors": [
                {
                    "_id": "68527e8ce329b3b5e93f2997",
                    "user": {
                        "_id": "6756970ce110734a48701a08",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6756970ce110734a48701a08/yn24gxS2LhZHRE3NYRkjr.jpeg",
                        "isPro": false,
                        "fullname": "Wenhao Wu",
                        "user": "Wenhao0",
                        "type": "user"
                    },
                    "name": "Wenhao Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:58:48.394Z",
                    "hidden": false
                },
                {
                    "_id": "68527e8ce329b3b5e93f2998",
                    "name": "Fuhong Liu",
                    "hidden": false
                },
                {
                    "_id": "68527e8ce329b3b5e93f2999",
                    "name": "Haoru Li",
                    "hidden": false
                },
                {
                    "_id": "68527e8ce329b3b5e93f299a",
                    "name": "Zican Hu",
                    "hidden": false
                },
                {
                    "_id": "68527e8ce329b3b5e93f299b",
                    "name": "Daoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "68527e8ce329b3b5e93f299c",
                    "name": "Chunlin Chen",
                    "hidden": false
                },
                {
                    "_id": "68527e8ce329b3b5e93f299d",
                    "name": "Zhi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-05T06:29:14.000Z",
            "submittedOnDailyAt": "2025-06-18T09:34:58.708Z",
            "title": "Mixture-of-Experts Meets In-Context Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6756970ce110734a48701a08",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6756970ce110734a48701a08/yn24gxS2LhZHRE3NYRkjr.jpeg",
                "isPro": false,
                "fullname": "Wenhao Wu",
                "user": "Wenhao0",
                "type": "user"
            },
            "summary": "In-context reinforcement learning (ICRL) has emerged as a promising paradigm\nfor adapting RL agents to downstream tasks through prompt conditioning.\nHowever, two notable challenges remain in fully harnessing in-context learning\nwithin RL domains: the intrinsic multi-modality of the state-action-reward data\nand the diverse, heterogeneous nature of decision tasks. To tackle these\nchallenges, we propose T2MIR (Token- and Task-wise\nMoE for In-context RL), an innovative framework that\nintroduces architectural advances of mixture-of-experts (MoE) into\ntransformer-based decision models. T2MIR substitutes the feedforward layer with\ntwo parallel layers: a token-wise MoE that captures distinct semantics of input\ntokens across multiple modalities, and a task-wise MoE that routes diverse\ntasks to specialized experts for managing a broad task distribution with\nalleviated gradient conflicts. To enhance task-wise routing, we introduce a\ncontrastive learning method that maximizes the mutual information between the\ntask and its router representation, enabling more precise capture of\ntask-relevant information. The outputs of two MoE components are concatenated\nand fed into the next layer. Comprehensive experiments show that T2MIR\nsignificantly facilitates in-context learning capacity and outperforms various\ntypes of baselines. We bring the potential and promise of MoE to ICRL, offering\na simple and scalable architectural enhancement to advance ICRL one step closer\ntoward achievements in language and vision communities. Our code is available\nat https://github.com/NJU-RL/T2MIR.",
            "upvotes": 2,
            "discussionId": "68527e8de329b3b5e93f299e",
            "ai_summary": "T2MIR, a framework using token-wise and task-wise MoE in transformer-based decision models, enhances in-context reinforcement learning by addressing multi-modality and task diversity.",
            "ai_keywords": [
                "Mixture-of-experts (MoE)",
                "transformer-based decision models",
                "token-wise MoE",
                "task-wise MoE",
                "contrastive learning",
                "mutual information"
            ]
        },
        "publishedAt": "2025-06-05T02:29:14.000Z",
        "title": "Mixture-of-Experts Meets In-Context Reinforcement Learning",
        "summary": "In-context reinforcement learning (ICRL) has emerged as a promising paradigm\nfor adapting RL agents to downstream tasks through prompt conditioning.\nHowever, two notable challenges remain in fully harnessing in-context learning\nwithin RL domains: the intrinsic multi-modality of the state-action-reward data\nand the diverse, heterogeneous nature of decision tasks. To tackle these\nchallenges, we propose T2MIR (Token- and Task-wise\nMoE for In-context RL), an innovative framework that\nintroduces architectural advances of mixture-of-experts (MoE) into\ntransformer-based decision models. T2MIR substitutes the feedforward layer with\ntwo parallel layers: a token-wise MoE that captures distinct semantics of input\ntokens across multiple modalities, and a task-wise MoE that routes diverse\ntasks to specialized experts for managing a broad task distribution with\nalleviated gradient conflicts. To enhance task-wise routing, we introduce a\ncontrastive learning method that maximizes the mutual information between the\ntask and its router representation, enabling more precise capture of\ntask-relevant information. The outputs of two MoE components are concatenated\nand fed into the next layer. Comprehensive experiments show that T2MIR\nsignificantly facilitates in-context learning capacity and outperforms various\ntypes of baselines. We bring the potential and promise of MoE to ICRL, offering\na simple and scalable architectural enhancement to advance ICRL one step closer\ntoward achievements in language and vision communities. Our code is available\nat https://github.com/NJU-RL/T2MIR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.05426.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6756970ce110734a48701a08",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6756970ce110734a48701a08/yn24gxS2LhZHRE3NYRkjr.jpeg",
            "fullname": "Wenhao Wu",
            "name": "Wenhao0",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14205",
            "authors": [
                {
                    "_id": "6853647b99bf39f9665c7941",
                    "name": "Jingxu Xie",
                    "hidden": false
                },
                {
                    "_id": "6853647b99bf39f9665c7942",
                    "name": "Dylan Xu",
                    "hidden": false
                },
                {
                    "_id": "6853647b99bf39f9665c7943",
                    "name": "Xuandong Zhao",
                    "hidden": false
                },
                {
                    "_id": "6853647b99bf39f9665c7944",
                    "name": "Dawn Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T05:46:52.000Z",
            "submittedOnDailyAt": "2025-06-18T23:47:08.172Z",
            "title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents",
            "submittedOnDailyBy": {
                "_id": "6275a465597c70eb8949fce5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
                "isPro": false,
                "fullname": "Xuandong Zhao",
                "user": "Xuandong",
                "type": "user"
            },
            "summary": "We introduce AgentSynth, a scalable and cost-efficient pipeline for\nautomatically synthesizing high-quality tasks and trajectory datasets for\ngeneralist computer-use agents. Leveraging information asymmetry, AgentSynth\nconstructs subtasks that are simple during generation but significantly more\nchallenging when composed into long-horizon tasks, enabling the creation of\nover 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based\ntask proposer guided by a persona, followed by an execution agent that\ncompletes the task and logs the trajectory. This process is repeated\niteratively to form a sequence of subtasks, which are then summarized by a\nseparate agent into a composite task of controllable difficulty. A key strength\nof AgentSynth is its ability to precisely modulate task complexity by varying\nthe number of subtasks. Empirical evaluations show that state-of-the-art LLM\nagents suffer a steep performance drop, from 18% success at difficulty level 1\nto just 4% at level 6, highlighting the benchmark's difficulty and\ndiscriminative power. Moreover, our pipeline achieves a low average cost of\n\\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our\ncode and data are publicly available at\nhttps://github.com/sunblaze-ucb/AgentSynth",
            "upvotes": 1,
            "discussionId": "6853647c99bf39f9665c7945",
            "ai_summary": "AgentSynth synthesizes high-quality, diverse tasks and trajectory datasets for generalist computer-use agents using LLMs and an iterative subtask construction approach, enabling precise control over task complexity and offering significant cost savings compared to human annotations.",
            "ai_keywords": [
                "LLM-based task proposer",
                "execution agent",
                "composite task",
                "subtasks",
                "state-of-the-art LLM agents",
                "performance drop",
                "human annotations"
            ]
        },
        "publishedAt": "2025-06-17T01:46:52.000Z",
        "title": "AgentSynth: Scalable Task Generation for Generalist Computer-Use Agents",
        "summary": "We introduce AgentSynth, a scalable and cost-efficient pipeline for\nautomatically synthesizing high-quality tasks and trajectory datasets for\ngeneralist computer-use agents. Leveraging information asymmetry, AgentSynth\nconstructs subtasks that are simple during generation but significantly more\nchallenging when composed into long-horizon tasks, enabling the creation of\nover 6,000 diverse and realistic tasks. Our pipeline begins with an LLM-based\ntask proposer guided by a persona, followed by an execution agent that\ncompletes the task and logs the trajectory. This process is repeated\niteratively to form a sequence of subtasks, which are then summarized by a\nseparate agent into a composite task of controllable difficulty. A key strength\nof AgentSynth is its ability to precisely modulate task complexity by varying\nthe number of subtasks. Empirical evaluations show that state-of-the-art LLM\nagents suffer a steep performance drop, from 18% success at difficulty level 1\nto just 4% at level 6, highlighting the benchmark's difficulty and\ndiscriminative power. Moreover, our pipeline achieves a low average cost of\n\\$0.60 per trajectory, orders of magnitude cheaper than human annotations. Our\ncode and data are publicly available at\nhttps://github.com/sunblaze-ucb/AgentSynth",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14205.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6275a465597c70eb8949fce5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
            "fullname": "Xuandong Zhao",
            "name": "Xuandong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.13901",
            "authors": [
                {
                    "_id": "685252860164cd13167105c7",
                    "name": "Abhilekh Borah",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105c8",
                    "name": "Chhavi Sharma",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105c9",
                    "name": "Danush Khanna",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105ca",
                    "name": "Utkarsh Bhatt",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105cb",
                    "name": "Gurpreet Singh",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105cc",
                    "name": "Hasnat Md Abdullah",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105cd",
                    "name": "Raghav Kaushik Ravi",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105ce",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105cf",
                    "name": "Jyoti Patel",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105d0",
                    "name": "Shubham Singh",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105d1",
                    "name": "Vasu Sharma",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105d2",
                    "name": "Arpita Vats",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105d3",
                    "name": "Rahul Raja",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105d4",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:58:55.038Z",
                    "hidden": false
                },
                {
                    "_id": "685252860164cd13167105d5",
                    "name": "Amitava Das",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T18:22:28.000Z",
            "submittedOnDailyAt": "2025-06-18T04:17:43.427Z",
            "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations",
            "submittedOnDailyBy": {
                "_id": "63a4754927f1f64ed7238dac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                "isPro": false,
                "fullname": "Aman Chadha",
                "user": "amanchadha",
                "type": "user"
            },
            "summary": "Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area.",
            "upvotes": 1,
            "discussionId": "685252870164cd13167105d6",
            "ai_summary": "A new evaluation metric called Alignment Quality Index (AQI) assesses the alignment of large language models by analyzing latent space activations, capturing clustering quality to detect misalignments and fake alignment, and complementing existing behavioral proxies.",
            "ai_keywords": [
                "Alignment Quality Index (AQI)",
                "latent space",
                "Davies-Bouldin Score (DBS)",
                "Dunn Index (DI)",
                "Xie-Beni Index (XBI)",
                "Calinski-Harabasz Index (CHI)",
                "LITMUS dataset",
                "DPO",
                "GRPO",
                "RLHF",
                "alignment faking",
                "external judges"
            ]
        },
        "publishedAt": "2025-06-16T14:22:28.000Z",
        "title": "Alignment Quality Index (AQI) : Beyond Refusals: AQI as an Intrinsic\n  Alignment Diagnostic via Latent Geometry, Cluster Divergence, and Layer wise\n  Pooled Representations",
        "summary": "Alignment is no longer a luxury, it is a necessity. As large language models\n(LLMs) enter high-stakes domains like education, healthcare, governance, and\nlaw, their behavior must reliably reflect human-aligned values and safety\nconstraints. Yet current evaluations rely heavily on behavioral proxies such as\nrefusal rates, G-Eval scores, and toxicity classifiers, all of which have\ncritical blind spots. Aligned models are often vulnerable to jailbreaking,\nstochasticity of generation, and alignment faking.\n  To address this issue, we introduce the Alignment Quality Index (AQI). This\nnovel geometric and prompt-invariant metric empirically assesses LLM alignment\nby analyzing the separation of safe and unsafe activations in latent space. By\ncombining measures such as the Davies-Bouldin Score (DBS), Dunn Index (DI),\nXie-Beni Index (XBI), and Calinski-Harabasz Index (CHI) across various\nformulations, AQI captures clustering quality to detect hidden misalignments\nand jailbreak risks, even when outputs appear compliant. AQI also serves as an\nearly warning signal for alignment faking, offering a robust, decoding\ninvariant tool for behavior agnostic safety auditing.\n  Additionally, we propose the LITMUS dataset to facilitate robust evaluation\nunder these challenging conditions. Empirical tests on LITMUS across different\nmodels trained under DPO, GRPO, and RLHF conditions demonstrate AQI's\ncorrelation with external judges and ability to reveal vulnerabilities missed\nby refusal metrics. We make our implementation publicly available to foster\nfuture research in this area.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13901.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13387",
            "authors": [
                {
                    "_id": "6852239f0164cd13167104a4",
                    "name": "Beilei Cui",
                    "hidden": false
                },
                {
                    "_id": "6852239f0164cd13167104a5",
                    "name": "Yiming Huang",
                    "hidden": false
                },
                {
                    "_id": "6852239f0164cd13167104a6",
                    "name": "Long Bai",
                    "hidden": false
                },
                {
                    "_id": "6852239f0164cd13167104a7",
                    "name": "Hongliang Ren",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T11:50:00.000Z",
            "submittedOnDailyAt": "2025-06-18T00:57:15.579Z",
            "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
            "submittedOnDailyBy": {
                "_id": "68518fb45452a74491857c5b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png",
                "isPro": false,
                "fullname": "Beilei Cui",
                "user": "BeileiCui",
                "type": "user"
            },
            "summary": "This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)",
            "upvotes": 1,
            "discussionId": "6852239f0164cd13167104a8",
            "ai_summary": "A framework, TR2M, uses multimodal inputs to rescale relative depth to metric depth, enhancing performance across various datasets through cross-modality attention and contrastive learning.",
            "ai_keywords": [
                "relative depth estimation",
                "metric depth estimation",
                "cross-modality attention",
                "contrastive learning",
                "rescale maps",
                "pseudo metric depth",
                "intrinsically aligned scale distribution"
            ]
        },
        "publishedAt": "2025-06-16T07:50:00.000Z",
        "title": "TR2M: Transferring Monocular Relative Depth to Metric Depth with\n  Language Descriptions and Scale-Oriented Contrast",
        "summary": "This work presents a generalizable framework to transfer relative depth to\nmetric depth. Current monocular depth estimation methods are mainly divided\ninto metric depth estimation (MMDE) and relative depth estimation (MRDE). MMDEs\nestimate depth in metric scale but are often limited to a specific domain.\nMRDEs generalize well across different domains, but with uncertain scales which\nhinders downstream applications. To this end, we aim to build up a framework to\nsolve scale uncertainty and transfer relative depth to metric depth. Previous\nmethods used language as input and estimated two factors for conducting\nrescaling. Our approach, TR2M, utilizes both text description and image as\ninputs and estimates two rescale maps to transfer relative depth to metric\ndepth at pixel level. Features from two modalities are fused with a\ncross-modality attention module to better capture scale information. A strategy\nis designed to construct and filter confident pseudo metric depth for more\ncomprehensive supervision. We also develop scale-oriented contrastive learning\nto utilize depth distribution as guidance to enforce the model learning about\nintrinsic knowledge aligning with the scale distribution. TR2M only exploits a\nsmall number of trainable parameters to train on datasets in various domains\nand experiments not only demonstrate TR2M's great performance in seen datasets\nbut also reveal superior zero-shot capabilities on five unseen datasets. We\nshow the huge potential in pixel-wise transferring relative depth to metric\ndepth with language assistance. (Code is available at:\nhttps://github.com/BeileiCui/TR2M)",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13387.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68518fb45452a74491857c5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SOYIB8T0LsNZM3ORcHXlr.png",
            "fullname": "Beilei Cui",
            "name": "BeileiCui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.12880",
            "authors": [
                {
                    "_id": "685296b1a523e43c8c016658",
                    "user": {
                        "_id": "635671cdef1d4c919152b8e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635671cdef1d4c919152b8e8/kXi7uO9z_Et5vbJQTWfW5.jpeg",
                        "isPro": false,
                        "fullname": "Matan BT",
                        "user": "MatanBT",
                        "type": "user"
                    },
                    "name": "Matan Ben-Tov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T10:58:44.651Z",
                    "hidden": false
                },
                {
                    "_id": "685296b1a523e43c8c016659",
                    "name": "Mor Geva",
                    "hidden": false
                },
                {
                    "_id": "685296b1a523e43c8c01665a",
                    "name": "Mahmood Sharif",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-15T15:20:37.000Z",
            "submittedOnDailyAt": "2025-06-18T10:20:54.475Z",
            "title": "Universal Jailbreak Suffixes Are Strong Attention Hijackers",
            "submittedOnDailyBy": {
                "_id": "635671cdef1d4c919152b8e8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635671cdef1d4c919152b8e8/kXi7uO9z_Et5vbJQTWfW5.jpeg",
                "isPro": false,
                "fullname": "Matan BT",
                "user": "MatanBT",
                "type": "user"
            },
            "summary": "We study suffix-based jailbreaksx2013a powerful family of attacks\nagainst large language models (LLMs) that optimize adversarial suffixes to\ncircumvent safety alignment. Focusing on the widely used foundational GCG\nattack (Zou et al., 2023), we observe that suffixes vary in efficacy: some\nmarkedly more universalx2013generalizing to many unseen harmful\ninstructionsx2013than others. We first show that GCG's\neffectiveness is driven by a shallow, critical mechanism, built on the\ninformation flow from the adversarial suffix to the final chat template tokens\nbefore generation. Quantifying the dominance of this mechanism during\ngeneration, we find GCG irregularly and aggressively hijacks the\ncontextualization process. Crucially, we tie hijacking to the universality\nphenomenon, with more universal suffixes being stronger hijackers.\nSubsequently, we show that these insights have practical implications: GCG\nuniversality can be efficiently enhanced (up to times5 in some cases) at no\nadditional computational cost, and can also be surgically mitigated, at least\nhalving attack success with minimal utility loss. We release our code and data\nat http://github.com/matanbt/interp-jailbreak.",
            "upvotes": 1,
            "discussionId": "685296b1a523e43c8c01665b",
            "githubRepo": "https://github.com/matanbt/interp-jailbreak",
            "ai_summary": "Suffix-based jailbreaks exploit adversarial suffixes to hijack large language models, with effectiveness linked to suffix universality; the method can be enhanced and mitigated with minimal computational or utility cost.",
            "ai_keywords": [
                "suffix-based jailbreaks",
                "large language models",
                "adversarial suffixes",
                "safety alignment",
                "GCG attack",
                "information flow",
                "contextualization process",
                "suffix universality",
                "hijacking",
                "attack mitigation"
            ]
        },
        "publishedAt": "2025-06-15T11:20:37.000Z",
        "title": "Universal Jailbreak Suffixes Are Strong Attention Hijackers",
        "summary": "We study suffix-based jailbreaksx2013a powerful family of attacks\nagainst large language models (LLMs) that optimize adversarial suffixes to\ncircumvent safety alignment. Focusing on the widely used foundational GCG\nattack (Zou et al., 2023), we observe that suffixes vary in efficacy: some\nmarkedly more universalx2013generalizing to many unseen harmful\ninstructionsx2013than others. We first show that GCG's\neffectiveness is driven by a shallow, critical mechanism, built on the\ninformation flow from the adversarial suffix to the final chat template tokens\nbefore generation. Quantifying the dominance of this mechanism during\ngeneration, we find GCG irregularly and aggressively hijacks the\ncontextualization process. Crucially, we tie hijacking to the universality\nphenomenon, with more universal suffixes being stronger hijackers.\nSubsequently, we show that these insights have practical implications: GCG\nuniversality can be efficiently enhanced (up to times5 in some cases) at no\nadditional computational cost, and can also be surgically mitigated, at least\nhalving attack success with minimal utility loss. We release our code and data\nat http://github.com/matanbt/interp-jailbreak.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12880.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635671cdef1d4c919152b8e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/635671cdef1d4c919152b8e8/kXi7uO9z_Et5vbJQTWfW5.jpeg",
            "fullname": "Matan BT",
            "name": "MatanBT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14629",
            "authors": [
                {
                    "_id": "6852b60f10fbcda7a650e024",
                    "user": {
                        "_id": "66ec02212eb421192b62c303",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aw0TJjO3ohRlEYWlCzxQY.png",
                        "isPro": false,
                        "fullname": "Md. Adnanul Islam",
                        "user": "Adnanul",
                        "type": "user"
                    },
                    "name": "Md. Adnanul Islam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T19:47:56.745Z",
                    "hidden": false
                },
                {
                    "_id": "6852b60f10fbcda7a650e025",
                    "user": {
                        "_id": "667dcb41bc9abbfa3408382a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/667dcb41bc9abbfa3408382a/U0b_xMDvZuzsErL-hm9tL.jpeg",
                        "isPro": false,
                        "fullname": "Md. Faiyaz Abdullah Sayeedi",
                        "user": "FaiyazAbdullah114708",
                        "type": "user"
                    },
                    "name": "Md. Faiyaz Abdullah Sayeedi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T16:15:25.634Z",
                    "hidden": false
                },
                {
                    "_id": "6852b60f10fbcda7a650e026",
                    "name": "Md. Asaduzzaman Shuvo",
                    "hidden": false
                },
                {
                    "_id": "6852b60f10fbcda7a650e027",
                    "name": "Muhammad Ziaur Rahman",
                    "hidden": false
                },
                {
                    "_id": "6852b60f10fbcda7a650e028",
                    "name": "Shahanur Rahman Bappy",
                    "hidden": false
                },
                {
                    "_id": "6852b60f10fbcda7a650e029",
                    "name": "Raiyan Rahman",
                    "hidden": false
                },
                {
                    "_id": "6852b60f10fbcda7a650e02a",
                    "name": "Swakkhar Shatabda",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T15:24:30.000Z",
            "submittedOnDailyAt": "2025-06-18T11:28:31.203Z",
            "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning",
            "submittedOnDailyBy": {
                "_id": "667dcb41bc9abbfa3408382a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/667dcb41bc9abbfa3408382a/U0b_xMDvZuzsErL-hm9tL.jpeg",
                "isPro": false,
                "fullname": "Md. Faiyaz Abdullah Sayeedi",
                "user": "FaiyazAbdullah114708",
                "type": "user"
            },
            "summary": "Mosquito-borne diseases pose a major global health risk, requiring early\ndetection and proactive control of breeding sites to prevent outbreaks. In this\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\nand textual data to support automated detection, segmentation, and reasoning\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\nimages for object detection, 142 images for water surface segmentation, and\nnatural language reasoning texts linked to each image. The YOLOv9s model\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\n\"Prevention is Better than Cure\", showcasing how AI-based detection can\nproactively address mosquito-borne disease risks. The dataset and\nimplementation code are publicly available at GitHub:\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito",
            "upvotes": 0,
            "discussionId": "6852b60f10fbcda7a650e02b",
            "projectPage": "https://data.mendeley.com/datasets/rtsfh7jh7p/2",
            "githubRepo": "https://github.com/adnanul-islam-jisun/VisText-Mosquito",
            "ai_summary": "VisText-Mosquito is a multimodal dataset combining visual and textual data for automated mosquito breeding site detection, segmentation, and reasoning, utilizing YOLOv9s, YOLOv11n-Seg, and a fine-tuned BLIP model.",
            "ai_keywords": [
                "YOLOv9s",
                "YOLOv11n-Seg",
                "BLIP model",
                "object detection",
                "water surface segmentation",
                "reasoning generation",
                "BLEU score",
                "BERTScore",
                "ROUGE-L"
            ]
        },
        "publishedAt": "2025-06-17T11:24:30.000Z",
        "title": "VisText-Mosquito: A Multimodal Dataset and Benchmark for AI-Based\n  Mosquito Breeding Site Detection and Reasoning",
        "summary": "Mosquito-borne diseases pose a major global health risk, requiring early\ndetection and proactive control of breeding sites to prevent outbreaks. In this\npaper, we present VisText-Mosquito, a multimodal dataset that integrates visual\nand textual data to support automated detection, segmentation, and reasoning\nfor mosquito breeding site analysis. The dataset includes 1,828 annotated\nimages for object detection, 142 images for water surface segmentation, and\nnatural language reasoning texts linked to each image. The YOLOv9s model\nachieves the highest precision of 0.92926 and mAP@50 of 0.92891 for object\ndetection, while YOLOv11n-Seg reaches a segmentation precision of 0.91587 and\nmAP@50 of 0.79795. For reasoning generation, our fine-tuned BLIP model achieves\na final loss of 0.0028, with a BLEU score of 54.7, BERTScore of 0.91, and\nROUGE-L of 0.87. This dataset and model framework emphasize the theme\n\"Prevention is Better than Cure\", showcasing how AI-based detection can\nproactively address mosquito-borne disease risks. The dataset and\nimplementation code are publicly available at GitHub:\nhttps://github.com/adnanul-islam-jisun/VisText-Mosquito",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14629.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "667dcb41bc9abbfa3408382a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/667dcb41bc9abbfa3408382a/U0b_xMDvZuzsErL-hm9tL.jpeg",
            "fullname": "Md. Faiyaz Abdullah Sayeedi",
            "name": "FaiyazAbdullah114708",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13922",
            "authors": [
                {
                    "_id": "6852c1cf8dd3b4a7bdb8d9a9",
                    "user": {
                        "_id": "64e153cf75fae2212e8a140c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e153cf75fae2212e8a140c/qvhUNND1ENTUImH-WpiHC.jpeg",
                        "isPro": false,
                        "fullname": "Maximilian Du",
                        "user": "MaxDu",
                        "type": "user"
                    },
                    "name": "Maximilian Du",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-18T15:41:45.280Z",
                    "hidden": false
                },
                {
                    "_id": "6852c1cf8dd3b4a7bdb8d9aa",
                    "name": "Shuran Song",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64e153cf75fae2212e8a140c/fCFLL8CQmmy_9LeN0YPhG.mp4"
            ],
            "publishedAt": "2025-06-16T19:00:54.000Z",
            "submittedOnDailyAt": "2025-06-18T12:14:52.738Z",
            "title": "DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance",
            "submittedOnDailyBy": {
                "_id": "64e153cf75fae2212e8a140c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e153cf75fae2212e8a140c/qvhUNND1ENTUImH-WpiHC.jpeg",
                "isPro": false,
                "fullname": "Maximilian Du",
                "user": "MaxDu",
                "type": "user"
            },
            "summary": "Deploying large, complex policies in the real world requires the ability to\nsteer them to fit the needs of a situation. Most common steering approaches,\nlike goal-conditioning, require training the robot policy with a distribution\nof test-time objectives in mind. To overcome this limitation, we present\nDynaGuide, a steering method for diffusion policies using guidance from an\nexternal dynamics model during the diffusion denoising process. DynaGuide\nseparates the dynamics model from the base policy, which gives it multiple\nadvantages, including the ability to steer towards multiple objectives, enhance\nunderrepresented base policy behaviors, and maintain robustness on low-quality\nobjectives. The separate guidance signal also allows DynaGuide to work with\noff-the-shelf pretrained diffusion policies. We demonstrate the performance and\nfeatures of DynaGuide against other steering approaches in a series of\nsimulated and real experiments, showing an average steering success of 70% on a\nset of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x\nwhen steered with low-quality objectives. We also successfully steer an\noff-the-shelf real robot policy to express preference for particular objects\nand even create novel behavior. Videos and more can be found on the project\nwebsite: https://dynaguide.github.io",
            "upvotes": 0,
            "discussionId": "6852c1cf8dd3b4a7bdb8d9ab",
            "projectPage": "https://dynaguide.github.io/",
            "githubRepo": "https://github.com/MaxDu17/DynaGuide",
            "ai_summary": "DynaGuide, a steering method using an external dynamics model, enhances diffusion policies by allowing them to adapt to multiple objectives and maintain robustness, outperforming goal-conditioning especially with low-quality objectives.",
            "ai_keywords": [
                "diffusion policies",
                "guidance",
                "external dynamics model",
                "diffusion denoising process",
                "articulated CALVIN tasks",
                "goal-conditioning",
                "off-the-shelf real robot policy"
            ]
        },
        "publishedAt": "2025-06-16T15:00:54.000Z",
        "title": "DynaGuide: Steering Diffusion Polices with Active Dynamic Guidance",
        "summary": "Deploying large, complex policies in the real world requires the ability to\nsteer them to fit the needs of a situation. Most common steering approaches,\nlike goal-conditioning, require training the robot policy with a distribution\nof test-time objectives in mind. To overcome this limitation, we present\nDynaGuide, a steering method for diffusion policies using guidance from an\nexternal dynamics model during the diffusion denoising process. DynaGuide\nseparates the dynamics model from the base policy, which gives it multiple\nadvantages, including the ability to steer towards multiple objectives, enhance\nunderrepresented base policy behaviors, and maintain robustness on low-quality\nobjectives. The separate guidance signal also allows DynaGuide to work with\noff-the-shelf pretrained diffusion policies. We demonstrate the performance and\nfeatures of DynaGuide against other steering approaches in a series of\nsimulated and real experiments, showing an average steering success of 70% on a\nset of articulated CALVIN tasks and outperforming goal-conditioning by 5.4x\nwhen steered with low-quality objectives. We also successfully steer an\noff-the-shelf real robot policy to express preference for particular objects\nand even create novel behavior. Videos and more can be found on the project\nwebsite: https://dynaguide.github.io",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64e153cf75fae2212e8a140c/fCFLL8CQmmy_9LeN0YPhG.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13922.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e153cf75fae2212e8a140c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e153cf75fae2212e8a140c/qvhUNND1ENTUImH-WpiHC.jpeg",
            "fullname": "Maximilian Du",
            "name": "MaxDu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.12015",
            "authors": [
                {
                    "_id": "68529976a523e43c8c016672",
                    "name": "Hsi-Che Lin",
                    "hidden": false
                },
                {
                    "_id": "68529976a523e43c8c016673",
                    "name": "Yu-Chu Yu",
                    "hidden": false
                },
                {
                    "_id": "68529976a523e43c8c016674",
                    "name": "Kai-Po Chang",
                    "hidden": false
                },
                {
                    "_id": "68529976a523e43c8c016675",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-13T17:59:58.000Z",
            "submittedOnDailyAt": "2025-06-18T09:33:40.094Z",
            "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
            "submittedOnDailyBy": {
                "_id": "65099a88c9aa376f76bf756e",
                "avatarUrl": "/avatars/1e9643721c152f9999b6f35ba117a0d6.svg",
                "isPro": false,
                "fullname": "HSI CHE LIN",
                "user": "hsichelin",
                "type": "user"
            },
            "summary": "Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users.",
            "upvotes": 0,
            "discussionId": "68529976a523e43c8c016676",
            "projectPage": "https://hsi-che-lin.github.io/EMLoC/",
            "githubRepo": "https://github.com/hsi-che-lin/EMLoC",
            "ai_summary": "EMLoC, an memory-efficient fine-tuning framework using activation-aware SVD and LoRA, allows model adaptation within inference memory constraints for diverse applications.",
            "ai_keywords": [
                "activation-aware singular value decomposition",
                "SVD",
                "Emulator-based Memory-efficient fine-tuning framework",
                "LoRA Correction",
                "LoRA",
                "fine-tuning",
                "model fine-tuning",
                "standard training pipelines",
                "model adaptation"
            ]
        },
        "publishedAt": "2025-06-13T13:59:58.000Z",
        "title": "EMLoC: Emulator-based Memory-efficient Fine-tuning with LoRA Correction",
        "summary": "Open-source foundation models have seen rapid adoption and development,\nenabling powerful general-purpose capabilities across diverse domains. However,\nfine-tuning large foundation models for domain-specific or personalized tasks\nremains prohibitively expensive for most users due to the significant memory\noverhead beyond that of inference. We introduce EMLoC, an Emulator-based\nMemory-efficient fine-tuning framework with LoRA Correction, which enables\nmodel fine-tuning within the same memory budget required for inference. EMLoC\nconstructs a task-specific light-weight emulator using activation-aware\nsingular value decomposition (SVD) on a small downstream calibration set.\nFine-tuning then is performed on this lightweight emulator via LoRA. To tackle\nthe misalignment between the original model and the compressed emulator, we\npropose a novel compensation algorithm to correct the fine-tuned LoRA module,\nwhich thus can be merged into the original model for inference. EMLoC supports\nflexible compression ratios and standard training pipelines, making it\nadaptable to a wide range of applications. Extensive experiments demonstrate\nthat EMLoC outperforms other baselines across multiple datasets and modalities.\nMoreover, without quantization, EMLoC enables fine-tuning of a 38B model on a\nsingle 24GB consumer GPU-bringing efficient and practical model adaptation to\nindividual users.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.12015.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65099a88c9aa376f76bf756e",
            "avatarUrl": "/avatars/1e9643721c152f9999b6f35ba117a0d6.svg",
            "fullname": "HSI CHE LIN",
            "name": "hsichelin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.03939",
            "authors": [
                {
                    "_id": "6852e674e6284bcd92d04b09",
                    "name": "Junqi Gao",
                    "hidden": false
                },
                {
                    "_id": "6852e674e6284bcd92d04b0a",
                    "name": "Xiang Zou",
                    "hidden": false
                },
                {
                    "_id": "6852e674e6284bcd92d04b0b",
                    "name": "YIng Ai",
                    "hidden": false
                },
                {
                    "_id": "6852e674e6284bcd92d04b0c",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "6852e674e6284bcd92d04b0d",
                    "name": "Yichen Niu",
                    "hidden": false
                },
                {
                    "_id": "6852e674e6284bcd92d04b0e",
                    "name": "Biqing Qi",
                    "hidden": false
                },
                {
                    "_id": "6852e674e6284bcd92d04b0f",
                    "name": "Jianxing Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-04T13:31:21.000Z",
            "submittedOnDailyAt": "2025-06-18T14:48:45.738Z",
            "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "67ab05fe4c6ca2d5db4c0c52",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png",
                "isPro": false,
                "fullname": "Junqi Gao",
                "user": "ChetKao",
                "type": "user"
            },
            "summary": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.",
            "upvotes": 0,
            "discussionId": "6852e674e6284bcd92d04b10",
            "ai_summary": "Graph Counselor enhances Large Language Models by using multi-agent collaboration and adaptive reasoning to integrate knowledge effectively, improving factual accuracy and generation quality in specialized domains.",
            "ai_keywords": [
                "Graph Retrieval Augmented Generation (GraphRAG)",
                "Large Language Models (LLMs)",
                "Information Aggregation",
                "Reasoning Mechanism",
                "Multi-agent collaboration",
                "Adaptive Graph Information Extraction Module (AGIEM)",
                "Planning Agents",
                "Thought Agents",
                "Execution Agents",
                "Self-Reflection with Multiple Perspectives (SR)",
                "multi-level dependency modeling",
                "adaptive reasoning depth",
                "graph reasoning tasks",
                "reasoning accuracy",
                "generalization ability"
            ]
        },
        "publishedAt": "2025-06-04T09:31:21.000Z",
        "title": "Graph Counselor: Adaptive Graph Exploration via Multi-Agent Synergy to Enhance LLM Reasoning",
        "summary": "Graph Retrieval Augmented Generation (GraphRAG) effectively enhances external knowledge integration capabilities by explicitly modeling knowledge relationships, thereby improving the factual accuracy and generation quality of Large Language Models (LLMs) in specialized domains. However, existing methods suffer from two inherent limitations: 1) Inefficient Information Aggregation: They rely on a single agent and fixed iterative patterns, making it difficult to adaptively capture multi-level textual, structural, and degree information within graph data. 2) Rigid Reasoning Mechanism: They employ preset reasoning schemes, which cannot dynamically adjust reasoning depth nor achieve precise semantic correction. To overcome these limitations, we propose Graph Counselor, an GraphRAG method based on multi-agent collaboration. This method uses the Adaptive Graph Information Extraction Module (AGIEM), where Planning, Thought, and Execution Agents work together to precisely model complex graph structures and dynamically adjust information extraction strategies, addressing the challenges of multi-level dependency modeling and adaptive reasoning depth. Additionally, the Self-Reflection with Multiple Perspectives (SR) module improves the accuracy and semantic consistency of reasoning results through self-reflection and backward reasoning mechanisms. Experiments demonstrate that Graph Counselor outperforms existing methods in multiple graph reasoning tasks, exhibiting higher reasoning accuracy and generalization ability. Our code is available at https://github.com/gjq100/Graph-Counselor.git.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.03939.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ab05fe4c6ca2d5db4c0c52",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/QpGUNDkeuKjX71s2GXlXF.png",
            "fullname": "Junqi Gao",
            "name": "ChetKao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]