[
    {
        "paper": {
            "id": "2506.15675",
            "authors": [
                {
                    "_id": "6853946599bf39f9665c79e0",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e1",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e2",
                    "name": "Xiaofeng Mao",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e3",
                    "name": "Shaoheng Lin",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e4",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e5",
                    "name": "Shitian Zhao",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e6",
                    "name": "Zhaopan Xu",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e7",
                    "name": "Xinyue Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e8",
                    "name": "Yukang Feng",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79e9",
                    "name": "Jianwen Sun",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ea",
                    "name": "Zizhen Li",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79eb",
                    "name": "Fanrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ec",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ed",
                    "name": "Zhixiang Wang",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ee",
                    "name": "Yuwei Wu",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79ef",
                    "name": "Tong He",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79f0",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79f1",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79f2",
                    "name": "Yunde Jia",
                    "hidden": false
                },
                {
                    "_id": "6853946599bf39f9665c79f3",
                    "user": {
                        "_id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-19T10:09:38.100Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
            ],
            "publishedAt": "2025-06-18T17:57:06.000Z",
            "submittedOnDailyAt": "2025-06-19T03:16:13.113Z",
            "title": "Sekai: A Video Dataset towards World Exploration",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
            "upvotes": 39,
            "discussionId": "6853946599bf39f9665c79f4",
            "projectPage": "https://lixsp11.github.io/sekai-project/",
            "githubRepo": "https://github.com/Lixsp11/sekai-codebase",
            "ai_summary": "Sekai, a worldwide video dataset with comprehensive annotations, is introduced to support world exploration applications, enhancing video generation models.",
            "ai_keywords": [
                "first-person view",
                "worldwide video dataset",
                "rich annotations",
                "FPV",
                "UVA",
                "video collection",
                "pre-processing",
                "camera trajectories",
                "interactive video world exploration model"
            ]
        },
        "publishedAt": "2025-06-18T13:57:06.000Z",
        "title": "Sekai: A Video Dataset towards World Exploration",
        "summary": "Video generation techniques have made remarkable progress, promising to be\nthe foundation of interactive world exploration. However, existing video\ngeneration datasets are not well-suited for world exploration training as they\nsuffer from some limitations: limited locations, short duration, static scenes,\nand a lack of annotations about exploration and the world. In this paper, we\nintroduce Sekai (meaning ``world'' in Japanese), a high-quality first-person\nview worldwide video dataset with rich annotations for world exploration. It\nconsists of over 5,000 hours of walking or drone view (FPV and UVA) videos from\nover 100 countries and regions across 750 cities. We develop an efficient and\neffective toolbox to collect, pre-process and annotate videos with location,\nscene, weather, crowd density, captions, and camera trajectories. Experiments\ndemonstrate the quality of the dataset. And, we use a subset to train an\ninteractive video world exploration model, named YUME (meaning ``dream'' in\nJapanese). We believe Sekai will benefit the area of video generation and world\nexploration, and motivate valuable applications.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/GFrjhPvZnILgeTd_w2kvc.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15675.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.15211",
            "authors": [
                {
                    "_id": "6853f43241586a7b0193a0b1",
                    "name": "Feng He",
                    "hidden": false
                },
                {
                    "_id": "6853f43241586a7b0193a0b2",
                    "name": "Zijun Chen",
                    "hidden": false
                },
                {
                    "_id": "6853f43241586a7b0193a0b3",
                    "user": {
                        "_id": "60adea21e3de7c7440abb863",
                        "avatarUrl": "/avatars/7fdba0e15ea435a4ea41106df574f312.svg",
                        "isPro": false,
                        "fullname": "Xinnian Liang",
                        "user": "EricLiang98",
                        "type": "user"
                    },
                    "name": "Xinnian Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-19T15:13:09.408Z",
                    "hidden": false
                },
                {
                    "_id": "6853f43241586a7b0193a0b4",
                    "user": {
                        "_id": "64f6be755f2dee8a6b9b5657",
                        "avatarUrl": "/avatars/8e06fe0155ae23432f31f77b406754c0.svg",
                        "isPro": false,
                        "fullname": "Tingting Ma",
                        "user": "violetma007",
                        "type": "user"
                    },
                    "name": "Tingting Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-19T15:13:33.868Z",
                    "hidden": false
                },
                {
                    "_id": "6853f43241586a7b0193a0b5",
                    "name": "Yunqi Qiu",
                    "hidden": false
                },
                {
                    "_id": "6853f43241586a7b0193a0b6",
                    "user": {
                        "_id": "637301f4bb66bd6b13206a25",
                        "avatarUrl": "/avatars/6925439441324f6fd00d167d471edff2.svg",
                        "isPro": false,
                        "fullname": "Shuangzhi Wu",
                        "user": "Shuangzhi",
                        "type": "user"
                    },
                    "name": "Shuangzhi Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-19T15:13:28.504Z",
                    "hidden": false
                },
                {
                    "_id": "6853f43241586a7b0193a0b7",
                    "user": {
                        "_id": "667289f903c802764985d8c6",
                        "avatarUrl": "/avatars/916befcbf0e52ce56be49617f31c7bb2.svg",
                        "isPro": false,
                        "fullname": "Junchi Yan",
                        "user": "Rethinker",
                        "type": "user"
                    },
                    "name": "Junchi Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-19T15:13:22.902Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T07:44:09.000Z",
            "submittedOnDailyAt": "2025-06-19T09:59:13.193Z",
            "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning\n  in LLMs",
            "submittedOnDailyBy": {
                "_id": "60adea21e3de7c7440abb863",
                "avatarUrl": "/avatars/7fdba0e15ea435a4ea41106df574f312.svg",
                "isPro": false,
                "fullname": "Xinnian Liang",
                "user": "EricLiang98",
                "type": "user"
            },
            "summary": "Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.",
            "upvotes": 23,
            "discussionId": "6853f43341586a7b0193a0b8",
            "ai_summary": "ProtoReasoning enhances large reasoning models through prototypical representations, leading to improved cross-domain generalization in logical reasoning, planning, and other tasks.",
            "ai_keywords": [
                "Large Reasoning Models",
                "Long Chain-of-Thought",
                "cross-domain generalization",
                "abstract reasoning prototypes",
                "ProtoReasoning",
                "prototype representations",
                "Prolog",
                "PDDL",
                "automated prototype construction pipeline",
                "verification system",
                "Enigmata-Eval",
                "MMLU",
                "AIME24"
            ]
        },
        "publishedAt": "2025-06-18T03:44:09.000Z",
        "title": "ProtoReasoning: Prototypes as the Foundation for Generalizable Reasoning\n  in LLMs",
        "summary": "Recent advances in Large Reasoning Models (LRMs) trained with Long\nChain-of-Thought (Long CoT) reasoning have demonstrated remarkable cross-domain\ngeneralization capabilities. However, the underlying mechanisms supporting such\ntransfer remain poorly understood. We hypothesize that cross-domain\ngeneralization arises from shared abstract reasoning prototypes -- fundamental\nreasoning patterns that capture the essence of problems across domains. These\nprototypes minimize the nuances of the representation, revealing that seemingly\ndiverse tasks are grounded in shared reasoning structures.Based on this\nhypothesis, we propose ProtoReasoning, a framework that enhances the reasoning\nability of LLMs by leveraging scalable and verifiable prototypical\nrepresentations (Prolog for logical reasoning, PDDL for\nplanning).ProtoReasoning features: (1) an automated prototype construction\npipeline that transforms problems into corresponding prototype representations;\n(2) a comprehensive verification system providing reliable feedback through\nProlog/PDDL interpreters; (3) the scalability to synthesize problems\narbitrarily within prototype space while ensuring correctness. Extensive\nexperiments show that ProtoReasoning achieves 4.7% improvement over baseline\nmodels on logical reasoning (Enigmata-Eval), 6.3% improvement on planning\ntasks, 4.0% improvement on general reasoning (MMLU) and 1.0% on mathematics\n(AIME24). Significantly, our ablation studies confirm that learning in\nprototype space also demonstrates enhanced generalization to structurally\nsimilar problems compared to training solely on natural language\nrepresentations, validating our hypothesis that reasoning prototypes serve as\nthe foundation for generalizable reasoning in large language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15211.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60adea21e3de7c7440abb863",
            "avatarUrl": "/avatars/7fdba0e15ea435a4ea41106df574f312.svg",
            "fullname": "Xinnian Liang",
            "name": "EricLiang98",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.15681",
            "authors": [
                {
                    "_id": "68536fc899bf39f9665c7961",
                    "user": {
                        "_id": "657152eb12f162153b50ec9d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
                        "isPro": false,
                        "fullname": "Byung-Kwan Lee",
                        "user": "BK-Lee",
                        "type": "user"
                    },
                    "name": "Byung-Kwan Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-19T09:10:25.415Z",
                    "hidden": false
                },
                {
                    "_id": "68536fc899bf39f9665c7962",
                    "user": {
                        "_id": "65b33e5f7cd0069ad648c4e8",
                        "avatarUrl": "/avatars/1a746ea535cffa92ea08006e05ea414a.svg",
                        "isPro": false,
                        "fullname": "Ryo Hachiuma",
                        "user": "rhachiuma",
                        "type": "user"
                    },
                    "name": "Ryo Hachiuma",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-19T02:02:49.546Z",
                    "hidden": false
                },
                {
                    "_id": "68536fc899bf39f9665c7963",
                    "name": "Yong Man Ro",
                    "hidden": false
                },
                {
                    "_id": "68536fc899bf39f9665c7964",
                    "name": "Yu-Chiang Frank Wang",
                    "hidden": false
                },
                {
                    "_id": "68536fc899bf39f9665c7965",
                    "name": "Yueh-Hua Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
            ],
            "publishedAt": "2025-06-18T17:59:49.000Z",
            "submittedOnDailyAt": "2025-06-19T00:36:10.331Z",
            "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "657152eb12f162153b50ec9d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
                "isPro": false,
                "fullname": "Byung-Kwan Lee",
                "user": "BK-Lee",
                "type": "user"
            },
            "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
            "upvotes": 19,
            "discussionId": "68536fc899bf39f9665c7966",
            "projectPage": "https://byungkwanlee.github.io/GenRecal-page/",
            "ai_summary": "GenRecal, a novel distillation framework, improves performance of vision-language models by aligning feature representations across different architectures.",
            "ai_keywords": [
                "vision-language models",
                "large language models",
                "distillation framework",
                "GenerRecal",
                "recalibration",
                "feature representations",
                "heterogeneous VLMs"
            ]
        },
        "publishedAt": "2025-06-18T13:59:49.000Z",
        "title": "GenRecal: Generation after Recalibration from Large to Small\n  Vision-Language Models",
        "summary": "Recent advancements in vision-language models (VLMs) have leveraged large\nlanguage models (LLMs) to achieve performance on par with closed-source systems\nlike GPT-4V. However, deploying these models in real-world scenarios,\nparticularly on resource-constrained devices, remains challenging due to their\nsubstantial computational demands. This has spurred interest in distilling\nknowledge from large VLMs into smaller, more efficient counterparts. A key\nchallenge arises here from the diversity of VLM architectures, which are built\non different LLMs and employ varying token types-differing in vocabulary size,\ntoken splits, and token index ordering. To address this challenge of limitation\nto a specific VLM type, we present Generation after Recalibration (GenRecal), a\nnovel, general-purpose distillation framework for VLMs. GenRecal incorporates a\nRecalibrator that aligns and adapts feature representations between\nheterogeneous VLMs, enabling effective knowledge transfer across different\ntypes of VLMs. Through extensive experiments on multiple challenging\nbenchmarks, we demonstrate that GenRecal significantly improves baseline\nperformances, eventually outperforming large-scale open- and closed-source\nVLMs.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/657152eb12f162153b50ec9d/2xNUivkkqkJWJLWtIPNvb.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15681.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "657152eb12f162153b50ec9d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/657152eb12f162153b50ec9d/qnldHP35PclV0pDz_05q8.jpeg",
            "fullname": "Byung-Kwan Lee",
            "name": "BK-Lee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 56
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.15677",
            "authors": [
                {
                    "_id": "68536b2399bf39f9665c794c",
                    "name": "Yining Hong",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c794d",
                    "name": "Rui Sun",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c794e",
                    "name": "Bingxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c794f",
                    "name": "Xingcheng Yao",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7950",
                    "name": "Maxine Wu",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7951",
                    "name": "Alexander Chien",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7952",
                    "name": "Da Yin",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7953",
                    "name": "Ying Nian Wu",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7954",
                    "name": "Zhecan James Wang",
                    "hidden": false
                },
                {
                    "_id": "68536b2399bf39f9665c7955",
                    "name": "Kai-Wei Chang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T17:58:17.000Z",
            "submittedOnDailyAt": "2025-06-19T00:14:22.240Z",
            "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
            "submittedOnDailyBy": {
                "_id": "6431b64df76c34519e93d1ba",
                "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
                "isPro": true,
                "fullname": "Yining Hong",
                "user": "evelynhong",
                "type": "user"
            },
            "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
            "upvotes": 13,
            "discussionId": "68536b2399bf39f9665c7956",
            "ai_summary": "Embodied Web Agents integrate physical interaction and web-scale reasoning to assess cross-domain intelligence in a novel benchmark environment.",
            "ai_keywords": [
                "Embodied Web Agents",
                "task environments",
                "simulation platform",
                "3D indoor and outdoor environments",
                "functional web interfaces",
                "Embodied Web Agents Benchmark",
                "systematic assessment",
                "cross-domain intelligence",
                "embodied cognition"
            ]
        },
        "publishedAt": "2025-06-18T13:58:17.000Z",
        "title": "Embodied Web Agents: Bridging Physical-Digital Realms for Integrated\n  Agent Intelligence",
        "summary": "AI agents today are mostly siloed - they either retrieve and reason over vast\namount of digital information and knowledge obtained online; or interact with\nthe physical world through embodied perception, planning and action - but\nrarely both. This separation limits their ability to solve tasks that require\nintegrated physical and digital intelligence, such as cooking from online\nrecipes, navigating with dynamic map data, or interpreting real-world landmarks\nusing web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI\nagents that fluidly bridge embodiment and web-scale reasoning. To\noperationalize this concept, we first develop the Embodied Web Agents task\nenvironments, a unified simulation platform that tightly integrates realistic\n3D indoor and outdoor environments with functional web interfaces. Building\nupon this platform, we construct and release the Embodied Web Agents Benchmark,\nwhich encompasses a diverse suite of tasks including cooking, navigation,\nshopping, tourism, and geolocation - all requiring coordinated reasoning across\nphysical and digital realms for systematic assessment of cross-domain\nintelligence. Experimental results reveal significant performance gaps between\nstate-of-the-art AI systems and human capabilities, establishing both\nchallenges and opportunities at the intersection of embodied cognition and\nweb-scale knowledge access. All datasets, codes and websites are publicly\navailable at our project page https://embodied-web-agent.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15677.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6431b64df76c34519e93d1ba",
            "avatarUrl": "/avatars/ea577762b6b4798f87a7a3f1d53d082c.svg",
            "fullname": "Yining Hong",
            "name": "evelynhong",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.13414",
            "authors": [
                {
                    "_id": "6853f62841586a7b0193a0ba",
                    "name": "Alexander Polok",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0bb",
                    "name": "Jiangyu Han",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0bc",
                    "name": "Dominik Klement",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0bd",
                    "name": "Samuele Cornell",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0be",
                    "name": "Jan Černocký",
                    "hidden": false
                },
                {
                    "_id": "6853f62841586a7b0193a0bf",
                    "name": "Lukáš Burget",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T12:28:35.000Z",
            "submittedOnDailyAt": "2025-06-19T10:07:26.619Z",
            "title": "BUT System for the MLC-SLM Challenge",
            "submittedOnDailyBy": {
                "_id": "638d2b765e14c2f38677987b",
                "avatarUrl": "/avatars/622c183e897a99e33717f4a92305fbd3.svg",
                "isPro": false,
                "fullname": "Alexander Polok",
                "user": "Lakoc",
                "type": "user"
            },
            "summary": "We present a two-speaker automatic speech recognition (ASR) system that\ncombines DiCoW -- a diarization-conditioned variant of Whisper -- with\nDiariZen, a diarization pipeline built on top of Pyannote. We first evaluate\nboth systems in out-of-domain (OOD) multilingual scenarios without any\nfine-tuning. In this scenario, DiariZen consistently outperforms the baseline\nPyannote diarization model, demonstrating strong generalization. Despite being\nfine-tuned on English-only data for target-speaker ASR, DiCoW retains solid\nmultilingual performance, indicating that encoder modifications preserve\nWhisper's multilingual capabilities. We then fine-tune both DiCoW and DiariZen\non the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform\nthe fine-tuned Pyannote baseline, while DiCoW sees further gains from domain\nadaptation. Our final system achieves a micro-average tcpWER/CER of 16.75% and\nranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several\nlabeling inconsistencies in the training data -- such as missing speech\nsegments and incorrect silence annotations -- which can hinder diarization\nfine-tuning. We propose simple mitigation strategies to address these issues\nand improve system robustness.",
            "upvotes": 12,
            "discussionId": "6853f62841586a7b0193a0c0",
            "ai_summary": "The combined DiCoW and DiariZen ASR system demonstrates strong performance in multilingual scenarios, with DiCoW preserving its multilingual capabilities and DiariZen improving through fine-tuning.",
            "ai_keywords": [
                "DiCoW",
                "DiariZen",
                "Whisper",
                "Pyannote",
                "ASR",
                "out-of-domain",
                "multilingual",
                "fine-tuning",
                "micro-average tcpWER/CER",
                "MLC-SLM challenge",
                "labeling inconsistencies"
            ]
        },
        "publishedAt": "2025-06-16T08:28:35.000Z",
        "title": "BUT System for the MLC-SLM Challenge",
        "summary": "We present a two-speaker automatic speech recognition (ASR) system that\ncombines DiCoW -- a diarization-conditioned variant of Whisper -- with\nDiariZen, a diarization pipeline built on top of Pyannote. We first evaluate\nboth systems in out-of-domain (OOD) multilingual scenarios without any\nfine-tuning. In this scenario, DiariZen consistently outperforms the baseline\nPyannote diarization model, demonstrating strong generalization. Despite being\nfine-tuned on English-only data for target-speaker ASR, DiCoW retains solid\nmultilingual performance, indicating that encoder modifications preserve\nWhisper's multilingual capabilities. We then fine-tune both DiCoW and DiariZen\non the MLC-SLM challenge data. The fine-tuned DiariZen continues to outperform\nthe fine-tuned Pyannote baseline, while DiCoW sees further gains from domain\nadaptation. Our final system achieves a micro-average tcpWER/CER of 16.75% and\nranks second in Task 2 of the MLC-SLM challenge. Lastly, we identify several\nlabeling inconsistencies in the training data -- such as missing speech\nsegments and incorrect silence annotations -- which can hinder diarization\nfine-tuning. We propose simple mitigation strategies to address these issues\nand improve system robustness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13414.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638d2b765e14c2f38677987b",
            "avatarUrl": "/avatars/622c183e897a99e33717f4a92305fbd3.svg",
            "fullname": "Alexander Polok",
            "name": "Lakoc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.15569",
            "authors": [
                {
                    "_id": "68537ca999bf39f9665c799a",
                    "name": "Chengye Wang",
                    "hidden": false
                },
                {
                    "_id": "68537ca999bf39f9665c799b",
                    "name": "Yifei Shen",
                    "hidden": false
                },
                {
                    "_id": "68537ca999bf39f9665c799c",
                    "name": "Zexi Kuang",
                    "hidden": false
                },
                {
                    "_id": "68537ca999bf39f9665c799d",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "68537ca999bf39f9665c799e",
                    "user": {
                        "_id": "62f662bcc58915315c4eccea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                        "isPro": true,
                        "fullname": "Yilun Zhao",
                        "user": "yilunzhao",
                        "type": "user"
                    },
                    "name": "Yilun Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-19T09:10:23.732Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T15:43:26.000Z",
            "submittedOnDailyAt": "2025-06-19T01:28:40.934Z",
            "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification",
            "submittedOnDailyBy": {
                "_id": "62f662bcc58915315c4eccea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                "isPro": true,
                "fullname": "Yilun Zhao",
                "user": "yilunzhao",
                "type": "user"
            },
            "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.",
            "upvotes": 9,
            "discussionId": "68537ca999bf39f9665c799f",
            "githubRepo": "https://github.com/QDRhhhh/SciVer",
            "ai_summary": "A benchmark named SciVer evaluates multimodal foundation models' claim verification capabilities within scientific contexts, revealing performance gaps and limitations in current models.",
            "ai_keywords": [
                "retrieval-augmented generation (RAG)"
            ]
        },
        "publishedAt": "2025-06-18T11:43:26.000Z",
        "title": "SciVer: Evaluating Foundation Models for Multimodal Scientific Claim\n  Verification",
        "summary": "We introduce SciVer, the first benchmark specifically designed to evaluate\nthe ability of foundation models to verify claims within a multimodal\nscientific context. SciVer consists of 3,000 expert-annotated examples over\n1,113 scientific papers, covering four subsets, each representing a common\nreasoning type in multimodal scientific claim verification. To enable\nfine-grained evaluation, each example includes expert-annotated supporting\nevidence. We assess the performance of 21 state-of-the-art multimodal\nfoundation models, including o4-mini, Gemini-2.5-Flash, Llama-3.2-Vision, and\nQwen2.5-VL. Our experiment reveals a substantial performance gap between these\nmodels and human experts on SciVer. Through an in-depth analysis of\nretrieval-augmented generation (RAG), and human-conducted error evaluations, we\nidentify critical limitations in current open-source models, offering key\ninsights to advance models' comprehension and reasoning in multimodal\nscientific literature tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15569.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "fullname": "Yilun Zhao",
            "name": "yilunzhao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.15068",
            "authors": [
                {
                    "_id": "68536bf399bf39f9665c7958",
                    "name": "Zongxia Li",
                    "hidden": false
                },
                {
                    "_id": "68536bf399bf39f9665c7959",
                    "name": "Yapei Chang",
                    "hidden": false
                },
                {
                    "_id": "68536bf399bf39f9665c795a",
                    "name": "Yuhang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68536bf399bf39f9665c795b",
                    "name": "Xiyang Wu",
                    "hidden": false
                },
                {
                    "_id": "68536bf399bf39f9665c795c",
                    "name": "Zichao Liang",
                    "hidden": false
                },
                {
                    "_id": "68536bf399bf39f9665c795d",
                    "name": "Yoo Yeon Sung",
                    "hidden": false
                },
                {
                    "_id": "68536bf399bf39f9665c795e",
                    "name": "Jordan Lee Boyd-Graber",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T02:16:53.000Z",
            "submittedOnDailyAt": "2025-06-19T04:04:25.155Z",
            "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation",
            "submittedOnDailyBy": {
                "_id": "64ea62f918d79efd533c93fe",
                "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
                "isPro": false,
                "fullname": "Xiyang Wu",
                "user": "wuxiyang",
                "type": "user"
            },
            "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
            "upvotes": 9,
            "discussionId": "68536bf399bf39f9665c795f",
            "ai_summary": "PrefBERT, a scoring model, improves open-ended long-form generation by providing better semantic reward feedback than traditional metrics.",
            "ai_keywords": [
                "PrefBERT",
                "GRPO",
                "multi-sentence responses",
                "paragraph-length responses",
                "Likert-rated quality",
                "LLM-as-a-judge",
                "human ratings",
                "qualitative analysis",
                "verifiable rewards"
            ]
        },
        "publishedAt": "2025-06-17T22:16:53.000Z",
        "title": "Semantically-Aware Rewards for Open-Ended R1 Training in Free-Form\n  Generation",
        "summary": "Evaluating open-ended long-form generation is challenging because it is hard\nto define what clearly separates good from bad outputs. Existing methods often\nmiss key aspects like coherence, style, or relevance, or are biased by\npretraining data, making open-ended long-form evaluation an underexplored\nproblem. To address this gap, we propose PrefBERT, a scoring model for\nevaluating open-ended long-form generation in GRPO and guiding its training\nwith distinct rewards for good and bad outputs. Trained on two response\nevaluation datasets with diverse long-form styles and Likert-rated quality,\nPrefBERT effectively supports GRPO by offering better semantic reward feedback\nthan traditional metrics ROUGE-L and BERTScore do. Through comprehensive\nevaluations, including LLM-as-a-judge, human ratings, and qualitative analysis,\nwe show that PrefBERT, trained on multi-sentence and paragraph-length\nresponses, remains reliable across varied long passages and aligns well with\nthe verifiable rewards GRPO needs. Human evaluations confirm that using\nPrefBERT as the reward signal to train policy models yields responses better\naligned with human preferences than those trained with traditional metrics. Our\ncode is available at https://github.com/zli12321/long_form_rl.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15068.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ea62f918d79efd533c93fe",
            "avatarUrl": "/avatars/9985a789ce11b788de2cba12adfb72fc.svg",
            "fullname": "Xiyang Wu",
            "name": "wuxiyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.15461",
            "authors": [
                {
                    "_id": "68546c107bc8d012d4ca9936",
                    "name": "Nikolay Blagoev",
                    "hidden": false
                },
                {
                    "_id": "68546c107bc8d012d4ca9937",
                    "name": "Oğuzhan Ersoy",
                    "hidden": false
                },
                {
                    "_id": "68546c107bc8d012d4ca9938",
                    "name": "Lydia Yiyu Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66d252ec8a438492b0d6e4ce/1jqPNLFfa-XhuNMM609Nd.jpeg"
            ],
            "publishedAt": "2025-06-18T13:48:33.000Z",
            "submittedOnDailyAt": "2025-06-19T18:32:51.275Z",
            "title": "All is Not Lost: LLM Recovery without Checkpoints",
            "submittedOnDailyBy": {
                "_id": "66d252ec8a438492b0d6e4ce",
                "avatarUrl": "/avatars/4f0fc7fbe8afae463741f786ea774e95.svg",
                "isPro": true,
                "fullname": "Ben",
                "user": "benfielding",
                "type": "user"
            },
            "summary": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.",
            "upvotes": 8,
            "discussionId": "68546c107bc8d012d4ca9939",
            "projectPage": "https://www.gensyn.ai/articles/checkfree",
            "githubRepo": "https://github.com/gensyn-ai/CheckFree",
            "ai_summary": "A novel method, CheckFree, and its extended version CheckFree+, efficiently recover from node failures during LLM training by substituting failed stages with averaged neighboring stages or through out-of-order pipeline execution, improving convergence time over existing checkpointing methods.",
            "ai_keywords": [
                "LLMs",
                "decentralized computation",
                "node churn",
                "checkpointing",
                "redundant computation",
                "out-of-order pipeline execution",
                "LLaMa models",
                "(de)embedding layers",
                "wall-clock time convergence"
            ]
        },
        "publishedAt": "2025-06-18T09:48:33.000Z",
        "title": "All is Not Lost: LLM Recovery without Checkpoints",
        "summary": "Training LLMs on decentralized and wimpy computation nodes, e.g., multiple\non-spot instances, lowers the training cost and enables model democratization.\nThe inevitable challenge here is the churn of nodes due to failures and the\noperator's scheduling policies, leading to losing a stage - a part of the\nmodel. The conventional approaches to recover from failures are to either use\ncheckpointing, where periodically a copy of the entire model is sent to an\nadditional storage, or redundant computation. These approaches yield\nsignificant communication and/or computation overhead even in non-failure cases\nand scale poorly in settings with large models. In this paper, we propose,\nCheckFree, an efficient recovery method where a failing stage is substituted by\na weighted average of the closest neighboring stages. In contrast to the state\nof the art, CheckFree requires no additional computation or storage. However,\nbecause of the nature of averaging neighbouring stages, it can only recover\nfailures of intermediate stages. We further extend our method to CheckFree+\nwith out-of-order pipeline execution to tolerate crashes of the first and last\nstages. Thanks to out-of-order pipelining, behaviour of those stages is\nmimicked by their neighboring ones, which allows CheckFree+ to recover them by\nsimply copying the weights from the immediate neighbour. To be able to recover\nthe (de)embedding layers, CheckFree+ copies those layers to the neighboring\nstages, which requires relatively small storage overhead. We extensively\nevaluate our method on LLaMa models of model sizes from 124M to 1.5B with\nvarying failure frequencies. In the case of low and medium failure rates\n(5-10%), CheckFree and CheckFree+ outperform both checkpointing and redundant\ncomputation in terms of convergence in wall-clock time by over 12%. Both of our\nproposals can be run via our code available at:\nhttps://github.com/gensyn-ai/CheckFree.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66d252ec8a438492b0d6e4ce/1jqPNLFfa-XhuNMM609Nd.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15461.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66d252ec8a438492b0d6e4ce",
            "avatarUrl": "/avatars/4f0fc7fbe8afae463741f786ea774e95.svg",
            "fullname": "Ben",
            "name": "benfielding",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.15672",
            "authors": [
                {
                    "_id": "6853bbcc99bf39f9665c7a50",
                    "user": {
                        "_id": "62cecf9c1415317d1fbf6cfe",
                        "avatarUrl": "/avatars/d30630ad96bcec1349728ba39476847a.svg",
                        "isPro": false,
                        "fullname": "Yao Zhang",
                        "user": "ZYao720",
                        "type": "user"
                    },
                    "name": "Yao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-19T15:11:29.957Z",
                    "hidden": false
                },
                {
                    "_id": "6853bbcc99bf39f9665c7a51",
                    "name": "Chenyang Lin",
                    "hidden": false
                },
                {
                    "_id": "6853bbcc99bf39f9665c7a52",
                    "name": "Shijie Tang",
                    "hidden": false
                },
                {
                    "_id": "6853bbcc99bf39f9665c7a53",
                    "name": "Haokun Chen",
                    "hidden": false
                },
                {
                    "_id": "6853bbcc99bf39f9665c7a54",
                    "name": "Shijie Zhou",
                    "hidden": false
                },
                {
                    "_id": "6853bbcc99bf39f9665c7a55",
                    "name": "Yunpu Ma",
                    "hidden": false
                },
                {
                    "_id": "6853bbcc99bf39f9665c7a56",
                    "name": "Volker Tresp",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T17:54:55.000Z",
            "submittedOnDailyAt": "2025-06-19T06:26:51.897Z",
            "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence",
            "submittedOnDailyBy": {
                "_id": "648cbea3dee03837c823cbf2",
                "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
                "isPro": false,
                "fullname": "Shuo Chen",
                "user": "ShuoChen99",
                "type": "user"
            },
            "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.",
            "upvotes": 5,
            "discussionId": "6853bbcc99bf39f9665c7a57",
            "ai_summary": "SwarmAgentic is a framework for automated agentic system generation that optimize agent functionality and collaboration through language-driven exploration, outperforming existing baselines in unconstrained tasks.",
            "ai_keywords": [
                "Large Language Models",
                "agentic systems",
                "from-scratch agent generation",
                "self-optimizing agent functionality",
                "collaboration",
                "Particle Swarm Optimization (PSO)",
                "TravelPlanner benchmark",
                "system-level coordination",
                "creative reasoning"
            ]
        },
        "publishedAt": "2025-06-18T13:54:55.000Z",
        "title": "SwarmAgentic: Towards Fully Automated Agentic System Generation via\n  Swarm Intelligence",
        "summary": "The rapid progress of Large Language Models has advanced agentic systems in\ndecision-making, coordination, and task execution. Yet, existing agentic system\ngeneration frameworks lack full autonomy, missing from-scratch agent\ngeneration, self-optimizing agent functionality, and collaboration, limiting\nadaptability and scalability. We propose SwarmAgentic, a framework for fully\nautomated agentic system generation that constructs agentic systems from\nscratch and jointly optimizes agent functionality and collaboration as\ninterdependent components through language-driven exploration. To enable\nefficient search over system-level structures, SwarmAgentic maintains a\npopulation of candidate systems and evolves them via feedback-guided updates,\ndrawing inspiration from Particle Swarm Optimization (PSO). We evaluate our\nmethod on six real-world, open-ended, and exploratory tasks involving\nhigh-level planning, system-level coordination, and creative reasoning. Given\nonly a task description and an objective function, SwarmAgentic outperforms all\nbaselines, achieving a +261.8% relative improvement over ADAS on the\nTravelPlanner benchmark, highlighting the effectiveness of full automation in\nstructurally unconstrained tasks. This framework marks a significant step\ntoward scalable and autonomous agentic system design, bridging swarm\nintelligence with fully automated system multi-agent generation. Our code is\npublicly released at https://yaoz720.github.io/SwarmAgentic/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15672.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "648cbea3dee03837c823cbf2",
            "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
            "fullname": "Shuo Chen",
            "name": "ShuoChen99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.15050",
            "authors": [
                {
                    "_id": "68539c6199bf39f9665c79f6",
                    "name": "Tiantian Fan",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c79f7",
                    "name": "Lingjun Liu",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c79f8",
                    "name": "Yu Yue",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c79f9",
                    "name": "Jiaze Chen",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c79fa",
                    "name": "Chengyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c79fb",
                    "name": "Qiying Yu",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c79fc",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c79fd",
                    "name": "Zhiqi Lin",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c79fe",
                    "name": "Ruofei Zhu",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c79ff",
                    "name": "Yufeng Yuan",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a00",
                    "name": "Xiaochen Zuo",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a01",
                    "name": "Bole Ma",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a02",
                    "name": "Mofan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a03",
                    "name": "Gaohong Liu",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a04",
                    "name": "Ru Zhang",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a05",
                    "name": "Haotian Zhou",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a06",
                    "name": "Cong Xie",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a07",
                    "name": "Ruidong Zhu",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a08",
                    "name": "Zhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a09",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a0a",
                    "name": "Mingxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a0b",
                    "name": "Lin Yan",
                    "hidden": false
                },
                {
                    "_id": "68539c6199bf39f9665c7a0c",
                    "name": "Yonghui Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T01:21:38.000Z",
            "submittedOnDailyAt": "2025-06-19T03:43:49.620Z",
            "title": "Truncated Proximal Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
            "upvotes": 5,
            "discussionId": "68539c6199bf39f9665c7a0d",
            "ai_summary": "T-PPO, an extension of PPO, improves training efficiency for Large Language Models by optimizing policy updates and utilizing hardware resources more effectively.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "chains-of-thought (CoT)",
                "reinforcement learning (RL)",
                "Proximal Policy Optimization (PPO)",
                "Truncated Proximal Policy Optimization (T-PPO)",
                "Extended Generalized Advantage Estimation (EGAE)",
                "advantage estimation",
                "policy and value models",
                "independent optimization",
                "prompt and truncated tokens",
                "AIME 2024",
                "base model"
            ]
        },
        "publishedAt": "2025-06-17T21:21:38.000Z",
        "title": "Truncated Proximal Policy Optimization",
        "summary": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated\nexceptional reasoning capabilities across scientific and professional tasks by\ngenerating long chains-of-thought (CoT). As a crucial component for developing\nthese reasoning models, reinforcement learning (RL), exemplified by Proximal\nPolicy Optimization (PPO) and its variants, allows models to learn through\ntrial and error. However, PPO can be time-consuming due to its inherent\non-policy nature, which is further exacerbated by increasing response lengths.\nIn this work, we propose Truncated Proximal Policy Optimization (T-PPO), a\nnovel extension to PPO that improves training efficiency by streamlining policy\nupdate and length-restricted response generation. T-PPO mitigates the issue of\nlow hardware utilization, an inherent drawback of fully synchronized\nlong-generation procedures, where resources often sit idle during the waiting\nperiods for complete rollouts. Our contributions are two-folds. First, we\npropose Extended Generalized Advantage Estimation (EGAE) for advantage\nestimation derived from incomplete responses while maintaining the integrity of\npolicy learning. Second, we devise a computationally optimized mechanism that\nallows for the independent optimization of the policy and value models. By\nselectively filtering prompt and truncated tokens, this mechanism reduces\nredundant computations and accelerates the training process without sacrificing\nconvergence performance. We demonstrate the effectiveness and efficacy of T-PPO\non AIME 2024 with a 32B base model. The experimental results show that T-PPO\nimproves the training efficiency of reasoning LLMs by up to 2.5x and\noutperforms its existing competitors.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15050.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 88
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.14842",
            "authors": [
                {
                    "_id": "6854007941586a7b0193a0f9",
                    "user": {
                        "_id": "64ba4e0bb14fad24e03d4afc",
                        "avatarUrl": "/avatars/65e402f5082c44ba7df2c3ed627930ce.svg",
                        "isPro": false,
                        "fullname": "Lukas Schießer",
                        "user": "lschiesser",
                        "type": "user"
                    },
                    "name": "Lukas Schiesser",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-19T12:20:53.136Z",
                    "hidden": false
                },
                {
                    "_id": "6854007941586a7b0193a0fa",
                    "user": {
                        "_id": "65d65a40531e0bc924f0b1a3",
                        "avatarUrl": "/avatars/ad074a453243f074fe49690f9b800dd9.svg",
                        "isPro": false,
                        "fullname": "Cornelius Wolff",
                        "user": "cwolff",
                        "type": "user"
                    },
                    "name": "Cornelius Wolff",
                    "status": "extracted_pending",
                    "statusLastChangedAt": "2025-06-19T12:20:09.933Z",
                    "hidden": false
                },
                {
                    "_id": "6854007941586a7b0193a0fb",
                    "user": {
                        "_id": "67851a64806abc109d1a3764",
                        "avatarUrl": "/avatars/0eb0056334f5b496a31b8085c26d3036.svg",
                        "isPro": false,
                        "fullname": "Sophie Haas",
                        "user": "sohaas",
                        "type": "user"
                    },
                    "name": "Sophie Haas",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-19T12:30:36.214Z",
                    "hidden": false
                },
                {
                    "_id": "6854007941586a7b0193a0fc",
                    "user": {
                        "_id": "655b6a9dc69231073903567c",
                        "avatarUrl": "/avatars/8b46f501d5877e463c3c7a1b2cd08126.svg",
                        "isPro": false,
                        "fullname": "Simon Pukrop",
                        "user": "SimonPukrop",
                        "type": "user"
                    },
                    "name": "Simon Pukrop",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-19T12:20:54.777Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65d65a40531e0bc924f0b1a3/B5P2aDCIPAsrjaTXjiZ9F.png"
            ],
            "publishedAt": "2025-06-16T08:57:03.000Z",
            "submittedOnDailyAt": "2025-06-19T11:10:49.989Z",
            "title": "PictSure: Pretraining Embeddings Matters for In-Context Learning Image\n  Classifiers",
            "submittedOnDailyBy": {
                "_id": "65d65a40531e0bc924f0b1a3",
                "avatarUrl": "/avatars/ad074a453243f074fe49690f9b800dd9.svg",
                "isPro": false,
                "fullname": "Cornelius Wolff",
                "user": "cwolff",
                "type": "user"
            },
            "summary": "Building image classification models remains cumbersome in data-scarce\ndomains, where collecting large labeled datasets is impractical. In-context\nlearning (ICL) has emerged as a promising paradigm for few-shot image\nclassification (FSIC), enabling models to generalize across domains without\ngradient-based adaptation. However, prior work has largely overlooked a\ncritical component of ICL-based FSIC pipelines: the role of image embeddings.\nIn this work, we present PictSure, an ICL framework that places the embedding\nmodel -- its architecture, pretraining, and training dynamics -- at the center\nof analysis. We systematically examine the effects of different visual encoder\ntypes, pretraining objectives, and fine-tuning strategies on downstream FSIC\nperformance. Our experiments show that the training success and the\nout-of-domain performance are highly dependent on how the embedding models are\npretrained. Consequently, PictSure manages to outperform existing ICL-based\nFSIC models on out-of-domain benchmarks that differ significantly from the\ntraining distribution, while maintaining comparable results on in-domain tasks.\nCode can be found at https://github.com/PictSure/pictsure-library.",
            "upvotes": 5,
            "discussionId": "6854007941586a7b0193a0fd",
            "githubRepo": "https://github.com/PictSure/pictsure-library",
            "ai_summary": "PictSure is an in-context learning framework that enhances few-shot image classification by optimizing embedding models' architecture, pretraining, and fine-tuning strategies to improve out-of-domain performance.",
            "ai_keywords": [
                "in-context learning",
                "few-shot image classification",
                "embedding models",
                "visual encoder types",
                "pretraining objectives",
                "fine-tuning strategies"
            ]
        },
        "publishedAt": "2025-06-16T04:57:03.000Z",
        "title": "PictSure: Pretraining Embeddings Matters for In-Context Learning Image\n  Classifiers",
        "summary": "Building image classification models remains cumbersome in data-scarce\ndomains, where collecting large labeled datasets is impractical. In-context\nlearning (ICL) has emerged as a promising paradigm for few-shot image\nclassification (FSIC), enabling models to generalize across domains without\ngradient-based adaptation. However, prior work has largely overlooked a\ncritical component of ICL-based FSIC pipelines: the role of image embeddings.\nIn this work, we present PictSure, an ICL framework that places the embedding\nmodel -- its architecture, pretraining, and training dynamics -- at the center\nof analysis. We systematically examine the effects of different visual encoder\ntypes, pretraining objectives, and fine-tuning strategies on downstream FSIC\nperformance. Our experiments show that the training success and the\nout-of-domain performance are highly dependent on how the embedding models are\npretrained. Consequently, PictSure manages to outperform existing ICL-based\nFSIC models on out-of-domain benchmarks that differ significantly from the\ntraining distribution, while maintaining comparable results on in-domain tasks.\nCode can be found at https://github.com/PictSure/pictsure-library.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65d65a40531e0bc924f0b1a3/B5P2aDCIPAsrjaTXjiZ9F.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14842.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65d65a40531e0bc924f0b1a3",
            "avatarUrl": "/avatars/ad074a453243f074fe49690f9b800dd9.svg",
            "fullname": "Cornelius Wolff",
            "name": "cwolff",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14824",
            "authors": [
                {
                    "_id": "6853c3af99bf39f9665c7a89",
                    "user": {
                        "_id": "62cecf9c1415317d1fbf6cfe",
                        "avatarUrl": "/avatars/d30630ad96bcec1349728ba39476847a.svg",
                        "isPro": false,
                        "fullname": "Yao Zhang",
                        "user": "ZYao720",
                        "type": "user"
                    },
                    "name": "Yao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-19T15:11:24.084Z",
                    "hidden": false
                },
                {
                    "_id": "6853c3af99bf39f9665c7a8a",
                    "name": "Hewei Gao",
                    "hidden": false
                },
                {
                    "_id": "6853c3af99bf39f9665c7a8b",
                    "name": "Haokun Chen",
                    "hidden": false
                },
                {
                    "_id": "6853c3af99bf39f9665c7a8c",
                    "name": "Weiguo Li",
                    "hidden": false
                },
                {
                    "_id": "6853c3af99bf39f9665c7a8d",
                    "name": "Yunpu Ma",
                    "hidden": false
                },
                {
                    "_id": "6853c3af99bf39f9665c7a8e",
                    "name": "Volker Tresp",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T17:50:50.000Z",
            "submittedOnDailyAt": "2025-06-19T06:31:06.085Z",
            "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "648cbea3dee03837c823cbf2",
                "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
                "isPro": false,
                "fullname": "Shuo Chen",
                "user": "ShuoChen99",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.",
            "upvotes": 5,
            "discussionId": "6853c3af99bf39f9665c7a8f",
            "ai_summary": "FedNano is a federated learning framework that centralizes large language models on servers and uses NanoEdge modules for client-specific adaptation, addressing scalability and privacy issues.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Federated Learning",
                "NanoEdge",
                "modality-specific encoders",
                "connectors",
                "NanoAdapters",
                "low-rank adaptation",
                "client-specific adaptation",
                "compact NanoAdapter updates",
                "decentralized multimodal AI systems"
            ]
        },
        "publishedAt": "2025-06-12T13:50:50.000Z",
        "title": "FedNano: Toward Lightweight Federated Tuning for Pretrained Multimodal\n  Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) excel in tasks like multimodal\nreasoning and cross-modal retrieval but face deployment challenges in\nreal-world scenarios due to distributed multimodal data and strict privacy\nrequirements. Federated Learning (FL) offers a solution by enabling\ncollaborative model training without centralizing data. However, realizing FL\nfor MLLMs presents significant challenges, including high computational\ndemands, limited client capacity, substantial communication costs, and\nheterogeneous client data. Existing FL methods assume client-side deployment of\nfull models, an assumption that breaks down for large-scale MLLMs due to their\nmassive size and communication demands. To address these limitations, we\npropose FedNano, the first FL framework that centralizes the LLM on the server\nwhile introducing NanoEdge, a lightweight module for client-specific\nadaptation. NanoEdge employs modality-specific encoders, connectors, and\ntrainable NanoAdapters with low-rank adaptation. This design eliminates the\nneed to deploy LLM on clients, reducing client-side storage by 95%, and\nlimiting communication overhead to only 0.01% of the model parameters. By\ntransmitting only compact NanoAdapter updates, FedNano handles heterogeneous\nclient data and resource constraints while preserving privacy. Experiments\ndemonstrate that FedNano outperforms prior FL baselines, bridging the gap\nbetween MLLM scale and FL feasibility, and enabling scalable, decentralized\nmultimodal AI systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14824.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "648cbea3dee03837c823cbf2",
            "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
            "fullname": "Shuo Chen",
            "name": "ShuoChen99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.06279",
            "authors": [
                {
                    "_id": "6850e0285e07650ecce890f3",
                    "user": {
                        "_id": "637f22d27119bd030dfd4af8",
                        "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
                        "isPro": false,
                        "fullname": "Shi Liu",
                        "user": "CLLBJ16",
                        "type": "user"
                    },
                    "name": "Shi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-18T12:16:48.071Z",
                    "hidden": false
                },
                {
                    "_id": "6850e0285e07650ecce890f4",
                    "user": {
                        "_id": "63e4562f9db5da2dc1f3b520",
                        "avatarUrl": "/avatars/f4eecf1396b05e1c72436e7026d85cef.svg",
                        "isPro": false,
                        "fullname": "Weijie Su",
                        "user": "jackroos",
                        "type": "user"
                    },
                    "name": "Weijie Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-19T09:10:33.557Z",
                    "hidden": false
                },
                {
                    "_id": "6850e0285e07650ecce890f5",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "6850e0285e07650ecce890f6",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "6850e0285e07650ecce890f7",
                    "name": "Jifeng Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-06T17:59:06.000Z",
            "submittedOnDailyAt": "2025-06-19T00:24:25.743Z",
            "title": "CoMemo: LVLMs Need Image Context with Image Memory",
            "submittedOnDailyBy": {
                "_id": "637f22d27119bd030dfd4af8",
                "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
                "isPro": false,
                "fullname": "Shi Liu",
                "user": "CLLBJ16",
                "type": "user"
            },
            "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
            "upvotes": 5,
            "discussionId": "6850e0295e07650ecce890f8",
            "projectPage": "https://lalbj.github.io/projects/CoMemo/",
            "githubRepo": "https://github.com/LALBJ/CoMemo",
            "ai_summary": "CoMemo addresses visual information neglect and spatial awareness in multimodal processing by using a dual-path architecture and a novel positional encoding mechanism.",
            "ai_keywords": [
                "Large Vision-Language Models",
                "Large Language Models",
                "multimodal processing",
                "bimodal distribution",
                "attention allocation",
                "middle visual content",
                "positional encoding",
                "visual processing",
                "image Memory path",
                "RoPE-DHR",
                "positional aggregation",
                "2D structural relationships",
                "spatial awareness",
                "remote decay",
                "long-context comprehension",
                "multi-image reasoning",
                "visual question answering"
            ]
        },
        "publishedAt": "2025-06-06T13:59:06.000Z",
        "title": "CoMemo: LVLMs Need Image Context with Image Memory",
        "summary": "Recent advancements in Large Vision-Language Models built upon Large Language\nModels have established aligning visual features with LLM representations as\nthe dominant paradigm. However, inherited LLM architectural designs introduce\nsuboptimal characteristics for multimodal processing. First, LVLMs exhibit a\nbimodal distribution in attention allocation, leading to the progressive\nneglect of middle visual content as context expands. Second, conventional\npositional encoding schemes fail to preserve vital 2D structural relationships\nwhen processing dynamic high-resolution images. To address these limitations,\nwe propose CoMemo - a dual-path architecture that combines a Context image path\nwith an image Memory path for visual processing, effectively alleviating visual\ninformation neglect. Additionally, we introduce RoPE-DHR, a novel positional\nencoding mechanism that employs thumbnail-based positional aggregation to\nmaintain 2D spatial awareness while mitigating remote decay in extended\nsequences. Evaluations across seven benchmarks,including long-context\ncomprehension, multi-image reasoning, and visual question answering,\ndemonstrate CoMemo's superior performance compared to conventional LVLM\narchitectures. Project page is available at\nhttps://lalbj.github.io/projects/CoMemo/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.06279.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637f22d27119bd030dfd4af8",
            "avatarUrl": "/avatars/55c468c40ad3bd3218d086759fb1be3c.svg",
            "fullname": "Shi Liu",
            "name": "CLLBJ16",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14435",
            "authors": [
                {
                    "_id": "68537b2a99bf39f9665c7990",
                    "name": "Hongyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68537b2a99bf39f9665c7991",
                    "name": "Jiayu Xu",
                    "hidden": false
                },
                {
                    "_id": "68537b2a99bf39f9665c7992",
                    "name": "Ruiping Wang",
                    "hidden": false
                },
                {
                    "_id": "68537b2a99bf39f9665c7993",
                    "name": "Yan Feng",
                    "hidden": false
                },
                {
                    "_id": "68537b2a99bf39f9665c7994",
                    "name": "Yitao Zhai",
                    "hidden": false
                },
                {
                    "_id": "68537b2a99bf39f9665c7995",
                    "name": "Peng Pei",
                    "hidden": false
                },
                {
                    "_id": "68537b2a99bf39f9665c7996",
                    "name": "Xunliang Cai",
                    "hidden": false
                },
                {
                    "_id": "68537b2a99bf39f9665c7997",
                    "name": "Xilin Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T11:53:49.000Z",
            "submittedOnDailyAt": "2025-06-19T01:22:11.793Z",
            "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models",
            "submittedOnDailyBy": {
                "_id": "63f71771d36951307fcb4dcd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
                "isPro": false,
                "fullname": "Hongyu Wang",
                "user": "hongyuw",
                "type": "user"
            },
            "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.",
            "upvotes": 4,
            "discussionId": "68537b2b99bf39f9665c7998",
            "ai_summary": "MoTE, a scalable and memory-efficient method, improves Mixture-of-Experts models using low-precision ternary experts, enhancing performance and reducing memory footprint for deployment on edge devices.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "MoEs",
                "sparse up-cycling",
                "low-precision",
                "ternary experts",
                "shared expert",
                "FFN",
                "pre-trained",
                "post-training quantization",
                "memory-constrained",
                "end tasks"
            ]
        },
        "publishedAt": "2025-06-17T07:53:49.000Z",
        "title": "MoTE: Mixture of Ternary Experts for Memory-efficient Large Multimodal\n  Models",
        "summary": "Large multimodal Mixture-of-Experts (MoEs) effectively scale the model size\nto boost performance while maintaining fixed active parameters. However,\nprevious works primarily utilized full-precision experts during sparse\nup-cycling. Despite they show superior performance on end tasks, the large\namount of experts introduces higher memory footprint, which poses significant\nchallenges for the deployment on edge devices. In this work, we propose MoTE, a\nscalable and memory-efficient approach to train Mixture-of-Ternary-Experts\nmodels from dense checkpoint. Instead of training fewer high-precision experts,\nwe propose to train more low-precision experts during up-cycling. Specifically,\nwe use the pre-trained FFN as a shared expert and train ternary routed experts\nwith parameters in {-1, 0, 1}. Extensive experiments show that our approach has\npromising scaling trend along model size. MoTE achieves comparable performance\nto full-precision baseline MoE-LLaVA while offering lower memory footprint.\nFurthermore, our approach is compatible with post-training quantization methods\nand the advantage further amplifies when memory-constraint goes lower. Given\nthe same amount of expert memory footprint of 3.4GB and combined with\npost-training quantization, MoTE outperforms MoE-LLaVA by a gain of 4.3%\naverage accuracy on end tasks, demonstrating its effectiveness and potential\nfor memory-constrained devices.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14435.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f71771d36951307fcb4dcd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f71771d36951307fcb4dcd/1c-GSQmSSuDXyVWjxZFy_.jpeg",
            "fullname": "Hongyu Wang",
            "name": "hongyuw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.14315",
            "authors": [
                {
                    "_id": "6853c08e99bf39f9665c7a71",
                    "user": {
                        "_id": "662e1323a314b134a229cc2f",
                        "avatarUrl": "/avatars/1eea9ab5edeb3d0e2db33dbfa15bfbd1.svg",
                        "isPro": false,
                        "fullname": "Jinyan Yuan",
                        "user": "yjyyy",
                        "type": "user"
                    },
                    "name": "Jinyan Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-19T09:10:07.549Z",
                    "hidden": false
                },
                {
                    "_id": "6853c08e99bf39f9665c7a72",
                    "user": {
                        "_id": "63d748ff6f49aa82306b7e48",
                        "avatarUrl": "/avatars/9f0b8b8a09b14d76e52ed1bd312e6b63.svg",
                        "isPro": false,
                        "fullname": "BB Yang",
                        "user": "ybbbbt",
                        "type": "user"
                    },
                    "name": "Bangbang Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-19T15:11:28.147Z",
                    "hidden": false
                },
                {
                    "_id": "6853c08e99bf39f9665c7a73",
                    "name": "Keke Wang",
                    "hidden": false
                },
                {
                    "_id": "6853c08e99bf39f9665c7a74",
                    "user": {
                        "_id": "654866e8cd0a5621395f8287",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654866e8cd0a5621395f8287/4Bccwd1ehn-Ee4T1rId5S.jpeg",
                        "isPro": true,
                        "fullname": "Panwang Pan",
                        "user": "paulpanwang",
                        "type": "user"
                    },
                    "name": "Panwang Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-19T15:11:25.970Z",
                    "hidden": false
                },
                {
                    "_id": "6853c08e99bf39f9665c7a75",
                    "name": "Lin Ma",
                    "hidden": false
                },
                {
                    "_id": "6853c08e99bf39f9665c7a76",
                    "name": "Xuehai Zhang",
                    "hidden": false
                },
                {
                    "_id": "6853c08e99bf39f9665c7a77",
                    "name": "Xiao Liu",
                    "hidden": false
                },
                {
                    "_id": "6853c08e99bf39f9665c7a78",
                    "name": "Zhaopeng Cui",
                    "hidden": false
                },
                {
                    "_id": "6853c08e99bf39f9665c7a79",
                    "name": "Yuewen Ma",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/662e1323a314b134a229cc2f/2DwGl0J7c6yv1H-KNCGim.mp4"
            ],
            "publishedAt": "2025-06-17T08:50:05.000Z",
            "submittedOnDailyAt": "2025-06-19T10:17:30.732Z",
            "title": "ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured\n  Proxies",
            "submittedOnDailyBy": {
                "_id": "662e1323a314b134a229cc2f",
                "avatarUrl": "/avatars/1eea9ab5edeb3d0e2db33dbfa15bfbd1.svg",
                "isPro": false,
                "fullname": "Jinyan Yuan",
                "user": "yjyyy",
                "type": "user"
            },
            "summary": "Automatic creation of 3D scenes for immersive VR presence has been a\nsignificant research focus for decades. However, existing methods often rely on\neither high-poly mesh modeling with post-hoc simplification or massive 3D\nGaussians, resulting in a complex pipeline or limited visual realism. In this\npaper, we demonstrate that such exhaustive modeling is unnecessary for\nachieving compelling immersive experience. We introduce ImmerseGen, a novel\nagent-guided framework for compact and photorealistic world modeling.\nImmerseGen represents scenes as hierarchical compositions of lightweight\ngeometric proxies, i.e., simplified terrain and billboard meshes, and generates\nphotorealistic appearance by synthesizing RGBA textures onto these proxies.\nSpecifically, we propose terrain-conditioned texturing for user-centric base\nworld synthesis, and RGBA asset texturing for midground and foreground scenery.\nThis reformulation offers several advantages: (i) it simplifies modeling by\nenabling agents to guide generative models in producing coherent textures that\nintegrate seamlessly with the scene; (ii) it bypasses complex geometry creation\nand decimation by directly synthesizing photorealistic textures on proxies,\npreserving visual quality without degradation; (iii) it enables compact\nrepresentations suitable for real-time rendering on mobile VR headsets. To\nautomate scene creation from text prompts, we introduce VLM-based modeling\nagents enhanced with semantic grid-based analysis for improved spatial\nreasoning and accurate asset placement. ImmerseGen further enriches scenes with\ndynamic effects and ambient audio to support multisensory immersion.\nExperiments on scene generation and live VR showcases demonstrate that\nImmerseGen achieves superior photorealism, spatial coherence and rendering\nefficiency compared to prior methods. Project webpage:\nhttps://immersegen.github.io.",
            "upvotes": 4,
            "discussionId": "6853c08e99bf39f9665c7a7a",
            "ai_summary": "An agent-guided framework generates photorealistic 3D scenes for VR by synthesizing textures onto lightweight geometric proxies, enabling real-time rendering and superior visual quality.",
            "ai_keywords": [
                "agent-guided framework",
                "photorealistic world modeling",
                "hierarchical compositions",
                "lightweight geometric proxies",
                "simplified terrain",
                "billboard meshes",
                "terrain-conditioned texturing",
                "RGBA asset texturing",
                "generative models",
                "coherent textures",
                "semantic grid-based analysis",
                "VLM-based modeling agents",
                "dynamic effects",
                "ambient audio",
                "spatial reasoning",
                "asset placement",
                "photorealism",
                "spatial coherence",
                "rendering efficiency"
            ]
        },
        "publishedAt": "2025-06-17T04:50:05.000Z",
        "title": "ImmerseGen: Agent-Guided Immersive World Generation with Alpha-Textured\n  Proxies",
        "summary": "Automatic creation of 3D scenes for immersive VR presence has been a\nsignificant research focus for decades. However, existing methods often rely on\neither high-poly mesh modeling with post-hoc simplification or massive 3D\nGaussians, resulting in a complex pipeline or limited visual realism. In this\npaper, we demonstrate that such exhaustive modeling is unnecessary for\nachieving compelling immersive experience. We introduce ImmerseGen, a novel\nagent-guided framework for compact and photorealistic world modeling.\nImmerseGen represents scenes as hierarchical compositions of lightweight\ngeometric proxies, i.e., simplified terrain and billboard meshes, and generates\nphotorealistic appearance by synthesizing RGBA textures onto these proxies.\nSpecifically, we propose terrain-conditioned texturing for user-centric base\nworld synthesis, and RGBA asset texturing for midground and foreground scenery.\nThis reformulation offers several advantages: (i) it simplifies modeling by\nenabling agents to guide generative models in producing coherent textures that\nintegrate seamlessly with the scene; (ii) it bypasses complex geometry creation\nand decimation by directly synthesizing photorealistic textures on proxies,\npreserving visual quality without degradation; (iii) it enables compact\nrepresentations suitable for real-time rendering on mobile VR headsets. To\nautomate scene creation from text prompts, we introduce VLM-based modeling\nagents enhanced with semantic grid-based analysis for improved spatial\nreasoning and accurate asset placement. ImmerseGen further enriches scenes with\ndynamic effects and ambient audio to support multisensory immersion.\nExperiments on scene generation and live VR showcases demonstrate that\nImmerseGen achieves superior photorealism, spatial coherence and rendering\nefficiency compared to prior methods. Project webpage:\nhttps://immersegen.github.io.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/662e1323a314b134a229cc2f/2DwGl0J7c6yv1H-KNCGim.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14315.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662e1323a314b134a229cc2f",
            "avatarUrl": "/avatars/1eea9ab5edeb3d0e2db33dbfa15bfbd1.svg",
            "fullname": "Jinyan Yuan",
            "name": "yjyyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14866",
            "authors": [
                {
                    "_id": "6853db4199bf39f9665c7ae5",
                    "name": "Thomas Kuntz",
                    "hidden": false
                },
                {
                    "_id": "6853db4199bf39f9665c7ae6",
                    "name": "Agatha Duzan",
                    "hidden": false
                },
                {
                    "_id": "6853db4199bf39f9665c7ae7",
                    "name": "Hao Zhao",
                    "hidden": false
                },
                {
                    "_id": "6853db4199bf39f9665c7ae8",
                    "name": "Francesco Croce",
                    "hidden": false
                },
                {
                    "_id": "6853db4199bf39f9665c7ae9",
                    "name": "Zico Kolter",
                    "hidden": false
                },
                {
                    "_id": "6853db4199bf39f9665c7aea",
                    "name": "Nicolas Flammarion",
                    "hidden": false
                },
                {
                    "_id": "6853db4199bf39f9665c7aeb",
                    "name": "Maksym Andriushchenko",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T17:59:31.000Z",
            "submittedOnDailyAt": "2025-06-19T08:15:06.478Z",
            "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
            "submittedOnDailyBy": {
                "_id": "64c225f0129617dbaba5ae88",
                "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
                "isPro": false,
                "fullname": "Maksym Andriushchenko",
                "user": "MaksymAndriushchenko",
                "type": "user"
            },
            "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
            "upvotes": 3,
            "discussionId": "6853db4199bf39f9665c7aec",
            "githubRepo": "https://github.com/tml-epfl/os-harm",
            "ai_summary": "A new benchmark called OS-Harm measures the safety of computer use agents interacting with GUIs, evaluating their susceptibility to misuse, prompt injection attacks, and misbehavior across various safety violations and applications.",
            "ai_keywords": [
                "LLM-based agents",
                "OS-Harm",
                "OSWorld environment",
                "deliberate user misuse",
                "prompt injection attacks",
                "model misbehavior",
                "harassment",
                "copyright infringement",
                "disinformation",
                "data exfiltration",
                "automated judge",
                "F1 score",
                "GUI"
            ]
        },
        "publishedAt": "2025-06-17T13:59:31.000Z",
        "title": "OS-Harm: A Benchmark for Measuring Safety of Computer Use Agents",
        "summary": "Computer use agents are LLM-based agents that can directly interact with a\ngraphical user interface, by processing screenshots or accessibility trees.\nWhile these systems are gaining popularity, their safety has been largely\noverlooked, despite the fact that evaluating and understanding their potential\nfor harmful behavior is essential for widespread adoption. To address this gap,\nwe introduce OS-Harm, a new benchmark for measuring safety of computer use\nagents. OS-Harm is built on top of the OSWorld environment and aims to test\nmodels across three categories of harm: deliberate user misuse, prompt\ninjection attacks, and model misbehavior. To cover these cases, we create 150\ntasks that span several types of safety violations (harassment, copyright\ninfringement, disinformation, data exfiltration, etc.) and require the agent to\ninteract with a variety of OS applications (email client, code editor, browser,\netc.). Moreover, we propose an automated judge to evaluate both accuracy and\nsafety of agents that achieves high agreement with human annotations (0.76 and\n0.79 F1 score). We evaluate computer use agents based on a range of frontier\nmodels - such as o4-mini, Claude 3.7 Sonnet, Gemini 2.5 Pro - and provide\ninsights into their safety. In particular, all models tend to directly comply\nwith many deliberate misuse queries, are relatively vulnerable to static prompt\ninjections, and occasionally perform unsafe actions. The OS-Harm benchmark is\navailable at https://github.com/tml-epfl/os-harm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14866.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64c225f0129617dbaba5ae88",
            "avatarUrl": "/avatars/b12db433cd6b37c8e5299e575bdf98f9.svg",
            "fullname": "Maksym Andriushchenko",
            "name": "MaksymAndriushchenko",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.11110",
            "authors": [
                {
                    "_id": "685197990164cd1316710352",
                    "user": {
                        "_id": "663826366169f2a25f10fada",
                        "avatarUrl": "/avatars/904d5090d66825458ccf7a5d90ecc51f.svg",
                        "isPro": false,
                        "fullname": "Jaeho Lee",
                        "user": "jlee24",
                        "type": "user"
                    },
                    "name": "Jaeho Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T21:13:00.114Z",
                    "hidden": false
                },
                {
                    "_id": "685197990164cd1316710353",
                    "user": {
                        "_id": "67196f2652c32a1397b6674b",
                        "avatarUrl": "/avatars/0115bee40d5fc80a1413ec9ae08f35f6.svg",
                        "isPro": false,
                        "fullname": "Atharv Chowdhary",
                        "user": "achowd32",
                        "type": "user"
                    },
                    "name": "Atharv Chowdhary",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-17T21:13:03.070Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-08T14:08:22.000Z",
            "submittedOnDailyAt": "2025-06-19T15:52:19.024Z",
            "title": "AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "67196f2652c32a1397b6674b",
                "avatarUrl": "/avatars/0115bee40d5fc80a1413ec9ae08f35f6.svg",
                "isPro": false,
                "fullname": "Atharv Chowdhary",
                "user": "achowd32",
                "type": "user"
            },
            "summary": "Recent benchmarks have probed factual consistency and rhetorical robustness\nin Large Language Models (LLMs). However, a knowledge gap exists regarding how\ndirectional framing of factually true statements influences model agreement, a\ncommon scenario for LLM users. AssertBench addresses this by sampling\nevidence-supported facts from FEVEROUS, a fact verification dataset. For each\n(evidence-backed) fact, we construct two framing prompts: one where the user\nclaims the statement is factually correct, and another where the user claims it\nis incorrect. We then record the model's agreement and reasoning. The desired\noutcome is that the model asserts itself, maintaining consistent truth\nevaluation across both framings, rather than switching its evaluation to agree\nwith the user. AssertBench isolates framing-induced variability from the\nmodel's underlying factual knowledge by stratifying results based on the\nmodel's accuracy on the same claims when presented neutrally. In doing so, this\nbenchmark aims to measure an LLM's ability to \"stick to its guns\" when\npresented with contradictory user assertions about the same fact. The complete\nsource code is available at https://github.com/achowd32/assert-bench.",
            "upvotes": 3,
            "discussionId": "685197990164cd1316710354",
            "githubRepo": "https://github.com/achowd32/assert-bench",
            "ai_summary": "AssertBench evaluates Large Language Models' ability to maintain consistent truth evaluation when faced with contradictory user assertions about factually true statements by analyzing framing-induced variability.",
            "ai_keywords": [
                "Large Language Models",
                "LLMS",
                "fact verification",
                "FEVEROUS",
                "framing",
                "model agreement",
                "reasoning",
                "neutrality",
                "truth evaluation",
                "contradictory assertions"
            ]
        },
        "publishedAt": "2025-06-08T10:08:22.000Z",
        "title": "AssertBench: A Benchmark for Evaluating Self-Assertion in Large Language\n  Models",
        "summary": "Recent benchmarks have probed factual consistency and rhetorical robustness\nin Large Language Models (LLMs). However, a knowledge gap exists regarding how\ndirectional framing of factually true statements influences model agreement, a\ncommon scenario for LLM users. AssertBench addresses this by sampling\nevidence-supported facts from FEVEROUS, a fact verification dataset. For each\n(evidence-backed) fact, we construct two framing prompts: one where the user\nclaims the statement is factually correct, and another where the user claims it\nis incorrect. We then record the model's agreement and reasoning. The desired\noutcome is that the model asserts itself, maintaining consistent truth\nevaluation across both framings, rather than switching its evaluation to agree\nwith the user. AssertBench isolates framing-induced variability from the\nmodel's underlying factual knowledge by stratifying results based on the\nmodel's accuracy on the same claims when presented neutrally. In doing so, this\nbenchmark aims to measure an LLM's ability to \"stick to its guns\" when\npresented with contradictory user assertions about the same fact. The complete\nsource code is available at https://github.com/achowd32/assert-bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.11110.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67196f2652c32a1397b6674b",
            "avatarUrl": "/avatars/0115bee40d5fc80a1413ec9ae08f35f6.svg",
            "fullname": "Atharv Chowdhary",
            "name": "achowd32",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14770",
            "authors": [
                {
                    "_id": "685427c0b80003dfb07701d0",
                    "name": "Zixuan Chen",
                    "hidden": false
                },
                {
                    "_id": "685427c0b80003dfb07701d1",
                    "name": "Mazeyu Ji",
                    "hidden": false
                },
                {
                    "_id": "685427c0b80003dfb07701d2",
                    "name": "Xuxin Cheng",
                    "hidden": false
                },
                {
                    "_id": "685427c0b80003dfb07701d3",
                    "name": "Xuanbin Peng",
                    "hidden": false
                },
                {
                    "_id": "685427c0b80003dfb07701d4",
                    "name": "Xue Bin Peng",
                    "hidden": false
                },
                {
                    "_id": "685427c0b80003dfb07701d5",
                    "name": "Xiaolong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T17:59:33.000Z",
            "submittedOnDailyAt": "2025-06-19T13:39:42.445Z",
            "title": "GMT: General Motion Tracking for Humanoid Whole-Body Control",
            "submittedOnDailyBy": {
                "_id": "64a02b4c1396c67ac07798bb",
                "avatarUrl": "/avatars/9b1b4319edbac5faeb7586a4933791d2.svg",
                "isPro": false,
                "fullname": "Eric Chen",
                "user": "zxuannn",
                "type": "user"
            },
            "summary": "The ability to track general whole-body motions in the real world is a useful\nway to build general-purpose humanoid robots. However, achieving this can be\nchallenging due to the temporal and kinematic diversity of the motions, the\npolicy's capability, and the difficulty of coordination of the upper and lower\nbodies. To address these issues, we propose GMT, a general and scalable\nmotion-tracking framework that trains a single unified policy to enable\nhumanoid robots to track diverse motions in the real world. GMT is built upon\ntwo core components: an Adaptive Sampling strategy and a Motion\nMixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically\nbalances easy and difficult motions during training. The MoE ensures better\nspecialization of different regions of the motion manifold. We show through\nextensive experiments in both simulation and the real world the effectiveness\nof GMT, achieving state-of-the-art performance across a broad spectrum of\nmotions using a unified general policy. Videos and additional information can\nbe found at https://gmt-humanoid.github.io.",
            "upvotes": 1,
            "discussionId": "685427c1b80003dfb07701d6",
            "ai_summary": "GMT, a unified motion-tracking framework, addresses challenges in tracking diverse humanoid robot motions through adaptive sampling and a motion mixture-of-experts architecture, achieving state-of-the-art performance.",
            "ai_keywords": [
                "GMT",
                "Adaptive Sampling",
                "Motion Mixture-of-Experts",
                "motion manifold"
            ]
        },
        "publishedAt": "2025-06-17T13:59:33.000Z",
        "title": "GMT: General Motion Tracking for Humanoid Whole-Body Control",
        "summary": "The ability to track general whole-body motions in the real world is a useful\nway to build general-purpose humanoid robots. However, achieving this can be\nchallenging due to the temporal and kinematic diversity of the motions, the\npolicy's capability, and the difficulty of coordination of the upper and lower\nbodies. To address these issues, we propose GMT, a general and scalable\nmotion-tracking framework that trains a single unified policy to enable\nhumanoid robots to track diverse motions in the real world. GMT is built upon\ntwo core components: an Adaptive Sampling strategy and a Motion\nMixture-of-Experts (MoE) architecture. The Adaptive Sampling automatically\nbalances easy and difficult motions during training. The MoE ensures better\nspecialization of different regions of the motion manifold. We show through\nextensive experiments in both simulation and the real world the effectiveness\nof GMT, achieving state-of-the-art performance across a broad spectrum of\nmotions using a unified general policy. Videos and additional information can\nbe found at https://gmt-humanoid.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14770.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a02b4c1396c67ac07798bb",
            "avatarUrl": "/avatars/9b1b4319edbac5faeb7586a4933791d2.svg",
            "fullname": "Eric Chen",
            "name": "zxuannn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.15682",
            "authors": [
                {
                    "_id": "68544c287bc8d012d4ca9902",
                    "name": "Anirud Aggarwal",
                    "hidden": false
                },
                {
                    "_id": "68544c287bc8d012d4ca9903",
                    "name": "Abhinav Shrivastava",
                    "hidden": false
                },
                {
                    "_id": "68544c287bc8d012d4ca9904",
                    "name": "Matthew Gwilliam",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T17:59:50.000Z",
            "submittedOnDailyAt": "2025-06-19T16:14:08.797Z",
            "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
            "submittedOnDailyBy": {
                "_id": "6250c5008c2cdfaca2a681d9",
                "avatarUrl": "/avatars/e4f168396c151c1f8077768430bfc673.svg",
                "isPro": false,
                "fullname": "Matt Gwilliam",
                "user": "mgwillia",
                "type": "user"
            },
            "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
            "upvotes": 0,
            "discussionId": "68544c287bc8d012d4ca9905",
            "ai_summary": "ECAD, a genetic algorithm, optimizes caching schedules for diffusion models, enhancing inference speed while maintaining quality across various benchmarks.",
            "ai_keywords": [
                "diffusion-based image generation",
                "diffusion transformers",
                "evolutionary caching",
                "genetic algorithm",
                "caching schedules",
                "inference speedups",
                "quality-latency trade-off",
                "calibration prompts",
                "PixArt-alpha",
                "PixArt-Sigma",
                "FLUX-1.dev",
                "FID",
                "CLIP",
                "Image Reward",
                "COCO",
                "MJHQ-30k",
                "PartiPrompts"
            ]
        },
        "publishedAt": "2025-06-18T13:59:50.000Z",
        "title": "Evolutionary Caching to Accelerate Your Off-the-Shelf Diffusion Model",
        "summary": "Diffusion-based image generation models excel at producing high-quality\nsynthetic content, but suffer from slow and computationally expensive\ninference. Prior work has attempted to mitigate this by caching and reusing\nfeatures within diffusion transformers across inference steps. These methods,\nhowever, often rely on rigid heuristics that result in limited acceleration or\npoor generalization across architectures. We propose Evolutionary Caching to\nAccelerate Diffusion models (ECAD), a genetic algorithm that learns efficient,\nper-model, caching schedules forming a Pareto frontier, using only a small set\nof calibration prompts. ECAD requires no modifications to network parameters or\nreference images. It offers significant inference speedups, enables\nfine-grained control over the quality-latency trade-off, and adapts seamlessly\nto different diffusion models. Notably, ECAD's learned schedules can generalize\neffectively to resolutions and model variants not seen during calibration. We\nevaluate ECAD on PixArt-alpha, PixArt-Sigma, and FLUX-1.dev using multiple\nmetrics (FID, CLIP, Image Reward) across diverse benchmarks (COCO, MJHQ-30k,\nPartiPrompts), demonstrating consistent improvements over previous approaches.\nOn PixArt-alpha, ECAD identifies a schedule that outperforms the previous\nstate-of-the-art method by 4.47 COCO FID while increasing inference speedup\nfrom 2.35x to 2.58x. Our results establish ECAD as a scalable and generalizable\napproach for accelerating diffusion inference. Our project website is available\nat https://aniaggarwal.github.io/ecad and our code is available at\nhttps://github.com/aniaggarwal/ecad.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15682.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6250c5008c2cdfaca2a681d9",
            "avatarUrl": "/avatars/e4f168396c151c1f8077768430bfc673.svg",
            "fullname": "Matt Gwilliam",
            "name": "mgwillia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]