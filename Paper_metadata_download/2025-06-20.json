[
    {
        "paper": {
            "id": "2506.14965",
            "authors": [
                {
                    "_id": "68538be099bf39f9665c79b9",
                    "name": "Zhoujun Cheng",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79ba",
                    "name": "Shibo Hao",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79bb",
                    "user": {
                        "_id": "629e2bcc46b4826be2c57fe3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/629e2bcc46b4826be2c57fe3/41BiA52XlZi31ABsljFiq.jpeg",
                        "isPro": false,
                        "fullname": "Tianyang Liu",
                        "user": "tianyang",
                        "type": "user"
                    },
                    "name": "Tianyang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T12:19:48.511Z",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79bc",
                    "user": {
                        "_id": "628f6e5ab90dde28ef57d293",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f6e5ab90dde28ef57d293/AxNzR2nvrND6Rf3RPkYMk.jpeg",
                        "isPro": false,
                        "fullname": "Fan Zhou",
                        "user": "koalazf99",
                        "type": "user"
                    },
                    "name": "Fan Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T12:19:46.579Z",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79bd",
                    "name": "Yutao Xie",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79be",
                    "user": {
                        "_id": "64c8b2c5c547ed5243e14a6e",
                        "avatarUrl": "/avatars/96d4a9010f96001c8cff235915926390.svg",
                        "isPro": false,
                        "fullname": "Feng Yao",
                        "user": "fengyao1909",
                        "type": "user"
                    },
                    "name": "Feng Yao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T12:19:50.740Z",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79bf",
                    "name": "Yuexin Bian",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c0",
                    "name": "Yonghao Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c1",
                    "name": "Nilabjo Dey",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c2",
                    "name": "Yuheng Zha",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c3",
                    "name": "Yi Gu",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c4",
                    "name": "Kun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c5",
                    "name": "Yuqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c6",
                    "name": "Yuan Li",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c7",
                    "name": "Richard Fan",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c8",
                    "name": "Jianshu She",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79c9",
                    "name": "Chengqian Gao",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79ca",
                    "name": "Abulhair Saparov",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79cb",
                    "name": "Haonan Li",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79cc",
                    "name": "Taylor W. Killian",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79cd",
                    "name": "Mikhail Yurochkin",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79ce",
                    "name": "Zhengzhong Liu",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79cf",
                    "name": "Eric P. Xing",
                    "hidden": false
                },
                {
                    "_id": "68538be099bf39f9665c79d0",
                    "name": "Zhiting Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T20:24:00.000Z",
            "submittedOnDailyAt": "2025-06-20T06:25:47.447Z",
            "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
            "submittedOnDailyBy": {
                "_id": "6083902e1e36b13a64497d91",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
                "isPro": false,
                "fullname": "cheng",
                "user": "zhoujun",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
            "upvotes": 27,
            "discussionId": "68538be099bf39f9665c79d1",
            "projectPage": "https://guru-reasoning.github.io/",
            "githubRepo": "https://github.com/LLM360/Reasoning360",
            "ai_summary": "Guru, a diverse RL reasoning corpus, highlights domain-specific training needs and demonstrates improved performance in complex tasks for RL-enhanced LLMs.",
            "ai_keywords": [
                "reinforcement learning",
                "large language model",
                "RL reasoning",
                "curated RL reasoning corpus",
                "domain-specific reward design",
                "dereplication",
                "filtering",
                "cross-domain RL training",
                "in-domain training",
                "Guru-7B",
                "Guru-32B",
                "Pass@k performance"
            ]
        },
        "publishedAt": "2025-06-17T16:24:00.000Z",
        "title": "Revisiting Reinforcement Learning for LLM Reasoning from A Cross-Domain\n  Perspective",
        "summary": "Reinforcement learning (RL) has emerged as a promising approach to improve\nlarge language model (LLM) reasoning, yet most open efforts focus narrowly on\nmath and code, limiting our understanding of its broader applicability to\ngeneral reasoning. A key challenge lies in the lack of reliable, scalable RL\nreward signals across diverse reasoning domains. We introduce Guru, a curated\nRL reasoning corpus of 92K verifiable examples spanning six reasoning\ndomains--Math, Code, Science, Logic, Simulation, and Tabular--each built\nthrough domain-specific reward design, deduplication, and filtering to ensure\nreliability and effectiveness for RL training. Based on Guru, we systematically\nrevisit established findings in RL for LLM reasoning and observe significant\nvariation across domains. For example, while prior work suggests that RL\nprimarily elicits existing knowledge from pretrained models, our results reveal\na more nuanced pattern: domains frequently seen during pretraining (Math, Code,\nScience) easily benefit from cross-domain RL training, while domains with\nlimited pretraining exposure (Logic, Simulation, and Tabular) require in-domain\ntraining to achieve meaningful performance gains, suggesting that RL is likely\nto facilitate genuine skill acquisition. Finally, we present Guru-7B and\nGuru-32B, two models that achieve state-of-the-art performance among open\nmodels RL-trained with publicly available data, outperforming best baselines by\n7.9% and 6.7% on our 17-task evaluation suite across six reasoning domains. We\nalso show that our models effectively improve the Pass@k performance of their\nbase models, particularly on complex tasks less likely to appear in pretraining\ndata. We release data, models, training and evaluation code to facilitate\ngeneral-purpose reasoning at: https://github.com/LLM360/Reasoning360",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14965.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6083902e1e36b13a64497d91",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6083902e1e36b13a64497d91/h4rGHMn2c6z5GesF0F6VU.png",
            "fullname": "cheng",
            "name": "zhoujun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.09827",
            "authors": [
                {
                    "_id": "685519bb4f1add9d4c5c5cbd",
                    "user": {
                        "_id": "61a24fc72101184cfb29c965",
                        "avatarUrl": "/avatars/e32aa61016caef50de28c16b30196799.svg",
                        "isPro": false,
                        "fullname": "Christoph Schuhmann",
                        "user": "ChristophSchuhmann",
                        "type": "user"
                    },
                    "name": "Christoph Schuhmann",
                    "status": "extracted_confirmed",
                    "statusLastChangedAt": "2025-06-20T10:22:26.676Z",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cbe",
                    "name": "Robert Kaczmarczyk",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cbf",
                    "user": {
                        "_id": "64ac21f11cacea8d4b8f2b3f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ac21f11cacea8d4b8f2b3f/asQOf8wFZ4vmqIeyxfvUR.jpeg",
                        "isPro": false,
                        "fullname": "Gollam Rabby",
                        "user": "tourist800",
                        "type": "user"
                    },
                    "name": "Gollam Rabby",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:29:47.779Z",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc0",
                    "user": {
                        "_id": "62e7dd4036a8e8a82700041c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
                        "isPro": false,
                        "fullname": "Felix Friedrich",
                        "user": "felfri",
                        "type": "user"
                    },
                    "name": "Felix Friedrich",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T08:57:36.090Z",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc1",
                    "name": "Maurice Kraus",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc2",
                    "name": "Kourosh Nadi",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc3",
                    "name": "Huu Nguyen",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc4",
                    "name": "Kristian Kersting",
                    "hidden": false
                },
                {
                    "_id": "685519bb4f1add9d4c5c5cc5",
                    "user": {
                        "_id": "62cd8f74342b1d5dab8da3a6",
                        "avatarUrl": "/avatars/51c237653aadc98c73df207d9d054597.svg",
                        "isPro": false,
                        "fullname": "Sören Auer",
                        "user": "soeren1611",
                        "type": "user"
                    },
                    "name": "Sören Auer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:06.624Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
            ],
            "publishedAt": "2025-06-11T15:06:59.000Z",
            "submittedOnDailyAt": "2025-06-20T06:53:47.262Z",
            "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection",
            "submittedOnDailyBy": {
                "_id": "62e7dd4036a8e8a82700041c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
                "isPro": false,
                "fullname": "Felix Friedrich",
                "user": "felfri",
                "type": "user"
            },
            "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.",
            "upvotes": 7,
            "discussionId": "685519bb4f1add9d4c5c5cc6",
            "ai_summary": "EmoNet-Voice, a new resource with large pre-training and benchmark datasets, advances speech emotion recognition by offering fine-grained emotion evaluation with synthetic, privacy-preserving audio.",
            "ai_keywords": [
                "speech emotion recognition",
                "SER",
                "EmoNet-Voice",
                "EmoNet-Voice Big",
                "EmoNet-Voice Bench",
                "human expert annotations",
                "synthetic audio snippets",
                "psychology experts",
                "high-arousal emotions",
                "low-arousal states",
                "Empathic Insight Voice models"
            ]
        },
        "publishedAt": "2025-06-11T11:06:59.000Z",
        "title": "EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech\n  Emotion Detection",
        "summary": "The advancement of text-to-speech and audio generation models necessitates\nrobust benchmarks for evaluating the emotional understanding capabilities of AI\nsystems. Current speech emotion recognition (SER) datasets often exhibit\nlimitations in emotional granularity, privacy concerns, or reliance on acted\nportrayals. This paper introduces EmoNet-Voice, a new resource for speech\nemotion detection, which includes EmoNet-Voice Big, a large-scale pre-training\ndataset (featuring over 4,500 hours of speech across 11 voices, 40 emotions,\nand 4 languages), and EmoNet-Voice Bench, a novel benchmark dataset with human\nexpert annotations. EmoNet-Voice is designed to evaluate SER models on a\nfine-grained spectrum of 40 emotion categories with different levels of\nintensities. Leveraging state-of-the-art voice generation, we curated synthetic\naudio snippets simulating actors portraying scenes designed to evoke specific\nemotions. Crucially, we conducted rigorous validation by psychology experts who\nassigned perceived intensity labels. This synthetic, privacy-preserving\napproach allows for the inclusion of sensitive emotional states often absent in\nexisting datasets. Lastly, we introduce Empathic Insight Voice models that set\na new standard in speech emotion recognition with high agreement with human\nexperts. Our evaluations across the current model landscape exhibit valuable\nfindings, such as high-arousal emotions like anger being much easier to detect\nthan low-arousal states like concentration.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62e7dd4036a8e8a82700041c/tvkMYrIKiGhbIVAuaxvlt.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.09827.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e7dd4036a8e8a82700041c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e7dd4036a8e8a82700041c/Dgk9mXYLVd4LpiNLWjn-q.jpeg",
            "fullname": "Felix Friedrich",
            "name": "felfri",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.15564",
            "authors": [
                {
                    "_id": "6854b5547bc8d012d4ca9979",
                    "name": "Jinheng Xie",
                    "hidden": false
                },
                {
                    "_id": "6854b5547bc8d012d4ca997a",
                    "name": "Zhenheng Yang",
                    "hidden": false
                },
                {
                    "_id": "6854b5547bc8d012d4ca997b",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T15:39:15.000Z",
            "submittedOnDailyAt": "2025-06-20T21:45:11.155Z",
            "title": "Show-o2: Improved Native Unified Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "This paper presents improved native unified multimodal models, i.e.,\nShow-o2, that leverage autoregressive modeling and flow matching. Built upon a\n3D causal variational autoencoder space, unified visual representations are\nconstructed through a dual-path of spatial (-temporal) fusion, enabling\nscalability across image and video modalities while ensuring effective\nmultimodal understanding and generation. Based on a language model,\nautoregressive modeling and flow matching are natively applied to the language\nhead and flow head, respectively, to facilitate text token prediction and\nimage/video generation. A two-stage training recipe is designed to effectively\nlearn and scale to larger models. The resulting Show-o2 models demonstrate\nversatility in handling a wide range of multimodal understanding and generation\ntasks across diverse modalities, including text, images, and videos. Code and\nmodels are released at https://github.com/showlab/Show-o.",
            "upvotes": 4,
            "discussionId": "6854b5557bc8d012d4ca997c",
            "ai_summary": "Show-o2 leverages autoregressive modeling and flow matching within a 3D causal variational autoencoder to create unified visual representations for multimodal understanding and generation tasks.",
            "ai_keywords": [
                "autoregressive modeling",
                "flow matching",
                "3D causal variational autoencoder",
                "spatial-temporal fusion",
                "unified multimodal models",
                "language model",
                "text token prediction",
                "image/video generation",
                "two-stage training"
            ]
        },
        "publishedAt": "2025-06-18T11:39:15.000Z",
        "title": "Show-o2: Improved Native Unified Multimodal Models",
        "summary": "This paper presents improved native unified multimodal models, i.e.,\nShow-o2, that leverage autoregressive modeling and flow matching. Built upon a\n3D causal variational autoencoder space, unified visual representations are\nconstructed through a dual-path of spatial (-temporal) fusion, enabling\nscalability across image and video modalities while ensuring effective\nmultimodal understanding and generation. Based on a language model,\nautoregressive modeling and flow matching are natively applied to the language\nhead and flow head, respectively, to facilitate text token prediction and\nimage/video generation. A two-stage training recipe is designed to effectively\nlearn and scale to larger models. The resulting Show-o2 models demonstrate\nversatility in handling a wide range of multimodal understanding and generation\ntasks across diverse modalities, including text, images, and videos. Code and\nmodels are released at https://github.com/showlab/Show-o.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15564.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7151
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.15154",
            "authors": [
                {
                    "_id": "685393f499bf39f9665c79db",
                    "user": {
                        "_id": "65fd1805883a1c3b4b1fde10",
                        "avatarUrl": "/avatars/ff475f757e190021f66e1f3c0fe5bd17.svg",
                        "isPro": false,
                        "fullname": "Anuradha Chopra",
                        "user": "annabeth97c",
                        "type": "user"
                    },
                    "name": "Anuradha Chopra",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:18.910Z",
                    "hidden": false
                },
                {
                    "_id": "685393f499bf39f9665c79dc",
                    "name": "Abhinaba Roy",
                    "hidden": false
                },
                {
                    "_id": "685393f499bf39f9665c79dd",
                    "user": {
                        "_id": "655431b2997379e9b0999d23",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
                        "isPro": false,
                        "fullname": "Dorien Herremans",
                        "user": "dorienh",
                        "type": "user"
                    },
                    "name": "Dorien Herremans",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:29.316Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
            ],
            "publishedAt": "2025-06-18T05:51:36.000Z",
            "submittedOnDailyAt": "2025-06-20T04:32:59.096Z",
            "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
            "submittedOnDailyBy": {
                "_id": "655431b2997379e9b0999d23",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
                "isPro": false,
                "fullname": "Dorien Herremans",
                "user": "dorienh",
                "type": "user"
            },
            "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
            "upvotes": 4,
            "discussionId": "685393f599bf39f9665c79de",
            "ai_summary": "SonicVerse, a multi-task music captioning model, integrates audio feature detection to enhance caption quality and enable detailed descriptions of music pieces.",
            "ai_keywords": [
                "multi-task music captioning",
                "SonicVerse",
                "caption generation",
                "key detection",
                "vocals detection",
                "projection-based architecture",
                "language tokens",
                "auxiliary heads",
                "time-informed descriptions",
                "large-language model",
                "MusicBench dataset",
                "MIRFLEX",
                "music feature extractor"
            ]
        },
        "publishedAt": "2025-06-18T01:51:36.000Z",
        "title": "SonicVerse: Multi-Task Learning for Music Feature-Informed Captioning",
        "summary": "Detailed captions that accurately reflect the characteristics of a music\npiece can enrich music databases and drive forward research in music AI. This\npaper introduces a multi-task music captioning model, SonicVerse, that\nintegrates caption generation with auxiliary music feature detection tasks such\nas key detection, vocals detection, and more, so as to directly capture both\nlow-level acoustic details as well as high-level musical attributes. The key\ncontribution is a projection-based architecture that transforms audio input\ninto language tokens, while simultaneously detecting music features through\ndedicated auxiliary heads. The outputs of these heads are also projected into\nlanguage tokens, to enhance the captioning input. This framework not only\nproduces rich, descriptive captions for short music fragments but also directly\nenables the generation of detailed time-informed descriptions for longer music\npieces, by chaining the outputs using a large-language model. To train the\nmodel, we extended the MusicBench dataset by annotating it with music features\nusing MIRFLEX, a modular music feature extractor, resulting in paired audio,\ncaptions and music feature data. Experimental results show that incorporating\nfeatures in this way improves the quality and detail of the generated captions.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/655431b2997379e9b0999d23/zAwOQ2zY8-8Eu7bUhFURQ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15154.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "655431b2997379e9b0999d23",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
            "fullname": "Dorien Herremans",
            "name": "dorienh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14837",
            "authors": [
                {
                    "_id": "6854ea7a7bc8d012d4ca998d",
                    "user": {
                        "_id": "672a29efe53061b3dc76fd70",
                        "avatarUrl": "/avatars/20c7100884f4a69a9ec781315f68ff0b.svg",
                        "isPro": false,
                        "fullname": "xuchengzhi",
                        "user": "dazhiga",
                        "type": "user"
                    },
                    "name": "Chengzhi Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:45.306Z",
                    "hidden": false
                },
                {
                    "_id": "6854ea7a7bc8d012d4ca998e",
                    "name": "Yuyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6854ea7a7bc8d012d4ca998f",
                    "user": {
                        "_id": "64a16b1aeacb4b50ba1c889d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
                        "isPro": false,
                        "fullname": "Lai Wei",
                        "user": "WaltonFuture",
                        "type": "user"
                    },
                    "name": "Lai Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T08:57:40.628Z",
                    "hidden": false
                },
                {
                    "_id": "6854ea7a7bc8d012d4ca9990",
                    "user": {
                        "_id": "65a52766215aabac489e3468",
                        "avatarUrl": "/avatars/fe05e22cd7e12e961296426434e17c76.svg",
                        "isPro": false,
                        "fullname": "Lichao Sun",
                        "user": "sunlichao137",
                        "type": "user"
                    },
                    "name": "Lichao Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:30:53.972Z",
                    "hidden": false
                },
                {
                    "_id": "6854ea7a7bc8d012d4ca9991",
                    "user": {
                        "_id": "65e095da35ad8b2fe8c80e71",
                        "avatarUrl": "/avatars/225d4be6411dcec2460047ea1a88a5c3.svg",
                        "isPro": false,
                        "fullname": "Weiran Huang",
                        "user": "weiranhuang",
                        "type": "user"
                    },
                    "name": "Weiran Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-20T12:31:00.023Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-15T14:10:16.000Z",
            "submittedOnDailyAt": "2025-06-20T03:28:58.032Z",
            "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
            "submittedOnDailyBy": {
                "_id": "64a16b1aeacb4b50ba1c889d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
                "isPro": false,
                "fullname": "Lai Wei",
                "user": "WaltonFuture",
                "type": "user"
            },
            "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.",
            "upvotes": 2,
            "discussionId": "6854ea7a7bc8d012d4ca9992",
            "ai_summary": "ChartIR uses structured instruction and iterative refinement to improve MLLM performance in chart-to-code generation by separating visual understanding and code translation tasks.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "visual understanding",
                "code translation",
                "structured instruction",
                "description instruction",
                "difference instruction",
                "language representations",
                "initial code generation",
                "iterative refinement",
                "Qwen2-VL",
                "GPT-4o"
            ]
        },
        "publishedAt": "2025-06-15T10:10:16.000Z",
        "title": "Improved Iterative Refinement for Chart-to-Code Generation via\n  Structured Instruction",
        "summary": "Recently, multimodal large language models (MLLMs) have attracted increasing\nresearch attention due to their powerful visual understanding capabilities.\nWhile they have achieved impressive results on various vision tasks, their\nperformance on chart-to-code generation remains suboptimal. This task requires\nMLLMs to generate executable code that can reproduce a given chart, demanding\nnot only precise visual understanding but also accurate translation of visual\nelements into structured code. Directly prompting MLLMs to perform this complex\ntask often yields unsatisfactory results. To address this challenge, we propose\n{ChartIR}, an iterative refinement method based on structured instruction.\nFirst, we distinguish two tasks: visual understanding and code translation. To\naccomplish the visual understanding component, we design two types of\nstructured instructions: description and difference. The description\ninstruction captures the visual elements of the reference chart, while the\ndifference instruction characterizes the discrepancies between the reference\nchart and the generated chart. These instructions effectively transform visual\nfeatures into language representations, thereby facilitating the subsequent\ncode translation process. Second, we decompose the overall chart generation\npipeline into two stages: initial code generation and iterative refinement,\nenabling progressive enhancement of the final output. Experimental results show\nthat, compared to other method, our method achieves superior performance on\nboth the open-source model Qwen2-VL and the closed-source model GPT-4o.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14837.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a16b1aeacb4b50ba1c889d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a16b1aeacb4b50ba1c889d/EhWoBGL6LlFGIVTUsp2F4.jpeg",
            "fullname": "Lai Wei",
            "name": "WaltonFuture",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.15455",
            "authors": [
                {
                    "_id": "68546a187bc8d012d4ca991f",
                    "name": "Xinnuo Xu",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9920",
                    "name": "Rachel Lawrence",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9921",
                    "user": {
                        "_id": "669e070afbc9ba8a128a2807",
                        "avatarUrl": "/avatars/babae90dc758a865dfb76a710ac1da5f.svg",
                        "isPro": false,
                        "fullname": "KD",
                        "user": "Ksh000",
                        "type": "user"
                    },
                    "name": "Kshitij Dubey",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-20T12:19:19.484Z",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9922",
                    "name": "Atharva Pandey",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9923",
                    "name": "Risa Ueno",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9924",
                    "name": "Fabian Falck",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9925",
                    "name": "Aditya V. Nori",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9926",
                    "name": "Rahul Sharma",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9927",
                    "name": "Amit Sharma",
                    "hidden": false
                },
                {
                    "_id": "68546a187bc8d012d4ca9928",
                    "name": "Javier Gonzalez",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/669e070afbc9ba8a128a2807/O6wK_gM9xelNdW9seuZXC.webp"
            ],
            "publishedAt": "2025-06-18T13:35:47.000Z",
            "submittedOnDailyAt": "2025-06-20T13:48:53.334Z",
            "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation",
            "submittedOnDailyBy": {
                "_id": "669e070afbc9ba8a128a2807",
                "avatarUrl": "/avatars/babae90dc758a865dfb76a710ac1da5f.svg",
                "isPro": false,
                "fullname": "KD",
                "user": "Ksh000",
                "type": "user"
            },
            "summary": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.",
            "upvotes": 1,
            "discussionId": "68546a187bc8d012d4ca9929",
            "ai_summary": "RE-IMAGINE evaluates the reasoning abilities of Large Language Models by generating variations of problems that cannot be solved by memorization, indicating reliance on statistical recall.",
            "ai_keywords": [
                "Large Language Models",
                "reasoning benchmarks",
                "ladder of causation",
                "RE-IMAGINE",
                "reasoning hierarchy",
                "symbolic representation",
                "problem variations",
                "reasoning domains",
                "memorization",
                "performance evaluation"
            ]
        },
        "publishedAt": "2025-06-18T09:35:47.000Z",
        "title": "RE-IMAGINE: Symbolic Benchmark Synthesis for Reasoning Evaluation",
        "summary": "Recent Large Language Models (LLMs) have reported high accuracy on reasoning\nbenchmarks. However, it is still unclear whether the observed results arise\nfrom true reasoning or from statistical recall of the training set. Inspired by\nthe ladder of causation (Pearl, 2009) and its three levels (associations,\ninterventions and counterfactuals), this paper introduces RE-IMAGINE, a\nframework to characterize a hierarchy of reasoning ability in LLMs, alongside\nan automated pipeline to generate problem variations at different levels of the\nhierarchy. By altering problems in an intermediate symbolic representation,\nRE-IMAGINE generates arbitrarily many problems that are not solvable using\nmemorization alone. Moreover, the framework is general and can work across\nreasoning domains, including math, code, and logic. We demonstrate our\nframework on four widely-used benchmarks to evaluate several families of LLMs,\nand observe reductions in performance when the models are queried with problem\nvariations. These assessments indicate a degree of reliance on statistical\nrecall for past performance, and open the door to further research targeting\nskills across the reasoning hierarchy.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/669e070afbc9ba8a128a2807/O6wK_gM9xelNdW9seuZXC.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15455.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "669e070afbc9ba8a128a2807",
            "avatarUrl": "/avatars/babae90dc758a865dfb76a710ac1da5f.svg",
            "fullname": "KD",
            "name": "Ksh000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]