[
    {
        "paper": {
            "id": "2506.18882",
            "authors": [
                {
                    "_id": "685a163c0e4ad7e219758569",
                    "name": "Hong Li",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e21975856a",
                    "name": "Houyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e21975856b",
                    "name": "Chongjie Ye",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e21975856c",
                    "name": "Zhaoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e21975856d",
                    "name": "Bohan Li",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e21975856e",
                    "name": "Shaocong Xu",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e21975856f",
                    "name": "Xianda Guo",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e219758570",
                    "name": "Xuhui Liu",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e219758571",
                    "name": "Yikai Wang",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e219758572",
                    "name": "Baochang Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e219758573",
                    "user": {
                        "_id": "64a7ec232431ebd64dd44cf4",
                        "avatarUrl": "/avatars/26f811ade720ffbd671deaacc03ee4ba.svg",
                        "isPro": false,
                        "fullname": "Satoshi Ikehata",
                        "user": "smyth",
                        "type": "user"
                    },
                    "name": "Satoshi Ikehata",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T12:51:11.759Z",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e219758574",
                    "name": "Boxin Shi",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e219758575",
                    "name": "Anyi Rao",
                    "hidden": false
                },
                {
                    "_id": "685a163c0e4ad7e219758576",
                    "name": "Hao Zhao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
            ],
            "publishedAt": "2025-06-23T17:53:11.000Z",
            "submittedOnDailyAt": "2025-06-24T02:59:45.489Z",
            "title": "Light of Normals: Unified Feature Representation for Universal\n  Photometric Stereo",
            "submittedOnDailyBy": {
                "_id": "643a1f5b58cb07c2a3745116",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
                "isPro": false,
                "fullname": "Hugo",
                "user": "chongjie",
                "type": "user"
            },
            "summary": "Universal photometric stereo (PS) aims to recover high-quality surface\nnormals from objects under arbitrary lighting conditions without relying on\nspecific illumination models. Despite recent advances such as SDM-UniPS and Uni\nMS-PS, two fundamental challenges persist: 1) the deep coupling between varying\nillumination and surface normal features, where ambiguity in observed intensity\nmakes it difficult to determine whether brightness variations stem from\nlighting changes or surface orientation; and 2) the preservation of\nhigh-frequency geometric details in complex surfaces, where intricate\ngeometries create self-shadowing, inter-reflections, and subtle normal\nvariations that conventional feature processing operations struggle to capture\naccurately.",
            "upvotes": 72,
            "discussionId": "685a163c0e4ad7e219758577",
            "githubRepo": "https://github.com/houyuanchen111/LINO_UniPS",
            "ai_summary": "Photometric stereo aims to recover high-quality surface normals under arbitrary lighting conditions, addressing challenges related to illumination-surface normal coupling and high-frequency geometric detail preservation.",
            "ai_keywords": [
                "photometric stereo",
                "deep coupling",
                "surface normals",
                "illumination conditions",
                "intensity variations",
                "self-shadowing",
                "inter-reflections",
                "subtle normal variations"
            ],
            "githubStars": 106
        },
        "publishedAt": "2025-06-23T13:53:11.000Z",
        "title": "Light of Normals: Unified Feature Representation for Universal\n  Photometric Stereo",
        "summary": "Universal photometric stereo (PS) aims to recover high-quality surface\nnormals from objects under arbitrary lighting conditions without relying on\nspecific illumination models. Despite recent advances such as SDM-UniPS and Uni\nMS-PS, two fundamental challenges persist: 1) the deep coupling between varying\nillumination and surface normal features, where ambiguity in observed intensity\nmakes it difficult to determine whether brightness variations stem from\nlighting changes or surface orientation; and 2) the preservation of\nhigh-frequency geometric details in complex surfaces, where intricate\ngeometries create self-shadowing, inter-reflections, and subtle normal\nvariations that conventional feature processing operations struggle to capture\naccurately.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643a1f5b58cb07c2a3745116/4JHMhL80xxrkPBOIt9-Cg.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18882.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643a1f5b58cb07c2a3745116",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a1f5b58cb07c2a3745116/OiSDfgfcCUWu0X4-FiNm0.jpeg",
            "fullname": "Hugo",
            "name": "chongjie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18871",
            "authors": [
                {
                    "_id": "685a0be90e4ad7e2197584f4",
                    "user": {
                        "_id": "65f19fa7f591e4538b65dea5",
                        "avatarUrl": "/avatars/a38e65701e1d2eb3eb93335d6d0b937c.svg",
                        "isPro": false,
                        "fullname": "Chenyuan Wu",
                        "user": "wcyno23",
                        "type": "user"
                    },
                    "name": "Chenyuan Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:41:49.563Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584f5",
                    "name": "Pengfei Zheng",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584f6",
                    "user": {
                        "_id": "661ac5b53d7248a6f20080c1",
                        "avatarUrl": "/avatars/26aef5944759c2e4366a71eb8c7fc50a.svg",
                        "isPro": false,
                        "fullname": "Ruiran Yan",
                        "user": "Ruiran",
                        "type": "user"
                    },
                    "name": "Ruiran Yan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:41:58.820Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584f7",
                    "user": {
                        "_id": "62612679bbcbd1c34f1638af",
                        "avatarUrl": "/avatars/c0675d05a52192ee14e9ab1633353956.svg",
                        "isPro": false,
                        "fullname": "Xiao",
                        "user": "Shitao",
                        "type": "user"
                    },
                    "name": "Shitao Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:42:07.997Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584f8",
                    "user": {
                        "_id": "641bd1737c21ab946bf69aff",
                        "avatarUrl": "/avatars/83759075ad893a69a0c2cf5493d7e988.svg",
                        "isPro": false,
                        "fullname": "xin luo",
                        "user": "sienna223",
                        "type": "user"
                    },
                    "name": "Xin Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:42.720Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584f9",
                    "user": {
                        "_id": "6458b59c7a7e192202df8fa0",
                        "avatarUrl": "/avatars/33ee716477e5686da8723d01e199cd27.svg",
                        "isPro": false,
                        "fullname": "Yueze Wang",
                        "user": "yzwang",
                        "type": "user"
                    },
                    "name": "Yueze Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T09:33:03.979Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584fa",
                    "user": {
                        "_id": "675bcb9ce16de4a95aac9950",
                        "avatarUrl": "/avatars/a1d0a2fd96ddee9cdea4f97819233fe5.svg",
                        "isPro": false,
                        "fullname": "Wanli Li",
                        "user": "liwanli",
                        "type": "user"
                    },
                    "name": "Wanli Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:42:15.514Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584fb",
                    "user": {
                        "_id": "674972973dc92067bd606877",
                        "avatarUrl": "/avatars/8366448b45baf7d7f3d3d2b8793479ed.svg",
                        "isPro": false,
                        "fullname": "Jiang Xiyan",
                        "user": "Emilia515",
                        "type": "user"
                    },
                    "name": "Xiyan Jiang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:42:29.622Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584fc",
                    "name": "Yexin Liu",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584fd",
                    "user": {
                        "_id": "6564a2ceedae9c33b7654a1f",
                        "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
                        "isPro": false,
                        "fullname": "JUNJIE ZHOU",
                        "user": "JUNJIE99",
                        "type": "user"
                    },
                    "name": "Junjie Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:40.699Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584fe",
                    "user": {
                        "_id": "66164f6245336ca774679611",
                        "avatarUrl": "/avatars/9baf0ab475bc8d5997abda9ffe8cfa28.svg",
                        "isPro": false,
                        "fullname": "Ze Liu",
                        "user": "marsh123",
                        "type": "user"
                    },
                    "name": "Ze Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:38.616Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e2197584ff",
                    "user": {
                        "_id": "6540617c7cadb2d1b42007c5",
                        "avatarUrl": "/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg",
                        "isPro": false,
                        "fullname": "Ziyi Xia",
                        "user": "ZiyiXia",
                        "type": "user"
                    },
                    "name": "Ziyi Xia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:42:47.445Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758500",
                    "name": "Chaofan Li",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758501",
                    "name": "Haoge Deng",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758502",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758503",
                    "name": "Kun Luo",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758504",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758505",
                    "user": {
                        "_id": "66ed026076a8038cb4ae6053",
                        "avatarUrl": "/avatars/99b6527da6b66c6b5df3fc8261587322.svg",
                        "isPro": false,
                        "fullname": "Defu Lian",
                        "user": "dove-ustc",
                        "type": "user"
                    },
                    "name": "Defu Lian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:43:23.976Z",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758506",
                    "name": "Xinlong Wang",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758507",
                    "name": "Zhongyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758508",
                    "name": "Tiejun Huang",
                    "hidden": false
                },
                {
                    "_id": "685a0be90e4ad7e219758509",
                    "name": "Zheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T17:38:54.000Z",
            "submittedOnDailyAt": "2025-06-24T01:06:04.763Z",
            "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
            "submittedOnDailyBy": {
                "_id": "6564a2ceedae9c33b7654a1f",
                "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
                "isPro": false,
                "fullname": "JUNJIE ZHOU",
                "user": "JUNJIE99",
                "type": "user"
            },
            "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
            "upvotes": 48,
            "discussionId": "685a0be90e4ad7e21975850a",
            "projectPage": "https://vectorspacelab.github.io/OmniGen2/",
            "githubRepo": "https://github.com/VectorSpaceLab/OmniGen2",
            "ai_summary": "OmniGen2, a versatile generative model, introduces dual decoding pathways for text and images, preserves original text generation, and achieves competitive results with a new subject-driven benchmark.",
            "ai_keywords": [
                "decoding pathways",
                "unshared parameters",
                "decoupled image tokenizer",
                "multimodal understanding models",
                "reflection mechanism",
                "reflection dataset",
                "OmniContext",
                "state-of-the-art performance"
            ],
            "githubStars": 777
        },
        "publishedAt": "2025-06-23T13:38:54.000Z",
        "title": "OmniGen2: Exploration to Advanced Multimodal Generation",
        "summary": "In this work, we introduce OmniGen2, a versatile and open-source generative\nmodel designed to provide a unified solution for diverse generation tasks,\nincluding text-to-image, image editing, and in-context generation. Unlike\nOmniGen v1, OmniGen2 features two distinct decoding pathways for text and image\nmodalities, utilizing unshared parameters and a decoupled image tokenizer. This\ndesign enables OmniGen2 to build upon existing multimodal understanding models\nwithout the need to re-adapt VAE inputs, thereby preserving the original text\ngeneration capabilities. To facilitate the training of OmniGen2, we developed\ncomprehensive data construction pipelines, encompassing image editing and\nin-context generation data. Additionally, we introduce a reflection mechanism\ntailored for image generation tasks and curate a dedicated reflection dataset\nbased on OmniGen2. Despite its relatively modest parameter size, OmniGen2\nachieves competitive results on multiple task benchmarks, including\ntext-to-image and image editing. To further evaluate in-context generation,\nalso referred to as subject-driven tasks, we introduce a new benchmark named\nOmniContext. OmniGen2 achieves state-of-the-art performance among open-source\nmodels in terms of consistency. We will release our models, training code,\ndatasets, and data construction pipeline to support future research in this\nfield. Project Page: https://vectorspacelab.github.io/OmniGen2; GitHub Link:\nhttps://github.com/VectorSpaceLab/OmniGen2",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18871.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6564a2ceedae9c33b7654a1f",
            "avatarUrl": "/avatars/42f09356a1282896573ccb44830cd327.svg",
            "fullname": "JUNJIE ZHOU",
            "name": "JUNJIE99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18841",
            "authors": [
                {
                    "_id": "685a0f330e4ad7e219758514",
                    "name": "Yuhao Wu",
                    "hidden": false
                },
                {
                    "_id": "685a0f330e4ad7e219758515",
                    "name": "Yushi Bai",
                    "hidden": false
                },
                {
                    "_id": "685a0f330e4ad7e219758516",
                    "user": {
                        "_id": "637f228152229c63921119c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637f228152229c63921119c3/acwXorra1r9_7i3KlBFjS.jpeg",
                        "isPro": false,
                        "fullname": "Zhiqiang Hu",
                        "user": "Zhiqiang007",
                        "type": "user"
                    },
                    "name": "Zhiqiang Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:35.954Z",
                    "hidden": false
                },
                {
                    "_id": "685a0f330e4ad7e219758517",
                    "name": "Roy Ka-Wei Lee",
                    "hidden": false
                },
                {
                    "_id": "685a0f330e4ad7e219758518",
                    "name": "Juanzi Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T16:59:02.000Z",
            "submittedOnDailyAt": "2025-06-24T01:08:07.123Z",
            "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
            "submittedOnDailyBy": {
                "_id": "63369da91ba5d5ece24118a4",
                "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
                "isPro": false,
                "fullname": "wuyuhao",
                "user": "mozhu",
                "type": "user"
            },
            "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
            "upvotes": 40,
            "discussionId": "685a0f340e4ad7e219758519",
            "ai_summary": "An incentivization-based reinforcement learning approach is used to develop a large language model capable of generating ultra-long, high-quality text without the need for synthetic data or supervised fine-tuning.",
            "ai_keywords": [
                "reinforcement learning",
                "reward models",
                "long-form text generation",
                "ultra-long generation",
                "large language models",
                "synthetic fine-tuning",
                "length control",
                "writing quality",
                "structural formatting",
                "WritingBench",
                "Arena-Write"
            ]
        },
        "publishedAt": "2025-06-23T12:59:02.000Z",
        "title": "LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement\n  Learning",
        "summary": "Ultra-long generation by large language models (LLMs) is a widely demanded\nscenario, yet it remains a significant challenge due to their maximum\ngeneration length limit and overall quality degradation as sequence length\nincreases. Previous approaches, exemplified by LongWriter, typically rely on\n''teaching'', which involves supervised fine-tuning (SFT) on synthetic\nlong-form outputs. However, this strategy heavily depends on synthetic SFT\ndata, which is difficult and costly to construct, often lacks coherence and\nconsistency, and tends to be overly artificial and structurally monotonous. In\nthis work, we propose an incentivization-based approach that, starting entirely\nfrom scratch and without relying on any annotated or synthetic data, leverages\nreinforcement learning (RL) to foster the emergence of ultra-long, high-quality\ntext generation capabilities in LLMs. We perform RL training starting from a\nbase model, similar to R1-Zero, guiding it to engage in reasoning that\nfacilitates planning and refinement during the writing process. To support\nthis, we employ specialized reward models that steer the LLM towards improved\nlength control, writing quality, and structural formatting. Experimental\nevaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B,\nconsistently outperforms traditional SFT methods on long-form writing tasks,\nachieving state-of-the-art results across all metrics on WritingBench and\nArena-Write, and even surpassing 100B+ models such as DeepSeek R1 and\nQwen3-235B. We open-source our data and model checkpoints under\nhttps://huggingface.co/THU-KEG/LongWriter-Zero-32B",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18841.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63369da91ba5d5ece24118a4",
            "avatarUrl": "/avatars/67889e1ecadb04100a77bc8b5284c6fd.svg",
            "fullname": "wuyuhao",
            "name": "mozhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18792",
            "authors": [
                {
                    "_id": "685a72a00e4ad7e219758702",
                    "user": {
                        "_id": "66f44a3df9252e0f50b59fdb",
                        "avatarUrl": "/avatars/4234203c77a0e6f594f3de26bfe8c649.svg",
                        "isPro": false,
                        "fullname": "Michal Nazarczuk",
                        "user": "michaal94",
                        "type": "user"
                    },
                    "name": "Michal Nazarczuk",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T12:43:11.760Z",
                    "hidden": false
                },
                {
                    "_id": "685a72a00e4ad7e219758703",
                    "name": "Sibi Catley-Chandar",
                    "hidden": false
                },
                {
                    "_id": "685a72a00e4ad7e219758704",
                    "name": "Thomas Tanay",
                    "hidden": false
                },
                {
                    "_id": "685a72a00e4ad7e219758705",
                    "name": "Zhensong Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a72a00e4ad7e219758706",
                    "name": "Gregory Slabaugh",
                    "hidden": false
                },
                {
                    "_id": "685a72a00e4ad7e219758707",
                    "name": "Eduardo Pérez-Pellitero",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T16:01:15.000Z",
            "submittedOnDailyAt": "2025-06-24T08:15:41.084Z",
            "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
            "submittedOnDailyBy": {
                "_id": "66f44a3df9252e0f50b59fdb",
                "avatarUrl": "/avatars/4234203c77a0e6f594f3de26bfe8c649.svg",
                "isPro": false,
                "fullname": "Michal Nazarczuk",
                "user": "michaal94",
                "type": "user"
            },
            "summary": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving\nsubjects from arbitrary viewpoints. This task is particularly challenging when\nrelying on monocular video, where disentangling structure from motion is\nill-posed and supervision is scarce. We introduce Video Diffusion-Aware\nReconstruction (ViDAR), a novel 4D reconstruction framework that leverages\npersonalised diffusion models to synthesise a pseudo multi-view supervision\nsignal for training a Gaussian splatting representation. By conditioning on\nscene-specific features, ViDAR recovers fine-grained appearance details while\nmitigating artefacts introduced by monocular ambiguity. To address the\nspatio-temporal inconsistency of diffusion-based supervision, we propose a\ndiffusion-aware loss function and a camera pose optimisation strategy that\naligns synthetic views with the underlying scene geometry. Experiments on\nDyCheck, a challenging benchmark with extreme viewpoint variation, show that\nViDAR outperforms all state-of-the-art baselines in visual quality and\ngeometric consistency. We further highlight ViDAR's strong improvement over\nbaselines on dynamic regions and provide a new benchmark to compare performance\nin reconstructing motion-rich parts of the scene. Project page:\nhttps://vidar-4d.github.io",
            "upvotes": 26,
            "discussionId": "685a72a10e4ad7e219758708",
            "ai_summary": "ViDAR uses diffusion-aware reconstruction to generate high-quality novel views of dynamic scenes from monocular video, outperforming existing methods in visual quality and geometric consistency.",
            "ai_keywords": [
                "Video Diffusion-Aware Reconstruction",
                "ViDAR",
                "Gaussian splatting",
                "diffusion models",
                "spatio-temporal inconsistency",
                "diffusion-aware loss function",
                "camera pose optimisation",
                "DyCheck benchmark"
            ]
        },
        "publishedAt": "2025-06-23T12:01:15.000Z",
        "title": "ViDAR: Video Diffusion-Aware 4D Reconstruction From Monocular Inputs",
        "summary": "Dynamic Novel View Synthesis aims to generate photorealistic views of moving\nsubjects from arbitrary viewpoints. This task is particularly challenging when\nrelying on monocular video, where disentangling structure from motion is\nill-posed and supervision is scarce. We introduce Video Diffusion-Aware\nReconstruction (ViDAR), a novel 4D reconstruction framework that leverages\npersonalised diffusion models to synthesise a pseudo multi-view supervision\nsignal for training a Gaussian splatting representation. By conditioning on\nscene-specific features, ViDAR recovers fine-grained appearance details while\nmitigating artefacts introduced by monocular ambiguity. To address the\nspatio-temporal inconsistency of diffusion-based supervision, we propose a\ndiffusion-aware loss function and a camera pose optimisation strategy that\naligns synthetic views with the underlying scene geometry. Experiments on\nDyCheck, a challenging benchmark with extreme viewpoint variation, show that\nViDAR outperforms all state-of-the-art baselines in visual quality and\ngeometric consistency. We further highlight ViDAR's strong improvement over\nbaselines on dynamic regions and provide a new benchmark to compare performance\nin reconstructing motion-rich parts of the scene. Project page:\nhttps://vidar-4d.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18792.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66f44a3df9252e0f50b59fdb",
            "avatarUrl": "/avatars/4234203c77a0e6f594f3de26bfe8c649.svg",
            "fullname": "Michal Nazarczuk",
            "name": "michaal94",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18896",
            "authors": [
                {
                    "_id": "685a02790e4ad7e2197584b2",
                    "name": "Jiaru Zou",
                    "hidden": false
                },
                {
                    "_id": "685a02790e4ad7e2197584b3",
                    "name": "Ling Yang",
                    "hidden": false
                },
                {
                    "_id": "685a02790e4ad7e2197584b4",
                    "name": "Jingwen Gu",
                    "hidden": false
                },
                {
                    "_id": "685a02790e4ad7e2197584b5",
                    "user": {
                        "_id": "65271df5abd7795aaa1fd86e",
                        "avatarUrl": "/avatars/99e1aabaefcf5e7f438817375cd1ddef.svg",
                        "isPro": false,
                        "fullname": "Jiahao Qiu",
                        "user": "jiahaoq",
                        "type": "user"
                    },
                    "name": "Jiahao Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T12:53:36.246Z",
                    "hidden": false
                },
                {
                    "_id": "685a02790e4ad7e2197584b6",
                    "name": "Ke Shen",
                    "hidden": false
                },
                {
                    "_id": "685a02790e4ad7e2197584b7",
                    "name": "Jingrui He",
                    "hidden": false
                },
                {
                    "_id": "685a02790e4ad7e2197584b8",
                    "name": "Mengdi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T17:59:02.000Z",
            "submittedOnDailyAt": "2025-06-24T01:03:32.146Z",
            "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
            "submittedOnDailyBy": {
                "_id": "64fde4e252e82dd432b74ce9",
                "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
                "isPro": false,
                "fullname": "Ling Yang",
                "user": "Lingaaaaaaa",
                "type": "user"
            },
            "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
            "upvotes": 25,
            "discussionId": "685a027a0e4ad7e2197584b9",
            "projectPage": "https://huggingface.co/collections/Gen-Verse/reasonflux-prm-68463c73cf1c6a0ec6fafeb5",
            "githubRepo": "https://github.com/Gen-Verse/ReasonFlux",
            "ai_summary": "ReasonFlux-PRM, a novel trajectory-aware Process Reward Model, evaluates reasoning traces with step-level and trajectory-level supervision, enhancing performance in model distillation, reinforcement learning, and test-time scaling.",
            "ai_keywords": [
                "Process Reward Models",
                "trajectory-aware PRM",
                "trajectory-response outputs",
                "step-level supervision",
                "trajectory-level supervision",
                "chain-of-thought data",
                "model distillation",
                "policy optimization",
                "reinforcement learning",
                "Best-of-N test-time scaling",
                "AIME",
                "MATH500",
                "GPQA-Diamond"
            ],
            "githubStars": 414
        },
        "publishedAt": "2025-06-23T13:59:02.000Z",
        "title": "ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought\n  Reasoning in LLMs",
        "summary": "Process Reward Models (PRMs) have recently emerged as a powerful framework\nfor supervising intermediate reasoning steps in large language models (LLMs).\nPrevious PRMs are primarily trained on model final output responses and\nstruggle to evaluate intermediate thinking trajectories robustly, especially in\nthe emerging setting of trajectory-response outputs generated by frontier\nreasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a\nnovel trajectory-aware PRM explicitly designed to evaluate the\ntrajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both\nstep-level and trajectory-level supervision, enabling fine-grained reward\nassignment aligned with structured chain-of-thought data. We adapt\nReasonFlux-PRM to support reward supervision under both offline and online\nsettings, including (i) selecting high-quality model distillation data for\ndownstream supervised fine-tuning of smaller models, (ii) providing dense\nprocess-level rewards for policy optimization during reinforcement learning,\nand (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results\non challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond\ndemonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs\n(e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our\nderived ReasonFlux-PRM-7B yields consistent performance improvements, achieving\naverage gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement\nlearning, and 6.3% in test-time scaling. We also release our efficient\nReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment.\nProjects: https://github.com/Gen-Verse/ReasonFlux",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18896.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "/avatars/061a69d858b86d1600be916122cae7fc.svg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18851",
            "authors": [
                {
                    "_id": "685a0fb40e4ad7e219758528",
                    "user": {
                        "_id": "6304e2dabad6ce7fc0287d57",
                        "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
                        "isPro": false,
                        "fullname": "Zhuowei_Chen",
                        "user": "ZhuoweiChen",
                        "type": "user"
                    },
                    "name": "Zhuowei Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:44:49.590Z",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e219758529",
                    "user": {
                        "_id": "63b415037af2e415f2599c18",
                        "avatarUrl": "/avatars/4afbe7d6d05a702f1beeed9c53e78153.svg",
                        "isPro": false,
                        "fullname": "Bingchuan Li",
                        "user": "lbc402",
                        "type": "user"
                    },
                    "name": "Bingchuan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:44:57.735Z",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e21975852a",
                    "user": {
                        "_id": "6804ce31d205d72ddbeec8a0",
                        "avatarUrl": "/avatars/772d20a653649063158cba166298801a.svg",
                        "isPro": false,
                        "fullname": "Tianxiang Ma",
                        "user": "TianxiangMa",
                        "type": "user"
                    },
                    "name": "Tianxiang Ma",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:45:10.156Z",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e21975852b",
                    "name": "Lijie Liu",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e21975852c",
                    "user": {
                        "_id": "619b404bab4c7b7f16a7d57d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619b404bab4c7b7f16a7d57d/coT_UGRfBOUAeSxjyhdlG.jpeg",
                        "isPro": false,
                        "fullname": "Mingcong Liu",
                        "user": "onion-liu",
                        "type": "user"
                    },
                    "name": "Mingcong Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:23.740Z",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e21975852d",
                    "name": "Yi Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e21975852e",
                    "name": "Gen Li",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e21975852f",
                    "user": {
                        "_id": "6752cd83ffaeeb979db974ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ci915Tdmv3uWbCqGy7LqL.png",
                        "isPro": false,
                        "fullname": "Xinghui Li",
                        "user": "Crayon-Shinchan",
                        "type": "user"
                    },
                    "name": "Xinghui Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:45:40.054Z",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e219758530",
                    "name": "Siyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e219758531",
                    "name": "Qian He",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e219758532",
                    "user": {
                        "_id": "67bc6b515d9470ec64bdcc33",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/dttL8Fb3bKCVG5zjg_02q.png",
                        "isPro": false,
                        "fullname": "Xinglong Wu",
                        "user": "Xingzhe-xlwu",
                        "type": "user"
                    },
                    "name": "Xinglong Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T09:45:50.807Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T17:11:56.000Z",
            "submittedOnDailyAt": "2025-06-24T01:12:12.451Z",
            "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
            "submittedOnDailyBy": {
                "_id": "6304e2dabad6ce7fc0287d57",
                "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
                "isPro": false,
                "fullname": "Zhuowei_Chen",
                "user": "ZhuoweiChen",
                "type": "user"
            },
            "summary": "Subject-to-video generation has witnessed substantial progress in recent\nyears. However, existing models still face significant challenges in faithfully\nfollowing textual instructions. This limitation, commonly known as the\ncopy-paste problem, arises from the widely used in-pair training paradigm. This\napproach inherently entangles subject identity with background and contextual\nattributes by sampling reference images from the same scene as the target\nvideo. To address this issue, we introduce Phantom-Data, the first\ngeneral-purpose cross-pair subject-to-video consistency dataset, containing\napproximately one million identity-consistent pairs across diverse categories.\nOur dataset is constructed via a three-stage pipeline: (1) a general and\ninput-aligned subject detection module, (2) large-scale cross-context subject\nretrieval from more than 53 million videos and 3 billion images, and (3)\nprior-guided identity verification to ensure visual consistency under\ncontextual variation. Comprehensive experiments show that training with\nPhantom-Data significantly improves prompt alignment and visual quality while\npreserving identity consistency on par with in-pair baselines.",
            "upvotes": 25,
            "discussionId": "685a0fb40e4ad7e219758535",
            "projectPage": "https://phantom-video.github.io/Phantom-Data/",
            "githubRepo": "https://github.com/Phantom-video/Phantom-Data",
            "ai_summary": "A cross-pair dataset called Phantom-Data improves subject-to-video generation by enhancing prompt alignment and visual quality while maintaining identity consistency.",
            "ai_keywords": [
                "Phantom-Data",
                "subject-to-video generation",
                "copy-paste problem",
                "in-pair training paradigm",
                "subject detection",
                "cross-context subject retrieval",
                "prior-guided identity verification"
            ],
            "githubStars": 28
        },
        "publishedAt": "2025-06-23T13:11:56.000Z",
        "title": "Phantom-Data : Towards a General Subject-Consistent Video Generation\n  Dataset",
        "summary": "Subject-to-video generation has witnessed substantial progress in recent\nyears. However, existing models still face significant challenges in faithfully\nfollowing textual instructions. This limitation, commonly known as the\ncopy-paste problem, arises from the widely used in-pair training paradigm. This\napproach inherently entangles subject identity with background and contextual\nattributes by sampling reference images from the same scene as the target\nvideo. To address this issue, we introduce Phantom-Data, the first\ngeneral-purpose cross-pair subject-to-video consistency dataset, containing\napproximately one million identity-consistent pairs across diverse categories.\nOur dataset is constructed via a three-stage pipeline: (1) a general and\ninput-aligned subject detection module, (2) large-scale cross-context subject\nretrieval from more than 53 million videos and 3 billion images, and (3)\nprior-guided identity verification to ensure visual consistency under\ncontextual variation. Comprehensive experiments show that training with\nPhantom-Data significantly improves prompt alignment and visual quality while\npreserving identity consistency on par with in-pair baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18851.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6304e2dabad6ce7fc0287d57",
            "avatarUrl": "/avatars/3fd4a9a62b0ef98db2573411463a9247.svg",
            "fullname": "Zhuowei_Chen",
            "name": "ZhuoweiChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18254",
            "authors": [
                {
                    "_id": "685a39d60e4ad7e219758622",
                    "user": {
                        "_id": "64abc4aa6cadc7aca585dddf",
                        "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
                        "isPro": false,
                        "fullname": "Tianyu Yu",
                        "user": "Yirany",
                        "type": "user"
                    },
                    "name": "Tianyu Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T12:43:15.822Z",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e219758623",
                    "name": "Bo Ji",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e219758624",
                    "name": "Shouli Wang",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e219758625",
                    "name": "Shu Yao",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e219758626",
                    "name": "Zefan Wang",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e219758627",
                    "name": "Ganqu Cui",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e219758628",
                    "name": "Lifan Yuan",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e219758629",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e21975862a",
                    "name": "Yuan Yao",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e21975862b",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e21975862c",
                    "name": "Maosong Sun",
                    "hidden": false
                },
                {
                    "_id": "685a39d60e4ad7e21975862d",
                    "user": {
                        "_id": "6570ae84c4993b8fb96f41a8",
                        "avatarUrl": "/avatars/21f7d79d46ac4df0ecff8eca7678b33f.svg",
                        "isPro": false,
                        "fullname": "Tat-Seng Chua",
                        "user": "chuats",
                        "type": "user"
                    },
                    "name": "Tat-Seng Chua",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:41:22.951Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T02:56:36.000Z",
            "submittedOnDailyAt": "2025-06-24T04:14:54.081Z",
            "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
            "submittedOnDailyBy": {
                "_id": "64abc4aa6cadc7aca585dddf",
                "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
                "isPro": false,
                "fullname": "Tianyu Yu",
                "user": "Yirany",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
            "upvotes": 24,
            "discussionId": "685a39d80e4ad7e21975862e",
            "projectPage": "https://github.com/OpenBMB/RLPR",
            "githubRepo": "https://github.com/OpenBMB/RLPR",
            "ai_summary": "RLPR, a verifier-free framework using LLM's token probability scores as reward signals, enhances reasoning capabilities across both general and mathematical domains, outperforming other methods in various benchmarks.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "reasoning capabilities",
                "LLM",
                "RLPR",
                "token probability scores",
                "prob-to-reward",
                "stabilizing methods",
                "TheoremQA",
                "Minerva",
                "General-Reasoner"
            ],
            "githubStars": 30
        },
        "publishedAt": "2025-06-22T22:56:36.000Z",
        "title": "RLPR: Extrapolating RLVR to General Domains without Verifiers",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) demonstrates promising\npotential in advancing the reasoning capabilities of LLMs. However, its success\nremains largely confined to mathematical and code domains. This primary\nlimitation stems from the heavy reliance on domain-specific verifiers, which\nresults in prohibitive complexity and limited scalability. To address the\nchallenge, our key observation is that LLM's intrinsic probability of\ngenerating a correct free-form answer directly indicates its own evaluation of\nthe reasoning reward (i.e., how well the reasoning process leads to the correct\nanswer). Building on this insight, we propose RLPR, a simple verifier-free\nframework that extrapolates RLVR to broader general domains. RLPR uses the\nLLM's own token probability scores for reference answers as the reward signal\nand maximizes the expected reward during training. We find that addressing the\nhigh variance of this noisy probability reward is crucial to make it work, and\npropose prob-to-reward and stabilizing methods to ensure a precise and stable\nreward from LLM intrinsic probabilities. Comprehensive experiments in four\ngeneral-domain benchmarks and three mathematical benchmarks show that RLPR\nconsistently improves reasoning capabilities in both areas for Gemma, Llama,\nand Qwen based models. Notably, RLPR outperforms concurrent VeriFree by 7.6\npoints on TheoremQA and 7.5 points on Minerva, and even surpasses strong\nverifier-model-dependent approaches General-Reasoner by 1.6 average points\nacross seven benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18254.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64abc4aa6cadc7aca585dddf",
            "avatarUrl": "/avatars/736afea979cd0021c7a37f68731524ea.svg",
            "fullname": "Tianyu Yu",
            "name": "Yirany",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.15741",
            "authors": [
                {
                    "_id": "685a48970e4ad7e219758662",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758663",
                    "user": {
                        "_id": "64301abe450c0de9a1d3d18e",
                        "avatarUrl": "/avatars/01b284874dadc7d21d656c53dcb77e42.svg",
                        "isPro": false,
                        "fullname": "tianrui",
                        "user": "tianyue818",
                        "type": "user"
                    },
                    "name": "Tianrui Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T07:14:35.823Z",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758664",
                    "user": {
                        "_id": "6578265ddea7e2122d02f6ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
                        "isPro": false,
                        "fullname": "kang zhu",
                        "user": "kangz",
                        "type": "user"
                    },
                    "name": "King Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T09:33:02.082Z",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758665",
                    "name": "Heyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758666",
                    "name": "Yeyi Guan",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758667",
                    "name": "Jinxiang Xia",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758668",
                    "name": "Yi Yao",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758669",
                    "name": "Hanhao Li",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e21975866a",
                    "user": {
                        "_id": "67380907ba72be94f292a066",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67380907ba72be94f292a066/08xnqDYxZqmVbfE1uMhdQ.jpeg",
                        "isPro": false,
                        "fullname": "Ningning Wang",
                        "user": "Juvenilecris",
                        "type": "user"
                    },
                    "name": "Ningning Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T17:44:04.939Z",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e21975866b",
                    "name": "Pai Liu",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e21975866c",
                    "name": "Tianhao Peng",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e21975866d",
                    "name": "Xin Gui",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e21975866e",
                    "name": "Xiaowan Li",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e21975866f",
                    "name": "Yuhui Liu",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758670",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758671",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758672",
                    "name": "Changwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758673",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758674",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758675",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758676",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758677",
                    "name": "Xitong Gao",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758678",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "685a48970e4ad7e219758679",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-17T17:59:02.000Z",
            "submittedOnDailyAt": "2025-06-24T05:17:03.413Z",
            "title": "OAgents: An Empirical Study of Building Effective Agents",
            "submittedOnDailyBy": {
                "_id": "628c8598ef14f971b698107f",
                "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
                "isPro": false,
                "fullname": "Zhou",
                "user": "Wangchunshu",
                "type": "user"
            },
            "summary": "Recently, Agentic AI has become an increasingly popular research field.\nHowever, we argue that current agent research practices lack standardization\nand scientific rigor, making it hard to conduct fair comparisons among methods.\nAs a result, it is still unclear how different design choices in agent\nframeworks affect effectiveness, and measuring their progress remains\nchallenging. In this work, we conduct a systematic empirical study on GAIA\nbenchmark and BrowseComp to examine the impact of popular design choices in key\nagent components in a fair and rigorous manner. We find that the lack of a\nstandard evaluation protocol makes previous works, even open-sourced ones,\nnon-reproducible, with significant variance between random runs. Therefore, we\nintroduce a more robust evaluation protocol to stabilize comparisons. Our study\nreveals which components and designs are crucial for effective agents, while\nothers are redundant, despite seeming logical. Based on our findings, we build\nand open-source OAgents, a new foundation agent framework that achieves\nstate-of-the-art performance among open-source projects. OAgents offers a\nmodular design for various agent components, promoting future research in\nAgentic AI.",
            "upvotes": 24,
            "discussionId": "685a48980e4ad7e21975867a"
        },
        "publishedAt": "2025-06-17T13:59:02.000Z",
        "title": "OAgents: An Empirical Study of Building Effective Agents",
        "summary": "Recently, Agentic AI has become an increasingly popular research field.\nHowever, we argue that current agent research practices lack standardization\nand scientific rigor, making it hard to conduct fair comparisons among methods.\nAs a result, it is still unclear how different design choices in agent\nframeworks affect effectiveness, and measuring their progress remains\nchallenging. In this work, we conduct a systematic empirical study on GAIA\nbenchmark and BrowseComp to examine the impact of popular design choices in key\nagent components in a fair and rigorous manner. We find that the lack of a\nstandard evaluation protocol makes previous works, even open-sourced ones,\nnon-reproducible, with significant variance between random runs. Therefore, we\nintroduce a more robust evaluation protocol to stabilize comparisons. Our study\nreveals which components and designs are crucial for effective agents, while\nothers are redundant, despite seeming logical. Based on our findings, we build\nand open-source OAgents, a new foundation agent framework that achieves\nstate-of-the-art performance among open-source projects. OAgents offers a\nmodular design for various agent components, promoting future research in\nAgentic AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15741.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628c8598ef14f971b698107f",
            "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
            "fullname": "Zhou",
            "name": "Wangchunshu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18898",
            "authors": [
                {
                    "_id": "685a07510e4ad7e2197584c6",
                    "user": {
                        "_id": "62318c0386753f5f41d0e261",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
                        "isPro": false,
                        "fullname": "Jiaming Han",
                        "user": "csuhan",
                        "type": "user"
                    },
                    "name": "Jiaming Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:47.088Z",
                    "hidden": false
                },
                {
                    "_id": "685a07510e4ad7e2197584c7",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "685a07510e4ad7e2197584c8",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "685a07510e4ad7e2197584c9",
                    "user": {
                        "_id": "6365a174ad2c9e8b6731dd0f",
                        "avatarUrl": "/avatars/2f4dd0eda92bca5a7464129fe7d961f9.svg",
                        "isPro": false,
                        "fullname": "Hanyu Wang",
                        "user": "hywang66",
                        "type": "user"
                    },
                    "name": "Hanyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:44.987Z",
                    "hidden": false
                },
                {
                    "_id": "685a07510e4ad7e2197584ca",
                    "name": "Qi Zhao",
                    "hidden": false
                },
                {
                    "_id": "685a07510e4ad7e2197584cb",
                    "name": "Ziyan Yang",
                    "hidden": false
                },
                {
                    "_id": "685a07510e4ad7e2197584cc",
                    "name": "Hao He",
                    "hidden": false
                },
                {
                    "_id": "685a07510e4ad7e2197584cd",
                    "user": {
                        "_id": "666a8f24e2990b0cb16b7bf9",
                        "avatarUrl": "/avatars/fcbaf8f1e3e53a2a4a819b7cb2c53aa4.svg",
                        "isPro": false,
                        "fullname": "Xiangyu Yue",
                        "user": "xyyue",
                        "type": "user"
                    },
                    "name": "Xiangyu Yue",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T12:50:57.558Z",
                    "hidden": false
                },
                {
                    "_id": "685a07510e4ad7e2197584ce",
                    "name": "Lu Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T17:59:14.000Z",
            "submittedOnDailyAt": "2025-06-24T01:02:20.046Z",
            "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
            "submittedOnDailyBy": {
                "_id": "62318c0386753f5f41d0e261",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
                "isPro": false,
                "fullname": "Jiaming Han",
                "user": "csuhan",
                "type": "user"
            },
            "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
            "upvotes": 22,
            "discussionId": "685a07520e4ad7e2197584cf",
            "projectPage": "https://tar.csuhan.com",
            "githubRepo": "https://github.com/csuhan/Tar",
            "ai_summary": "A multimodal framework uses a Text-Aligned Tokenizer (TA-Tok) to integrate vision and text into a unified space, employing a generative de-tokenizer with autoregressive and diffusion-based models for efficient and high-fidelity visual outputs.",
            "ai_keywords": [
                "Text-Aligned Tokenizer (TA-Tok)",
                "multimodal LLM",
                "Tar",
                "scale-adaptive encoding",
                "diffusion-based model",
                "autoregressive model",
                "modality fusion",
                "pre-training tasks"
            ],
            "githubStars": 41
        },
        "publishedAt": "2025-06-23T13:59:14.000Z",
        "title": "Vision as a Dialect: Unifying Visual Understanding and Generation via\n  Text-Aligned Representations",
        "summary": "This paper presents a multimodal framework that attempts to unify visual\nunderstanding and generation within a shared discrete semantic representation.\nAt its core is the Text-Aligned Tokenizer (TA-Tok), which converts images into\ndiscrete tokens using a text-aligned codebook projected from a large language\nmodel's (LLM) vocabulary. By integrating vision and text into a unified space\nwith an expanded vocabulary, our multimodal LLM, Tar, enables cross-modal input\nand output through a shared interface, without the need for modality-specific\ndesigns. Additionally, we propose scale-adaptive encoding and decoding to\nbalance efficiency and visual detail, along with a generative de-tokenizer to\nproduce high-fidelity visual outputs. To address diverse decoding needs, we\nutilize two complementary de-tokenizers: a fast autoregressive model and a\ndiffusion-based model. To enhance modality fusion, we investigate advanced\npre-training tasks, demonstrating improvements in both visual understanding and\ngeneration. Experiments across benchmarks show that Tar matches or surpasses\nexisting multimodal LLM methods, achieving faster convergence and greater\ntraining efficiency. Code, models, and data are available at\nhttps://tar.csuhan.com",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18898.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62318c0386753f5f41d0e261",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62318c0386753f5f41d0e261/xO_5PvOf7lXhQPnQLcmnq.jpeg",
            "fullname": "Jiaming Han",
            "name": "csuhan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18903",
            "authors": [
                {
                    "_id": "685a1ce80e4ad7e2197585b7",
                    "user": {
                        "_id": "638e29cf319f9c746b87ad4b",
                        "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
                        "isPro": true,
                        "fullname": "Runjia Li",
                        "user": "liguang0115",
                        "type": "user"
                    },
                    "name": "Runjia Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T12:54:50.758Z",
                    "hidden": false
                },
                {
                    "_id": "685a1ce80e4ad7e2197585b8",
                    "user": {
                        "_id": "6565ed28a5ec0231cb07225f",
                        "avatarUrl": "/avatars/7f95bba9aa7811d56eecb380827abfac.svg",
                        "isPro": false,
                        "fullname": "prof philip torr",
                        "user": "philiptorr",
                        "type": "user"
                    },
                    "name": "Philip Torr",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T12:54:41.794Z",
                    "hidden": false
                },
                {
                    "_id": "685a1ce80e4ad7e2197585b9",
                    "user": {
                        "_id": "68055d1082c2bae6af503848",
                        "avatarUrl": "/avatars/e32c2f0a8efbf54707ead520ae9374bf.svg",
                        "isPro": false,
                        "fullname": "Andrea Vedaldi",
                        "user": "vedaldi",
                        "type": "user"
                    },
                    "name": "Andrea Vedaldi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T12:54:34.616Z",
                    "hidden": false
                },
                {
                    "_id": "685a1ce80e4ad7e2197585ba",
                    "user": {
                        "_id": "6435823f76dbfd731bca13ab",
                        "avatarUrl": "/avatars/67cd864bb8b1310a344e60a27b1ed075.svg",
                        "isPro": false,
                        "fullname": "Tomas Jakab",
                        "user": "tmsj",
                        "type": "user"
                    },
                    "name": "Tomas Jakab",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T12:54:25.185Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
            ],
            "publishedAt": "2025-06-23T17:59:56.000Z",
            "submittedOnDailyAt": "2025-06-24T02:12:37.233Z",
            "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory",
            "submittedOnDailyBy": {
                "_id": "638e29cf319f9c746b87ad4b",
                "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
                "isPro": true,
                "fullname": "Runjia Li",
                "user": "liguang0115",
                "type": "user"
            },
            "summary": "We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.",
            "upvotes": 13,
            "discussionId": "685a1ce80e4ad7e2197585bb",
            "projectPage": "https://v-mem.github.io/",
            "githubRepo": "https://github.com/runjiali-rl/vmem",
            "ai_summary": "A novel memory mechanism called Surfel-Indexed View Memory enhances video generation by efficiently remembering and retrieving relevant past views, improving long-term scene coherence and reducing computational cost.",
            "ai_keywords": [
                "memory mechanism",
                "video generators",
                "out-painting",
                "3D geometry",
                "context window",
                "Surfel-Indexed View Memory",
                "surfels",
                "scene coherence",
                "camera control",
                "long-term scene synthesis benchmarks"
            ],
            "githubStars": 47
        },
        "publishedAt": "2025-06-23T13:59:56.000Z",
        "title": "VMem: Consistent Interactive Video Scene Generation with Surfel-Indexed\n  View Memory",
        "summary": "We propose a novel memory mechanism to build video generators that can\nexplore environments interactively. Similar results have previously been\nachieved by out-painting 2D views of the scene while incrementally\nreconstructing its 3D geometry, which quickly accumulates errors, or by video\ngenerators with a short context window, which struggle to maintain scene\ncoherence over the long term. To address these limitations, we introduce\nSurfel-Indexed View Memory (VMem), a mechanism that remembers past views by\nindexing them geometrically based on the 3D surface elements (surfels) they\nhave observed. VMem enables the efficient retrieval of the most relevant past\nviews when generating new ones. By focusing only on these relevant views, our\nmethod produces consistent explorations of imagined environments at a fraction\nof the computational cost of using all past views as context. We evaluate our\napproach on challenging long-term scene synthesis benchmarks and demonstrate\nsuperior performance compared to existing methods in maintaining scene\ncoherence and camera control.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638e29cf319f9c746b87ad4b/Gt6zbaM3ILEQIG4Cl1W2J.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18903.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "638e29cf319f9c746b87ad4b",
            "avatarUrl": "/avatars/70cac8d47847c389eb0393051a64c4a4.svg",
            "fullname": "Runjia Li",
            "name": "liguang0115",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18463",
            "authors": [
                {
                    "_id": "685ab6570e4ad7e2197587ae",
                    "user": {
                        "_id": "67e426c3118deb3746243ce3",
                        "avatarUrl": "/avatars/cfca0c2d8e4b683bc046f4d4fe0b716a.svg",
                        "isPro": false,
                        "fullname": "Sophia Sirko-Galouchenko",
                        "user": "SophiaSirko",
                        "type": "user"
                    },
                    "name": "Sophia Sirko-Galouchenko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T17:44:07.015Z",
                    "hidden": false
                },
                {
                    "_id": "685ab6570e4ad7e2197587af",
                    "name": "Spyros Gidaris",
                    "hidden": false
                },
                {
                    "_id": "685ab6570e4ad7e2197587b0",
                    "name": "Antonin Vobecky",
                    "hidden": false
                },
                {
                    "_id": "685ab6570e4ad7e2197587b1",
                    "name": "Andrei Bursuc",
                    "hidden": false
                },
                {
                    "_id": "685ab6570e4ad7e2197587b2",
                    "name": "Nicolas Thome",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T10:01:14.000Z",
            "submittedOnDailyAt": "2025-06-24T13:38:21.578Z",
            "title": "DIP: Unsupervised Dense In-Context Post-training of Visual\n  Representations",
            "submittedOnDailyBy": {
                "_id": "678cd947d13c19c917644792",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/010zyg-JoIjaqVn9lSDRT.png",
                "isPro": false,
                "fullname": "Spyros Gidaris",
                "user": "sgidaris",
                "type": "user"
            },
            "summary": "We introduce DIP, a novel unsupervised post-training method designed to\nenhance dense image representations in large-scale pretrained vision encoders\nfor in-context scene understanding. Unlike prior approaches that rely on\ncomplex self-distillation architectures, our method trains the vision encoder\nusing pseudo-tasks that explicitly simulate downstream in-context scenarios,\ninspired by meta-learning principles. To enable post-training on unlabeled\ndata, we propose an automatic mechanism for generating in-context tasks that\ncombines a pretrained diffusion model and the vision encoder itself. DIP is\nsimple, unsupervised, and computationally efficient, requiring less than 9\nhours on a single A100 GPU. By learning dense representations through pseudo\nin-context tasks, it achieves strong performance across a wide variety of\ndownstream real-world in-context scene understanding tasks. It outperforms both\nthe initial vision encoder and prior methods, offering a practical and\neffective solution for improving dense representations. Code available here:\nhttps://github.com/sirkosophia/DIP",
            "upvotes": 12,
            "discussionId": "685ab6570e4ad7e2197587b3",
            "ai_summary": "A novel unsupervised post-training method improves dense image representations using pseudo-tasks and a pretrained diffusion model for in-context scene understanding.",
            "ai_keywords": [
                "DIP",
                "vision encoders",
                "self-distillation",
                "diffusion model",
                "in-context tasks",
                "meta-learning",
                "dense representations",
                "scene understanding"
            ]
        },
        "publishedAt": "2025-06-23T06:01:14.000Z",
        "title": "DIP: Unsupervised Dense In-Context Post-training of Visual\n  Representations",
        "summary": "We introduce DIP, a novel unsupervised post-training method designed to\nenhance dense image representations in large-scale pretrained vision encoders\nfor in-context scene understanding. Unlike prior approaches that rely on\ncomplex self-distillation architectures, our method trains the vision encoder\nusing pseudo-tasks that explicitly simulate downstream in-context scenarios,\ninspired by meta-learning principles. To enable post-training on unlabeled\ndata, we propose an automatic mechanism for generating in-context tasks that\ncombines a pretrained diffusion model and the vision encoder itself. DIP is\nsimple, unsupervised, and computationally efficient, requiring less than 9\nhours on a single A100 GPU. By learning dense representations through pseudo\nin-context tasks, it achieves strong performance across a wide variety of\ndownstream real-world in-context scene understanding tasks. It outperforms both\nthe initial vision encoder and prior methods, offering a practical and\neffective solution for improving dense representations. Code available here:\nhttps://github.com/sirkosophia/DIP",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18463.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "678cd947d13c19c917644792",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/010zyg-JoIjaqVn9lSDRT.png",
            "fullname": "Spyros Gidaris",
            "name": "sgidaris",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18309",
            "authors": [
                {
                    "_id": "685a2df10e4ad7e219758604",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e219758605",
                    "name": "Di Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e219758606",
                    "name": "Fangkai Yang",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e219758607",
                    "name": "Pu Zhao",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e219758608",
                    "name": "Jianfeng Liu",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e219758609",
                    "name": "Yuefeng Zhan",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e21975860a",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e21975860b",
                    "name": "Qingwei Lin",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e21975860c",
                    "user": {
                        "_id": "642da49dc1adada5b345c72b",
                        "avatarUrl": "/avatars/5fa8d345a7248246454444999756105b.svg",
                        "isPro": false,
                        "fullname": "deng",
                        "user": "weiweideng",
                        "type": "user"
                    },
                    "name": "Weiwei Deng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T12:58:19.210Z",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e21975860d",
                    "name": "Dongmei Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e21975860e",
                    "name": "Feng Sun",
                    "hidden": false
                },
                {
                    "_id": "685a2df10e4ad7e21975860f",
                    "name": "Qi Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T05:51:52.000Z",
            "submittedOnDailyAt": "2025-06-24T03:21:15.103Z",
            "title": "LettinGo: Explore User Profile Generation for Recommendation System",
            "submittedOnDailyBy": {
                "_id": "654dbac9938fbf1e696be8aa",
                "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
                "isPro": false,
                "fullname": "Chaoyun Zhang",
                "user": "vyokky",
                "type": "user"
            },
            "summary": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
            "upvotes": 8,
            "discussionId": "685a2df20e4ad7e219758610",
            "ai_summary": "LettinGo enhances user profiling via diverse, adaptive profiles generated using LLMs and Direct Preference Optimization, improving recommendation accuracy and flexibility.",
            "ai_keywords": [
                "large language models (LLMs)",
                "semantically richer",
                "more transparent",
                "supervised fine-tuning (SFT)",
                "Direct Preference Optimization (DPO)",
                "profile generator",
                "pairwise preference data",
                "recommendation systems",
                "recommendation accuracy",
                "flexibility",
                "contextual awareness"
            ]
        },
        "publishedAt": "2025-06-23T01:51:52.000Z",
        "title": "LettinGo: Explore User Profile Generation for Recommendation System",
        "summary": "User profiling is pivotal for recommendation systems, as it transforms raw\nuser interaction data into concise and structured representations that drive\npersonalized recommendations. While traditional embedding-based profiles lack\ninterpretability and adaptability, recent advances with large language models\n(LLMs) enable text-based profiles that are semantically richer and more\ntransparent. However, existing methods often adhere to fixed formats that limit\ntheir ability to capture the full diversity of user behaviors. In this paper,\nwe introduce LettinGo, a novel framework for generating diverse and adaptive\nuser profiles. By leveraging the expressive power of LLMs and incorporating\ndirect feedback from downstream recommendation tasks, our approach avoids the\nrigid constraints imposed by supervised fine-tuning (SFT). Instead, we employ\nDirect Preference Optimization (DPO) to align the profile generator with\ntask-specific performance, ensuring that the profiles remain adaptive and\neffective. LettinGo operates in three stages: (1) exploring diverse user\nprofiles via multiple LLMs, (2) evaluating profile quality based on their\nimpact in recommendation systems, and (3) aligning the profile generation\nthrough pairwise preference data derived from task performance. Experimental\nresults demonstrate that our framework significantly enhances recommendation\naccuracy, flexibility, and contextual awareness. This work enhances profile\ngeneration as a key innovation for next-generation recommendation systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18309.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654dbac9938fbf1e696be8aa",
            "avatarUrl": "/avatars/b3c4035c48169c1bfb04a439fce3499f.svg",
            "fullname": "Chaoyun Zhang",
            "name": "vyokky",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18839",
            "authors": [
                {
                    "_id": "685aa5810e4ad7e219758779",
                    "name": "Chaoyang Wang",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e21975877a",
                    "name": "Ashkan Mirzaei",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e21975877b",
                    "name": "Vidit Goel",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e21975877c",
                    "name": "Willi Menapace",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e21975877d",
                    "name": "Aliaksandr Siarohin",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e21975877e",
                    "name": "Avalon Vinella",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e21975877f",
                    "name": "Michael Vasilkovsky",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e219758780",
                    "name": "Ivan Skorokhodov",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e219758781",
                    "name": "Vladislav Shakhrai",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e219758782",
                    "name": "Sergey Korolev",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e219758783",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                },
                {
                    "_id": "685aa5810e4ad7e219758784",
                    "name": "Peter Wonka",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T23:44:59.000Z",
            "submittedOnDailyAt": "2025-06-24T11:48:49.494Z",
            "title": "4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction\n  for 4D Scene Generation",
            "submittedOnDailyBy": {
                "_id": "634db15dd00bb5d92c3bd94f",
                "avatarUrl": "/avatars/9a6a1231bc5205911272d83527593f1a.svg",
                "isPro": false,
                "fullname": "Ashkan Mirzaei",
                "user": "ashmrz",
                "type": "user"
            },
            "summary": "We propose the first framework capable of computing a 4D spatio-temporal grid\nof video frames and 3D Gaussian particles for each time step using a\nfeed-forward architecture. Our architecture has two main components, a 4D video\nmodel and a 4D reconstruction model. In the first part, we analyze current 4D\nvideo diffusion architectures that perform spatial and temporal attention\neither sequentially or in parallel within a two-stream design. We highlight the\nlimitations of existing approaches and introduce a novel fused architecture\nthat performs spatial and temporal attention within a single layer. The key to\nour method is a sparse attention pattern, where tokens attend to others in the\nsame frame, at the same timestamp, or from the same viewpoint. In the second\npart, we extend existing 3D reconstruction algorithms by introducing a Gaussian\nhead, a camera token replacement algorithm, and additional dynamic layers and\ntraining. Overall, we establish a new state of the art for 4D generation,\nimproving both visual quality and reconstruction capability.",
            "upvotes": 8,
            "discussionId": "685aa5820e4ad7e219758785",
            "ai_summary": "A new framework combines 4D video modeling and 3D reconstruction using a unified architecture with sparse attention patterns, achieving superior visual quality and reconstruction.",
            "ai_keywords": [
                "4D spatio-temporal grid",
                "feed-forward architecture",
                "4D video model",
                "4D reconstruction model",
                "4D video diffusion",
                "spatial attention",
                "temporal attention",
                "fused architecture",
                "sparse attention pattern",
                "Gaussian head",
                "camera token replacement",
                "dynamic layers"
            ]
        },
        "publishedAt": "2025-06-18T19:44:59.000Z",
        "title": "4Real-Video-V2: Fused View-Time Attention and Feedforward Reconstruction\n  for 4D Scene Generation",
        "summary": "We propose the first framework capable of computing a 4D spatio-temporal grid\nof video frames and 3D Gaussian particles for each time step using a\nfeed-forward architecture. Our architecture has two main components, a 4D video\nmodel and a 4D reconstruction model. In the first part, we analyze current 4D\nvideo diffusion architectures that perform spatial and temporal attention\neither sequentially or in parallel within a two-stream design. We highlight the\nlimitations of existing approaches and introduce a novel fused architecture\nthat performs spatial and temporal attention within a single layer. The key to\nour method is a sparse attention pattern, where tokens attend to others in the\nsame frame, at the same timestamp, or from the same viewpoint. In the second\npart, we extend existing 3D reconstruction algorithms by introducing a Gaussian\nhead, a camera token replacement algorithm, and additional dynamic layers and\ntraining. Overall, we establish a new state of the art for 4D generation,\nimproving both visual quality and reconstruction capability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18839.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "634db15dd00bb5d92c3bd94f",
            "avatarUrl": "/avatars/9a6a1231bc5205911272d83527593f1a.svg",
            "fullname": "Ashkan Mirzaei",
            "name": "ashmrz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18901",
            "authors": [
                {
                    "_id": "685a8f0e0e4ad7e21975872a",
                    "user": {
                        "_id": "64897b1f0ec897cfe579a399",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg",
                        "isPro": false,
                        "fullname": "wenq",
                        "user": "wenqsun",
                        "type": "user"
                    },
                    "name": "Wenqiang Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:32:35.425Z",
                    "hidden": false
                },
                {
                    "_id": "685a8f0e0e4ad7e21975872b",
                    "name": "Fangyun Wei",
                    "hidden": false
                },
                {
                    "_id": "685a8f0e0e4ad7e21975872c",
                    "name": "Jinjing Zhao",
                    "hidden": false
                },
                {
                    "_id": "685a8f0e0e4ad7e21975872d",
                    "name": "Xi Chen",
                    "hidden": false
                },
                {
                    "_id": "685a8f0e0e4ad7e21975872e",
                    "user": {
                        "_id": "644e3e5f030210812f413073",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/uW8TKV2sds97lFBwnt6JK.jpeg",
                        "isPro": false,
                        "fullname": "Zilong Chen",
                        "user": "heheyas",
                        "type": "user"
                    },
                    "name": "Zilong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:33:00.269Z",
                    "hidden": false
                },
                {
                    "_id": "685a8f0e0e4ad7e21975872f",
                    "name": "Hongyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a8f0e0e4ad7e219758730",
                    "name": "Jun Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a8f0e0e4ad7e219758731",
                    "name": "Yan Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T17:59:53.000Z",
            "submittedOnDailyAt": "2025-06-24T10:12:58.089Z",
            "title": "From Virtual Games to Real-World Play",
            "submittedOnDailyBy": {
                "_id": "64897b1f0ec897cfe579a399",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg",
                "isPro": false,
                "fullname": "wenq",
                "user": "wenqsun",
                "type": "user"
            },
            "summary": "We introduce RealPlay, a neural network-based real-world game engine that\nenables interactive video generation from user control signals. Unlike prior\nworks focused on game-style visuals, RealPlay aims to produce photorealistic,\ntemporally consistent video sequences that resemble real-world footage. It\noperates in an interactive loop: users observe a generated scene, issue a\ncontrol command, and receive a short video chunk in response. To enable such\nrealistic and responsive generation, we address key challenges including\niterative chunk-wise prediction for low-latency feedback, temporal consistency\nacross iterations, and accurate control response. RealPlay is trained on a\ncombination of labeled game data and unlabeled real-world videos, without\nrequiring real-world action annotations. Notably, we observe two forms of\ngeneralization: (1) control transfer-RealPlay effectively maps control signals\nfrom virtual to real-world scenarios; and (2) entity transfer-although training\nlabels originate solely from a car racing game, RealPlay generalizes to control\ndiverse real-world entities, including bicycles and pedestrians, beyond\nvehicles. Project page can be found: https://wenqsun.github.io/RealPlay/",
            "upvotes": 7,
            "discussionId": "685a8f0f0e4ad7e219758732",
            "ai_summary": "RealPlay generates photorealistic, temporally consistent video sequences from user control signals through iterative prediction and generalizes to various real-world entities.",
            "ai_keywords": [
                "neural network-based real-world game engine",
                "interactive video generation",
                "control signals",
                "photorealistic",
                "temporally consistent",
                "video sequences",
                "chunk-wise prediction",
                "low-latency feedback",
                "temporal consistency",
                "control response",
                "labeled game data",
                "unlabeled real-world videos",
                "control transfer",
                "entity transfer",
                "car racing game",
                "bicycles",
                "pedestrians"
            ]
        },
        "publishedAt": "2025-06-23T13:59:53.000Z",
        "title": "From Virtual Games to Real-World Play",
        "summary": "We introduce RealPlay, a neural network-based real-world game engine that\nenables interactive video generation from user control signals. Unlike prior\nworks focused on game-style visuals, RealPlay aims to produce photorealistic,\ntemporally consistent video sequences that resemble real-world footage. It\noperates in an interactive loop: users observe a generated scene, issue a\ncontrol command, and receive a short video chunk in response. To enable such\nrealistic and responsive generation, we address key challenges including\niterative chunk-wise prediction for low-latency feedback, temporal consistency\nacross iterations, and accurate control response. RealPlay is trained on a\ncombination of labeled game data and unlabeled real-world videos, without\nrequiring real-world action annotations. Notably, we observe two forms of\ngeneralization: (1) control transfer-RealPlay effectively maps control signals\nfrom virtual to real-world scenarios; and (2) entity transfer-although training\nlabels originate solely from a car racing game, RealPlay generalizes to control\ndiverse real-world entities, including bicycles and pedestrians, beyond\nvehicles. Project page can be found: https://wenqsun.github.io/RealPlay/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18901.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64897b1f0ec897cfe579a399",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64897b1f0ec897cfe579a399/ICR_75b877BaSE94gjBuj.jpeg",
            "fullname": "wenq",
            "name": "wenqsun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18349",
            "authors": [
                {
                    "_id": "685a02460e4ad7e2197584a9",
                    "user": {
                        "_id": "659c6a50615d5e661222fe16",
                        "avatarUrl": "/avatars/8a946482e49a821dbe397dc3898f22c5.svg",
                        "isPro": false,
                        "fullname": "Zichong Li",
                        "user": "Pearush",
                        "type": "user"
                    },
                    "name": "Zichong Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:56.270Z",
                    "hidden": false
                },
                {
                    "_id": "685a02460e4ad7e2197584aa",
                    "name": "Chen Liang",
                    "hidden": false
                },
                {
                    "_id": "685a02460e4ad7e2197584ab",
                    "name": "Zixuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a02460e4ad7e2197584ac",
                    "name": "Ilgee Hong",
                    "hidden": false
                },
                {
                    "_id": "685a02460e4ad7e2197584ad",
                    "name": "Young Jin Kim",
                    "hidden": false
                },
                {
                    "_id": "685a02460e4ad7e2197584ae",
                    "name": "Weizhu Chen",
                    "hidden": false
                },
                {
                    "_id": "685a02460e4ad7e2197584af",
                    "name": "Tuo Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T07:15:59.000Z",
            "submittedOnDailyAt": "2025-06-24T02:19:52.155Z",
            "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation",
            "submittedOnDailyBy": {
                "_id": "63e6b5e22d2c508de9001afd",
                "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
                "isPro": false,
                "fullname": "Chen Liang",
                "user": "cliang1453",
                "type": "user"
            },
            "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
            "upvotes": 7,
            "discussionId": "685a02460e4ad7e2197584b0",
            "ai_summary": "SlimMoE compresses large MoE models into smaller, efficient variants using multi-stage compression without full retraining, maintaining competitive performance with significantly fewer resources.",
            "ai_keywords": [
                "Mixture of Experts (MoE)",
                "large language models (LLMs)",
                "parameter counts",
                "knowledge transfer",
                "one-shot pruning",
                "Phi 3.5-MoE",
                "Phi-mini-MoE",
                "Phi-tiny-MoE",
                "structured pruning",
                "staged distillation",
                "MMLU scores",
                "latency"
            ]
        },
        "publishedAt": "2025-06-23T03:15:59.000Z",
        "title": "SlimMoE: Structured Compression of Large MoE Models via Expert Slimming\n  and Distillation",
        "summary": "The Mixture of Experts (MoE) architecture has emerged as a powerful paradigm\nfor scaling large language models (LLMs) while maintaining inference\nefficiency. However, their enormous memory requirements make them prohibitively\nexpensive to fine-tune or deploy in resource-constrained environments. To\naddress this challenge, we introduce SlimMoE, a multi-stage compression\nframework for transforming large MoE models into much smaller, efficient\nvariants without incurring the prohibitive costs of training from scratch. Our\nmethod systematically reduces parameter counts by slimming experts and\ntransferring knowledge through intermediate stages, effectively mitigating the\nperformance degradation common in one-shot pruning approaches. Using this\nframework, we compress Phi 3.5-MoE (41.9B total/6.6B activated parameters) to\ncreate Phi-mini-MoE (7.6B total/2.4B activated parameters) and Phi-tiny-MoE\n(3.8B total/1.1B activated parameters) using only 400B tokens--less than 10% of\nthe original model's training data. These compressed models can be fine-tuned\non a single GPU (A100 for Phi-mini-MoE, A6000 for Phi-tiny-MoE), making them\nhighly suitable for academic and resource-limited settings. Our experiments\ndemonstrate that these compressed models outperform others of similar size and\nremain competitive with larger models. For instance, Phi-mini-MoE achieves\nsimilar or better performance to Phi-3-mini using only 2/3 of the activated\nparameters and yields comparable MMLU scores to Llama 3.1 8B despite having\nsignificantly lower latency. Our findings demonstrate that structured pruning\ncombined with staged distillation offers an effective path to creating\nhigh-quality, compact MoE models, paving the way for broader adoption of MoE\narchitectures. We make our models publicly available at\nhttps://huggingface.co/microsoft/Phi-mini-MoE-instruct and\nhttps://huggingface.co/microsoft/Phi-tiny-MoE-instruct .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18349.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63e6b5e22d2c508de9001afd",
            "avatarUrl": "/avatars/43cec9e8b8d490bd259e383954846a1e.svg",
            "fullname": "Chen Liang",
            "name": "cliang1453",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.16962",
            "authors": [
                {
                    "_id": "6858d907c0c8e29df8ea3ce2",
                    "user": {
                        "_id": "67547707f168984215451697",
                        "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
                        "isPro": false,
                        "fullname": "manglu",
                        "user": "manglu3935",
                        "type": "user"
                    },
                    "name": "Haoran Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-23T15:52:05.938Z",
                    "hidden": false
                },
                {
                    "_id": "6858d907c0c8e29df8ea3ce3",
                    "name": "Yankai Jiang",
                    "hidden": false
                },
                {
                    "_id": "6858d907c0c8e29df8ea3ce4",
                    "name": "Wenjie Lou",
                    "hidden": false
                },
                {
                    "_id": "6858d907c0c8e29df8ea3ce5",
                    "name": "Yujie Zhang",
                    "hidden": false
                },
                {
                    "_id": "6858d907c0c8e29df8ea3ce6",
                    "name": "Wenjie Li",
                    "hidden": false
                },
                {
                    "_id": "6858d907c0c8e29df8ea3ce7",
                    "name": "Lilong Wang",
                    "hidden": false
                },
                {
                    "_id": "6858d907c0c8e29df8ea3ce8",
                    "name": "Mianxin Liu",
                    "hidden": false
                },
                {
                    "_id": "6858d907c0c8e29df8ea3ce9",
                    "name": "Lei Liu",
                    "hidden": false
                },
                {
                    "_id": "6858d907c0c8e29df8ea3cea",
                    "name": "Xiaosong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-20T12:51:19.000Z",
            "submittedOnDailyAt": "2025-06-24T02:20:40.889Z",
            "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
            "submittedOnDailyBy": {
                "_id": "67547707f168984215451697",
                "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
                "isPro": false,
                "fullname": "manglu",
                "user": "manglu3935",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs",
            "upvotes": 7,
            "discussionId": "6858d907c0c8e29df8ea3ceb",
            "githubRepo": "https://github.com/manglu097/Chiron-o1",
            "ai_summary": "MICS, a novel reasoning-path searching scheme, enhances medical MLLMs like Chiron-o1 with robust generalizable reasoning and visual question-answering capabilities through comprehensive chain-of-thought data generation.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "chain-of-thought",
                "Mentor-Intern Collaborative Search",
                "MICS",
                "mentor models",
                "intern models",
                "MICS-Score",
                "multi-task medical reasoning dataset",
                "MMRP",
                "curriculum learning",
                "medical visual question answering",
                "reasoning benchmarks"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-06-20T08:51:19.000Z",
        "title": "Enhancing Step-by-Step and Verifiable Medical Reasoning in MLLMs",
        "summary": "Multimodal large language models (MLLMs) have begun to demonstrate robust\nreasoning capabilities on general tasks, yet their application in the medical\ndomain remains in its early stages. Constructing chain-of-thought (CoT)\ntraining data is essential for bolstering the reasoning abilities of medical\nMLLMs. However, existing approaches exhibit a deficiency in offering a\ncomprehensive framework for searching and evaluating effective reasoning paths\ntowards critical diagnosis. To address this challenge, we propose Mentor-Intern\nCollaborative Search (MICS), a novel reasoning-path searching scheme to\ngenerate rigorous and effective medical CoT data. MICS first leverages mentor\nmodels to initialize the reasoning, one step at a time, then prompts each\nintern model to continue the thinking along those initiated paths, and finally\nselects the optimal reasoning path according to the overall reasoning\nperformance of multiple intern models. The reasoning performance is determined\nby an MICS-Score, which assesses the quality of generated reasoning paths.\nEventually, we construct MMRP, a multi-task medical reasoning dataset with\nranked difficulty, and Chiron-o1, a new medical MLLM devised via a curriculum\nlearning strategy, with robust visual question-answering and generalizable\nreasoning capabilities. Extensive experiments demonstrate that Chiron-o1,\ntrained on our CoT dataset constructed using MICS, achieves state-of-the-art\nperformance across a list of medical visual question answering and reasoning\nbenchmarks. Codes are available at GitHub - manglu097/Chiron-o1: Enhancing\nStep-by-Step and Verifiable Medical Reasoning in MLLMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16962.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67547707f168984215451697",
            "avatarUrl": "/avatars/630329ed6585036d60cdc27490cc01b0.svg",
            "fullname": "manglu",
            "name": "manglu3935",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.16123",
            "authors": [
                {
                    "_id": "6859359b0e4ad7e21975834b",
                    "user": {
                        "_id": "64705d3890482b0e0f6591ed",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64705d3890482b0e0f6591ed/HqOaaRjzkXrC8POGtZYwh.jpeg",
                        "isPro": false,
                        "fullname": "Natapong Nitarach (Schwyter)",
                        "user": "natnitaract",
                        "type": "user"
                    },
                    "name": "Natapong Nitarach",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:09:20.191Z",
                    "hidden": false
                },
                {
                    "_id": "6859359b0e4ad7e21975834c",
                    "name": "Warit Sirichotedumrong",
                    "hidden": false
                },
                {
                    "_id": "6859359b0e4ad7e21975834d",
                    "name": "Panop Pitchayarthorn",
                    "hidden": false
                },
                {
                    "_id": "6859359b0e4ad7e21975834e",
                    "user": {
                        "_id": "615313b0793ef66b3324da1f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/615313b0793ef66b3324da1f/VyJniD3dxbV5a2CMgVVQ2.jpeg",
                        "isPro": false,
                        "fullname": "Pittawat Taveekitworachai",
                        "user": "pittawat",
                        "type": "user"
                    },
                    "name": "Pittawat Taveekitworachai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:14:47.952Z",
                    "hidden": false
                },
                {
                    "_id": "6859359b0e4ad7e21975834f",
                    "name": "Potsawee Manakul",
                    "hidden": false
                },
                {
                    "_id": "6859359b0e4ad7e219758350",
                    "name": "Kunat Pipatanakul",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64705d3890482b0e0f6591ed/QvDl8hs70u3p4q96oqSnq.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64705d3890482b0e0f6591ed/VKFKma2OMowAcWaMwRe0z.jpeg"
            ],
            "publishedAt": "2025-06-19T08:18:55.000Z",
            "submittedOnDailyAt": "2025-06-24T08:57:07.253Z",
            "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning",
            "submittedOnDailyBy": {
                "_id": "64705d3890482b0e0f6591ed",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64705d3890482b0e0f6591ed/HqOaaRjzkXrC8POGtZYwh.jpeg",
                "isPro": false,
                "fullname": "Natapong Nitarach (Schwyter)",
                "user": "natnitaract",
                "type": "user"
            },
            "summary": "This paper presents FinCoT, a structured chain-of-thought (CoT) prompting\napproach that incorporates insights from domain-specific expert financial\nreasoning to guide the reasoning traces of large language models. We\ninvestigate that there are three main prompting styles in FinNLP: (1) standard\nprompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an\nexplicit reasoning structure, such as the use of tags; and (3) structured CoT\nprompting--CoT prompting with explicit instructions or examples that define\nstructured reasoning steps. Previously, FinNLP has primarily focused on prompt\nengineering with either standard or unstructured CoT prompting. However,\nstructured CoT prompting has received limited attention in prior work.\nFurthermore, the design of reasoning structures in structured CoT prompting is\noften based on heuristics from non-domain experts. In this study, we\ninvestigate each prompting approach in FinNLP. We evaluate the three main\nprompting styles and FinCoT on CFA-style questions spanning ten financial\ndomains. We observe that FinCoT improves performance from 63.2% to 80.5% and\nQwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens\neight-fold compared to structured CoT prompting. Our findings show that\ndomain-aligned structured prompts not only improve performance and reduce\ninference costs but also yield more interpretable and expert-aligned reasoning\ntraces.",
            "upvotes": 7,
            "discussionId": "6859359b0e4ad7e219758351",
            "githubRepo": "https://github.com/nat-nischw/toward-agentic-finance-systems-2025",
            "ai_summary": "A structured chain-of-thought prompting method in financial natural language processing improves performance and reduces computational cost while enhancing interpretability.",
            "ai_keywords": [
                "chain-of-thought",
                "FinCoT",
                "standard prompting",
                "unstructured CoT",
                "structured CoT",
                "prompt engineering",
                "CFA-style questions",
                "domain-aligned",
                "inference costs"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-06-19T04:18:55.000Z",
        "title": "FinCoT: Grounding Chain-of-Thought in Expert Financial Reasoning",
        "summary": "This paper presents FinCoT, a structured chain-of-thought (CoT) prompting\napproach that incorporates insights from domain-specific expert financial\nreasoning to guide the reasoning traces of large language models. We\ninvestigate that there are three main prompting styles in FinNLP: (1) standard\nprompting--zero-shot prompting; (2) unstructured CoT--CoT prompting without an\nexplicit reasoning structure, such as the use of tags; and (3) structured CoT\nprompting--CoT prompting with explicit instructions or examples that define\nstructured reasoning steps. Previously, FinNLP has primarily focused on prompt\nengineering with either standard or unstructured CoT prompting. However,\nstructured CoT prompting has received limited attention in prior work.\nFurthermore, the design of reasoning structures in structured CoT prompting is\noften based on heuristics from non-domain experts. In this study, we\ninvestigate each prompting approach in FinNLP. We evaluate the three main\nprompting styles and FinCoT on CFA-style questions spanning ten financial\ndomains. We observe that FinCoT improves performance from 63.2% to 80.5% and\nQwen-2.5-7B-Instruct from 69.7% to 74.2%, while reducing generated tokens\neight-fold compared to structured CoT prompting. Our findings show that\ndomain-aligned structured prompts not only improve performance and reduce\ninference costs but also yield more interpretable and expert-aligned reasoning\ntraces.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64705d3890482b0e0f6591ed/QvDl8hs70u3p4q96oqSnq.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64705d3890482b0e0f6591ed/VKFKma2OMowAcWaMwRe0z.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16123.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64705d3890482b0e0f6591ed",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64705d3890482b0e0f6591ed/HqOaaRjzkXrC8POGtZYwh.jpeg",
            "fullname": "Natapong Nitarach (Schwyter)",
            "name": "natnitaract",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18904",
            "authors": [
                {
                    "_id": "685a9b770e4ad7e219758743",
                    "user": {
                        "_id": "66ef6fd0ea7d19a2399d6b1f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/rJIMhaB4NB8bvBtmsw1vA.png",
                        "isPro": false,
                        "fullname": "Yang Liu",
                        "user": "TeslaYang123",
                        "type": "user"
                    },
                    "name": "Yang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T13:33:35.287Z",
                    "hidden": false
                },
                {
                    "_id": "685a9b770e4ad7e219758744",
                    "user": {
                        "_id": "6423d96030b0e4ab36dcd5b7",
                        "avatarUrl": "/avatars/4e4a489771bcf5fbef1530e179e2e0c5.svg",
                        "isPro": false,
                        "fullname": "Chuanchen Luo",
                        "user": "LogicTrainer",
                        "type": "user"
                    },
                    "name": "Chuanchen Luo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:18:30.531Z",
                    "hidden": false
                },
                {
                    "_id": "685a9b770e4ad7e219758745",
                    "name": "Zimo Tang",
                    "hidden": false
                },
                {
                    "_id": "685a9b770e4ad7e219758746",
                    "name": "Yingyan Li",
                    "hidden": false
                },
                {
                    "_id": "685a9b770e4ad7e219758747",
                    "name": "Yuran Yang",
                    "hidden": false
                },
                {
                    "_id": "685a9b770e4ad7e219758748",
                    "name": "Yuanyong Ning",
                    "hidden": false
                },
                {
                    "_id": "685a9b770e4ad7e219758749",
                    "name": "Lue Fan",
                    "hidden": false
                },
                {
                    "_id": "685a9b770e4ad7e21975874a",
                    "user": {
                        "_id": "62c11a7360edb2dd7765b80d",
                        "avatarUrl": "/avatars/92db3258d0ba44daee22952f0644cd93.svg",
                        "isPro": false,
                        "fullname": "Junran Peng",
                        "user": "JrPeng",
                        "type": "user"
                    },
                    "name": "Junran Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:18:38.122Z",
                    "hidden": false
                },
                {
                    "_id": "685a9b770e4ad7e21975874b",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T17:59:58.000Z",
            "submittedOnDailyAt": "2025-06-24T11:06:40.281Z",
            "title": "TC-Light: Temporally Consistent Relighting for Dynamic Long Videos",
            "submittedOnDailyBy": {
                "_id": "6423d96030b0e4ab36dcd5b7",
                "avatarUrl": "/avatars/4e4a489771bcf5fbef1530e179e2e0c5.svg",
                "isPro": false,
                "fullname": "Chuanchen Luo",
                "user": "LogicTrainer",
                "type": "user"
            },
            "summary": "Editing illumination in long videos with complex dynamics has significant\nvalue in various downstream tasks, including visual content creation and\nmanipulation, as well as data scaling up for embodied AI through sim2real and\nreal2real transfer. Nevertheless, existing video relighting techniques are\npredominantly limited to portrait videos or fall into the bottleneck of\ntemporal consistency and computation efficiency. In this paper, we propose\nTC-Light, a novel paradigm characterized by the proposed two-stage post\noptimization mechanism. Starting from the video preliminarily relighted by an\ninflated video relighting model, it optimizes appearance embedding in the first\nstage to align global illumination. Then it optimizes the proposed canonical\nvideo representation, i.e., Unique Video Tensor (UVT), to align fine-grained\ntexture and lighting in the second stage. To comprehensively evaluate\nperformance, we also establish a long and highly dynamic video benchmark.\nExtensive experiments show that our method enables physically plausible\nrelighting results with superior temporal coherence and low computation cost.\nThe code and video demos are available at\nhttps://dekuliutesla.github.io/tclight/.",
            "upvotes": 6,
            "discussionId": "685a9b780e4ad7e21975874c",
            "projectPage": "https://dekuliutesla.github.io/tclight",
            "githubRepo": "https://github.com/Linketic/TC-Light",
            "ai_summary": "TC-Light, a novel two-stage video relighting model, achieves high temporal coherence and low computational cost through appearance embedding and unique video tensor optimization.",
            "ai_keywords": [
                "video relighting",
                "inflated video relighting model",
                "appearance embedding",
                "canonical video representation",
                "Unique Video Tensor",
                "UVT",
                "physically plausible relighting",
                "temporal coherence",
                "computational cost"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-06-23T13:59:58.000Z",
        "title": "TC-Light: Temporally Consistent Relighting for Dynamic Long Videos",
        "summary": "Editing illumination in long videos with complex dynamics has significant\nvalue in various downstream tasks, including visual content creation and\nmanipulation, as well as data scaling up for embodied AI through sim2real and\nreal2real transfer. Nevertheless, existing video relighting techniques are\npredominantly limited to portrait videos or fall into the bottleneck of\ntemporal consistency and computation efficiency. In this paper, we propose\nTC-Light, a novel paradigm characterized by the proposed two-stage post\noptimization mechanism. Starting from the video preliminarily relighted by an\ninflated video relighting model, it optimizes appearance embedding in the first\nstage to align global illumination. Then it optimizes the proposed canonical\nvideo representation, i.e., Unique Video Tensor (UVT), to align fine-grained\ntexture and lighting in the second stage. To comprehensively evaluate\nperformance, we also establish a long and highly dynamic video benchmark.\nExtensive experiments show that our method enables physically plausible\nrelighting results with superior temporal coherence and low computation cost.\nThe code and video demos are available at\nhttps://dekuliutesla.github.io/tclight/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18904.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6423d96030b0e4ab36dcd5b7",
            "avatarUrl": "/avatars/4e4a489771bcf5fbef1530e179e2e0c5.svg",
            "fullname": "Chuanchen Luo",
            "name": "LogicTrainer",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18631",
            "authors": [
                {
                    "_id": "685a1b390e4ad7e2197585a9",
                    "name": "Chenxing Wei",
                    "hidden": false
                },
                {
                    "_id": "685a1b390e4ad7e2197585aa",
                    "name": "Jiarui Yu",
                    "hidden": false
                },
                {
                    "_id": "685a1b390e4ad7e2197585ab",
                    "name": "Ying Tiffany He",
                    "hidden": false
                },
                {
                    "_id": "685a1b390e4ad7e2197585ac",
                    "name": "Hande Dong",
                    "hidden": false
                },
                {
                    "_id": "685a1b390e4ad7e2197585ad",
                    "name": "Yao Shu",
                    "hidden": false
                },
                {
                    "_id": "685a1b390e4ad7e2197585ae",
                    "name": "Fei Yu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
            ],
            "publishedAt": "2025-06-23T13:36:24.000Z",
            "submittedOnDailyAt": "2025-06-24T02:05:49.825Z",
            "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "65ed3051492a7f35db21fea2",
                "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
                "isPro": false,
                "fullname": "Chenxing Wei",
                "user": "kittttttt",
                "type": "user"
            },
            "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
            "upvotes": 6,
            "discussionId": "685a1b390e4ad7e2197585af",
            "githubRepo": "https://github.com/kithib/ReDit",
            "ai_summary": "ReDit, a reward dithering method, addresses issues in discrete reward systems by introducing noise, leading to smoother optimization and faster convergence compared to standard methods.",
            "ai_keywords": [
                "rule-based reward system",
                "reward hacking",
                "discrete rewards",
                "gradient anomaly",
                "unstable optimization",
                "slow convergence",
                "random noise",
                "exploratory gradients",
                "flat reward regions",
                "local optima",
                "vanilla GRPO",
                "performance improvement",
                "gradient issues"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-06-23T09:36:24.000Z",
        "title": "ReDit: Reward Dithering for Improved LLM Policy Optimization",
        "summary": "DeepSeek-R1 has successfully enhanced Large Language Model (LLM) reasoning\ncapabilities through its rule-based reward system. While it's a ''perfect''\nreward system that effectively mitigates reward hacking, such reward functions\nare often discrete. Our experimental observations suggest that discrete rewards\ncan lead to gradient anomaly, unstable optimization, and slow convergence. To\naddress this issue, we propose ReDit (Reward Dithering), a method that dithers\nthe discrete reward signal by adding simple random noise. With this perturbed\nreward, exploratory gradients are continuously provided throughout the learning\nprocess, enabling smoother gradient updates and accelerating convergence. The\ninjected noise also introduces stochasticity into flat reward regions,\nencouraging the model to explore novel policies and escape local optima.\nExperiments across diverse tasks demonstrate the effectiveness and efficiency\nof ReDit. On average, ReDit achieves performance comparable to vanilla GRPO\nwith only approximately 10% the training steps, and furthermore, still exhibits\na 4% performance improvement over vanilla GRPO when trained for a similar\nduration. Visualizations confirm significant mitigation of gradient issues with\nReDit. Moreover, theoretical analyses are provided to further validate these\nadvantages.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65ed3051492a7f35db21fea2/Oc7EWF5wDYqu1FEvfuAlq.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18631.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65ed3051492a7f35db21fea2",
            "avatarUrl": "/avatars/4fc0ccc21aa88e4e8ff74a6f850570b8.svg",
            "fullname": "Chenxing Wei",
            "name": "kittttttt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.17538",
            "authors": [
                {
                    "_id": "685a33c50e4ad7e219758612",
                    "name": "Yile Gu",
                    "hidden": false
                },
                {
                    "_id": "685a33c50e4ad7e219758613",
                    "name": "Rohan Kadekodi",
                    "hidden": false
                },
                {
                    "_id": "685a33c50e4ad7e219758614",
                    "name": "Hoang Nguyen",
                    "hidden": false
                },
                {
                    "_id": "685a33c50e4ad7e219758615",
                    "user": {
                        "_id": "6304ac1a412a1b9d381ca378",
                        "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
                        "isPro": false,
                        "fullname": "Keisuke Kamahori",
                        "user": "kamahori",
                        "type": "user"
                    },
                    "name": "Keisuke Kamahori",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:07:52.905Z",
                    "hidden": false
                },
                {
                    "_id": "685a33c50e4ad7e219758616",
                    "name": "Yiyu Liu",
                    "hidden": false
                },
                {
                    "_id": "685a33c50e4ad7e219758617",
                    "name": "Baris Kasikci",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-21T01:32:22.000Z",
            "submittedOnDailyAt": "2025-06-24T03:49:17.409Z",
            "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User\n  Devices",
            "submittedOnDailyBy": {
                "_id": "6304ac1a412a1b9d381ca378",
                "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
                "isPro": false,
                "fullname": "Keisuke Kamahori",
                "user": "kamahori",
                "type": "user"
            },
            "summary": "The recent shift in Generative AI (GenAI) applications from cloud-only\nenvironments to end-user devices introduces new challenges in resource\nmanagement, system efficiency, and user experience. This paper presents\nConsumerBench, a comprehensive benchmarking framework designed to evaluate the\nsystem efficiency and response time of GenAI models running on end-user\ndevices. Unlike existing benchmarks that assume exclusive model access on\ndedicated GPUs, ConsumerBench simulates realistic multi-application scenarios\nexecuting concurrently on constrained hardware. Furthermore, ConsumerBench\nsupports customizable workflows that simulate complex tasks requiring\ncoordination among multiple applications. ConsumerBench captures both\napplication-level metrics, including latency and Service Level Objective (SLO)\nattainment, and system-level metrics like CPU/GPU utilization and memory\nbandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies\nin resource sharing, unfair scheduling under greedy allocation, and performance\npitfalls of static model server configurations. The paper also provides\npractical insights for model developers and system designers, highlighting the\nbenefits of custom kernels tailored to consumer-grade GPU architectures and the\nvalue of implementing SLO-aware scheduling strategies.",
            "upvotes": 6,
            "discussionId": "685a33c70e4ad7e219758618",
            "githubRepo": "https://github.com/efeslab/ConsumerBench",
            "ai_summary": "ConsumerBench evaluates GenAI system efficiency and response time on end-user devices through a comprehensive benchmarking framework, emphasizing realistic multi-application scenarios and customizable workflows.",
            "ai_keywords": [
                "Generative AI",
                "ConsumerBench",
                "system efficiency",
                "response time",
                "benchmarking framework",
                "multi-application scenarios",
                "application-level metrics",
                "latency",
                "Service Level Objective",
                "SLO",
                "system-level metrics",
                "CPU utilization",
                "GPU utilization",
                "memory bandwidth",
                "greedy allocation",
                "static model server configurations",
                "custom kernels",
                "SLO-aware scheduling strategies"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-06-20T21:32:22.000Z",
        "title": "ConsumerBench: Benchmarking Generative AI Applications on End-User\n  Devices",
        "summary": "The recent shift in Generative AI (GenAI) applications from cloud-only\nenvironments to end-user devices introduces new challenges in resource\nmanagement, system efficiency, and user experience. This paper presents\nConsumerBench, a comprehensive benchmarking framework designed to evaluate the\nsystem efficiency and response time of GenAI models running on end-user\ndevices. Unlike existing benchmarks that assume exclusive model access on\ndedicated GPUs, ConsumerBench simulates realistic multi-application scenarios\nexecuting concurrently on constrained hardware. Furthermore, ConsumerBench\nsupports customizable workflows that simulate complex tasks requiring\ncoordination among multiple applications. ConsumerBench captures both\napplication-level metrics, including latency and Service Level Objective (SLO)\nattainment, and system-level metrics like CPU/GPU utilization and memory\nbandwidth. Through extensive experiments, ConsumerBench reveals inefficiencies\nin resource sharing, unfair scheduling under greedy allocation, and performance\npitfalls of static model server configurations. The paper also provides\npractical insights for model developers and system designers, highlighting the\nbenefits of custom kernels tailored to consumer-grade GPU architectures and the\nvalue of implementing SLO-aware scheduling strategies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17538.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6304ac1a412a1b9d381ca378",
            "avatarUrl": "/avatars/f4724eb5afc2a3b0e61e6da7bfa7be27.svg",
            "fullname": "Keisuke Kamahori",
            "name": "kamahori",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.16507",
            "authors": [
                {
                    "_id": "685a752f0e4ad7e21975870a",
                    "user": {
                        "_id": "639ccab166106be1436e1640",
                        "avatarUrl": "/avatars/1e3806e18ac427be20e93e5400f153d4.svg",
                        "isPro": false,
                        "fullname": "Pragya Srivastava",
                        "user": "pragsri8",
                        "type": "user"
                    },
                    "name": "Pragya Srivastava",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:33:25.360Z",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e21975870b",
                    "name": "Harman Singh",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e21975870c",
                    "name": "Rahul Madhavan",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e21975870d",
                    "user": {
                        "_id": "65fbbfa4dae495e776dfaf50",
                        "avatarUrl": "/avatars/75630742e1c3ae93f1ba9ee900e22cf0.svg",
                        "isPro": false,
                        "fullname": "Gandharv Patil",
                        "user": "gp02-mcgill",
                        "type": "user"
                    },
                    "name": "Gandharv Patil",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:33:41.718Z",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e21975870e",
                    "user": {
                        "_id": "654355390d2e27122a64d7db",
                        "avatarUrl": "/avatars/38f93647d1f98af387762781fbdff5de.svg",
                        "isPro": false,
                        "fullname": "Sravanti Addepalli",
                        "user": "a-sravanti",
                        "type": "user"
                    },
                    "name": "Sravanti Addepalli",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:33:47.759Z",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e21975870f",
                    "name": "Arun Suggala",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e219758710",
                    "name": "Rengarajan Aravamudhan",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e219758711",
                    "name": "Soumya Sharma",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e219758712",
                    "user": {
                        "_id": "660bc971a276be904ac1d5fc",
                        "avatarUrl": "/avatars/543a5d18710ba6260d5ea635f99f37ea.svg",
                        "isPro": false,
                        "fullname": "Anirban Laha",
                        "user": "anirbanl",
                        "type": "user"
                    },
                    "name": "Anirban Laha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:34:03.892Z",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e219758713",
                    "user": {
                        "_id": "6208f528ace0f815845c6de0",
                        "avatarUrl": "/avatars/6d31e68edb7fc1680e12e0939470a6e5.svg",
                        "isPro": false,
                        "fullname": "Aravindan R",
                        "user": "Aravindan",
                        "type": "user"
                    },
                    "name": "Aravindan Raghuveer",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:34:13.363Z",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e219758714",
                    "name": "Karthikeyan Shanmugam",
                    "hidden": false
                },
                {
                    "_id": "685a752f0e4ad7e219758715",
                    "name": "Doina Precup",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-19T17:59:47.000Z",
            "submittedOnDailyAt": "2025-06-24T08:24:29.234Z",
            "title": "Robust Reward Modeling via Causal Rubrics",
            "submittedOnDailyBy": {
                "_id": "639ccab166106be1436e1640",
                "avatarUrl": "/avatars/1e3806e18ac427be20e93e5400f153d4.svg",
                "isPro": false,
                "fullname": "Pragya Srivastava",
                "user": "pragsri8",
                "type": "user"
            },
            "summary": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)\nvia human feedback, yet they often suffer from reward hacking. They tend to\nlatch on to superficial or spurious attributes, such as response length or\nformatting, mistaking these cues learned from correlations in training data for\nthe true causal drivers of quality (e.g., factuality, relevance). This occurs\nbecause standard training objectives struggle to disentangle these factors,\nleading to brittle RMs and misaligned policies. We introduce Crome (Causally\nRobust Reward Modeling), a novel framework grounded in an explicit causal model\ndesigned to mitigate reward hacking. Crome employs the following synthetic\ntargeted augmentations during training: (1) Causal Augmentations, which are\npairs that differ along specific causal attributes, to enforce sensitivity\nalong each causal attribute individually, and (2) Neutral Augmentations, which\nare tie-label pairs varying primarily in spurious attributes, to enforce\ninvariance along spurious attributes. Notably, our augmentations are produced\nwithout any knowledge of spurious factors, via answer interventions only along\ncausal rubrics, that are identified by querying an oracle LLM. Empirically,\nCrome significantly outperforms standard baselines on RewardBench, improving\naverage accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in\nspecific categories. The robustness of Crome is further testified by the\nconsistent gains obtained in a Best-of-N inference setting across increasing N,\nacross various benchmarks, including the popular RewardBench (covering chat,\nchat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and\nthe reasoning-specific GSM8k.",
            "upvotes": 6,
            "discussionId": "685a752f0e4ad7e219758716",
            "ai_summary": "Crome, a novel reward modeling framework using causal and neutral augmentations, significantly improves the robustness and accuracy of reward models against reward hacking.",
            "ai_keywords": [
                "Reward models",
                "Large Language Models",
                "reward hacking",
                "causal model",
                "causal augmentations",
                "neutral augmentations",
                "answer interventions",
                "oracle LLM",
                "RewardBench",
                "WildGuardTest",
                "GSM8k",
                "Best-of-N inference"
            ]
        },
        "publishedAt": "2025-06-19T13:59:47.000Z",
        "title": "Robust Reward Modeling via Causal Rubrics",
        "summary": "Reward models (RMs) are fundamental to aligning Large Language Models (LLMs)\nvia human feedback, yet they often suffer from reward hacking. They tend to\nlatch on to superficial or spurious attributes, such as response length or\nformatting, mistaking these cues learned from correlations in training data for\nthe true causal drivers of quality (e.g., factuality, relevance). This occurs\nbecause standard training objectives struggle to disentangle these factors,\nleading to brittle RMs and misaligned policies. We introduce Crome (Causally\nRobust Reward Modeling), a novel framework grounded in an explicit causal model\ndesigned to mitigate reward hacking. Crome employs the following synthetic\ntargeted augmentations during training: (1) Causal Augmentations, which are\npairs that differ along specific causal attributes, to enforce sensitivity\nalong each causal attribute individually, and (2) Neutral Augmentations, which\nare tie-label pairs varying primarily in spurious attributes, to enforce\ninvariance along spurious attributes. Notably, our augmentations are produced\nwithout any knowledge of spurious factors, via answer interventions only along\ncausal rubrics, that are identified by querying an oracle LLM. Empirically,\nCrome significantly outperforms standard baselines on RewardBench, improving\naverage accuracy by up to 5.4% and achieving gains of up to 13.2% and 7.2% in\nspecific categories. The robustness of Crome is further testified by the\nconsistent gains obtained in a Best-of-N inference setting across increasing N,\nacross various benchmarks, including the popular RewardBench (covering chat,\nchat-hard, safety, and reasoning tasks), the safety-focused WildGuardTest, and\nthe reasoning-specific GSM8k.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16507.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "639ccab166106be1436e1640",
            "avatarUrl": "/avatars/1e3806e18ac427be20e93e5400f153d4.svg",
            "fullname": "Pragya Srivastava",
            "name": "pragsri8",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18887",
            "authors": [
                {
                    "_id": "685a176c0e4ad7e21975858b",
                    "user": {
                        "_id": "665ca52693b937f83e1b970e",
                        "avatarUrl": "/avatars/a5b202dadb31fb51688a0e5b7a482727.svg",
                        "isPro": false,
                        "fullname": "Vansh Sharma",
                        "user": "vanshs1",
                        "type": "user"
                    },
                    "name": "Vansh Sharma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:13.705Z",
                    "hidden": false
                },
                {
                    "_id": "685a176c0e4ad7e21975858c",
                    "name": "Venkat Raman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T17:56:34.000Z",
            "submittedOnDailyAt": "2025-06-24T11:02:54.467Z",
            "title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation",
            "submittedOnDailyBy": {
                "_id": "665ca52693b937f83e1b970e",
                "avatarUrl": "/avatars/a5b202dadb31fb51688a0e5b7a482727.svg",
                "isPro": false,
                "fullname": "Vansh Sharma",
                "user": "vanshs1",
                "type": "user"
            },
            "summary": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems.",
            "upvotes": 5,
            "discussionId": "685a176c0e4ad7e21975858d",
            "ai_summary": "A gradient-refined adaptive activation steering framework enables LLMs to reliably generate scientific code in a specific programming language by selectively steering latent subspaces.",
            "ai_keywords": [
                "latent subspaces",
                "language models (LLMs)",
                "causal LLMs",
                "scientific coding prompts",
                "neuron-attribution method",
                "MLP weight",
                "gradient-refined adaptive activation steering framework (G-ACT)",
                "steering directions",
                "per-layer probes",
                "probe classification accuracy",
                "attention-head signals",
                "concept-level control"
            ]
        },
        "publishedAt": "2025-06-23T13:56:34.000Z",
        "title": "Steering Conceptual Bias via Transformer Latent-Subspace Activation",
        "summary": "This work examines whether activating latent subspaces in language models\n(LLMs) can steer scientific code generation toward a specific programming\nlanguage. Five causal LLMs were first evaluated on scientific coding prompts to\nquantify their baseline bias among four programming languages. A static\nneuron-attribution method, perturbing the highest activated MLP weight for a\nC++ or CPP token, proved brittle and exhibited limited generalization across\nprompt styles and model scales. To address these limitations, a\ngradient-refined adaptive activation steering framework (G-ACT) was developed:\nper-prompt activation differences are clustered into a small set of steering\ndirections, and lightweight per-layer probes are trained and refined online to\nselect the appropriate steering vector. In LLaMA-3.2 3B, this approach reliably\nbiases generation towards the CPP language by increasing the average probe\nclassification accuracy by 15% and the early layers (0-6) improving the probe\nclassification accuracy by 61.5% compared to the standard ACT framework. For\nLLaMA-3.3 70B, where attention-head signals become more diffuse, targeted\ninjections at key layers still improve language selection. Although per-layer\nprobing introduces a modest inference overhead, it remains practical by\nsteering only a subset of layers and enables reproducible model behavior. These\nresults demonstrate a scalable, interpretable and efficient mechanism for\nconcept-level control for practical agentic systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18887.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "665ca52693b937f83e1b970e",
            "avatarUrl": "/avatars/a5b202dadb31fb51688a0e5b7a482727.svg",
            "fullname": "Vansh Sharma",
            "name": "vanshs1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18787",
            "authors": [
                {
                    "_id": "685a19100e4ad7e219758595",
                    "user": {
                        "_id": "624b4a964056e2a6914a05c5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672164046414-624b4a964056e2a6914a05c5.png",
                        "isPro": false,
                        "fullname": "Dylan Ebert",
                        "user": "dylanebert",
                        "type": "user"
                    },
                    "name": "Dylan Ebert",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T16:25:14.250Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/624b4a964056e2a6914a05c5/hUBsDTN6fgkLGwKdUB1jV.mp4"
            ],
            "publishedAt": "2025-06-23T15:57:10.000Z",
            "submittedOnDailyAt": "2025-06-24T15:03:06.441Z",
            "title": "3D Arena: An Open Platform for Generative 3D Evaluation",
            "submittedOnDailyBy": {
                "_id": "624b4a964056e2a6914a05c5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672164046414-624b4a964056e2a6914a05c5.png",
                "isPro": false,
                "fullname": "Dylan Ebert",
                "user": "dylanebert",
                "type": "user"
            },
            "summary": "Evaluating Generative 3D models remains challenging due to misalignment\nbetween automated metrics and human perception of quality. Current benchmarks\nrely on image-based metrics that ignore 3D structure or geometric measures that\nfail to capture perceptual appeal and real-world utility. To address this gap,\nwe present 3D Arena, an open platform for evaluating image-to-3D generation\nmodels through large-scale human preference collection using pairwise\ncomparisons.\n  Since launching in June 2024, the platform has collected 123,243 votes from\n8,096 users across 19 state-of-the-art models, establishing the largest human\npreference evaluation for Generative 3D. We contribute the iso3d dataset of 100\nevaluation prompts and demonstrate quality control achieving 99.75% user\nauthenticity through statistical fraud detection. Our ELO-based ranking system\nprovides reliable model assessment, with the platform becoming an established\nevaluation resource.\n  Through analysis of this preference data, we present insights into human\npreference patterns. Our findings reveal preferences for visual presentation\nfeatures, with Gaussian splat outputs achieving a 16.6 ELO advantage over\nmeshes and textured models receiving a 144.1 ELO advantage over untextured\nmodels. We provide recommendations for improving evaluation methods, including\nmulti-criteria assessment, task-oriented evaluation, and format-aware\ncomparison. The platform's community engagement establishes 3D Arena as a\nbenchmark for the field while advancing understanding of human-centered\nevaluation in Generative 3D.",
            "upvotes": 5,
            "discussionId": "685a19110e4ad7e219758596",
            "projectPage": "https://huggingface.co/spaces/dylanebert/3d-arena",
            "ai_summary": "3D Arena evaluates generative 3D models using human preferences, revealing insights into visual and textural features' impact on quality.",
            "ai_keywords": [
                "3D Arena",
                "human preference collection",
                "image-to-3D generation",
                "pairwise comparisons",
                "iso3d dataset",
                "quality control",
                "statistical fraud detection",
                "ELO-based ranking system",
                "Gaussian splat",
                "mesh",
                "textured models",
                "untextured models",
                "multi-criteria assessment",
                "task-oriented evaluation",
                "format-aware comparison"
            ]
        },
        "publishedAt": "2025-06-23T11:57:10.000Z",
        "title": "3D Arena: An Open Platform for Generative 3D Evaluation",
        "summary": "Evaluating Generative 3D models remains challenging due to misalignment\nbetween automated metrics and human perception of quality. Current benchmarks\nrely on image-based metrics that ignore 3D structure or geometric measures that\nfail to capture perceptual appeal and real-world utility. To address this gap,\nwe present 3D Arena, an open platform for evaluating image-to-3D generation\nmodels through large-scale human preference collection using pairwise\ncomparisons.\n  Since launching in June 2024, the platform has collected 123,243 votes from\n8,096 users across 19 state-of-the-art models, establishing the largest human\npreference evaluation for Generative 3D. We contribute the iso3d dataset of 100\nevaluation prompts and demonstrate quality control achieving 99.75% user\nauthenticity through statistical fraud detection. Our ELO-based ranking system\nprovides reliable model assessment, with the platform becoming an established\nevaluation resource.\n  Through analysis of this preference data, we present insights into human\npreference patterns. Our findings reveal preferences for visual presentation\nfeatures, with Gaussian splat outputs achieving a 16.6 ELO advantage over\nmeshes and textured models receiving a 144.1 ELO advantage over untextured\nmodels. We provide recommendations for improving evaluation methods, including\nmulti-criteria assessment, task-oriented evaluation, and format-aware\ncomparison. The platform's community engagement establishes 3D Arena as a\nbenchmark for the field while advancing understanding of human-centered\nevaluation in Generative 3D.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/624b4a964056e2a6914a05c5/hUBsDTN6fgkLGwKdUB1jV.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18787.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "624b4a964056e2a6914a05c5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672164046414-624b4a964056e2a6914a05c5.png",
            "fullname": "Dylan Ebert",
            "name": "dylanebert",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2760
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18527",
            "authors": [
                {
                    "_id": "685a0cba0e4ad7e21975850c",
                    "user": {
                        "_id": "64ccd5cc4726a3f833831087",
                        "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
                        "isPro": false,
                        "fullname": "Hu",
                        "user": "Jiakui",
                        "type": "user"
                    },
                    "name": "JiaKui Hu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:35:44.052Z",
                    "hidden": false
                },
                {
                    "_id": "685a0cba0e4ad7e21975850d",
                    "name": "Yuxiao Yang",
                    "hidden": false
                },
                {
                    "_id": "685a0cba0e4ad7e21975850e",
                    "user": {
                        "_id": "64df25e49bc64767a6411aa5",
                        "avatarUrl": "/avatars/301fb787186717f8a7654d82b6906ad9.svg",
                        "isPro": false,
                        "fullname": "Jialun Liu",
                        "user": "JialunLiu",
                        "type": "user"
                    },
                    "name": "Jialun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:35:50.679Z",
                    "hidden": false
                },
                {
                    "_id": "685a0cba0e4ad7e21975850f",
                    "name": "Jinbo Wu",
                    "hidden": false
                },
                {
                    "_id": "685a0cba0e4ad7e219758510",
                    "name": "Chen Zhao",
                    "hidden": false
                },
                {
                    "_id": "685a0cba0e4ad7e219758511",
                    "name": "Yanye Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T11:28:37.000Z",
            "submittedOnDailyAt": "2025-06-24T03:17:08.044Z",
            "title": "Auto-Regressively Generating Multi-View Consistent Images",
            "submittedOnDailyBy": {
                "_id": "64ccd5cc4726a3f833831087",
                "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
                "isPro": false,
                "fullname": "Hu",
                "user": "Jiakui",
                "type": "user"
            },
            "summary": "Generating multi-view images from human instructions is crucial for 3D\ncontent creation. The primary challenges involve maintaining consistency across\nmultiple views and effectively synthesizing shapes and textures under diverse\nconditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)\nmethod, which leverages an auto-regressive model to progressively generate\nconsistent multi-view images from arbitrary prompts. Firstly, the\nnext-token-prediction capability of the AR model significantly enhances its\neffectiveness in facilitating progressive multi-view synthesis. When generating\nwidely-separated views, MV-AR can utilize all its preceding views to extract\neffective reference information. Subsequently, we propose a unified model that\naccommodates various prompts via architecture designing and training\nstrategies. To address multiple conditions, we introduce condition injection\nmodules for text, camera pose, image, and shape. To manage multi-modal\nconditions simultaneously, a progressive training strategy is employed. This\nstrategy initially adopts the text-to-multi-view (t2mv) model as a baseline to\nenhance the development of a comprehensive X-to-multi-view (X2mv) model through\nthe randomly dropping and combining conditions. Finally, to alleviate the\noverfitting problem caused by limited high-quality data, we propose the\n\"Shuffle View\" data augmentation technique, thus significantly expanding the\ntraining data by several magnitudes. Experiments demonstrate the performance\nand versatility of our MV-AR, which consistently generates consistent\nmulti-view images across a range of conditions and performs on par with leading\ndiffusion-based multi-view image generation models. Code and models will be\nreleased at https://github.com/MILab-PKU/MVAR.",
            "upvotes": 5,
            "discussionId": "685a0cbb0e4ad7e219758512",
            "githubRepo": "https://github.com/MILab-PKU/MVAR",
            "ai_summary": "The Multi-View Auto-Regressive (MV-AR) method uses an auto-regressive model to generate consistent multi-view images from prompts, addressing challenges in shape and texture synthesis across diverse conditions.",
            "ai_keywords": [
                "Multi-View Auto-Regressive",
                "MV-AR",
                "auto-regressive model",
                "next-token-prediction",
                "condition injection modules",
                "text-to-multi-view",
                "X-to-multi-view",
                "progressive training strategy",
                "Shuffle View",
                "data augmentation"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-06-23T07:28:37.000Z",
        "title": "Auto-Regressively Generating Multi-View Consistent Images",
        "summary": "Generating multi-view images from human instructions is crucial for 3D\ncontent creation. The primary challenges involve maintaining consistency across\nmultiple views and effectively synthesizing shapes and textures under diverse\nconditions. In this paper, we propose the Multi-View Auto-Regressive (MV-AR)\nmethod, which leverages an auto-regressive model to progressively generate\nconsistent multi-view images from arbitrary prompts. Firstly, the\nnext-token-prediction capability of the AR model significantly enhances its\neffectiveness in facilitating progressive multi-view synthesis. When generating\nwidely-separated views, MV-AR can utilize all its preceding views to extract\neffective reference information. Subsequently, we propose a unified model that\naccommodates various prompts via architecture designing and training\nstrategies. To address multiple conditions, we introduce condition injection\nmodules for text, camera pose, image, and shape. To manage multi-modal\nconditions simultaneously, a progressive training strategy is employed. This\nstrategy initially adopts the text-to-multi-view (t2mv) model as a baseline to\nenhance the development of a comprehensive X-to-multi-view (X2mv) model through\nthe randomly dropping and combining conditions. Finally, to alleviate the\noverfitting problem caused by limited high-quality data, we propose the\n\"Shuffle View\" data augmentation technique, thus significantly expanding the\ntraining data by several magnitudes. Experiments demonstrate the performance\nand versatility of our MV-AR, which consistently generates consistent\nmulti-view images across a range of conditions and performs on par with leading\ndiffusion-based multi-view image generation models. Code and models will be\nreleased at https://github.com/MILab-PKU/MVAR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18527.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64ccd5cc4726a3f833831087",
            "avatarUrl": "/avatars/6364b1ebb1d06c68245f4c786fb07ee9.svg",
            "fullname": "Hu",
            "name": "Jiakui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.17673",
            "authors": [
                {
                    "_id": "685a5c8e0e4ad7e2197586c7",
                    "user": {
                        "_id": "64105805928400b416439f10",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
                        "isPro": false,
                        "fullname": "Seonglae Cho",
                        "user": "seonglae",
                        "type": "user"
                    },
                    "name": "Seonglae Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:09:45.757Z",
                    "hidden": false
                },
                {
                    "_id": "685a5c8e0e4ad7e2197586c8",
                    "name": "Harryn Oh",
                    "hidden": false
                },
                {
                    "_id": "685a5c8e0e4ad7e2197586c9",
                    "name": "Donghyun Lee",
                    "hidden": false
                },
                {
                    "_id": "685a5c8e0e4ad7e2197586ca",
                    "name": "Luis Eduardo Rodrigues Vieira",
                    "hidden": false
                },
                {
                    "_id": "685a5c8e0e4ad7e2197586cb",
                    "name": "Andrew Bermingham",
                    "hidden": false
                },
                {
                    "_id": "685a5c8e0e4ad7e2197586cc",
                    "name": "Ziad El Sayed",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-21T10:18:25.000Z",
            "submittedOnDailyAt": "2025-06-24T06:38:36.202Z",
            "title": "FaithfulSAE: Towards Capturing Faithful Features with Sparse\n  Autoencoders without External Dataset Dependencies",
            "submittedOnDailyBy": {
                "_id": "64105805928400b416439f10",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
                "isPro": false,
                "fullname": "Seonglae Cho",
                "user": "seonglae",
                "type": "user"
            },
            "summary": "Sparse Autoencoders (SAEs) have emerged as a promising solution for\ndecomposing large language model representations into interpretable features.\nHowever, Paulo and Belrose (2025) have highlighted instability across different\ninitialization seeds, and Heap et al. (2025) have pointed out that SAEs may not\ncapture model-internal features. These problems likely stem from training SAEs\non external datasets - either collected from the Web or generated by another\nmodel - which may contain out-of-distribution (OOD) data beyond the model's\ngeneralisation capabilities. This can result in hallucinated SAE features,\nwhich we term \"Fake Features\", that misrepresent the model's internal\nactivations. To address these issues, we propose FaithfulSAE, a method that\ntrains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we\ndemonstrate that training SAEs on less-OOD instruction datasets results in SAEs\nbeing more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained\non web-based datasets in the SAE probing task and exhibit a lower Fake Feature\nRatio in 5 out of 7 models. Overall, our approach eliminates the dependency on\nexternal datasets, advancing interpretability by better capturing\nmodel-internal features while highlighting the often neglected importance of\nSAE training datasets.",
            "upvotes": 5,
            "discussionId": "685a5c8e0e4ad7e2197586cd",
            "githubRepo": "https://github.com/seonglae/FaithfulSAE",
            "ai_summary": "FaithfulSAE improves Sparse Autoencoder stability and interpretability by training on synthetic datasets generated by the model itself, reducing the occurrence of fake features and out-of-distribution data issues.",
            "ai_keywords": [
                "Sparse Autoencoders",
                "SAEs",
                "interpretability",
                "instability",
                "initialization seeds",
                "model-internal features",
                "out-of-distribution",
                "OOD",
                "Fake Features",
                "SAE probing task",
                "synthetic dataset"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-06-21T06:18:25.000Z",
        "title": "FaithfulSAE: Towards Capturing Faithful Features with Sparse\n  Autoencoders without External Dataset Dependencies",
        "summary": "Sparse Autoencoders (SAEs) have emerged as a promising solution for\ndecomposing large language model representations into interpretable features.\nHowever, Paulo and Belrose (2025) have highlighted instability across different\ninitialization seeds, and Heap et al. (2025) have pointed out that SAEs may not\ncapture model-internal features. These problems likely stem from training SAEs\non external datasets - either collected from the Web or generated by another\nmodel - which may contain out-of-distribution (OOD) data beyond the model's\ngeneralisation capabilities. This can result in hallucinated SAE features,\nwhich we term \"Fake Features\", that misrepresent the model's internal\nactivations. To address these issues, we propose FaithfulSAE, a method that\ntrains SAEs on the model's own synthetic dataset. Using FaithfulSAEs, we\ndemonstrate that training SAEs on less-OOD instruction datasets results in SAEs\nbeing more stable across seeds. Notably, FaithfulSAEs outperform SAEs trained\non web-based datasets in the SAE probing task and exhibit a lower Fake Feature\nRatio in 5 out of 7 models. Overall, our approach eliminates the dependency on\nexternal datasets, advancing interpretability by better capturing\nmodel-internal features while highlighting the often neglected importance of\nSAE training datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17673.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64105805928400b416439f10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
            "fullname": "Seonglae Cho",
            "name": "seonglae",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18879",
            "authors": [
                {
                    "_id": "685a1a090e4ad7e21975859c",
                    "user": {
                        "_id": "62d09eb86a61a88ea0d83918",
                        "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
                        "isPro": false,
                        "fullname": "Junyan Li",
                        "user": "senfu",
                        "type": "user"
                    },
                    "name": "Junyan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:34:41.316Z",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e21975859d",
                    "name": "Yang Zhang",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e21975859e",
                    "name": "Muhammad Yusuf Hassan",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e21975859f",
                    "user": {
                        "_id": "6146f91079d11bc3c1714775",
                        "avatarUrl": "/avatars/418247a90388a59f5d1ee8899144b329.svg",
                        "isPro": false,
                        "fullname": "Talha Chafekar",
                        "user": "talha1503",
                        "type": "user"
                    },
                    "name": "Talha Chafekar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T13:35:02.649Z",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e2197585a0",
                    "name": "Tianle Cai",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e2197585a1",
                    "name": "Zhile Ren",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e2197585a2",
                    "name": "Pengsheng Guo",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e2197585a3",
                    "name": "Foroozan Karimzadeh",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e2197585a4",
                    "name": "Colorado Reed",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e2197585a5",
                    "name": "Chong Wang",
                    "hidden": false
                },
                {
                    "_id": "685a1a090e4ad7e2197585a6",
                    "name": "Chuang Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T17:50:11.000Z",
            "submittedOnDailyAt": "2025-06-24T01:53:55.727Z",
            "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
            "submittedOnDailyBy": {
                "_id": "62d09eb86a61a88ea0d83918",
                "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
                "isPro": false,
                "fullname": "Junyan Li",
                "user": "senfu",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
            "upvotes": 4,
            "discussionId": "685a1a090e4ad7e2197585a7",
            "ai_summary": "Commutative Vector Quantization (CommVQ) reduces memory usage in long-context LLM inference by compressing the KV cache with additive quantization and integration of Rotary Position Embedding (RoPE).",
            "ai_keywords": [
                "Commutative Vector Quantization",
                "CommVQ",
                "additive quantization",
                "codebook",
                "Rotary Position Embedding",
                "RoPE",
                "Expectation-Maximization",
                "self-attention",
                "FP16",
                "KV cache quantization",
                "GSM8K",
                "LLaMA-3.1 8B model",
                "RTX 4090 GPU"
            ]
        },
        "publishedAt": "2025-06-23T13:50:11.000Z",
        "title": "CommVQ: Commutative Vector Quantization for KV Cache Compression",
        "summary": "Large Language Models (LLMs) are increasingly used in applications requiring\nlong context lengths, but the key-value (KV) cache often becomes a memory\nbottleneck on GPUs as context grows. To address this, we propose Commutative\nVector Quantization (CommVQ) to significantly reduce memory usage for\nlong-context LLM inference. We first introduce additive quantization with a\nlightweight encoder and codebook to compress the KV cache, which can be decoded\nvia simple matrix multiplication. To further reduce computational costs during\ndecoding, we design the codebook to be commutative with Rotary Position\nEmbedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm.\nThis enables efficient integration of decoding into the self-attention\nmechanism. Our approach achieves high accuracy with additive quantization and\nlow overhead via the RoPE-commutative codebook. Experiments on long-context\nbenchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5%\nwith 2-bit quantization, while outperforming state-of-the-art KV cache\nquantization methods. Notably, it enables 1-bit KV cache quantization with\nminimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context\nlength on a single RTX 4090 GPU. The source code is available at:\nhttps://github.com/UMass-Embodied-AGI/CommVQ.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18879.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62d09eb86a61a88ea0d83918",
            "avatarUrl": "/avatars/81b511d94cced304ffca058caff662d4.svg",
            "fullname": "Junyan Li",
            "name": "senfu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.17871",
            "authors": [
                {
                    "_id": "685ac45648f3141afa7a3d50",
                    "name": "Chenghao Yang",
                    "hidden": false
                },
                {
                    "_id": "685ac45648f3141afa7a3d51",
                    "name": "Ari Holtzman",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62fb49bafcce44435d7e079a/yXjJx1ZsydJyWQmK_EBy0.mp4"
            ],
            "publishedAt": "2025-06-22T02:00:37.000Z",
            "submittedOnDailyAt": "2025-06-24T14:02:09.933Z",
            "title": "How Alignment Shrinks the Generative Horizon",
            "submittedOnDailyBy": {
                "_id": "62fb49bafcce44435d7e079a",
                "avatarUrl": "/avatars/116cb4371206ee7010e161c986b09e85.svg",
                "isPro": false,
                "fullname": "Chenghao Yang",
                "user": "chromeNLP",
                "type": "user"
            },
            "summary": "Despite their impressive capabilities, aligned large language models (LLMs)\noften generate outputs that lack diversity. What drives this stability in the\ngeneration? We investigate this phenomenon through the lens of probability\nconcentration in the model's output distribution. To quantify this\nconcentration, we introduce the Branching Factor (BF) -- a token-invariant\nmeasure of the effective number of plausible next steps during generation. Our\nempirical analysis reveals two key findings: (1) BF often decreases as\ngeneration progresses, suggesting that LLMs become more predictable as they\ngenerate. (2) alignment tuning substantially sharpens the model's output\ndistribution from the outset, reducing BF by nearly an order of magnitude\n(e.g., from 12 to 1.2) relative to base models. This stark reduction helps\nexplain why aligned models often appear less sensitive to decoding strategies.\nBuilding on this insight, we find this stability has surprising implications\nfor complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,\nDeepSeek-distilled models), for instance, leverage this effect; by generating\nlonger reasoning chains, they push generation into later, more deterministic\n(lower BF) stages, resulting in more stable outputs. We hypothesize that\nalignment tuning does not fundamentally change a model's behavior, but instead\nsteers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy\ntrajectories already present in the base model. This view is supported by\nnudging experiments, which show that prompting base models with such tokens can\nsimilarly reduce BF. Together, our findings establish BF as a powerful\ndiagnostic for understanding and controlling LLM outputs - clarifying how\nalignment reduces variability, how CoT promotes stable generations, and how\nbase models can be steered away from diversity.",
            "upvotes": 4,
            "discussionId": "685ac45648f3141afa7a3d52",
            "ai_summary": "The Branching Factor (BF) quantifies the effective number of plausible next steps during generation and reveals how alignment tuning and longer reasoning chains reduce variability in aligned large language models (LLMs).",
            "ai_keywords": [
                "Branching Factor (BF)",
                "token-invariant",
                "aligned large language models (LLMs)",
                "probability concentration",
                "output distribution",
                "aligned Chain-of-Thought (CoT)",
                "DeepSeek-distilled models",
                "stylistic tokens",
                "low-entropy trajectories"
            ]
        },
        "publishedAt": "2025-06-21T22:00:37.000Z",
        "title": "How Alignment Shrinks the Generative Horizon",
        "summary": "Despite their impressive capabilities, aligned large language models (LLMs)\noften generate outputs that lack diversity. What drives this stability in the\ngeneration? We investigate this phenomenon through the lens of probability\nconcentration in the model's output distribution. To quantify this\nconcentration, we introduce the Branching Factor (BF) -- a token-invariant\nmeasure of the effective number of plausible next steps during generation. Our\nempirical analysis reveals two key findings: (1) BF often decreases as\ngeneration progresses, suggesting that LLMs become more predictable as they\ngenerate. (2) alignment tuning substantially sharpens the model's output\ndistribution from the outset, reducing BF by nearly an order of magnitude\n(e.g., from 12 to 1.2) relative to base models. This stark reduction helps\nexplain why aligned models often appear less sensitive to decoding strategies.\nBuilding on this insight, we find this stability has surprising implications\nfor complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g.,\nDeepSeek-distilled models), for instance, leverage this effect; by generating\nlonger reasoning chains, they push generation into later, more deterministic\n(lower BF) stages, resulting in more stable outputs. We hypothesize that\nalignment tuning does not fundamentally change a model's behavior, but instead\nsteers it toward stylistic tokens (e.g., \"Sure\") that unlock low-entropy\ntrajectories already present in the base model. This view is supported by\nnudging experiments, which show that prompting base models with such tokens can\nsimilarly reduce BF. Together, our findings establish BF as a powerful\ndiagnostic for understanding and controlling LLM outputs - clarifying how\nalignment reduces variability, how CoT promotes stable generations, and how\nbase models can be steered away from diversity.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62fb49bafcce44435d7e079a/yXjJx1ZsydJyWQmK_EBy0.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17871.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62fb49bafcce44435d7e079a",
            "avatarUrl": "/avatars/116cb4371206ee7010e161c986b09e85.svg",
            "fullname": "Chenghao Yang",
            "name": "chromeNLP",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.17323",
            "authors": [
                {
                    "_id": "685a6c720e4ad7e2197586f2",
                    "name": "Tamas Bisztray",
                    "hidden": false
                },
                {
                    "_id": "685a6c720e4ad7e2197586f3",
                    "user": {
                        "_id": "64d3db80aea0ccb1b4975d95",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
                        "isPro": false,
                        "fullname": "Bilel Cherif",
                        "user": "Neo111x",
                        "type": "user"
                    },
                    "name": "Bilel Cherif",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T09:32:57.540Z",
                    "hidden": false
                },
                {
                    "_id": "685a6c720e4ad7e2197586f4",
                    "user": {
                        "_id": "66f1a481fa0f7d6823e95149",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66f1a481fa0f7d6823e95149/2_r8tJZSr95KkezLNDRu7.jpeg",
                        "isPro": false,
                        "fullname": "Richard A. Dubniczky",
                        "user": "dubniczky",
                        "type": "user"
                    },
                    "name": "Richard A. Dubniczky",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T14:43:49.522Z",
                    "hidden": false
                },
                {
                    "_id": "685a6c720e4ad7e2197586f5",
                    "name": "Nils Gruschka",
                    "hidden": false
                },
                {
                    "_id": "685a6c720e4ad7e2197586f6",
                    "name": "Bertalan Borsos",
                    "hidden": false
                },
                {
                    "_id": "685a6c720e4ad7e2197586f7",
                    "user": {
                        "_id": "64404fbc21ec7c165d859938",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64404fbc21ec7c165d859938/cM9oS5U44j6N5retQ6ZMX.jpeg",
                        "isPro": false,
                        "fullname": "Mohamed Amine Ferrag",
                        "user": "Ferrag",
                        "type": "user"
                    },
                    "name": "Mohamed Amine Ferrag",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T14:43:51.470Z",
                    "hidden": false
                },
                {
                    "_id": "685a6c720e4ad7e2197586f8",
                    "name": "Attila Kovacs",
                    "hidden": false
                },
                {
                    "_id": "685a6c720e4ad7e2197586f9",
                    "name": "Vasileios Mavroeidis",
                    "hidden": false
                },
                {
                    "_id": "685a6c720e4ad7e2197586fa",
                    "user": {
                        "_id": "65ae586052e1b2aae48e01fb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ae586052e1b2aae48e01fb/Ga6PC31XgXlfzCteqInNp.png",
                        "isPro": false,
                        "fullname": "Norbert Tihanyi",
                        "user": "tihanyin",
                        "type": "user"
                    },
                    "name": "Norbert Tihanyi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T13:36:01.107Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T19:49:41.000Z",
            "submittedOnDailyAt": "2025-06-24T07:45:13.848Z",
            "title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code\n  Stylometry for Authorship Attribution",
            "submittedOnDailyBy": {
                "_id": "64d3db80aea0ccb1b4975d95",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
                "isPro": false,
                "fullname": "Bilel Cherif",
                "user": "Neo111x",
                "type": "user"
            },
            "summary": "Detecting AI-generated code, deepfakes, and other synthetic content is an\nemerging research challenge. As code generated by Large Language Models (LLMs)\nbecomes more common, identifying the specific model behind each sample is\nincreasingly important. This paper presents the first systematic study of LLM\nauthorship attribution for C programs. We released CodeT5-Authorship, a novel\nmodel that uses only the encoder layers from the original CodeT5\nencoder-decoder architecture, discarding the decoder to focus on\nclassification. Our model's encoder output (first token) is passed through a\ntwo-layer classification head with GELU activation and dropout, producing a\nprobability distribution over possible authors. To evaluate our approach, we\nintroduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs\ngenerated by eight state-of-the-art LLMs across diverse tasks. We compare our\nmodel to seven traditional ML classifiers and eight fine-tuned transformer\nmodels, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,\nLongformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model\nachieves 97.56% accuracy in distinguishing C programs generated by closely\nrelated models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class\nattribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,\nGPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the\nCodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant\nGoogle Colab scripts on GitHub: https://github.com/LLMauthorbench/.",
            "upvotes": 4,
            "discussionId": "685a6c720e4ad7e2197586fb",
            "projectPage": "https://github.com/LLMauthorbench",
            "githubRepo": "https://github.com/LLMauthorbench/LLMauthorbench",
            "ai_summary": "A novel model, CodeT5-Authorship, is introduced to classify the authorship of C programs generated by Large Language Models, achieving high accuracy compared to traditional and transformer-based classifiers.",
            "ai_keywords": [
                "Large Language Models",
                "LLM authorship attribution",
                "CodeT5-Authorship",
                "encoder-decoder architecture",
                "GELU activation",
                "dropout",
                "LLM-AuthorBench",
                "traditional ML classifiers",
                "BERT",
                "RoBERTa",
                "CodeBERT",
                "ModernBERT",
                "DistilBERT",
                "DeBERTa-V3",
                "Longformer",
                "LoRA",
                "Qwen2-1.5B",
                "binary classification",
                "multi-class attribution"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-06-18T15:49:41.000Z",
        "title": "I Know Which LLM Wrote Your Code Last Summer: LLM generated Code\n  Stylometry for Authorship Attribution",
        "summary": "Detecting AI-generated code, deepfakes, and other synthetic content is an\nemerging research challenge. As code generated by Large Language Models (LLMs)\nbecomes more common, identifying the specific model behind each sample is\nincreasingly important. This paper presents the first systematic study of LLM\nauthorship attribution for C programs. We released CodeT5-Authorship, a novel\nmodel that uses only the encoder layers from the original CodeT5\nencoder-decoder architecture, discarding the decoder to focus on\nclassification. Our model's encoder output (first token) is passed through a\ntwo-layer classification head with GELU activation and dropout, producing a\nprobability distribution over possible authors. To evaluate our approach, we\nintroduce LLM-AuthorBench, a benchmark of 32,000 compilable C programs\ngenerated by eight state-of-the-art LLMs across diverse tasks. We compare our\nmodel to seven traditional ML classifiers and eight fine-tuned transformer\nmodels, including BERT, RoBERTa, CodeBERT, ModernBERT, DistilBERT, DeBERTa-V3,\nLongformer, and LoRA-fine-tuned Qwen2-1.5B. In binary classification, our model\nachieves 97.56% accuracy in distinguishing C programs generated by closely\nrelated models such as GPT-4.1 and GPT-4o, and 95.40% accuracy for multi-class\nattribution among five leading LLMs (Gemini 2.5 Flash, Claude 3.5 Haiku,\nGPT-4.1, Llama 3.3, and DeepSeek-V3). To support open science, we release the\nCodeT5-Authorship architecture, the LLM-AuthorBench benchmark, and all relevant\nGoogle Colab scripts on GitHub: https://github.com/LLMauthorbench/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17323.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64d3db80aea0ccb1b4975d95",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Mi0eKzNp6wKFrqketK-DN.png",
            "fullname": "Bilel Cherif",
            "name": "Neo111x",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.15645",
            "authors": [
                {
                    "_id": "6859a3f90e4ad7e219758453",
                    "name": "Shuo Xing",
                    "hidden": false
                },
                {
                    "_id": "6859a3f90e4ad7e219758454",
                    "user": {
                        "_id": "663b9ca3077aba104d588da4",
                        "avatarUrl": "/avatars/1de2d00e896ed609ffbdb4f8f32d5cc7.svg",
                        "isPro": false,
                        "fullname": "GUO LANQING",
                        "user": "Smurfsguo",
                        "type": "user"
                    },
                    "name": "Lanqing Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T17:44:39.718Z",
                    "hidden": false
                },
                {
                    "_id": "6859a3f90e4ad7e219758455",
                    "user": {
                        "_id": "64926a1d0b0ad03ceb6dd80e",
                        "avatarUrl": "/avatars/f4cf771734d0c3d3747f74ccf47d241d.svg",
                        "isPro": false,
                        "fullname": "hongyuan hua",
                        "user": "hongyuanhua",
                        "type": "user"
                    },
                    "name": "Hongyuan Hua",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T17:44:10.434Z",
                    "hidden": false
                },
                {
                    "_id": "6859a3f90e4ad7e219758456",
                    "name": "Seoyoung Lee",
                    "hidden": false
                },
                {
                    "_id": "6859a3f90e4ad7e219758457",
                    "name": "Peiran Li",
                    "hidden": false
                },
                {
                    "_id": "6859a3f90e4ad7e219758458",
                    "name": "Yufei Wang",
                    "hidden": false
                },
                {
                    "_id": "6859a3f90e4ad7e219758459",
                    "name": "Zhangyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6859a3f90e4ad7e21975845a",
                    "user": {
                        "_id": "62548d5fef3debb2ddf91217",
                        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
                        "isPro": false,
                        "fullname": "Zhengzhong Tu",
                        "user": "vztu",
                        "type": "user"
                    },
                    "name": "Zhengzhong Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T17:44:17.149Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T17:14:07.000Z",
            "submittedOnDailyAt": "2025-06-24T15:53:45.667Z",
            "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "62df9ee3c7e6f74fc92a1351",
                "avatarUrl": "/avatars/2a4f60ecc2fc4be2a8ac21c3f2ceb3cc.svg",
                "isPro": false,
                "fullname": "Shuo Xing",
                "user": "shuoxing",
                "type": "user"
            },
            "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
            "upvotes": 4,
            "discussionId": "6859a3fa0e4ad7e21975845b",
            "ai_summary": "Visual-Quality Test-Time Tuning (VQ-TTT) improves Multimodal Large Language Models (MLLMs) performance on vision-language tasks by dynamically adjusting input images, demonstrating the importance of adaptive rather than universally clean imagery.",
            "ai_keywords": [
                "Multimodal Large Language Models (MLLMs)",
                "vision-language tasks",
                "visual-quality paradox",
                "Visual-Quality Test-Time Tuning (VQ-TTT)",
                "learnable",
                "low-rank kernel",
                "vision encoder",
                "LoRA",
                "task-specific model preferences"
            ]
        },
        "publishedAt": "2025-06-18T13:14:07.000Z",
        "title": "Demystifying the Visual Quality Paradox in Multimodal Large Language\n  Models",
        "summary": "Recent Multimodal Large Language Models (MLLMs) excel on benchmark\nvision-language tasks, yet little is known about how input visual quality\nshapes their responses. Does higher perceptual quality of images already\ntranslate to better MLLM understanding? We conduct the first systematic study\nspanning leading MLLMs and a suite of vision-language benchmarks, applying\ncontrolled degradations and stylistic shifts to each image. Surprisingly, we\nuncover a visual-quality paradox: model, task, and even individual-instance\nperformance can improve when images deviate from human-perceived fidelity.\nOff-the-shelf restoration pipelines fail to reconcile these idiosyncratic\npreferences. To close the gap, we introduce Visual-Quality Test-Time Tuning\n(VQ-TTT)-a lightweight adaptation module that: (1) inserts a learnable,\nlow-rank kernel before the frozen vision encoder to modulate frequency content;\nand (2) fine-tunes only shallow vision-encoder layers via LoRA. VQ-TTT\ndynamically adjusts each input image in a single forward pass, aligning it with\ntask-specific model preferences. Across the evaluated MLLMs and all datasets,\nVQ-TTT lifts significant average accuracy, with no external models, cached\nfeatures, or extra training data. These findings redefine ``better'' visual\ninputs for MLLMs and highlight the need for adaptive, rather than universally\n``clean'', imagery, in the new era of AI being the main data customer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15645.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62df9ee3c7e6f74fc92a1351",
            "avatarUrl": "/avatars/2a4f60ecc2fc4be2a8ac21c3f2ceb3cc.svg",
            "fullname": "Shuo Xing",
            "name": "shuoxing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18890",
            "authors": [
                {
                    "_id": "685b168dd2ee4fac76521d68",
                    "name": "Ziqiao Ma",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d69",
                    "name": "Xuweiyi Chen",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d6a",
                    "name": "Shoubin Yu",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d6b",
                    "name": "Sai Bi",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d6c",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d6d",
                    "name": "Chen Ziwen",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d6e",
                    "name": "Sihan Xu",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d6f",
                    "name": "Jianing Yang",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d70",
                    "name": "Zexiang Xu",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d71",
                    "name": "Kalyan Sunkavalli",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d72",
                    "name": "Mohit Bansal",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d73",
                    "name": "Joyce Chai",
                    "hidden": false
                },
                {
                    "_id": "685b168dd2ee4fac76521d74",
                    "name": "Hao Tan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T17:57:47.000Z",
            "submittedOnDailyAt": "2025-06-24T19:52:59.823Z",
            "title": "4D-LRM: Large Space-Time Reconstruction Model From and To Any View at\n  Any Time",
            "submittedOnDailyBy": {
                "_id": "63ee8fb05f1300034de097fd",
                "avatarUrl": "/avatars/bceedb88927d8633948266503c2dd0b1.svg",
                "isPro": true,
                "fullname": "Yu",
                "user": "Shoubin",
                "type": "user"
            },
            "summary": "Can we scale 4D pretraining to learn general space-time representations that\nreconstruct an object from a few views at some times to any view at any time?\nWe provide an affirmative answer with 4D-LRM, the first large-scale 4D\nreconstruction model that takes input from unconstrained views and timestamps\nand renders arbitrary novel view-time combinations. Unlike prior 4D approaches,\ne.g., optimization-based, geometry-based, or generative, that struggle with\nefficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time\nrepresentation and directly predicts per-pixel 4D Gaussian primitives from\nposed image tokens across time, enabling fast, high-quality rendering at, in\nprinciple, infinite frame rate. Our results demonstrate that scaling\nspatiotemporal pretraining enables accurate and efficient 4D reconstruction. We\nshow that 4D-LRM generalizes to novel objects, interpolates across time, and\nhandles diverse camera setups. It reconstructs 24-frame sequences in one\nforward pass with less than 1.5 seconds on a single A100 GPU.",
            "upvotes": 3,
            "discussionId": "685b168dd2ee4fac76521d75",
            "ai_summary": "4D-LRM is a large-scale model that efficiently reconstructs objects from multiple views and times into any view-time combination using space-time representations and Gaussian primitives.",
            "ai_keywords": [
                "4D-LRM",
                "space-time representation",
                "Gaussian primitives",
                "posed image tokens",
                "4D reconstruction",
                "frame rate"
            ]
        },
        "publishedAt": "2025-06-23T13:57:47.000Z",
        "title": "4D-LRM: Large Space-Time Reconstruction Model From and To Any View at\n  Any Time",
        "summary": "Can we scale 4D pretraining to learn general space-time representations that\nreconstruct an object from a few views at some times to any view at any time?\nWe provide an affirmative answer with 4D-LRM, the first large-scale 4D\nreconstruction model that takes input from unconstrained views and timestamps\nand renders arbitrary novel view-time combinations. Unlike prior 4D approaches,\ne.g., optimization-based, geometry-based, or generative, that struggle with\nefficiency, generalization, or faithfulness, 4D-LRM learns a unified space-time\nrepresentation and directly predicts per-pixel 4D Gaussian primitives from\nposed image tokens across time, enabling fast, high-quality rendering at, in\nprinciple, infinite frame rate. Our results demonstrate that scaling\nspatiotemporal pretraining enables accurate and efficient 4D reconstruction. We\nshow that 4D-LRM generalizes to novel objects, interpolates across time, and\nhandles diverse camera setups. It reconstructs 24-frame sequences in one\nforward pass with less than 1.5 seconds on a single A100 GPU.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18890.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63ee8fb05f1300034de097fd",
            "avatarUrl": "/avatars/bceedb88927d8633948266503c2dd0b1.svg",
            "fullname": "Yu",
            "name": "Shoubin",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.17939",
            "authors": [
                {
                    "_id": "685a9d2c0e4ad7e219758756",
                    "user": {
                        "_id": "61e6dd8a82b19b93e1a51fa6",
                        "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg",
                        "isPro": false,
                        "fullname": "Kelvin Liu",
                        "user": "BoKelvin",
                        "type": "user"
                    },
                    "name": "Bo Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T14:43:47.534Z",
                    "hidden": false
                },
                {
                    "_id": "685a9d2c0e4ad7e219758757",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "685a9d2c0e4ad7e219758758",
                    "name": "Along He",
                    "hidden": false
                },
                {
                    "_id": "685a9d2c0e4ad7e219758759",
                    "name": "Yidi Chen",
                    "hidden": false
                },
                {
                    "_id": "685a9d2c0e4ad7e21975875a",
                    "name": "Huazhu Fu",
                    "hidden": false
                },
                {
                    "_id": "685a9d2c0e4ad7e21975875b",
                    "name": "Xiao-Ming Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-22T08:09:58.000Z",
            "submittedOnDailyAt": "2025-06-24T13:20:00.357Z",
            "title": "GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "61e6dd8a82b19b93e1a51fa6",
                "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg",
                "isPro": false,
                "fullname": "Kelvin Liu",
                "user": "BoKelvin",
                "type": "user"
            },
            "summary": "Medical visual question answering aims to support clinical decision-making by\nenabling models to answer natural language questions based on medical images.\nWhile recent advances in multi-modal learning have significantly improved\nperformance, current methods still suffer from limited answer reliability and\npoor interpretability, impairing the ability of clinicians and patients to\nunderstand and trust model-generated answers. To address this, this work first\nproposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer\ngeneration is decomposed into intermediate reasoning steps that explicitly\nground relevant visual regions of the medical image, thereby providing\nfine-grained explainability. Furthermore, we introduce a novel verifiable\nreward mechanism for reinforcement learning to guide post-training, improving\nthe alignment between the model's reasoning process and its final answer.\nRemarkably, our method achieves comparable performance using only one-eighth of\nthe training data, demonstrating the efficiency and effectiveness of the\nproposal. The dataset is available at\nhttps://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.",
            "upvotes": 3,
            "discussionId": "685a9d2c0e4ad7e21975875c",
            "ai_summary": "A novel dataset and verifiable reward mechanism enhance the explainability and efficiency of medical visual question answering models.",
            "ai_keywords": [
                "Thinking with Visual Grounding",
                "verifiable reward mechanism",
                "reinforcement learning",
                "intermediate reasoning steps",
                "explainability",
                "medical visual question answering"
            ]
        },
        "publishedAt": "2025-06-22T04:09:58.000Z",
        "title": "GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via\n  Reinforcement Learning",
        "summary": "Medical visual question answering aims to support clinical decision-making by\nenabling models to answer natural language questions based on medical images.\nWhile recent advances in multi-modal learning have significantly improved\nperformance, current methods still suffer from limited answer reliability and\npoor interpretability, impairing the ability of clinicians and patients to\nunderstand and trust model-generated answers. To address this, this work first\nproposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer\ngeneration is decomposed into intermediate reasoning steps that explicitly\nground relevant visual regions of the medical image, thereby providing\nfine-grained explainability. Furthermore, we introduce a novel verifiable\nreward mechanism for reinforcement learning to guide post-training, improving\nthe alignment between the model's reasoning process and its final answer.\nRemarkably, our method achieves comparable performance using only one-eighth of\nthe training data, demonstrating the efficiency and effectiveness of the\nproposal. The dataset is available at\nhttps://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17939.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61e6dd8a82b19b93e1a51fa6",
            "avatarUrl": "/avatars/babbee52793a35dd5754d000946dd1ee.svg",
            "fullname": "Kelvin Liu",
            "name": "BoKelvin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.17818",
            "authors": [
                {
                    "_id": "685a887b0e4ad7e219758725",
                    "user": {
                        "_id": "640e517d612e7f36f90f0016",
                        "avatarUrl": "/avatars/48104e174fe028aef246e889e468f940.svg",
                        "isPro": false,
                        "fullname": "Angelos-Nikolaos Kanatas",
                        "user": "akanatas",
                        "type": "user"
                    },
                    "name": "Angelos-Nikolaos Kanatas",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T12:43:08.653Z",
                    "hidden": false
                },
                {
                    "_id": "685a887b0e4ad7e219758726",
                    "user": {
                        "_id": "65c0ae0cfdb8d33cef6947cf",
                        "avatarUrl": "/avatars/60cdbf1fc85d433d0fb4ff4a240f39cd.svg",
                        "isPro": false,
                        "fullname": "Charis Papaioannou",
                        "user": "pxaris",
                        "type": "user"
                    },
                    "name": "Charilaos Papaioannou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T16:25:12.261Z",
                    "hidden": false
                },
                {
                    "_id": "685a887b0e4ad7e219758727",
                    "name": "Alexandros Potamianos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-21T21:16:39.000Z",
            "submittedOnDailyAt": "2025-06-24T11:35:19.057Z",
            "title": "CultureMERT: Continual Pre-Training for Cross-Cultural Music\n  Representation Learning",
            "submittedOnDailyBy": {
                "_id": "640e517d612e7f36f90f0016",
                "avatarUrl": "/avatars/48104e174fe028aef246e889e468f940.svg",
                "isPro": false,
                "fullname": "Angelos-Nikolaos Kanatas",
                "user": "akanatas",
                "type": "user"
            },
            "summary": "Recent advances in music foundation models have improved audio representation\nlearning, yet their effectiveness across diverse musical traditions remains\nlimited. We introduce CultureMERT-95M, a multi-culturally adapted foundation\nmodel developed to enhance cross-cultural music representation learning and\nunderstanding. To achieve this, we propose a two-stage continual pre-training\nstrategy that integrates learning rate re-warming and re-decaying, enabling\nstable adaptation even with limited computational resources. Training on a\n650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music\ntraditions, results in an average improvement of 4.9% in ROC-AUC and AP across\ndiverse non-Western music auto-tagging tasks, surpassing prior\nstate-of-the-art, with minimal forgetting on Western-centric benchmarks. We\nfurther investigate task arithmetic, an alternative approach to multi-cultural\nadaptation that merges single-culture adapted models in the weight space. Task\narithmetic performs on par with our multi-culturally trained model on\nnon-Western auto-tagging tasks and shows no regression on Western datasets.\nCross-cultural evaluation reveals that single-culture models transfer with\nvarying effectiveness across musical traditions, whereas the multi-culturally\nadapted model achieves the best overall performance. To support research on\nworld music representation learning, we publicly release CultureMERT-95M and\nCultureMERT-TA-95M, fostering the development of more culturally aware music\nfoundation models.",
            "upvotes": 3,
            "discussionId": "685a887c0e4ad7e219758728",
            "ai_summary": "CultureMERT-95M, a multi-culturally adapted foundation model, enhances cross-cultural music representation learning with a two-stage continual pre-training strategy, demonstrating superior performance in diverse non-Western music tasks.",
            "ai_keywords": [
                "Music foundation models",
                "audio representation learning",
                "CultureMERT-95M",
                "two-stage continual pre-training",
                "learning rate re-warming",
                "learning rate re-decaying",
                "multi-cultural data mix",
                "ROC-AUC",
                "AP",
                "non-Western music auto-tagging tasks",
                "task arithmetic",
                "single-culture adapted models",
                "cross-cultural evaluation",
                "CultureMERT-TA-95M"
            ]
        },
        "publishedAt": "2025-06-21T17:16:39.000Z",
        "title": "CultureMERT: Continual Pre-Training for Cross-Cultural Music\n  Representation Learning",
        "summary": "Recent advances in music foundation models have improved audio representation\nlearning, yet their effectiveness across diverse musical traditions remains\nlimited. We introduce CultureMERT-95M, a multi-culturally adapted foundation\nmodel developed to enhance cross-cultural music representation learning and\nunderstanding. To achieve this, we propose a two-stage continual pre-training\nstrategy that integrates learning rate re-warming and re-decaying, enabling\nstable adaptation even with limited computational resources. Training on a\n650-hour multi-cultural data mix, comprising Greek, Turkish, and Indian music\ntraditions, results in an average improvement of 4.9% in ROC-AUC and AP across\ndiverse non-Western music auto-tagging tasks, surpassing prior\nstate-of-the-art, with minimal forgetting on Western-centric benchmarks. We\nfurther investigate task arithmetic, an alternative approach to multi-cultural\nadaptation that merges single-culture adapted models in the weight space. Task\narithmetic performs on par with our multi-culturally trained model on\nnon-Western auto-tagging tasks and shows no regression on Western datasets.\nCross-cultural evaluation reveals that single-culture models transfer with\nvarying effectiveness across musical traditions, whereas the multi-culturally\nadapted model achieves the best overall performance. To support research on\nworld music representation learning, we publicly release CultureMERT-95M and\nCultureMERT-TA-95M, fostering the development of more culturally aware music\nfoundation models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17818.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "640e517d612e7f36f90f0016",
            "avatarUrl": "/avatars/48104e174fe028aef246e889e468f940.svg",
            "fullname": "Angelos-Nikolaos Kanatas",
            "name": "akanatas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.10597",
            "authors": [
                {
                    "_id": "685a0fb40e4ad7e219758522",
                    "user": {
                        "_id": "6601853162471e0981261241",
                        "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
                        "isPro": false,
                        "fullname": "XunguangWang",
                        "user": "xunguangwang",
                        "type": "user"
                    },
                    "name": "Xunguang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T17:43:21.005Z",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e219758523",
                    "name": "Zhenlan Ji",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e219758524",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e219758525",
                    "name": "Zongjie Li",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e219758526",
                    "name": "Daoyuan Wu",
                    "hidden": false
                },
                {
                    "_id": "685a0fb40e4ad7e219758527",
                    "name": "Shuai Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-12T11:42:40.000Z",
            "submittedOnDailyAt": "2025-06-24T01:16:03.239Z",
            "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6601853162471e0981261241",
                "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
                "isPro": false,
                "fullname": "XunguangWang",
                "user": "xunguangwang",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.",
            "upvotes": 3,
            "discussionId": "685a0fb40e4ad7e219758533",
            "ai_summary": "A systematic analysis and evaluation framework for jailbreak guardrails in Large Language Models is presented, categorizing and assessing their effectiveness and optimization potential.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "jailbreak attacks",
                "guardrails",
                "Security-Efficiency-Utility framework",
                "multi-dimensional taxonomy"
            ]
        },
        "publishedAt": "2025-06-12T07:42:40.000Z",
        "title": "SoK: Evaluating Jailbreak Guardrails for Large Language Models",
        "summary": "Large Language Models (LLMs) have achieved remarkable progress, but their\ndeployment has exposed critical vulnerabilities, particularly to jailbreak\nattacks that circumvent safety mechanisms. Guardrails--external defense\nmechanisms that monitor and control LLM interaction--have emerged as a\npromising solution. However, the current landscape of LLM guardrails is\nfragmented, lacking a unified taxonomy and comprehensive evaluation framework.\nIn this Systematization of Knowledge (SoK) paper, we present the first holistic\nanalysis of jailbreak guardrails for LLMs. We propose a novel,\nmulti-dimensional taxonomy that categorizes guardrails along six key\ndimensions, and introduce a Security-Efficiency-Utility evaluation framework to\nassess their practical effectiveness. Through extensive analysis and\nexperiments, we identify the strengths and limitations of existing guardrail\napproaches, explore their universality across attack types, and provide\ninsights into optimizing defense combinations. Our work offers a structured\nfoundation for future research and development, aiming to guide the principled\nadvancement and deployment of robust LLM guardrails. The code is available at\nhttps://github.com/xunguangwang/SoK4JailbreakGuardrails.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.10597.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6601853162471e0981261241",
            "avatarUrl": "/avatars/ccd1c5ce9d2f6fe7c2aff80fd9c39270.svg",
            "fullname": "XunguangWang",
            "name": "xunguangwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18900",
            "authors": [
                {
                    "_id": "685ab2ad0e4ad7e21975879b",
                    "user": {
                        "_id": "6658e05bb20730965d6d3046",
                        "avatarUrl": "/avatars/ca295711e0795d956025265ba1598acd.svg",
                        "isPro": false,
                        "fullname": "Kiymet Akdemir",
                        "user": "kakdemir",
                        "type": "user"
                    },
                    "name": "Kiymet Akdemir",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T17:43:52.586Z",
                    "hidden": false
                },
                {
                    "_id": "685ab2ad0e4ad7e21975879c",
                    "user": {
                        "_id": "66fec311343c151be4fb0b73",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png",
                        "isPro": false,
                        "fullname": "Tahira Kazimi",
                        "user": "tahirakazimi77",
                        "type": "user"
                    },
                    "name": "Tahira Kazimi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T14:43:45.414Z",
                    "hidden": false
                },
                {
                    "_id": "685ab2ad0e4ad7e21975879d",
                    "name": "Pinar Yanardag",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66fec311343c151be4fb0b73/QEXzI7qA7Xdb6ykKyNnQ9.png",
                "https://cdn-uploads.huggingface.co/production/uploads/66fec311343c151be4fb0b73/MIV5spf7JSEY2cDYUsPZ8.png"
            ],
            "publishedAt": "2025-06-23T17:59:29.000Z",
            "submittedOnDailyAt": "2025-06-24T14:04:27.452Z",
            "title": "Audit & Repair: An Agentic Framework for Consistent Story Visualization\n  in Text-to-Image Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "66fec311343c151be4fb0b73",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png",
                "isPro": false,
                "fullname": "Tahira Kazimi",
                "user": "tahirakazimi77",
                "type": "user"
            },
            "summary": "Story visualization has become a popular task where visual scenes are\ngenerated to depict a narrative across multiple panels. A central challenge in\nthis setting is maintaining visual consistency, particularly in how characters\nand objects persist and evolve throughout the story. Despite recent advances in\ndiffusion models, current approaches often fail to preserve key character\nattributes, leading to incoherent narratives. In this work, we propose a\ncollaborative multi-agent framework that autonomously identifies, corrects, and\nrefines inconsistencies across multi-panel story visualizations. The agents\noperate in an iterative loop, enabling fine-grained, panel-level updates\nwithout re-generating entire sequences. Our framework is model-agnostic and\nflexibly integrates with a variety of diffusion models, including rectified\nflow transformers such as Flux and latent diffusion models such as Stable\nDiffusion. Quantitative and qualitative experiments show that our method\noutperforms prior approaches in terms of multi-panel consistency.",
            "upvotes": 2,
            "discussionId": "685ab2ae0e4ad7e21975879e",
            "projectPage": "https://auditandrepair.github.io/",
            "ai_summary": "A collaborative multi-agent framework improves consistency in multi-panel story visualizations by refining inconsistencies across panels using diffusion models like Flux and Stable Diffusion.",
            "ai_keywords": [
                "collaborative multi-agent framework",
                "diffusion models",
                "rectified flow transformers",
                "Flux",
                "latent diffusion models",
                "Stable Diffusion",
                "multi-panel consistency"
            ]
        },
        "publishedAt": "2025-06-23T13:59:29.000Z",
        "title": "Audit & Repair: An Agentic Framework for Consistent Story Visualization\n  in Text-to-Image Diffusion Models",
        "summary": "Story visualization has become a popular task where visual scenes are\ngenerated to depict a narrative across multiple panels. A central challenge in\nthis setting is maintaining visual consistency, particularly in how characters\nand objects persist and evolve throughout the story. Despite recent advances in\ndiffusion models, current approaches often fail to preserve key character\nattributes, leading to incoherent narratives. In this work, we propose a\ncollaborative multi-agent framework that autonomously identifies, corrects, and\nrefines inconsistencies across multi-panel story visualizations. The agents\noperate in an iterative loop, enabling fine-grained, panel-level updates\nwithout re-generating entire sequences. Our framework is model-agnostic and\nflexibly integrates with a variety of diffusion models, including rectified\nflow transformers such as Flux and latent diffusion models such as Stable\nDiffusion. Quantitative and qualitative experiments show that our method\noutperforms prior approaches in terms of multi-panel consistency.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66fec311343c151be4fb0b73/QEXzI7qA7Xdb6ykKyNnQ9.png",
            "https://cdn-uploads.huggingface.co/production/uploads/66fec311343c151be4fb0b73/MIV5spf7JSEY2cDYUsPZ8.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18900.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66fec311343c151be4fb0b73",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/FXeHseCn4DKSVUzk-VKdA.png",
            "fullname": "Tahira Kazimi",
            "name": "tahirakazimi77",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18369",
            "authors": [
                {
                    "_id": "685b1e9bd2ee4fac76521d87",
                    "name": "Yeongtak Oh",
                    "hidden": false
                },
                {
                    "_id": "685b1e9bd2ee4fac76521d88",
                    "name": "Jisoo Mok",
                    "hidden": false
                },
                {
                    "_id": "685b1e9bd2ee4fac76521d89",
                    "name": "Dohyun Chung",
                    "hidden": false
                },
                {
                    "_id": "685b1e9bd2ee4fac76521d8a",
                    "name": "Juhyeon Shin",
                    "hidden": false
                },
                {
                    "_id": "685b1e9bd2ee4fac76521d8b",
                    "name": "Sangha Park",
                    "hidden": false
                },
                {
                    "_id": "685b1e9bd2ee4fac76521d8c",
                    "name": "Johan Barthelemy",
                    "hidden": false
                },
                {
                    "_id": "685b1e9bd2ee4fac76521d8d",
                    "name": "Sungroh Yoon",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T07:55:52.000Z",
            "submittedOnDailyAt": "2025-06-24T20:25:21.085Z",
            "title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "636f6e8a31af06da86499ebc",
                "avatarUrl": "/avatars/9430fbc05774aad8e46c0861769b3c30.svg",
                "isPro": false,
                "fullname": "Yeongtak",
                "user": "Yeongtak",
                "type": "user"
            },
            "summary": "Recent multi-modal large language models (MLLMs) often struggle to generate\npersonalized image captions, even when trained on high-quality captions. In\nthis work, we observe that such limitations persist in existing\npost-training-based MLLM personalization methods. Specifically, despite being\npost-tuned with large-scale caption data through supervised fine-tuning (SFT),\nthese models frequently fail to produce faithful descriptions in real-world\nscenarios, such as multi-concept image captioning. However, acquiring\nlarge-scale, high-quality captions for such complex settings is both costly and\ndifficult. To address the data-centric nature of SFT, we propose a\nreinforcement learning (RL)-based post-training framework. To the best of our\nknowledge, this is the first RL-based approach to post-train MLLMs for\npersonalized image captioning. Our method significantly enhances both visual\nrecognition and personalized generation capabilities of MLLMs, and consistently\noutperforms existing SFT-based baselines, especially in the challenging\nmulti-concept image captioning task.",
            "upvotes": 2,
            "discussionId": "685b1e9bd2ee4fac76521d8e",
            "ai_summary": "A reinforcement learning-based post-training framework improves the personalized image captioning capabilities of multi-modal large language models compared to supervised fine-tuning methods.",
            "ai_keywords": [
                "multi-modal large language models",
                "personalized image captions",
                "supervised fine-tuning",
                "reinforcement learning",
                "visual recognition"
            ]
        },
        "publishedAt": "2025-06-23T03:55:52.000Z",
        "title": "RePIC: Reinforced Post-Training for Personalizing Multi-Modal Language\n  Models",
        "summary": "Recent multi-modal large language models (MLLMs) often struggle to generate\npersonalized image captions, even when trained on high-quality captions. In\nthis work, we observe that such limitations persist in existing\npost-training-based MLLM personalization methods. Specifically, despite being\npost-tuned with large-scale caption data through supervised fine-tuning (SFT),\nthese models frequently fail to produce faithful descriptions in real-world\nscenarios, such as multi-concept image captioning. However, acquiring\nlarge-scale, high-quality captions for such complex settings is both costly and\ndifficult. To address the data-centric nature of SFT, we propose a\nreinforcement learning (RL)-based post-training framework. To the best of our\nknowledge, this is the first RL-based approach to post-train MLLMs for\npersonalized image captioning. Our method significantly enhances both visual\nrecognition and personalized generation capabilities of MLLMs, and consistently\noutperforms existing SFT-based baselines, especially in the challenging\nmulti-concept image captioning task.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18369.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "636f6e8a31af06da86499ebc",
            "avatarUrl": "/avatars/9430fbc05774aad8e46c0861769b3c30.svg",
            "fullname": "Yeongtak",
            "name": "Yeongtak",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.16929",
            "authors": [
                {
                    "_id": "6858b7c0c0c8e29df8ea3c29",
                    "name": "Mohon Raihan",
                    "hidden": false
                },
                {
                    "_id": "6858b7c0c0c8e29df8ea3c2a",
                    "name": "Plabon Kumar Saha",
                    "hidden": false
                },
                {
                    "_id": "6858b7c0c0c8e29df8ea3c2b",
                    "user": {
                        "_id": "67a3002c637d195f3c4bf371",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
                        "isPro": false,
                        "fullname": "Rajan Das Gupta",
                        "user": "rajandasgupta",
                        "type": "user"
                    },
                    "name": "Rajan Das Gupta",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-23T08:15:03.417Z",
                    "hidden": false
                },
                {
                    "_id": "6858b7c0c0c8e29df8ea3c2c",
                    "name": "A Z M Tahmidul Kabir",
                    "hidden": false
                },
                {
                    "_id": "6858b7c0c0c8e29df8ea3c2d",
                    "name": "Afia Anjum Tamanna",
                    "hidden": false
                },
                {
                    "_id": "6858b7c0c0c8e29df8ea3c2e",
                    "name": "Md. Harun-Ur-Rashid",
                    "hidden": false
                },
                {
                    "_id": "6858b7c0c0c8e29df8ea3c2f",
                    "name": "Adnan Bin Abdus Salam",
                    "hidden": false
                },
                {
                    "_id": "6858b7c0c0c8e29df8ea3c30",
                    "name": "Md Tanvir Anjum",
                    "hidden": false
                },
                {
                    "_id": "6858b7c0c0c8e29df8ea3c31",
                    "name": "A Z M Ahteshamul Kabir",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Kx6P69oj873BKEjH3euNc.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/0S6fvcxnAdj4DiU-NEhtz.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/EBuiK2wVbSXnLVWmxDiZ5.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Id__vCzRydC1Bmxv-j1W0.png"
            ],
            "publishedAt": "2025-06-20T11:44:48.000Z",
            "submittedOnDailyAt": "2025-06-24T08:29:18.461Z",
            "title": "A deep learning and machine learning approach to predict neonatal death\n  in the context of São Paulo",
            "submittedOnDailyBy": {
                "_id": "67a3002c637d195f3c4bf371",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
                "isPro": false,
                "fullname": "Rajan Das Gupta",
                "user": "rajandasgupta",
                "type": "user"
            },
            "summary": "Neonatal death is still a concerning reality for underdeveloped and even some\ndeveloped countries. Worldwide data indicate that 26.693 babies out of 1,000\nbirths die, according to Macro Trades. To reduce this number, early prediction\nof endangered babies is crucial. Such prediction enables the opportunity to\ntake ample care of the child and mother so that early child death can be\navoided. In this context, machine learning was used to determine whether a\nnewborn baby is at risk. To train the predictive model, historical data of 1.4\nmillion newborns was used. Machine learning and deep learning techniques such\nas logical regression, K-nearest neighbor, random forest classifier, extreme\ngradient boosting (XGBoost), convolutional neural network, and long short-term\nmemory (LSTM) were implemented using the dataset to identify the most accurate\nmodel for predicting neonatal mortality. Among the machine learning algorithms,\nXGBoost and random forest classifier achieved the best accuracy with 94%, while\namong the deep learning models, LSTM delivered the highest accuracy with 99%.\nTherefore, using LSTM appears to be the most suitable approach to predict\nwhether precautionary measures for a child are necessary.",
            "upvotes": 2,
            "discussionId": "6858b7c1c0c8e29df8ea3c32",
            "ai_summary": "Deep learning, specifically LSTM, outperforms other machine learning techniques in predicting neonatal mortality using historical data.",
            "ai_keywords": [
                "logical regression",
                "K-nearest neighbor",
                "random forest classifier",
                "extreme gradient boosting (XGBoost)",
                "convolutional neural network",
                "long short-term memory (LSTM)"
            ]
        },
        "publishedAt": "2025-06-20T07:44:48.000Z",
        "title": "A deep learning and machine learning approach to predict neonatal death\n  in the context of São Paulo",
        "summary": "Neonatal death is still a concerning reality for underdeveloped and even some\ndeveloped countries. Worldwide data indicate that 26.693 babies out of 1,000\nbirths die, according to Macro Trades. To reduce this number, early prediction\nof endangered babies is crucial. Such prediction enables the opportunity to\ntake ample care of the child and mother so that early child death can be\navoided. In this context, machine learning was used to determine whether a\nnewborn baby is at risk. To train the predictive model, historical data of 1.4\nmillion newborns was used. Machine learning and deep learning techniques such\nas logical regression, K-nearest neighbor, random forest classifier, extreme\ngradient boosting (XGBoost), convolutional neural network, and long short-term\nmemory (LSTM) were implemented using the dataset to identify the most accurate\nmodel for predicting neonatal mortality. Among the machine learning algorithms,\nXGBoost and random forest classifier achieved the best accuracy with 94%, while\namong the deep learning models, LSTM delivered the highest accuracy with 99%.\nTherefore, using LSTM appears to be the most suitable approach to predict\nwhether precautionary measures for a child are necessary.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Kx6P69oj873BKEjH3euNc.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/0S6fvcxnAdj4DiU-NEhtz.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/EBuiK2wVbSXnLVWmxDiZ5.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/Id__vCzRydC1Bmxv-j1W0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16929.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a3002c637d195f3c4bf371",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
            "fullname": "Rajan Das Gupta",
            "name": "rajandasgupta",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.13905",
            "authors": [
                {
                    "_id": "685b0f64d2ee4fac76521d60",
                    "name": "Zhongzhi Yu",
                    "hidden": false
                },
                {
                    "_id": "685b0f64d2ee4fac76521d61",
                    "name": "Mingjie Liu",
                    "hidden": false
                },
                {
                    "_id": "685b0f64d2ee4fac76521d62",
                    "name": "Michael Zimmer",
                    "hidden": false
                },
                {
                    "_id": "685b0f64d2ee4fac76521d63",
                    "name": "Yingyan Celine Lin",
                    "hidden": false
                },
                {
                    "_id": "685b0f64d2ee4fac76521d64",
                    "name": "Yong Liu",
                    "hidden": false
                },
                {
                    "_id": "685b0f64d2ee4fac76521d65",
                    "name": "Haoxing Ren",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T18:33:25.000Z",
            "submittedOnDailyAt": "2025-06-24T19:21:20.533Z",
            "title": "Spec2RTL-Agent: Automated Hardware Code Generation from Complex\n  Specifications Using LLM Agent Systems",
            "submittedOnDailyBy": {
                "_id": "643f72a80d1194da249ea688",
                "avatarUrl": "/avatars/7d1ad54cb2c731f3b004fda4c7d2967c.svg",
                "isPro": false,
                "fullname": "Zhongzhi Yu",
                "user": "kevin1020",
                "type": "user"
            },
            "summary": "Despite recent progress in generating hardware RTL code with LLMs, existing\nsolutions still suffer from a substantial gap between practical application\nscenarios and the requirements of real-world RTL code development. Prior\napproaches either focus on overly simplified hardware descriptions or depend on\nextensive human guidance to process complex specifications, limiting their\nscalability and automation potential. In this paper, we address this gap by\nproposing an LLM agent system, termed Spec2RTL-Agent, designed to directly\nprocess complex specification documentation and generate corresponding RTL code\nimplementations, advancing LLM-based RTL code generation toward more realistic\napplication settings. To achieve this goal, Spec2RTL-Agent introduces a novel\nmulti-agent collaboration framework that integrates three key enablers: (1) a\nreasoning and understanding module that translates specifications into\nstructured, step-by-step implementation plans; (2) a progressive coding and\nprompt optimization module that iteratively refines the code across multiple\nrepresentations to enhance correctness and synthesisability for RTL conversion;\nand (3) an adaptive reflection module that identifies and traces the source of\nerrors during generation, ensuring a more robust code generation flow. Instead\nof directly generating RTL from natural language, our system strategically\ngenerates synthesizable C++ code, which is then optimized for HLS. This\nagent-driven refinement ensures greater correctness and compatibility compared\nto naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three\nspecification documents, showing it generates accurate RTL code with up to 75%\nfewer human interventions than existing methods. This highlights its role as\nthe first fully automated multi-agent system for RTL generation from\nunstructured specs, reducing reliance on human effort in hardware design.",
            "upvotes": 2,
            "discussionId": "685b0f64d2ee4fac76521d66",
            "ai_summary": "Spec2RTL-Agent, a multi-agent system, automates RTL code generation from complex specifications by improving correctness and reducing human intervention.",
            "ai_keywords": [
                "LLM",
                "RTL code generation",
                "Spec2RTL-Agent",
                "multi-agent collaboration framework",
                "reasoning and understanding module",
                "progressive coding",
                "prompt optimization",
                "adaptive reflection",
                "synthesizable C++ code",
                "HLS",
                "FPGA"
            ]
        },
        "publishedAt": "2025-06-16T14:33:25.000Z",
        "title": "Spec2RTL-Agent: Automated Hardware Code Generation from Complex\n  Specifications Using LLM Agent Systems",
        "summary": "Despite recent progress in generating hardware RTL code with LLMs, existing\nsolutions still suffer from a substantial gap between practical application\nscenarios and the requirements of real-world RTL code development. Prior\napproaches either focus on overly simplified hardware descriptions or depend on\nextensive human guidance to process complex specifications, limiting their\nscalability and automation potential. In this paper, we address this gap by\nproposing an LLM agent system, termed Spec2RTL-Agent, designed to directly\nprocess complex specification documentation and generate corresponding RTL code\nimplementations, advancing LLM-based RTL code generation toward more realistic\napplication settings. To achieve this goal, Spec2RTL-Agent introduces a novel\nmulti-agent collaboration framework that integrates three key enablers: (1) a\nreasoning and understanding module that translates specifications into\nstructured, step-by-step implementation plans; (2) a progressive coding and\nprompt optimization module that iteratively refines the code across multiple\nrepresentations to enhance correctness and synthesisability for RTL conversion;\nand (3) an adaptive reflection module that identifies and traces the source of\nerrors during generation, ensuring a more robust code generation flow. Instead\nof directly generating RTL from natural language, our system strategically\ngenerates synthesizable C++ code, which is then optimized for HLS. This\nagent-driven refinement ensures greater correctness and compatibility compared\nto naive direct RTL generation approaches. We evaluate Spec2RTL-Agent on three\nspecification documents, showing it generates accurate RTL code with up to 75%\nfewer human interventions than existing methods. This highlights its role as\nthe first fully automated multi-agent system for RTL generation from\nunstructured specs, reducing reliance on human effort in hardware design.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.13905.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643f72a80d1194da249ea688",
            "avatarUrl": "/avatars/7d1ad54cb2c731f3b004fda4c7d2967c.svg",
            "fullname": "Zhongzhi Yu",
            "name": "kevin1020",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.19028",
            "authors": [
                {
                    "_id": "685b50b6d2ee4fac76521db9",
                    "name": "Weijie Xu",
                    "hidden": false
                },
                {
                    "_id": "685b50b6d2ee4fac76521dba",
                    "name": "Yiwen Wang",
                    "hidden": false
                },
                {
                    "_id": "685b50b6d2ee4fac76521dbb",
                    "name": "Chi Xue",
                    "hidden": false
                },
                {
                    "_id": "685b50b6d2ee4fac76521dbc",
                    "name": "Xiangkun Hu",
                    "hidden": false
                },
                {
                    "_id": "685b50b6d2ee4fac76521dbd",
                    "name": "Xi Fang",
                    "hidden": false
                },
                {
                    "_id": "685b50b6d2ee4fac76521dbe",
                    "name": "Guimin Dong",
                    "hidden": false
                },
                {
                    "_id": "685b50b6d2ee4fac76521dbf",
                    "name": "Chandan K. Reddy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T18:31:22.000Z",
            "submittedOnDailyAt": "2025-06-24T23:59:21.809Z",
            "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical\n  Perspective",
            "submittedOnDailyBy": {
                "_id": "63e3f57754f51ea342ce26be",
                "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
                "isPro": false,
                "fullname": "Weijie Xu",
                "user": "xwjzds",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose\nFiSCo(Fine-grained Semantic Computation), a novel statistical framework to\nevaluate group-level fairness in LLMs by detecting subtle semantic differences\nin long-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSco more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics.",
            "upvotes": 1,
            "discussionId": "685b50b7d2ee4fac76521dc0",
            "ai_summary": "FiSCo evaluates LLM fairness by detecting semantic differences in long-form responses across demographic groups, using entailment checks and statistical hypothesis testing to identify subtle biases.",
            "ai_keywords": [
                "Large Language Models",
                "FiSCo",
                "Fine-grained Semantic Computation",
                "group-level fairness",
                "semantic differences",
                "entailment checks",
                "statistical hypothesis testing",
                "group counterfactual fairness"
            ]
        },
        "publishedAt": "2025-06-23T14:31:22.000Z",
        "title": "Quantifying Fairness in LLMs Beyond Tokens: A Semantic and Statistical\n  Perspective",
        "summary": "Large Language Models (LLMs) often generate responses with inherent biases,\nundermining their reliability in real-world applications. Existing evaluation\nmethods often overlook biases in long-form responses and the intrinsic\nvariability of LLM outputs. To address these challenges, we propose\nFiSCo(Fine-grained Semantic Computation), a novel statistical framework to\nevaluate group-level fairness in LLMs by detecting subtle semantic differences\nin long-form responses across demographic groups. Unlike prior work focusing on\nsentiment or token-level comparisons, FiSCo goes beyond surface-level analysis\nby operating at the claim level, leveraging entailment checks to assess the\nconsistency of meaning across responses. We decompose model outputs into\nsemantically distinct claims and apply statistical hypothesis testing to\ncompare inter- and intra-group similarities, enabling robust detection of\nsubtle biases. We formalize a new group counterfactual fairness definition and\nvalidate FiSCo on both synthetic and human-annotated datasets spanning gender,\nrace, and age. Experiments show that FiSco more reliably identifies nuanced\nbiases while reducing the impact of stochastic LLM variability, outperforming\nvarious evaluation metrics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19028.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63e3f57754f51ea342ce26be",
            "avatarUrl": "/avatars/df9d52c376bed6868d341bb006bec212.svg",
            "fullname": "Weijie Xu",
            "name": "xwjzds",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.17671",
            "authors": [
                {
                    "_id": "685ab1900e4ad7e21975878c",
                    "name": "Fabien Furfaro",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-21T10:06:07.000Z",
            "submittedOnDailyAt": "2025-06-24T12:45:55.284Z",
            "title": "TPTT: Transforming Pretrained Transformer into Titans",
            "submittedOnDailyBy": {
                "_id": "662ac66c0ee2fbf668ef9fe7",
                "avatarUrl": "/avatars/80d097210627c0322e1f33122c7a1174.svg",
                "isPro": false,
                "fullname": "Fabien Furfaro",
                "user": "ffurfaro",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress in natural language processing, but their computational and memory\ndemands remain a significant challenge, particularly for long-context\ninference. We introduce TPTT (Transforming Pretrained Transformer into Titans),\na novel framework for enhancing pretrained Transformer models with efficient\nlinearized attention mechanisms and advanced memory management. TPTT employs\ntechniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).\nIt is fully compatible with the Hugging Face Transformers library, enabling\nseamless adaptation of any causal LLM through parameter-efficient fine-tuning\n(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU\nbenchmark with models of approximately 1 billion parameters, observing\nsubstantial improvements in both efficiency and accuracy. For instance,\nTitans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its\nbaseline. Statistical analyses and comparisons with recent state-of-the-art\nmethods confirm the practical scalability and robustness of TPTT. Code is\navailable at https://github.com/fabienfrfr/tptt . Python package at\nhttps://pypi.org/project/tptt/ .",
            "upvotes": 1,
            "discussionId": "685ab1900e4ad7e21975878d",
            "githubRepo": "https://github.com/fabienfrfr/tptt",
            "ai_summary": "TPTT enhances large language models with efficient linearized attention and advanced memory management, improving both efficiency and accuracy for long-context inference.",
            "ai_keywords": [
                "Transformers",
                "large language models (LLMs)",
                "linearized attention mechanisms",
                "Memory as Gate (MaG)",
                "mixed linearized attention (LiZA)",
                "parameter-efficient fine-tuning (LoRA)",
                "causal LLM",
                "MMLU benchmark",
                "Exact Match (EM)"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-06-21T06:06:07.000Z",
        "title": "TPTT: Transforming Pretrained Transformer into Titans",
        "summary": "Recent advances in large language models (LLMs) have led to remarkable\nprogress in natural language processing, but their computational and memory\ndemands remain a significant challenge, particularly for long-context\ninference. We introduce TPTT (Transforming Pretrained Transformer into Titans),\na novel framework for enhancing pretrained Transformer models with efficient\nlinearized attention mechanisms and advanced memory management. TPTT employs\ntechniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA).\nIt is fully compatible with the Hugging Face Transformers library, enabling\nseamless adaptation of any causal LLM through parameter-efficient fine-tuning\n(LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU\nbenchmark with models of approximately 1 billion parameters, observing\nsubstantial improvements in both efficiency and accuracy. For instance,\nTitans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its\nbaseline. Statistical analyses and comparisons with recent state-of-the-art\nmethods confirm the practical scalability and robustness of TPTT. Code is\navailable at https://github.com/fabienfrfr/tptt . Python package at\nhttps://pypi.org/project/tptt/ .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17671.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "662ac66c0ee2fbf668ef9fe7",
            "avatarUrl": "/avatars/80d097210627c0322e1f33122c7a1174.svg",
            "fullname": "Fabien Furfaro",
            "name": "ffurfaro",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]