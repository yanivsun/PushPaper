[
    {
        "paper": {
            "id": "2506.19851",
            "authors": [
                {
                    "_id": "685b5a46d2ee4fac76521dce",
                    "user": {
                        "_id": "6375d136dee28348a9c63cbf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6375d136dee28348a9c63cbf/gK465HBrQWIOZ-qtHb-Vh.jpeg",
                        "isPro": false,
                        "fullname": "zehuan-huang",
                        "user": "huanngzh",
                        "type": "user"
                    },
                    "name": "Zehuan Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-25T08:19:21.031Z",
                    "hidden": false
                },
                {
                    "_id": "685b5a46d2ee4fac76521dcf",
                    "user": {
                        "_id": "65240d0ca801972b6eb12ed8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65240d0ca801972b6eb12ed8/hl2RAssBperb5JlgOIDvw.jpeg",
                        "isPro": false,
                        "fullname": "Haoran Feng",
                        "user": "fenghora",
                        "type": "user"
                    },
                    "name": "Haoran Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T09:19:31.409Z",
                    "hidden": false
                },
                {
                    "_id": "685b5a46d2ee4fac76521dd0",
                    "user": {
                        "_id": "63a41cb584a6a25c65bd8316",
                        "avatarUrl": "/avatars/1d474831c320c7f9ca9e6d88f68acc06.svg",
                        "isPro": false,
                        "fullname": "Yangtian Sun",
                        "user": "Yang-Tian",
                        "type": "user"
                    },
                    "name": "Yangtian Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T09:19:29.490Z",
                    "hidden": false
                },
                {
                    "_id": "685b5a46d2ee4fac76521dd1",
                    "name": "Yuanchen Guo",
                    "hidden": false
                },
                {
                    "_id": "685b5a46d2ee4fac76521dd2",
                    "user": {
                        "_id": "638066faf022c8a5803f7eb8",
                        "avatarUrl": "/avatars/4cfd699c3f6c5461b12b7dc5e3fe183d.svg",
                        "isPro": false,
                        "fullname": "Yanpei Cao",
                        "user": "pookiefoof",
                        "type": "user"
                    },
                    "name": "Yanpei Cao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-25T08:21:27.467Z",
                    "hidden": false
                },
                {
                    "_id": "685b5a46d2ee4fac76521dd3",
                    "user": {
                        "_id": "65b722dbe02a17f0f8d1cc6b",
                        "avatarUrl": "/avatars/65f20601ef9b8ebfdddadd737f9153d6.svg",
                        "isPro": false,
                        "fullname": "Lu Sheng",
                        "user": "lsheng2024",
                        "type": "user"
                    },
                    "name": "Lu Sheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T09:19:33.186Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
            ],
            "publishedAt": "2025-06-24T17:59:58.000Z",
            "submittedOnDailyAt": "2025-06-25T01:12:40.364Z",
            "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
            "submittedOnDailyBy": {
                "_id": "64a96a375a69e2ca889abdff",
                "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
                "isPro": false,
                "fullname": "fanhongxing",
                "user": "fanhongxing",
                "type": "user"
            },
            "summary": "We present AnimaX, a feed-forward 3D animation framework that bridges the\nmotion priors of video diffusion models with the controllable structure of\nskeleton-based animation. Traditional motion synthesis methods are either\nrestricted to fixed skeletal topologies or require costly optimization in\nhigh-dimensional deformation spaces. In contrast, AnimaX effectively transfers\nvideo-based motion knowledge to the 3D domain, supporting diverse articulated\nmeshes with arbitrary skeletons. Our method represents 3D motion as multi-view,\nmulti-frame 2D pose maps, and enables joint video-pose diffusion conditioned on\ntemplate renderings and a textual motion prompt. We introduce shared positional\nencodings and modality-aware embeddings to ensure spatial-temporal alignment\nbetween video and pose sequences, effectively transferring video priors to\nmotion generation task. The resulting multi-view pose sequences are\ntriangulated into 3D joint positions and converted into mesh animation via\ninverse kinematics. Trained on a newly curated dataset of 160,000 rigged\nsequences, AnimaX achieves state-of-the-art results on VBench in\ngeneralization, motion fidelity, and efficiency, offering a scalable solution\nfor category-agnostic 3D animation. Project page:\nhttps://anima-x.github.io/{https://anima-x.github.io/}.",
            "upvotes": 41,
            "discussionId": "685b5a47d2ee4fac76521dd4",
            "projectPage": "https://anima-x.github.io/",
            "githubRepo": "https://github.com/anima-x/anima-x",
            "ai_summary": "AnimaX creates multi-skeleton 3D animations by blending video diffusion model priors with skeleton-based control, using joint video-pose diffusion and shared positional encodings.",
            "ai_keywords": [
                "feed-forward 3D animation framework",
                "video diffusion models",
                "skeleton-based animation",
                "motion synthesis",
                "high-dimensional deformation spaces",
                "2D pose maps",
                "joint video-pose diffusion",
                "template renderings",
                "textual motion prompt",
                "shared positional encodings",
                "modality-aware embeddings",
                "spatial-temporal alignment",
                "inverse kinematics",
                "VBench",
                "category-agnostic 3D animation"
            ],
            "githubStars": 50
        },
        "publishedAt": "2025-06-24T13:59:58.000Z",
        "title": "AnimaX: Animating the Inanimate in 3D with Joint Video-Pose Diffusion\n  Models",
        "summary": "We present AnimaX, a feed-forward 3D animation framework that bridges the\nmotion priors of video diffusion models with the controllable structure of\nskeleton-based animation. Traditional motion synthesis methods are either\nrestricted to fixed skeletal topologies or require costly optimization in\nhigh-dimensional deformation spaces. In contrast, AnimaX effectively transfers\nvideo-based motion knowledge to the 3D domain, supporting diverse articulated\nmeshes with arbitrary skeletons. Our method represents 3D motion as multi-view,\nmulti-frame 2D pose maps, and enables joint video-pose diffusion conditioned on\ntemplate renderings and a textual motion prompt. We introduce shared positional\nencodings and modality-aware embeddings to ensure spatial-temporal alignment\nbetween video and pose sequences, effectively transferring video priors to\nmotion generation task. The resulting multi-view pose sequences are\ntriangulated into 3D joint positions and converted into mesh animation via\ninverse kinematics. Trained on a newly curated dataset of 160,000 rigged\nsequences, AnimaX achieves state-of-the-art results on VBench in\ngeneralization, motion fidelity, and efficiency, offering a scalable solution\nfor category-agnostic 3D animation. Project page:\nhttps://anima-x.github.io/{https://anima-x.github.io/}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a96a375a69e2ca889abdff/ttfXuQOJQmOqnO-_-F1op.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19851.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64a96a375a69e2ca889abdff",
            "avatarUrl": "/avatars/f288c66ace09d907f132a79a740a3701.svg",
            "fullname": "fanhongxing",
            "name": "fanhongxing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18701",
            "authors": [
                {
                    "_id": "685a14da0e4ad7e21975854d",
                    "user": {
                        "_id": "63aed0e7f873109b112dbb1b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aed0e7f873109b112dbb1b/JkSkQ1a2SLq5eTTnz9F05.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhang",
                        "user": "Vanint",
                        "type": "user"
                    },
                    "name": "Yifan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:09:21.431Z",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e21975854e",
                    "user": {
                        "_id": "68378e584ed6982099b1a1aa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68378e584ed6982099b1a1aa/B23K1b3EoRYPDTbYYbPzY.jpeg",
                        "isPro": false,
                        "fullname": "CHUNLI PENG",
                        "user": "chunli-peng",
                        "type": "user"
                    },
                    "name": "Chunli Peng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-25T13:32:40.141Z",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e21975854f",
                    "name": "Boyang Wang",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e219758550",
                    "user": {
                        "_id": "664717a50860c78e7c7b7c52",
                        "avatarUrl": "/avatars/ca17216b6d73234e1a68510f87653b3a.svg",
                        "isPro": false,
                        "fullname": "Puyi Wang",
                        "user": "Puyiiii",
                        "type": "user"
                    },
                    "name": "Puyi Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-25T13:33:20.411Z",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e219758551",
                    "name": "Qingcheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e219758552",
                    "name": "Fei Kang",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e219758553",
                    "name": "Biao Jiang",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e219758554",
                    "name": "Zedong Gao",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e219758555",
                    "name": "Eric Li",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e219758556",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "685a14da0e4ad7e219758557",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63aed0e7f873109b112dbb1b/2AbnZSPpuvFOad20XamiG.mp4"
            ],
            "publishedAt": "2025-06-23T14:40:49.000Z",
            "submittedOnDailyAt": "2025-06-25T07:50:07.299Z",
            "title": "Matrix-Game: Interactive World Foundation Model",
            "submittedOnDailyBy": {
                "_id": "63aed0e7f873109b112dbb1b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aed0e7f873109b112dbb1b/JkSkQ1a2SLq5eTTnz9F05.jpeg",
                "isPro": false,
                "fullname": "Yifan Zhang",
                "user": "Vanint",
                "type": "user"
            },
            "summary": "We introduce Matrix-Game, an interactive world foundation model for\ncontrollable game world generation. Matrix-Game is trained using a two-stage\npipeline that first performs large-scale unlabeled pretraining for environment\nunderstanding, followed by action-labeled training for interactive video\ngeneration. To support this, we curate Matrix-Game-MC, a comprehensive\nMinecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips\nand over 1,000 hours of high-quality labeled clips with fine-grained keyboard\nand mouse action annotations. Our model adopts a controllable image-to-world\ngeneration paradigm, conditioned on a reference image, motion context, and user\nactions. With over 17 billion parameters, Matrix-Game enables precise control\nover character actions and camera movements, while maintaining high visual\nquality and temporal coherence. To evaluate performance, we develop GameWorld\nScore, a unified benchmark measuring visual quality, temporal quality, action\ncontrollability, and physical rule understanding for Minecraft world\ngeneration. Extensive experiments show that Matrix-Game consistently\noutperforms prior open-source Minecraft world models (including Oasis and\nMineWorld) across all metrics, with particularly strong gains in\ncontrollability and physical consistency. Double-blind human evaluations\nfurther confirm the superiority of Matrix-Game, highlighting its ability to\ngenerate perceptually realistic and precisely controllable videos across\ndiverse game scenarios. To facilitate future research on interactive\nimage-to-world generation, we will open-source the Matrix-Game model weights\nand the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.",
            "upvotes": 41,
            "discussionId": "685a14da0e4ad7e219758558",
            "projectPage": "https://matrix-game-homepage.github.io",
            "githubRepo": "https://github.com/SkyworkAI/Matrix-Game",
            "ai_summary": "Matrix-Game, a controllable game world generation model trained in a two-stage process, outperforms existing models by producing high-quality, action-controllable, and physically consistent Minecraft world videos.",
            "ai_keywords": [
                "Matrix-Game",
                "interactive world foundation model",
                "large-scale unlabeled pretraining",
                "action-labeled training",
                "contrrollable image-to-world generation",
                "Matrix-Game-MC",
                "motion context",
                "character actions",
                "camera movements",
                "visual quality",
                "temporal coherence",
                "GameWorld Score",
                "double-blind human evaluations",
                "interactive image-to-world generation",
                "Oasis",
                "MineWorld",
                "perceptually realistic"
            ],
            "githubStars": 757
        },
        "publishedAt": "2025-06-23T10:40:49.000Z",
        "title": "Matrix-Game: Interactive World Foundation Model",
        "summary": "We introduce Matrix-Game, an interactive world foundation model for\ncontrollable game world generation. Matrix-Game is trained using a two-stage\npipeline that first performs large-scale unlabeled pretraining for environment\nunderstanding, followed by action-labeled training for interactive video\ngeneration. To support this, we curate Matrix-Game-MC, a comprehensive\nMinecraft dataset comprising over 2,700 hours of unlabeled gameplay video clips\nand over 1,000 hours of high-quality labeled clips with fine-grained keyboard\nand mouse action annotations. Our model adopts a controllable image-to-world\ngeneration paradigm, conditioned on a reference image, motion context, and user\nactions. With over 17 billion parameters, Matrix-Game enables precise control\nover character actions and camera movements, while maintaining high visual\nquality and temporal coherence. To evaluate performance, we develop GameWorld\nScore, a unified benchmark measuring visual quality, temporal quality, action\ncontrollability, and physical rule understanding for Minecraft world\ngeneration. Extensive experiments show that Matrix-Game consistently\noutperforms prior open-source Minecraft world models (including Oasis and\nMineWorld) across all metrics, with particularly strong gains in\ncontrollability and physical consistency. Double-blind human evaluations\nfurther confirm the superiority of Matrix-Game, highlighting its ability to\ngenerate perceptually realistic and precisely controllable videos across\ndiverse game scenarios. To facilitate future research on interactive\nimage-to-world generation, we will open-source the Matrix-Game model weights\nand the GameWorld Score benchmark at https://github.com/SkyworkAI/Matrix-Game.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63aed0e7f873109b112dbb1b/2AbnZSPpuvFOad20XamiG.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18701.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63aed0e7f873109b112dbb1b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63aed0e7f873109b112dbb1b/JkSkQ1a2SLq5eTTnz9F05.jpeg",
            "fullname": "Yifan Zhang",
            "name": "Vanint",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.19290",
            "authors": [
                {
                    "_id": "685b6640d2ee4fac76521e42",
                    "user": {
                        "_id": "6621efe1a6eec3ad03e38759",
                        "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
                        "isPro": false,
                        "fullname": "Liang Zeng",
                        "user": "zengliangcs",
                        "type": "user"
                    },
                    "name": "Liang Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:08:30.529Z",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e43",
                    "user": {
                        "_id": "612cfc6e1f69b222aacf831b",
                        "avatarUrl": "/avatars/b6c7d15ebc7b5dd4b56620bfab324c77.svg",
                        "isPro": false,
                        "fullname": "lycfight",
                        "user": "lycfight",
                        "type": "user"
                    },
                    "name": "Yongcong Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:08:28.275Z",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e44",
                    "name": "Yuzhen Xiao",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e45",
                    "name": "Changshi Li",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e46",
                    "user": {
                        "_id": "658229ef5f6d83438257fce5",
                        "avatarUrl": "/avatars/b4417de9a338e95dc69cc547a46348e8.svg",
                        "isPro": false,
                        "fullname": "Chris (Yuhao) Liu",
                        "user": "chrisliu298",
                        "type": "user"
                    },
                    "name": "Chris Yuhao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:08:22.815Z",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e47",
                    "name": "Rui Yan",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e48",
                    "name": "Tianwen Wei",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e49",
                    "name": "Jujie He",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e4a",
                    "name": "Xuchen Song",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e4b",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "685b6640d2ee4fac76521e4c",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T03:53:36.000Z",
            "submittedOnDailyAt": "2025-06-25T01:35:02.603Z",
            "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "6621efe1a6eec3ad03e38759",
                "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
                "isPro": false,
                "fullname": "Liang Zeng",
                "user": "zengliangcs",
                "type": "user"
            },
            "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch.",
            "upvotes": 30,
            "discussionId": "685b6641d2ee4fac76521e4d",
            "projectPage": "https://quixotic-sting-239.notion.site/eb17f379610040ceb54da5d5d24065bd",
            "ai_summary": "An automated data-curation pipeline for software engineering improves large language model performance on SWE tasks, achieving state-of-the-art results with and without test-time scaling techniques.",
            "ai_keywords": [
                "LLM agents",
                "iterative problem-solving",
                "long-context dependency resolution",
                "code file filtering",
                "unit tests",
                "runtime environments",
                "data-curation pipeline",
                "software engineering capabilities",
                "Skywork-SWE model",
                "SWE-bench Verified",
                "pass@1 accuracy",
                "OpenHands agent framework",
                "test-time scaling techniques",
                "parameter-efficient fine-tuning"
            ]
        },
        "publishedAt": "2025-06-23T23:53:36.000Z",
        "title": "Skywork-SWE: Unveiling Data Scaling Laws for Software Engineering in\n  LLMs",
        "summary": "Software engineering (SWE) has recently emerged as a crucial testbed for\nnext-generation LLM agents, demanding inherent capabilities in two critical\ndimensions: sustained iterative problem-solving (e.g., >50 interaction rounds)\nand long-context dependency resolution (e.g., >32k tokens). However, the data\ncuration process in SWE remains notoriously time-consuming, as it heavily\nrelies on manual annotation for code file filtering and the setup of dedicated\nruntime environments to execute and validate unit tests. Consequently, most\nexisting datasets are limited to only a few thousand GitHub-sourced instances.\nTo this end, we propose an incremental, automated data-curation pipeline that\nsystematically scales both the volume and diversity of SWE datasets. Our\ndataset comprises 10,169 real-world Python task instances from 2,531 distinct\nGitHub repositories, each accompanied by a task specified in natural language\nand a dedicated runtime-environment image for automated unit-test validation.\nWe have carefully curated over 8,000 successfully runtime-validated training\ntrajectories from our proposed SWE dataset. When fine-tuning the Skywork-SWE\nmodel on these trajectories, we uncover a striking data scaling phenomenon: the\ntrained model's performance for software engineering capabilities in LLMs\ncontinues to improve as the data size increases, showing no signs of\nsaturation. Notably, our Skywork-SWE model achieves 38.0% pass@1 accuracy on\nthe SWE-bench Verified benchmark without using verifiers or multiple rollouts,\nestablishing a new state-of-the-art (SOTA) among the Qwen2.5-Coder-32B-based\nLLMs built on the OpenHands agent framework. Furthermore, with the\nincorporation of test-time scaling techniques, the performance further improves\nto 47.0% accuracy, surpassing the previous SOTA results for sub-32B parameter\nmodels. We release the Skywork-SWE-32B model checkpoint to accelerate future\nresearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19290.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6621efe1a6eec3ad03e38759",
            "avatarUrl": "/avatars/c35acce69f244ec0833dffd53eedf6a3.svg",
            "fullname": "Liang Zeng",
            "name": "zengliangcs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.16141",
            "authors": [
                {
                    "_id": "6858b1fac0c8e29df8ea3c18",
                    "name": "Yi Chen",
                    "hidden": false
                },
                {
                    "_id": "6858b1fac0c8e29df8ea3c19",
                    "user": {
                        "_id": "6455cc8f654d8bccae50e4d4",
                        "avatarUrl": "/avatars/506a9992e5bf52e06d37cc22e4b307c0.svg",
                        "isPro": false,
                        "fullname": "Yuying Ge",
                        "user": "tttoaster",
                        "type": "user"
                    },
                    "name": "Yuying Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-25T13:34:16.831Z",
                    "hidden": false
                },
                {
                    "_id": "6858b1fac0c8e29df8ea3c1a",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "6858b1fac0c8e29df8ea3c1b",
                    "user": {
                        "_id": "640e9762b03f4cd29f58d982",
                        "avatarUrl": "/avatars/81da37d628163fe3e094b247c7c3a3b5.svg",
                        "isPro": false,
                        "fullname": "Yixiao Ge",
                        "user": "yxgeee",
                        "type": "user"
                    },
                    "name": "Yixiao Ge",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-25T13:34:24.360Z",
                    "hidden": false
                },
                {
                    "_id": "6858b1fac0c8e29df8ea3c1c",
                    "user": {
                        "_id": "6496f2bb33611fb4330455bd",
                        "avatarUrl": "/avatars/1bc94098010145d302c7bea3c221b39a.svg",
                        "isPro": false,
                        "fullname": "Junhao Cheng",
                        "user": "RefineMe",
                        "type": "user"
                    },
                    "name": "Junhao Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-25T13:34:30.450Z",
                    "hidden": false
                },
                {
                    "_id": "6858b1fac0c8e29df8ea3c1d",
                    "name": "Ying Shan",
                    "hidden": false
                },
                {
                    "_id": "6858b1fac0c8e29df8ea3c1e",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-19T08:49:13.000Z",
            "submittedOnDailyAt": "2025-06-25T01:50:33.428Z",
            "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "60d045c4778bafd0fbcfa3f5",
                "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
                "isPro": false,
                "fullname": "Yi Chen",
                "user": "ChenYi99",
                "type": "user"
            },
            "summary": "Recent reinforcement learning approaches, such as outcome-supervised GRPO,\nhave advanced Chain-of-Thought reasoning in large language models (LLMs), yet\ntheir adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack\nof rigorous evaluation for MLLM post-training methods, we introduce\nSEED-Bench-R1, a benchmark with complex real-world videos requiring balanced\nperception and reasoning. It offers a large training set and evaluates\ngeneralization across three escalating challenges: in-distribution,\ncross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,\nwe find that standard GRPO, while improving answer accuracy, often reduces\nlogical coherence between reasoning steps and answers, with only a 57.9%\nconsistency rate. This stems from reward signals focusing solely on final\nanswers, encouraging shortcuts, and strict KL penalties limiting exploration.To\naddress this, we propose GRPO-CARE, a consistency-aware RL framework optimizing\nboth answer correctness and reasoning coherence without explicit supervision.\nGRPO-CARE introduces a two-tiered reward: (1) a base reward for answer\ncorrectness, and (2) an adaptive consistency bonus, computed by comparing the\nmodel's reasoning-to-answer likelihood (via a slowly-evolving reference model)\nagainst group peers.This dual mechanism amplifies rewards for reasoning paths\nthat are both correct and logically consistent. Replacing KL penalties with\nthis adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,\nachieving a 6.7% performance gain on the hardest evaluation level and a 24.5%\nimprovement in consistency. It also shows strong transferability, improving\nmodel performance across diverse video understanding benchmarks. Our work\ncontributes a systematically designed benchmark and a generalizable\npost-training framework, advancing the development of more interpretable and\nrobust MLLMs.",
            "upvotes": 24,
            "discussionId": "6858b1fac0c8e29df8ea3c1f",
            "githubRepo": "https://github.com/TencentARC/GRPO-CARE",
            "ai_summary": "GRPO-CARE, a reinforcement learning framework optimizing for consistency and correctness, outperforms standard GRPO on a new video understanding benchmark, SEED-Bench-R1, improving both performance and logical coherence in multimodal large language models.",
            "ai_keywords": [
                "reinforcement learning",
                "outcome-supervised GRPO",
                "Chain-of-Thought reasoning",
                "large language models",
                "multimodal large language models",
                "SEED-Bench-R1",
                "in-distribution",
                "cross-environment",
                "cross-environment-task",
                "logical coherence",
                "reasoning steps",
                "answer accuracy",
                "reward signals",
                "shortcuts",
                "KL penalties",
                "exploration",
                "consistency-aware RL framework",
                "two-tiered reward",
                "reasoning-to-answer likelihood",
                "adaptive consistency bonus",
                "video understanding benchmarks",
                "transferability",
                "interpretable models",
                "robust models"
            ],
            "githubStars": 27
        },
        "publishedAt": "2025-06-19T04:49:13.000Z",
        "title": "GRPO-CARE: Consistency-Aware Reinforcement Learning for Multimodal\n  Reasoning",
        "summary": "Recent reinforcement learning approaches, such as outcome-supervised GRPO,\nhave advanced Chain-of-Thought reasoning in large language models (LLMs), yet\ntheir adaptation to multimodal LLMs (MLLMs) is unexplored. To address the lack\nof rigorous evaluation for MLLM post-training methods, we introduce\nSEED-Bench-R1, a benchmark with complex real-world videos requiring balanced\nperception and reasoning. It offers a large training set and evaluates\ngeneralization across three escalating challenges: in-distribution,\ncross-environment, and cross-environment-task scenarios. Using SEED-Bench-R1,\nwe find that standard GRPO, while improving answer accuracy, often reduces\nlogical coherence between reasoning steps and answers, with only a 57.9%\nconsistency rate. This stems from reward signals focusing solely on final\nanswers, encouraging shortcuts, and strict KL penalties limiting exploration.To\naddress this, we propose GRPO-CARE, a consistency-aware RL framework optimizing\nboth answer correctness and reasoning coherence without explicit supervision.\nGRPO-CARE introduces a two-tiered reward: (1) a base reward for answer\ncorrectness, and (2) an adaptive consistency bonus, computed by comparing the\nmodel's reasoning-to-answer likelihood (via a slowly-evolving reference model)\nagainst group peers.This dual mechanism amplifies rewards for reasoning paths\nthat are both correct and logically consistent. Replacing KL penalties with\nthis adaptive bonus, GRPO-CARE outperforms standard GRPO on SEED-Bench-R1,\nachieving a 6.7% performance gain on the hardest evaluation level and a 24.5%\nimprovement in consistency. It also shows strong transferability, improving\nmodel performance across diverse video understanding benchmarks. Our work\ncontributes a systematically designed benchmark and a generalizable\npost-training framework, advancing the development of more interpretable and\nrobust MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16141.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60d045c4778bafd0fbcfa3f5",
            "avatarUrl": "/avatars/0cc0c2739c1934430ea09df7e9668c80.svg",
            "fullname": "Yi Chen",
            "name": "ChenYi99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.19848",
            "authors": [
                {
                    "_id": "685b7cc2d2ee4fac76521e83",
                    "name": "Long Xing",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e84",
                    "user": {
                        "_id": "656f1b21b075b63c90ba02ee",
                        "avatarUrl": "/avatars/d6856815ef06261394178161e4d511b4.svg",
                        "isPro": false,
                        "fullname": "Huang Qidong",
                        "user": "shikiw",
                        "type": "user"
                    },
                    "name": "Qidong Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:07:23.757Z",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e85",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e86",
                    "name": "Pan Zhang",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e87",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:07:21.449Z",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e88",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e89",
                    "name": "Jinsong Li",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e8a",
                    "name": "Shuangrui Ding",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e8b",
                    "name": "Weiming Zhang",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e8c",
                    "name": "Nenghai Yu",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e8d",
                    "name": "Jiaqi Wang",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e8e",
                    "name": "Feng Wu",
                    "hidden": false
                },
                {
                    "_id": "685b7cc2d2ee4fac76521e8f",
                    "name": "Dahua Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T17:59:55.000Z",
            "submittedOnDailyAt": "2025-06-25T03:07:04.508Z",
            "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing",
            "submittedOnDailyBy": {
                "_id": "64b4eec4faa3181a5eab9c46",
                "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                "isPro": true,
                "fullname": "Jiaqi Wang",
                "user": "myownskyW7",
                "type": "user"
            },
            "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
            "upvotes": 23,
            "discussionId": "685b7cc2d2ee4fac76521e90",
            "githubRepo": "https://github.com/Cooperx521/ScaleCap",
            "ai_summary": "ScaleCap enhances image captioning by iteratively enriching and calibrating captions using heuristic question answering and contrastive sentence rating, addressing multimodal and linguistic biases to improve accuracy, balance, and informativeness.",
            "ai_keywords": [
                "LVLMs",
                "multimodal bias",
                "linguistic bias",
                "heuristic question answering",
                "contrastive sentence rating",
                "VQA task"
            ],
            "githubStars": 27
        },
        "publishedAt": "2025-06-24T13:59:55.000Z",
        "title": "ScaleCap: Inference-Time Scalable Image Captioning via Dual-Modality\n  Debiasing",
        "summary": "This paper presents ScaleCap, an inference-time scalable image captioning\nstrategy that generates comprehensive and detailed image captions. The key\nchallenges of high-quality image captioning lie in the inherent biases of\nLVLMs: multimodal bias resulting in imbalanced descriptive granularity,\noffering detailed accounts of some elements while merely skimming over others;\nlinguistic bias leading to hallucinated descriptions of non-existent objects.\nTo address these issues, we propose a scalable debiased captioning strategy,\nwhich continuously enriches and calibrates the caption with increased inference\nbudget. Specifically, we propose two novel components: heuristic question\nanswering and contrastive sentence rating. The former generates\ncontent-specific questions based on the image and answers them to progressively\ninject relevant information into the caption. The latter employs sentence-level\noffline contrastive decoding to effectively identify and eliminate\nhallucinations caused by linguistic biases. With increased inference cost, more\nheuristic questions are raised by ScaleCap to progressively capture additional\nvisual details, generating captions that are more accurate, balanced, and\ninformative. Extensive modality alignment experiments demonstrate the\neffectiveness of ScaleCap. Annotating 450K images with ScaleCap and using them\nfor LVLM pretraining leads to consistent performance gains across 11 widely\nused benchmarks. Furthermore, ScaleCap showcases superb richness and fidelity\nof generated captions with two additional tasks: replacing images with captions\nin VQA task, and reconstructing images from captions to assess semantic\ncoverage. Code is available at https://github.com/Cooperx521/ScaleCap.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19848.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64b4eec4faa3181a5eab9c46",
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "fullname": "Jiaqi Wang",
            "name": "myownskyW7",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.17612",
            "authors": [
                {
                    "_id": "685b7538d2ee4fac76521e63",
                    "user": {
                        "_id": "64ecb174f22081b4ac7ca397",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
                        "isPro": true,
                        "fullname": "Yunlong Lin",
                        "user": "LYL1015",
                        "type": "user"
                    },
                    "name": "Yunlong Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:07:25.557Z",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e64",
                    "name": "Zixu Lin",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e65",
                    "name": "Kunjie Lin",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e66",
                    "name": "Jinbin Bai",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e67",
                    "name": "Panwang Pan",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e68",
                    "name": "Chenxin Li",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e69",
                    "name": "Haoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e6a",
                    "name": "Zhongdao Wang",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e6b",
                    "name": "Xinghao Ding",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e6c",
                    "name": "Wenbo Li",
                    "hidden": false
                },
                {
                    "_id": "685b7538d2ee4fac76521e6d",
                    "name": "Shuicheng Yan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
            ],
            "publishedAt": "2025-06-21T06:36:00.000Z",
            "submittedOnDailyAt": "2025-06-25T02:43:05.885Z",
            "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent",
            "submittedOnDailyBy": {
                "_id": "64ecb174f22081b4ac7ca397",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
                "isPro": true,
                "fullname": "Yunlong Lin",
                "user": "LYL1015",
                "type": "user"
            },
            "summary": "Photo retouching has become integral to contemporary visual storytelling,\nenabling users to capture aesthetics and express creativity. While professional\ntools such as Adobe Lightroom offer powerful capabilities, they demand\nsubstantial expertise and manual effort. In contrast, existing AI-based\nsolutions provide automation but often suffer from limited adjustability and\npoor generalization, failing to meet diverse and personalized editing needs. To\nbridge this gap, we introduce JarvisArt, a multi-modal large language model\n(MLLM)-driven agent that understands user intent, mimics the reasoning process\nof professional artists, and intelligently coordinates over 200 retouching\ntools within Lightroom. JarvisArt undergoes a two-stage training process: an\ninitial Chain-of-Thought supervised fine-tuning to establish basic reasoning\nand tool-use skills, followed by Group Relative Policy Optimization for\nRetouching (GRPO-R) to further enhance its decision-making and tool\nproficiency. We also propose the Agent-to-Lightroom Protocol to facilitate\nseamless integration with Lightroom. To evaluate performance, we develop\nMMArt-Bench, a novel benchmark constructed from real-world user edits.\nJarvisArt demonstrates user-friendly interaction, superior generalization, and\nfine-grained control over both global and local adjustments, paving a new\navenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a\n60% improvement in average pixel-level metrics on MMArt-Bench for content\nfidelity, while maintaining comparable instruction-following capabilities.\nProject Page: https://jarvisart.vercel.app/.",
            "upvotes": 18,
            "discussionId": "685b7539d2ee4fac76521e6e",
            "projectPage": "https://jarvisart.vercel.app/",
            "githubRepo": "https://github.com/LYL1015/JarvisArt",
            "ai_summary": "JarvisArt, an MLLM-driven agent, achieves superior photo retouching by understanding user intent and coordinating multiple retouching tools in Lightroom, outperforming GPT-4o on a novel benchmark.",
            "ai_keywords": [
                "multi-modal large language model",
                "Chain-of-Thought supervised fine-tuning",
                "Group Relative Policy Optimization",
                "Agent-to-Lightroom Protocol",
                "MMArt-Bench",
                "global adjustments",
                "local adjustments",
                "content fidelity",
                "instruction-following capabilities"
            ],
            "githubStars": 36
        },
        "publishedAt": "2025-06-21T02:36:00.000Z",
        "title": "JarvisArt: Liberating Human Artistic Creativity via an Intelligent Photo\n  Retouching Agent",
        "summary": "Photo retouching has become integral to contemporary visual storytelling,\nenabling users to capture aesthetics and express creativity. While professional\ntools such as Adobe Lightroom offer powerful capabilities, they demand\nsubstantial expertise and manual effort. In contrast, existing AI-based\nsolutions provide automation but often suffer from limited adjustability and\npoor generalization, failing to meet diverse and personalized editing needs. To\nbridge this gap, we introduce JarvisArt, a multi-modal large language model\n(MLLM)-driven agent that understands user intent, mimics the reasoning process\nof professional artists, and intelligently coordinates over 200 retouching\ntools within Lightroom. JarvisArt undergoes a two-stage training process: an\ninitial Chain-of-Thought supervised fine-tuning to establish basic reasoning\nand tool-use skills, followed by Group Relative Policy Optimization for\nRetouching (GRPO-R) to further enhance its decision-making and tool\nproficiency. We also propose the Agent-to-Lightroom Protocol to facilitate\nseamless integration with Lightroom. To evaluate performance, we develop\nMMArt-Bench, a novel benchmark constructed from real-world user edits.\nJarvisArt demonstrates user-friendly interaction, superior generalization, and\nfine-grained control over both global and local adjustments, paving a new\navenue for intelligent photo retouching. Notably, it outperforms GPT-4o with a\n60% improvement in average pixel-level metrics on MMArt-Bench for content\nfidelity, while maintaining comparable instruction-following capabilities.\nProject Page: https://jarvisart.vercel.app/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64ecb174f22081b4ac7ca397/PCTU9EbA4XUz8eKeLbxVC.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17612.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ecb174f22081b4ac7ca397",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ecb174f22081b4ac7ca397/PiAPtD_rbuhGOqfE6ZSIu.jpeg",
            "fullname": "Yunlong Lin",
            "name": "LYL1015",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.19467",
            "authors": [
                {
                    "_id": "685bbdb2aafa002dfd2bdeb4",
                    "user": {
                        "_id": "64493579b6ac93fe65152f4f",
                        "avatarUrl": "/avatars/4047cf8549f162994f44101b4e7c27fa.svg",
                        "isPro": false,
                        "fullname": "Jingwei Ni",
                        "user": "JingweiNi",
                        "type": "user"
                    },
                    "name": "Jingwei Ni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T11:04:06.025Z",
                    "hidden": false
                },
                {
                    "_id": "685bbdb2aafa002dfd2bdeb5",
                    "name": "Yu Fan",
                    "hidden": false
                },
                {
                    "_id": "685bbdb2aafa002dfd2bdeb6",
                    "name": "Vilm Zouhar",
                    "hidden": false
                },
                {
                    "_id": "685bbdb2aafa002dfd2bdeb7",
                    "name": "Donya Rooein",
                    "hidden": false
                },
                {
                    "_id": "685bbdb2aafa002dfd2bdeb8",
                    "name": "Alexander Hoyle",
                    "hidden": false
                },
                {
                    "_id": "685bbdb2aafa002dfd2bdeb9",
                    "name": "Mrinmaya Sachan",
                    "hidden": false
                },
                {
                    "_id": "685bbdb2aafa002dfd2bdeba",
                    "name": "Markus Leippold",
                    "hidden": false
                },
                {
                    "_id": "685bbdb2aafa002dfd2bdebb",
                    "name": "Dirk Hovy",
                    "hidden": false
                },
                {
                    "_id": "685bbdb2aafa002dfd2bdebc",
                    "name": "Elliott Ash",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64493579b6ac93fe65152f4f/bQ52lKBfeF0adzEBgQcAh.png"
            ],
            "publishedAt": "2025-06-24T09:49:26.000Z",
            "submittedOnDailyAt": "2025-06-25T10:11:33.433Z",
            "title": "Can Large Language Models Capture Human Annotator Disagreements?",
            "submittedOnDailyBy": {
                "_id": "64493579b6ac93fe65152f4f",
                "avatarUrl": "/avatars/4047cf8549f162994f44101b4e7c27fa.svg",
                "isPro": false,
                "fullname": "Jingwei Ni",
                "user": "JingweiNi",
                "type": "user"
            },
            "summary": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction.",
            "upvotes": 14,
            "discussionId": "685bbdb2aafa002dfd2bdebd",
            "githubRepo": "https://github.com/EdisonNi-hku/Disagreement_Prediction",
            "ai_summary": "LLMs struggle to predict human annotation disagreements, contrary to their performance in predicting majority labels, and RLVR-style reasoning exacerbates this issue.",
            "ai_keywords": [
                "Large Language Models",
                "RLVR-style",
                "Reinforcement learning with verifiable rewards",
                "disagreement modeling"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-06-24T05:49:26.000Z",
        "title": "Can Large Language Models Capture Human Annotator Disagreements?",
        "summary": "Human annotation variation (i.e., annotation disagreements) is common in NLP\nand often reflects important information such as task subjectivity and sample\nambiguity. While Large Language Models (LLMs) are increasingly used for\nautomatic annotation to reduce human effort, their evaluation often focuses on\npredicting the majority-voted \"ground truth\" labels. It is still unclear,\nhowever, whether these models also capture informative human annotation\nvariation. Our work addresses this gap by extensively evaluating LLMs' ability\nto predict annotation disagreements without access to repeated human labels.\nOur results show that LLMs struggle with modeling disagreements, which can be\noverlooked by majority label-based evaluations. Notably, while RLVR-style\n(Reinforcement learning with verifiable rewards) reasoning generally boosts LLM\nperformance, it degrades performance in disagreement prediction. Our findings\nhighlight the critical need for evaluating and improving LLM annotators in\ndisagreement modeling. Code and data at\nhttps://github.com/EdisonNi-hku/Disagreement_Prediction.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64493579b6ac93fe65152f4f/bQ52lKBfeF0adzEBgQcAh.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19467.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64493579b6ac93fe65152f4f",
            "avatarUrl": "/avatars/4047cf8549f162994f44101b4e7c27fa.svg",
            "fullname": "Jingwei Ni",
            "name": "JingweiNi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18951",
            "authors": [
                {
                    "_id": "685ba757d2ee4fac76521f47",
                    "name": "Jinyang Li",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f48",
                    "user": {
                        "_id": "653693cb8ee17cfd44eed8ce",
                        "avatarUrl": "/avatars/82be2428bec4e06c0a15a27647b9b8aa.svg",
                        "isPro": false,
                        "fullname": "Xiaolong Li",
                        "user": "xia01ongLi",
                        "type": "user"
                    },
                    "name": "Xiaolong Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:07:05.354Z",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f49",
                    "name": "Ge Qu",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f4a",
                    "name": "Per Jacobsson",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f4b",
                    "name": "Bowen Qin",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f4c",
                    "name": "Binyuan Hui",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f4d",
                    "name": "Shuzheng Si",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f4e",
                    "name": "Nan Huo",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f4f",
                    "user": {
                        "_id": "63a3eb8af460e4379b5991e7",
                        "avatarUrl": "/avatars/7564a048d8496cac38d689178d90a8f9.svg",
                        "isPro": false,
                        "fullname": "Xiaohan Xu",
                        "user": "Tebmer",
                        "type": "user"
                    },
                    "name": "Xiaohan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T09:19:04.596Z",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f50",
                    "name": "Yue Zhang",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f51",
                    "name": "Ziwei Tang",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f52",
                    "name": "Yuanshuai Li",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f53",
                    "user": {
                        "_id": "665e7d0fddb96a4d725341ac",
                        "avatarUrl": "/avatars/30b235b10ee1436eb86c6a2ac884c904.svg",
                        "isPro": false,
                        "fullname": "Florensia Widjaja",
                        "user": "florensia712",
                        "type": "user"
                    },
                    "name": "Florensia Widjaja",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T20:59:20.731Z",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f54",
                    "name": "Xintong Zhu",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f55",
                    "name": "Feige Zhou",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f56",
                    "name": "Yongfeng Huang",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f57",
                    "name": "Yannis Papakonstantinou",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f58",
                    "name": "Fatma Ozcan",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f59",
                    "name": "Chenhao Ma",
                    "hidden": false
                },
                {
                    "_id": "685ba757d2ee4fac76521f5a",
                    "name": "Reynold Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T09:41:37.000Z",
            "submittedOnDailyAt": "2025-06-25T06:09:19.766Z",
            "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications",
            "submittedOnDailyBy": {
                "_id": "6419435385030eca6ac94701",
                "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
                "isPro": false,
                "fullname": "Ge Qu",
                "user": "gq2138",
                "type": "user"
            },
            "summary": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/",
            "upvotes": 14,
            "discussionId": "685ba758d2ee4fac76521f5b",
            "ai_summary": "A new benchmark and training environment for debugging SQL issues using advanced open-source models significantly improves their performance compared to proprietary solutions.",
            "ai_keywords": [
                "BIRD-CRITIC",
                "BIRD-CRITIC-PG",
                "BIRD-CRITIC-Multi",
                "PostgreSQL",
                "Six-Gym (Sql-fIX-Gym)",
                "SQL-Rewind",
                "f-Plan Boosting",
                "Bird-Fixer",
                "Qwen-2.5-Coder-14B",
                "Claude-3.7-Sonnet",
                "GPT-4.1"
            ]
        },
        "publishedAt": "2025-06-23T05:41:37.000Z",
        "title": "SWE-SQL: Illuminating LLM Pathways to Solve User SQL Issues in\n  Real-World Applications",
        "summary": "Resolution of complex SQL issues persists as a significant bottleneck in\nreal-world database applications. Current Large Language Models (LLMs), while\nadept at text-to-SQL translation, have not been rigorously evaluated on the\nmore challenging task of debugging SQL issues. To address this gap, we\nintroduce BIRD-CRITIC, a new SQL issue debugging benchmark comprising 530\nPostgreSQL tasks (BIRD-CRITIC-PG) and 570 multi-dialect tasks\n(BIRD-CRITIC-Multi), distilled from authentic user issues and replayed within\nnew environments to facilitate rigorous evaluation. Baseline evaluations\nunderscore the task's complexity, with the leading reasoning model O3-Mini\nachieving only 38.87% success rate on BIRD-CRITIC-PG and 33.33% on\nBIRD-CRITIC-Multi. Meanwhile, advancing open-source models for database tasks\nis crucial for empowering local development while safeguarding data privacy.\nTherefore, we present Six-Gym (Sql-fIX-Gym), a training environment for\nelevating open-source model capabilities for SQL issue debugging. This\nenvironment leverages SQL-Rewind strategy, which automatically generates\nexecutable issue-solution datasets by reverse-engineering issues from verified\nSQLs. However, popular trajectory-based fine-tuning methods do not explore\nsubstantial supervisory signals. We further propose f-Plan Boosting, which\nextracts high-level debugging plans from SQL solutions, enabling teacher LLMs\nto produce 73.7% more successful trajectories for training. We integrate these\ncomponents into an open-source agent, Bird-Fixer. Based on Qwen-2.5-Coder-14B,\nBird-Fixer achieves 38.11% success rate on BIRD-CRITIC-PG and 29.65% on\nBIRD-CRITIC-Multi, surpassing leading proprietary models such as\nClaude-3.7-Sonnet and GPT-4.1, marking a significant step toward democratizing\nsophisticated SQL-debugging capabilities. The leaderboard and source code are\navailable: https://bird-critic.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18951.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6419435385030eca6ac94701",
            "avatarUrl": "/avatars/c49ff1991739de49ec98c8310ab21e46.svg",
            "fullname": "Ge Qu",
            "name": "gq2138",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.19850",
            "authors": [
                {
                    "_id": "685b63c2d2ee4fac76521dee",
                    "name": "Yuqi Wang",
                    "hidden": false
                },
                {
                    "_id": "685b63c2d2ee4fac76521def",
                    "name": "Xinghang Li",
                    "hidden": false
                },
                {
                    "_id": "685b63c2d2ee4fac76521df0",
                    "name": "Wenxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "685b63c2d2ee4fac76521df1",
                    "name": "Junbo Zhang",
                    "hidden": false
                },
                {
                    "_id": "685b63c2d2ee4fac76521df2",
                    "name": "Yingyan Li",
                    "hidden": false
                },
                {
                    "_id": "685b63c2d2ee4fac76521df3",
                    "name": "Yuntao Chen",
                    "hidden": false
                },
                {
                    "_id": "685b63c2d2ee4fac76521df4",
                    "name": "Xinlong Wang",
                    "hidden": false
                },
                {
                    "_id": "685b63c2d2ee4fac76521df5",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T17:59:57.000Z",
            "submittedOnDailyAt": "2025-06-25T06:01:30.093Z",
            "title": "Unified Vision-Language-Action Model",
            "submittedOnDailyBy": {
                "_id": "649fe21d59c1ae90dbfacf91",
                "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
                "isPro": false,
                "fullname": "Wang Yuqi",
                "user": "Yuqi1997",
                "type": "user"
            },
            "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.",
            "upvotes": 11,
            "discussionId": "685b63c3d2ee4fac76521df6",
            "ai_summary": "UniVLA is a multimodal VLA model that autoregressively processes vision, language, and action as token sequences, incorporating world modeling for effective long-horizon policy learning and achieving state-of-the-art results across simulation and real-world benchmarks.",
            "ai_keywords": [
                "vision-language-action models",
                "VLAs",
                "vision-language models",
                "VLMs",
                "autoregressive models",
                "discrete token sequences",
                "multimodal tasks learning",
                "world modeling",
                "causal dynamics",
                "policy learning",
                "simulation benchmarks",
                "CALVIN",
                "LIBERO",
                "Simplenv-Bridge",
                "ALOHA manipulation",
                "autonomous driving"
            ]
        },
        "publishedAt": "2025-06-24T13:59:57.000Z",
        "title": "Unified Vision-Language-Action Model",
        "summary": "Vision-language-action models (VLAs) have garnered significant attention for\ntheir potential in advancing robotic manipulation. However, previous approaches\npredominantly rely on the general comprehension capabilities of vision-language\nmodels (VLMs) to generate action signals, often overlooking the rich temporal\nand causal structure embedded in visual observations. In this paper, we present\nUniVLA, a unified and native multimodal VLA model that autoregressively models\nvision, language, and action signals as discrete token sequences. This\nformulation enables flexible multimodal tasks learning, particularly from\nlarge-scale video data. By incorporating world modeling during post-training,\nUniVLA captures causal dynamics from videos, facilitating effective transfer to\ndownstream policy learning--especially for long-horizon tasks. Our approach\nsets new state-of-the-art results across several widely used simulation\nbenchmarks, including CALVIN, LIBERO, and Simplenv-Bridge, significantly\nsurpassing previous methods. For example, UniVLA achieves 95.5% average success\nrate on LIBERO benchmark, surpassing pi0-FAST's 85.5%. We further demonstrate\nits broad applicability on real-world ALOHA manipulation and autonomous\ndriving.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19850.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "649fe21d59c1ae90dbfacf91",
            "avatarUrl": "/avatars/7f77fa77113e80cb45406927e2386387.svg",
            "fullname": "Wang Yuqi",
            "name": "Yuqi1997",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.19713",
            "authors": [
                {
                    "_id": "685b9a5dd2ee4fac76521ecc",
                    "user": {
                        "_id": "63b4b02a103617b0a5b0ee2e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
                        "isPro": false,
                        "fullname": "Seyedmorteza Sadat",
                        "user": "msadat97",
                        "type": "user"
                    },
                    "name": "Seyedmorteza Sadat",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:07:14.606Z",
                    "hidden": false
                },
                {
                    "_id": "685b9a5dd2ee4fac76521ecd",
                    "name": "Tobias Vontobel",
                    "hidden": false
                },
                {
                    "_id": "685b9a5dd2ee4fac76521ece",
                    "name": "Farnood Salehi",
                    "hidden": false
                },
                {
                    "_id": "685b9a5dd2ee4fac76521ecf",
                    "user": {
                        "_id": "630f7646197cd3f24e7f8e9f",
                        "avatarUrl": "/avatars/59bbd4ed38277b313051aac78f6808ac.svg",
                        "isPro": false,
                        "fullname": "Romann Weber",
                        "user": "RMW",
                        "type": "user"
                    },
                    "name": "Romann M. Weber",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T11:04:08.967Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T15:19:42.000Z",
            "submittedOnDailyAt": "2025-06-25T05:16:23.771Z",
            "title": "Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low\n  CFG Scales",
            "submittedOnDailyBy": {
                "_id": "63b4b02a103617b0a5b0ee2e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
                "isPro": false,
                "fullname": "Seyedmorteza Sadat",
                "user": "msadat97",
                "type": "user"
            },
            "summary": "Classifier-free guidance (CFG) has become an essential component of modern\nconditional diffusion models. Although highly effective in practice, the\nunderlying mechanisms by which CFG enhances quality, detail, and prompt\nalignment are not fully understood. We present a novel perspective on CFG by\nanalyzing its effects in the frequency domain, showing that low and high\nfrequencies have distinct impacts on generation quality. Specifically,\nlow-frequency guidance governs global structure and condition alignment, while\nhigh-frequency guidance mainly enhances visual fidelity. However, applying a\nuniform scale across all frequencies -- as is done in standard CFG -- leads to\noversaturation and reduced diversity at high scales and degraded visual quality\nat low scales. Based on these insights, we propose frequency-decoupled guidance\n(FDG), an effective approach that decomposes CFG into low- and high-frequency\ncomponents and applies separate guidance strengths to each component. FDG\nimproves image quality at low guidance scales and avoids the drawbacks of high\nCFG scales by design. Through extensive experiments across multiple datasets\nand models, we demonstrate that FDG consistently enhances sample fidelity while\npreserving diversity, leading to improved FID and recall compared to CFG,\nestablishing our method as a plug-and-play alternative to standard\nclassifier-free guidance.",
            "upvotes": 11,
            "discussionId": "685b9a5ed2ee4fac76521ed0",
            "ai_summary": "Frequency-decoupled guidance (FDG) enhances image quality and diversity by separately controlling low- and high-frequency guidance components in diffusion models, outperforming standard classifier-free guidance.",
            "ai_keywords": [
                "classifier-free guidance",
                "conditional diffusion models",
                "frequency domain",
                "low-frequency guidance",
                "high-frequency guidance",
                "frequency-decoupled guidance",
                "FID",
                "recall"
            ]
        },
        "publishedAt": "2025-06-24T11:19:42.000Z",
        "title": "Guidance in the Frequency Domain Enables High-Fidelity Sampling at Low\n  CFG Scales",
        "summary": "Classifier-free guidance (CFG) has become an essential component of modern\nconditional diffusion models. Although highly effective in practice, the\nunderlying mechanisms by which CFG enhances quality, detail, and prompt\nalignment are not fully understood. We present a novel perspective on CFG by\nanalyzing its effects in the frequency domain, showing that low and high\nfrequencies have distinct impacts on generation quality. Specifically,\nlow-frequency guidance governs global structure and condition alignment, while\nhigh-frequency guidance mainly enhances visual fidelity. However, applying a\nuniform scale across all frequencies -- as is done in standard CFG -- leads to\noversaturation and reduced diversity at high scales and degraded visual quality\nat low scales. Based on these insights, we propose frequency-decoupled guidance\n(FDG), an effective approach that decomposes CFG into low- and high-frequency\ncomponents and applies separate guidance strengths to each component. FDG\nimproves image quality at low guidance scales and avoids the drawbacks of high\nCFG scales by design. Through extensive experiments across multiple datasets\nand models, we demonstrate that FDG consistently enhances sample fidelity while\npreserving diversity, leading to improved FID and recall compared to CFG,\nestablishing our method as a plug-and-play alternative to standard\nclassifier-free guidance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19713.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b4b02a103617b0a5b0ee2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63b4b02a103617b0a5b0ee2e/LasBC-pCnPJ6XuCt6qqcp.jpeg",
            "fullname": "Seyedmorteza Sadat",
            "name": "msadat97",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18945",
            "authors": [
                {
                    "_id": "685c6cb7696820ba1f28f20e",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "685c6cb7696820ba1f28f20f",
                    "name": "Rui Pan",
                    "hidden": false
                },
                {
                    "_id": "685c6cb7696820ba1f28f210",
                    "name": "Jiarui Yao",
                    "hidden": false
                },
                {
                    "_id": "685c6cb7696820ba1f28f211",
                    "name": "Robert Csordas",
                    "hidden": false
                },
                {
                    "_id": "685c6cb7696820ba1f28f212",
                    "name": "Linjie Li",
                    "hidden": false
                },
                {
                    "_id": "685c6cb7696820ba1f28f213",
                    "name": "Lu Yin",
                    "hidden": false
                },
                {
                    "_id": "685c6cb7696820ba1f28f214",
                    "name": "Jiajun Wu",
                    "hidden": false
                },
                {
                    "_id": "685c6cb7696820ba1f28f215",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "685c6cb7696820ba1f28f216",
                    "name": "Manling Li",
                    "hidden": false
                },
                {
                    "_id": "685c6cb7696820ba1f28f217",
                    "name": "Shiwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T02:15:43.000Z",
            "submittedOnDailyAt": "2025-06-25T20:11:05.800Z",
            "title": "Chain-of-Experts: Unlocking the Communication Power of\n  Mixture-of-Experts Models",
            "submittedOnDailyBy": {
                "_id": "63318e014bd187e9ce05cec6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318e014bd187e9ce05cec6/rKN84BAXF_u0osSu6KJy5.jpeg",
                "isPro": true,
                "fullname": "Zihan Wang",
                "user": "ZihanWang314",
                "type": "user"
            },
            "summary": "We propose Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE)\narchitecture that introduces sequential expert communication within each layer.\nUnlike traditional MoE models, where experts operate independently in parallel,\nCoE processes tokens iteratively across a chain of experts inside a layer. To\nsupport dynamic expert selection across iterations, CoE employs a dedicated\nrouter at each iteration step within a layer. This design allows tokens to\nre-evaluate and select different experts during each iteration, rather than\nbeing statically assigned. As a result, CoE introduces a flexible routing\nmechanism that increases the diversity of expert combinations and enriches the\nmodel's representational capacity. CoE demonstrates improved performance under\nfixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to\n1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling\naxis: depth through expert iteration, which complements conventional\nwidth/depth scaling. For example, using 2x iterations matches the performance\nof 3x expert selections (in width), while reducing memory usage by 17.6-42%\nrelative to other scaling strategies. Our analysis reveals that CoE's benefits\nstem from its iterative residual structure and enhanced expert specialization\nempowered by iterative routing, which together unlock more expressive\nrepresentations. Code is available at https://github.com/ZihanWang314/coe.",
            "upvotes": 11,
            "discussionId": "685c6cb7696820ba1f28f218",
            "ai_summary": "Chain-of-Experts (CoE) improves performance and memory efficiency in mixture-of-experts models by iteratively routing tokens through experts within each layer.",
            "ai_keywords": [
                "Chain-of-Experts",
                "Mixture-of-Experts",
                "sequential expert communication",
                "dedicated router",
                "expert iteration",
                "iterative routing",
                "iterative residual structure",
                "expert specialization"
            ]
        },
        "publishedAt": "2025-06-22T22:15:43.000Z",
        "title": "Chain-of-Experts: Unlocking the Communication Power of\n  Mixture-of-Experts Models",
        "summary": "We propose Chain-of-Experts (CoE), a new Mixture-of-Experts (MoE)\narchitecture that introduces sequential expert communication within each layer.\nUnlike traditional MoE models, where experts operate independently in parallel,\nCoE processes tokens iteratively across a chain of experts inside a layer. To\nsupport dynamic expert selection across iterations, CoE employs a dedicated\nrouter at each iteration step within a layer. This design allows tokens to\nre-evaluate and select different experts during each iteration, rather than\nbeing statically assigned. As a result, CoE introduces a flexible routing\nmechanism that increases the diversity of expert combinations and enriches the\nmodel's representational capacity. CoE demonstrates improved performance under\nfixed compute: on math reasoning tasks, it reduces validation loss from 1.20 to\n1.12 compared to a standard MoE. Beyond performance, CoE offers a new scaling\naxis: depth through expert iteration, which complements conventional\nwidth/depth scaling. For example, using 2x iterations matches the performance\nof 3x expert selections (in width), while reducing memory usage by 17.6-42%\nrelative to other scaling strategies. Our analysis reveals that CoE's benefits\nstem from its iterative residual structure and enhanced expert specialization\nempowered by iterative routing, which together unlock more expressive\nrepresentations. Code is available at https://github.com/ZihanWang314/coe.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18945.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63318e014bd187e9ce05cec6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63318e014bd187e9ce05cec6/rKN84BAXF_u0osSu6KJy5.jpeg",
            "fullname": "Zihan Wang",
            "name": "ZihanWang314",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.19838",
            "authors": [
                {
                    "_id": "685b5e05d2ee4fac76521ddd",
                    "user": {
                        "_id": "6315dcb493fdd4a5633f7b30",
                        "avatarUrl": "/avatars/c58d228dbdf4f1edb5aa4e635dacb51d.svg",
                        "isPro": false,
                        "fullname": "Liangbin Xie",
                        "user": "Liangbin",
                        "type": "user"
                    },
                    "name": "Liangbin Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-25T13:32:05.492Z",
                    "hidden": false
                },
                {
                    "_id": "685b5e05d2ee4fac76521dde",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "685b5e05d2ee4fac76521ddf",
                    "name": "Shian Du",
                    "hidden": false
                },
                {
                    "_id": "685b5e05d2ee4fac76521de0",
                    "name": "Menghan Xia",
                    "hidden": false
                },
                {
                    "_id": "685b5e05d2ee4fac76521de1",
                    "name": "Xintao Wang",
                    "hidden": false
                },
                {
                    "_id": "685b5e05d2ee4fac76521de2",
                    "name": "Fanghua Yu",
                    "hidden": false
                },
                {
                    "_id": "685b5e05d2ee4fac76521de3",
                    "name": "Ziyan Chen",
                    "hidden": false
                },
                {
                    "_id": "685b5e05d2ee4fac76521de4",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "685b5e05d2ee4fac76521de5",
                    "name": "Jiantao Zhou",
                    "hidden": false
                },
                {
                    "_id": "685b5e05d2ee4fac76521de6",
                    "name": "Chao Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T17:57:26.000Z",
            "submittedOnDailyAt": "2025-06-25T00:55:41.694Z",
            "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.",
            "upvotes": 9,
            "discussionId": "685b5e05d2ee4fac76521de7",
            "ai_summary": "Researchers propose design principles for cascaded video super-resolution models to improve high-resolution video generation by introduces degradation strategies, timestep sampling, noise augmentation, and interleaving temporal units with sparse local attention.",
            "ai_keywords": [
                "latent diffusion models",
                "video generation",
                "cascaded video super-resolution",
                "VSR",
                "degradation strategies",
                "timestep sampling",
                "noise augmentation",
                "interleaving temporal unit",
                "sparse local attention"
            ]
        },
        "publishedAt": "2025-06-24T13:57:26.000Z",
        "title": "SimpleGVR: A Simple Baseline for Latent-Cascaded Video Super-Resolution",
        "summary": "Latent diffusion models have emerged as a leading paradigm for efficient\nvideo generation. However, as user expectations shift toward higher-resolution\noutputs, relying solely on latent computation becomes inadequate. A promising\napproach involves decoupling the process into two stages: semantic content\ngeneration and detail synthesis. The former employs a computationally intensive\nbase model at lower resolutions, while the latter leverages a lightweight\ncascaded video super-resolution (VSR) model to achieve high-resolution output.\nIn this work, we focus on studying key design principles for latter cascaded\nVSR models, which are underexplored currently. First, we propose two\ndegradation strategies to generate training pairs that better mimic the output\ncharacteristics of the base model, ensuring alignment between the VSR model and\nits upstream generator. Second, we provide critical insights into VSR model\nbehavior through systematic analysis of (1) timestep sampling strategies, (2)\nnoise augmentation effects on low-resolution (LR) inputs. These findings\ndirectly inform our architectural and training innovations. Finally, we\nintroduce interleaving temporal unit and sparse local attention to achieve\nefficient training and inference, drastically reducing computational overhead.\nExtensive experiments demonstrate the superiority of our framework over\nexisting methods, with ablation studies confirming the efficacy of each design\nchoice. Our work establishes a simple yet effective baseline for cascaded video\nsuper-resolution generation, offering practical insights to guide future\nadvancements in efficient cascaded synthesis systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19838.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7186
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.19767",
            "authors": [
                {
                    "_id": "685b5791d2ee4fac76521dc2",
                    "user": {
                        "_id": "670aa09d35918e99fe7ff6b1",
                        "avatarUrl": "/avatars/5cbea2284165191e96544bacf2bfb50f.svg",
                        "isPro": false,
                        "fullname": "Yuqian Fu",
                        "user": "Yuqian-Fu",
                        "type": "user"
                    },
                    "name": "Yuqian Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:08:54.115Z",
                    "hidden": false
                },
                {
                    "_id": "685b5791d2ee4fac76521dc3",
                    "name": "Tinghong Chen",
                    "hidden": false
                },
                {
                    "_id": "685b5791d2ee4fac76521dc4",
                    "name": "Jiajun Chai",
                    "hidden": false
                },
                {
                    "_id": "685b5791d2ee4fac76521dc5",
                    "name": "Xihuai Wang",
                    "hidden": false
                },
                {
                    "_id": "685b5791d2ee4fac76521dc6",
                    "user": {
                        "_id": "66e14f4142ceed655c731966",
                        "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
                        "isPro": false,
                        "fullname": "SONGJUN TU",
                        "user": "SONGJUNTU",
                        "type": "user"
                    },
                    "name": "Songjun Tu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:08:52.066Z",
                    "hidden": false
                },
                {
                    "_id": "685b5791d2ee4fac76521dc7",
                    "name": "Guojun Yin",
                    "hidden": false
                },
                {
                    "_id": "685b5791d2ee4fac76521dc8",
                    "name": "Wei Lin",
                    "hidden": false
                },
                {
                    "_id": "685b5791d2ee4fac76521dc9",
                    "name": "Qichao Zhang",
                    "hidden": false
                },
                {
                    "_id": "685b5791d2ee4fac76521dca",
                    "name": "Yuanheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "685b5791d2ee4fac76521dcb",
                    "name": "Dongbin Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T16:31:37.000Z",
            "submittedOnDailyAt": "2025-06-25T01:33:02.160Z",
            "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
            "submittedOnDailyBy": {
                "_id": "66e14f4142ceed655c731966",
                "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
                "isPro": false,
                "fullname": "SONGJUN TU",
                "user": "SONGJUNTU",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
            "upvotes": 9,
            "discussionId": "685b5792d2ee4fac76521dcc",
            "projectPage": "https://anonymous.4open.science/w/SRFT2025",
            "ai_summary": " Supervised Reinforcement Fine-Tuning (SRFT) integrates Supervised Fine-Tuning and Reinforcement Learning through entropy-aware weighting to achieve high accuracy in language model optimization.",
            "ai_keywords": [
                "Large language models",
                "Supervised Fine-Tuning",
                "Reinforcement Learning",
                "token distributions",
                "learning dynamics",
                "entropy",
                "Supervised Reinforcement Fine-Tuning"
            ]
        },
        "publishedAt": "2025-06-24T12:31:37.000Z",
        "title": "SRFT: A Single-Stage Method with Supervised and Reinforcement\n  Fine-Tuning for Reasoning",
        "summary": "Large language models (LLMs) have achieved remarkable progress in reasoning\ntasks, yet the optimal integration of Supervised Fine-Tuning (SFT) and\nReinforcement Learning (RL) remains a fundamental challenge. Through\ncomprehensive analysis of token distributions, learning dynamics, and\nintegration mechanisms from entropy-based perspectives, we reveal key\ndifferences between these paradigms: SFT induces coarse-grained global changes\nto LLM policy distributions, while RL performs fine-grained selective\noptimizations, with entropy serving as a critical indicator of training\neffectiveness. Building on these observations, we propose Supervised\nReinforcement Fine-Tuning (SRFT), a single-stage method that unifies both\nfine-tuning paradigms through entropy-aware weighting mechanisms. Our approach\nsimultaneously applies SFT and RL to directly optimize the LLM using\ndemonstrations and self-exploration rollouts rather than through two-stage\nsequential methods. Extensive experiments show that SRFT achieves 59.1% average\naccuracy, outperforming zero-RL methods by 9.0% on five mathematical reasoning\nbenchmarks and 10.9% on three out-of-distribution benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19767.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66e14f4142ceed655c731966",
            "avatarUrl": "/avatars/d708562349913b89c8c4cf384628f82a.svg",
            "fullname": "SONGJUN TU",
            "name": "SONGJUNTU",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18843",
            "authors": [
                {
                    "_id": "685a06460e4ad7e2197584c0",
                    "user": {
                        "_id": "6179f36a2a4e9edab3a95798",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
                        "isPro": false,
                        "fullname": "Heng-Jui Chang",
                        "user": "vectominist",
                        "type": "user"
                    },
                    "name": "Heng-Jui Chang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:08:49.104Z",
                    "hidden": false
                },
                {
                    "_id": "685a06460e4ad7e2197584c1",
                    "user": {
                        "_id": "67d301cbba86f5d66eb73d7c",
                        "avatarUrl": "/avatars/8546bbd2145c16d4be5675624516b649.svg",
                        "isPro": false,
                        "fullname": "Saurabhchand Bhati",
                        "user": "saurabhati",
                        "type": "user"
                    },
                    "name": "Saurabhchand Bhati",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:09:27.586Z",
                    "hidden": false
                },
                {
                    "_id": "685a06460e4ad7e2197584c2",
                    "name": "James Glass",
                    "hidden": false
                },
                {
                    "_id": "685a06460e4ad7e2197584c3",
                    "name": "Alexander H. Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
            ],
            "publishedAt": "2025-06-23T17:02:00.000Z",
            "submittedOnDailyAt": "2025-06-25T02:21:09.630Z",
            "title": "USAD: Universal Speech and Audio Representation via Distillation",
            "submittedOnDailyBy": {
                "_id": "6179f36a2a4e9edab3a95798",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
                "isPro": false,
                "fullname": "Heng-Jui Chang",
                "user": "vectominist",
                "type": "user"
            },
            "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.",
            "upvotes": 9,
            "discussionId": "685a06470e4ad7e2197584c4",
            "ai_summary": "USAD integrates diverse audio types using efficient layer-to-layer distillation from domain-specific models, achieving competitive performance across various benchmarks with a single encoder.",
            "ai_keywords": [
                "self-supervised learning",
                "universal speech and audio distillation",
                "domain-specific models",
                "layer-to-layer distillation",
                "frame and instance-level speech processing",
                "audio tagging",
                "sound classification",
                "encoder",
                "SUPERB benchmarks",
                "HEAR benchmarks"
            ]
        },
        "publishedAt": "2025-06-23T13:02:00.000Z",
        "title": "USAD: Universal Speech and Audio Representation via Distillation",
        "summary": "Self-supervised learning (SSL) has revolutionized audio representations, yet\nmodels often remain domain-specific, focusing on either speech or non-speech\ntasks. In this work, we present Universal Speech and Audio Distillation (USAD),\na unified approach to audio representation learning that integrates diverse\naudio types - speech, sound, and music - into a single model. USAD employs\nefficient layer-to-layer distillation from domain-specific SSL models to train\na student on a comprehensive audio dataset. USAD offers competitive performance\nacross various benchmarks and datasets, including frame and instance-level\nspeech processing tasks, audio tagging, and sound classification, achieving\nnear state-of-the-art results with a single encoder on SUPERB and HEAR\nbenchmarks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6179f36a2a4e9edab3a95798/QPPxw2hyPglQF_q3uWSyk.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18843.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6179f36a2a4e9edab3a95798",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6179f36a2a4e9edab3a95798/0mmFY5lFzPC5k6_GSdmYQ.jpeg",
            "fullname": "Heng-Jui Chang",
            "name": "vectominist",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.14012",
            "authors": [
                {
                    "_id": "685b863bd2ee4fac76521e92",
                    "user": {
                        "_id": "655efd24afee0e00788bb589",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
                        "isPro": false,
                        "fullname": "Amr Mohamed",
                        "user": "amr-mohamed",
                        "type": "user"
                    },
                    "name": "Amr Mohamed",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:07:19.266Z",
                    "hidden": false
                },
                {
                    "_id": "685b863bd2ee4fac76521e93",
                    "user": {
                        "_id": "64f1f92eefacc7da583a9e22",
                        "avatarUrl": "/avatars/18abbf9e31ca916f8d9a4495639a1329.svg",
                        "isPro": false,
                        "fullname": "Yang ZHANG",
                        "user": "yangzhang33",
                        "type": "user"
                    },
                    "name": "Yang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T12:56:26.602Z",
                    "hidden": false
                },
                {
                    "_id": "685b863bd2ee4fac76521e94",
                    "name": "Michalis Vazirgiannis",
                    "hidden": false
                },
                {
                    "_id": "685b863bd2ee4fac76521e95",
                    "user": {
                        "_id": "6087e598e2b7cc3a117b0dc5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
                        "isPro": false,
                        "fullname": "Guokan Shang",
                        "user": "guokan-shang",
                        "type": "user"
                    },
                    "name": "Guokan Shang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:07:16.772Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-16T21:19:27.000Z",
            "submittedOnDailyAt": "2025-06-25T03:51:26.828Z",
            "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text",
            "submittedOnDailyBy": {
                "_id": "655efd24afee0e00788bb589",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
                "isPro": false,
                "fullname": "Amr Mohamed",
                "user": "amr-mohamed",
                "type": "user"
            },
            "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English textx2013even under linguistic\nconstraintsx2013embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.",
            "upvotes": 9,
            "discussionId": "685b863bd2ee4fac76521e96",
            "ai_summary": "LLMs' comprehension and reasoning skills are evaluated under code-switching conditions, revealing that embedding English into other languages can improve understanding, while prompts and fine-tuning affect degradation mitigation differently.",
            "ai_keywords": [
                "Large Language Models",
                "code-switching",
                "CSW",
                "reasoning benchmarks",
                "comprehension benchmarks",
                "foreign tokens",
                "embedding",
                "fine-tuning"
            ]
        },
        "publishedAt": "2025-06-16T17:19:27.000Z",
        "title": "Lost in the Mix: Evaluating LLM Understanding of Code-Switched Text",
        "summary": "Code-switching (CSW) is the act of alternating between two or more languages\nwithin a single discourse. This phenomenon is widespread in multilingual\ncommunities, and increasingly prevalent in online content, where users\nnaturally mix languages in everyday communication. As a result, Large Language\nModels (LLMs), now central to content processing and generation, are frequently\nexposed to code-switched inputs. Given their widespread use, it is crucial to\nunderstand how LLMs process and reason about such mixed-language text. This\npaper presents a systematic evaluation of LLM comprehension under\ncode-switching by generating CSW variants of established reasoning and\ncomprehension benchmarks. While degradation is evident when foreign tokens\ndisrupt English textx2013even under linguistic\nconstraintsx2013embedding English into other languages often\nimproves comprehension. Though prompting yields mixed results, fine-tuning\noffers a more stable path to degradation mitigation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.14012.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655efd24afee0e00788bb589",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
            "fullname": "Amr Mohamed",
            "name": "amr-mohamed",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.19794",
            "authors": [
                {
                    "_id": "685b75d0d2ee4fac76521e70",
                    "user": {
                        "_id": "679b6d152583e6629337c592",
                        "avatarUrl": "/avatars/7ad77d7384fd41ad987b6eb5907a8f08.svg",
                        "isPro": false,
                        "fullname": "Yuqi Zhu",
                        "user": "Yukirsh",
                        "type": "user"
                    },
                    "name": "Yuqi Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T13:32:57.204Z",
                    "hidden": false
                },
                {
                    "_id": "685b75d0d2ee4fac76521e71",
                    "name": "Yi Zhong",
                    "hidden": false
                },
                {
                    "_id": "685b75d0d2ee4fac76521e72",
                    "name": "Jintian Zhang",
                    "hidden": false
                },
                {
                    "_id": "685b75d0d2ee4fac76521e73",
                    "name": "Ziheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "685b75d0d2ee4fac76521e74",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "685b75d0d2ee4fac76521e75",
                    "name": "Yujie Luo",
                    "hidden": false
                },
                {
                    "_id": "685b75d0d2ee4fac76521e76",
                    "name": "Lun Du",
                    "hidden": false
                },
                {
                    "_id": "685b75d0d2ee4fac76521e77",
                    "name": "Da Zheng",
                    "hidden": false
                },
                {
                    "_id": "685b75d0d2ee4fac76521e78",
                    "name": "Huajun Chen",
                    "hidden": false
                },
                {
                    "_id": "685b75d0d2ee4fac76521e79",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T11:04:12.367Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T17:04:23.000Z",
            "submittedOnDailyAt": "2025-06-25T02:37:00.536Z",
            "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
            "upvotes": 8,
            "discussionId": "685b75d1d2ee4fac76521e7a",
            "ai_summary": "Enhancements to open-source large language models' data analysis capabilities through strategic planning, interaction design, and data quality improvements were identified and applied.",
            "ai_keywords": [
                "Large Language Models",
                "data analysis",
                "data understanding",
                "code generation",
                "strategic planning",
                "interaction design",
                "task complexity",
                "data quality",
                "data synthesis methodology"
            ]
        },
        "publishedAt": "2025-06-24T13:04:23.000Z",
        "title": "Why Do Open-Source LLMs Struggle with Data Analysis? A Systematic\n  Empirical Study",
        "summary": "Large Language Models (LLMs) hold promise in automating data analysis tasks,\nyet open-source models face significant limitations in these kinds of\nreasoning-intensive scenarios. In this work, we investigate strategies to\nenhance the data analysis capabilities of open-source LLMs. By curating a seed\ndataset of diverse, realistic scenarios, we evaluate models across three\ndimensions: data understanding, code generation, and strategic planning. Our\nanalysis reveals three key findings: (1) Strategic planning quality serves as\nthe primary determinant of model performance; (2) Interaction design and task\ncomplexity significantly influence reasoning capabilities; (3) Data quality\ndemonstrates a greater impact than diversity in achieving optimal performance.\nWe leverage these insights to develop a data synthesis methodology,\ndemonstrating significant improvements in open-source LLMs' analytical\nreasoning capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19794.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 24
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.19839",
            "authors": [
                {
                    "_id": "685b6d7dd2ee4fac76521e5a",
                    "user": {
                        "_id": "665aa3c1957e1e79ff031acb",
                        "avatarUrl": "/avatars/8fbe062bc8c635337991b5bf90c07052.svg",
                        "isPro": false,
                        "fullname": "Moayed Haji Ali",
                        "user": "mali6",
                        "type": "user"
                    },
                    "name": "Moayed Haji-Ali",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T08:08:19.723Z",
                    "hidden": false
                },
                {
                    "_id": "685b6d7dd2ee4fac76521e5b",
                    "name": "Willi Menapace",
                    "hidden": false
                },
                {
                    "_id": "685b6d7dd2ee4fac76521e5c",
                    "name": "Ivan Skorokhodov",
                    "hidden": false
                },
                {
                    "_id": "685b6d7dd2ee4fac76521e5d",
                    "name": "Arpit Sahni",
                    "hidden": false
                },
                {
                    "_id": "685b6d7dd2ee4fac76521e5e",
                    "name": "Sergey Tulyakov",
                    "hidden": false
                },
                {
                    "_id": "685b6d7dd2ee4fac76521e5f",
                    "name": "Vicente Ordonez",
                    "hidden": false
                },
                {
                    "_id": "685b6d7dd2ee4fac76521e60",
                    "name": "Aliaksandr Siarohin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T17:58:02.000Z",
            "submittedOnDailyAt": "2025-06-25T14:08:58.473Z",
            "title": "Improving Progressive Generation with Decomposable Flow Matching",
            "submittedOnDailyBy": {
                "_id": "665aa3c1957e1e79ff031acb",
                "avatarUrl": "/avatars/8fbe062bc8c635337991b5bf90c07052.svg",
                "isPro": false,
                "fullname": "Moayed Haji Ali",
                "user": "mali6",
                "type": "user"
            },
            "summary": "Generating high-dimensional visual modalities is a computationally intensive\ntask. A common solution is progressive generation, where the outputs are\nsynthesized in a coarse-to-fine spectral autoregressive manner. While diffusion\nmodels benefit from the coarse-to-fine nature of denoising, explicit\nmulti-stage architectures are rarely adopted. These architectures have\nincreased the complexity of the overall approach, introducing the need for a\ncustom diffusion formulation, decomposition-dependent stage transitions,\nadd-hoc samplers, or a model cascade. Our contribution, Decomposable Flow\nMatching (DFM), is a simple and effective framework for the progressive\ngeneration of visual media. DFM applies Flow Matching independently at each\nlevel of a user-defined multi-scale representation (such as Laplacian pyramid).\nAs shown by our experiments, our approach improves visual quality for both\nimages and videos, featuring superior results compared to prior multistage\nframeworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores\nover the base architecture and 26.4% over the best-performing baseline, under\nthe same training compute. When applied to finetuning of large models, such as\nFLUX, DFM shows faster convergence speed to the training distribution.\nCrucially, all these advantages are achieved with a single model, architectural\nsimplicity, and minimal modifications to existing training pipelines.",
            "upvotes": 5,
            "discussionId": "685b6d7dd2ee4fac76521e61",
            "ai_summary": "Decomposable Flow Matching (DFM) framework enhances visual generation and video quality by applying Flow Matching at multiple scales without requiring complex multi-stage architectures.",
            "ai_keywords": [
                "diffusion models",
                "denoising",
                "Flow Matching",
                "Laplacian pyramid",
                "multi-scale representation",
                "progressive generation",
                "Imagenet-1k",
                "FDD scores",
                "finetuning",
                "FLUX",
                "training distribution",
                "convergence speed"
            ]
        },
        "publishedAt": "2025-06-24T13:58:02.000Z",
        "title": "Improving Progressive Generation with Decomposable Flow Matching",
        "summary": "Generating high-dimensional visual modalities is a computationally intensive\ntask. A common solution is progressive generation, where the outputs are\nsynthesized in a coarse-to-fine spectral autoregressive manner. While diffusion\nmodels benefit from the coarse-to-fine nature of denoising, explicit\nmulti-stage architectures are rarely adopted. These architectures have\nincreased the complexity of the overall approach, introducing the need for a\ncustom diffusion formulation, decomposition-dependent stage transitions,\nadd-hoc samplers, or a model cascade. Our contribution, Decomposable Flow\nMatching (DFM), is a simple and effective framework for the progressive\ngeneration of visual media. DFM applies Flow Matching independently at each\nlevel of a user-defined multi-scale representation (such as Laplacian pyramid).\nAs shown by our experiments, our approach improves visual quality for both\nimages and videos, featuring superior results compared to prior multistage\nframeworks. On Imagenet-1k 512px, DFM achieves 35.2% improvements in FDD scores\nover the base architecture and 26.4% over the best-performing baseline, under\nthe same training compute. When applied to finetuning of large models, such as\nFLUX, DFM shows faster convergence speed to the training distribution.\nCrucially, all these advantages are achieved with a single model, architectural\nsimplicity, and minimal modifications to existing training pipelines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19839.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "665aa3c1957e1e79ff031acb",
            "avatarUrl": "/avatars/8fbe062bc8c635337991b5bf90c07052.svg",
            "fullname": "Moayed Haji Ali",
            "name": "mali6",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.19830",
            "authors": [
                {
                    "_id": "685c0361df8a0d6c70bbf91d",
                    "user": {
                        "_id": "62ba66296501b0ff15ba1075",
                        "avatarUrl": "/avatars/de468727cb1240fd4b1f24a19fb237a6.svg",
                        "isPro": true,
                        "fullname": "Yichao Fu",
                        "user": "Viol2000",
                        "type": "user"
                    },
                    "name": "Yichao Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T20:59:06.015Z",
                    "hidden": false
                },
                {
                    "_id": "685c0361df8a0d6c70bbf91e",
                    "name": "Rui Ge",
                    "hidden": false
                },
                {
                    "_id": "685c0361df8a0d6c70bbf91f",
                    "name": "Zelei Shao",
                    "hidden": false
                },
                {
                    "_id": "685c0361df8a0d6c70bbf920",
                    "name": "Zhijie Deng",
                    "hidden": false
                },
                {
                    "_id": "685c0361df8a0d6c70bbf921",
                    "name": "Hao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T17:48:10.000Z",
            "submittedOnDailyAt": "2025-06-25T12:41:53.934Z",
            "title": "Scaling Speculative Decoding with Lookahead Reasoning",
            "submittedOnDailyBy": {
                "_id": "62ba66296501b0ff15ba1075",
                "avatarUrl": "/avatars/de468727cb1240fd4b1f24a19fb237a6.svg",
                "isPro": true,
                "fullname": "Yichao Fu",
                "user": "Viol2000",
                "type": "user"
            },
            "summary": "Reasoning models excel by generating long chain-of-thoughts, but decoding the\nresulting thousands of tokens is slow. Token-level speculative decoding (SD)\nhelps, but its benefit is capped, because the chance that an entire\ngamma-token guess is correct falls exponentially as gamma grows. This\nmeans allocating more compute for longer token drafts faces an algorithmic\nceiling -- making the speedup modest and hardware-agnostic. We raise this\nceiling with Lookahead Reasoning, which exploits a second, step-level layer of\nparallelism. Our key insight is that reasoning models generate step-by-step,\nand each step needs only to be semantically correct, not exact token matching.\nIn Lookahead Reasoning, a lightweight draft model proposes several future\nsteps; the target model expands each proposal in one batched pass, and a\nverifier keeps semantically correct steps while letting the target regenerate\nany that fail. Token-level SD still operates within each reasoning step, so the\ntwo layers of parallelism multiply. We show Lookahead Reasoning lifts the peak\nspeedup of SD both theoretically and empirically. Across GSM8K, AIME, and other\nbenchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x\nwhile preserving answer quality, and its speedup scales better with additional\nGPU throughput. Our code is available at\nhttps://github.com/hao-ai-lab/LookaheadReasoning",
            "upvotes": 5,
            "discussionId": "685c0361df8a0d6c70bbf922",
            "ai_summary": "Lookahead Reasoning enhances the speed of speculative decoding by introducing step-level parallelism, improving speedup over token-level decoding while maintaining answer quality.",
            "ai_keywords": [
                "Lookahead Reasoning",
                "speculative decoding",
                "step-level parallelism",
                "token-level speculative decoding",
                "draft model",
                "target model",
                "verifier",
                "GSM8K",
                "AIME"
            ]
        },
        "publishedAt": "2025-06-24T13:48:10.000Z",
        "title": "Scaling Speculative Decoding with Lookahead Reasoning",
        "summary": "Reasoning models excel by generating long chain-of-thoughts, but decoding the\nresulting thousands of tokens is slow. Token-level speculative decoding (SD)\nhelps, but its benefit is capped, because the chance that an entire\ngamma-token guess is correct falls exponentially as gamma grows. This\nmeans allocating more compute for longer token drafts faces an algorithmic\nceiling -- making the speedup modest and hardware-agnostic. We raise this\nceiling with Lookahead Reasoning, which exploits a second, step-level layer of\nparallelism. Our key insight is that reasoning models generate step-by-step,\nand each step needs only to be semantically correct, not exact token matching.\nIn Lookahead Reasoning, a lightweight draft model proposes several future\nsteps; the target model expands each proposal in one batched pass, and a\nverifier keeps semantically correct steps while letting the target regenerate\nany that fail. Token-level SD still operates within each reasoning step, so the\ntwo layers of parallelism multiply. We show Lookahead Reasoning lifts the peak\nspeedup of SD both theoretically and empirically. Across GSM8K, AIME, and other\nbenchmarks, Lookahead Reasoning improves the speedup of SD from 1.4x to 2.1x\nwhile preserving answer quality, and its speedup scales better with additional\nGPU throughput. Our code is available at\nhttps://github.com/hao-ai-lab/LookaheadReasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19830.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62ba66296501b0ff15ba1075",
            "avatarUrl": "/avatars/de468727cb1240fd4b1f24a19fb237a6.svg",
            "fullname": "Yichao Fu",
            "name": "Viol2000",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.19807",
            "authors": [
                {
                    "_id": "685b75edd2ee4fac76521e7c",
                    "name": "Baochang Ren",
                    "hidden": false
                },
                {
                    "_id": "685b75edd2ee4fac76521e7d",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "685b75edd2ee4fac76521e7e",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "685b75edd2ee4fac76521e7f",
                    "name": "Huajun Chen",
                    "hidden": false
                },
                {
                    "_id": "685b75edd2ee4fac76521e80",
                    "user": {
                        "_id": "620b3bbb0668e435407c8d0a",
                        "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                        "isPro": false,
                        "fullname": "Ningyu Zhang",
                        "user": "Ningyu",
                        "type": "user"
                    },
                    "name": "Ningyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T11:04:10.718Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T17:17:17.000Z",
            "submittedOnDailyAt": "2025-06-25T02:37:40.331Z",
            "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": false,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
            "upvotes": 5,
            "discussionId": "685b75edd2ee4fac76521e81",
            "ai_summary": "KnowRL, a knowledge-enhanced reinforcement learning approach, reduces hallucinations in slow-thinking large language models by incorporating factuality rewards based on knowledge verification during training.",
            "ai_keywords": [
                "Large Language Models",
                "slow-thinking models",
                "hallucination",
                "Reinforcement Learning",
                "KnowRL",
                "factuality reward",
                "knowledge verification",
                "reasoning",
                "reasoning capabilities"
            ]
        },
        "publishedAt": "2025-06-24T13:17:17.000Z",
        "title": "KnowRL: Exploring Knowledgeable Reinforcement Learning for Factuality",
        "summary": "Large Language Models (LLMs), particularly slow-thinking models, often\nexhibit severe hallucination, outputting incorrect content due to an inability\nto accurately recognize knowledge boundaries during reasoning. While\nReinforcement Learning (RL) can enhance complex reasoning abilities, its\noutcome-oriented reward mechanism often lacks factual supervision over the\nthinking process, further exacerbating the hallucination problem. To address\nthe high hallucination in slow-thinking models, we propose Knowledge-enhanced\nRL, KnowRL. KnowRL guides models to perform fact-based slow thinking by\nintegrating a factuality reward, based on knowledge verification, into the RL\ntraining process, helping them recognize their knowledge boundaries. KnowRL\nguides models to perform fact-based slow thinking by integrating a factuality\nreward, based on knowledge verification, into the RL training process, helping\nthem recognize their knowledge boundaries. This targeted factual input during\nRL training enables the model to learn and internalize fact-based reasoning\nstrategies. By directly rewarding adherence to facts within the reasoning\nsteps, KnowRL fosters a more reliable thinking process. Experimental results on\nthree hallucination evaluation datasets and two reasoning evaluation datasets\ndemonstrate that KnowRL effectively mitigates hallucinations in slow-thinking\nmodels while maintaining their original strong reasoning capabilities. Our code\nis available at https://github.com/zjunlp/KnowRL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19807.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 24
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.16095",
            "authors": [
                {
                    "_id": "6858b7e0c0c8e29df8ea3c34",
                    "name": "Xun Liu",
                    "hidden": false
                },
                {
                    "_id": "6858b7e0c0c8e29df8ea3c35",
                    "name": "Xiaobin Wu",
                    "hidden": false
                },
                {
                    "_id": "6858b7e0c0c8e29df8ea3c36",
                    "name": "Jiaqi He",
                    "hidden": false
                },
                {
                    "_id": "6858b7e0c0c8e29df8ea3c37",
                    "user": {
                        "_id": "67a3002c637d195f3c4bf371",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
                        "isPro": false,
                        "fullname": "Rajan Das Gupta",
                        "user": "rajandasgupta",
                        "type": "user"
                    },
                    "name": "Rajan Das Gupta",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-23T08:15:00.029Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/wCnHflgqkL983T3LdLLrx.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/MpGm52JrHe3cwwE9j2Iw5.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/t4_MkdqzdTAO15E0vDWhx.png",
                "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/E1IudHPH4ZZXiveMGc4ea.png"
            ],
            "publishedAt": "2025-06-19T07:31:03.000Z",
            "submittedOnDailyAt": "2025-06-25T08:56:10.173Z",
            "title": "Intelligent Operation and Maintenance and Prediction Model Optimization\n  for Improving Wind Power Generation Efficiency",
            "submittedOnDailyBy": {
                "_id": "67a3002c637d195f3c4bf371",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
                "isPro": false,
                "fullname": "Rajan Das Gupta",
                "user": "rajandasgupta",
                "type": "user"
            },
            "summary": "This study explores the effectiveness of predictive maintenance models and\nthe optimization of intelligent Operation and Maintenance (O&M) systems in\nimproving wind power generation efficiency. Through qualitative research,\nstructured interviews were conducted with five wind farm engineers and\nmaintenance managers, each with extensive experience in turbine operations.\nUsing thematic analysis, the study revealed that while predictive maintenance\nmodels effectively reduce downtime by identifying major faults, they often\nstruggle with detecting smaller, gradual failures. Key challenges identified\ninclude false positives, sensor malfunctions, and difficulties in integrating\nnew models with older turbine systems. Advanced technologies such as digital\ntwins, SCADA systems, and condition monitoring have significantly enhanced\nturbine maintenance practices. However, these technologies still require\nimprovements, particularly in AI refinement and real-time data integration. The\nfindings emphasize the need for continuous development to fully optimize wind\nturbine performance and support the broader adoption of renewable energy.",
            "upvotes": 4,
            "discussionId": "6858b7e1c0c8e29df8ea3c38"
        },
        "publishedAt": "2025-06-19T03:31:03.000Z",
        "title": "Intelligent Operation and Maintenance and Prediction Model Optimization\n  for Improving Wind Power Generation Efficiency",
        "summary": "This study explores the effectiveness of predictive maintenance models and\nthe optimization of intelligent Operation and Maintenance (O&M) systems in\nimproving wind power generation efficiency. Through qualitative research,\nstructured interviews were conducted with five wind farm engineers and\nmaintenance managers, each with extensive experience in turbine operations.\nUsing thematic analysis, the study revealed that while predictive maintenance\nmodels effectively reduce downtime by identifying major faults, they often\nstruggle with detecting smaller, gradual failures. Key challenges identified\ninclude false positives, sensor malfunctions, and difficulties in integrating\nnew models with older turbine systems. Advanced technologies such as digital\ntwins, SCADA systems, and condition monitoring have significantly enhanced\nturbine maintenance practices. However, these technologies still require\nimprovements, particularly in AI refinement and real-time data integration. The\nfindings emphasize the need for continuous development to fully optimize wind\nturbine performance and support the broader adoption of renewable energy.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/wCnHflgqkL983T3LdLLrx.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/MpGm52JrHe3cwwE9j2Iw5.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/t4_MkdqzdTAO15E0vDWhx.png",
            "https://cdn-uploads.huggingface.co/production/uploads/67a3002c637d195f3c4bf371/E1IudHPH4ZZXiveMGc4ea.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16095.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a3002c637d195f3c4bf371",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JbaMFjmWFDd-tZmUxQ8aT.jpeg",
            "fullname": "Rajan Das Gupta",
            "name": "rajandasgupta",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.19847",
            "authors": [
                {
                    "_id": "685c03e9df8a0d6c70bbf924",
                    "name": "Zeju Qiu",
                    "hidden": false
                },
                {
                    "_id": "685c03e9df8a0d6c70bbf925",
                    "name": "Weiyang Liu",
                    "hidden": false
                },
                {
                    "_id": "685c03e9df8a0d6c70bbf926",
                    "name": "Adrian Weller",
                    "hidden": false
                },
                {
                    "_id": "685c03e9df8a0d6c70bbf927",
                    "name": "Bernhard Schlkopf",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T17:59:49.000Z",
            "submittedOnDailyAt": "2025-06-25T12:43:42.012Z",
            "title": "Orthogonal Finetuning Made Scalable",
            "submittedOnDailyBy": {
                "_id": "648905d1a15c43c791d4381f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
                "isPro": false,
                "fullname": "Weiyang Liu",
                "user": "wy1iu",
                "type": "user"
            },
            "summary": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage.",
            "upvotes": 2,
            "discussionId": "685c03eadf8a0d6c70bbf928",
            "ai_summary": "OFTv2 optimizes orthogonal fine-tuning by shifting from matrix-matrix to matrix-vector multiplications and introducing efficient Cayley-Neumann parameterization, enhancing speed, memory usage, and performance.",
            "ai_keywords": [
                "Orthogonal finetuning (OFT)",
                "weight-centric implementation",
                "matrix-matrix multiplications",
                "matrix-vector multiplications",
                "Cayley-Neumann parameterization",
                "matrix-free computation",
                "Cayley transform",
                "Neumann series",
                "GPU memory usage",
                "QLoRA"
            ]
        },
        "publishedAt": "2025-06-24T13:59:49.000Z",
        "title": "Orthogonal Finetuning Made Scalable",
        "summary": "Orthogonal finetuning (OFT) offers highly parameter-efficient adaptation\nwhile preventing catastrophic forgetting, but its high runtime and memory\ndemands limit practical deployment. We identify the core computational\nbottleneck in OFT as its weight-centric implementation, which relies on costly\nmatrix-matrix multiplications with cubic complexity. To overcome this, we\npropose OFTv2, an input-centric reformulation that instead uses matrix-vector\nmultiplications (i.e., matrix-free computation), reducing the computational\ncost to quadratic. We further introduce the Cayley-Neumann parameterization, an\nefficient orthogonal parameterization that approximates the matrix inversion in\nCayley transform via a truncated Neumann series. These modifications allow\nOFTv2 to achieve up to 10x faster training and 3x lower GPU memory usage\nwithout compromising performance. In addition, we extend OFTv2 to support\nfinetuning quantized foundation models and show that it outperforms the popular\nQLoRA in training stability, efficiency, and memory usage.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19847.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "648905d1a15c43c791d4381f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648905d1a15c43c791d4381f/GpqGBzsLiMHX0gWZEz3qn.jpeg",
            "fullname": "Weiyang Liu",
            "name": "wy1iu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.19433",
            "authors": [
                {
                    "_id": "685bd821aafa002dfd2bdf09",
                    "name": "Lixuan He",
                    "hidden": false
                },
                {
                    "_id": "685bd821aafa002dfd2bdf0a",
                    "name": "Haoyu Dong",
                    "hidden": false
                },
                {
                    "_id": "685bd821aafa002dfd2bdf0b",
                    "name": "Zhenxing Chen",
                    "hidden": false
                },
                {
                    "_id": "685bd821aafa002dfd2bdf0c",
                    "name": "Yangcheng Yu",
                    "hidden": false
                },
                {
                    "_id": "685bd821aafa002dfd2bdf0d",
                    "user": {
                        "_id": "6465d3bd63e7e09dd02e95c3",
                        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                        "isPro": false,
                        "fullname": "Jie Feng",
                        "user": "JJ-TMT",
                        "type": "user"
                    },
                    "name": "Jie Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-25T12:56:19.744Z",
                    "hidden": false
                },
                {
                    "_id": "685bd821aafa002dfd2bdf0e",
                    "name": "Yong Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/y2o42dXy8IQu2LgEurTV4.jpeg"
            ],
            "publishedAt": "2025-06-24T09:00:43.000Z",
            "submittedOnDailyAt": "2025-06-25T09:39:02.618Z",
            "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
            "submittedOnDailyBy": {
                "_id": "6465d3bd63e7e09dd02e95c3",
                "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                "isPro": false,
                "fullname": "Jie Feng",
                "user": "JJ-TMT",
                "type": "user"
            },
            "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\nMem4Nav, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
            "upvotes": 2,
            "discussionId": "685bd821aafa002dfd2bdf0f",
            "githubRepo": "https://github.com/tsinghua-fib-lab/Mem4Nav",
            "ai_summary": "Mem4Nav enhances Vision-and-Language Navigation by integrating a hierarchical spatial-cognition system with dual-memory modules using a reversible Transformer for improved task completion, speed, and detour detection.",
            "ai_keywords": [
                "sparse octree",
                "semantic topology graph",
                "trainable memory tokens",
                "reversible Transformer",
                "long-term memory",
                "short-term memory",
                "Touchdown",
                "Map2Seq",
                "modular VLN",
                "prompt-based LLM",
                "strided-attention MLLM",
                "hierarchical map",
                "dual memory modules"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-06-24T05:00:43.000Z",
        "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
        "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\nMem4Nav, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/y2o42dXy8IQu2LgEurTV4.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19433.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "fullname": "Jie Feng",
            "name": "JJ-TMT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.20670",
            "authors": [
                {
                    "_id": "685c9ef4696820ba1f28f263",
                    "name": "Jinming Wu",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f264",
                    "name": "Zihao Deng",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f265",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f266",
                    "name": "Yiding Liu",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f267",
                    "name": "Bo You",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f268",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f269",
                    "name": "Zejun Ma",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f26a",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-25T17:59:42.000Z",
            "submittedOnDailyAt": "2025-06-25T23:54:23.682Z",
            "title": "MMSearch-R1: Incentivizing LMMs to Search",
            "submittedOnDailyBy": {
                "_id": "652fbe8cb2acab0b82f855a6",
                "avatarUrl": "/avatars/c35672b2229bb4f986ef01c61211c3f2.svg",
                "isPro": false,
                "fullname": "Jinming Wu",
                "user": "kimingng",
                "type": "user"
            },
            "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.",
            "upvotes": 1,
            "discussionId": "685c9ef5696820ba1f28f26b",
            "githubRepo": "https://github.com/EvolvingLMMs-Lab/multimodal-search-r1",
            "ai_summary": "MMSearch-R1, a reinforcement learning framework, enables large multimodal models to perform efficient, on-demand, multi-turn search in real-world environments, outperforming existing approaches.",
            "ai_keywords": [
                "multimodal models",
                "retrieval-augmented generation",
                "prompt engineered search agents",
                "reinforcement learning",
                "image search",
                "text search",
                "outcome-based reward",
                "search penalty",
                "multimodal search VQA dataset",
                "knowledge-intensive VQA tasks",
                "info-seeking VQA tasks"
            ]
        },
        "publishedAt": "2025-06-25T13:59:42.000Z",
        "title": "MMSearch-R1: Incentivizing LMMs to Search",
        "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20670.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "652fbe8cb2acab0b82f855a6",
            "avatarUrl": "/avatars/c35672b2229bb4f986ef01c61211c3f2.svg",
            "fullname": "Jinming Wu",
            "name": "kimingng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    }
]