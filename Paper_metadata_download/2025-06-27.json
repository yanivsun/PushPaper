[
    {
        "paper": {
            "id": "2506.20670",
            "authors": [
                {
                    "_id": "685c9ef4696820ba1f28f263",
                    "user": {
                        "_id": "652fbe8cb2acab0b82f855a6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fbe8cb2acab0b82f855a6/lVpzeEoFRQ6dnGAoNS9b3.jpeg",
                        "isPro": false,
                        "fullname": "Jinming Wu",
                        "user": "kimingng",
                        "type": "user"
                    },
                    "name": "Jinming Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-26T09:24:03.068Z",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f264",
                    "name": "Zihao Deng",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f265",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f266",
                    "name": "Yiding Liu",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f267",
                    "name": "Bo You",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f268",
                    "user": {
                        "_id": "62d3f7d84b0933c48f3cdd9c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d3f7d84b0933c48f3cdd9c/rSoSyH0Td9fD9HVlyK7bh.jpeg",
                        "isPro": true,
                        "fullname": "Bo Li",
                        "user": "luodian",
                        "type": "user"
                    },
                    "name": "Bo Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T20:31:10.951Z",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f269",
                    "name": "Zejun Ma",
                    "hidden": false
                },
                {
                    "_id": "685c9ef4696820ba1f28f26a",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-25T17:59:42.000Z",
            "submittedOnDailyAt": "2025-06-27T00:45:57.876Z",
            "title": "MMSearch-R1: Incentivizing LMMs to Search",
            "submittedOnDailyBy": {
                "_id": "652fbe8cb2acab0b82f855a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fbe8cb2acab0b82f855a6/lVpzeEoFRQ6dnGAoNS9b3.jpeg",
                "isPro": false,
                "fullname": "Jinming Wu",
                "user": "kimingng",
                "type": "user"
            },
            "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.",
            "upvotes": 37,
            "discussionId": "685c9ef5696820ba1f28f26b",
            "githubRepo": "https://github.com/EvolvingLMMs-Lab/multimodal-search-r1",
            "ai_summary": "MMSearch-R1, a reinforcement learning framework, enables large multimodal models to perform efficient, on-demand, multi-turn search in real-world environments, outperforming existing approaches.",
            "ai_keywords": [
                "multimodal models",
                "retrieval-augmented generation",
                "prompt engineered search agents",
                "reinforcement learning",
                "image search",
                "text search",
                "outcome-based reward",
                "search penalty",
                "multimodal search VQA dataset",
                "knowledge-intensive VQA tasks",
                "info-seeking VQA tasks"
            ],
            "githubStars": 157
        },
        "publishedAt": "2025-06-25T13:59:42.000Z",
        "title": "MMSearch-R1: Incentivizing LMMs to Search",
        "summary": "Robust deployment of large multimodal models (LMMs) in real-world scenarios\nrequires access to external knowledge sources, given the complexity and dynamic\nnature of real-world information. Existing approaches such as\nretrieval-augmented generation (RAG) and prompt engineered search agents rely\non rigid pipelines, often leading to inefficient or excessive search behaviors.\nWe present MMSearch-R1, the first end-to-end reinforcement learning framework\nthat enables LMMs to perform on-demand, multi-turn search in real-world\nInternet environments. Our framework integrates both image and text search\ntools, allowing the model to reason about when and how to invoke them guided by\nan outcome-based reward with a search penalty. To support training, We collect\na multimodal search VQA dataset through a semi-automated pipeline that covers\ndiverse visual and textual knowledge needs and curate a search-balanced subset\nwith both search-required and search-free samples, which proves essential for\nshaping efficient and on-demand search behavior. Extensive experiments on\nknowledge-intensive and info-seeking VQA tasks show that our model not only\noutperforms RAG-based baselines of the same model size, but also matches the\nperformance of a larger RAG-based model while reducing search calls by over\n30%. We further analyze key empirical findings to offer actionable insights for\nadvancing research in multimodal search.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20670.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "652fbe8cb2acab0b82f855a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652fbe8cb2acab0b82f855a6/lVpzeEoFRQ6dnGAoNS9b3.jpeg",
            "fullname": "Jinming Wu",
            "name": "kimingng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21520",
            "authors": [
                {
                    "_id": "685e61f671131fa43be08b80",
                    "name": "Polina Karpikova",
                    "hidden": false
                },
                {
                    "_id": "685e61f671131fa43be08b81",
                    "user": {
                        "_id": "64a42977250bfdecd9570a9e",
                        "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
                        "isPro": false,
                        "fullname": "Daniil Selikhanovych",
                        "user": "apryc1",
                        "type": "user"
                    },
                    "name": "Daniil Selikhanovych",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T10:21:04.685Z",
                    "hidden": false
                },
                {
                    "_id": "685e61f671131fa43be08b82",
                    "name": "Kirill Struminsky",
                    "hidden": false
                },
                {
                    "_id": "685e61f671131fa43be08b83",
                    "user": {
                        "_id": "67dc355c829f818b4b6b64b6",
                        "avatarUrl": "/avatars/2795a0e2d8136640da4bc439135dadb4.svg",
                        "isPro": false,
                        "fullname": "Ruslan Musaev",
                        "user": "Rusamus19",
                        "type": "user"
                    },
                    "name": "Ruslan Musaev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T12:57:53.106Z",
                    "hidden": false
                },
                {
                    "_id": "685e61f671131fa43be08b84",
                    "name": "Maria Golitsyna",
                    "hidden": false
                },
                {
                    "_id": "685e61f671131fa43be08b85",
                    "name": "Dmitry Baranchuk",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T17:41:07.000Z",
            "submittedOnDailyAt": "2025-06-27T07:52:02.348Z",
            "title": "MADrive: Memory-Augmented Driving Scene Modeling",
            "submittedOnDailyBy": {
                "_id": "64a42977250bfdecd9570a9e",
                "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
                "isPro": false,
                "fullname": "Daniil Selikhanovych",
                "user": "apryc1",
                "type": "user"
            },
            "summary": "Recent advances in scene reconstruction have pushed toward highly realistic\nmodeling of autonomous driving (AD) environments using 3D Gaussian splatting.\nHowever, the resulting reconstructions remain closely tied to the original\nobservations and struggle to support photorealistic synthesis of significantly\naltered or novel driving scenarios. This work introduces MADrive, a\nmemory-augmented reconstruction framework designed to extend the capabilities\nof existing scene reconstruction methods by replacing observed vehicles with\nvisually similar 3D assets retrieved from a large-scale external memory bank.\nSpecifically, we release MAD-Cars, a curated dataset of {sim}70K 360{\\deg}\ncar videos captured in the wild and present a retrieval module that finds the\nmost similar car instances in the memory bank, reconstructs the corresponding\n3D assets from video, and integrates them into the target scene through\norientation alignment and relighting. The resulting replacements provide\ncomplete multi-view representations of vehicles in the scene, enabling\nphotorealistic synthesis of substantially altered configurations, as\ndemonstrated in our experiments. Project page:\nhttps://yandex-research.github.io/madrive/",
            "upvotes": 28,
            "discussionId": "685e61f771131fa43be08b86",
            "projectPage": "https://yandex-research.github.io/madrive/",
            "ai_summary": "MADrive enhances scene reconstruction for autonomous driving by integrating visually similar 3D car assets from an external memory bank to achieve photorealistic synthesis of altered scenarios.",
            "ai_keywords": [
                "3D Gaussian splatting",
                "scene reconstruction",
                "memory-augmented reconstruction",
                "MADrive",
                "MAD-Cars",
                "360° car videos",
                "retrieval module",
                "3D asset reconstruction",
                "orientation alignment",
                "relighting"
            ]
        },
        "publishedAt": "2025-06-26T13:41:07.000Z",
        "title": "MADrive: Memory-Augmented Driving Scene Modeling",
        "summary": "Recent advances in scene reconstruction have pushed toward highly realistic\nmodeling of autonomous driving (AD) environments using 3D Gaussian splatting.\nHowever, the resulting reconstructions remain closely tied to the original\nobservations and struggle to support photorealistic synthesis of significantly\naltered or novel driving scenarios. This work introduces MADrive, a\nmemory-augmented reconstruction framework designed to extend the capabilities\nof existing scene reconstruction methods by replacing observed vehicles with\nvisually similar 3D assets retrieved from a large-scale external memory bank.\nSpecifically, we release MAD-Cars, a curated dataset of {sim}70K 360{\\deg}\ncar videos captured in the wild and present a retrieval module that finds the\nmost similar car instances in the memory bank, reconstructs the corresponding\n3D assets from video, and integrates them into the target scene through\norientation alignment and relighting. The resulting replacements provide\ncomplete multi-view representations of vehicles in the scene, enabling\nphotorealistic synthesis of substantially altered configurations, as\ndemonstrated in our experiments. Project page:\nhttps://yandex-research.github.io/madrive/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21520.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64a42977250bfdecd9570a9e",
            "avatarUrl": "/avatars/df5d7cf159e6bb9e961e1c77d1b89d36.svg",
            "fullname": "Daniil Selikhanovych",
            "name": "apryc1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21506",
            "authors": [
                {
                    "_id": "685df93d71131fa43be08a96",
                    "user": {
                        "_id": "6500870f1e14749e84f8f887",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500870f1e14749e84f8f887/wfvx4BZvh2OyW-vpq5jEy.jpeg",
                        "isPro": false,
                        "fullname": "Boyu Gou",
                        "user": "BoyuNLP",
                        "type": "user"
                    },
                    "name": "Boyu Gou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:57:06.439Z",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08a97",
                    "user": {
                        "_id": "6745089cc681f914069f42a1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6745089cc681f914069f42a1/az6adRBIs8grHd0koJV1A.jpeg",
                        "isPro": false,
                        "fullname": "Zanming Huang",
                        "user": "huangtom",
                        "type": "user"
                    },
                    "name": "Zanming Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T20:31:09.247Z",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08a98",
                    "user": {
                        "_id": "65ace92f64c9b93eca5c2bce",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ace92f64c9b93eca5c2bce/pG0JRXH-8zEy0IoaEnMNw.jpeg",
                        "isPro": false,
                        "fullname": "Yuting Ning",
                        "user": "nnnyt",
                        "type": "user"
                    },
                    "name": "Yuting Ning",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:57:04.054Z",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08a99",
                    "name": "Yu Gu",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08a9a",
                    "name": "Michael Lin",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08a9b",
                    "name": "Weijian Qi",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08a9c",
                    "name": "Andrei Kopanev",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08a9d",
                    "name": "Botao Yu",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08a9e",
                    "name": "Bernal Jiménez Gutiérrez",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08a9f",
                    "user": {
                        "_id": "60a4ebfbaa9320dbbe69e37c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60a4ebfbaa9320dbbe69e37c/QLaEohXCWaUy8YX3wKQ_w.jpeg",
                        "isPro": false,
                        "fullname": "Yiheng Shu",
                        "user": "yhshu",
                        "type": "user"
                    },
                    "name": "Yiheng Shu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:57:01.825Z",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa0",
                    "user": {
                        "_id": "63d19365b30415240fd6515b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63d19365b30415240fd6515b/eOEYSsyDTfPTDrR6Cm5Jn.jpeg",
                        "isPro": true,
                        "fullname": "Chan Hee Song",
                        "user": "chanhee-luke",
                        "type": "user"
                    },
                    "name": "Chan Hee Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T20:31:06.896Z",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa1",
                    "name": "Jiaman Wu",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa2",
                    "name": "Shijie Chen",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa3",
                    "user": {
                        "_id": "62de6cea86220b5cb895c7ee",
                        "avatarUrl": "/avatars/f247687a5da264d3702c234bb4aee8ba.svg",
                        "isPro": false,
                        "fullname": "Hanane Nour Moussa",
                        "user": "hananour",
                        "type": "user"
                    },
                    "name": "Hanane Nour Moussa",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T20:31:03.818Z",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa4",
                    "name": "Tianshu Zhang",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa5",
                    "name": "Jian Xie",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa6",
                    "name": "Yifei Li",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa7",
                    "name": "Tianci Xue",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa8",
                    "name": "Zeyi Liao",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aa9",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aaa",
                    "name": "Boyuan Zheng",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aab",
                    "name": "Zhaowei Cai",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aac",
                    "name": "Viktor Rozgic",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aad",
                    "name": "Morteza Ziyadi",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aae",
                    "name": "Huan Sun",
                    "hidden": false
                },
                {
                    "_id": "685df93d71131fa43be08aaf",
                    "name": "Yu Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T17:32:50.000Z",
            "submittedOnDailyAt": "2025-06-27T00:23:59.896Z",
            "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
            "submittedOnDailyBy": {
                "_id": "6500870f1e14749e84f8f887",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500870f1e14749e84f8f887/wfvx4BZvh2OyW-vpq5jEy.jpeg",
                "isPro": false,
                "fullname": "Boyu Gou",
                "user": "BoyuNLP",
                "type": "user"
            },
            "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.",
            "upvotes": 28,
            "discussionId": "685df93d71131fa43be08ab0",
            "projectPage": "https://osu-nlp-group.github.io/Mind2Web-2",
            "githubRepo": "https://github.com/OSU-NLP-Group/Mind2Web-2/",
            "ai_summary": "Mind2Web 2 benchmark evaluates agentic search systems with a suite of realistic, long-horizon tasks, introducing an Agent-as-a-Judge framework to assess accuracy and source attribution.",
            "ai_keywords": [
                "Deep Research systems",
                "large language models",
                "autonomous browsing",
                "information synthesis",
                "citation-backed answers",
                "evaluation benchmarks",
                "search horizons",
                "static answers",
                "Mind2Web 2",
                "high-quality tasks",
                "real-time web browsing",
                "extensive information synthesis",
                "task-specific judge agents",
                "tree-structured rubric design",
                "answer correctness",
                "source attribution",
                "agentic search systems",
                "human performance",
                "error analysis",
                "OpenAI Deep Research"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-06-26T13:32:50.000Z",
        "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge",
        "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21506.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6500870f1e14749e84f8f887",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6500870f1e14749e84f8f887/wfvx4BZvh2OyW-vpq5jEy.jpeg",
            "fullname": "Boyu Gou",
            "name": "BoyuNLP",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.20911",
            "authors": [
                {
                    "_id": "685e151c71131fa43be08afe",
                    "user": {
                        "_id": "67a99ec47b754f038d110926",
                        "avatarUrl": "/avatars/e1ff318a42ccb75b094bbe7dae0cabec.svg",
                        "isPro": false,
                        "fullname": "Advait Gupta",
                        "user": "advaitgupta",
                        "type": "user"
                    },
                    "name": "Advait Gupta",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T20:31:01.825Z",
                    "hidden": false
                },
                {
                    "_id": "685e151c71131fa43be08aff",
                    "name": "Rishie Raj",
                    "hidden": false
                },
                {
                    "_id": "685e151c71131fa43be08b00",
                    "name": "Dang Nguyen",
                    "hidden": false
                },
                {
                    "_id": "685e151c71131fa43be08b01",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:56:47.348Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/NtOZn2DhSh9N1uAbfrj7k.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ZIpYLZ5fG1gO1XJd-jFkj.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wxV2SepqrDMNMwErA1pdC.png"
            ],
            "publishedAt": "2025-06-26T00:33:43.000Z",
            "submittedOnDailyAt": "2025-06-27T03:18:51.228Z",
            "title": "FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient\n  Multi-turn Image Editing",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "We develop a cost-efficient neurosymbolic agent to address challenging\nmulti-turn image editing tasks such as \"Detect the bench in the image while\nrecoloring it to pink. Also, remove the cat for a clearer view and recolor the\nwall to yellow.'' It combines the fast, high-level subtask planning by large\nlanguage models (LLMs) with the slow, accurate, tool-use, and local A^*\nsearch per subtask to find a cost-efficient toolpath -- a sequence of calls to\nAI tools. To save the cost of A^* on similar subtasks, we perform inductive\nreasoning on previously successful toolpaths via LLMs to continuously\nextract/refine frequently used subroutines and reuse them as new tools for\nfuture tasks in an adaptive fast-slow planning, where the higher-level\nsubroutines are explored first, and only when they fail, the low-level A^*\nsearch is activated. The reusable symbolic subroutines considerably save\nexploration cost on the same types of subtasks applied to similar images,\nyielding a human-like fast-slow toolpath agent \"FaSTA^*'': fast subtask\nplanning followed by rule-based subroutine selection per subtask is attempted\nby LLMs at first, which is expected to cover most tasks, while slow A^*\nsearch is only triggered for novel and challenging subtasks. By comparing with\nrecent image editing approaches, we demonstrate FaSTA^* is significantly more\ncomputationally efficient while remaining competitive with the state-of-the-art\nbaseline in terms of success rate.",
            "upvotes": 27,
            "discussionId": "685e151d71131fa43be08b02",
            "githubRepo": "https://github.com/tianyi-lab/FaSTAR",
            "ai_summary": "A neurosymbolic agent combines language models for fast subtask planning with A$^*$ search for detailed toolpaths, creating a cost-efficient multi-turn image editing solution.",
            "ai_keywords": [
                "neurosymbolic agent",
                "LLM",
                "A$^*$ search",
                "subtask planning",
                "toolpath",
                "inductive reasoning",
                "symbolic subroutines",
                "adaptive fast-slow planning"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-06-25T20:33:43.000Z",
        "title": "FaSTA^*: Fast-Slow Toolpath Agent with Subroutine Mining for Efficient\n  Multi-turn Image Editing",
        "summary": "We develop a cost-efficient neurosymbolic agent to address challenging\nmulti-turn image editing tasks such as \"Detect the bench in the image while\nrecoloring it to pink. Also, remove the cat for a clearer view and recolor the\nwall to yellow.'' It combines the fast, high-level subtask planning by large\nlanguage models (LLMs) with the slow, accurate, tool-use, and local A^*\nsearch per subtask to find a cost-efficient toolpath -- a sequence of calls to\nAI tools. To save the cost of A^* on similar subtasks, we perform inductive\nreasoning on previously successful toolpaths via LLMs to continuously\nextract/refine frequently used subroutines and reuse them as new tools for\nfuture tasks in an adaptive fast-slow planning, where the higher-level\nsubroutines are explored first, and only when they fail, the low-level A^*\nsearch is activated. The reusable symbolic subroutines considerably save\nexploration cost on the same types of subtasks applied to similar images,\nyielding a human-like fast-slow toolpath agent \"FaSTA^*'': fast subtask\nplanning followed by rule-based subroutine selection per subtask is attempted\nby LLMs at first, which is expected to cover most tasks, while slow A^*\nsearch is only triggered for novel and challenging subtasks. By comparing with\nrecent image editing approaches, we demonstrate FaSTA^* is significantly more\ncomputationally efficient while remaining competitive with the state-of-the-art\nbaseline in terms of success rate.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/NtOZn2DhSh9N1uAbfrj7k.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/ZIpYLZ5fG1gO1XJd-jFkj.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/wxV2SepqrDMNMwErA1pdC.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20911.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21539",
            "authors": [
                {
                    "_id": "685e06f771131fa43be08abe",
                    "name": "Jun Cen",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08abf",
                    "name": "Chaohui Yu",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac0",
                    "user": {
                        "_id": "649d54b314afbb10ce2a9eeb",
                        "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                        "isPro": false,
                        "fullname": "Hangjie Yuan",
                        "user": "JacobYuan",
                        "type": "user"
                    },
                    "name": "Hangjie Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:56:57.326Z",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac1",
                    "name": "Yuming Jiang",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac2",
                    "name": "Siteng Huang",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac3",
                    "name": "Jiayan Guo",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac4",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac5",
                    "name": "Yibing Song",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac6",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac7",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac8",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "685e06f771131fa43be08ac9",
                    "name": "Hao Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T17:55:40.000Z",
            "submittedOnDailyAt": "2025-06-27T01:21:09.686Z",
            "title": "WorldVLA: Towards Autoregressive Action World Model",
            "submittedOnDailyBy": {
                "_id": "649d54b314afbb10ce2a9eeb",
                "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
                "isPro": false,
                "fullname": "Hangjie Yuan",
                "user": "JacobYuan",
                "type": "user"
            },
            "summary": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.",
            "upvotes": 25,
            "discussionId": "685e06f871131fa43be08aca",
            "projectPage": "https://github.com/alibaba-damo-academy/WorldVLA",
            "githubRepo": "https://github.com/alibaba-damo-academy/WorldVLA",
            "ai_summary": "WorldVLA, an autoregressive action world model integrating vision-language-action (VLA) and world models, enhances performance through mutual understanding and generation, improving action prediction and sequence generation with an attention mask strategy.",
            "ai_keywords": [
                "autoregressive action world model",
                "Vision-Language-Action (VLA) model",
                "world model",
                "action generation",
                "action prediction",
                "attention mask strategy"
            ],
            "githubStars": 72
        },
        "publishedAt": "2025-06-26T13:55:40.000Z",
        "title": "WorldVLA: Towards Autoregressive Action World Model",
        "summary": "We present WorldVLA, an autoregressive action world model that unifies action\nand image understanding and generation. Our WorldVLA intergrates\nVision-Language-Action (VLA) model and world model in one single framework. The\nworld model predicts future images by leveraging both action and image\nunderstanding, with the purpose of learning the underlying physics of the\nenvironment to improve action generation. Meanwhile, the action model generates\nthe subsequent actions based on image observations, aiding in visual\nunderstanding and in turn helps visual generation of the world model. We\ndemonstrate that WorldVLA outperforms standalone action and world models,\nhighlighting the mutual enhancement between the world model and the action\nmodel. In addition, we find that the performance of the action model\ndeteriorates when generating sequences of actions in an autoregressive manner.\nThis phenomenon can be attributed to the model's limited generalization\ncapability for action prediction, leading to the propagation of errors from\nearlier actions to subsequent ones. To address this issue, we propose an\nattention mask strategy that selectively masks prior actions during the\ngeneration of the current action, which shows significant performance\nimprovement in the action chunk generation task.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21539.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "649d54b314afbb10ce2a9eeb",
            "avatarUrl": "/avatars/15c325d8c2273ff63569f23015e98486.svg",
            "fullname": "Hangjie Yuan",
            "name": "JacobYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21551",
            "authors": [
                {
                    "_id": "685e12a171131fa43be08af1",
                    "name": "Ziyue Li",
                    "hidden": false
                },
                {
                    "_id": "685e12a171131fa43be08af2",
                    "user": {
                        "_id": "64a8121e35fab7cd04c30ed0",
                        "avatarUrl": "/avatars/48849b84703158772f1022932331b143.svg",
                        "isPro": false,
                        "fullname": "Chenrui Fan",
                        "user": "Fcr09",
                        "type": "user"
                    },
                    "name": "Chenrui Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:56:55.357Z",
                    "hidden": false
                },
                {
                    "_id": "685e12a171131fa43be08af3",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:56:53.481Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/B_SAKrX_CUkbOy_G2Utsz.png",
                "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/GMugplIqsF3q7PFg2Ywia.png"
            ],
            "publishedAt": "2025-06-26T17:59:58.000Z",
            "submittedOnDailyAt": "2025-06-27T02:17:48.590Z",
            "title": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.",
            "upvotes": 23,
            "discussionId": "685e12a271131fa43be08af4",
            "ai_summary": "Grokking, or continued test performance improvement after training loss convergence, is observed during pretraining of a large language model, showcasing a memorization-to-generalization process.",
            "ai_keywords": [
                "grokking",
                "training loss",
                "generalization",
                "pretraining",
                "large language model",
                "OLMoE",
                "math reasoning",
                "code generation",
                "knowledge retrieval",
                "expert choices",
                "pathway distance",
                "pathway complexity",
                "generalization bound"
            ]
        },
        "publishedAt": "2025-06-26T13:59:58.000Z",
        "title": "Where to find Grokking in LLM Pretraining? Monitor\n  Memorization-to-Generalization without Test",
        "summary": "Grokking, i.e., test performance keeps improving long after training loss\nconverged, has been recently witnessed in neural network training, making the\nmechanism of generalization and other emerging capabilities such as reasoning\nmysterious. While prior studies usually train small models on a few toy or\nhighly-specific tasks for thousands of epochs, we conduct the first study of\ngrokking on checkpoints during one-pass pretraining of a 7B large language\nmodel (LLM), i.e., OLMoE. We compute the training loss and evaluate\ngeneralization on diverse benchmark tasks, including math reasoning, code\ngeneration, and commonsense/domain-specific knowledge retrieval tasks.\n  Our study, for the first time, verifies that grokking still happens in the\npretraining of large-scale foundation models, though different data may enter\ngrokking stages asynchronously. We further demystify grokking's \"emergence of\ngeneralization\" by investigating LLM internal dynamics. Specifically, we find\nthat training samples' pathways (i.e., expert choices across layers) evolve\nfrom random, instance-specific to more structured and shareable between samples\nduring grokking. Also, the complexity of a sample's pathway reduces despite the\nconverged loss. These indicate a memorization-to-generalization conversion,\nproviding a mechanistic explanation of delayed generalization. In the study, we\ndevelop two novel metrics to quantify pathway distance and the complexity of a\nsingle pathway. We show their ability to predict the generalization improvement\non diverse downstream tasks. They are efficient, simple to compute and solely\ndependent on training data. Hence, they have practical value for pretraining,\nenabling us to monitor the generalization performance without finetuning and\ntest. Theoretically, we show that more structured pathways reduce model\ncomplexity and improve the generalization bound.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/B_SAKrX_CUkbOy_G2Utsz.png",
            "https://cdn-uploads.huggingface.co/production/uploads/647f5af5b0e96764589f3b2a/GMugplIqsF3q7PFg2Ywia.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21551.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21547",
            "authors": [
                {
                    "_id": "685e004071131fa43be08ab2",
                    "name": "Jianyun Xu",
                    "hidden": false
                },
                {
                    "_id": "685e004071131fa43be08ab3",
                    "user": {
                        "_id": "66863d26e2b71e3d09189ae9",
                        "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
                        "isPro": false,
                        "fullname": "Song Wang",
                        "user": "songw-zju",
                        "type": "user"
                    },
                    "name": "Song Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:56:59.774Z",
                    "hidden": false
                },
                {
                    "_id": "685e004071131fa43be08ab4",
                    "name": "Ziqian Ni",
                    "hidden": false
                },
                {
                    "_id": "685e004071131fa43be08ab5",
                    "name": "Chunyong Hu",
                    "hidden": false
                },
                {
                    "_id": "685e004071131fa43be08ab6",
                    "name": "Sheng Yang",
                    "hidden": false
                },
                {
                    "_id": "685e004071131fa43be08ab7",
                    "name": "Jianke Zhu",
                    "hidden": false
                },
                {
                    "_id": "685e004071131fa43be08ab8",
                    "name": "Qiang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T17:59:14.000Z",
            "submittedOnDailyAt": "2025-06-27T00:53:00.453Z",
            "title": "SAM4D: Segment Anything in Camera and LiDAR Streams",
            "submittedOnDailyBy": {
                "_id": "66863d26e2b71e3d09189ae9",
                "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
                "isPro": false,
                "fullname": "Song Wang",
                "user": "songw-zju",
                "type": "user"
            },
            "summary": "We present SAM4D, a multi-modal and temporal foundation model designed for\npromptable segmentation across camera and LiDAR streams. Unified Multi-modal\nPositional Encoding (UMPE) is introduced to align camera and LiDAR features in\na shared 3D space, enabling seamless cross-modal prompting and interaction.\nAdditionally, we propose Motion-aware Cross-modal Memory Attention (MCMA),\nwhich leverages ego-motion compensation to enhance temporal consistency and\nlong-horizon feature retrieval, ensuring robust segmentation across dynamically\nchanging autonomous driving scenes. To avoid annotation bottlenecks, we develop\na multi-modal automated data engine that synergizes VFM-driven video masklets,\nspatiotemporal 4D reconstruction, and cross-modal masklet fusion. This\nframework generates camera-LiDAR aligned pseudo-labels at a speed orders of\nmagnitude faster than human annotation while preserving VFM-derived semantic\nfidelity in point cloud representations. We conduct extensive experiments on\nthe constructed Waymo-4DSeg, which demonstrate the powerful cross-modal\nsegmentation ability and great potential in data annotation of proposed SAM4D.",
            "upvotes": 9,
            "discussionId": "685e004071131fa43be08ab9",
            "projectPage": "https://SAM4D-Project.github.io",
            "githubRepo": "https://github.com/CN-ADLab/SAM4D",
            "ai_summary": "SAM4D is a multi-modal and temporal foundation model for segmentation in autonomous driving using Unified Multi-modal Positional Encoding and Motion-aware Cross-modal Memory Attention, with a multi-modal automated data engine generating pseudo-labels.",
            "ai_keywords": [
                "multi-modal",
                "temporal foundation model",
                "promptable segmentation",
                "camera",
                "LiDAR",
                "Unified Multi-modal Positional Encoding",
                "shared 3D space",
                "cross-modal prompting",
                "Motion-aware Cross-modal Memory Attention",
                "ego-motion compensation",
                "temporal consistency",
                "spatiotemporal 4D reconstruction",
                "cross-modal masklet fusion",
                "pseudo-labels",
                "Waymo-4DSeg"
            ],
            "githubStars": 15
        },
        "publishedAt": "2025-06-26T13:59:14.000Z",
        "title": "SAM4D: Segment Anything in Camera and LiDAR Streams",
        "summary": "We present SAM4D, a multi-modal and temporal foundation model designed for\npromptable segmentation across camera and LiDAR streams. Unified Multi-modal\nPositional Encoding (UMPE) is introduced to align camera and LiDAR features in\na shared 3D space, enabling seamless cross-modal prompting and interaction.\nAdditionally, we propose Motion-aware Cross-modal Memory Attention (MCMA),\nwhich leverages ego-motion compensation to enhance temporal consistency and\nlong-horizon feature retrieval, ensuring robust segmentation across dynamically\nchanging autonomous driving scenes. To avoid annotation bottlenecks, we develop\na multi-modal automated data engine that synergizes VFM-driven video masklets,\nspatiotemporal 4D reconstruction, and cross-modal masklet fusion. This\nframework generates camera-LiDAR aligned pseudo-labels at a speed orders of\nmagnitude faster than human annotation while preserving VFM-derived semantic\nfidelity in point cloud representations. We conduct extensive experiments on\nthe constructed Waymo-4DSeg, which demonstrate the powerful cross-modal\nsegmentation ability and great potential in data annotation of proposed SAM4D.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21547.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66863d26e2b71e3d09189ae9",
            "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
            "fullname": "Song Wang",
            "name": "songw-zju",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.16655",
            "authors": [
                {
                    "_id": "6858de6bc0c8e29df8ea3d03",
                    "name": "Co Tran",
                    "hidden": false
                },
                {
                    "_id": "6858de6bc0c8e29df8ea3d04",
                    "user": {
                        "_id": "66b681906c8d3b36786b764c",
                        "avatarUrl": "/avatars/c04e97278b2275e2b34182229efa1c20.svg",
                        "isPro": true,
                        "fullname": "Salman",
                        "user": "parachas",
                        "type": "user"
                    },
                    "name": "Salman Paracha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-23T08:14:15.659Z",
                    "hidden": false
                },
                {
                    "_id": "6858de6bc0c8e29df8ea3d05",
                    "name": "Adil Hafeez",
                    "hidden": false
                },
                {
                    "_id": "6858de6bc0c8e29df8ea3d06",
                    "user": {
                        "_id": "622e9e56165ba2c1bcbc76da",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648869498279-622e9e56165ba2c1bcbc76da.jpeg",
                        "isPro": false,
                        "fullname": "Shuguang Chen",
                        "user": "nehcgs",
                        "type": "user"
                    },
                    "name": "Shuguang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-24T08:09:48.654Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66b681906c8d3b36786b764c/yAiHzS_Ymy5geNcfh2s1K.png"
            ],
            "publishedAt": "2025-06-19T23:57:41.000Z",
            "submittedOnDailyAt": "2025-06-27T03:00:21.170Z",
            "title": "Arch-Router: Aligning LLM Routing with Human Preferences",
            "submittedOnDailyBy": {
                "_id": "66b681906c8d3b36786b764c",
                "avatarUrl": "/avatars/c04e97278b2275e2b34182229efa1c20.svg",
                "isPro": true,
                "fullname": "Salman",
                "user": "parachas",
                "type": "user"
            },
            "summary": "With the rapid proliferation of large language models (LLMs) -- each\noptimized for different strengths, style, or latency/cost profile -- routing\nhas become an essential technique to operationalize the use of different\nmodels. However, existing LLM routing approaches are limited in two key ways:\nthey evaluate performance using benchmarks that often fail to capture human\npreferences driven by subjective evaluation criteria, and they typically select\nfrom a limited pool of models. In this work, we propose a preference-aligned\nrouting framework that guides model selection by matching queries to\nuser-defined domains (e.g., travel) or action types (e.g., image editing) --\noffering a practical mechanism to encode preferences in routing decisions.\nSpecifically, we introduce Arch-Router, a compact 1.5B model that\nlearns to map queries to domain-action preferences for model routing decisions.\nOur approach also supports seamlessly adding new models for routing without\nrequiring retraining or architectural modifications. Experiments on\nconversational datasets demonstrate that our approach achieves state-of-the-art\n(SOTA) results in matching queries with human preferences, outperforming top\nproprietary models. Our approach captures subjective evaluation criteria and\nmakes routing decisions more transparent and flexible. Our model is available\nat: https://huggingface.co/katanemo/Arch-Router-1.5B.",
            "upvotes": 5,
            "discussionId": "6858de6bc0c8e29df8ea3d07",
            "githubRepo": "https://github.com/katanemo/archgw/",
            "ai_summary": "A preference-aligned routing framework using a compact 1.5B model effectively matches queries to user-defined domains and action types, outperforming proprietary models in subjective evaluation criteria.",
            "ai_keywords": [
                "large language models",
                "LLM routing",
                "Arch-Router",
                "domain-action preferences"
            ],
            "githubStars": 2783
        },
        "publishedAt": "2025-06-19T19:57:41.000Z",
        "title": "Arch-Router: Aligning LLM Routing with Human Preferences",
        "summary": "With the rapid proliferation of large language models (LLMs) -- each\noptimized for different strengths, style, or latency/cost profile -- routing\nhas become an essential technique to operationalize the use of different\nmodels. However, existing LLM routing approaches are limited in two key ways:\nthey evaluate performance using benchmarks that often fail to capture human\npreferences driven by subjective evaluation criteria, and they typically select\nfrom a limited pool of models. In this work, we propose a preference-aligned\nrouting framework that guides model selection by matching queries to\nuser-defined domains (e.g., travel) or action types (e.g., image editing) --\noffering a practical mechanism to encode preferences in routing decisions.\nSpecifically, we introduce Arch-Router, a compact 1.5B model that\nlearns to map queries to domain-action preferences for model routing decisions.\nOur approach also supports seamlessly adding new models for routing without\nrequiring retraining or architectural modifications. Experiments on\nconversational datasets demonstrate that our approach achieves state-of-the-art\n(SOTA) results in matching queries with human preferences, outperforming top\nproprietary models. Our approach captures subjective evaluation criteria and\nmakes routing decisions more transparent and flexible. Our model is available\nat: https://huggingface.co/katanemo/Arch-Router-1.5B.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66b681906c8d3b36786b764c/yAiHzS_Ymy5geNcfh2s1K.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.16655.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b681906c8d3b36786b764c",
            "avatarUrl": "/avatars/c04e97278b2275e2b34182229efa1c20.svg",
            "fullname": "Salman",
            "name": "parachas",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21552",
            "authors": [
                {
                    "_id": "685e161b71131fa43be08b04",
                    "user": {
                        "_id": "6332253749a95639154cc894",
                        "avatarUrl": "/avatars/f8e53fff5a324591026114c431cd407e.svg",
                        "isPro": true,
                        "fullname": "Yutong Bai",
                        "user": "Emma02",
                        "type": "user"
                    },
                    "name": "Yutong Bai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:56:43.620Z",
                    "hidden": false
                },
                {
                    "_id": "685e161b71131fa43be08b05",
                    "user": {
                        "_id": "658a1f4a35f23c0f1c4f689f",
                        "avatarUrl": "/avatars/e800fcbbcd242f311c3896a603862416.svg",
                        "isPro": false,
                        "fullname": "Danny Tran",
                        "user": "dans123",
                        "type": "user"
                    },
                    "name": "Danny Tran",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:56:45.405Z",
                    "hidden": false
                },
                {
                    "_id": "685e161b71131fa43be08b06",
                    "name": "Amir Bar",
                    "hidden": false
                },
                {
                    "_id": "685e161b71131fa43be08b07",
                    "name": "Yann LeCun",
                    "hidden": false
                },
                {
                    "_id": "685e161b71131fa43be08b08",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "685e161b71131fa43be08b09",
                    "name": "Jitendra Malik",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T17:59:59.000Z",
            "submittedOnDailyAt": "2025-06-27T03:04:29.837Z",
            "title": "Whole-Body Conditioned Egocentric Video Prediction",
            "submittedOnDailyBy": {
                "_id": "6332253749a95639154cc894",
                "avatarUrl": "/avatars/f8e53fff5a324591026114c431cd407e.svg",
                "isPro": true,
                "fullname": "Yutong Bai",
                "user": "Emma02",
                "type": "user"
            },
            "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given\nthe past video and an action represented by the relative 3D body pose. By\nconditioning on kinematic pose trajectories, structured by the joint hierarchy\nof the body, our model learns to simulate how physical human actions shape the\nenvironment from a first-person point of view. We train an auto-regressive\nconditional diffusion transformer on Nymeria, a large-scale dataset of\nreal-world egocentric video and body pose capture. We further design a\nhierarchical evaluation protocol with increasingly challenging tasks, enabling\na comprehensive analysis of the model's embodied prediction and control\nabilities. Our work represents an initial attempt to tackle the challenges of\nmodeling complex real-world environments and embodied agent behaviors with\nvideo prediction from the perspective of a human.",
            "upvotes": 4,
            "discussionId": "685e161b71131fa43be08b0a",
            "ai_summary": "A model trained on real-world egocentric video and body pose predicts video from human actions using an auto-regressive conditional diffusion transformer, evaluated with a hierarchical protocol of tasks.",
            "ai_keywords": [
                "auto-regressive conditional diffusion transformer"
            ]
        },
        "publishedAt": "2025-06-26T13:59:59.000Z",
        "title": "Whole-Body Conditioned Egocentric Video Prediction",
        "summary": "We train models to Predict Ego-centric Video from human Actions (PEVA), given\nthe past video and an action represented by the relative 3D body pose. By\nconditioning on kinematic pose trajectories, structured by the joint hierarchy\nof the body, our model learns to simulate how physical human actions shape the\nenvironment from a first-person point of view. We train an auto-regressive\nconditional diffusion transformer on Nymeria, a large-scale dataset of\nreal-world egocentric video and body pose capture. We further design a\nhierarchical evaluation protocol with increasingly challenging tasks, enabling\na comprehensive analysis of the model's embodied prediction and control\nabilities. Our work represents an initial attempt to tackle the challenges of\nmodeling complex real-world environments and embodied agent behaviors with\nvideo prediction from the perspective of a human.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21552.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6332253749a95639154cc894",
            "avatarUrl": "/avatars/f8e53fff5a324591026114c431cd407e.svg",
            "fullname": "Yutong Bai",
            "name": "Emma02",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.20430",
            "authors": [
                {
                    "_id": "685e119b71131fa43be08adf",
                    "name": "Weike Zhao",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae0",
                    "name": "Chaoyi Wu",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae1",
                    "name": "Yanjie Fan",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae2",
                    "name": "Xiaoman Zhang",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae3",
                    "name": "Pengcheng Qiu",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae4",
                    "name": "Yuze Sun",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae5",
                    "name": "Xiao Zhou",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae6",
                    "name": "Yanfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae7",
                    "name": "Ya Zhang",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae8",
                    "name": "Yongguo Yu",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08ae9",
                    "name": "Kun Sun",
                    "hidden": false
                },
                {
                    "_id": "685e119b71131fa43be08aea",
                    "name": "Weidi Xie",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64365addfae287005149dd24/fw_obfpXb7KTCzzHWtZIc.jpeg"
            ],
            "publishedAt": "2025-06-25T13:42:26.000Z",
            "submittedOnDailyAt": "2025-06-27T02:10:11.490Z",
            "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
            "submittedOnDailyBy": {
                "_id": "64365addfae287005149dd24",
                "avatarUrl": "/avatars/bd6d4512d66fd9fd7fd5476ea7a44b46.svg",
                "isPro": false,
                "fullname": "Weike Zhao",
                "user": "Angelakeke",
                "type": "user"
            },
            "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
            "upvotes": 4,
            "discussionId": "685e119b71131fa43be08aeb",
            "ai_summary": "DeepRare, a large language model-based system, provides accurate rare disease diagnoses using heterogeneous clinical inputs and outperforms other diagnostic methods across various datasets.",
            "ai_keywords": [
                "large language model",
                "LLm",
                "diagnostic hypotheses",
                "chain of reasoning",
                "long-term memory module",
                "domain-specific analytical tasks",
                "medical knowledge sources",
                "HPO-based evaluations",
                "Recall@1 score",
                "multi-modal input scenarios",
                "web application"
            ]
        },
        "publishedAt": "2025-06-25T09:42:26.000Z",
        "title": "An Agentic System for Rare Disease Diagnosis with Traceable Reasoning",
        "summary": "Rare diseases collectively affect over 300 million individuals worldwide, yet\ntimely and accurate diagnosis remains a pervasive challenge. This is largely\ndue to their clinical heterogeneity, low individual prevalence, and the limited\nfamiliarity most clinicians have with rare conditions. Here, we introduce\nDeepRare, the first rare disease diagnosis agentic system powered by a large\nlanguage model (LLM), capable of processing heterogeneous clinical inputs. The\nsystem generates ranked diagnostic hypotheses for rare diseases, each\naccompanied by a transparent chain of reasoning that links intermediate\nanalytic steps to verifiable medical evidence.\n  DeepRare comprises three key components: a central host with a long-term\nmemory module; specialized agent servers responsible for domain-specific\nanalytical tasks integrating over 40 specialized tools and web-scale,\nup-to-date medical knowledge sources, ensuring access to the most current\nclinical information. This modular and scalable design enables complex\ndiagnostic reasoning while maintaining traceability and adaptability. We\nevaluate DeepRare on eight datasets. The system demonstrates exceptional\ndiagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013\ndiseases. In HPO-based evaluations, DeepRare significantly outperforms other 15\nmethods, like traditional bioinformatics diagnostic tools, LLMs, and other\nagentic systems, achieving an average Recall@1 score of 57.18% and surpassing\nthe second-best method (Reasoning LLM) by a substantial margin of 23.79\npercentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at\nRecall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of\nreasoning chains by clinical experts achieves 95.40% agreements. Furthermore,\nthe DeepRare system has been implemented as a user-friendly web application\nhttp://raredx.cn/doctor.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64365addfae287005149dd24/fw_obfpXb7KTCzzHWtZIc.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20430.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64365addfae287005149dd24",
            "avatarUrl": "/avatars/bd6d4512d66fd9fd7fd5476ea7a44b46.svg",
            "fullname": "Weike Zhao",
            "name": "Angelakeke",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.20936",
            "authors": [
                {
                    "_id": "685ebc37898493f2aff27101",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "685ebc37898493f2aff27102",
                    "name": "Haolan Xu",
                    "hidden": false
                },
                {
                    "_id": "685ebc37898493f2aff27103",
                    "name": "Chun Feng",
                    "hidden": false
                },
                {
                    "_id": "685ebc37898493f2aff27104",
                    "name": "Varun Jampani",
                    "hidden": false
                },
                {
                    "_id": "685ebc37898493f2aff27105",
                    "name": "Narendra Ahuja",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65712f530ea91e592a1e8126/jicXLIyL4poynrpE7EIu1.mp4"
            ],
            "publishedAt": "2025-06-26T01:58:09.000Z",
            "submittedOnDailyAt": "2025-06-27T14:14:50.176Z",
            "title": "PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for\n  Realistic Articulated Object Modeling",
            "submittedOnDailyBy": {
                "_id": "65712f530ea91e592a1e8126",
                "avatarUrl": "/avatars/2ed00ca40b7ff2b88e4a75ef04807447.svg",
                "isPro": true,
                "fullname": "Hao Zhang",
                "user": "haoz19",
                "type": "user"
            },
            "summary": "Skinning and rigging are fundamental components in animation, articulated\nobject reconstruction, motion transfer, and 4D generation. Existing approaches\npredominantly rely on Linear Blend Skinning (LBS), due to its simplicity and\ndifferentiability. However, LBS introduces artifacts such as volume loss and\nunnatural deformations, and it fails to model elastic materials like soft\ntissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty\ntissues). In this work, we propose PhysRig: a differentiable physics-based\nskinning and rigging framework that overcomes these limitations by embedding\nthe rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh),\nwhich is simulated as a deformable soft-body structure driven by the animated\nskeleton. Our method leverages continuum mechanics and discretizes the object\nas particles embedded in an Eulerian background grid to ensure\ndifferentiability with respect to both material properties and skeletal motion.\nAdditionally, we introduce material prototypes, significantly reducing the\nlearning space while maintaining high expressiveness. To evaluate our\nframework, we construct a comprehensive synthetic dataset using meshes from\nObjaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object\ncategories and motion patterns. Our method consistently outperforms traditional\nLBS-based approaches, generating more realistic and physically plausible\nresults. Furthermore, we demonstrate the applicability of our framework in the\npose transfer task highlighting its versatility for articulated object\nmodeling.",
            "upvotes": 3,
            "discussionId": "685ebc37898493f2aff27106",
            "projectPage": "https://physrig.github.io/",
            "ai_summary": "A physics-based skinning and rigging framework called PhysRig uses volumetric representation and continuum mechanics for more realistic and physically plausible animations.",
            "ai_keywords": [
                "Linear Blend Skinning (LBS)",
                "volumetric representation",
                "tetrahedral mesh",
                "deformable soft-body structure",
                "continuum mechanics",
                "particle-based simulation",
                "Eulerian background grid",
                "material prototypes",
                "pose transfer"
            ]
        },
        "publishedAt": "2025-06-25T21:58:09.000Z",
        "title": "PhysRig: Differentiable Physics-Based Skinning and Rigging Framework for\n  Realistic Articulated Object Modeling",
        "summary": "Skinning and rigging are fundamental components in animation, articulated\nobject reconstruction, motion transfer, and 4D generation. Existing approaches\npredominantly rely on Linear Blend Skinning (LBS), due to its simplicity and\ndifferentiability. However, LBS introduces artifacts such as volume loss and\nunnatural deformations, and it fails to model elastic materials like soft\ntissues, fur, and flexible appendages (e.g., elephant trunks, ears, and fatty\ntissues). In this work, we propose PhysRig: a differentiable physics-based\nskinning and rigging framework that overcomes these limitations by embedding\nthe rigid skeleton into a volumetric representation (e.g., a tetrahedral mesh),\nwhich is simulated as a deformable soft-body structure driven by the animated\nskeleton. Our method leverages continuum mechanics and discretizes the object\nas particles embedded in an Eulerian background grid to ensure\ndifferentiability with respect to both material properties and skeletal motion.\nAdditionally, we introduce material prototypes, significantly reducing the\nlearning space while maintaining high expressiveness. To evaluate our\nframework, we construct a comprehensive synthetic dataset using meshes from\nObjaverse, The Amazing Animals Zoo, and MixaMo, covering diverse object\ncategories and motion patterns. Our method consistently outperforms traditional\nLBS-based approaches, generating more realistic and physically plausible\nresults. Furthermore, we demonstrate the applicability of our framework in the\npose transfer task highlighting its versatility for articulated object\nmodeling.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65712f530ea91e592a1e8126/jicXLIyL4poynrpE7EIu1.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20936.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65712f530ea91e592a1e8126",
            "avatarUrl": "/avatars/2ed00ca40b7ff2b88e4a75ef04807447.svg",
            "fullname": "Hao Zhang",
            "name": "haoz19",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21272",
            "authors": [
                {
                    "_id": "685e7bb371131fa43be08baa",
                    "name": "Jiayi Zheng",
                    "hidden": false
                },
                {
                    "_id": "685e7bb371131fa43be08bab",
                    "name": "Xiaodong Cun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T13:58:16.000Z",
            "submittedOnDailyAt": "2025-06-27T09:39:26.696Z",
            "title": "FairyGen: Storied Cartoon Video from a Single Child-Drawn Character",
            "submittedOnDailyBy": {
                "_id": "63184c517ca1b876d99b7e0e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
                "isPro": false,
                "fullname": "Xiaodong Cun",
                "user": "vinthony",
                "type": "user"
            },
            "summary": "We propose FairyGen, an automatic system for generating story-driven cartoon\nvideos from a single child's drawing, while faithfully preserving its unique\nartistic style. Unlike previous storytelling methods that primarily focus on\ncharacter consistency and basic motion, FairyGen explicitly disentangles\ncharacter modeling from stylized background generation and incorporates\ncinematic shot design to support expressive and coherent storytelling. Given a\nsingle character sketch, we first employ an MLLM to generate a structured\nstoryboard with shot-level descriptions that specify environment settings,\ncharacter actions, and camera perspectives. To ensure visual consistency, we\nintroduce a style propagation adapter that captures the character's visual\nstyle and applies it to the background, faithfully retaining the character's\nfull visual identity while synthesizing style-consistent scenes. A shot design\nmodule further enhances visual diversity and cinematic quality through frame\ncropping and multi-view synthesis based on the storyboard. To animate the\nstory, we reconstruct a 3D proxy of the character to derive physically\nplausible motion sequences, which are then used to fine-tune an MMDiT-based\nimage-to-video diffusion model. We further propose a two-stage motion\ncustomization adapter: the first stage learns appearance features from\ntemporally unordered frames, disentangling identity from motion; the second\nstage models temporal dynamics using a timestep-shift strategy with frozen\nidentity weights. Once trained, FairyGen directly renders diverse and coherent\nvideo scenes aligned with the storyboard. Extensive experiments demonstrate\nthat our system produces animations that are stylistically faithful,\nnarratively structured natural motion, highlighting its potential for\npersonalized and engaging story animation. The code will be available at\nhttps://github.com/GVCLab/FairyGen",
            "upvotes": 2,
            "discussionId": "685e7bb371131fa43be08bac",
            "projectPage": "https://jayleejia.github.io/FairyGen/",
            "githubRepo": "https://github.com/GVCLab/FairyGen",
            "ai_summary": "FairyGen generates story-driven cartoon videos from a single drawing by disentangling character modeling and background styling, employing MLLM for storyboards, style propagation for consistency, and MMDiT-based diffusion models for motion.",
            "ai_keywords": [
                "MLLM",
                "style propagation adapter",
                "cinematic shot design",
                "storyboard",
                "shot design module",
                "3D proxy",
                "MMDiT-based image-to-video diffusion model",
                "two-stage motion customization adapter",
                "timestep-shift strategy",
                "frozen identity weights"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-06-26T09:58:16.000Z",
        "title": "FairyGen: Storied Cartoon Video from a Single Child-Drawn Character",
        "summary": "We propose FairyGen, an automatic system for generating story-driven cartoon\nvideos from a single child's drawing, while faithfully preserving its unique\nartistic style. Unlike previous storytelling methods that primarily focus on\ncharacter consistency and basic motion, FairyGen explicitly disentangles\ncharacter modeling from stylized background generation and incorporates\ncinematic shot design to support expressive and coherent storytelling. Given a\nsingle character sketch, we first employ an MLLM to generate a structured\nstoryboard with shot-level descriptions that specify environment settings,\ncharacter actions, and camera perspectives. To ensure visual consistency, we\nintroduce a style propagation adapter that captures the character's visual\nstyle and applies it to the background, faithfully retaining the character's\nfull visual identity while synthesizing style-consistent scenes. A shot design\nmodule further enhances visual diversity and cinematic quality through frame\ncropping and multi-view synthesis based on the storyboard. To animate the\nstory, we reconstruct a 3D proxy of the character to derive physically\nplausible motion sequences, which are then used to fine-tune an MMDiT-based\nimage-to-video diffusion model. We further propose a two-stage motion\ncustomization adapter: the first stage learns appearance features from\ntemporally unordered frames, disentangling identity from motion; the second\nstage models temporal dynamics using a timestep-shift strategy with frozen\nidentity weights. Once trained, FairyGen directly renders diverse and coherent\nvideo scenes aligned with the storyboard. Extensive experiments demonstrate\nthat our system produces animations that are stylistically faithful,\nnarratively structured natural motion, highlighting its potential for\npersonalized and engaging story animation. The code will be available at\nhttps://github.com/GVCLab/FairyGen",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21272.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63184c517ca1b876d99b7e0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
            "fullname": "Xiaodong Cun",
            "name": "vinthony",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 324
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21103",
            "authors": [
                {
                    "_id": "685e38fe71131fa43be08b3e",
                    "user": {
                        "_id": "65f15414f2c28f56ad2d663b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f15414f2c28f56ad2d663b/KLI4KQcVauCH9tCO4WC5w.jpeg",
                        "isPro": false,
                        "fullname": "Tim Lawson",
                        "user": "tim-lawson",
                        "type": "user"
                    },
                    "name": "Tim Lawson",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:56:41.555Z",
                    "hidden": false
                },
                {
                    "_id": "685e38fe71131fa43be08b3f",
                    "name": "Laurence Aitchison",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T09:01:19.000Z",
            "submittedOnDailyAt": "2025-06-27T07:54:14.957Z",
            "title": "Learning to Skip the Middle Layers of Transformers",
            "submittedOnDailyBy": {
                "_id": "65f15414f2c28f56ad2d663b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f15414f2c28f56ad2d663b/KLI4KQcVauCH9tCO4WC5w.jpeg",
                "isPro": false,
                "fullname": "Tim Lawson",
                "user": "tim-lawson",
                "type": "user"
            },
            "summary": "Conditional computation is a popular strategy to make Transformers more\nefficient. Existing methods often target individual modules (e.g.,\nmixture-of-experts layers) or skip layers independently of one another.\nHowever, interpretability research has demonstrated that the middle layers of\nTransformers exhibit greater redundancy, and that early layers aggregate\ninformation into token positions. Guided by these insights, we propose a novel\narchitecture that dynamically skips a variable number of layers from the middle\noutward. In particular, a learned gating mechanism determines whether to bypass\na symmetric span of central blocks based on the input, and a gated attention\nmechanism prevents subsequent tokens from attending to skipped token positions.\nResidual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and\ngate sparsity with an adaptive regularization loss. We had aimed to reduce\ncompute requirements for 'simpler' tokens and potentially foster an emergent\nmulti-level representational hierarchy but, at the scales investigated, our\napproach does not achieve improvements in the trade-off between validation\ncross-entropy and estimated FLOPs compared to dense baselines with fewer\nlayers. We release our code at https://github.com/tim-lawson/skip-middle.",
            "upvotes": 2,
            "discussionId": "685e38ff71131fa43be08b40",
            "githubRepo": "https://github.com/tim-lawson/skip-middle",
            "ai_summary": "A novel conditional computation architecture for Transformers dynamically skips middle layers based on input and a gating mechanism, but does not outperform dense baselines in reducing computational cost or improving validation performance.",
            "ai_keywords": [
                "conditional computation",
                "Transformers",
                "mixture-of-experts layers",
                "skip layers",
                "gating mechanism",
                "gated attention mechanism",
                "residual norms",
                "sandwich normalization",
                "perilayernorm",
                "adaptive regularization loss",
                "token positions",
                "multi-level representational hierarchy"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-06-26T05:01:19.000Z",
        "title": "Learning to Skip the Middle Layers of Transformers",
        "summary": "Conditional computation is a popular strategy to make Transformers more\nefficient. Existing methods often target individual modules (e.g.,\nmixture-of-experts layers) or skip layers independently of one another.\nHowever, interpretability research has demonstrated that the middle layers of\nTransformers exhibit greater redundancy, and that early layers aggregate\ninformation into token positions. Guided by these insights, we propose a novel\narchitecture that dynamically skips a variable number of layers from the middle\noutward. In particular, a learned gating mechanism determines whether to bypass\na symmetric span of central blocks based on the input, and a gated attention\nmechanism prevents subsequent tokens from attending to skipped token positions.\nResidual norms are controlled with a 'sandwich' or 'perilayernorm' scheme and\ngate sparsity with an adaptive regularization loss. We had aimed to reduce\ncompute requirements for 'simpler' tokens and potentially foster an emergent\nmulti-level representational hierarchy but, at the scales investigated, our\napproach does not achieve improvements in the trade-off between validation\ncross-entropy and estimated FLOPs compared to dense baselines with fewer\nlayers. We release our code at https://github.com/tim-lawson/skip-middle.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21103.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65f15414f2c28f56ad2d663b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f15414f2c28f56ad2d663b/KLI4KQcVauCH9tCO4WC5w.jpeg",
            "fullname": "Tim Lawson",
            "name": "tim-lawson",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21263",
            "authors": [
                {
                    "_id": "685e8cf171131fa43be08bb5",
                    "name": "Ji Qi",
                    "hidden": false
                },
                {
                    "_id": "685e8cf171131fa43be08bb6",
                    "name": "WenPeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "685e8cf171131fa43be08bb7",
                    "name": "Li Li",
                    "hidden": false
                },
                {
                    "_id": "685e8cf171131fa43be08bb8",
                    "name": "Ming Wu",
                    "hidden": false
                },
                {
                    "_id": "685e8cf171131fa43be08bb9",
                    "name": "YingJun Wu",
                    "hidden": false
                },
                {
                    "_id": "685e8cf171131fa43be08bba",
                    "name": "Wu He",
                    "hidden": false
                },
                {
                    "_id": "685e8cf171131fa43be08bbb",
                    "name": "Xun Gao",
                    "hidden": false
                },
                {
                    "_id": "685e8cf171131fa43be08bbc",
                    "name": "Jason Zeng",
                    "hidden": false
                },
                {
                    "_id": "685e8cf171131fa43be08bbd",
                    "name": "Michael Heinrich",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T13:45:04.000Z",
            "submittedOnDailyAt": "2025-06-27T10:52:55.433Z",
            "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for\n  Decentralized Cluster",
            "submittedOnDailyBy": {
                "_id": "622792366303bf1dc304f49f",
                "avatarUrl": "/avatars/975c1cc3eb2f97cf8e848162056d5bea.svg",
                "isPro": false,
                "fullname": "Arthur Douillard",
                "user": "ArthurDouillard",
                "type": "user"
            },
            "summary": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters.",
            "upvotes": 1,
            "discussionId": "685e8cf271131fa43be08bbe",
            "ai_summary": "DiLoCoX, a decentralized cluster training framework, enhances the training of large-scale models over slow networks by utilizing pipeline parallelism, dual optimizer policy, and gradient compression, achieving significant speed improvements and effective scalability.",
            "ai_keywords": [
                "Pipeline Parallelism",
                "Dual Optimizer Policy",
                "One-Step-Delay Overlap",
                "Adaptive Gradient Compression Scheme",
                "decentralized training",
                "large-scale models",
                "gradient compression",
                "model convergence",
                "AllReduce"
            ]
        },
        "publishedAt": "2025-06-26T09:45:04.000Z",
        "title": "DiLoCoX: A Low-Communication Large-Scale Training Framework for\n  Decentralized Cluster",
        "summary": "The distributed training of foundation models, particularly large language\nmodels (LLMs), demands a high level of communication. Consequently, it is\nhighly dependent on a centralized cluster with fast and reliable interconnects.\nCan we conduct training on slow networks and thereby unleash the power of\ndecentralized clusters when dealing with models exceeding 100 billion\nparameters? In this paper, we propose DiLoCoX, a low-communication large-scale\ndecentralized cluster training framework. It combines Pipeline Parallelism with\nDual Optimizer Policy, One-Step-Delay Overlap of Communication and Local\nTraining, and an Adaptive Gradient Compression Scheme. This combination\nsignificantly improves the scale of parameters and the speed of model\npre-training. We justify the benefits of one-step-delay overlap of\ncommunication and local training, as well as the adaptive gradient compression\nscheme, through a theoretical analysis of convergence. Empirically, we\ndemonstrate that DiLoCoX is capable of pre-training a 107B foundation model\nover a 1Gbps network. Compared to vanilla AllReduce, DiLoCoX can achieve a 357x\nspeedup in distributed training while maintaining negligible degradation in\nmodel convergence. To the best of our knowledge, this is the first\ndecentralized training framework successfully applied to models with over 100\nbillion parameters.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21263.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "622792366303bf1dc304f49f",
            "avatarUrl": "/avatars/975c1cc3eb2f97cf8e848162056d5bea.svg",
            "fullname": "Arthur Douillard",
            "name": "ArthurDouillard",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.20703",
            "authors": [
                {
                    "_id": "685ef79a9e7509383d29a992",
                    "user": {
                        "_id": "631219973a773344e06c2fd4",
                        "avatarUrl": "/avatars/44a09471f7b0d551505cb14cc878a09f.svg",
                        "isPro": false,
                        "fullname": "Vaibhav Vavilala",
                        "user": "vv1233",
                        "type": "user"
                    },
                    "name": "Vaibhav Vavilala",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T20:30:31.925Z",
                    "hidden": false
                },
                {
                    "_id": "685ef79a9e7509383d29a993",
                    "name": "Seemandhar Jain",
                    "hidden": false
                },
                {
                    "_id": "685ef79a9e7509383d29a994",
                    "name": "Rahul Vasanth",
                    "hidden": false
                },
                {
                    "_id": "685ef79a9e7509383d29a995",
                    "name": "D. A. Forsyth",
                    "hidden": false
                },
                {
                    "_id": "685ef79a9e7509383d29a996",
                    "name": "Anand Bhattad",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-25T17:59:55.000Z",
            "submittedOnDailyAt": "2025-06-27T19:02:02.439Z",
            "title": "Generative Blocks World: Moving Things Around in Pictures",
            "submittedOnDailyBy": {
                "_id": "631219973a773344e06c2fd4",
                "avatarUrl": "/avatars/44a09471f7b0d551505cb14cc878a09f.svg",
                "isPro": false,
                "fullname": "Vaibhav Vavilala",
                "user": "vv1233",
                "type": "user"
            },
            "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
            "upvotes": 1,
            "discussionId": "685ef79a9e7509383d29a997",
            "ai_summary": "A generative method that edits 3D scenes using convex primitives and regenerates images with enhanced texture consistency and visual fidelity.",
            "ai_keywords": [
                "flow-based method",
                "convex 3D primitives",
                "depth",
                "texture hint",
                "key-value caching",
                "visual fidelity",
                "editability",
                "compositional generalization"
            ]
        },
        "publishedAt": "2025-06-25T13:59:55.000Z",
        "title": "Generative Blocks World: Moving Things Around in Pictures",
        "summary": "We describe Generative Blocks World to interact with the scene of a generated\nimage by manipulating simple geometric abstractions. Our method represents\nscenes as assemblies of convex 3D primitives, and the same scene can be\nrepresented by different numbers of primitives, allowing an editor to move\neither whole structures or small details. Once the scene geometry has been\nedited, the image is generated by a flow-based method which is conditioned on\ndepth and a texture hint. Our texture hint takes into account the modified 3D\nprimitives, exceeding texture-consistency provided by existing key-value\ncaching techniques. These texture hints (a) allow accurate object and camera\nmoves and (b) largely preserve the identity of objects depicted. Quantitative\nand qualitative experiments demonstrate that our approach outperforms prior\nworks in visual fidelity, editability, and compositional generalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20703.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631219973a773344e06c2fd4",
            "avatarUrl": "/avatars/44a09471f7b0d551505cb14cc878a09f.svg",
            "fullname": "Vaibhav Vavilala",
            "name": "vv1233",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.18729",
            "authors": [
                {
                    "_id": "685d3bfd696820ba1f28f3b7",
                    "user": {
                        "_id": "6665b1f48c8082c85956a038",
                        "avatarUrl": "/avatars/bed84c343deb4c10e8165a501b152e79.svg",
                        "isPro": false,
                        "fullname": "Fang Duo Tsai",
                        "user": "fundwotsai2001",
                        "type": "user"
                    },
                    "name": "Fang-Duo Tsai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:57:21.464Z",
                    "hidden": false
                },
                {
                    "_id": "685d3bfd696820ba1f28f3b8",
                    "name": "Shih-Lun Wu",
                    "hidden": false
                },
                {
                    "_id": "685d3bfd696820ba1f28f3b9",
                    "name": "Weijaw Lee",
                    "hidden": false
                },
                {
                    "_id": "685d3bfd696820ba1f28f3ba",
                    "name": "Sheng-Ping Yang",
                    "hidden": false
                },
                {
                    "_id": "685d3bfd696820ba1f28f3bb",
                    "name": "Bo-Rui Chen",
                    "hidden": false
                },
                {
                    "_id": "685d3bfd696820ba1f28f3bc",
                    "name": "Hao-Chung Cheng",
                    "hidden": false
                },
                {
                    "_id": "685d3bfd696820ba1f28f3bd",
                    "name": "Yi-Hsuan Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T15:08:03.000Z",
            "submittedOnDailyAt": "2025-06-27T07:46:34.631Z",
            "title": "MuseControlLite: Multifunctional Music Generation with Lightweight\n  Conditioners",
            "submittedOnDailyBy": {
                "_id": "6665b1f48c8082c85956a038",
                "avatarUrl": "/avatars/bed84c343deb4c10e8165a501b152e79.svg",
                "isPro": false,
                "fullname": "Fang Duo Tsai",
                "user": "fundwotsai2001",
                "type": "user"
            },
            "summary": "We propose MuseControlLite, a lightweight mechanism designed to fine-tune\ntext-to-music generation models for precise conditioning using various\ntime-varying musical attributes and reference audio signals. The key finding is\nthat positional embeddings, which have been seldom used by text-to-music\ngeneration models in the conditioner for text conditions, are critical when the\ncondition of interest is a function of time. Using melody control as an\nexample, our experiments show that simply adding rotary positional embeddings\nto the decoupled cross-attention layers increases control accuracy from 56.6%\nto 61.1%, while requiring 6.75 times fewer trainable parameters than\nstate-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion\nTransformer model of Stable Audio Open. We evaluate various forms of musical\nattribute control, audio inpainting, and audio outpainting, demonstrating\nimproved controllability over MusicGen-Large and Stable Audio Open ControlNet\nat a significantly lower fine-tuning cost, with only 85M trainble parameters.\nSource code, model checkpoints, and demo examples are available at:\nhttps://musecontrollite.github.io/web/.",
            "upvotes": 1,
            "discussionId": "685d3bfd696820ba1f28f3be",
            "projectPage": "https://musecontrollite.github.io/web/",
            "githubRepo": "https://github.com/fundwotsai2001/MuseControlLite",
            "ai_summary": "Rotary positional embeddings enhance time-varying control in text-to-music generation models with fewer parameters.",
            "ai_keywords": [
                "positional embeddings",
                "rotary positional embeddings",
                "cross-attention layers",
                "decoupled cross-attention layers",
                "diffusion Transformer",
                "MusicGen-Large",
                "Stable Audio Open ControlNet",
                "audio inpainting",
                "audio outpainting"
            ],
            "githubStars": 21
        },
        "publishedAt": "2025-06-23T11:08:03.000Z",
        "title": "MuseControlLite: Multifunctional Music Generation with Lightweight\n  Conditioners",
        "summary": "We propose MuseControlLite, a lightweight mechanism designed to fine-tune\ntext-to-music generation models for precise conditioning using various\ntime-varying musical attributes and reference audio signals. The key finding is\nthat positional embeddings, which have been seldom used by text-to-music\ngeneration models in the conditioner for text conditions, are critical when the\ncondition of interest is a function of time. Using melody control as an\nexample, our experiments show that simply adding rotary positional embeddings\nto the decoupled cross-attention layers increases control accuracy from 56.6%\nto 61.1%, while requiring 6.75 times fewer trainable parameters than\nstate-of-the-art fine-tuning mechanisms, using the same pre-trained diffusion\nTransformer model of Stable Audio Open. We evaluate various forms of musical\nattribute control, audio inpainting, and audio outpainting, demonstrating\nimproved controllability over MusicGen-Large and Stable Audio Open ControlNet\nat a significantly lower fine-tuning cost, with only 85M trainble parameters.\nSource code, model checkpoints, and demo examples are available at:\nhttps://musecontrollite.github.io/web/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18729.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6665b1f48c8082c85956a038",
            "avatarUrl": "/avatars/bed84c343deb4c10e8165a501b152e79.svg",
            "fullname": "Fang Duo Tsai",
            "name": "fundwotsai2001",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.17533",
            "authors": [
                {
                    "_id": "685ad3e8c9f2cb1e24e9c4b3",
                    "user": {
                        "_id": "61fbe895869c51c62902e9c4",
                        "avatarUrl": "/avatars/c7dc13c025a2b0e33a762bde01eac5f3.svg",
                        "isPro": false,
                        "fullname": "Yuanhao Wu",
                        "user": "wuyhthu",
                        "type": "user"
                    },
                    "name": "Yuanhao Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T20:31:12.729Z",
                    "hidden": false
                },
                {
                    "_id": "685ad3e8c9f2cb1e24e9c4b4",
                    "name": "Juntong Song",
                    "hidden": false
                },
                {
                    "_id": "685ad3e8c9f2cb1e24e9c4b5",
                    "name": "Hanning Zhang",
                    "hidden": false
                },
                {
                    "_id": "685ad3e8c9f2cb1e24e9c4b6",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "685ad3e8c9f2cb1e24e9c4b7",
                    "name": "Cheng Niu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-21T01:11:01.000Z",
            "submittedOnDailyAt": "2025-06-27T17:00:19.739Z",
            "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for\n  Mathematical Reasoning",
            "submittedOnDailyBy": {
                "_id": "61fbe895869c51c62902e9c4",
                "avatarUrl": "/avatars/c7dc13c025a2b0e33a762bde01eac5f3.svg",
                "isPro": false,
                "fullname": "Yuanhao Wu",
                "user": "wuyhthu",
                "type": "user"
            },
            "summary": "In this paper, we propose DuaShepherd, a novel reward modeling framework that\nintegrates two complementary reward signals, correctness and potential, to\nenhance the mathematical reasoning capabilities of Large Language Models\n(LLMs). While correctness-based signals emphasize identification of stepwise\nerrors, potential-based signals focus on the likelihood of reaching the correct\nfinal answer. We developed an automated pipeline for constructing large-scale\nreward modeling dataset with both signals. A unified, multi-head architecture\nwas explored to train the two reward models in a multi-task setup,\ndemonstrating benefits from learning both correctness and potential in\nparallel. By combining these two signals into a compound probability, our model\nachieves consistent performance improvements across multiple benchmarks.\nEmpirical evaluations on MATH500 and ProcessBench confirm that this combined\nreward significantly outperforms models trained on either reward type alone,\nachieving state-of-the-art performance under comparable resource constraints.",
            "upvotes": 1,
            "discussionId": "685ad3e9c9f2cb1e24e9c4b8",
            "ai_summary": "A novel reward modeling framework DuaShepherd integrates correctness and potential signals into a unified multi-head architecture to enhance LLMs' mathematical reasoning capabilities and achieve state-of-the-art performance.",
            "ai_keywords": [
                "reward modeling",
                "Large Language Models (LLMs)",
                "correctness signals",
                "potential signals",
                "automated pipeline",
                "multi-head architecture",
                "multi-task setup",
                "compound probability",
                "MATH500",
                "ProcessBench"
            ]
        },
        "publishedAt": "2025-06-20T21:11:01.000Z",
        "title": "DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for\n  Mathematical Reasoning",
        "summary": "In this paper, we propose DuaShepherd, a novel reward modeling framework that\nintegrates two complementary reward signals, correctness and potential, to\nenhance the mathematical reasoning capabilities of Large Language Models\n(LLMs). While correctness-based signals emphasize identification of stepwise\nerrors, potential-based signals focus on the likelihood of reaching the correct\nfinal answer. We developed an automated pipeline for constructing large-scale\nreward modeling dataset with both signals. A unified, multi-head architecture\nwas explored to train the two reward models in a multi-task setup,\ndemonstrating benefits from learning both correctness and potential in\nparallel. By combining these two signals into a compound probability, our model\nachieves consistent performance improvements across multiple benchmarks.\nEmpirical evaluations on MATH500 and ProcessBench confirm that this combined\nreward significantly outperforms models trained on either reward type alone,\nachieving state-of-the-art performance under comparable resource constraints.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17533.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61fbe895869c51c62902e9c4",
            "avatarUrl": "/avatars/c7dc13c025a2b0e33a762bde01eac5f3.svg",
            "fullname": "Yuanhao Wu",
            "name": "wuyhthu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.15196",
            "authors": [
                {
                    "_id": "685d2b86696820ba1f28f3a8",
                    "user": {
                        "_id": "64d9a2439fef656cfd570232",
                        "avatarUrl": "/avatars/57e7282c21bb5eb716163ea4d679c158.svg",
                        "isPro": false,
                        "fullname": "Xianliang Yang",
                        "user": "VictorYXL",
                        "type": "user"
                    },
                    "name": "Xianliang Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:57:23.564Z",
                    "hidden": false
                },
                {
                    "_id": "685d2b86696820ba1f28f3a9",
                    "name": "Ling Zhang",
                    "hidden": false
                },
                {
                    "_id": "685d2b86696820ba1f28f3aa",
                    "name": "Haolong Qian",
                    "hidden": false
                },
                {
                    "_id": "685d2b86696820ba1f28f3ab",
                    "name": "Lei Song",
                    "hidden": false
                },
                {
                    "_id": "685d2b86696820ba1f28f3ac",
                    "name": "Jiang Bian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T07:20:01.000Z",
            "submittedOnDailyAt": "2025-06-27T07:32:20.633Z",
            "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges",
            "submittedOnDailyBy": {
                "_id": "64d9a2439fef656cfd570232",
                "avatarUrl": "/avatars/57e7282c21bb5eb716163ea4d679c158.svg",
                "isPro": false,
                "fullname": "Xianliang Yang",
                "user": "VictorYXL",
                "type": "user"
            },
            "summary": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\nHeurAgenix, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.",
            "upvotes": 1,
            "discussionId": "685d2b87696820ba1f28f3ad",
            "projectPage": "https://github.com/microsoft/HeurAgenix",
            "githubRepo": "https://github.com/microsoft/HeurAgenix",
            "ai_summary": "HeurAgenix, a two-stage hyper-heuristic framework using large language models, evolves and selects heuristics dynamically for combinatorial optimization problems, achieving performance on par with specialized solvers.",
            "ai_keywords": [
                "hyper-heuristic framework",
                "large language models (LLMs)",
                "heuristic evolution",
                "selection preferences",
                "state perception",
                "dual-reward mechanism",
                "combinatorial optimization (CO)"
            ],
            "githubStars": 20
        },
        "publishedAt": "2025-06-18T03:20:01.000Z",
        "title": "HeurAgenix: Leveraging LLMs for Solving Complex Combinatorial\n  Optimization Challenges",
        "summary": "Heuristic algorithms play a vital role in solving combinatorial optimization\n(CO) problems, yet traditional designs depend heavily on manual expertise and\nstruggle to generalize across diverse instances. We introduce\nHeurAgenix, a two-stage hyper-heuristic framework powered by large\nlanguage models (LLMs) that first evolves heuristics and then selects among\nthem automatically. In the heuristic evolution phase, HeurAgenix leverages an\nLLM to compare seed heuristic solutions with higher-quality solutions and\nextract reusable evolution strategies. During problem solving, it dynamically\npicks the most promising heuristic for each problem state, guided by the LLM's\nperception ability. For flexibility, this selector can be either a\nstate-of-the-art LLM or a fine-tuned lightweight model with lower inference\ncost. To mitigate the scarcity of reliable supervision caused by CO complexity,\nwe fine-tune the lightweight heuristic selector with a dual-reward mechanism\nthat jointly exploits singals from selection preferences and state perception,\nenabling robust selection under noisy annotations. Extensive experiments on\ncanonical benchmarks show that HeurAgenix not only outperforms existing\nLLM-based hyper-heuristics but also matches or exceeds specialized solvers.\nCode is available at https://github.com/microsoft/HeurAgenix.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15196.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d9a2439fef656cfd570232",
            "avatarUrl": "/avatars/57e7282c21bb5eb716163ea4d679c158.svg",
            "fullname": "Xianliang Yang",
            "name": "VictorYXL",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]