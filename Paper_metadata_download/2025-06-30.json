[
    {
        "paper": {
            "id": "2506.17450",
            "authors": [
                {
                    "_id": "68620adf9e7509383d29ab98",
                    "user": {
                        "_id": "655bca95360e4f90cb61ba83",
                        "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
                        "isPro": true,
                        "fullname": "Jiacheng Chen",
                        "user": "cccjc",
                        "type": "user"
                    },
                    "name": "Jiacheng Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-30T06:22:01.226Z",
                    "hidden": false
                },
                {
                    "_id": "68620adf9e7509383d29ab99",
                    "name": "Ramin Mehran",
                    "hidden": false
                },
                {
                    "_id": "68620adf9e7509383d29ab9a",
                    "name": "Xuhui Jia",
                    "hidden": false
                },
                {
                    "_id": "68620adf9e7509383d29ab9b",
                    "name": "Saining Xie",
                    "hidden": false
                },
                {
                    "_id": "68620adf9e7509383d29ab9c",
                    "name": "Sanghyun Woo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-20T19:38:34.000Z",
            "submittedOnDailyAt": "2025-06-30T02:33:26.106Z",
            "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
            "submittedOnDailyBy": {
                "_id": "655bca95360e4f90cb61ba83",
                "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
                "isPro": true,
                "fullname": "Jiacheng Chen",
                "user": "cccjc",
                "type": "user"
            },
            "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.",
            "upvotes": 45,
            "discussionId": "68620adf9e7509383d29ab9d",
            "projectPage": "https://blenderfusion.github.io/",
            "ai_summary": "A generative visual compositing framework using a diffusion model for scene editing and composition with source masking and simulated object jittering.",
            "ai_keywords": [
                "diffusion model",
                "source masking",
                "simulated object jittering"
            ]
        },
        "publishedAt": "2025-06-20T15:38:34.000Z",
        "title": "BlenderFusion: 3D-Grounded Visual Editing and Generative Compositing",
        "summary": "We present BlenderFusion, a generative visual compositing framework that\nsynthesizes new scenes by recomposing objects, camera, and background. It\nfollows a layering-editing-compositing pipeline: (i) segmenting and converting\nvisual inputs into editable 3D entities (layering), (ii) editing them in\nBlender with 3D-grounded control (editing), and (iii) fusing them into a\ncoherent scene using a generative compositor (compositing). Our generative\ncompositor extends a pre-trained diffusion model to process both the original\n(source) and edited (target) scenes in parallel. It is fine-tuned on video\nframes with two key training strategies: (i) source masking, enabling flexible\nmodifications like background replacement; (ii) simulated object jittering,\nfacilitating disentangled control over objects and camera. BlenderFusion\nsignificantly outperforms prior methods in complex compositional scene editing\ntasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17450.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "655bca95360e4f90cb61ba83",
            "avatarUrl": "/avatars/1a187beb91a5e2fdc2303620b742aab1.svg",
            "fullname": "Jiacheng Chen",
            "name": "cccjc",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21862",
            "authors": [
                {
                    "_id": "6861eea79e7509383d29ab2f",
                    "name": "Boyuan Sun",
                    "hidden": false
                },
                {
                    "_id": "6861eea79e7509383d29ab30",
                    "name": "Jiaxing Zhao",
                    "hidden": false
                },
                {
                    "_id": "6861eea79e7509383d29ab31",
                    "name": "Xihan Wei",
                    "hidden": false
                },
                {
                    "_id": "6861eea79e7509383d29ab32",
                    "name": "Qibin Hou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-27T02:29:58.000Z",
            "submittedOnDailyAt": "2025-06-30T00:31:12.107Z",
            "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
            "submittedOnDailyBy": {
                "_id": "6686044047f2a33570e59e31",
                "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
                "isPro": false,
                "fullname": "Jiaxing Zhao",
                "user": "StarJiaxing",
                "type": "user"
            },
            "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
            "upvotes": 29,
            "discussionId": "6861eea89e7509383d29ab33",
            "ai_summary": "LLaVA-Scissor, a token compression strategy for video multimodal large language models, uses Semantic Connected Components to compress tokens effectively while maintaining semantic coverage and outperforming other methods.",
            "ai_keywords": [
                "token compression strategy",
                "Semantic Connected Components (SCC)",
                "spatio-temporal token compression strategy",
                "video question answering",
                "long video understanding",
                "comprehensive multi-choice benchmarks"
            ]
        },
        "publishedAt": "2025-06-26T22:29:58.000Z",
        "title": "LLaVA-Scissor: Token Compression with Semantic Connected Components for\n  Video LLMs",
        "summary": "In this paper, we present LLaVA-Scissor, a training-free token compression\nstrategy designed for video multimodal large language models. Previous methods\nmostly attempt to compress tokens based on attention scores, but fail to\neffectively capture all semantic regions and often lead to token redundancy.\nDifferently, we propose to leverage the Semantic Connected Components (SCC)\napproach that assigns tokens to distinct semantic regions within the token set,\nensuring comprehensive semantic coverage. The outcome is a two-step\nspatio-temporal token compression strategy that utilizes SCC in both spatial\nand temporal domains. This strategy can effectively compress tokens by\nrepresenting the entire video with a set of non-overlapping semantic tokens. We\nconduct extensive evaluations of the token compression capabilities of\nLLaVA-Scissor across diverse video understanding benchmarks, including video\nquestion answering, long video understanding, and comprehensive multi-choices\nbenchmarks. Experimental results show that the proposed LLaVA-Scissor\noutperforms other token compression methods, achieving superior performance in\nvarious video understanding benchmarks, particularly at low token retention\nratios. Project page: https://github.com/HumanMLLM/LLaVA-Scissor.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21862.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6686044047f2a33570e59e31",
            "avatarUrl": "/avatars/2656bf2cecd6d7cbffd0a912a54d25de.svg",
            "fullname": "Jiaxing Zhao",
            "name": "StarJiaxing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21416",
            "authors": [
                {
                    "_id": "685e084071131fa43be08acc",
                    "user": {
                        "_id": "6361dd166945df7441b893fa",
                        "avatarUrl": "/avatars/b3ae6888a41aab8c2a7ef9f7320565c4.svg",
                        "isPro": false,
                        "fullname": "Bowen Chen ",
                        "user": "chenbowen",
                        "type": "user"
                    },
                    "name": "Bowen Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-30T06:22:45.351Z",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08acd",
                    "name": "Mengyi Zhao",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08ace",
                    "name": "Haomiao Sun",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08acf",
                    "name": "Li Chen",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08ad0",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08ad1",
                    "name": "Kang Du",
                    "hidden": false
                },
                {
                    "_id": "685e084071131fa43be08ad2",
                    "name": "Xinglong Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T16:04:16.000Z",
            "submittedOnDailyAt": "2025-06-30T04:43:14.606Z",
            "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
            "submittedOnDailyBy": {
                "_id": "6498038ece9190ebb8693034",
                "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
                "isPro": false,
                "fullname": "Zhao",
                "user": "Mengyi",
                "type": "user"
            },
            "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
            "upvotes": 24,
            "discussionId": "685e084071131fa43be08ad3",
            "projectPage": "https://bytedance.github.io/XVerse/",
            "githubRepo": "https://github.com/bytedance/XVerse",
            "ai_summary": "XVerse enhances text-to-image generation by enabling precise and independent control over multiple subjects using token-specific text-stream modulation, improving image coherence and fidelity.",
            "ai_keywords": [
                "Diffusion Transformers",
                "DiTs",
                "text-to-image generation",
                "multi-subject controlled generation",
                "reference images",
                "token-specific text-stream modulation",
                "image latents",
                "multi-subject image synthesis",
                "semantic attributes"
            ],
            "githubStars": 108
        },
        "publishedAt": "2025-06-26T12:04:16.000Z",
        "title": "XVerse: Consistent Multi-Subject Control of Identity and Semantic\n  Attributes via DiT Modulation",
        "summary": "Achieving fine-grained control over subject identity and semantic attributes\n(pose, style, lighting) in text-to-image generation, particularly for multiple\nsubjects, often undermines the editability and coherence of Diffusion\nTransformers (DiTs). Many approaches introduce artifacts or suffer from\nattribute entanglement. To overcome these challenges, we propose a novel\nmulti-subject controlled generation model XVerse. By transforming reference\nimages into offsets for token-specific text-stream modulation, XVerse allows\nfor precise and independent control for specific subject without disrupting\nimage latents or features. Consequently, XVerse offers high-fidelity, editable\nmulti-subject image synthesis with robust control over individual subject\ncharacteristics and semantic attributes. This advancement significantly\nimproves personalized and complex scene generation capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21416.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6498038ece9190ebb8693034",
            "avatarUrl": "/avatars/06ec2457932e05572d917ba286cdef25.svg",
            "fullname": "Zhao",
            "name": "Mengyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21356",
            "authors": [
                {
                    "_id": "6861fb7a9e7509383d29ab4b",
                    "name": "Hongbo Liu",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab4c",
                    "name": "Jingwen He",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab4d",
                    "name": "Yi Jin",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab4e",
                    "name": "Dian Zheng",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab4f",
                    "name": "Yuhao Dong",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab50",
                    "name": "Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab51",
                    "name": "Ziqi Huang",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab52",
                    "name": "Yinan He",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab53",
                    "name": "Yangguang Li",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab54",
                    "name": "Weichao Chen",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab55",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab56",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab57",
                    "name": "Shengjie Zhao",
                    "hidden": false
                },
                {
                    "_id": "6861fb7a9e7509383d29ab58",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T15:09:21.000Z",
            "submittedOnDailyAt": "2025-06-30T04:32:34.261Z",
            "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "652965773a416e1f2173443b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
                "isPro": false,
                "fullname": "Yuhao Dong",
                "user": "THUdyh",
                "type": "user"
            },
            "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\nShotBench, a comprehensive benchmark specifically designed for cinematic\nlanguage understanding. It features over 3.5k expert-annotated QA pairs from\nimages and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\nShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic\nQA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning\nand Group Relative Policy Optimization. ShotVL significantly outperforms all\nexisting open-source and proprietary models on ShotBench, establishing new\nstate-of-the-art performance. We open-source our models, data, and code to\nfoster rapid progress in this crucial area of AI-driven cinematic understanding\nand generation.",
            "upvotes": 20,
            "discussionId": "6861fb7a9e7509383d29ab59",
            "projectPage": "https://vchitect.github.io/ShotBench-project/",
            "githubRepo": "https://github.com/Vchitect/ShotBench/tree/main",
            "ai_summary": "ShotBench and ShotQA datasets, along with ShotVL model, enhance AI's understanding and generation capabilities by specifically targeting nuanced cinematic language comprehension.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLMs",
                "ShotBench",
                "QA pairs",
                "cinematic grammar",
                "fine-grained visual comprehension",
                "AI-assisted video generation",
                "ShotQA",
                "multimodal dataset",
                "supervised fine-tuning",
                "Group Relative Policy Optimization",
                "ShotVL",
                "AI-driven cinematic understanding",
                "state-of-the-art performance"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-06-26T11:09:21.000Z",
        "title": "ShotBench: Expert-Level Cinematic Understanding in Vision-Language\n  Models",
        "summary": "Cinematography, the fundamental visual language of film, is essential for\nconveying narrative, emotion, and aesthetic quality. While recent\nVision-Language Models (VLMs) demonstrate strong general visual understanding,\ntheir proficiency in comprehending the nuanced cinematic grammar embedded\nwithin individual shots remains largely unexplored and lacks robust evaluation.\nThis critical gap limits both fine-grained visual comprehension and the\nprecision of AI-assisted video generation. To address this, we introduce\nShotBench, a comprehensive benchmark specifically designed for cinematic\nlanguage understanding. It features over 3.5k expert-annotated QA pairs from\nimages and video clips, meticulously curated from over 200 acclaimed\n(predominantly Oscar-nominated) films and spanning eight key cinematography\ndimensions. Our evaluation of 24 leading VLMs on ShotBench reveals their\nsubstantial limitations: even the top-performing model achieves less than 60%\naverage accuracy, particularly struggling with fine-grained visual cues and\ncomplex spatial reasoning. To catalyze advancement in this domain, we construct\nShotQA, a large-scale multimodal dataset comprising approximately 70k cinematic\nQA pairs. Leveraging ShotQA, we develop ShotVL through supervised fine-tuning\nand Group Relative Policy Optimization. ShotVL significantly outperforms all\nexisting open-source and proprietary models on ShotBench, establishing new\nstate-of-the-art performance. We open-source our models, data, and code to\nfoster rapid progress in this crucial area of AI-driven cinematic understanding\nand generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21356.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "652965773a416e1f2173443b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652965773a416e1f2173443b/y9MB8YgHzbwCXAc4EI9T3.jpeg",
            "fullname": "Yuhao Dong",
            "name": "THUdyh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 43
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.20279",
            "authors": [
                {
                    "_id": "686218679e7509383d29abb3",
                    "name": "Changliang Xia",
                    "hidden": false
                },
                {
                    "_id": "686218679e7509383d29abb4",
                    "name": "Chengyou Jia",
                    "hidden": false
                },
                {
                    "_id": "686218679e7509383d29abb5",
                    "name": "Zhuohang Dang",
                    "hidden": false
                },
                {
                    "_id": "686218679e7509383d29abb6",
                    "name": "Minnan Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-25T09:40:50.000Z",
            "submittedOnDailyAt": "2025-06-30T03:24:47.090Z",
            "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
            "submittedOnDailyBy": {
                "_id": "6602548a68d519ed324b47c5",
                "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
                "isPro": false,
                "fullname": "ChengyouJia",
                "user": "ChengyouJia",
                "type": "user"
            },
            "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
            "upvotes": 16,
            "discussionId": "686218689e7509383d29abb7",
            "projectPage": "https://xcltql666.github.io/DenseDiTProj/",
            "githubRepo": "https://github.com/xcltql666/DenseDiT",
            "ai_summary": "DenseDiT, a generative model-based approach, achieves superior performance in real-world dense prediction tasks using minimal training data compared to existing methods.",
            "ai_keywords": [
                "dense prediction",
                "generative models",
                "visual priors",
                "parameter-reuse mechanism",
                "lightweight branches",
                "multi-scale context",
                "DenseWorld",
                "DenseDiT"
            ],
            "githubStars": 21
        },
        "publishedAt": "2025-06-25T05:40:50.000Z",
        "title": "From Ideal to Real: Unified and Data-Efficient Dense Prediction for\n  Real-World Scenarios",
        "summary": "Dense prediction tasks hold significant importance of computer vision, aiming\nto learn pixel-wise annotated label for an input image. Despite advances in\nthis field, existing methods primarily focus on idealized conditions, with\nlimited generalization to real-world scenarios and facing the challenging\nscarcity of real-world data. To systematically study this problem, we first\nintroduce DenseWorld, a benchmark spanning a broad set of 25 dense prediction\ntasks that correspond to urgent real-world applications, featuring unified\nevaluation across tasks. Then, we propose DenseDiT, which maximally exploits\ngenerative models' visual priors to perform diverse real-world dense prediction\ntasks through a unified strategy. DenseDiT combines a parameter-reuse mechanism\nand two lightweight branches that adaptively integrate multi-scale context,\nworking with less than 0.1% additional parameters. Evaluations on DenseWorld\nreveal significant performance drops in existing general and specialized\nbaselines, highlighting their limited real-world generalization. In contrast,\nDenseDiT achieves superior results using less than 0.01% training data of\nbaselines, underscoring its practical value for real-world deployment. Our\ndata, and checkpoints and codes are available at\nhttps://xcltql666.github.io/DenseDiTProj",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20279.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6602548a68d519ed324b47c5",
            "avatarUrl": "/avatars/5ab411f87440cc2a98c7a1c6a3ed5548.svg",
            "fullname": "ChengyouJia",
            "name": "ChengyouJia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21628",
            "authors": [
                {
                    "_id": "686261739e7509383d29ac6e",
                    "name": "Magnus Dierking",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac6f",
                    "name": "Christopher E. Mower",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac70",
                    "name": "Sarthak Das",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac71",
                    "name": "Huang Helong",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac72",
                    "name": "Jiacheng Qiu",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac73",
                    "name": "Cody Reading",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac74",
                    "name": "Wei Chen",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac75",
                    "name": "Huidong Liang",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac76",
                    "name": "Huang Guowei",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac77",
                    "name": "Jan Peters",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac78",
                    "name": "Quan Xingyue",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac79",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "686261739e7509383d29ac7a",
                    "name": "Haitham Bou-Ammar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T20:23:39.000Z",
            "submittedOnDailyAt": "2025-06-30T08:38:41.700Z",
            "title": "Ark: An Open-source Python-based Framework for Robot Learning",
            "submittedOnDailyBy": {
                "_id": "631c375768f7da9ad2496bf6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
                "isPro": false,
                "fullname": "Haitham Bou Ammar",
                "user": "hba123",
                "type": "user"
            },
            "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.",
            "upvotes": 12,
            "discussionId": "686261739e7509383d29ac7b",
            "ai_summary": "ARK is an open-source Python-first framework that integrates modern imitation-learning algorithms and seamless simulation-physical robot interactions to simplify robotics development and deployment.",
            "ai_keywords": [
                "Gym-style environment interface",
                "imitation-learning algorithms",
                "ACT",
                "Diffusion Policy",
                "lightweight client-server architecture",
                "publisher-subscriber communication",
                "reusable modules",
                "control",
                "SLAM",
                "motion planning",
                "system identification",
                "visualization",
                "native ROS interoperability",
                "end-to-end pipelines"
            ]
        },
        "publishedAt": "2025-06-24T16:23:39.000Z",
        "title": "Ark: An Open-source Python-based Framework for Robot Learning",
        "summary": "Robotics has made remarkable hardware strides-from DARPA's Urban and Robotics\nChallenges to the first humanoid-robot kickboxing tournament-yet commercial\nautonomy still lags behind progress in machine learning. A major bottleneck is\nsoftware: current robot stacks demand steep learning curves, low-level C/C++\nexpertise, fragmented tooling, and intricate hardware integration, in stark\ncontrast to the Python-centric, well-documented ecosystems that propelled\nmodern AI. We introduce ARK, an open-source, Python-first robotics framework\ndesigned to close that gap. ARK presents a Gym-style environment interface that\nallows users to collect data, preprocess it, and train policies using\nstate-of-the-art imitation-learning algorithms (e.g., ACT, Diffusion Policy)\nwhile seamlessly toggling between high-fidelity simulation and physical robots.\nA lightweight client-server architecture provides networked\npublisher-subscriber communication, and optional C/C++ bindings ensure\nreal-time performance when needed. ARK ships with reusable modules for control,\nSLAM, motion planning, system identification, and visualization, along with\nnative ROS interoperability. Comprehensive documentation and case studies-from\nmanipulation to mobile navigation-demonstrate rapid prototyping, effortless\nhardware swapping, and end-to-end pipelines that rival the convenience of\nmainstream machine-learning workflows. By unifying robotics and AI practices\nunder a common Python umbrella, ARK lowers entry barriers and accelerates\nresearch and commercial deployment of autonomous robots.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21628.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631c375768f7da9ad2496bf6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/631c375768f7da9ad2496bf6/1sDOoecA6e1v_hn_VAgUq.jpeg",
            "fullname": "Haitham Bou Ammar",
            "name": "hba123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.21411",
            "authors": [
                {
                    "_id": "686237f69e7509383d29abe9",
                    "user": {
                        "_id": "64d5deb154bb9eb704f83122",
                        "avatarUrl": "/avatars/86ce09bcca903319051e2307581a43f4.svg",
                        "isPro": false,
                        "fullname": "Yehui Tang",
                        "user": "tangyehui",
                        "type": "user"
                    },
                    "name": "Yehui Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-30T07:11:35.262Z",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abea",
                    "name": "Xiaosong Li",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abeb",
                    "user": {
                        "_id": "64b78295479b934973e2c40e",
                        "avatarUrl": "/avatars/9213e385964132fa50859264a838d891.svg",
                        "isPro": false,
                        "fullname": "liu",
                        "user": "Fangcheng2",
                        "type": "user"
                    },
                    "name": "Fangcheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-30T07:11:52.170Z",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abec",
                    "name": "Wei Guo",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abed",
                    "name": "Hang Zhou",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abee",
                    "name": "Yaoyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abef",
                    "name": "Kai Han",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf0",
                    "name": "Xianzhi Yu",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf1",
                    "name": "Jinpeng Li",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf2",
                    "name": "Hui Zang",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf3",
                    "name": "Fei Mi",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf4",
                    "name": "Xiaojun Meng",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf5",
                    "name": "Zhicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf6",
                    "name": "Hanting Chen",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf7",
                    "name": "Binfan Zheng",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf8",
                    "name": "Can Chen",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abf9",
                    "name": "Youliang Yan",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abfa",
                    "name": "Ruiming Tang",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abfb",
                    "name": "Peifeng Qin",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abfc",
                    "name": "Xinghao Chen",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abfd",
                    "name": "Dacheng Tao",
                    "hidden": false
                },
                {
                    "_id": "686237f69e7509383d29abfe",
                    "user": {
                        "_id": "658bdf7b925aadd43304f05c",
                        "avatarUrl": "/avatars/64d9e9dea27c376c3bc7b2a54efc2a46.svg",
                        "isPro": false,
                        "fullname": "Yunhe Wang",
                        "user": "MightyCrane",
                        "type": "user"
                    },
                    "name": "Yunhe Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-30T07:11:59.146Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-27T16:40:21.000Z",
            "submittedOnDailyAt": "2025-06-30T05:41:23.309Z",
            "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo. Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
            "upvotes": 12,
            "discussionId": "686237f79e7509383d29abff",
            "ai_summary": "Mixture of Grouped Experts (MoGE) improves expert load balancing and execution efficiency for large language models, enhancing throughput and cost-to-performance on Ascend NPUs.",
            "ai_keywords": [
                "Mixture of Experts (MoE)",
                "Mixture of Grouped Experts (MoGE)",
                "large language models",
                "expert load balancing",
                "computational load",
                "inference phase",
                "sparse model",
                "Ascend NPUs",
                "system simulation",
                "speculative acceleration",
                "Dense models",
                "GLM-Z1-32B",
                "Qwen3-32B"
            ]
        },
        "publishedAt": "2025-05-27T12:40:21.000Z",
        "title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
        "summary": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a\nsmall price of execution cost for a much larger model parameter count and\nlearning capacity, because only a small fraction of parameters are activated\nfor each input token. However, it is commonly observed that some experts are\nactivated far more often than others, leading to system inefficiency when\nrunning the experts on different devices in parallel. Therefore, we introduce\nMixture of Grouped Experts (MoGE), which groups the experts during selection\nand balances the expert workload better than MoE in nature. It constrains\ntokens to activate an equal number of experts within each predefined expert\ngroup. When a model execution is distributed on multiple devices, this\narchitectural design ensures a balanced computational load across devices,\nsignificantly enhancing throughput, particularly for the inference phase.\nFurther, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE\nwith 72 billion total parameters, 16 billion of which are activated for each\ntoken. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and\n800I A2 through extensive system simulation studies. Our experiments indicate\nthat MoGE indeed leads to better expert load balancing and more efficient\nexecution for both model training and inference on Ascend NPUs. The inference\nperformance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further\nimproved to 1528 tokens/s per card by speculative acceleration, outperforming\ncomparable 32B and 72B Dense models. Furthermore, we achieve an excellent\ncost-to-performance ratio for model inference on Ascend 300I Duo. Our studies\nshow that Ascend NPUs are capable of training Pangu Pro MoE with massive\nparallelization to make it a leading model within the sub-100B total parameter\nclass, outperforming prominent open-source models like GLM-Z1-32B and\nQwen3-32B.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.21411.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 774
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21656",
            "authors": [
                {
                    "_id": "6861f2b89e7509383d29ab35",
                    "name": "Yifan Shen",
                    "hidden": false
                },
                {
                    "_id": "6861f2b89e7509383d29ab36",
                    "name": "Yuanzhe Liu",
                    "hidden": false
                },
                {
                    "_id": "6861f2b89e7509383d29ab37",
                    "name": "Jingyuan Zhu",
                    "hidden": false
                },
                {
                    "_id": "6861f2b89e7509383d29ab38",
                    "name": "Xu Cao",
                    "hidden": false
                },
                {
                    "_id": "6861f2b89e7509383d29ab39",
                    "name": "Xiaofeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6861f2b89e7509383d29ab3a",
                    "name": "Yixiao He",
                    "hidden": false
                },
                {
                    "_id": "6861f2b89e7509383d29ab3b",
                    "name": "Wenming Ye",
                    "hidden": false
                },
                {
                    "_id": "6861f2b89e7509383d29ab3c",
                    "name": "James Matthew Rehg",
                    "hidden": false
                },
                {
                    "_id": "6861f2b89e7509383d29ab3d",
                    "name": "Ismini Lourentzou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T18:00:00.000Z",
            "submittedOnDailyAt": "2025-06-30T00:44:37.025Z",
            "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
            "submittedOnDailyBy": {
                "_id": "65e387095132c2edd193ae49",
                "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
                "isPro": false,
                "fullname": "Yifan Shen",
                "user": "SivanSX",
                "type": "user"
            },
            "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
            "upvotes": 10,
            "discussionId": "6861f2b99e7509383d29ab3e",
            "ai_summary": "SpatialReasoner-R1, a vision-language reasoning model, uses Multi-Model Monte Carlo Tree Search and fine-grained Direct Preference Optimization to improve spatial reasoning, setting a new state-of-the-art on SPATIALRGPT-Bench.",
            "ai_keywords": [
                "vision-language models",
                "SpatialReasoner-R1",
                "Multi-Model Monte Carlo Tree Search",
                "M3CTS",
                "Long Chain-of-Thought",
                "LongCoT",
                "fine-grained Direct Preference Optimization",
                "fDPO",
                "segment-specific preference granularity",
                "descriptive grounding",
                "logical reasoning",
                "spatial reward mechanism",
                "visual consistency",
                "spatial grounding",
                "logical coherence",
                "SPATIALRGPT-Bench"
            ]
        },
        "publishedAt": "2025-06-26T14:00:00.000Z",
        "title": "Fine-Grained Preference Optimization Improves Spatial Reasoning in VLMs",
        "summary": "Current Vision-Language Models (VLMs) struggle with fine-grained spatial\nreasoning, particularly when multi-step logic and precise spatial alignment are\nrequired. In this work, we introduce SpatialReasoner-R1, a vision-language\nreasoning model designed to address these limitations. To construct\nhigh-quality supervision for spatial reasoning, we design a Multi-Model Monte\nCarlo Tree Search (M3CTS) method that generates diverse, logically consistent\nLong Chain-of-Thought (LongCoT) reasoning trajectories. In addition, we propose\nfine-grained Direct Preference Optimization (fDPO), which introduces\nsegment-specific preference granularity for descriptive grounding and logical\nreasoning, guided by a spatial reward mechanism that evaluates candidate\nresponses based on visual consistency, spatial grounding, and logical\ncoherence. Experimental results demonstrate that fDPO achieves an average\nimprovement of 4.1% over standard DPO across spatial quality tasks, and a 9.0%\ngain in spatial quantity tasks. SpatialReasoner-R1, trained with fDPO, sets a\nnew SoTA on SPATIALRGPT-Bench, outperforming the strongest baseline by 9.8% in\naverage accuracy, while maintaining competitive performance on general\nvision-language tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21656.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65e387095132c2edd193ae49",
            "avatarUrl": "/avatars/39278e5b026bcbdde88c560fc54018c5.svg",
            "fullname": "Yifan Shen",
            "name": "SivanSX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.22434",
            "authors": [
                {
                    "_id": "686205ad9e7509383d29ab80",
                    "user": {
                        "_id": "644a1b6401e18bf93a6f45c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
                        "isPro": false,
                        "fullname": "xichen",
                        "user": "xichenhku",
                        "type": "user"
                    },
                    "name": "Xi Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-30T14:03:26.611Z",
                    "hidden": false
                },
                {
                    "_id": "686205ad9e7509383d29ab81",
                    "name": "Mingkang Zhu",
                    "hidden": false
                },
                {
                    "_id": "686205ad9e7509383d29ab82",
                    "name": "Shaoteng Liu",
                    "hidden": false
                },
                {
                    "_id": "686205ad9e7509383d29ab83",
                    "name": "Xiaoyang Wu",
                    "hidden": false
                },
                {
                    "_id": "686205ad9e7509383d29ab84",
                    "name": "Xiaogang Xu",
                    "hidden": false
                },
                {
                    "_id": "686205ad9e7509383d29ab85",
                    "name": "Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "686205ad9e7509383d29ab86",
                    "name": "Xiang Bai",
                    "hidden": false
                },
                {
                    "_id": "686205ad9e7509383d29ab87",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-27T17:59:27.000Z",
            "submittedOnDailyAt": "2025-06-30T02:04:48.511Z",
            "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
            "submittedOnDailyBy": {
                "_id": "644a1b6401e18bf93a6f45c1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
                "isPro": false,
                "fullname": "xichen",
                "user": "xichenhku",
                "type": "user"
            },
            "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.",
            "upvotes": 9,
            "discussionId": "686205ad9e7509383d29ab88",
            "ai_summary": "Self-supervised learning using image triplets enhances the reasoning ability of Vision-Language Models (VLMs) on multi-image tasks without the need for human-annotated question-answer pairs.",
            "ai_keywords": [
                "Vision-Language Models",
                "self-supervised learning",
                "image triplets",
                "reasoning ability",
                "multi-image reasoning benchmarks",
                "general vision tasks"
            ]
        },
        "publishedAt": "2025-06-27T13:59:27.000Z",
        "title": "MiCo: Multi-image Contrast for Reinforcement Visual Reasoning",
        "summary": "This work explores enabling Chain-of-Thought (CoT) reasoning to link visual\ncues across multiple images. A straightforward solution is to adapt rule-based\nreinforcement learning for Vision-Language Models (VLMs). However, such methods\ntypically rely on manually curated question-answer pairs, which can be\nparticularly challenging when dealing with fine grained visual details and\ncomplex logic across images. Inspired by self-supervised visual representation\nlearning, we observe that images contain inherent constraints that can serve as\nsupervision. Based on this insight, we construct image triplets comprising two\naugmented views of the same image and a third, similar but distinct image.\nDuring training, the model is prompted to generate a reasoning process to\ncompare these images (i.e., determine same or different). Then we optimize the\nmodel with rule-based reinforcement learning. Due to the high visual similarity\nand the presence of augmentations, the model must attend to subtle visual\nchanges and perform logical reasoning to succeed. Experiments show that,\nalthough trained solely on visual comparison tasks, the learned reasoning\nability generalizes effectively to a wide range of questions. Without relying\non any human-annotated question-answer pairs, our method achieves significant\nimprovements on multi-image reasoning benchmarks and shows strong performance\non general vision tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22434.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "644a1b6401e18bf93a6f45c1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/644a1b6401e18bf93a6f45c1/P0i_CgCrIzOS2tYRlxoE9.png",
            "fullname": "xichen",
            "name": "xichenhku",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 43
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.22432",
            "authors": [
                {
                    "_id": "6861e87a9e7509383d29ab1e",
                    "user": {
                        "_id": "6351463b8445bbe32e944f6c",
                        "avatarUrl": "/avatars/ec0e8f378d5314d4af97d6c488771b3d.svg",
                        "isPro": false,
                        "fullname": "Yuhao Liu",
                        "user": "LeoLau",
                        "type": "user"
                    },
                    "name": "Yuhao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-30T14:03:52.940Z",
                    "hidden": false
                },
                {
                    "_id": "6861e87a9e7509383d29ab1f",
                    "name": "Tengfei Wang",
                    "hidden": false
                },
                {
                    "_id": "6861e87a9e7509383d29ab20",
                    "name": "Fang Liu",
                    "hidden": false
                },
                {
                    "_id": "6861e87a9e7509383d29ab21",
                    "name": "Zhenwei Wang",
                    "hidden": false
                },
                {
                    "_id": "6861e87a9e7509383d29ab22",
                    "name": "Rynson W. H. Lau",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6351463b8445bbe32e944f6c/G9Obsmul3o96NxO4Nw9eZ.mp4"
            ],
            "publishedAt": "2025-06-27T17:59:01.000Z",
            "submittedOnDailyAt": "2025-06-30T14:32:42.303Z",
            "title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy",
            "submittedOnDailyBy": {
                "_id": "6351463b8445bbe32e944f6c",
                "avatarUrl": "/avatars/ec0e8f378d5314d4af97d6c488771b3d.svg",
                "isPro": false,
                "fullname": "Yuhao Liu",
                "user": "LeoLau",
                "type": "user"
            },
            "summary": "Recent advances in deep generative modeling have unlocked unprecedented\nopportunities for video synthesis. In real-world applications, however, users\noften seek tools to faithfully realize their creative editing intentions with\nprecise and consistent control. Despite the progress achieved by existing\nmethods, ensuring fine-grained alignment with user intentions remains an open\nand challenging problem. In this work, we present Shape-for-Motion, a novel\nframework that incorporates a 3D proxy for precise and consistent video\nediting. Shape-for-Motion achieves this by converting the target object in the\ninput video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be\nperformed directly on the proxy and then inferred back to the video frames. To\nsimplify the editing process, we design a novel Dual-Propagation Strategy that\nallows users to perform edits on the 3D mesh of a single frame, and the edits\nare then automatically propagated to the 3D meshes of the other frames. The 3D\nmeshes for different frames are further projected onto the 2D space to produce\nthe edited geometry and texture renderings, which serve as inputs to a\ndecoupled video diffusion model for generating edited results. Our framework\nsupports various precise and physically-consistent manipulations across the\nvideo frames, including pose editing, rotation, scaling, translation, texture\nmodification, and object composition. Our approach marks a key step toward\nhigh-quality, controllable video editing workflows. Extensive experiments\ndemonstrate the superiority and effectiveness of our approach. Project page:\nhttps://shapeformotion.github.io/",
            "upvotes": 9,
            "discussionId": "6861e87b9e7509383d29ab23",
            "projectPage": "https://shapeformotion.github.io/",
            "githubRepo": "https://github.com/yuhaoliu7456/Shape-for-Motion",
            "ai_summary": "A novel framework integrates 3D proxy meshes and a decoupled video diffusion model to achieve precise and consistent video editing.",
            "ai_keywords": [
                "3D proxy",
                "Dual-Propagation Strategy",
                "3D meshes",
                "2D space",
                "video diffusion model",
                "pose editing",
                "rotation",
                "scaling",
                "translation",
                "texture modification",
                "object composition"
            ],
            "githubStars": 28
        },
        "publishedAt": "2025-06-27T13:59:01.000Z",
        "title": "Shape-for-Motion: Precise and Consistent Video Editing with 3D Proxy",
        "summary": "Recent advances in deep generative modeling have unlocked unprecedented\nopportunities for video synthesis. In real-world applications, however, users\noften seek tools to faithfully realize their creative editing intentions with\nprecise and consistent control. Despite the progress achieved by existing\nmethods, ensuring fine-grained alignment with user intentions remains an open\nand challenging problem. In this work, we present Shape-for-Motion, a novel\nframework that incorporates a 3D proxy for precise and consistent video\nediting. Shape-for-Motion achieves this by converting the target object in the\ninput video to a time-consistent mesh, i.e., a 3D proxy, allowing edits to be\nperformed directly on the proxy and then inferred back to the video frames. To\nsimplify the editing process, we design a novel Dual-Propagation Strategy that\nallows users to perform edits on the 3D mesh of a single frame, and the edits\nare then automatically propagated to the 3D meshes of the other frames. The 3D\nmeshes for different frames are further projected onto the 2D space to produce\nthe edited geometry and texture renderings, which serve as inputs to a\ndecoupled video diffusion model for generating edited results. Our framework\nsupports various precise and physically-consistent manipulations across the\nvideo frames, including pose editing, rotation, scaling, translation, texture\nmodification, and object composition. Our approach marks a key step toward\nhigh-quality, controllable video editing workflows. Extensive experiments\ndemonstrate the superiority and effectiveness of our approach. Project page:\nhttps://shapeformotion.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6351463b8445bbe32e944f6c/G9Obsmul3o96NxO4Nw9eZ.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22432.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6351463b8445bbe32e944f6c",
            "avatarUrl": "/avatars/ec0e8f378d5314d4af97d6c488771b3d.svg",
            "fullname": "Yuhao Liu",
            "name": "LeoLau",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21876",
            "authors": [
                {
                    "_id": "6862b46b3b7fc785c7ca5e1b",
                    "name": "Qiyue Gao",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e1c",
                    "name": "Xinyu Pi",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e1d",
                    "name": "Kevin Liu",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e1e",
                    "name": "Junrong Chen",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e1f",
                    "name": "Ruolan Yang",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e20",
                    "name": "Xinqi Huang",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e21",
                    "name": "Xinyu Fang",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e22",
                    "name": "Lu Sun",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e23",
                    "name": "Gautham Kishore",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e24",
                    "name": "Bo Ai",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e25",
                    "name": "Stone Tao",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e26",
                    "name": "Mengyang Liu",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e27",
                    "name": "Jiaxi Yang",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e28",
                    "name": "Chao-Jung Lai",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e29",
                    "name": "Chuanyang Jin",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e2a",
                    "name": "Jiannan Xiang",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e2b",
                    "name": "Benhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e2c",
                    "name": "Zeming Chen",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e2d",
                    "name": "David Danks",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e2e",
                    "name": "Hao Su",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e2f",
                    "name": "Tianmin Shu",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e30",
                    "name": "Ziqiao Ma",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e31",
                    "name": "Lianhui Qin",
                    "hidden": false
                },
                {
                    "_id": "6862b46b3b7fc785c7ca5e32",
                    "name": "Zhiting Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-27T03:24:29.000Z",
            "submittedOnDailyAt": "2025-06-30T16:54:21.364Z",
            "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic\n  Evaluation",
            "submittedOnDailyBy": {
                "_id": "665bfa1b0d71762b8613282d",
                "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
                "isPro": false,
                "fullname": "Zhiting Hu",
                "user": "zhitinghu",
                "type": "user"
            },
            "summary": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling.",
            "upvotes": 9,
            "discussionId": "6862b46b3b7fc785c7ca5e33",
            "projectPage": "https://wm-abench.maitrix.org",
            "ai_summary": "A benchmark framework evaluates the world modeling capabilities of Vision-Language Models, highlighting their limitations in perception and prediction.",
            "ai_keywords": [
                "internal world models",
                "Vision-Language Models",
                "o3",
                "GPT-4o",
                "Gemini",
                "comparative psychology",
                "cognitive science",
                "perceptual evaluation",
                "prediction evaluation",
                "mechanical simulation",
                "transitive inference",
                "compositional inference",
                "WM-ABench",
                "simulated environments",
                "disentangled understanding"
            ]
        },
        "publishedAt": "2025-06-26T23:24:29.000Z",
        "title": "Do Vision-Language Models Have Internal World Models? Towards an Atomic\n  Evaluation",
        "summary": "Internal world models (WMs) enable agents to understand the world's state and\npredict transitions, serving as the basis for advanced deliberative reasoning.\nRecent large Vision-Language Models (VLMs), such as OpenAI o3, GPT-4o and\nGemini, exhibit potential as general-purpose WMs. While the latest studies have\nevaluated and shown limitations in specific capabilities such as visual\nunderstanding, a systematic evaluation of VLMs' fundamental WM abilities\nremains absent. Drawing on comparative psychology and cognitive science, we\npropose a two-stage framework that assesses Perception (visual, spatial,\ntemporal, quantitative, and motion) and Prediction (mechanistic simulation,\ntransitive inference, compositional inference) to provide an atomic evaluation\nof VLMs as WMs. Guided by this framework, we introduce WM-ABench, a large-scale\nbenchmark comprising 23 fine-grained evaluation dimensions across 6 diverse\nsimulated environments with controlled counterfactual simulations. Through 660\nexperiments on 15 latest commercial and open-source VLMs, we find that these\nmodels exhibit striking limitations in basic world modeling abilities. For\ninstance, almost all models perform at near-random accuracy when distinguishing\nmotion trajectories. Additionally, they lack disentangled understanding --\ne.g., some models tend to believe blue objects move faster than green ones.\nMore rich results and analyses reveal significant gaps between VLMs and\nhuman-level world modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21876.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "665bfa1b0d71762b8613282d",
            "avatarUrl": "/avatars/edbde7b1b47032339a1ecc59f8ea8f1a.svg",
            "fullname": "Zhiting Hu",
            "name": "zhitinghu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.22419",
            "authors": [
                {
                    "_id": "686229249e7509383d29abd0",
                    "name": "Bingchen Zhao",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abd1",
                    "name": "Despoina Magka",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abd2",
                    "name": "Minqi Jiang",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abd3",
                    "name": "Xian Li",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abd4",
                    "name": "Roberta Raileanu",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abd5",
                    "name": "Tatiana Shavrina",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abd6",
                    "name": "Jean-Christophe Gagnon-Audet",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abd7",
                    "name": "Kelvin Niu",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abd8",
                    "name": "Shagun Sodhani",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abd9",
                    "name": "Michael Shvartsman",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abda",
                    "name": "Andrei Lupu",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abdb",
                    "name": "Alisia Lupidi",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abdc",
                    "name": "Edan Toledo",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abdd",
                    "name": "Karen Hambardzumyan",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abde",
                    "name": "Martin Josifoski",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abdf",
                    "name": "Thomas Foster",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abe0",
                    "name": "Lucia Cipolina-Kun",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abe1",
                    "name": "Abhishek Charnalia",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abe2",
                    "name": "Derek Dunfield",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abe3",
                    "name": "Alexander H. Miller",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abe4",
                    "name": "Oisin Mac Aodha",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abe5",
                    "name": "Jakob Foerster",
                    "hidden": false
                },
                {
                    "_id": "686229249e7509383d29abe6",
                    "name": "Yoram Bachrach",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-27T17:44:32.000Z",
            "submittedOnDailyAt": "2025-06-30T06:29:15.385Z",
            "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
            "submittedOnDailyBy": {
                "_id": "62dcd71075e9787ec5aa41ba",
                "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
                "isPro": true,
                "fullname": "Bingchen Zhao",
                "user": "tennant",
                "type": "user"
            },
            "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
            "upvotes": 8,
            "discussionId": "686229249e7509383d29abe7",
            "ai_summary": "An Automated LLM Speedrunning Benchmark evaluates AI agents' ability to reproduce scientific results by leveraging NanoGPT speedrun tasks, indicating that even recent reasoning LLMs struggle with re-implementing known improvements.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "AI agents",
                "Automated LLM Speedrunning Benchmark",
                "NanoGPT speedrun",
                "GPT-2",
                "high-level algorithmic advancements",
                "hardware-aware optimizations"
            ]
        },
        "publishedAt": "2025-06-27T13:44:32.000Z",
        "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT\n  Improvements",
        "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22419.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62dcd71075e9787ec5aa41ba",
            "avatarUrl": "/avatars/f37ce036b76180ed0fa004f9c8c09363.svg",
            "fullname": "Bingchen Zhao",
            "name": "tennant",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21355",
            "authors": [
                {
                    "_id": "6862b6b43b7fc785c7ca5e41",
                    "name": "Melanie Rieff",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e42",
                    "name": "Maya Varma",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e43",
                    "name": "Ossian Rabow",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e44",
                    "name": "Subathra Adithan",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e45",
                    "name": "Julie Kim",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e46",
                    "name": "Ken Chang",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e47",
                    "name": "Hannah Lee",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e48",
                    "name": "Nidhi Rohatgi",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e49",
                    "name": "Christian Bluethgen",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e4a",
                    "name": "Mohamed S. Muneer",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e4b",
                    "name": "Jean-Benoit Delbrouck",
                    "hidden": false
                },
                {
                    "_id": "6862b6b43b7fc785c7ca5e4c",
                    "name": "Michael Moor",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T15:08:18.000Z",
            "submittedOnDailyAt": "2025-06-30T14:40:12.217Z",
            "title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context\n  Learning",
            "submittedOnDailyBy": {
                "_id": "6438d1d843d932c462404500",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
                "isPro": false,
                "fullname": "Michael Moor",
                "user": "mdmoor",
                "type": "user"
            },
            "summary": "Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, example ordering exhibits a recency bias,\ni.e., placing the most relevant example last can lead to substantial\nperformance improvements by up to 71%. Our findings highlight critical\nlimitations and biases in current MLLMs when learning multimodal medical tasks\nfrom context.",
            "upvotes": 6,
            "discussionId": "6862b6b43b7fc785c7ca5e4d",
            "projectPage": "https://smmile-benchmark.github.io/",
            "githubRepo": "https://github.com/eth-medical-ai-lab/smmile",
            "ai_summary": "Current multimodal large language models show moderate to poor performance in multimodal in-context learning for medical tasks, with sensitivity to example relevance and ordering.",
            "ai_keywords": [
                "multimodal in-context learning",
                "MLLMs",
                "medical visual question answering",
                "in-context examples",
                "multimodal benchmark",
                "medical specialties",
                "imaging modalities",
                "expert-driven",
                "recency bias",
                "zero-shot learning"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-06-26T11:08:18.000Z",
        "title": "SMMILE: An Expert-Driven Benchmark for Multimodal Medical In-Context\n  Learning",
        "summary": "Multimodal in-context learning (ICL) remains underexplored despite\nsignificant potential for domains such as medicine. Clinicians routinely\nencounter diverse, specialized tasks requiring adaptation from limited\nexamples, such as drawing insights from a few relevant prior cases or\nconsidering a constrained set of differential diagnoses. While multimodal large\nlanguage models (MLLMs) have shown advances in medical visual question\nanswering (VQA), their ability to learn multimodal tasks from context is\nlargely unknown. We introduce SMMILE, the first expert-driven multimodal ICL\nbenchmark for medical tasks. Eleven medical experts curated problems, each\nincluding a multimodal query and multimodal in-context examples as task\ndemonstrations. SMMILE encompasses 111 problems (517 question-image-answer\ntriplets) covering 6 medical specialties and 13 imaging modalities. We further\nintroduce SMMILE++, an augmented variant with 1038 permuted problems. A\ncomprehensive evaluation of 15 MLLMs demonstrates that most models exhibit\nmoderate to poor multimodal ICL ability in medical tasks. In open-ended\nevaluations, ICL contributes only 8% average improvement over zero-shot on\nSMMILE and 9.4% on SMMILE++. We observe a susceptibility for irrelevant\nin-context examples: even a single noisy or irrelevant example can degrade\nperformance by up to 9.5%. Moreover, example ordering exhibits a recency bias,\ni.e., placing the most relevant example last can lead to substantial\nperformance improvements by up to 71%. Our findings highlight critical\nlimitations and biases in current MLLMs when learning multimodal medical tasks\nfrom context.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21355.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6438d1d843d932c462404500",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6438d1d843d932c462404500/6UrtJbed9N5ETbImgIh-C.png",
            "fullname": "Michael Moor",
            "name": "mdmoor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21458",
            "authors": [
                {
                    "_id": "685e93e2efec81aca075fd2a",
                    "name": "Baiqiao Yin",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd2b",
                    "name": "Qineng Wang",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd2c",
                    "name": "Pingyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd2d",
                    "name": "Jianshu Zhang",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd2e",
                    "name": "Kangrui Wang",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd2f",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd30",
                    "name": "Jieyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd31",
                    "name": "Keshigeyan Chandrasegaran",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd32",
                    "name": "Han Liu",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd33",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd34",
                    "name": "Saining Xie",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd35",
                    "name": "Manling Li",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd36",
                    "name": "Jiajun Wu",
                    "hidden": false
                },
                {
                    "_id": "685e93e2efec81aca075fd37",
                    "name": "Li Fei-Fei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T16:38:19.000Z",
            "submittedOnDailyAt": "2025-06-30T15:57:14.287Z",
            "title": "Spatial Mental Modeling from Limited Views",
            "submittedOnDailyBy": {
                "_id": "640c3b28c5fa12d61a50cd92",
                "avatarUrl": "/avatars/81556de3214c848b3c3e118f50fd2968.svg",
                "isPro": false,
                "fullname": "Qineng Wang",
                "user": "Inevitablevalor",
                "type": "user"
            },
            "summary": "Can Vision Language Models (VLMs) imagine the full scene from just a few\nviews, like humans do? Humans form spatial mental models, internal\nrepresentations of unseen space, to reason about layout, perspective, and\nmotion. Our new MindCube benchmark with 21,154 questions across 3,268 images\nexposes this critical gap, where existing VLMs exhibit near-random performance.\nUsing MindCube, we systematically evaluate how well VLMs build robust spatial\nmental models through representing positions (cognitive mapping), orientations\n(perspective-taking), and dynamics (mental simulation for \"what-if\" movements).\nWe then explore three approaches to help VLMs approximate spatial mental\nmodels, including unseen intermediate views, natural language reasoning chains,\nand cognitive maps. The significant improvement comes from a synergistic\napproach, \"map-then-reason\", that jointly trains the model to first generate a\ncognitive map and then reason upon it. By training models to reason over these\ninternal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding\nreinforcement learning pushed performance even further to 70.7% (+32.9%). Our\nkey insight is that such scaffolding of spatial mental models, actively\nconstructing and utilizing internal structured spatial representations with\nflexible reasoning processes, significantly improves understanding of\nunobservable space.",
            "upvotes": 5,
            "discussionId": "685e93e2efec81aca075fd38",
            "ai_summary": "A new benchmark, MindCube, shows that VLMs can improve their understanding of unseen spaces by forming internal spatial representations and reasoning over them.",
            "ai_keywords": [
                "Vision Language Models",
                "MindCube",
                "spatial mental models",
                "cognitive mapping",
                "perspective-taking",
                "mental simulation",
                "cognitive maps",
                "map-then-reason",
                "reinforcement learning"
            ]
        },
        "publishedAt": "2025-06-26T12:38:19.000Z",
        "title": "Spatial Mental Modeling from Limited Views",
        "summary": "Can Vision Language Models (VLMs) imagine the full scene from just a few\nviews, like humans do? Humans form spatial mental models, internal\nrepresentations of unseen space, to reason about layout, perspective, and\nmotion. Our new MindCube benchmark with 21,154 questions across 3,268 images\nexposes this critical gap, where existing VLMs exhibit near-random performance.\nUsing MindCube, we systematically evaluate how well VLMs build robust spatial\nmental models through representing positions (cognitive mapping), orientations\n(perspective-taking), and dynamics (mental simulation for \"what-if\" movements).\nWe then explore three approaches to help VLMs approximate spatial mental\nmodels, including unseen intermediate views, natural language reasoning chains,\nand cognitive maps. The significant improvement comes from a synergistic\napproach, \"map-then-reason\", that jointly trains the model to first generate a\ncognitive map and then reason upon it. By training models to reason over these\ninternal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding\nreinforcement learning pushed performance even further to 70.7% (+32.9%). Our\nkey insight is that such scaffolding of spatial mental models, actively\nconstructing and utilizing internal structured spatial representations with\nflexible reasoning processes, significantly improves understanding of\nunobservable space.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21458.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "640c3b28c5fa12d61a50cd92",
            "avatarUrl": "/avatars/81556de3214c848b3c3e118f50fd2968.svg",
            "fullname": "Qineng Wang",
            "name": "Inevitablevalor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.17859",
            "authors": [
                {
                    "_id": "686228969e7509383d29abc8",
                    "user": {
                        "_id": "662838fe4a0dc08679dfe06d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662838fe4a0dc08679dfe06d/xd71eX59Tu5ATxRnYD07V.png",
                        "isPro": false,
                        "fullname": "Daniel Wurgaft",
                        "user": "DanielWurgaft",
                        "type": "user"
                    },
                    "name": "Daniel Wurgaft",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-30T14:03:24.866Z",
                    "hidden": false
                },
                {
                    "_id": "686228969e7509383d29abc9",
                    "name": "Ekdeep Singh Lubana",
                    "hidden": false
                },
                {
                    "_id": "686228969e7509383d29abca",
                    "name": "Core Francisco Park",
                    "hidden": false
                },
                {
                    "_id": "686228969e7509383d29abcb",
                    "name": "Hidenori Tanaka",
                    "hidden": false
                },
                {
                    "_id": "686228969e7509383d29abcc",
                    "name": "Gautam Reddy",
                    "hidden": false
                },
                {
                    "_id": "686228969e7509383d29abcd",
                    "name": "Noah D. Goodman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-21T23:49:08.000Z",
            "submittedOnDailyAt": "2025-06-30T12:45:44.475Z",
            "title": "In-Context Learning Strategies Emerge Rationally",
            "submittedOnDailyBy": {
                "_id": "662838fe4a0dc08679dfe06d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662838fe4a0dc08679dfe06d/xd71eX59Tu5ATxRnYD07V.png",
                "isPro": false,
                "fullname": "Daniel Wurgaft",
                "user": "DanielWurgaft",
                "type": "user"
            },
            "summary": "Recent work analyzing in-context learning (ICL) has identified a broad set of\nstrategies that describe model behavior in different experimental conditions.\nWe aim to unify these findings by asking why a model learns these disparate\nstrategies in the first place. Specifically, we start with the observation that\nwhen trained to learn a mixture of tasks, as is popular in the literature, the\nstrategies learned by a model for performing ICL can be captured by a family of\nBayesian predictors: a memorizing predictor, which assumes a discrete prior on\nthe set of seen tasks, and a generalizing predictor, where the prior matches\nthe underlying task distribution. Adopting the normative lens of rational\nanalysis, where a learner's behavior is explained as an optimal adaptation to\ndata given computational constraints, we develop a hierarchical Bayesian\nframework that almost perfectly predicts Transformer next-token predictions\nthroughout training -- without assuming access to its weights. Under this\nframework, pretraining is viewed as a process of updating the posterior\nprobability of different strategies, and inference-time behavior as a\nposterior-weighted average over these strategies' predictions. Our framework\ndraws on common assumptions about neural network learning dynamics, which make\nexplicit a tradeoff between loss and complexity among candidate strategies:\nbeyond how well it explains the data, a model's preference towards implementing\na strategy is dictated by its complexity. This helps explain well-known ICL\nphenomena, while offering novel predictions: e.g., we show a superlinear trend\nin the timescale for transitioning from generalization to memorization as task\ndiversity increases. Overall, our work advances an explanatory and predictive\naccount of ICL grounded in tradeoffs between strategy loss and complexity.",
            "upvotes": 5,
            "discussionId": "686228979e7509383d29abce",
            "ai_summary": "A hierarchical Bayesian framework explains in-context learning behavior by modeling it as a tradeoff between strategy loss and complexity, offering both explanatory power and predictive insights.",
            "ai_keywords": [
                "Bayesian predictors",
                "memorizing predictor",
                "generalizing predictor",
                "rational analysis",
                "hierarchical Bayesian framework",
                "next-token predictions",
                "pretraining",
                "posterior probability",
                "inference-time behavior",
                "posterior-weighted average",
                "loss and complexity",
                "strategy loss",
                "strategy complexity"
            ]
        },
        "publishedAt": "2025-06-21T19:49:08.000Z",
        "title": "In-Context Learning Strategies Emerge Rationally",
        "summary": "Recent work analyzing in-context learning (ICL) has identified a broad set of\nstrategies that describe model behavior in different experimental conditions.\nWe aim to unify these findings by asking why a model learns these disparate\nstrategies in the first place. Specifically, we start with the observation that\nwhen trained to learn a mixture of tasks, as is popular in the literature, the\nstrategies learned by a model for performing ICL can be captured by a family of\nBayesian predictors: a memorizing predictor, which assumes a discrete prior on\nthe set of seen tasks, and a generalizing predictor, where the prior matches\nthe underlying task distribution. Adopting the normative lens of rational\nanalysis, where a learner's behavior is explained as an optimal adaptation to\ndata given computational constraints, we develop a hierarchical Bayesian\nframework that almost perfectly predicts Transformer next-token predictions\nthroughout training -- without assuming access to its weights. Under this\nframework, pretraining is viewed as a process of updating the posterior\nprobability of different strategies, and inference-time behavior as a\nposterior-weighted average over these strategies' predictions. Our framework\ndraws on common assumptions about neural network learning dynamics, which make\nexplicit a tradeoff between loss and complexity among candidate strategies:\nbeyond how well it explains the data, a model's preference towards implementing\na strategy is dictated by its complexity. This helps explain well-known ICL\nphenomena, while offering novel predictions: e.g., we show a superlinear trend\nin the timescale for transitioning from generalization to memorization as task\ndiversity increases. Overall, our work advances an explanatory and predictive\naccount of ICL grounded in tradeoffs between strategy loss and complexity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.17859.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "662838fe4a0dc08679dfe06d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/662838fe4a0dc08679dfe06d/xd71eX59Tu5ATxRnYD07V.png",
            "fullname": "Daniel Wurgaft",
            "name": "DanielWurgaft",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21594",
            "authors": [
                {
                    "_id": "68625a4c9e7509383d29ac4c",
                    "name": "Ahmed M. Adly",
                    "hidden": false
                },
                {
                    "_id": "68625a4c9e7509383d29ac4d",
                    "name": "Mostafa Samy",
                    "hidden": false
                },
                {
                    "_id": "68625a4c9e7509383d29ac4e",
                    "name": "Amr Fawzy",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-18T09:44:21.000Z",
            "submittedOnDailyAt": "2025-06-30T08:07:09.494Z",
            "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with\n  Parameter-Efficient Two-Stage Training",
            "submittedOnDailyBy": {
                "_id": "63aca106e3b217fb36cf1950",
                "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
                "isPro": false,
                "fullname": "Ahmed Mostafa",
                "user": "AhmedMostafa",
                "type": "user"
            },
            "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability.",
            "upvotes": 5,
            "discussionId": "68625a4c9e7509383d29ac4f",
            "ai_summary": "Gazal-R1, a 32-billion-parameter language model, achieves top performance in medical reasoning through strategic training, including advanced parameter-efficient techniques and reinforcement learning, providing detailed explanations for clinical decisions.",
            "ai_keywords": [
                "Weight-Decomposed Low-Rank Adaptation (DoRA)",
                "Rank-Stabilized LoRA (rsLoRA)",
                "Group Relative Policy Optimization (GRPO)",
                "MedQA",
                "MMLU Pro (Medical)",
                "PubMedQA",
                "reasoning-capable models",
                "reward hacking",
                "training instability"
            ]
        },
        "publishedAt": "2025-06-18T05:44:21.000Z",
        "title": "Gazal-R1: Achieving State-of-the-Art Medical Reasoning with\n  Parameter-Efficient Two-Stage Training",
        "summary": "We present Gazal-R1, a 32-billion-parameter language model that achieves\nstate-of-the-art performance in medical reasoning while providing transparent,\nstep-by-step explanations for clinical decision-making. Built upon Qwen3 32B,\nour model demonstrates that strategic training can enable mid-sized models to\noutperform significantly larger counterparts in specialized domains. We\ndeveloped a novel two-stage training pipeline: first, supervised fine-tuning on\na carefully curated dataset of 107,033 synthetic medical reasoning examples\nthat teaches structured clinical thinking, enhanced by advanced\nparameter-efficient techniques including Weight-Decomposed Low-Rank Adaptation\n(DoRA) and Rank-Stabilized LoRA (rsLoRA); second, reinforcement learning using\nGroup Relative Policy Optimization (GRPO) with a sophisticated multi-component\nreward system that refines accuracy, format adherence, and reasoning quality.\nGazal-R1 achieves exceptional performance across medical benchmarks, scoring\n87.1% on MedQA, 81.6% on MMLU Pro (Medical), and 79.6% on PubMedQA, surpassing\nmodels up to 12x larger. Beyond its strong empirical results, this work\nprovides detailed insights into the challenges of training reasoning-capable\nmodels in specialized domains, including issues with reward hacking, training\ninstability, and the fundamental tension between factual recall and detailed\nreasoning. Our methodology offers a reproducible framework for developing\nhigh-capability, domain-specific language models that balance performance,\nefficiency, and explainability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21594.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63aca106e3b217fb36cf1950",
            "avatarUrl": "/avatars/b37cc9102f875b6ce0c55a294c052078.svg",
            "fullname": "Ahmed Mostafa",
            "name": "AhmedMostafa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.19741",
            "authors": [
                {
                    "_id": "686209869e7509383d29ab92",
                    "name": "Yihong Luo",
                    "hidden": false
                },
                {
                    "_id": "686209869e7509383d29ab93",
                    "name": "Shuchen Xue",
                    "hidden": false
                },
                {
                    "_id": "686209869e7509383d29ab94",
                    "name": "Tianyang Hu",
                    "hidden": false
                },
                {
                    "_id": "686209869e7509383d29ab95",
                    "name": "Jing Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T15:58:55.000Z",
            "submittedOnDailyAt": "2025-06-30T02:20:48.977Z",
            "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
            "submittedOnDailyBy": {
                "_id": "65f7e6856bd4bac5b6a4ecc3",
                "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
                "isPro": false,
                "fullname": "Yihong Luo",
                "user": "Luo-Yihong",
                "type": "user"
            },
            "summary": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT",
            "upvotes": 4,
            "discussionId": "686209869e7509383d29ab96",
            "ai_summary": "A novel Noise Consistency Training approach integrates new control signals into pre-trained one-step generators efficiently without retraining, outperforming existing methods in quality and computational efficiency.",
            "ai_keywords": [
                "diffusion distillation",
                "Noise Consistency Training",
                "NCT",
                "one-step generators",
                "adapter module",
                "noise consistency loss",
                "noise space",
                "conditional distribution",
                "generative modeling",
                "data-efficient",
                "computational efficiency"
            ]
        },
        "publishedAt": "2025-06-24T11:58:55.000Z",
        "title": "Noise Consistency Training: A Native Approach for One-Step Generator in\n  Learning Additional Controls",
        "summary": "The pursuit of efficient and controllable high-quality content generation\nremains a central challenge in artificial intelligence-generated content\n(AIGC). While one-step generators, enabled by diffusion distillation\ntechniques, offer excellent generation quality and computational efficiency,\nadapting them to new control conditions--such as structural constraints,\nsemantic guidelines, or external inputs--poses a significant challenge.\nConventional approaches often necessitate computationally expensive\nmodifications to the base model and subsequent diffusion distillation. This\npaper introduces Noise Consistency Training (NCT), a novel and lightweight\napproach to directly integrate new control signals into pre-trained one-step\ngenerators without requiring access to original training images or retraining\nthe base diffusion model. NCT operates by introducing an adapter module and\nemploys a noise consistency loss in the noise space of the generator. This loss\naligns the adapted model's generation behavior across noises that are\nconditionally dependent to varying degrees, implicitly guiding it to adhere to\nthe new control. Theoretically, this training objective can be understood as\nminimizing the distributional distance between the adapted generator and the\nconditional distribution induced by the new conditions. NCT is modular,\ndata-efficient, and easily deployable, relying only on the pre-trained one-step\ngenerator and a control signal model. Extensive experiments demonstrate that\nNCT achieves state-of-the-art controllable generation in a single forward pass,\nsurpassing existing multi-step and distillation-based methods in both\ngeneration quality and computational efficiency. Code is available at\nhttps://github.com/Luo-Yihong/NCT",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19741.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65f7e6856bd4bac5b6a4ecc3",
            "avatarUrl": "/avatars/9c0c48310fb5e99c19700316a0ab531e.svg",
            "fullname": "Yihong Luo",
            "name": "Luo-Yihong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.18330",
            "authors": [
                {
                    "_id": "685a0f9e0e4ad7e21975851b",
                    "user": {
                        "_id": "5f0fee727a6784092812b6f7",
                        "avatarUrl": "/avatars/ccc23c76eded98b50a554c79cea0aa92.svg",
                        "isPro": false,
                        "fullname": "nomadlx",
                        "user": "nomadlx",
                        "type": "user"
                    },
                    "name": "Lixin Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-06-24T15:23:10.622Z",
                    "hidden": false
                },
                {
                    "_id": "685a0f9e0e4ad7e21975851c",
                    "name": "Na Cai",
                    "hidden": false
                },
                {
                    "_id": "685a0f9e0e4ad7e21975851d",
                    "name": "Qiao Cheng",
                    "hidden": false
                },
                {
                    "_id": "685a0f9e0e4ad7e21975851e",
                    "name": "Jiachen Wang",
                    "hidden": false
                },
                {
                    "_id": "685a0f9e0e4ad7e21975851f",
                    "name": "Yitao Duan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-23T06:23:53.000Z",
            "submittedOnDailyAt": "2025-06-30T10:20:43.651Z",
            "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning",
            "submittedOnDailyBy": {
                "_id": "5f0fee727a6784092812b6f7",
                "avatarUrl": "/avatars/ccc23c76eded98b50a554c79cea0aa92.svg",
                "isPro": false,
                "fullname": "nomadlx",
                "user": "nomadlx",
                "type": "user"
            },
            "summary": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math.",
            "upvotes": 4,
            "discussionId": "685a0f9f0e4ad7e219758520",
            "ai_summary": "Confucius3-Math, a 14B parameter large language model, achieves state-of-the-art performance on mathematical reasoning tasks using reinforcement learning techniques and is optimized for education in China.",
            "ai_keywords": [
                "large language model",
                "parameters",
                "consumer-grade GPU",
                "SOTA",
                "mathematical reasoning tasks",
                "post-training",
                "reinforcement learning",
                "RL",
                "Targeted Entropy Regularization",
                "Recent Sample Recovery",
                "Policy-Specific Hardness Weighting",
                "entropy regularization",
                "data scheduling policy",
                "group-relative advantage estimator",
                "reasoning models"
            ]
        },
        "publishedAt": "2025-06-23T02:23:53.000Z",
        "title": "Confucius3-Math: A Lightweight High-Performance Reasoning LLM for\n  Chinese K-12 Mathematics Learning",
        "summary": "We introduce Confucius3-Math, an open-source large language model with 14B\nparameters that (1) runs efficiently on a single consumer-grade GPU; (2)\nachieves SOTA performances on a range of mathematical reasoning tasks,\noutperforming many models with significantly larger sizes. In particular, as\npart of our mission to enhancing education and knowledge dissemination with AI,\nConfucius3-Math is specifically committed to mathematics learning for Chinese\nK-12 students and educators. Built via post-training with large-scale\nreinforcement learning (RL), Confucius3-Math aligns with national curriculum\nand excels at solving main-stream Chinese K-12 mathematical problems with low\ncost. In this report we share our development recipe, the challenges we\nencounter and the techniques we develop to overcome them. In particular, we\nintroduce three technical innovations: Targeted Entropy Regularization, Recent\nSample Recovery and Policy-Specific Hardness Weighting. These innovations\nencompass a new entropy regularization, a novel data scheduling policy, and an\nimproved group-relative advantage estimator. Collectively, they significantly\nstabilize the RL training, improve data efficiency, and boost performance. Our\nwork demonstrates the feasibility of building strong reasoning models in a\nparticular domain at low cost. We open-source our model and code at\nhttps://github.com/netease-youdao/Confucius3-Math.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.18330.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f0fee727a6784092812b6f7",
            "avatarUrl": "/avatars/ccc23c76eded98b50a554c79cea0aa92.svg",
            "fullname": "nomadlx",
            "name": "nomadlx",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.22760",
            "authors": [
                {
                    "_id": "686335d7588cea0da970c849",
                    "name": "Alan Dao",
                    "hidden": false
                },
                {
                    "_id": "686335d7588cea0da970c84a",
                    "name": "Dinh Bach Vu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/gvfqkbX__eLlQowlpvpHL.mp4"
            ],
            "publishedAt": "2025-06-28T05:44:57.000Z",
            "submittedOnDailyAt": "2025-06-30T23:43:52.354Z",
            "title": "Jan-nano Technical Report",
            "submittedOnDailyBy": {
                "_id": "62d7b2339b629105a5d6888a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d7b2339b629105a5d6888a/pYi994fqzXctgoed79oUo.png",
                "isPro": false,
                "fullname": "Alan Dao",
                "user": "alandao",
                "type": "user"
            },
            "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.",
            "upvotes": 3,
            "discussionId": "686335d7588cea0da970c84b",
            "ai_summary": "A 4B parameter language model, Jan-nano, fine-tuned with a novel multi-stage RLVR system, achieves high performance on SimpleQA and can run on consumer hardware due to radical specialization.",
            "ai_keywords": [
                "multi-stage RLVR",
                "next token prediction training (SFT)",
                "SimpleQA",
                "context length"
            ]
        },
        "publishedAt": "2025-06-28T01:44:57.000Z",
        "title": "Jan-nano Technical Report",
        "summary": "Most language models face a fundamental tradeoff where powerful capabilities\nrequire substantial computational resources. We shatter this constraint with\nJan-nano, a 4B parameter language model that redefines efficiency through\nradical specialization: instead of trying to know everything, it masters the\nart of finding anything instantly. Fine-tuned from Qwen3-4B using our novel\nmulti-stage RLVR system that completely eliminates reliance on next token\nprediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with\nMCP integration while running on consumer hardware. With 128K context length,\nJan-nano proves that intelligence isn't about scale, it's about strategy.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62d7b2339b629105a5d6888a/gvfqkbX__eLlQowlpvpHL.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22760.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62d7b2339b629105a5d6888a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d7b2339b629105a5d6888a/pYi994fqzXctgoed79oUo.png",
            "fullname": "Alan Dao",
            "name": "alandao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21718",
            "authors": [
                {
                    "_id": "6862a52d3b7fc785c7ca5dfb",
                    "name": "Yash Akhauri",
                    "hidden": false
                },
                {
                    "_id": "6862a52d3b7fc785c7ca5dfc",
                    "name": "Bryan Lewandowski",
                    "hidden": false
                },
                {
                    "_id": "6862a52d3b7fc785c7ca5dfd",
                    "name": "Cheng-Hsi Lin",
                    "hidden": false
                },
                {
                    "_id": "6862a52d3b7fc785c7ca5dfe",
                    "name": "Adrian N. Reyes",
                    "hidden": false
                },
                {
                    "_id": "6862a52d3b7fc785c7ca5dff",
                    "name": "Grant C. Forbes",
                    "hidden": false
                },
                {
                    "_id": "6862a52d3b7fc785c7ca5e00",
                    "name": "Arissa Wongpanich",
                    "hidden": false
                },
                {
                    "_id": "6862a52d3b7fc785c7ca5e01",
                    "name": "Bangding Yang",
                    "hidden": false
                },
                {
                    "_id": "6862a52d3b7fc785c7ca5e02",
                    "name": "Mohamed S. Abdelfattah",
                    "hidden": false
                },
                {
                    "_id": "6862a52d3b7fc785c7ca5e03",
                    "name": "Sagi Perel",
                    "hidden": false
                },
                {
                    "_id": "6862a52d3b7fc785c7ca5e04",
                    "name": "Xingyou Song",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64c93c8c4524c2aea71d2ec6/VqYhh60L3cSczlp81s_Zs.gif"
            ],
            "publishedAt": "2025-06-26T19:10:08.000Z",
            "submittedOnDailyAt": "2025-06-30T13:28:53.733Z",
            "title": "Performance Prediction for Large Systems via Text-to-Text Regression",
            "submittedOnDailyBy": {
                "_id": "64c93c8c4524c2aea71d2ec6",
                "avatarUrl": "/avatars/1961fb6209182bf4954778dc3d97dde5.svg",
                "isPro": true,
                "fullname": "Xingyou Song",
                "user": "Srizzle",
                "type": "user"
            },
            "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.",
            "upvotes": 2,
            "discussionId": "6862a52e3b7fc785c7ca5e05",
            "githubRepo": "https://github.com/google-deepmind/regress-lm",
            "ai_summary": "A text-to-text regression model achieves high accuracy in predicting resource efficiency for Google's Borg system, surpassing tabular methods, and demonstrates adaptability and uncertainty quantification.",
            "ai_keywords": [
                "text-to-text regression",
                "encoder-decoder",
                "feature engineering",
                "few-shot learning",
                "rank correlation",
                "mean squared error",
                "uncertainty quantification",
                "universal simulators"
            ],
            "githubStars": 29
        },
        "publishedAt": "2025-06-26T15:10:08.000Z",
        "title": "Performance Prediction for Large Systems via Text-to-Text Regression",
        "summary": "In many industries, predicting metric outcomes of large systems is a\nfundamental problem, driven largely by traditional tabular regression. However,\nsuch methods struggle on complex systems data in the wild such as configuration\nfiles or system logs, where feature engineering is often infeasible. We propose\ntext-to-text regression as a general, scalable alternative. For predicting\nresource efficiency on Borg, Google's massive compute cluster scheduling\nsystem, a 60M parameter encoder-decoder, trained from random initialization,\nachieves up to a near perfect 0.99 (0.9 average) rank correlation across the\nentire fleet, and 100x lower MSE than tabular approaches. The model also easily\nadapts to new tasks in only 500 few-shot examples and captures the densities of\ncomplex outcome distributions. Ablation studies highlight the importance of\nusing encoders, increasing sequence length, and the model's inherent\nuncertainty quantification. These findings pave the way for universal\nsimulators of real-world outcomes.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64c93c8c4524c2aea71d2ec6/VqYhh60L3cSczlp81s_Zs.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21718.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64c93c8c4524c2aea71d2ec6",
            "avatarUrl": "/avatars/1961fb6209182bf4954778dc3d97dde5.svg",
            "fullname": "Xingyou Song",
            "name": "Srizzle",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.22149",
            "authors": [
                {
                    "_id": "68625f5d9e7509383d29ac62",
                    "name": "Ronald Fecso",
                    "hidden": false
                },
                {
                    "_id": "68625f5d9e7509383d29ac63",
                    "name": "José Morano",
                    "hidden": false
                },
                {
                    "_id": "68625f5d9e7509383d29ac64",
                    "name": "Ursula Schmidt-Erfurth",
                    "hidden": false
                },
                {
                    "_id": "68625f5d9e7509383d29ac65",
                    "name": "Hrvoje Bogunović",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-27T11:53:54.000Z",
            "submittedOnDailyAt": "2025-06-30T08:31:01.610Z",
            "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation\n  Models",
            "submittedOnDailyBy": {
                "_id": "655b3383ed8df831286969f0",
                "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
                "isPro": false,
                "fullname": "José Morano",
                "user": "j-morano",
                "type": "user"
            },
            "summary": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.",
            "upvotes": 1,
            "discussionId": "68625f5d9e7509383d29ac66",
            "ai_summary": "RetFiner, a vision-language refinement scheme, enhances self-supervised foundation models for OCT by leveraging textual data, improving their downstream performance in retinal disease classification tasks.",
            "ai_keywords": [
                "optical coherence tomography (OCT)",
                "deep learning (DL)",
                "self-supervised learning (SSL)",
                "foundation models (FMs)",
                "supervised fine-tuning",
                "RetFiner",
                "vision-language refinement",
                "linear probing performance"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-06-27T07:53:54.000Z",
        "title": "RetFiner: A Vision-Language Refinement Scheme for Retinal Foundation\n  Models",
        "summary": "The rise of imaging techniques such as optical coherence tomography (OCT) and\nadvances in deep learning (DL) have enabled clinicians and researchers to\nstreamline retinal disease staging. A popular DL approach is self-supervised\nlearning (SSL), where models learn from vast amounts of unlabeled data,\navoiding costly annotation. SSL has allowed the development of foundation\nmodels (FMs), large models that can be used for a variety of downstream tasks.\nHowever, existing FMs for OCT, trained solely on image data, lack a\ncomprehensive and robust semantic understanding of images, as evidenced by\ntheir downstream performance (especially for complex tasks), and thus require\nsupervised fine-tuning (which may be unfeasible) to better adapt to specific\napplications and populations. To address this, we propose RetFiner, an SSL\nvision-language refinement scheme that improves the representations of existing\nFMs and enables their efficient and direct adaptation to specific populations\nfor improved downstream performance. Our method uses a diverse set of training\nobjectives which take advantage of the rich supervisory signal found in textual\ndata. We tested RetFiner on the retinal FMs RETFound, UrFound, and VisionFM,\nshowing significant improvements in linear probing performance on seven highly\ndiverse OCT classification tasks, with an average increase of 5.8, 3.9, and 2.1\npercentage points over their baselines, respectively. Our code and model\nweights are publicly available at https://github.com/ronnief1/RetFiner.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22149.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "655b3383ed8df831286969f0",
            "avatarUrl": "/avatars/38f9a73c6ec40ba0e00de5bffec03bc0.svg",
            "fullname": "José Morano",
            "name": "j-morano",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.21476",
            "authors": [
                {
                    "_id": "6862a56b3b7fc785c7ca5e07",
                    "name": "Srikumar Sastry",
                    "hidden": false
                },
                {
                    "_id": "6862a56b3b7fc785c7ca5e08",
                    "name": "Aayush Dhakal",
                    "hidden": false
                },
                {
                    "_id": "6862a56b3b7fc785c7ca5e09",
                    "name": "Eric Xing",
                    "hidden": false
                },
                {
                    "_id": "6862a56b3b7fc785c7ca5e0a",
                    "name": "Subash Khanal",
                    "hidden": false
                },
                {
                    "_id": "6862a56b3b7fc785c7ca5e0b",
                    "name": "Nathan Jacobs",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T17:05:06.000Z",
            "submittedOnDailyAt": "2025-06-30T13:29:17.027Z",
            "title": "Global and Local Entailment Learning for Natural World Imagery",
            "submittedOnDailyBy": {
                "_id": "64e93050e574e31915800b5e",
                "avatarUrl": "/avatars/7ff4adb4ffb8b47fe293d0b424baf5d1.svg",
                "isPro": false,
                "fullname": "Srikumar Sastry",
                "user": "Srikumar26",
                "type": "user"
            },
            "summary": "Learning the hierarchical structure of data in vision-language models is a\nsignificant challenge. Previous works have attempted to address this challenge\nby employing entailment learning. However, these approaches fail to model the\ntransitive nature of entailment explicitly, which establishes the relationship\nbetween order and semantics within a representation space. In this work, we\nintroduce Radial Cross-Modal Embeddings (RCME), a framework that enables the\nexplicit modeling of transitivity-enforced entailment. Our proposed framework\noptimizes for the partial order of concepts within vision-language models. By\nleveraging our framework, we develop a hierarchical vision-language foundation\nmodel capable of representing the hierarchy in the Tree of Life. Our\nexperiments on hierarchical species classification and hierarchical retrieval\ntasks demonstrate the enhanced performance of our models compared to the\nexisting state-of-the-art models. Our code and models are open-sourced at\nhttps://vishu26.github.io/RCME/index.html.",
            "upvotes": 1,
            "discussionId": "6862a56b3b7fc785c7ca5e0c",
            "projectPage": "https://vishu26.github.io/RCME/index.html",
            "ai_summary": "Radial Cross-Modal Embeddings enable explicit modeling of transitive entailment in vision-language models, leading to improved performance in hierarchical species classification and retrieval tasks.",
            "ai_keywords": [
                "entailment learning",
                "Radial Cross-Modal Embeddings (RCME)",
                "transitive entailment",
                "partial order of concepts",
                "hierarchical vision-language foundation model",
                "Tree of Life"
            ]
        },
        "publishedAt": "2025-06-26T13:05:06.000Z",
        "title": "Global and Local Entailment Learning for Natural World Imagery",
        "summary": "Learning the hierarchical structure of data in vision-language models is a\nsignificant challenge. Previous works have attempted to address this challenge\nby employing entailment learning. However, these approaches fail to model the\ntransitive nature of entailment explicitly, which establishes the relationship\nbetween order and semantics within a representation space. In this work, we\nintroduce Radial Cross-Modal Embeddings (RCME), a framework that enables the\nexplicit modeling of transitivity-enforced entailment. Our proposed framework\noptimizes for the partial order of concepts within vision-language models. By\nleveraging our framework, we develop a hierarchical vision-language foundation\nmodel capable of representing the hierarchy in the Tree of Life. Our\nexperiments on hierarchical species classification and hierarchical retrieval\ntasks demonstrate the enhanced performance of our models compared to the\nexisting state-of-the-art models. Our code and models are open-sourced at\nhttps://vishu26.github.io/RCME/index.html.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21476.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64e93050e574e31915800b5e",
            "avatarUrl": "/avatars/7ff4adb4ffb8b47fe293d0b424baf5d1.svg",
            "fullname": "Srikumar Sastry",
            "name": "Srikumar26",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.19592",
            "authors": [
                {
                    "_id": "6860f1639e7509383d29aaba",
                    "user": {
                        "_id": "63d94d4e89a9683b46d559ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675185437868-noauth.png",
                        "isPro": false,
                        "fullname": "Harisankar Babu",
                        "user": "harisankar95",
                        "type": "user"
                    },
                    "name": "Harisankar Babu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-29T10:14:11.847Z",
                    "hidden": false
                },
                {
                    "_id": "6860f1639e7509383d29aabb",
                    "name": "Philipp Schillinger",
                    "hidden": false
                },
                {
                    "_id": "6860f1639e7509383d29aabc",
                    "name": "Tamim Asfour",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-24T13:02:06.000Z",
            "submittedOnDailyAt": "2025-06-30T20:17:33.317Z",
            "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning",
            "submittedOnDailyBy": {
                "_id": "63d94d4e89a9683b46d559ba",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675185437868-noauth.png",
                "isPro": false,
                "fullname": "Harisankar Babu",
                "user": "harisankar95",
                "type": "user"
            },
            "summary": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment.",
            "upvotes": 1,
            "discussionId": "6860f1639e7509383d29aabd",
            "projectPage": "https://sites.google.com/view/adaptive-llm-planning/",
            "ai_summary": "TAPAS integrates LLMs with symbolic planning to dynamically adapt and generate domain models, initial states, and goals for complex tasks, achieving strong performance in various environments and with real-world robots.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "symbolic planning",
                "TAPAS",
                "multi-agent framework",
                "tool-calling mechanisms",
                "ReAct",
                "natural language plan translation"
            ]
        },
        "publishedAt": "2025-06-24T09:02:06.000Z",
        "title": "Adaptive Domain Modeling with Language Models: A Multi-Agent Approach to\n  Task Planning",
        "summary": "We introduce TAPAS (Task-based Adaptation and Planning using AgentS), a\nmulti-agent framework that integrates Large Language Models (LLMs) with\nsymbolic planning to solve complex tasks without the need for manually defined\nenvironment models. TAPAS employs specialized LLM-based agents that\ncollaboratively generate and adapt domain models, initial states, and goal\nspecifications as needed using structured tool-calling mechanisms. Through this\ntool-based interaction, downstream agents can request modifications from\nupstream agents, enabling adaptation to novel attributes and constraints\nwithout manual domain redefinition. A ReAct (Reason+Act)-style execution agent,\ncoupled with natural language plan translation, bridges the gap between\ndynamically generated plans and real-world robot capabilities. TAPAS\ndemonstrates strong performance in benchmark planning domains and in the\nVirtualHome simulated real-world environment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19592.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63d94d4e89a9683b46d559ba",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675185437868-noauth.png",
            "fullname": "Harisankar Babu",
            "name": "harisankar95",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.15882",
            "authors": [
                {
                    "_id": "686309b9588cea0da970c822",
                    "name": "Sheng Liu",
                    "hidden": false
                },
                {
                    "_id": "686309b9588cea0da970c823",
                    "name": "Tianlang Chen",
                    "hidden": false
                },
                {
                    "_id": "686309b9588cea0da970c824",
                    "name": "Pan Lu",
                    "hidden": false
                },
                {
                    "_id": "686309b9588cea0da970c825",
                    "name": "Haotian Ye",
                    "hidden": false
                },
                {
                    "_id": "686309b9588cea0da970c826",
                    "name": "Yizheng Chen",
                    "hidden": false
                },
                {
                    "_id": "686309b9588cea0da970c827",
                    "name": "Lei Xing",
                    "hidden": false
                },
                {
                    "_id": "686309b9588cea0da970c828",
                    "name": "James Zou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/653585666141b3927a083b4f/xOKzambxbAhT9589qsYnx.png"
            ],
            "publishedAt": "2025-06-18T21:15:59.000Z",
            "submittedOnDailyAt": "2025-06-30T20:37:57.222Z",
            "title": "Fractional Reasoning via Latent Steering Vectors Improves Inference Time\n  Compute",
            "submittedOnDailyBy": {
                "_id": "653585666141b3927a083b4f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653585666141b3927a083b4f/EpMnW_gKz9FV3OvogvUzi.jpeg",
                "isPro": false,
                "fullname": "Sheng Liu",
                "user": "shengliu66",
                "type": "user"
            },
            "summary": "Test-time compute has emerged as a powerful paradigm for improving the\nperformance of large language models (LLMs), where generating multiple outputs\nor refining individual chains can significantly boost answer accuracy. However,\nexisting methods like Best-of-N, majority voting, and self-reflection typically\napply reasoning in a uniform way across inputs, overlooking the fact that\ndifferent problems may require different levels of reasoning depth. In this\nwork, we propose Fractional Reasoning, a training-free and model-agnostic\nframework that enables continuous control over reasoning intensity at inference\ntime, going beyond the limitations of fixed instructional prompts. Our method\noperates by extracting the latent steering vector associated with deeper\nreasoning and reapplying it with a tunable scaling factor, allowing the model\nto tailor its reasoning process to the complexity of each input. This supports\ntwo key modes of test-time scaling: (1) improving output quality in\nbreadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing\nthe correctness of individual reasoning chains in depth-based strategies (e.g.,\nself-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that\nFractional Reasoning consistently improves performance across diverse reasoning\ntasks and models.",
            "upvotes": 1,
            "discussionId": "686309ba588cea0da970c829",
            "projectPage": "https://shengliu66.github.io/fractreason/",
            "ai_summary": "Fractional Reasoning dynamically adjusts reasoning depth during inference to enhance the performance of large language models across various tasks.",
            "ai_keywords": [
                "large language models",
                "test-time compute",
                "Best-of-N",
                "majority voting",
                "self-reflection",
                "Fractional Reasoning",
                "latent steering vector",
                "reasoning intensity",
                "GSM8K",
                "MATH500",
                "GPQA"
            ]
        },
        "publishedAt": "2025-06-18T17:15:59.000Z",
        "title": "Fractional Reasoning via Latent Steering Vectors Improves Inference Time\n  Compute",
        "summary": "Test-time compute has emerged as a powerful paradigm for improving the\nperformance of large language models (LLMs), where generating multiple outputs\nor refining individual chains can significantly boost answer accuracy. However,\nexisting methods like Best-of-N, majority voting, and self-reflection typically\napply reasoning in a uniform way across inputs, overlooking the fact that\ndifferent problems may require different levels of reasoning depth. In this\nwork, we propose Fractional Reasoning, a training-free and model-agnostic\nframework that enables continuous control over reasoning intensity at inference\ntime, going beyond the limitations of fixed instructional prompts. Our method\noperates by extracting the latent steering vector associated with deeper\nreasoning and reapplying it with a tunable scaling factor, allowing the model\nto tailor its reasoning process to the complexity of each input. This supports\ntwo key modes of test-time scaling: (1) improving output quality in\nbreadth-based strategies (e.g., Best-of-N, majority voting), and (2) enhancing\nthe correctness of individual reasoning chains in depth-based strategies (e.g.,\nself-reflection). Experiments on GSM8K, MATH500, and GPQA demonstrate that\nFractional Reasoning consistently improves performance across diverse reasoning\ntasks and models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/653585666141b3927a083b4f/xOKzambxbAhT9589qsYnx.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.15882.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653585666141b3927a083b4f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653585666141b3927a083b4f/EpMnW_gKz9FV3OvogvUzi.jpeg",
            "fullname": "Sheng Liu",
            "name": "shengliu66",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.22049",
            "authors": [
                {
                    "_id": "6862a0ce3b7fc785c7ca5dea",
                    "name": "Tianhao Chen",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5deb",
                    "name": "Xin Xu",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5dec",
                    "name": "Zijing Liu",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5ded",
                    "name": "Pengxiang Li",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5dee",
                    "name": "Xinyuan Song",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5def",
                    "name": "Ajay Kumar Jaiswal",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5df0",
                    "name": "Fan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5df1",
                    "name": "Jishan Hu",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5df2",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5df3",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5df4",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5df5",
                    "name": "Shiwei Liu",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5df6",
                    "name": "Yu Li",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5df7",
                    "name": "Yin Lu",
                    "hidden": false
                },
                {
                    "_id": "6862a0ce3b7fc785c7ca5df8",
                    "name": "Can Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-27T09:45:15.000Z",
            "submittedOnDailyAt": "2025-06-30T13:06:38.137Z",
            "title": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling",
            "submittedOnDailyBy": {
                "_id": "64245f2c089d5fae56b4549a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
                "isPro": false,
                "fullname": "Pengxiang Li",
                "user": "pengxiang",
                "type": "user"
            },
            "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.",
            "upvotes": 0,
            "discussionId": "6862a0ce3b7fc785c7ca5df9",
            "ai_summary": "Gradient-Preserving Activation Scaling (GPAS) mitigates activation variance issues in Pre-LayerNorm Transformers and enhances training dynamics across different architectures.",
            "ai_keywords": [
                "Pre-LayerNorm",
                "Transformer architecture",
                "activation variance",
                "residual path",
                "learning capacity",
                "Gradient-Preserving Activation Scaling",
                "GPAS",
                "gradient vanishing problem",
                "Sandwich-LN",
                "DeepNorm",
                "training dynamics"
            ]
        },
        "publishedAt": "2025-06-27T05:45:15.000Z",
        "title": "GPAS: Accelerating Convergence of LLM Pretraining via\n  Gradient-Preserving Activation Scaling",
        "summary": "Modern Large Language Models, such as the LLaMA, Qwen and DeepSeek series,\npredominantly adopt the Pre-LayerNorm (Pre-LN) Transformer architecture. While\nbeing stable during pretraining and scalable to large model sizes, Pre-LN\nsuffers from an exponential growth in activation variance across layers,\ncausing the residual path to dominate over sub-layer outputs and limiting the\nlearning capacity of deeper layers. To mitigate this issue, we propose\nGradient-Preserving Activation Scaling (GPAS), a simple technique that can be\nused in combination with existing approaches. GPAS works by scaling down the\nintermediate activations while keeping their gradients unchanged. This leaves\ninformation in the activations intact, and avoids the gradient vanishing\nproblem associated with gradient downscaling. Extensive experiments across\nvarious model sizes from 71M to 1B show that GPAS achieves consistent\nperformance gains. Beyond enhancing Pre-LN Transformers, GPAS also shows\npromise in improving alternative architectures such as Sandwich-LN and\nDeepNorm, demonstrating its versatility and potential for improving training\ndynamics in a wide range of settings.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22049.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "fullname": "Pengxiang Li",
            "name": "pengxiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    }
]