[
    {
        "paper": {
            "id": "2507.01006",
            "authors": [
                {
                    "_id": "68649318d59a9eda59024a6f",
                    "user": {
                        "_id": "62ecd24cb8764c7738ef2793",
                        "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
                        "isPro": false,
                        "fullname": "Wenyi Hong",
                        "user": "wenyi",
                        "type": "user"
                    },
                    "name": "Wenyi Hong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-02T08:33:49.277Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a70",
                    "user": {
                        "_id": "649d3d271bafbcc83acec930",
                        "avatarUrl": "/avatars/0c42aabf4c6601686c22cc1308c318de.svg",
                        "isPro": false,
                        "fullname": "Wenmeng Yu",
                        "user": "iyuge2",
                        "type": "user"
                    },
                    "name": "Wenmeng Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:01:31.023Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a71",
                    "user": {
                        "_id": "6438bca4a5e10f6d58694b47",
                        "avatarUrl": "/avatars/3aeb25fbc73c5cab1265e13d11adfb76.svg",
                        "isPro": false,
                        "fullname": "XG",
                        "user": "xgeric",
                        "type": "user"
                    },
                    "name": "Xiaotao Gu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:24:12.081Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a72",
                    "name": "Guo Wang",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a73",
                    "user": {
                        "_id": "6752e1774d9c048280780bc5",
                        "avatarUrl": "/avatars/a4e47645f898b69afba2744ef1e64bf9.svg",
                        "isPro": false,
                        "fullname": "GuobingGan",
                        "user": "bigganbing",
                        "type": "user"
                    },
                    "name": "Guobing Gan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:34:24.248Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a74",
                    "user": {
                        "_id": "6864fee46534c596c47483cd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8pBis_JAOXOOPZuO_sRyp.png",
                        "isPro": false,
                        "fullname": "Haomiao Tang",
                        "user": "tanghme0www",
                        "type": "user"
                    },
                    "name": "Haomiao Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:44:29.600Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a75",
                    "user": {
                        "_id": "627626d42d26ac639e56f565",
                        "avatarUrl": "/avatars/805c5f909f52656345b8bde486c9fa8f.svg",
                        "isPro": false,
                        "fullname": "Jiale Cheng",
                        "user": "CCCCCC",
                        "type": "user"
                    },
                    "name": "Jiale Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:39:50.429Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a76",
                    "user": {
                        "_id": "6864b9f90a269bace8c92164",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/zSeWHZRCZtzZ13CzFrr5u.png",
                        "isPro": false,
                        "fullname": "Ji Qi",
                        "user": "miracle11121",
                        "type": "user"
                    },
                    "name": "Ji Qi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:24:35.989Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a77",
                    "user": {
                        "_id": "649cffffe89a765015a243a4",
                        "avatarUrl": "/avatars/20753e7ed68d1fad6ff633101b0ee2e4.svg",
                        "isPro": false,
                        "fullname": "Junhui Ji",
                        "user": "jasonnoy",
                        "type": "user"
                    },
                    "name": "Junhui Ji",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:32:44.176Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a78",
                    "user": {
                        "_id": "65d421895ab888c9d4a0d333",
                        "avatarUrl": "/avatars/3c278eac6eb3296337863de96d120d18.svg",
                        "isPro": false,
                        "fullname": "kinnplh",
                        "user": "kinnplh",
                        "type": "user"
                    },
                    "name": "Lihang Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:13:43.126Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a79",
                    "user": {
                        "_id": "64004a2d261cfa61f39ab8a0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64004a2d261cfa61f39ab8a0/cr7fmheqwXkPh6p78gVui.jpeg",
                        "isPro": false,
                        "fullname": "ShuaiqiDuan",
                        "user": "ShayDuane",
                        "type": "user"
                    },
                    "name": "Shuaiqi Duan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:32:29.607Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a7a",
                    "user": {
                        "_id": "63403c75b8b51e0098d3555c",
                        "avatarUrl": "/avatars/85725280b63788e387fe73319a54164d.svg",
                        "isPro": false,
                        "fullname": "王维汉",
                        "user": "mactavish91",
                        "type": "user"
                    },
                    "name": "Weihan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T12:22:40.933Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a7b",
                    "user": {
                        "_id": "6441529fb3e14d9ba7b79d61",
                        "avatarUrl": "/avatars/79e81bbde65b5caaa6f7c000bba25296.svg",
                        "isPro": false,
                        "fullname": "Yan",
                        "user": "lykeven",
                        "type": "user"
                    },
                    "name": "Yan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T12:22:30.467Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a7c",
                    "user": {
                        "_id": "65acc5afe2a2c8635614de43",
                        "avatarUrl": "/avatars/c5fce792792cc0b52ed7475d72460c58.svg",
                        "isPro": false,
                        "fullname": "Yean Cheng",
                        "user": "LiquidAmmonia",
                        "type": "user"
                    },
                    "name": "Yean Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:31.993Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a7d",
                    "user": {
                        "_id": "64f5c2f19eaf9d8fb74d2b47",
                        "avatarUrl": "/avatars/2623c5fc9bff757f20b66f7626065d52.svg",
                        "isPro": false,
                        "fullname": "Zehai He",
                        "user": "he-zh22",
                        "type": "user"
                    },
                    "name": "Zehai He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:32:38.491Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a7e",
                    "name": "Zhe Su",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a7f",
                    "user": {
                        "_id": "650c0f7fdab7deefd4631109",
                        "avatarUrl": "/avatars/3d0683d1113c3bd530b5e8b1499b17f6.svg",
                        "isPro": false,
                        "fullname": "ZhenYang21",
                        "user": "ZhenYang21",
                        "type": "user"
                    },
                    "name": "Zhen Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:55:04.067Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a80",
                    "user": {
                        "_id": "64a4fe94c6e1167c3c67053a",
                        "avatarUrl": "/avatars/00cb76622d6afe5786a512abe04f6d6e.svg",
                        "isPro": false,
                        "fullname": "Ziyang Pan",
                        "user": "soupsheep",
                        "type": "user"
                    },
                    "name": "Ziyang Pan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:55:18.341Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a81",
                    "user": {
                        "_id": "62dc173789b4cf157d36ebee",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1659772051636-62dc173789b4cf157d36ebee.jpeg",
                        "isPro": false,
                        "fullname": "Zeng Aohan",
                        "user": "Sengxian",
                        "type": "user"
                    },
                    "name": "Aohan Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:46.680Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a82",
                    "user": {
                        "_id": "6864fbe0f1a19f5474f6d66a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/tSoyjsw2E8z0DqZa6n7D0.png",
                        "isPro": false,
                        "fullname": "wangbaoxu",
                        "user": "wangbaoxu",
                        "type": "user"
                    },
                    "name": "Baoxu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:30:50.007Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a83",
                    "user": {
                        "_id": "6864f3023f1637b315c7a94b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/1LaoqhDJhA5_ZucwotiMt.png",
                        "isPro": false,
                        "fullname": "Boyan Shi",
                        "user": "Boyan118",
                        "type": "user"
                    },
                    "name": "Boyan Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:13.128Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a84",
                    "user": {
                        "_id": "65d9878ce4d11d592e37949b",
                        "avatarUrl": "/avatars/51b35b1f33c526ac8e1b737254d79898.svg",
                        "isPro": false,
                        "fullname": "Pcy",
                        "user": "huohuohuoha",
                        "type": "user"
                    },
                    "name": "Changyu Pang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:34.155Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a85",
                    "user": {
                        "_id": "657bc6df504b90a3c59c91f8",
                        "avatarUrl": "/avatars/d0d99192204adf3da935c5336de238b5.svg",
                        "isPro": false,
                        "fullname": "Chenhui Zhang",
                        "user": "zhangch",
                        "type": "user"
                    },
                    "name": "Chenhui Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:15.146Z",
                    "hidden": true
                },
                {
                    "_id": "68649318d59a9eda59024a86",
                    "name": "Da Yin",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a87",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a88",
                    "name": "Guoqing Chen",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a89",
                    "user": {
                        "_id": "62d7b131f6e8ba66107af761",
                        "avatarUrl": "/avatars/f1c5df47aef69c824fd166722df8f670.svg",
                        "isPro": false,
                        "fullname": "Jiazheng Xu",
                        "user": "xujz0703",
                        "type": "user"
                    },
                    "name": "Jiazheng Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:13:37.291Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a8a",
                    "user": {
                        "_id": "629b5af8ba19b79d3542e755",
                        "avatarUrl": "/avatars/ce7aefa86d8ab865789bb0ba7add16c5.svg",
                        "isPro": false,
                        "fullname": "Gary Chen",
                        "user": "Garygedegege",
                        "type": "user"
                    },
                    "name": "Jiali Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:32:36.660Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a8b",
                    "name": "Jing Chen",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a8c",
                    "user": {
                        "_id": "6604160b8d50be909bea5ff6",
                        "avatarUrl": "/avatars/04c17cbe98fec21aec73bb6e208cb4e8.svg",
                        "isPro": false,
                        "fullname": "Jinhao Chen",
                        "user": "DexterChan",
                        "type": "user"
                    },
                    "name": "Jinhao Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:13:48.911Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a8d",
                    "user": {
                        "_id": "62f89aa89fd0218c2934bdc3",
                        "avatarUrl": "/avatars/313720d2f6b05778e0170ebd0aae2fdd.svg",
                        "isPro": false,
                        "fullname": "linlincode",
                        "user": "linlincode",
                        "type": "user"
                    },
                    "name": "Jinghao Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T12:22:38.009Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a8e",
                    "name": "Jinjiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a8f",
                    "user": {
                        "_id": "6649ae0990135abe9b2b4006",
                        "avatarUrl": "/avatars/559285896e65ea4e03f0b0b2334a0947.svg",
                        "isPro": false,
                        "fullname": "Chen Junjie",
                        "user": "chenjj826",
                        "type": "user"
                    },
                    "name": "Junjie Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T12:22:28.214Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a90",
                    "user": {
                        "_id": "652e814fc22d404ebf7af84a",
                        "avatarUrl": "/avatars/66407dba33beba8f381f1fd18f30ebac.svg",
                        "isPro": false,
                        "fullname": "LEI Le-qi",
                        "user": "le-qi",
                        "type": "user"
                    },
                    "name": "Leqi Lei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:32:33.140Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a91",
                    "name": "Leyi Pan",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a92",
                    "name": "Mingzhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a93",
                    "name": "Qinkai Zheng",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a94",
                    "user": {
                        "_id": "64ba1a106a7c71f00472a053",
                        "avatarUrl": "/avatars/fcf8b0c0ff7cabf3d456e1f72d5b218d.svg",
                        "isPro": false,
                        "fullname": "Sheng Yang",
                        "user": "SuunnYang",
                        "type": "user"
                    },
                    "name": "Sheng Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:32:34.866Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a95",
                    "name": "Shi Zhong",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a96",
                    "user": {
                        "_id": "6406db5cd684369027166986",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6406db5cd684369027166986/Zl-orrGcbY0RbfjfKszn1.jpeg",
                        "isPro": false,
                        "fullname": "Shiyu Huang",
                        "user": "ShiyuHuang",
                        "type": "user"
                    },
                    "name": "Shiyu Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:29.870Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a97",
                    "user": {
                        "_id": "66d9148ec2433bcb235bfd9d",
                        "avatarUrl": "/avatars/edcea7d3066c9bcbe3060862b0a41e91.svg",
                        "isPro": false,
                        "fullname": "ZhaoShuyuan",
                        "user": "Cyrus0246",
                        "type": "user"
                    },
                    "name": "Shuyuan Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:30:55.730Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a98",
                    "user": {
                        "_id": "6604d5b94b0ad9bc40b17cea",
                        "avatarUrl": "/avatars/aefdc6c6056c14e8622d9b3b639a4494.svg",
                        "isPro": false,
                        "fullname": "Sean Xue",
                        "user": "xue4y",
                        "type": "user"
                    },
                    "name": "Siyan Xue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:30:51.968Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a99",
                    "user": {
                        "_id": "648c48d8c0ddeee6df5b6d22",
                        "avatarUrl": "/avatars/8706b0b16dfc332b96c91d3ced31bd0b.svg",
                        "isPro": false,
                        "fullname": "Shangqing Tu",
                        "user": "tsq2000",
                        "type": "user"
                    },
                    "name": "Shangqin Tu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:39:08.904Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a9a",
                    "user": {
                        "_id": "686507e6270dd21f5dbe1610",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cUFPkSj-v1zvBU25WRsiM.png",
                        "isPro": false,
                        "fullname": "mengshengbiao",
                        "user": "fym89",
                        "type": "user"
                    },
                    "name": "Shengbiao Meng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T12:22:33.375Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a9b",
                    "user": {
                        "_id": "64697af7dcbb937d56b98237",
                        "avatarUrl": "/avatars/167b1a52ea74d6d61e54053742412fde.svg",
                        "isPro": false,
                        "fullname": "Tianshu Zhang",
                        "user": "huoyuezaiyuan",
                        "type": "user"
                    },
                    "name": "Tianshu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:13:45.161Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a9c",
                    "user": {
                        "_id": "6739b8405d4d88ae26140940",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/l_5rD6EVQ9vDui0UFDEJq.png",
                        "isPro": false,
                        "fullname": "罗天蔚",
                        "user": "Cyan666",
                        "type": "user"
                    },
                    "name": "Tianwei Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:56.978Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a9d",
                    "user": {
                        "_id": "67fc7ba101fde81f68836d0a",
                        "avatarUrl": "/avatars/482657bba0dc0226df2098b89f915cd9.svg",
                        "isPro": false,
                        "fullname": "Tianxiang Hao",
                        "user": "beyondhtx",
                        "type": "user"
                    },
                    "name": "Tianxiang Hao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:17.165Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a9e",
                    "name": "Tianle Gong",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024a9f",
                    "user": {
                        "_id": "66bd7103cc58b899916f954a",
                        "avatarUrl": "/avatars/a1fe24fe35105613c24ea9db6e5487c5.svg",
                        "isPro": false,
                        "fullname": "liwenkai",
                        "user": "lwk21",
                        "type": "user"
                    },
                    "name": "Wenkai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:51.083Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa0",
                    "user": {
                        "_id": "673c2fcff31343826f64081b",
                        "avatarUrl": "/avatars/0ffb6502d2caf35a8c6b03ea2140a441.svg",
                        "isPro": false,
                        "fullname": "Wei Jia",
                        "user": "WeiJia1220",
                        "type": "user"
                    },
                    "name": "Wei Jia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:01:34.637Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa1",
                    "user": {
                        "_id": "648c64829c935db2b527a764",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648c64829c935db2b527a764/AnTu1dvpac3hUxLQo8A8K.jpeg",
                        "isPro": false,
                        "fullname": "Xin Lv",
                        "user": "davidlvxin",
                        "type": "user"
                    },
                    "name": "Xin Lyu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:54.640Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa2",
                    "user": {
                        "_id": "677dffd82b763404dd896c6e",
                        "avatarUrl": "/avatars/044225ddeafaeec51e098530f481adbd.svg",
                        "isPro": false,
                        "fullname": "Huang Xuancheng",
                        "user": "xchuang17",
                        "type": "user"
                    },
                    "name": "Xuancheng Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:13:47.245Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa3",
                    "user": {
                        "_id": "67cd327432668b04f4555270",
                        "avatarUrl": "/avatars/15e2cef976cbe05c4c5858c88dccf4af.svg",
                        "isPro": false,
                        "fullname": "Yanling Wang",
                        "user": "WYLing",
                        "type": "user"
                    },
                    "name": "Yanling Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:54:49.022Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa4",
                    "user": {
                        "_id": "64d4b5808b65d477e68f2fba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/upUtVjLXZ9lAulngeFh_i.jpeg",
                        "isPro": false,
                        "fullname": "Xue Yadong",
                        "user": "ataraxy3",
                        "type": "user"
                    },
                    "name": "Yadong Xue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:32:40.396Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa5",
                    "user": {
                        "_id": "6864fe5a3063b37e17135486",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/aJYirqgL9qBgGS0YSLyzH.png",
                        "isPro": false,
                        "fullname": "Yanfeng Wang",
                        "user": "wRonG118",
                        "type": "user"
                    },
                    "name": "Yanfeng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:44:20.930Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa6",
                    "user": {
                        "_id": "67a479dc15b391c3d467d202",
                        "avatarUrl": "/avatars/af3587515e3b3bde306a5fffb593293f.svg",
                        "isPro": false,
                        "fullname": "A1phaN",
                        "user": "anyifan",
                        "type": "user"
                    },
                    "name": "Yifan An",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-02T09:43:27.504Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa7",
                    "user": {
                        "_id": "66eae378aed2e0ec9689e9df",
                        "avatarUrl": "/avatars/0ed27107e803cae70f8080d4ec54bfa0.svg",
                        "isPro": false,
                        "fullname": "Evan Du",
                        "user": "EvanDu037",
                        "type": "user"
                    },
                    "name": "Yifan Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:32:42.389Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa8",
                    "user": {
                        "_id": "642ebcefd09a9c63c2124bd2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642ebcefd09a9c63c2124bd2/HFSkoaxIM6nTq1CrY6jyF.jpeg",
                        "isPro": false,
                        "fullname": "Yiming Shi",
                        "user": "Shiym",
                        "type": "user"
                    },
                    "name": "Yiming Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:39:52.595Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aa9",
                    "user": {
                        "_id": "6865031df35853980850da4f",
                        "avatarUrl": "/avatars/2e496e6ac24d508ad1b4549bf4cb6bc1.svg",
                        "isPro": false,
                        "fullname": "yiheng huang",
                        "user": "hyheng",
                        "type": "user"
                    },
                    "name": "Yiheng Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T10:05:42.668Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aaa",
                    "name": "Yilin Niu",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aab",
                    "user": {
                        "_id": "6864ef64d4dde09fbabad95a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sbEmJxiJg4-8rrXDqUuFy.jpeg",
                        "isPro": false,
                        "fullname": "Yuan Wang",
                        "user": "traveler2333",
                        "type": "user"
                    },
                    "name": "Yuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:55:14.252Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aac",
                    "user": {
                        "_id": "68248c7c2fa630b9a0b803c5",
                        "avatarUrl": "/avatars/6f5692457b0dbcad03d0a08707ee304c.svg",
                        "isPro": false,
                        "fullname": "Yuanchang Yue",
                        "user": "yueyuanchang",
                        "type": "user"
                    },
                    "name": "Yuanchang Yue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:55:16.579Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aad",
                    "user": {
                        "_id": "65af5363101482afcc788e9d",
                        "avatarUrl": "/avatars/e58daff222c847557b136d26b70e3342.svg",
                        "isPro": false,
                        "fullname": "Yuchen Li",
                        "user": "comrade007134",
                        "type": "user"
                    },
                    "name": "Yuchen Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:55:21.496Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aae",
                    "user": {
                        "_id": "655ecdea19fd101f14afc65e",
                        "avatarUrl": "/avatars/e596b4c65a11ad2c4494e31ee2d61c76.svg",
                        "isPro": false,
                        "fullname": "zyt",
                        "user": "zyt1024",
                        "type": "user"
                    },
                    "name": "Yutao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:24:13.857Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aaf",
                    "user": {
                        "_id": "643507d1ce04fdb57e9d7e05",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643507d1ce04fdb57e9d7e05/QuCFAO3v7G3LrVGusqFj1.png",
                        "isPro": false,
                        "fullname": "zR",
                        "user": "ZAHNGYUXUAN",
                        "type": "user"
                    },
                    "name": "Yuxuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T08:02:44.813Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab0",
                    "user": {
                        "_id": "63033dc4e1e7f0e03a5e1a31",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1661157784937-63033dc4e1e7f0e03a5e1a31.jpeg",
                        "isPro": false,
                        "fullname": "Zhengxiao Du",
                        "user": "zxdu20",
                        "type": "user"
                    },
                    "name": "Zhanxiao Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:30:53.686Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab1",
                    "user": {
                        "_id": "62b196646d3d059f40c3df19",
                        "avatarUrl": "/avatars/dbddf54ae949437223f3a438d30ef653.svg",
                        "isPro": false,
                        "fullname": "Zhenyu Hou",
                        "user": "think2try",
                        "type": "user"
                    },
                    "name": "Zhenyu Hou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T09:01:33.136Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab2",
                    "user": {
                        "_id": "6865034aabd541be6b66c5f7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/6lOkw7oiy7H8M7YwSYE2c.png",
                        "isPro": false,
                        "fullname": "xuezhao",
                        "user": "xuezhao998",
                        "type": "user"
                    },
                    "name": "Zhao Xue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T10:05:44.994Z",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab3",
                    "name": "Zhengxiao Du",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab4",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab5",
                    "name": "Peng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab6",
                    "name": "Debing Liu",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab7",
                    "name": "Bin Xu",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab8",
                    "name": "Juanzi Li",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024ab9",
                    "name": "Minlie Huang",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024aba",
                    "name": "Yuxiao Dong",
                    "hidden": false
                },
                {
                    "_id": "68649318d59a9eda59024abb",
                    "name": "Jie Tang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/zDkNTP70lk6UrrLAk_Sdt.jpeg"
            ],
            "publishedAt": "2025-07-01T17:55:04.000Z",
            "submittedOnDailyAt": "2025-07-02T03:07:41.410Z",
            "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "62ecd24cb8764c7738ef2793",
                "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
                "isPro": false,
                "fullname": "Wenyi Hong",
                "user": "wenyi",
                "type": "user"
            },
            "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.",
            "upvotes": 140,
            "discussionId": "68649319d59a9eda59024abc",
            "githubRepo": "https://github.com/THUDM/GLM-4.1V-Thinking",
            "ai_summary": "A vision-language model, GLM-4.1V-Thinking, enhances general-purpose multimodal reasoning through large-scale pre-training and reinforcement learning, achieving state-of-the-art performance across various tasks.",
            "ai_keywords": [
                "vision-language model",
                "reasoning-centric training framework",
                "reinforcement learning",
                "curriculum sampling",
                "vision foundation model",
                "STEM problem solving",
                "video understanding",
                "content recognition",
                "coding",
                "grounding",
                "GUI-based agents",
                "long document understanding"
            ],
            "githubStars": 361
        },
        "publishedAt": "2025-07-01T13:55:04.000Z",
        "title": "GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable\n  Reinforcement Learning",
        "summary": "We present GLM-4.1V-Thinking, a vision-language model (VLM) designed to\nadvance general-purpose multimodal reasoning. In this report, we share our key\nfindings in the development of the reasoning-centric training framework. We\nfirst develop a capable vision foundation model with significant potential\nthrough large-scale pre-training, which arguably sets the upper bound for the\nfinal performance. Reinforcement Learning with Curriculum Sampling (RLCS) then\nunlocks the full potential of the model, leading to comprehensive capability\nenhancement across a diverse range of tasks, including STEM problem solving,\nvideo understanding, content recognition, coding, grounding, GUI-based agents,\nand long document understanding, among others. To facilitate research in this\nfield, we open-source GLM-4.1V-9B-Thinking, which achieves state-of-the-art\nperformance among models of comparable size. In a comprehensive evaluation\nacross 28 public benchmarks, our model outperforms Qwen2.5-VL-7B on nearly all\ntasks and achieves comparable or even superior performance on 18 benchmarks\nrelative to the significantly larger Qwen2.5-VL-72B. Notably,\nGLM-4.1V-9B-Thinking also demonstrates competitive or superior performance\ncompared to closed-source models such as GPT-4o on challenging tasks including\nlong document understanding and STEM reasoning, further underscoring its strong\ncapabilities. Code, models and more information are released at\nhttps://github.com/THUDM/GLM-4.1V-Thinking.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62ecd24cb8764c7738ef2793/zDkNTP70lk6UrrLAk_Sdt.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01006.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62ecd24cb8764c7738ef2793",
            "avatarUrl": "/avatars/c1b80b5c55f9d652c1aaac7919e1fa32.svg",
            "fullname": "Wenyi Hong",
            "name": "wenyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.01001",
            "authors": [
                {
                    "_id": "6864a4ddd59a9eda59024aea",
                    "user": {
                        "_id": "62f662bcc58915315c4eccea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                        "isPro": true,
                        "fullname": "Yilun Zhao",
                        "user": "yilunzhao",
                        "type": "user"
                    },
                    "name": "Yilun Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:38:54.448Z",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024aeb",
                    "user": {
                        "_id": "676ce7767fff9075b5d526fa",
                        "avatarUrl": "/avatars/bf6697163b91564a8d4b773d3f6420bf.svg",
                        "isPro": false,
                        "fullname": "Kaiyan Zhang",
                        "user": "maxzky",
                        "type": "user"
                    },
                    "name": "Kaiyan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:38:52.477Z",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024aec",
                    "user": {
                        "_id": "67492b9e347c3876f22b3684",
                        "avatarUrl": "/avatars/0d80d23f7b10ce8bac689f6e8317a014.svg",
                        "isPro": false,
                        "fullname": "Tiansheng Hu",
                        "user": "HughieHu",
                        "type": "user"
                    },
                    "name": "Tiansheng Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:38:50.408Z",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024aed",
                    "user": {
                        "_id": "67911b57692e0b97d9936ee3",
                        "avatarUrl": "/avatars/270cb16773b0ba2a1fde8e7e18979971.svg",
                        "isPro": false,
                        "fullname": "Sihong Wu",
                        "user": "Primrose255",
                        "type": "user"
                    },
                    "name": "Sihong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T16:07:51.232Z",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024aee",
                    "name": "Ronan Le Bras",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024aef",
                    "name": "Taira Anderson",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af0",
                    "name": "Jonathan Bragg",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af1",
                    "name": "Joseph Chee Chang",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af2",
                    "name": "Jesse Dodge",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af3",
                    "name": "Matt Latzke",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af4",
                    "name": "Yixin Liu",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af5",
                    "name": "Charles McGrady",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af6",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af7",
                    "user": {
                        "_id": "65d6d12012e036f74a08ce75",
                        "avatarUrl": "/avatars/27ccbeb7c6f7d67ad7a6eed744e92a37.svg",
                        "isPro": false,
                        "fullname": "Zihang Wang",
                        "user": "zihang93",
                        "type": "user"
                    },
                    "name": "Zihang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T12:23:36.042Z",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af8",
                    "name": "Chen Zhao",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024af9",
                    "name": "Hannaneh Hajishirzi",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024afa",
                    "name": "Doug Downey",
                    "hidden": false
                },
                {
                    "_id": "6864a4ddd59a9eda59024afb",
                    "name": "Arman Cohan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-01T17:51:59.000Z",
            "submittedOnDailyAt": "2025-07-02T01:48:24.934Z",
            "title": "SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks",
            "submittedOnDailyBy": {
                "_id": "62f662bcc58915315c4eccea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                "isPro": true,
                "fullname": "Yilun Zhao",
                "user": "yilunzhao",
                "type": "user"
            },
            "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.",
            "upvotes": 33,
            "discussionId": "6864a4ded59a9eda59024afc",
            "projectPage": "https://sciarena.allen.ai/",
            "githubRepo": "https://github.com/yale-nlp/SciArena",
            "ai_summary": "SciArena is a community-driven platform for evaluating foundation models on scientific literature tasks, using collective voter judgments to rank models and address the need for reliable automated evaluation.",
            "ai_keywords": [
                "Chatbot Arena",
                "collective intelligence",
                "open-ended scientific tasks",
                "literature-grounded",
                "long-form responses",
                "meta-evaluation benchmark",
                "automated evaluation systems"
            ],
            "githubStars": 26
        },
        "publishedAt": "2025-07-01T13:51:59.000Z",
        "title": "SciArena: An Open Evaluation Platform for Foundation Models in\n  Scientific Literature Tasks",
        "summary": "We present SciArena, an open and collaborative platform for evaluating\nfoundation models on scientific literature tasks. Unlike traditional benchmarks\nfor scientific literature understanding and synthesis, SciArena engages the\nresearch community directly, following the Chatbot Arena evaluation approach of\ncommunity voting on model comparisons. By leveraging collective intelligence,\nSciArena offers a community-driven evaluation of model performance on\nopen-ended scientific tasks that demand literature-grounded, long-form\nresponses. The platform currently supports 23 open-source and proprietary\nfoundation models and has collected over 13,000 votes from trusted researchers\nacross diverse scientific domains. We analyze the data collected so far and\nconfirm that the submitted questions are diverse, aligned with real-world\nliterature needs, and that participating researchers demonstrate strong\nself-consistency and inter-annotator agreement in their evaluations. We discuss\nthe results and insights based on the model ranking leaderboard. To further\npromote research in building model-based automated evaluation systems for\nliterature tasks, we release SciArena-Eval, a meta-evaluation benchmark based\non our collected preference data. The benchmark measures the accuracy of models\nin judging answer quality by comparing their pairwise assessments with human\nvotes. Our experiments highlight the benchmark's challenges and emphasize the\nneed for more reliable automated evaluation methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01001.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f662bcc58915315c4eccea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
            "fullname": "Yilun Zhao",
            "name": "yilunzhao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.00432",
            "authors": [
                {
                    "_id": "686490e9d59a9eda59024a64",
                    "name": "Maggie Huan",
                    "hidden": false
                },
                {
                    "_id": "686490e9d59a9eda59024a65",
                    "name": "Yuetai Li",
                    "hidden": false
                },
                {
                    "_id": "686490e9d59a9eda59024a66",
                    "user": {
                        "_id": "64ab99dcb76bfd863eba64c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
                        "isPro": false,
                        "fullname": "TY.Zheng",
                        "user": "aaabiao",
                        "type": "user"
                    },
                    "name": "Tuney Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:39:59.058Z",
                    "hidden": false
                },
                {
                    "_id": "686490e9d59a9eda59024a67",
                    "name": "Xiaoyu Xu",
                    "hidden": false
                },
                {
                    "_id": "686490e9d59a9eda59024a68",
                    "name": "Seungone Kim",
                    "hidden": false
                },
                {
                    "_id": "686490e9d59a9eda59024a69",
                    "name": "Minxin Du",
                    "hidden": false
                },
                {
                    "_id": "686490e9d59a9eda59024a6a",
                    "name": "Radha Poovendran",
                    "hidden": false
                },
                {
                    "_id": "686490e9d59a9eda59024a6b",
                    "name": "Graham Neubig",
                    "hidden": false
                },
                {
                    "_id": "686490e9d59a9eda59024a6c",
                    "user": {
                        "_id": "6230d750d93e84e233882dbc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
                        "isPro": false,
                        "fullname": "Xiang Yue",
                        "user": "yuexiang96",
                        "type": "user"
                    },
                    "name": "Xiang Yue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T16:07:53.653Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-01T05:23:05.000Z",
            "submittedOnDailyAt": "2025-07-02T00:24:45.275Z",
            "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "6230d750d93e84e233882dbc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
                "isPro": false,
                "fullname": "Xiang Yue",
                "user": "yuexiang96",
                "type": "user"
            },
            "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.",
            "upvotes": 32,
            "discussionId": "686490e9d59a9eda59024a6d",
            "githubRepo": "https://github.com/ReasoningTransfer/Transferability-of-LLM-Reasoning",
            "ai_summary": "Reinforcement learning-tuned models outperform supervised fine-tuned models in generalizing mathematical problem-solving abilities to other domains, indicating a need to re-evaluate training methods for reasoning models.",
            "ai_keywords": [
                "reinforcement learning",
                "supervised fine-tuning",
                "latent-space representation",
                "token-space distribution shift",
                "general-domain structure"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-07-01T01:23:05.000Z",
        "title": "Does Math Reasoning Improve General LLM Capabilities? Understanding\n  Transferability of LLM Reasoning",
        "summary": "Math reasoning has become the poster child of progress in large language\nmodels (LLMs), with new models rapidly surpassing human-level performance on\nbenchmarks like MATH and AIME. But as math leaderboards improve week by week,\nit is worth asking: do these gains reflect broader problem-solving ability or\njust narrow overfitting? To answer this question, we evaluate over 20\nopen-weight reasoning-tuned models across a broad suite of tasks, including\nmath, scientific QA, agent planning, coding, and standard\ninstruction-following. We surprisingly find that most models that succeed in\nmath fail to transfer their gains to other domains. To rigorously study this\nphenomenon, we conduct controlled experiments on Qwen3-14B models using\nmath-only data but different tuning methods. We find that reinforcement\nlearning (RL)-tuned models generalize well across domains, while supervised\nfine-tuning (SFT)-tuned models often forget general capabilities. Latent-space\nrepresentation and token-space distribution shift analyses reveal that SFT\ninduces substantial representation and output drift, while RL preserves\ngeneral-domain structure. Our results suggest a need to rethink standard\npost-training recipes, particularly the reliance on SFT-distilled data for\nadvancing reasoning models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00432.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6230d750d93e84e233882dbc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6230d750d93e84e233882dbc/4MGEekLW3oWzqeFWDWvIK.jpeg",
            "fullname": "Xiang Yue",
            "name": "yuexiang96",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 36
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.23115",
            "authors": [
                {
                    "_id": "68634959588cea0da970c8a5",
                    "user": {
                        "_id": "66add675c7a575aa0e03d5f3",
                        "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
                        "isPro": true,
                        "fullname": "Haonan Chen",
                        "user": "Haon-Chen",
                        "type": "user"
                    },
                    "name": "Haonan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-01T10:13:35.715Z",
                    "hidden": false
                },
                {
                    "_id": "68634959588cea0da970c8a6",
                    "user": {
                        "_id": "648ba0d68e7f7a927675d4a3",
                        "avatarUrl": "/avatars/d82ced93656e03d60c8b55010694f908.svg",
                        "isPro": false,
                        "fullname": "Hong Liu",
                        "user": "hongliu9903",
                        "type": "user"
                    },
                    "name": "Hong Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-01T10:13:40.207Z",
                    "hidden": false
                },
                {
                    "_id": "68634959588cea0da970c8a7",
                    "user": {
                        "_id": "64ed68ce60f6345da7014b38",
                        "avatarUrl": "/avatars/adfc156482ef5570dc69329aa53975e6.svg",
                        "isPro": false,
                        "fullname": "Yuping Luo",
                        "user": "roosephu",
                        "type": "user"
                    },
                    "name": "Yuping Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-01T10:13:37.881Z",
                    "hidden": false
                },
                {
                    "_id": "68634959588cea0da970c8a8",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "68634959588cea0da970c8a9",
                    "name": "Nan Yang",
                    "hidden": false
                },
                {
                    "_id": "68634959588cea0da970c8aa",
                    "name": "Furu Wei",
                    "hidden": false
                },
                {
                    "_id": "68634959588cea0da970c8ab",
                    "name": "Zhicheng Dou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-29T06:41:00.000Z",
            "submittedOnDailyAt": "2025-07-02T00:57:35.541Z",
            "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings",
            "submittedOnDailyBy": {
                "_id": "66add675c7a575aa0e03d5f3",
                "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
                "isPro": true,
                "fullname": "Haonan Chen",
                "user": "Haon-Chen",
                "type": "user"
            },
            "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.",
            "upvotes": 30,
            "discussionId": "6863495a588cea0da970c8ac",
            "projectPage": "https://haon-chen.github.io/MoCa",
            "githubRepo": "https://github.com/haon-chen/MoCa",
            "ai_summary": "MoCa, a two-stage framework, enhances pre-trained causal vision-language models for multimodal embedding by introducing bidirectional attention, scaling with unlabeled data, and diverse training objectives.",
            "ai_keywords": [
                "Vision Language Models",
                "VLMs",
                "bidirectional multimodal embedding models",
                "modality-aware continual pre-training",
                "joint reconstruction objective",
                "heterogeneous contrastive fine-tuning",
                "bidirectional attention",
                "massive unlabeled datasets",
                "MMEB",
                "ViDoRe-v2"
            ],
            "githubStars": 32
        },
        "publishedAt": "2025-06-29T02:41:00.000Z",
        "title": "MoCa: Modality-aware Continual Pre-training Makes Better Bidirectional\n  Multimodal Embeddings",
        "summary": "Multimodal embedding models, built upon causal Vision Language Models (VLMs),\nhave shown promise in various tasks. However, current approaches face three key\nlimitations: the use of causal attention in VLM backbones is suboptimal for\nembedding tasks; scalability issues due to reliance on high-quality labeled\npaired data for contrastive learning; and limited diversity in training\nobjectives and data. To address these issues, we propose MoCa, a two-stage\nframework for transforming pre-trained VLMs into effective bidirectional\nmultimodal embedding models. The first stage, Modality-aware Continual\nPre-training, introduces a joint reconstruction objective that simultaneously\ndenoises interleaved text and image inputs, enhancing bidirectional\ncontext-aware reasoning. The second stage, Heterogeneous Contrastive\nFine-tuning, leverages diverse, semantically rich multimodal data beyond simple\nimage-caption pairs to enhance generalization and alignment. Our method\naddresses the stated limitations by introducing bidirectional attention through\ncontinual pre-training, scaling effectively with massive unlabeled datasets via\njoint reconstruction objectives, and utilizing diverse multimodal data for\nenhanced representation robustness. Experiments demonstrate that MoCa\nconsistently improves performance across MMEB and ViDoRe-v2 benchmarks,\nachieving new state-of-the-art results, and exhibits strong scalability with\nboth model size and training data on MMEB.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23115.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66add675c7a575aa0e03d5f3",
            "avatarUrl": "/avatars/b72b18130664c1de197c1f8df371aa70.svg",
            "fullname": "Haonan Chen",
            "name": "Haon-Chen",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.19852",
            "authors": [
                {
                    "_id": "685d790d61ef876fd500e925",
                    "name": "Xingyang Li",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e926",
                    "user": {
                        "_id": "63129589bbaa385279d1826e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
                        "isPro": false,
                        "fullname": "Muyang Li",
                        "user": "Lmxyy",
                        "type": "user"
                    },
                    "name": "Muyang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-06-27T08:57:19.218Z",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e927",
                    "name": "Tianle Cai",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e928",
                    "name": "Haocheng Xi",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e929",
                    "name": "Shuo Yang",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e92a",
                    "name": "Yujun Lin",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e92b",
                    "name": "Lvmin Zhang",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e92c",
                    "name": "Songlin Yang",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e92d",
                    "name": "Jinbo Hu",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e92e",
                    "name": "Kelly Peng",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e92f",
                    "name": "Maneesh Agrawala",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e930",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e931",
                    "name": "Kurt Keutzer",
                    "hidden": false
                },
                {
                    "_id": "685d790d61ef876fd500e932",
                    "name": "Song Han",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63129589bbaa385279d1826e/X6ojt3M2m0VZGqlL3M-my.mp4"
            ],
            "publishedAt": "2025-06-24T17:59:59.000Z",
            "submittedOnDailyAt": "2025-07-02T02:22:22.027Z",
            "title": "Radial Attention: O(nlog n) Sparse Attention with Energy Decay for\n  Long Video Generation",
            "submittedOnDailyBy": {
                "_id": "63129589bbaa385279d1826e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
                "isPro": false,
                "fullname": "Muyang Li",
                "user": "Lmxyy",
                "type": "user"
            },
            "summary": "Recent advances in diffusion models have enabled high-quality video\ngeneration, but the additional temporal dimension significantly increases\ncomputational costs, making training and inference on long videos prohibitively\nexpensive. In this paper, we identify a phenomenon we term Spatiotemporal\nEnergy Decay in video diffusion models: post-softmax attention scores diminish\nas spatial and temporal distance between tokens increase, akin to the physical\ndecay of signal or waves over space and time in nature. Motivated by this, we\npropose Radial Attention, a scalable sparse attention mechanism with O(n log\nn) complexity that translates energy decay into exponentially decaying compute\ndensity, which is significantly more efficient than standard O(n^2) dense\nattention and more expressive than linear attention. Specifically, Radial\nAttention employs a simple, static attention mask where each token attends to\nspatially nearby tokens, with the attention window size shrinking with temporal\ndistance. Moreover, it allows pre-trained video diffusion models to extend\ntheir generation length with efficient LoRA-based fine-tuning. Extensive\nexperiments show that Radial Attention maintains video quality across\nWan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9times speedup\nover the original dense attention. With minimal tuning, it enables video\ngeneration up to 4times longer while reducing training costs by up to\n4.4times compared to direct fine-tuning and accelerating inference by up to\n3.7times compared to dense attention inference.",
            "upvotes": 28,
            "discussionId": "685d790d61ef876fd500e933",
            "projectPage": "https://hanlab.mit.edu/projects/radial-attention",
            "githubRepo": "https://github.com/mit-han-lab/radial-attention/",
            "ai_summary": "Radial Attention, a scalable sparse attention mechanism, improves efficiency and preserves video quality in diffusion models by leveraging spatiotemporal energy decay.",
            "ai_keywords": [
                "Spatiotemporal Energy Decay",
                "diffusion models",
                "radial attention",
                "attention scores",
                "dense attention",
                "linear attention",
                "attention mask",
                "LoRA-based fine-tuning",
                "video quality",
                "Wan2.1-14B",
                "HunyuanVideo",
                "Mochi 1"
            ],
            "githubStars": 237
        },
        "publishedAt": "2025-06-24T13:59:59.000Z",
        "title": "Radial Attention: O(nlog n) Sparse Attention with Energy Decay for\n  Long Video Generation",
        "summary": "Recent advances in diffusion models have enabled high-quality video\ngeneration, but the additional temporal dimension significantly increases\ncomputational costs, making training and inference on long videos prohibitively\nexpensive. In this paper, we identify a phenomenon we term Spatiotemporal\nEnergy Decay in video diffusion models: post-softmax attention scores diminish\nas spatial and temporal distance between tokens increase, akin to the physical\ndecay of signal or waves over space and time in nature. Motivated by this, we\npropose Radial Attention, a scalable sparse attention mechanism with O(n log\nn) complexity that translates energy decay into exponentially decaying compute\ndensity, which is significantly more efficient than standard O(n^2) dense\nattention and more expressive than linear attention. Specifically, Radial\nAttention employs a simple, static attention mask where each token attends to\nspatially nearby tokens, with the attention window size shrinking with temporal\ndistance. Moreover, it allows pre-trained video diffusion models to extend\ntheir generation length with efficient LoRA-based fine-tuning. Extensive\nexperiments show that Radial Attention maintains video quality across\nWan2.1-14B, HunyuanVideo, and Mochi 1, achieving up to a 1.9times speedup\nover the original dense attention. With minimal tuning, it enables video\ngeneration up to 4times longer while reducing training costs by up to\n4.4times compared to direct fine-tuning and accelerating inference by up to\n3.7times compared to dense attention inference.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63129589bbaa385279d1826e/X6ojt3M2m0VZGqlL3M-my.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.19852.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63129589bbaa385279d1826e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63129589bbaa385279d1826e/0AUs3q4ngRZ-wXuY1jP9G.jpeg",
            "fullname": "Muyang Li",
            "name": "Lmxyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.20639",
            "authors": [
                {
                    "_id": "685d2773696820ba1f28f38c",
                    "user": {
                        "_id": "628c83d186fc004b14e1ed48",
                        "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg",
                        "isPro": false,
                        "fullname": "Shansan Gong",
                        "user": "Sansa",
                        "type": "user"
                    },
                    "name": "Shansan Gong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-02T16:09:38.248Z",
                    "hidden": false
                },
                {
                    "_id": "685d2773696820ba1f28f38d",
                    "name": "Ruixiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "685d2773696820ba1f28f38e",
                    "name": "Huangjie Zheng",
                    "hidden": false
                },
                {
                    "_id": "685d2773696820ba1f28f38f",
                    "name": "Jiatao Gu",
                    "hidden": false
                },
                {
                    "_id": "685d2773696820ba1f28f390",
                    "name": "Navdeep Jaitly",
                    "hidden": false
                },
                {
                    "_id": "685d2773696820ba1f28f391",
                    "name": "Lingpeng Kong",
                    "hidden": false
                },
                {
                    "_id": "685d2773696820ba1f28f392",
                    "name": "Yizhe Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-25T17:35:47.000Z",
            "submittedOnDailyAt": "2025-07-02T01:51:58.159Z",
            "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
            "submittedOnDailyBy": {
                "_id": "628c83d186fc004b14e1ed48",
                "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg",
                "isPro": false,
                "fullname": "Shansan Gong",
                "user": "Sansa",
                "type": "user"
            },
            "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose coupled-GRPO, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.",
            "upvotes": 17,
            "discussionId": "685d2774696820ba1f28f393",
            "githubRepo": "https://github.com/apple/ml-diffucoder",
            "ai_summary": "Diffusion large language models are applied to code generation, revealing their unique denoising processes and benefiting from a novel reinforcement learning sampling scheme.",
            "ai_keywords": [
                "diffusion large language models",
                "autoregressive models",
                "denoising models",
                "global planning",
                "iterative refinement",
                "code generation",
                "decoding behavior",
                "causal generation",
                "sampling temperature",
                "coupled-GRPO",
                "token log-likelihood estimates",
                "RL rollouts"
            ],
            "githubStars": 132
        },
        "publishedAt": "2025-06-25T13:35:47.000Z",
        "title": "DiffuCoder: Understanding and Improving Masked Diffusion Models for Code\n  Generation",
        "summary": "Diffusion large language models (dLLMs) are compelling alternatives to\nautoregressive (AR) models because their denoising models operate over the\nentire sequence. The global planning and iterative refinement features of dLLMs\nare particularly useful for code generation. However, current training and\ninference mechanisms for dLLMs in coding are still under-explored. To demystify\nthe decoding behavior of dLLMs and unlock their potential for coding, we\nsystematically investigate their denoising processes and reinforcement learning\n(RL) methods. We train a 7B dLLM, DiffuCoder, on 130B tokens of code.\nUsing this model as a testbed, we analyze its decoding behavior, revealing how\nit differs from that of AR models: (1) dLLMs can decide how causal their\ngeneration should be without relying on semi-AR decoding, and (2) increasing\nthe sampling temperature diversifies not only token choices but also their\ngeneration order. This diversity creates a rich search space for RL rollouts.\nFor RL training, to reduce the variance of token log-likelihood estimates and\nmaintain training efficiency, we propose coupled-GRPO, a novel\nsampling scheme that constructs complementary mask noise for completions used\nin training. In our experiments, coupled-GRPO significantly improves\nDiffuCoder's performance on code generation benchmarks (+4.4\\% on EvalPlus) and\nreduces reliance on AR causal during decoding. Our work provides deeper insight\ninto the machinery of dLLM generation and offers an effective, diffusion-native\nRL training framework. https://github.com/apple/ml-diffucoder.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.20639.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "628c83d186fc004b14e1ed48",
            "avatarUrl": "/avatars/05ff943a9b89b5f67c5bc254bf45b8f5.svg",
            "fullname": "Shansan Gong",
            "name": "Sansa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21277",
            "authors": [
                {
                    "_id": "68634a2c588cea0da970c8ae",
                    "user": {
                        "_id": "66a097801a26a2350395edc7",
                        "avatarUrl": "/avatars/1e7e127cb7222df7d56e5bfda6bab519.svg",
                        "isPro": false,
                        "fullname": "Qize Yang",
                        "user": "PhilipC",
                        "type": "user"
                    },
                    "name": "Qize Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:40:53.951Z",
                    "hidden": false
                },
                {
                    "_id": "68634a2c588cea0da970c8af",
                    "name": "Shimin Yao",
                    "hidden": false
                },
                {
                    "_id": "68634a2c588cea0da970c8b0",
                    "name": "Weixuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68634a2c588cea0da970c8b1",
                    "user": {
                        "_id": "67067633351e0c16a5c27497",
                        "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
                        "isPro": false,
                        "fullname": "Shenghao Fu",
                        "user": "fushh7",
                        "type": "user"
                    },
                    "name": "Shenghao Fu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:40:56.838Z",
                    "hidden": false
                },
                {
                    "_id": "68634a2c588cea0da970c8b2",
                    "name": "Detao Bai",
                    "hidden": false
                },
                {
                    "_id": "68634a2c588cea0da970c8b3",
                    "name": "Jiaxing Zhao",
                    "hidden": false
                },
                {
                    "_id": "68634a2c588cea0da970c8b4",
                    "user": {
                        "_id": "66ef2611fcc1c455f8dce832",
                        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
                        "isPro": false,
                        "fullname": "Boyuan Sun",
                        "user": "BBBBCHAN",
                        "type": "user"
                    },
                    "name": "Boyuan Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-01T10:13:33.669Z",
                    "hidden": false
                },
                {
                    "_id": "68634a2c588cea0da970c8b5",
                    "name": "Bowen Yin",
                    "hidden": false
                },
                {
                    "_id": "68634a2c588cea0da970c8b6",
                    "name": "Xihan Wei",
                    "hidden": false
                },
                {
                    "_id": "68634a2c588cea0da970c8b7",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T14:01:03.000Z",
            "submittedOnDailyAt": "2025-07-02T00:57:00.933Z",
            "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
            "submittedOnDailyBy": {
                "_id": "67067633351e0c16a5c27497",
                "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
                "isPro": false,
                "fullname": "Shenghao Fu",
                "user": "fushh7",
                "type": "user"
            },
            "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.",
            "upvotes": 10,
            "discussionId": "68634a2c588cea0da970c8b8",
            "githubRepo": "https://github.com/HumanMLLM/HumanOmniV2",
            "ai_summary": "A reinforcement learning-based approach enhances multimodal reasoning by addressing context understanding and shortcut problems, using context, format, accuracy, and logical rewards, and achieving superior performance on the IntentBench benchmark.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Large Language Models",
                "multimodal reasoning",
                "global context understanding",
                "shortcut problem",
                "context reward",
                "logical reward",
                "IntentBench",
                "multimodal benchmark"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-06-26T10:01:03.000Z",
        "title": "HumanOmniV2: From Understanding to Omni-Modal Reasoning with Context",
        "summary": "With the rapid evolution of multimodal large language models, the capacity to\ndeeply understand and interpret human intentions has emerged as a critical\ncapability, which demands detailed and thoughtful reasoning. In recent studies,\nReinforcement Learning (RL) has demonstrated potential in enhancing the\nreasoning capabilities of Large Language Models (LLMs). Nonetheless, the\nchallenges associated with adapting RL to multimodal data and formats remain\nlargely unaddressed. In this paper, we identify two issues in existing\nmultimodal reasoning models: insufficient global context understanding and\nshortcut problems. Insufficient context understanding can happen when a model\nmisinterprets multimodal context, resulting in incorrect answers. The shortcut\nproblem occurs when the model overlooks crucial clues in multimodal inputs,\ndirectly addressing the query without considering the multimodal information.\nTo tackle these issues, we emphasize the necessity for the model to reason with\na clear understanding of the global context within multimodal inputs. This\nglobal context understanding can effectively prevent the model from overlooking\nkey multimodal cues and ensure a thorough reasoning process. To ensure the\naccurate interpretation of multimodal context information, we implement a\ncontext reward judged by a large language model, alongside format and accuracy\nrewards. Additionally, to improve complex reasoning capability, we employ the\nLLM to assess the logical reward, determining whether the reasoning process\nsuccessfully integrates multimodal information with logical methods. We also\nintroduce a reasoning omni-modal benchmark, IntentBench, aimed at evaluating\nmodels in understanding complex human intentions and emotions. Our proposed\nmethod demonstrates advanced performance across multiple omni-modal benchmarks\ncompared to other open-source omni-modal models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21277.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67067633351e0c16a5c27497",
            "avatarUrl": "/avatars/356aa3431198c8931b820a714bcfb19d.svg",
            "fullname": "Shenghao Fu",
            "name": "fushh7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.00951",
            "authors": [
                {
                    "_id": "6864a0cdd59a9eda59024ad4",
                    "name": "Rizwan Qureshi",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ad5",
                    "name": "Ranjan Sapkota",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ad6",
                    "name": "Abbas Shah",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ad7",
                    "name": "Amgad Muneer",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ad8",
                    "name": "Anas Zafar",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ad9",
                    "name": "Ashmal Vayani",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ada",
                    "name": "Maged Shoman",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024adb",
                    "name": "Abdelrahman B. M. Eldaly",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024adc",
                    "name": "Kai Zhang",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024add",
                    "name": "Ferhat Sadak",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ade",
                    "user": {
                        "_id": "5e466ca12d2efc729dc309ad",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1601989543860-5e466ca12d2efc729dc309ad.png",
                        "isPro": false,
                        "fullname": "shaina",
                        "user": "shainaraza",
                        "type": "user"
                    },
                    "name": "Shaina Raza",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:38:56.857Z",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024adf",
                    "name": "Xinqi Fan",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ae0",
                    "name": "Ravid Shwartz-Ziv",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ae1",
                    "name": "Hong Yan",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ae2",
                    "name": "Vinjia Jain",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ae3",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:38:59.004Z",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ae4",
                    "name": "Manoj Karkee",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ae5",
                    "name": "Jia Wu",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ae6",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "6864a0cdd59a9eda59024ae7",
                    "name": "Seyedali Mirjalili",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/sRonQRwdqad3gk75XnJAO.png"
            ],
            "publishedAt": "2025-07-01T16:52:25.000Z",
            "submittedOnDailyAt": "2025-07-02T01:32:29.326Z",
            "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact",
            "submittedOnDailyBy": {
                "_id": "67ddd80896ac367438d400a6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
                "isPro": false,
                "fullname": "Ranjan Sapkota",
                "user": "RanjanSapkota",
                "type": "user"
            },
            "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.",
            "upvotes": 9,
            "discussionId": "6864a0ced59a9eda59024ae8",
            "ai_summary": "The paper synthesizes the interdisciplinary approach to achieving Artificial General Intelligence, emphasizing modular reasoning, memory, multi-agent coordination, and the integration of neurosymbolic systems and reinforcement learning to overcome current model limitations.",
            "ai_keywords": [
                "Artificial General Intelligence (AGI)",
                "Agentic RAG frameworks",
                "retrieval",
                "planning",
                "dynamic tool use",
                "generalization strategies",
                "information compression",
                "test-time adaptation",
                "training-free methods",
                "Vision-Language Models (VLMs)",
                "neurosymbolic systems",
                "reinforcement learning",
                "cognitive scaffolding"
            ]
        },
        "publishedAt": "2025-07-01T12:52:25.000Z",
        "title": "Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive\n  Foundations for Artificial General Intelligence and its Societal Impact",
        "summary": "Can machines truly think, reason and act in domains like humans? This\nenduring question continues to shape the pursuit of Artificial General\nIntelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,\nDeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal\nfluency and partial reasoning, these systems remain fundamentally limited by\ntheir reliance on token-level prediction and lack of grounded agency. This\npaper offers a cross-disciplinary synthesis of AGI development, spanning\nartificial intelligence, cognitive neuroscience, psychology, generative models,\nand agent-based systems. We analyze the architectural and cognitive foundations\nof general intelligence, highlighting the role of modular reasoning, persistent\nmemory, and multi-agent coordination. In particular, we emphasize the rise of\nAgentic RAG frameworks that combine retrieval, planning, and dynamic tool use\nto enable more adaptive behavior. We discuss generalization strategies,\nincluding information compression, test-time adaptation, and training-free\nmethods, as critical pathways toward flexible, domain-agnostic intelligence.\nVision-Language Models (VLMs) are reexamined not just as perception modules but\nas evolving interfaces for embodied understanding and collaborative task\ncompletion. We also argue that true intelligence arises not from scale alone\nbut from the integration of memory and reasoning: an orchestration of modular,\ninteractive, and self-improving components where compression enables adaptive\nbehavior. Drawing on advances in neurosymbolic systems, reinforcement learning,\nand cognitive scaffolding, we explore how recent architectures begin to bridge\nthe gap between statistical learning and goal-directed cognition. Finally, we\nidentify key scientific, technical, and ethical challenges on the path to AGI.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67ddd80896ac367438d400a6/sRonQRwdqad3gk75XnJAO.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00951.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67ddd80896ac367438d400a6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/C1NY6Nv5i0erwLnzCrTUM.png",
            "fullname": "Ranjan Sapkota",
            "name": "RanjanSapkota",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.00339",
            "authors": [
                {
                    "_id": "6865584b554cb97d1ed72815",
                    "name": "Alexander Moore",
                    "hidden": false
                },
                {
                    "_id": "6865584b554cb97d1ed72816",
                    "user": {
                        "_id": "68521750c209a81f4c937aa8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WJM-mczUKezts-ly0Az7r.jpeg",
                        "isPro": false,
                        "fullname": "Amar Saini",
                        "user": "Amar-S",
                        "type": "user"
                    },
                    "name": "Amar Saini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T16:12:48.018Z",
                    "hidden": false
                },
                {
                    "_id": "6865584b554cb97d1ed72817",
                    "name": "Kylie Cancilla",
                    "hidden": false
                },
                {
                    "_id": "6865584b554cb97d1ed72818",
                    "name": "Doug Poland",
                    "hidden": false
                },
                {
                    "_id": "6865584b554cb97d1ed72819",
                    "name": "Carmen Carrano",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/68521750c209a81f4c937aa8/UqGtoK3jp3R7Zwxsi88Ig.mp4"
            ],
            "publishedAt": "2025-07-01T00:36:56.000Z",
            "submittedOnDailyAt": "2025-07-02T14:48:02.828Z",
            "title": "Training for X-Ray Vision: Amodal Segmentation, Amodal Content\n  Completion, and View-Invariant Object Representation from Multi-Camera Video",
            "submittedOnDailyBy": {
                "_id": "68521750c209a81f4c937aa8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WJM-mczUKezts-ly0Az7r.jpeg",
                "isPro": false,
                "fullname": "Amar Saini",
                "user": "Amar-S",
                "type": "user"
            },
            "summary": "Amodal segmentation and amodal content completion require using object priors\nto estimate occluded masks and features of objects in complex scenes. Until\nnow, no data has provided an additional dimension for object context: the\npossibility of multiple cameras sharing a view of a scene. We introduce\nMOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the\nlargest amodal segmentation and first amodal content dataset to date. Cluttered\nscenes of generic household objects are simulated in multi-camera video.\nMOVi-MC-AC contributes to the growing literature of object detection, tracking,\nand segmentation by including two new contributions to the deep learning for\ncomputer vision world. Multiple Camera (MC) settings where objects can be\nidentified and tracked between various unique camera perspectives are rare in\nboth synthetic and real-world video. We introduce a new complexity to synthetic\nvideo by providing consistent object ids for detections and segmentations\nbetween both frames and multiple cameras each with unique features and motion\npatterns on a single scene. Amodal Content (AC) is a reconstructive task in\nwhich models predict the appearance of target objects through occlusions. In\nthe amodal segmentation literature, some datasets have been released with\namodal detection, tracking, and segmentation labels. While other methods rely\non slow cut-and-paste schemes to generate amodal content pseudo-labels, they do\nnot account for natural occlusions present in the modal masks. MOVi-MC-AC\nprovides labels for ~5.8 million object instances, setting a new maximum in the\namodal dataset literature, along with being the first to provide ground-truth\namodal content. The full dataset is available at\nhttps://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,",
            "upvotes": 8,
            "discussionId": "6865584c554cb97d1ed7281a",
            "projectPage": "https://huggingface.co/datasets/Amar-S/MOVi-MC-AC"
        },
        "publishedAt": "2025-06-30T20:36:56.000Z",
        "title": "Training for X-Ray Vision: Amodal Segmentation, Amodal Content\n  Completion, and View-Invariant Object Representation from Multi-Camera Video",
        "summary": "Amodal segmentation and amodal content completion require using object priors\nto estimate occluded masks and features of objects in complex scenes. Until\nnow, no data has provided an additional dimension for object context: the\npossibility of multiple cameras sharing a view of a scene. We introduce\nMOVi-MC-AC: Multiple Object Video with Multi-Cameras and Amodal Content, the\nlargest amodal segmentation and first amodal content dataset to date. Cluttered\nscenes of generic household objects are simulated in multi-camera video.\nMOVi-MC-AC contributes to the growing literature of object detection, tracking,\nand segmentation by including two new contributions to the deep learning for\ncomputer vision world. Multiple Camera (MC) settings where objects can be\nidentified and tracked between various unique camera perspectives are rare in\nboth synthetic and real-world video. We introduce a new complexity to synthetic\nvideo by providing consistent object ids for detections and segmentations\nbetween both frames and multiple cameras each with unique features and motion\npatterns on a single scene. Amodal Content (AC) is a reconstructive task in\nwhich models predict the appearance of target objects through occlusions. In\nthe amodal segmentation literature, some datasets have been released with\namodal detection, tracking, and segmentation labels. While other methods rely\non slow cut-and-paste schemes to generate amodal content pseudo-labels, they do\nnot account for natural occlusions present in the modal masks. MOVi-MC-AC\nprovides labels for ~5.8 million object instances, setting a new maximum in the\namodal dataset literature, along with being the first to provide ground-truth\namodal content. The full dataset is available at\nhttps://huggingface.co/datasets/Amar-S/MOVi-MC-AC ,",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/68521750c209a81f4c937aa8/UqGtoK3jp3R7Zwxsi88Ig.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00339.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "68521750c209a81f4c937aa8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/WJM-mczUKezts-ly0Az7r.jpeg",
            "fullname": "Amar Saini",
            "name": "Amar-S",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.21545",
            "authors": [
                {
                    "_id": "6864cbbad59a9eda59024b40",
                    "name": "Yalun Dai",
                    "hidden": false
                },
                {
                    "_id": "6864cbbad59a9eda59024b41",
                    "user": {
                        "_id": "64c66647725ffa04b2fd6c94",
                        "avatarUrl": "/avatars/620f63f27fa1e90423b0dc22aa8e5809.svg",
                        "isPro": false,
                        "fullname": "yangyu huang",
                        "user": "yangyu90",
                        "type": "user"
                    },
                    "name": "Yangyu Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-02T16:10:20.088Z",
                    "hidden": false
                },
                {
                    "_id": "6864cbbad59a9eda59024b42",
                    "name": "Xin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6864cbbad59a9eda59024b43",
                    "name": "Wenshan Wu",
                    "hidden": false
                },
                {
                    "_id": "6864cbbad59a9eda59024b44",
                    "name": "Chong Li",
                    "hidden": false
                },
                {
                    "_id": "6864cbbad59a9eda59024b45",
                    "name": "Wenhui Lu",
                    "hidden": false
                },
                {
                    "_id": "6864cbbad59a9eda59024b46",
                    "name": "Shijie Cao",
                    "hidden": false
                },
                {
                    "_id": "6864cbbad59a9eda59024b47",
                    "name": "Li Dong",
                    "hidden": false
                },
                {
                    "_id": "6864cbbad59a9eda59024b48",
                    "user": {
                        "_id": "685802f14008a9347510f1d2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8q7mZyRELYdK7BHYf1C9w.png",
                        "isPro": false,
                        "fullname": "Scarlett Li",
                        "user": "Scarlettli",
                        "type": "user"
                    },
                    "name": "Scarlett Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-02T16:10:29.486Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-26T17:59:07.000Z",
            "submittedOnDailyAt": "2025-07-02T04:34:17.870Z",
            "title": "Data Efficacy for Language Model Training",
            "submittedOnDailyBy": {
                "_id": "64d98ef7a4839890b25eb78b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
                "isPro": true,
                "fullname": "Fangyuan Yu",
                "user": "Ksgk-fy",
                "type": "user"
            },
            "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.",
            "upvotes": 7,
            "discussionId": "6864cbbbd59a9eda59024b49",
            "ai_summary": "DELT, a paradigm for enhancing language model performance through data efficacy, consists of data scoring, selection, and ordering, demonstrating significant improvements without increasing data scale or model size.",
            "ai_keywords": [
                "data efficiency",
                "language models",
                "data efficacy",
                "DELT",
                "Data Scoring",
                "Data Selection",
                "Data Ordering",
                "Learnability-Quality Scoring",
                "LQS",
                "Folding Ordering",
                "model forgetting",
                "data distribution bias"
            ]
        },
        "publishedAt": "2025-06-26T13:59:07.000Z",
        "title": "Data Efficacy for Language Model Training",
        "summary": "Data is fundamental to the training of language models (LM). Recent research\nhas been dedicated to data efficiency, which aims to maximize performance by\nselecting a minimal or optimal subset of training data. Techniques such as data\nfiltering, sampling, and selection play a crucial role in this area. To\ncomplement it, we define Data Efficacy, which focuses on maximizing performance\nby optimizing the organization of training data and remains relatively\nunderexplored. This work introduces a general paradigm, DELT, for considering\ndata efficacy in LM training, which highlights the significance of training\ndata organization. DELT comprises three components: Data Scoring, Data\nSelection, and Data Ordering. Among these components, we design\nLearnability-Quality Scoring (LQS), as a new instance of Data Scoring, which\nconsiders both the learnability and quality of each data sample from the\ngradient consistency perspective. We also devise Folding Ordering (FO), as a\nnovel instance of Data Ordering, which addresses issues such as model\nforgetting and data distribution bias. Comprehensive experiments validate the\ndata efficacy in LM training, which demonstrates the following: Firstly,\nvarious instances of the proposed DELT enhance LM performance to varying\ndegrees without increasing the data scale and model size. Secondly, among these\ninstances, the combination of our proposed LQS for data scoring and Folding for\ndata ordering achieves the most significant improvement. Lastly, data efficacy\ncan be achieved together with data efficiency by applying data selection.\nTherefore, we believe that data efficacy is a promising foundational area in LM\ntraining.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.21545.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64d98ef7a4839890b25eb78b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d98ef7a4839890b25eb78b/215-CSVLl81z6CAq0ECWU.jpeg",
            "fullname": "Fangyuan Yu",
            "name": "Ksgk-fy",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.23009",
            "authors": [
                {
                    "_id": "6865807e554cb97d1ed7282c",
                    "name": "Jian Chen",
                    "hidden": false
                },
                {
                    "_id": "6865807e554cb97d1ed7282d",
                    "name": "Wenye Ma",
                    "hidden": false
                },
                {
                    "_id": "6865807e554cb97d1ed7282e",
                    "name": "Penghang Liu",
                    "hidden": false
                },
                {
                    "_id": "6865807e554cb97d1ed7282f",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "6865807e554cb97d1ed72830",
                    "name": "Tengwei Song",
                    "hidden": false
                },
                {
                    "_id": "6865807e554cb97d1ed72831",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "6865807e554cb97d1ed72832",
                    "name": "Chenguang Wang",
                    "hidden": false
                },
                {
                    "_id": "6865807e554cb97d1ed72833",
                    "name": "Ruiyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6865807e554cb97d1ed72834",
                    "name": "Changyou Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-28T20:46:47.000Z",
            "submittedOnDailyAt": "2025-07-02T17:45:13.019Z",
            "title": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "64593836c16ecb4815e082ce",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64593836c16ecb4815e082ce/T9WM0Ly6ADi3a9JUSIsSV.png",
                "isPro": false,
                "fullname": "Jian Chen",
                "user": "puar-playground",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable visual\nreasoning abilities in natural images, text-rich documents, and graphic\ndesigns. However, their ability to interpret music sheets remains\nunderexplored. To bridge this gap, we introduce MusiXQA, the first\ncomprehensive dataset for evaluating and advancing MLLMs in music sheet\nunderstanding. MusiXQA features high-quality synthetic music sheets generated\nvia MusiXTeX, with structured annotations covering note pitch and duration,\nchords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.\nThrough extensive evaluations, we reveal significant limitations of current\nstate-of-the-art MLLMs in this domain. Beyond benchmarking, we developed\nPhi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant\nperformance gains over GPT-based methods. The proposed dataset and model\nestablish a foundation for future advances in MLLMs for music sheet\nunderstanding. Code, data, and model will be released upon acceptance.",
            "upvotes": 6,
            "discussionId": "6865807f554cb97d1ed72835"
        },
        "publishedAt": "2025-06-28T16:46:47.000Z",
        "title": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large\n  Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable visual\nreasoning abilities in natural images, text-rich documents, and graphic\ndesigns. However, their ability to interpret music sheets remains\nunderexplored. To bridge this gap, we introduce MusiXQA, the first\ncomprehensive dataset for evaluating and advancing MLLMs in music sheet\nunderstanding. MusiXQA features high-quality synthetic music sheets generated\nvia MusiXTeX, with structured annotations covering note pitch and duration,\nchords, clefs, key/time signatures, and text, enabling diverse visual QA tasks.\nThrough extensive evaluations, we reveal significant limitations of current\nstate-of-the-art MLLMs in this domain. Beyond benchmarking, we developed\nPhi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant\nperformance gains over GPT-based methods. The proposed dataset and model\nestablish a foundation for future advances in MLLMs for music sheet\nunderstanding. Code, data, and model will be released upon acceptance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23009.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64593836c16ecb4815e082ce",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64593836c16ecb4815e082ce/T9WM0Ly6ADi3a9JUSIsSV.png",
            "fullname": "Jian Chen",
            "name": "puar-playground",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.00162",
            "authors": [
                {
                    "_id": "6864d9fad59a9eda59024b7e",
                    "user": {
                        "_id": "639ae93c3786549794e97c69",
                        "avatarUrl": "/avatars/65f7c0b641145f68f22072a5f77e086d.svg",
                        "isPro": true,
                        "fullname": "YL",
                        "user": "Simase",
                        "type": "user"
                    },
                    "name": "Yu Lu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-02T16:11:04.986Z",
                    "hidden": false
                },
                {
                    "_id": "6864d9fad59a9eda59024b7f",
                    "name": "Yi Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-30T18:11:21.000Z",
            "submittedOnDailyAt": "2025-07-02T07:48:03.620Z",
            "title": "FreeLong++: Training-Free Long Video Generation via Multi-band\n  SpectralFusion",
            "submittedOnDailyBy": {
                "_id": "639ae93c3786549794e97c69",
                "avatarUrl": "/avatars/65f7c0b641145f68f22072a5f77e086d.svg",
                "isPro": true,
                "fullname": "YL",
                "user": "Simase",
                "type": "user"
            },
            "summary": "Recent advances in video generation models have enabled high-quality short\nvideo generation from text prompts. However, extending these models to longer\nvideos remains a significant challenge, primarily due to degraded temporal\nconsistency and visual fidelity. Our preliminary observations show that naively\napplying short-video generation models to longer sequences leads to noticeable\nquality degradation. Further analysis identifies a systematic trend where\nhigh-frequency components become increasingly distorted as video length grows,\nan issue we term high-frequency distortion. To address this, we propose\nFreeLong, a training-free framework designed to balance the frequency\ndistribution of long video features during the denoising process. FreeLong\nachieves this by blending global low-frequency features, which capture holistic\nsemantics across the full video, with local high-frequency features extracted\nfrom short temporal windows to preserve fine details. Building on this,\nFreeLong++ extends FreeLong dual-branch design into a multi-branch architecture\nwith multiple attention branches, each operating at a distinct temporal scale.\nBy arranging multiple window sizes from global to local, FreeLong++ enables\nmulti-band frequency fusion from low to high frequencies, ensuring both\nsemantic continuity and fine-grained motion dynamics across longer video\nsequences. Without any additional training, FreeLong++ can be plugged into\nexisting video generation models (e.g. Wan2.1 and LTX-Video) to produce longer\nvideos with substantially improved temporal consistency and visual fidelity. We\ndemonstrate that our approach outperforms previous methods on longer video\ngeneration tasks (e.g. 4x and 8x of native length). It also supports coherent\nmulti-prompt video generation with smooth scene transitions and enables\ncontrollable video generation using long depth or pose sequences.",
            "upvotes": 4,
            "discussionId": "6864d9fad59a9eda59024b80"
        },
        "publishedAt": "2025-06-30T14:11:21.000Z",
        "title": "FreeLong++: Training-Free Long Video Generation via Multi-band\n  SpectralFusion",
        "summary": "Recent advances in video generation models have enabled high-quality short\nvideo generation from text prompts. However, extending these models to longer\nvideos remains a significant challenge, primarily due to degraded temporal\nconsistency and visual fidelity. Our preliminary observations show that naively\napplying short-video generation models to longer sequences leads to noticeable\nquality degradation. Further analysis identifies a systematic trend where\nhigh-frequency components become increasingly distorted as video length grows,\nan issue we term high-frequency distortion. To address this, we propose\nFreeLong, a training-free framework designed to balance the frequency\ndistribution of long video features during the denoising process. FreeLong\nachieves this by blending global low-frequency features, which capture holistic\nsemantics across the full video, with local high-frequency features extracted\nfrom short temporal windows to preserve fine details. Building on this,\nFreeLong++ extends FreeLong dual-branch design into a multi-branch architecture\nwith multiple attention branches, each operating at a distinct temporal scale.\nBy arranging multiple window sizes from global to local, FreeLong++ enables\nmulti-band frequency fusion from low to high frequencies, ensuring both\nsemantic continuity and fine-grained motion dynamics across longer video\nsequences. Without any additional training, FreeLong++ can be plugged into\nexisting video generation models (e.g. Wan2.1 and LTX-Video) to produce longer\nvideos with substantially improved temporal consistency and visual fidelity. We\ndemonstrate that our approach outperforms previous methods on longer video\ngeneration tasks (e.g. 4x and 8x of native length). It also supports coherent\nmulti-prompt video generation with smooth scene transitions and enables\ncontrollable video generation using long depth or pose sequences.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00162.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "639ae93c3786549794e97c69",
            "avatarUrl": "/avatars/65f7c0b641145f68f22072a5f77e086d.svg",
            "fullname": "YL",
            "name": "Simase",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.23329",
            "authors": [
                {
                    "_id": "68653d00d706a6e48024af8f",
                    "user": {
                        "_id": "647896de5bf35e70ab5da887",
                        "avatarUrl": "/avatars/50a874a0048047e51f25746c5fbe85bb.svg",
                        "isPro": false,
                        "fullname": "Liu Hengyu",
                        "user": "Piang",
                        "type": "user"
                    },
                    "name": "Parker Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T14:11:22.217Z",
                    "hidden": false
                },
                {
                    "_id": "68653d00d706a6e48024af90",
                    "name": "Chenxin Li",
                    "hidden": false
                },
                {
                    "_id": "68653d00d706a6e48024af91",
                    "name": "Zhengxin Li",
                    "hidden": false
                },
                {
                    "_id": "68653d00d706a6e48024af92",
                    "name": "Yipeng Wu",
                    "hidden": false
                },
                {
                    "_id": "68653d00d706a6e48024af93",
                    "name": "Wuyang Li",
                    "hidden": false
                },
                {
                    "_id": "68653d00d706a6e48024af94",
                    "name": "Zhiqin Yang",
                    "hidden": false
                },
                {
                    "_id": "68653d00d706a6e48024af95",
                    "name": "Zhenyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68653d00d706a6e48024af96",
                    "name": "Yunlong Lin",
                    "hidden": false
                },
                {
                    "_id": "68653d00d706a6e48024af97",
                    "name": "Sirui Han",
                    "hidden": false
                },
                {
                    "_id": "68653d00d706a6e48024af98",
                    "name": "Brandon Y. Feng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/0OIt-WPJ1ThZf6kuVlOE0.png"
            ],
            "publishedAt": "2025-06-29T17:02:57.000Z",
            "submittedOnDailyAt": "2025-07-02T12:38:41.099Z",
            "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as\n  Agentic Inverse Rendering",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) excel at descriptive tasks, but whether they\ntruly understand scenes from visual observations remains uncertain. We\nintroduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding\nthrough active creation rather than passive recognition. Grounded in the\nanalysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)\nwith actively using programming and rendering tools to recreate the underlying\n3D structure of an input image, achieving agentic inverse rendering through\ntool use. This \"understanding-by-creating\" approach probes the tool-using\ngenerative capacity of VLAs, moving beyond the descriptive or conversational\ncapacity measured by traditional scene understanding benchmarks. We provide a\ncomprehensive suite of metrics to evaluate geometric accuracy, spatial\nrelations, appearance attributes, and overall plausibility. Initial experiments\non agentic inverse rendering powered by various state-of-the-art VLMs highlight\ncurrent limitations, particularly in visual precision rather than basic tool\nusage. IR3D-Bench, including data and evaluation protocols, is released to\nfacilitate systematic study and development of tool-using VLAs towards genuine\nscene understanding by creating.",
            "upvotes": 4,
            "discussionId": "68653d01d706a6e48024af99",
            "projectPage": "https://ir3d-bench.github.io/",
            "githubRepo": "https://github.com/LiuHengyu321/IR3D-Bench",
            "githubStars": 26
        },
        "publishedAt": "2025-06-29T13:02:57.000Z",
        "title": "IR3D-Bench: Evaluating Vision-Language Model Scene Understanding as\n  Agentic Inverse Rendering",
        "summary": "Vision-language models (VLMs) excel at descriptive tasks, but whether they\ntruly understand scenes from visual observations remains uncertain. We\nintroduce IR3D-Bench, a benchmark challenging VLMs to demonstrate understanding\nthrough active creation rather than passive recognition. Grounded in the\nanalysis-by-synthesis paradigm, IR3D-Bench tasks Vision-Language Agents (VLAs)\nwith actively using programming and rendering tools to recreate the underlying\n3D structure of an input image, achieving agentic inverse rendering through\ntool use. This \"understanding-by-creating\" approach probes the tool-using\ngenerative capacity of VLAs, moving beyond the descriptive or conversational\ncapacity measured by traditional scene understanding benchmarks. We provide a\ncomprehensive suite of metrics to evaluate geometric accuracy, spatial\nrelations, appearance attributes, and overall plausibility. Initial experiments\non agentic inverse rendering powered by various state-of-the-art VLMs highlight\ncurrent limitations, particularly in visual precision rather than basic tool\nusage. IR3D-Bench, including data and evaluation protocols, is released to\nfacilitate systematic study and development of tool-using VLAs towards genuine\nscene understanding by creating.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/0OIt-WPJ1ThZf6kuVlOE0.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23329.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 786
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.22960",
            "authors": [
                {
                    "_id": "6864a811d59a9eda59024afe",
                    "name": "Shreyas Dixit",
                    "hidden": false
                },
                {
                    "_id": "6864a811d59a9eda59024aff",
                    "name": "Ashhar Aziz",
                    "hidden": false
                },
                {
                    "_id": "6864a811d59a9eda59024b00",
                    "name": "Shashwat Bajpai",
                    "hidden": false
                },
                {
                    "_id": "6864a811d59a9eda59024b01",
                    "name": "Vasu Sharma",
                    "hidden": false
                },
                {
                    "_id": "6864a811d59a9eda59024b02",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:38:48.204Z",
                    "hidden": false
                },
                {
                    "_id": "6864a811d59a9eda59024b03",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "6864a811d59a9eda59024b04",
                    "name": "Amitava Das",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-28T17:34:08.000Z",
            "submittedOnDailyAt": "2025-07-02T02:11:36.000Z",
            "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images",
            "submittedOnDailyBy": {
                "_id": "63a4754927f1f64ed7238dac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                "isPro": false,
                "fullname": "Aman Chadha",
                "user": "amanchadha",
                "type": "user"
            },
            "summary": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced.",
            "upvotes": 4,
            "discussionId": "6864a811d59a9eda59024b05",
            "ai_summary": "PECCAVI is a robust image watermarking technique that is resistant to visual paraphrase attacks and distortions, utilizing NMPs and multi-channel frequency domain watermarking.",
            "ai_keywords": [
                "Generative AI",
                "visual paraphrase attack",
                "Non-Melting Points (NMPs)",
                "multi-channel frequency domain watermarking",
                "noisy burnishing",
                "model-agnostic"
            ]
        },
        "publishedAt": "2025-06-28T13:34:08.000Z",
        "title": "Peccavi: Visual Paraphrase Attack Safe and Distortion Free Image\n  Watermarking Technique for AI-Generated Images",
        "summary": "A report by the European Union Law Enforcement Agency predicts that by 2026,\nup to 90 percent of online content could be synthetically generated, raising\nconcerns among policymakers, who cautioned that \"Generative AI could act as a\nforce multiplier for political disinformation. The combined effect of\ngenerative text, images, videos, and audio may surpass the influence of any\nsingle modality.\" In response, California's Bill AB 3211 mandates the\nwatermarking of AI-generated images, videos, and audio. However, concerns\nremain regarding the vulnerability of invisible watermarking techniques to\ntampering and the potential for malicious actors to bypass them entirely.\nGenerative AI-powered de-watermarking attacks, especially the newly introduced\nvisual paraphrase attack, have shown an ability to fully remove watermarks,\nresulting in a paraphrase of the original image. This paper introduces PECCAVI,\nthe first visual paraphrase attack-safe and distortion-free image watermarking\ntechnique. In visual paraphrase attacks, an image is altered while preserving\nits core semantic regions, termed Non-Melting Points (NMPs). PECCAVI\nstrategically embeds watermarks within these NMPs and employs multi-channel\nfrequency domain watermarking. It also incorporates noisy burnishing to counter\nreverse-engineering efforts aimed at locating NMPs to disrupt the embedded\nwatermark, thereby enhancing durability. PECCAVI is model-agnostic. All\nrelevant resources and codes will be open-sourced.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22960.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.00606",
            "authors": [
                {
                    "_id": "68656617554cb97d1ed7281c",
                    "name": "Tao Xiong",
                    "hidden": false
                },
                {
                    "_id": "68656617554cb97d1ed7281d",
                    "name": "Xavier Hu",
                    "hidden": false
                },
                {
                    "_id": "68656617554cb97d1ed7281e",
                    "name": "Wenyan Fan",
                    "hidden": false
                },
                {
                    "_id": "68656617554cb97d1ed7281f",
                    "name": "Shengyu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-01T09:39:04.000Z",
            "submittedOnDailyAt": "2025-07-02T15:34:04.002Z",
            "title": "Mixture of Reasonings: Teach Large Language Models to Reason with\n  Adaptive Strategies",
            "submittedOnDailyBy": {
                "_id": "65897684f8b453e1f57cdb26",
                "avatarUrl": "/avatars/80096d6c808805e1a84a68fb6194a7d4.svg",
                "isPro": false,
                "fullname": "huxueyu",
                "user": "huxueyu",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.",
            "upvotes": 3,
            "discussionId": "68656618554cb97d1ed72820"
        },
        "publishedAt": "2025-07-01T05:39:04.000Z",
        "title": "Mixture of Reasonings: Teach Large Language Models to Reason with\n  Adaptive Strategies",
        "summary": "Large language models (LLMs) excel in complex tasks through advanced\nprompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but\ntheir reliance on manually crafted, task-specific prompts limits adaptability\nand efficiency. We introduce Mixture of Reasoning (MoR), a training framework\nthat embeds diverse reasoning strategies into LLMs for autonomous,\ntask-adaptive reasoning without external prompt engineering. MoR has two\nphases: Thought Generation, creating reasoning chain templates with models like\nGPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets\nfor supervised fine-tuning.Our experiments show that MoR significantly enhances\nperformance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting\nand 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need\nfor task-specific prompts, offering a generalizable solution for robust\nreasoning across diverse tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00606.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65897684f8b453e1f57cdb26",
            "avatarUrl": "/avatars/80096d6c808805e1a84a68fb6194a7d4.svg",
            "fullname": "huxueyu",
            "name": "huxueyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.24019",
            "authors": [
                {
                    "_id": "6865a43f8b0804f8bd58e61e",
                    "name": "Hongxin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6865a43f8b0804f8bd58e61f",
                    "name": "Zheyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6865a43f8b0804f8bd58e620",
                    "name": "Zeyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6865a43f8b0804f8bd58e621",
                    "name": "Zunzhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "6865a43f8b0804f8bd58e622",
                    "name": "Lixing Fang",
                    "hidden": false
                },
                {
                    "_id": "6865a43f8b0804f8bd58e623",
                    "name": "Qinhong Zhou",
                    "hidden": false
                },
                {
                    "_id": "6865a43f8b0804f8bd58e624",
                    "name": "Chuang Gan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-30T16:22:51.000Z",
            "submittedOnDailyAt": "2025-07-02T19:59:01.976Z",
            "title": "Ella: Embodied Social Agents with Lifelong Memory",
            "submittedOnDailyBy": {
                "_id": "6319ef09bc8f3b313f7d04d1",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663253467220-6319ef09bc8f3b313f7d04d1.jpeg",
                "isPro": false,
                "fullname": "Hongxin Zhang",
                "user": "Icefox",
                "type": "user"
            },
            "summary": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/.",
            "upvotes": 2,
            "discussionId": "6865a4408b0804f8bd58e625",
            "projectPage": "https://umass-embodied-agi.github.io/Ella/"
        },
        "publishedAt": "2025-06-30T12:22:51.000Z",
        "title": "Ella: Embodied Social Agents with Lifelong Memory",
        "summary": "We introduce Ella, an embodied social agent capable of lifelong learning\nwithin a community in a 3D open world, where agents accumulate experiences and\nacquire knowledge through everyday visual observations and social interactions.\nAt the core of Ella's capabilities is a structured, long-term multimodal memory\nsystem that stores, updates, and retrieves information effectively. It consists\nof a name-centric semantic memory for organizing acquired knowledge and a\nspatiotemporal episodic memory for capturing multimodal experiences. By\nintegrating this lifelong memory system with foundation models, Ella retrieves\nrelevant information for decision-making, plans daily activities, builds social\nrelationships, and evolves autonomously while coexisting with other intelligent\nbeings in the open world. We conduct capability-oriented evaluations in a\ndynamic 3D open world where 15 agents engage in social activities for days and\nare assessed with a suite of unseen controlled evaluations. Experimental\nresults show that Ella can influence, lead, and cooperate with other agents\nwell to achieve goals, showcasing its ability to learn effectively through\nobservation and social interaction. Our findings highlight the transformative\npotential of combining structured memory systems with foundation models for\nadvancing embodied intelligence. More videos can be found at\nhttps://umass-embodied-agi.github.io/Ella/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24019.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6319ef09bc8f3b313f7d04d1",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1663253467220-6319ef09bc8f3b313f7d04d1.jpeg",
            "fullname": "Hongxin Zhang",
            "name": "Icefox",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.00476",
            "authors": [
                {
                    "_id": "68656c2a554cb97d1ed72822",
                    "name": "Chenliang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68656c2a554cb97d1ed72823",
                    "name": "Zheyuan Hu",
                    "hidden": false
                },
                {
                    "_id": "68656c2a554cb97d1ed72824",
                    "name": "Cengiz Oztireli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-01T06:48:50.000Z",
            "submittedOnDailyAt": "2025-07-02T16:00:37.412Z",
            "title": "FreNBRDF: A Frequency-Rectified Neural Material Representation",
            "submittedOnDailyBy": {
                "_id": "653053c6657ae56cdb5c490b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png",
                "isPro": false,
                "fullname": "Peter Hu",
                "user": "Peter2023HuggingFace",
                "type": "user"
            },
            "summary": "Accurate material modeling is crucial for achieving photorealistic rendering,\nbridging the gap between computer-generated imagery and real-world photographs.\nWhile traditional approaches rely on tabulated BRDF data, recent work has\nshifted towards implicit neural representations, which offer compact and\nflexible frameworks for a range of tasks. However, their behavior in the\nfrequency domain remains poorly understood. To address this, we introduce\nFreNBRDF, a frequency-rectified neural material representation. By leveraging\nspherical harmonics, we integrate frequency-domain considerations into neural\nBRDF modeling. We propose a novel frequency-rectified loss, derived from a\nfrequency analysis of neural materials, and incorporate it into a generalizable\nand adaptive reconstruction and editing pipeline. This framework enhances\nfidelity, adaptability, and efficiency. Extensive experiments demonstrate that\n\\ours improves the accuracy and robustness of material appearance\nreconstruction and editing compared to state-of-the-art baselines, enabling\nmore structured and interpretable downstream tasks and applications.",
            "upvotes": 1,
            "discussionId": "68656c2b554cb97d1ed72825",
            "projectPage": "https://chenliang-zhou.github.io/FrePolad/",
            "githubRepo": "https://github.com/Chenliang-Zhou/FrePolad",
            "githubStars": 4
        },
        "publishedAt": "2025-07-01T02:48:50.000Z",
        "title": "FreNBRDF: A Frequency-Rectified Neural Material Representation",
        "summary": "Accurate material modeling is crucial for achieving photorealistic rendering,\nbridging the gap between computer-generated imagery and real-world photographs.\nWhile traditional approaches rely on tabulated BRDF data, recent work has\nshifted towards implicit neural representations, which offer compact and\nflexible frameworks for a range of tasks. However, their behavior in the\nfrequency domain remains poorly understood. To address this, we introduce\nFreNBRDF, a frequency-rectified neural material representation. By leveraging\nspherical harmonics, we integrate frequency-domain considerations into neural\nBRDF modeling. We propose a novel frequency-rectified loss, derived from a\nfrequency analysis of neural materials, and incorporate it into a generalizable\nand adaptive reconstruction and editing pipeline. This framework enhances\nfidelity, adaptability, and efficiency. Extensive experiments demonstrate that\n\\ours improves the accuracy and robustness of material appearance\nreconstruction and editing compared to state-of-the-art baselines, enabling\nmore structured and interpretable downstream tasks and applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00476.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "653053c6657ae56cdb5c490b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/653053c6657ae56cdb5c490b/Zl0bZZ7-tD9AR6muE8esU.png",
            "fullname": "Peter Hu",
            "name": "Peter2023HuggingFace",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.22973",
            "authors": [
                {
                    "_id": "68641fe0d59a9eda590249f7",
                    "user": {
                        "_id": "648490c2ad9a14dd2ec6960b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/DjtPNSc90hX66EcyRh8qu.png",
                        "isPro": false,
                        "fullname": "Amirhossein Razlighi",
                        "user": "AmirHossein-razlighi",
                        "type": "user"
                    },
                    "name": "AmirHossein Naghi Razlighi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-02T07:40:20.262Z",
                    "hidden": false
                },
                {
                    "_id": "68641fe0d59a9eda590249f8",
                    "name": "Elaheh Badali Golezani",
                    "hidden": false
                },
                {
                    "_id": "68641fe0d59a9eda590249f9",
                    "name": "Shohreh Kasaei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-28T18:11:30.000Z",
            "submittedOnDailyAt": "2025-07-02T14:28:48.493Z",
            "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian\n  Splatting via Learnable Beta Distributions",
            "submittedOnDailyBy": {
                "_id": "648490c2ad9a14dd2ec6960b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/DjtPNSc90hX66EcyRh8qu.png",
                "isPro": false,
                "fullname": "Amirhossein Razlighi",
                "user": "AmirHossein-razlighi",
                "type": "user"
            },
            "summary": "3D Gaussian Splatting enables high-quality real-time rendering but often\nproduces millions of splats, resulting in excessive storage and computational\noverhead. We propose a novel lossy compression method based on learnable\nconfidence scores modeled as Beta distributions. Each splat's confidence is\noptimized through reconstruction-aware losses, enabling pruning of\nlow-confidence splats while preserving visual fidelity. The proposed approach\nis architecture-agnostic and can be applied to any Gaussian Splatting variant.\nIn addition, the average confidence values serve as a new metric to assess the\nquality of the scene. Extensive experiments demonstrate favorable trade-offs\nbetween compression and fidelity compared to prior work. Our code and data are\npublicly available at\nhttps://github.com/amirhossein-razlighi/Confident-Splatting",
            "upvotes": 1,
            "discussionId": "68641fe0d59a9eda590249fa",
            "ai_summary": "A novel lossy compression method using learnable confidence scores improves storage and computational efficiency in 3D Gaussian Splatting without sacrificing visual quality.",
            "ai_keywords": [
                "Gaussian Splatting",
                "learnable confidence scores",
                "Beta distributions",
                "reconstruction-aware losses",
                "pruning",
                "visual fidelity",
                "architecture-agnostic"
            ]
        },
        "publishedAt": "2025-06-28T14:11:30.000Z",
        "title": "Confident Splatting: Confidence-Based Compression of 3D Gaussian\n  Splatting via Learnable Beta Distributions",
        "summary": "3D Gaussian Splatting enables high-quality real-time rendering but often\nproduces millions of splats, resulting in excessive storage and computational\noverhead. We propose a novel lossy compression method based on learnable\nconfidence scores modeled as Beta distributions. Each splat's confidence is\noptimized through reconstruction-aware losses, enabling pruning of\nlow-confidence splats while preserving visual fidelity. The proposed approach\nis architecture-agnostic and can be applied to any Gaussian Splatting variant.\nIn addition, the average confidence values serve as a new metric to assess the\nquality of the scene. Extensive experiments demonstrate favorable trade-offs\nbetween compression and fidelity compared to prior work. Our code and data are\npublicly available at\nhttps://github.com/amirhossein-razlighi/Confident-Splatting",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22973.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "648490c2ad9a14dd2ec6960b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/DjtPNSc90hX66EcyRh8qu.png",
            "fullname": "Amirhossein Razlighi",
            "name": "AmirHossein-razlighi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]