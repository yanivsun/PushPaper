[
    {
        "paper": {
            "id": "2507.01949",
            "authors": [
                {
                    "_id": "6865e6218c83dab5f72d1e47",
                    "name": "Kwai Keye Team",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e48",
                    "name": "Biao Yang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e49",
                    "name": "Bin Wen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4a",
                    "name": "Changyi Liu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4b",
                    "name": "Chenglong Chu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4c",
                    "name": "Chengru Song",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4d",
                    "name": "Chongling Rao",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4e",
                    "name": "Chuan Yi",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e4f",
                    "name": "Da Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e50",
                    "name": "Dunju Zang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e51",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e52",
                    "name": "Guorui Zhou",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e53",
                    "name": "Hao Peng",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e54",
                    "name": "Haojie Ding",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e55",
                    "name": "Jiaming Huang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e56",
                    "user": {
                        "_id": "65802e81b701ff85a37caba8",
                        "avatarUrl": "/avatars/b2e9726893caa7e62aad83b1d02e5b41.svg",
                        "isPro": false,
                        "fullname": "jiangxia cao",
                        "user": "caojiangxia",
                        "type": "user"
                    },
                    "name": "Jiangxia Cao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:44.694Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e57",
                    "name": "Jiankang Chen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e58",
                    "user": {
                        "_id": "61540338e5b9ae6774201e58",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61540338e5b9ae6774201e58/p_minqil1sdiqg5wEVxT5.jpeg",
                        "isPro": false,
                        "fullname": "jingyun",
                        "user": "hjy",
                        "type": "user"
                    },
                    "name": "Jingyun Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:15:01.253Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e59",
                    "name": "Jin Ouyang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5a",
                    "name": "Kaibing Chen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5b",
                    "name": "Kaiyu Jiang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5c",
                    "name": "Kaiyu Tang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5d",
                    "name": "Kun Gai",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5e",
                    "user": {
                        "_id": "662f09423f9804ca5f967e48",
                        "avatarUrl": "/avatars/139436300511b67e4a6c4fb7f36ee166.svg",
                        "isPro": false,
                        "fullname": "Sophie Zhang",
                        "user": "suafie",
                        "type": "user"
                    },
                    "name": "Shengnan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T16:19:02.360Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e5f",
                    "name": "Siyang Mao",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e60",
                    "name": "Sui Huang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e61",
                    "name": "Tianke Zhang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e62",
                    "user": {
                        "_id": "68652063e29f1407b58da60f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hTgS65zsKRPxELjSMDSNm.png",
                        "isPro": false,
                        "fullname": "tingting gao",
                        "user": "TinaGao",
                        "type": "user"
                    },
                    "name": "Tingting Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:47:43.679Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e63",
                    "name": "Wei Chen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e64",
                    "user": {
                        "_id": "65423daba385933e812516d5",
                        "avatarUrl": "/avatars/d18b85b4206ab5905ef5bc95622dff3e.svg",
                        "isPro": false,
                        "fullname": "wei yuan",
                        "user": "yw95",
                        "type": "user"
                    },
                    "name": "Wei Yuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:49.219Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e65",
                    "name": "Xiangyu Wu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e66",
                    "user": {
                        "_id": "64a4dba8fe950993d2d89113",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a4dba8fe950993d2d89113/yukb9NNeVspIX7eFTreUq.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Hu",
                        "user": "huxiao09",
                        "type": "user"
                    },
                    "name": "Xiao Hu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:42.172Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e67",
                    "user": {
                        "_id": "673597cfc2424474d12ca58c",
                        "avatarUrl": "/avatars/f748f19619b07838a66bc419a7a6db9d.svg",
                        "isPro": false,
                        "fullname": "xingyulu",
                        "user": "Xingyulu47",
                        "type": "user"
                    },
                    "name": "Xingyu Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:46.882Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e68",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e69",
                    "user": {
                        "_id": "623d8ca4c29adf5ef6175615",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                        "isPro": false,
                        "fullname": "Yi-Fan Zhang",
                        "user": "yifanzhang114",
                        "type": "user"
                    },
                    "name": "Yi-Fan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:15:06.461Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6a",
                    "name": "Yiping Yang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6b",
                    "name": "Yulong Chen",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6c",
                    "name": "Zhenhua Wu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6d",
                    "name": "Zhenyu Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6e",
                    "user": {
                        "_id": "641948b7d13ffa40812eb239",
                        "avatarUrl": "/avatars/65a0262fea6907bec48ddc1d966742da.svg",
                        "isPro": false,
                        "fullname": "Zhixin Ling",
                        "user": "NamingIsTroublesome",
                        "type": "user"
                    },
                    "name": "Zhixin Ling",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:15:03.454Z",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e6f",
                    "name": "Ziming Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e70",
                    "name": "Dehua Ma",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e71",
                    "name": "Di Xu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e72",
                    "name": "Haixuan Gao",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e73",
                    "name": "Hang Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e74",
                    "name": "Jiawei Guo",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e75",
                    "name": "Jing Wang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e76",
                    "name": "Lejian Ren",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e77",
                    "name": "Muhao Wei",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e78",
                    "name": "Qianqian Wang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e79",
                    "name": "Qigen Hu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7a",
                    "name": "Shiyao Wang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7b",
                    "name": "Tao Yu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7c",
                    "name": "Xinchen Luo",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7d",
                    "name": "Yan Li",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7e",
                    "name": "Yiming Liang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e7f",
                    "name": "Yuhang Hu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e80",
                    "name": "Zeyi Lu",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e81",
                    "name": "Zhuoran Yang",
                    "hidden": false
                },
                {
                    "_id": "6865e6218c83dab5f72d1e82",
                    "name": "Zixing Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T17:57:28.000Z",
            "submittedOnDailyAt": "2025-07-03T00:40:58.759Z",
            "title": "Kwai Keye-VL Technical Report",
            "submittedOnDailyBy": {
                "_id": "623d8ca4c29adf5ef6175615",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
                "isPro": false,
                "fullname": "Yi-Fan Zhang",
                "user": "yifanzhang114",
                "type": "user"
            },
            "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
            "upvotes": 96,
            "discussionId": "6865e6218c83dab5f72d1e83",
            "projectPage": "https://kwai-keye.github.io/",
            "githubRepo": "https://github.com/Kwai-Keye/Keye",
            "githubStars": 383
        },
        "publishedAt": "2025-07-02T13:57:28.000Z",
        "title": "Kwai Keye-VL Technical Report",
        "summary": "While Multimodal Large Language Models (MLLMs) demonstrate remarkable\ncapabilities on static images, they often fall short in comprehending dynamic,\ninformation-dense short-form videos, a dominant medium in today's digital\nlandscape. To bridge this gap, we introduce Kwai Keye-VL, an\n8-billion-parameter multimodal foundation model engineered for leading-edge\nperformance in short-video understanding while maintaining robust\ngeneral-purpose vision-language abilities. The development of Keye-VL rests on\ntwo core pillars: a massive, high-quality dataset exceeding 600 billion tokens\nwith a strong emphasis on video, and an innovative training recipe. This recipe\nfeatures a four-stage pre-training process for solid vision-language alignment,\nfollowed by a meticulous two-phase post-training process. The first\npost-training stage enhances foundational capabilities like instruction\nfollowing, while the second phase focuses on stimulating advanced reasoning. In\nthis second phase, a key innovation is our five-mode ``cold-start'' data\nmixture, which includes ``thinking'', ``non-thinking'', ``auto-think'', ``think\nwith image'', and high-quality video data. This mixture teaches the model to\ndecide when and how to reason. Subsequent reinforcement learning (RL) and\nalignment steps further enhance these reasoning capabilities and correct\nabnormal model behaviors, such as repetitive outputs. To validate our approach,\nwe conduct extensive evaluations, showing that Keye-VL achieves\nstate-of-the-art results on public video benchmarks and remains highly\ncompetitive on general image-based tasks (Figure 1). Furthermore, we develop\nand release the KC-MMBench, a new benchmark tailored for real-world\nshort-video scenarios, where Keye-VL shows a significant advantage.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01949.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "623d8ca4c29adf5ef6175615",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/623d8ca4c29adf5ef6175615/q7lHao7UPwU1u7YLSP56m.jpeg",
            "fullname": "Yi-Fan Zhang",
            "name": "yifanzhang114",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.01945",
            "authors": [
                {
                    "_id": "6865e4b88c83dab5f72d1e41",
                    "user": {
                        "_id": "6629d7c9fa14eaccf07d8633",
                        "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
                        "isPro": false,
                        "fullname": "Nan Chen",
                        "user": "CNcreator0331",
                        "type": "user"
                    },
                    "name": "Nan Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:51.437Z",
                    "hidden": false
                },
                {
                    "_id": "6865e4b88c83dab5f72d1e42",
                    "name": "Mengqi Huang",
                    "hidden": false
                },
                {
                    "_id": "6865e4b88c83dab5f72d1e43",
                    "name": "Yihao Meng",
                    "hidden": false
                },
                {
                    "_id": "6865e4b88c83dab5f72d1e44",
                    "name": "Zhendong Mao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
            ],
            "publishedAt": "2025-07-02T17:55:50.000Z",
            "submittedOnDailyAt": "2025-07-03T00:57:17.831Z",
            "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
            "submittedOnDailyBy": {
                "_id": "6629d7c9fa14eaccf07d8633",
                "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
                "isPro": false,
                "fullname": "Nan Chen",
                "user": "CNcreator0331",
                "type": "user"
            },
            "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
            "upvotes": 61,
            "discussionId": "6865e4b88c83dab5f72d1e45",
            "projectPage": "https://cn-makers.github.io/long_animation_web/",
            "githubRepo": "https://github.com/CN-makers/LongAnimation",
            "githubStars": 87
        },
        "publishedAt": "2025-07-02T13:55:50.000Z",
        "title": "LongAnimation: Long Animation Generation with Dynamic Global-Local\n  Memory",
        "summary": "Animation colorization is a crucial part of real animation industry\nproduction. Long animation colorization has high labor costs. Therefore,\nautomated long animation colorization based on the video generation model has\nsignificant research value. Existing studies are limited to short-term\ncolorization. These studies adopt a local paradigm, fusing overlapping features\nto achieve smooth transitions between local segments. However, the local\nparadigm neglects global information, failing to maintain long-term color\nconsistency. In this study, we argue that ideal long-term color consistency can\nbe achieved through a dynamic global-local paradigm, i.e., dynamically\nextracting global color-consistent features relevant to the current generation.\nSpecifically, we propose LongAnimation, a novel framework, which mainly\nincludes a SketchDiT, a Dynamic Global-Local Memory (DGLM), and a Color\nConsistency Reward. The SketchDiT captures hybrid reference features to support\nthe DGLM module. The DGLM module employs a long video understanding model to\ndynamically compress global historical features and adaptively fuse them with\nthe current generation features. To refine the color consistency, we introduce\na Color Consistency Reward. During inference, we propose a color consistency\nfusion to smooth the video segment transition. Extensive experiments on both\nshort-term (14 frames) and long-term (average 500 frames) animations show the\neffectiveness of LongAnimation in maintaining short-term and long-term color\nconsistency for open-domain animation colorization task. The code can be found\nat https://cn-makers.github.io/long_animation_web/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6629d7c9fa14eaccf07d8633/0WDfsnDIDJ9hzI6iJym9L.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01945.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6629d7c9fa14eaccf07d8633",
            "avatarUrl": "/avatars/dceb2f6c804c583adf15a3536c8c995b.svg",
            "fullname": "Nan Chen",
            "name": "CNcreator0331",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.01634",
            "authors": [
                {
                    "_id": "6865e04b8c83dab5f72d1e2d",
                    "user": {
                        "_id": "66ef2611fcc1c455f8dce832",
                        "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
                        "isPro": false,
                        "fullname": "Boyuan Sun",
                        "user": "BBBBCHAN",
                        "type": "user"
                    },
                    "name": "Boyuan Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:54.412Z",
                    "hidden": false
                },
                {
                    "_id": "6865e04b8c83dab5f72d1e2e",
                    "name": "Modi Jin",
                    "hidden": false
                },
                {
                    "_id": "6865e04b8c83dab5f72d1e2f",
                    "name": "Bowen Yin",
                    "hidden": false
                },
                {
                    "_id": "6865e04b8c83dab5f72d1e30",
                    "name": "Qibin Hou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T12:05:57.000Z",
            "submittedOnDailyAt": "2025-07-03T00:16:17.577Z",
            "title": "Depth Anything at Any Condition",
            "submittedOnDailyBy": {
                "_id": "66ef2611fcc1c455f8dce832",
                "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
                "isPro": false,
                "fullname": "Boyuan Sun",
                "user": "BBBBCHAN",
                "type": "user"
            },
            "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
            "upvotes": 35,
            "discussionId": "6865e04b8c83dab5f72d1e31",
            "projectPage": "https://ghost233lism.github.io/depthanything-AC-page/",
            "githubRepo": "https://github.com/HVision-NKU/DepthAnythingAC",
            "githubStars": 121
        },
        "publishedAt": "2025-07-02T08:05:57.000Z",
        "title": "Depth Anything at Any Condition",
        "summary": "We present Depth Anything at Any Condition (DepthAnything-AC), a foundation\nmonocular depth estimation (MDE) model capable of handling diverse\nenvironmental conditions. Previous foundation MDE models achieve impressive\nperformance across general scenes but not perform well in complex open-world\nenvironments that involve challenging conditions, such as illumination\nvariations, adverse weather, and sensor-induced distortions. To overcome the\nchallenges of data scarcity and the inability of generating high-quality\npseudo-labels from corrupted images, we propose an unsupervised consistency\nregularization finetuning paradigm that requires only a relatively small amount\nof unlabeled data. Furthermore, we propose the Spatial Distance Constraint to\nexplicitly enforce the model to learn patch-level relative relationships,\nresulting in clearer semantic boundaries and more accurate details.\nExperimental results demonstrate the zero-shot capabilities of DepthAnything-AC\nacross diverse benchmarks, including real-world adverse weather benchmarks,\nsynthetic corruption benchmarks, and general benchmarks.\n  Project Page: https://ghost233lism.github.io/depthanything-AC-page\n  Code: https://github.com/HVision-NKU/DepthAnythingAC",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01634.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66ef2611fcc1c455f8dce832",
            "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
            "fullname": "Boyuan Sun",
            "name": "BBBBCHAN",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.01925",
            "authors": [
                {
                    "_id": "686600cf8c83dab5f72d1ed0",
                    "name": "Yifan Zhong",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed1",
                    "name": "Fengshuo Bai",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed2",
                    "user": {
                        "_id": "6578459d62d3ac1817ed79fe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578459d62d3ac1817ed79fe/AXDJuwLUoEOb4Fj3U0Xxo.jpeg",
                        "isPro": false,
                        "fullname": "Shaofei Cai",
                        "user": "phython96",
                        "type": "user"
                    },
                    "name": "Shaofei Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:14:49.923Z",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed3",
                    "user": {
                        "_id": "66ee5cf1801ea45d7a44a542",
                        "avatarUrl": "/avatars/04bafbcbf1aea3920a79bddbd1a18f42.svg",
                        "isPro": false,
                        "fullname": "XUCHUAN HUANG",
                        "user": "Feernnn",
                        "type": "user"
                    },
                    "name": "Xuchuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:14:52.141Z",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed4",
                    "name": "Zhang Chen",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed5",
                    "name": "Xiaowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed6",
                    "name": "Yuanfei Wang",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed7",
                    "name": "Shaoyang Guo",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed8",
                    "name": "Tianrui Guan",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1ed9",
                    "name": "Ka Nam Lui",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1eda",
                    "name": "Zhiquan Qi",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1edb",
                    "name": "Yitao Liang",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1edc",
                    "name": "Yuanpei Chen",
                    "hidden": false
                },
                {
                    "_id": "686600cf8c83dab5f72d1edd",
                    "name": "Yaodong Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T17:34:52.000Z",
            "submittedOnDailyAt": "2025-07-03T03:39:33.625Z",
            "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
            "submittedOnDailyBy": {
                "_id": "655d9f43b5da99edaf3f2f81",
                "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
                "isPro": false,
                "fullname": "Yifan Zhong",
                "user": "Yifan-Zhong",
                "type": "user"
            },
            "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof action tokens that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
            "upvotes": 22,
            "discussionId": "686600cf8c83dab5f72d1ede",
            "githubRepo": "https://github.com/Psi-Robot/Awesome-VLA-Papers",
            "githubStars": 38
        },
        "publishedAt": "2025-07-02T13:34:52.000Z",
        "title": "A Survey on Vision-Language-Action Models: An Action Tokenization\n  Perspective",
        "summary": "The remarkable advancements of vision and language foundation models in\nmultimodal understanding, reasoning, and generation has sparked growing efforts\nto extend such intelligence to the physical world, fueling the flourishing of\nvision-language-action (VLA) models. Despite seemingly diverse approaches, we\nobserve that current VLA models can be unified under a single framework: vision\nand language inputs are processed by a series of VLA modules, producing a chain\nof action tokens that progressively encode more grounded and\nactionable information, ultimately generating executable actions. We further\ndetermine that the primary design choice distinguishing VLA models lies in how\naction tokens are formulated, which can be categorized into language\ndescription, code, affordance, trajectory, goal state, latent representation,\nraw action, and reasoning. However, there remains a lack of comprehensive\nunderstanding regarding action tokens, significantly impeding effective VLA\ndevelopment and obscuring future directions. Therefore, this survey aims to\ncategorize and interpret existing VLA research through the lens of action\ntokenization, distill the strengths and limitations of each token type, and\nidentify areas for improvement. Through this systematic review and analysis, we\noffer a synthesized outlook on the broader evolution of VLA models, highlight\nunderexplored yet promising directions, and contribute guidance for future\nresearch, hoping to bring the field closer to general-purpose intelligence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01925.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "655d9f43b5da99edaf3f2f81",
            "avatarUrl": "/avatars/c7225b3ed54d099a4fd87682427fb5bf.svg",
            "fullname": "Yifan Zhong",
            "name": "Yifan-Zhong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.01953",
            "authors": [
                {
                    "_id": "686601648c83dab5f72d1ee0",
                    "name": "Yukang Cao",
                    "hidden": false
                },
                {
                    "_id": "686601648c83dab5f72d1ee1",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "686601648c83dab5f72d1ee2",
                    "name": "Jinghao Wang",
                    "hidden": false
                },
                {
                    "_id": "686601648c83dab5f72d1ee3",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T17:58:20.000Z",
            "submittedOnDailyAt": "2025-07-03T02:40:23.949Z",
            "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
            "submittedOnDailyBy": {
                "_id": "63a07c3ab5515dccd40fdb71",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
                "isPro": false,
                "fullname": "Yukang Cao",
                "user": "yukangcao",
                "type": "user"
            },
            "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.",
            "upvotes": 13,
            "discussionId": "686601658c83dab5f72d1ee4",
            "projectPage": "https://yukangcao.github.io/FreeMorph/",
            "githubRepo": "https://github.com/yukangcao/FreeMorph",
            "githubStars": 24
        },
        "publishedAt": "2025-07-02T13:58:20.000Z",
        "title": "FreeMorph: Tuning-Free Generalized Image Morphing with Diffusion Model",
        "summary": "We present FreeMorph, the first tuning-free method for image morphing that\naccommodates inputs with different semantics or layouts. Unlike existing\nmethods that rely on finetuning pre-trained diffusion models and are limited by\ntime constraints and semantic/layout discrepancies, FreeMorph delivers\nhigh-fidelity image morphing without requiring per-instance training. Despite\ntheir efficiency and potential, tuning-free methods face challenges in\nmaintaining high-quality results due to the non-linear nature of the multi-step\ndenoising process and biases inherited from the pre-trained diffusion model. In\nthis paper, we introduce FreeMorph to address these challenges by integrating\ntwo key innovations. 1) We first propose a guidance-aware spherical\ninterpolation design that incorporates explicit guidance from the input images\nby modifying the self-attention modules, thereby addressing identity loss and\nensuring directional transitions throughout the generated sequence. 2) We\nfurther introduce a step-oriented variation trend that blends self-attention\nmodules derived from each input image to achieve controlled and consistent\ntransitions that respect both inputs. Our extensive evaluations demonstrate\nthat FreeMorph outperforms existing methods, being 10x ~ 50x faster and\nestablishing a new state-of-the-art for image morphing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01953.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63a07c3ab5515dccd40fdb71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a07c3ab5515dccd40fdb71/ly3pwhjWVge25LAeVgriV.png",
            "fullname": "Yukang Cao",
            "name": "yukangcao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.01957",
            "authors": [
                {
                    "_id": "686633d28c83dab5f72d1f39",
                    "name": "Zhuoyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "686633d28c83dab5f72d1f3a",
                    "name": "Luke J. Huang",
                    "hidden": false
                },
                {
                    "_id": "686633d28c83dab5f72d1f3b",
                    "user": {
                        "_id": "617526c9de8feb54b0ce45ad",
                        "avatarUrl": "/avatars/7faf8c6f71fc318a0113d780d376c381.svg",
                        "isPro": false,
                        "fullname": "Wu Chengyue",
                        "user": "WuChengyue",
                        "type": "user"
                    },
                    "name": "Chengyue Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T16:19:00.266Z",
                    "hidden": false
                },
                {
                    "_id": "686633d28c83dab5f72d1f3c",
                    "name": "Shang Yang",
                    "hidden": false
                },
                {
                    "_id": "686633d28c83dab5f72d1f3d",
                    "name": "Kelly Peng",
                    "hidden": false
                },
                {
                    "_id": "686633d28c83dab5f72d1f3e",
                    "name": "Yao Lu",
                    "hidden": false
                },
                {
                    "_id": "686633d28c83dab5f72d1f3f",
                    "name": "Song Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T17:59:23.000Z",
            "submittedOnDailyAt": "2025-07-03T06:13:07.255Z",
            "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation",
            "submittedOnDailyBy": {
                "_id": "650e6ab08f3228d807707735",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
                "isPro": false,
                "fullname": "Zhuoyang Zhang",
                "user": "zhuoyang20",
                "type": "user"
            },
            "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256times256 res.) and 1024 to 48 (512times512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4times lower latency than previous parallelized\nautoregressive models.",
            "upvotes": 12,
            "discussionId": "686633d28c83dab5f72d1f40",
            "githubRepo": "https://github.com/mit-han-lab/lpd",
            "githubStars": 22
        },
        "publishedAt": "2025-07-02T13:59:23.000Z",
        "title": "Locality-aware Parallel Decoding for Efficient Autoregressive Image\n  Generation",
        "summary": "We present Locality-aware Parallel Decoding (LPD) to accelerate\nautoregressive image generation. Traditional autoregressive image generation\nrelies on next-patch prediction, a memory-bound process that leads to high\nlatency. Existing works have tried to parallelize next-patch prediction by\nshifting to multi-patch prediction to accelerate the process, but only achieved\nlimited parallelization. To achieve high parallelization while maintaining\ngeneration quality, we introduce two key techniques: (1) Flexible Parallelized\nAutoregressive Modeling, a novel architecture that enables arbitrary generation\nordering and degrees of parallelization. It uses learnable position query\ntokens to guide generation at target positions while ensuring mutual visibility\namong concurrently generated tokens for consistent parallel decoding. (2)\nLocality-aware Generation Ordering, a novel schedule that forms groups to\nminimize intra-group dependencies and maximize contextual support, enhancing\ngeneration quality. With these designs, we reduce the generation steps from 256\nto 20 (256times256 res.) and 1024 to 48 (512times512 res.) without\ncompromising quality on the ImageNet class-conditional generation, and\nachieving at least 3.4times lower latency than previous parallelized\nautoregressive models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01957.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "650e6ab08f3228d807707735",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650e6ab08f3228d807707735/yFo6aLuyGH06t9yG8AOp7.png",
            "fullname": "Zhuoyang Zhang",
            "name": "zhuoyang20",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.01544",
            "authors": [
                {
                    "_id": "6865e0688c83dab5f72d1e33",
                    "name": "Benjamin Feuer",
                    "hidden": false
                },
                {
                    "_id": "6865e0688c83dab5f72d1e34",
                    "user": {
                        "_id": "677d00ac70b9142c01cc90f9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677d00ac70b9142c01cc90f9/c2utzkEFwdUHPLN8-_AHb.png",
                        "isPro": false,
                        "fullname": "Lennart Purucker",
                        "user": "LennartPurucker",
                        "type": "user"
                    },
                    "name": "Lennart Purucker",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T16:19:04.420Z",
                    "hidden": false
                },
                {
                    "_id": "6865e0688c83dab5f72d1e35",
                    "name": "Oussama Elachqar",
                    "hidden": false
                },
                {
                    "_id": "6865e0688c83dab5f72d1e36",
                    "name": "Chinmay Hegde",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62f7f4efe7c1c9bf10c81465/RW4Y5ygyoqxGZr23qImZ0.jpeg"
            ],
            "publishedAt": "2025-07-02T09:56:24.000Z",
            "submittedOnDailyAt": "2025-07-03T08:52:49.078Z",
            "title": "MARVIS: Modality Adaptive Reasoning over VISualizations",
            "submittedOnDailyBy": {
                "_id": "62f7f4efe7c1c9bf10c81465",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7f4efe7c1c9bf10c81465/AYlOg0fkP1o4GAP-8Y3xt.jpeg",
                "isPro": true,
                "fullname": "Benjamin Feuer",
                "user": "penfever",
                "type": "user"
            },
            "summary": "Scientific applications of machine learning often rely on small, specialized\nmodels tuned to particular domains. Such models often achieve excellent\nperformance, but lack flexibility. Foundation models offer versatility, but\ntypically underperform specialized approaches, especially on non-traditional\nmodalities and long-tail domains. We propose MARVIS (Modality Adaptive\nReasoning over VISualizations), a training-free method that enables even small\nvision-language models to predict any data modality with high accuracy. MARVIS\ntransforms latent embedding spaces into visual representations and then\nleverages the spatial and fine-grained reasoning skills of VLMs to successfully\ninterpret and utilize them. MARVIS achieves competitive performance on vision,\naudio, biological, and tabular domains using a single 3B parameter model,\nachieving results that beat Gemini by 16\\% on average and approach specialized\nmethods, without exposing personally identifiable information (P.I.I.) or\nrequiring any domain-specific training. We open source our code and datasets at\nhttps://github.com/penfever/marvis",
            "upvotes": 8,
            "discussionId": "6865e0698c83dab5f72d1e37",
            "githubRepo": "https://github.com/penfever/marvis",
            "githubStars": 3
        },
        "publishedAt": "2025-07-02T05:56:24.000Z",
        "title": "MARVIS: Modality Adaptive Reasoning over VISualizations",
        "summary": "Scientific applications of machine learning often rely on small, specialized\nmodels tuned to particular domains. Such models often achieve excellent\nperformance, but lack flexibility. Foundation models offer versatility, but\ntypically underperform specialized approaches, especially on non-traditional\nmodalities and long-tail domains. We propose MARVIS (Modality Adaptive\nReasoning over VISualizations), a training-free method that enables even small\nvision-language models to predict any data modality with high accuracy. MARVIS\ntransforms latent embedding spaces into visual representations and then\nleverages the spatial and fine-grained reasoning skills of VLMs to successfully\ninterpret and utilize them. MARVIS achieves competitive performance on vision,\naudio, biological, and tabular domains using a single 3B parameter model,\nachieving results that beat Gemini by 16\\% on average and approach specialized\nmethods, without exposing personally identifiable information (P.I.I.) or\nrequiring any domain-specific training. We open source our code and datasets at\nhttps://github.com/penfever/marvis",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62f7f4efe7c1c9bf10c81465/RW4Y5ygyoqxGZr23qImZ0.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01544.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62f7f4efe7c1c9bf10c81465",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f7f4efe7c1c9bf10c81465/AYlOg0fkP1o4GAP-8Y3xt.jpeg",
            "fullname": "Benjamin Feuer",
            "name": "penfever",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.00316",
            "authors": [
                {
                    "_id": "68653e95d706a6e48024af9b",
                    "user": {
                        "_id": "62d53e556dff888212d58724",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d53e556dff888212d58724/kJxZGBvR0iMnmGC890g-e.png",
                        "isPro": true,
                        "fullname": "Siyou Li",
                        "user": "SiyouLi",
                        "type": "user"
                    },
                    "name": "Siyou Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:32:44.870Z",
                    "hidden": false
                },
                {
                    "_id": "68653e95d706a6e48024af9c",
                    "name": "Pengyao Qin",
                    "hidden": false
                },
                {
                    "_id": "68653e95d706a6e48024af9d",
                    "name": "Huanan Wu",
                    "hidden": false
                },
                {
                    "_id": "68653e95d706a6e48024af9e",
                    "name": "Dong Nie",
                    "hidden": false
                },
                {
                    "_id": "68653e95d706a6e48024af9f",
                    "name": "Arun J. Thirunavukarasu",
                    "hidden": false
                },
                {
                    "_id": "68653e95d706a6e48024afa0",
                    "name": "Juntao Yu",
                    "hidden": false
                },
                {
                    "_id": "68653e95d706a6e48024afa1",
                    "user": {
                        "_id": "6866735fcb2dbd91988e8bed",
                        "avatarUrl": "/avatars/afa97d6e209d107ab01501fb4510ae08.svg",
                        "isPro": false,
                        "fullname": "Le Zhang",
                        "user": "thisislez",
                        "type": "user"
                    },
                    "name": "Le Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T16:19:10.289Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-30T23:14:49.000Z",
            "submittedOnDailyAt": "2025-07-03T18:30:48.514Z",
            "title": "μ^2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for\n  Radiology Report Generation",
            "submittedOnDailyBy": {
                "_id": "62d53e556dff888212d58724",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d53e556dff888212d58724/kJxZGBvR0iMnmGC890g-e.png",
                "isPro": true,
                "fullname": "Siyou Li",
                "user": "SiyouLi",
                "type": "user"
            },
            "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose mu^2LLM, a textbf{mu}ltiscale\ntextbf{mu}ltimodal large language models for RRG tasks. The\nnovel {mu}^2Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasetdemonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned mu^2LLMs on limited\ndata for RRG tasks.",
            "upvotes": 8,
            "discussionId": "68653e95d706a6e48024afa2",
            "projectPage": "https://u2tokenizer.github.io",
            "githubRepo": "https://github.com/Siyou-Li/u2Tokenizer",
            "githubStars": 145
        },
        "publishedAt": "2025-06-30T19:14:49.000Z",
        "title": "μ^2Tokenizer: Differentiable Multi-Scale Multi-Modal Tokenizer for\n  Radiology Report Generation",
        "summary": "Automated radiology report generation (RRG) aims to produce detailed textual\nreports from clinical imaging, such as computed tomography (CT) scans, to\nimprove the accuracy and efficiency of diagnosis and provision of management\nadvice. RRG is complicated by two key challenges: (1) inherent complexity in\nextracting relevant information from imaging data under resource constraints,\nand (2) difficulty in objectively evaluating discrepancies between\nmodel-generated and expert-written reports. To address these challenges, we\npropose mu^2LLM, a textbf{mu}ltiscale\ntextbf{mu}ltimodal large language models for RRG tasks. The\nnovel {mu}^2Tokenizer, as an intermediate layer, integrates multi-modal\nfeatures from the multiscale visual tokenizer and the text tokenizer, then\nenhances report generation quality through direct preference optimization\n(DPO), guided by GREEN-RedLlama. Experimental results on four large CT\nimage-report medical datasetdemonstrate that our method outperforms existing\napproaches, highlighting the potential of our fine-tuned mu^2LLMs on limited\ndata for RRG tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00316.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62d53e556dff888212d58724",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62d53e556dff888212d58724/kJxZGBvR0iMnmGC890g-e.png",
            "fullname": "Siyou Li",
            "name": "SiyouLi",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.22868",
            "authors": [
                {
                    "_id": "68637f0d588cea0da970c95e",
                    "user": {
                        "_id": "6719de3235b6494469ab69f6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
                        "isPro": false,
                        "fullname": "Junsung Lee",
                        "user": "jslee525",
                        "type": "user"
                    },
                    "name": "Junsung Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:32:55.680Z",
                    "hidden": false
                },
                {
                    "_id": "68637f0d588cea0da970c95f",
                    "name": "Junoh Kang",
                    "hidden": false
                },
                {
                    "_id": "68637f0d588cea0da970c960",
                    "name": "Bohyung Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-28T12:36:19.000Z",
            "submittedOnDailyAt": "2025-07-03T06:08:50.749Z",
            "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing",
            "submittedOnDailyBy": {
                "_id": "6719de3235b6494469ab69f6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
                "isPro": false,
                "fullname": "Junsung Lee",
                "user": "jslee525",
                "type": "user"
            },
            "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.",
            "upvotes": 5,
            "discussionId": "68637f0e588cea0da970c961",
            "projectPage": "https://jslee525.github.io/str-match",
            "githubRepo": "https://github.com/jslee525/STR-Match_official",
            "ai_summary": "STR-Match uses latent optimization and a novel STR score to produce spatiotemporally coherent and visually appealing edited videos by leveraging 2D spatial and 1D temporal attention in T2V diffusion models.",
            "ai_keywords": [
                "T2V diffusion models",
                "latent optimization",
                "spatiotemporal pixel relevance",
                "latent mask",
                "2D spatial attention",
                "1D temporal modules"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-06-28T08:36:19.000Z",
        "title": "STR-Match: Matching SpatioTemporal Relevance Score for Training-Free\n  Video Editing",
        "summary": "Previous text-guided video editing methods often suffer from temporal\ninconsistency, motion distortion, and-most notably-limited domain\ntransformation. We attribute these limitations to insufficient modeling of\nspatiotemporal pixel relevance during the editing process. To address this, we\npropose STR-Match, a training-free video editing algorithm that produces\nvisually appealing and spatiotemporally coherent videos through latent\noptimization guided by our novel STR score. The score captures spatiotemporal\npixel relevance across adjacent frames by leveraging 2D spatial attention and\n1D temporal modules in text-to-video (T2V) diffusion models, without the\noverhead of computationally expensive 3D attention mechanisms. Integrated into\na latent optimization framework with a latent mask, STR-Match generates\ntemporally consistent and visually faithful videos, maintaining strong\nperformance even under significant domain transformations while preserving key\nvisual attributes of the source. Extensive experiments demonstrate that\nSTR-Match consistently outperforms existing methods in both visual quality and\nspatiotemporal consistency.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.22868.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6719de3235b6494469ab69f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/67l_hybjE-E2nH8_EyBuO.png",
            "fullname": "Junsung Lee",
            "name": "jslee525",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2506.23552",
            "authors": [
                {
                    "_id": "6865e0148c83dab5f72d1e26",
                    "name": "Mingi Kwon",
                    "hidden": false
                },
                {
                    "_id": "6865e0148c83dab5f72d1e27",
                    "user": {
                        "_id": "631074d895c34b95407945f0",
                        "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
                        "isPro": false,
                        "fullname": "Joonghyuk Shin",
                        "user": "alex4727",
                        "type": "user"
                    },
                    "name": "Joonghyuk Shin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-03T07:16:56.605Z",
                    "hidden": false
                },
                {
                    "_id": "6865e0148c83dab5f72d1e28",
                    "name": "Jaeseok Jung",
                    "hidden": false
                },
                {
                    "_id": "6865e0148c83dab5f72d1e29",
                    "name": "Jaesik Park",
                    "hidden": false
                },
                {
                    "_id": "6865e0148c83dab5f72d1e2a",
                    "name": "Youngjung Uh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-30T06:51:40.000Z",
            "submittedOnDailyAt": "2025-07-03T00:15:16.476Z",
            "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
            "submittedOnDailyBy": {
                "_id": "631074d895c34b95407945f0",
                "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
                "isPro": false,
                "fullname": "Joonghyuk Shin",
                "user": "alex4727",
                "type": "user"
            },
            "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web",
            "upvotes": 4,
            "discussionId": "6865e0148c83dab5f72d1e2b"
        },
        "publishedAt": "2025-06-30T02:51:40.000Z",
        "title": "JAM-Flow: Joint Audio-Motion Synthesis with Flow Matching",
        "summary": "The intrinsic link between facial motion and speech is often overlooked in\ngenerative modeling, where talking head synthesis and text-to-speech (TTS) are\ntypically addressed as separate tasks. This paper introduces JAM-Flow, a\nunified framework to simultaneously synthesize and condition on both facial\nmotion and speech. Our approach leverages flow matching and a novel Multi-Modal\nDiffusion Transformer (MM-DiT) architecture, integrating specialized Motion-DiT\nand Audio-DiT modules. These are coupled via selective joint attention layers\nand incorporate key architectural choices, such as temporally aligned\npositional embeddings and localized joint attention masking, to enable\neffective cross-modal interaction while preserving modality-specific strengths.\nTrained with an inpainting-style objective, JAM-Flow supports a wide array of\nconditioning inputs-including text, reference audio, and reference\nmotion-facilitating tasks such as synchronized talking head generation from\ntext, audio-driven animation, and much more, within a single, coherent model.\nJAM-Flow significantly advances multi-modal generative modeling by providing a\npractical solution for holistic audio-visual synthesis. project page:\nhttps://joonghyuk.com/jamflow-web",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.23552.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "631074d895c34b95407945f0",
            "avatarUrl": "/avatars/699baf06ec818650dec5752aca87c5b4.svg",
            "fullname": "Joonghyuk Shin",
            "name": "alex4727",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.00472",
            "authors": [
                {
                    "_id": "68669614c415dc4bb9b2fd81",
                    "name": "Ying Guo",
                    "hidden": false
                },
                {
                    "_id": "68669614c415dc4bb9b2fd82",
                    "name": "Xi Liu",
                    "hidden": false
                },
                {
                    "_id": "68669614c415dc4bb9b2fd83",
                    "name": "Cheng Zhen",
                    "hidden": false
                },
                {
                    "_id": "68669614c415dc4bb9b2fd84",
                    "name": "Pengfei Yan",
                    "hidden": false
                },
                {
                    "_id": "68669614c415dc4bb9b2fd85",
                    "name": "Xiaoming Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-01T06:38:14.000Z",
            "submittedOnDailyAt": "2025-07-03T13:10:10.687Z",
            "title": "ARIG: Autoregressive Interactive Head Generation for Real-time\n  Conversations",
            "submittedOnDailyBy": {
                "_id": "624bebf604abc7ebb01789af",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg",
                "isPro": true,
                "fullname": "Apolinário from multimodal AI art",
                "user": "multimodalart",
                "type": "user"
            },
            "summary": "Face-to-face communication, as a common human activity, motivates the\nresearch on interactive head generation. A virtual agent can generate motion\nresponses with both listening and speaking capabilities based on the audio or\nmotion signals of the other user and itself. However, previous clip-wise\ngeneration paradigm or explicit listener/speaker generator-switching methods\nhave limitations in future signal acquisition, contextual behavioral\nunderstanding, and switching smoothness, making it challenging to be real-time\nand realistic. In this paper, we propose an autoregressive (AR) based\nframe-wise framework called ARIG to realize the real-time generation with\nbetter interaction realism. To achieve real-time generation, we model motion\nprediction as a non-vector-quantized AR process. Unlike discrete codebook-index\nprediction, we represent motion distribution using diffusion procedure,\nachieving more accurate predictions in continuous space. To improve interaction\nrealism, we emphasize interactive behavior understanding (IBU) and detailed\nconversational state understanding (CSU). In IBU, based on dual-track\ndual-modal signals, we summarize short-range behaviors through\nbidirectional-integrated learning and perform contextual understanding over\nlong ranges. In CSU, we use voice activity signals and context features of IBU\nto understand the various states (interruption, feedback, pause, etc.) that\nexist in actual conversations. These serve as conditions for the final\nprogressive motion prediction. Extensive experiments have verified the\neffectiveness of our model.",
            "upvotes": 3,
            "discussionId": "68669614c415dc4bb9b2fd86",
            "projectPage": "https://jinyugy21.github.io/ARIG/"
        },
        "publishedAt": "2025-07-01T02:38:14.000Z",
        "title": "ARIG: Autoregressive Interactive Head Generation for Real-time\n  Conversations",
        "summary": "Face-to-face communication, as a common human activity, motivates the\nresearch on interactive head generation. A virtual agent can generate motion\nresponses with both listening and speaking capabilities based on the audio or\nmotion signals of the other user and itself. However, previous clip-wise\ngeneration paradigm or explicit listener/speaker generator-switching methods\nhave limitations in future signal acquisition, contextual behavioral\nunderstanding, and switching smoothness, making it challenging to be real-time\nand realistic. In this paper, we propose an autoregressive (AR) based\nframe-wise framework called ARIG to realize the real-time generation with\nbetter interaction realism. To achieve real-time generation, we model motion\nprediction as a non-vector-quantized AR process. Unlike discrete codebook-index\nprediction, we represent motion distribution using diffusion procedure,\nachieving more accurate predictions in continuous space. To improve interaction\nrealism, we emphasize interactive behavior understanding (IBU) and detailed\nconversational state understanding (CSU). In IBU, based on dual-track\ndual-modal signals, we summarize short-range behaviors through\nbidirectional-integrated learning and perform contextual understanding over\nlong ranges. In CSU, we use voice activity signals and context features of IBU\nto understand the various states (interruption, feedback, pause, etc.) that\nexist in actual conversations. These serve as conditions for the final\nprogressive motion prediction. Extensive experiments have verified the\neffectiveness of our model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.00472.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "624bebf604abc7ebb01789af",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649143001781-624bebf604abc7ebb01789af.jpeg",
            "fullname": "Apolinário from multimodal AI art",
            "name": "multimodalart",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4522
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.02856",
            "authors": [
                {
                    "_id": "68672b7f9db35afc9c304c95",
                    "name": "Nikhil Chandak",
                    "hidden": false
                },
                {
                    "_id": "68672b7f9db35afc9c304c96",
                    "name": "Shashwat Goel",
                    "hidden": false
                },
                {
                    "_id": "68672b7f9db35afc9c304c97",
                    "name": "Ameya Prabhu",
                    "hidden": false
                },
                {
                    "_id": "68672b7f9db35afc9c304c98",
                    "name": "Moritz Hardt",
                    "hidden": false
                },
                {
                    "_id": "68672b7f9db35afc9c304c99",
                    "name": "Jonas Geiping",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-03T17:59:02.000Z",
            "submittedOnDailyAt": "2025-07-03T23:47:39.480Z",
            "title": "Answer Matching Outperforms Multiple Choice for Language Model\n  Evaluation",
            "submittedOnDailyBy": {
                "_id": "6506832221ac448013f94995",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
                "isPro": false,
                "fullname": "Shashwat Goel",
                "user": "shash42",
                "type": "user"
            },
            "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.",
            "upvotes": 0,
            "discussionId": "68672b809db35afc9c304c9a"
        },
        "publishedAt": "2025-07-03T13:59:02.000Z",
        "title": "Answer Matching Outperforms Multiple Choice for Language Model\n  Evaluation",
        "summary": "Multiple choice benchmarks have long been the workhorse of language model\nevaluation because grading multiple choice is objective and easy to automate.\nHowever, we show multiple choice questions from popular benchmarks can often be\nanswered without even seeing the question. These shortcuts arise from a\nfundamental limitation of discriminative evaluation not shared by evaluations\nof the model's free-form, generative answers. Until recently, there appeared to\nbe no viable, scalable alternative to multiple choice--but, we show that this\nhas changed. We consider generative evaluation via what we call answer\nmatching: Give the candidate model the question without the options, have it\ngenerate a free-form response, then use a modern language model with the\nreference answer to determine if the response matches the reference. To compare\nthe validity of different evaluation strategies, we annotate MMLU-Pro and\nGPQA-Diamond to obtain human grading data, and measure the agreement of each\nevaluation approach. We find answer matching using recent models--even small\nones--achieves near-perfect agreement, in the range of inter-annotator\nagreement. In contrast, both multiple choice evaluation and using\nLLM-as-a-judge without reference answers aligns poorly with human grading.\nImproving evaluations via answer matching is not merely a conceptual concern:\nthe rankings of several models change significantly when evaluating their\nfree-form responses with answer matching. In light of these findings, we\ndiscuss how to move the evaluation ecosystem from multiple choice to answer\nmatching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02856.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6506832221ac448013f94995",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
            "fullname": "Shashwat Goel",
            "name": "shash42",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]