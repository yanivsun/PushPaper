[
    {
        "paper": {
            "id": "2507.05566",
            "authors": [
                {
                    "_id": "686e0a5dcb5725779c60b4e6",
                    "name": "David Bensaïd",
                    "hidden": false
                },
                {
                    "_id": "686e0a5dcb5725779c60b4e7",
                    "user": {
                        "_id": "62b3e85bcbd2a402fc7804b1",
                        "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
                        "isPro": false,
                        "fullname": "noam rotstein",
                        "user": "noamrot",
                        "type": "user"
                    },
                    "name": "Noam Rotstein",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:36.311Z",
                    "hidden": false
                },
                {
                    "_id": "686e0a5dcb5725779c60b4e8",
                    "user": {
                        "_id": "63ecaf7460ff4b318ad03ebb",
                        "avatarUrl": "/avatars/7a5bf1854f1eae9cc5fd8392a3f9fba3.svg",
                        "isPro": false,
                        "fullname": "Roy Velich",
                        "user": "royve",
                        "type": "user"
                    },
                    "name": "Roy Velich",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:33.313Z",
                    "hidden": false
                },
                {
                    "_id": "686e0a5dcb5725779c60b4e9",
                    "name": "Daniel Bensaïd",
                    "hidden": false
                },
                {
                    "_id": "686e0a5dcb5725779c60b4ea",
                    "name": "Ron Kimmel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T01:11:30.000Z",
            "submittedOnDailyAt": "2025-07-09T04:55:03.373Z",
            "title": "SingLoRA: Low Rank Adaptation Using a Single Matrix",
            "submittedOnDailyBy": {
                "_id": "62b3e85bcbd2a402fc7804b1",
                "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
                "isPro": false,
                "fullname": "noam rotstein",
                "user": "noamrot",
                "type": "user"
            },
            "summary": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient\nfine-tuning of large pretrained models. LoRA augments the pre-trained weights\nof a model by adding the product of two smaller matrices that together form a\nlow-rank matrix update. Recent research has shown that scale disparities\nbetween these two matrices often cause unstable training dynamics, leading to\nsuboptimal performance. In this paper, we propose SingLoRA, which reformulates\nlow-rank adaptation by learning the weights update as a decomposition of a\nsingle low-rank matrix multiplied by its transpose. This simple design\ninherently removes inter-matrix scale conflicts, ensuring stable optimization,\nand roughly halves the parameter count. We analyze SingLoRA within the\ninfinite-width neural network framework, showing that it guarantees stable\nfeature learning by construction. Extensive experiments on multiple tasks\nvalidate these benefits. In common sense reasoning, fine-tuning LLama 7B on\nMNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+\n(90.2%) - while using only 60% of their parameter budget. In image generation,\nfine-tuning Stable Diffusion with SingLoRA significantly improves image\nfidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to\nscores of 0.148 and 0.143 for DoRA and LoRA, respectively.",
            "upvotes": 65,
            "discussionId": "686e0a5dcb5725779c60b4eb",
            "ai_summary": "SingLoRA, a reformulated low-rank adaptation method, enhances parameter-efficient fine-tuning by learning a single low-rank matrix update, ensuring stable optimization and reduced parameter count.",
            "ai_keywords": [
                "Low-Rank Adaptation",
                "LoRA",
                "SingLoRA",
                "low-rank matrix",
                "infinite-width neural network",
                "feature learning",
                "common sense reasoning",
                "fine-tuning",
                "LLama 7B",
                "MNLI",
                "image generation",
                "Stable Diffusion",
                "DreamBooth",
                "DINO similarity score",
                "DoRA"
            ]
        },
        "publishedAt": "2025-07-07T21:11:30.000Z",
        "title": "SingLoRA: Low Rank Adaptation Using a Single Matrix",
        "summary": "Low-Rank Adaptation (LoRA) has significantly advanced parameter-efficient\nfine-tuning of large pretrained models. LoRA augments the pre-trained weights\nof a model by adding the product of two smaller matrices that together form a\nlow-rank matrix update. Recent research has shown that scale disparities\nbetween these two matrices often cause unstable training dynamics, leading to\nsuboptimal performance. In this paper, we propose SingLoRA, which reformulates\nlow-rank adaptation by learning the weights update as a decomposition of a\nsingle low-rank matrix multiplied by its transpose. This simple design\ninherently removes inter-matrix scale conflicts, ensuring stable optimization,\nand roughly halves the parameter count. We analyze SingLoRA within the\ninfinite-width neural network framework, showing that it guarantees stable\nfeature learning by construction. Extensive experiments on multiple tasks\nvalidate these benefits. In common sense reasoning, fine-tuning LLama 7B on\nMNLI with SingLoRA achieves 91.3% accuracy - surpassing LoRA (89.1%) and LoRA+\n(90.2%) - while using only 60% of their parameter budget. In image generation,\nfine-tuning Stable Diffusion with SingLoRA significantly improves image\nfidelity on DreamBooth, achieving a DINO similarity score of 0.151, compared to\nscores of 0.148 and 0.143 for DoRA and LoRA, respectively.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05566.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62b3e85bcbd2a402fc7804b1",
            "avatarUrl": "/avatars/63125ce8a1e20b8c6e836f223d24284f.svg",
            "fullname": "noam rotstein",
            "name": "noamrot",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06203",
            "authors": [
                {
                    "_id": "686ddd7fcb5725779c60b444",
                    "user": {
                        "_id": "63ff09f24852102d4871c19c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
                        "isPro": false,
                        "fullname": "Rui-Jie Zhu",
                        "user": "ridger",
                        "type": "user"
                    },
                    "name": "Rui-Jie Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:55.890Z",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b445",
                    "name": "Tianhao Peng",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b446",
                    "name": "Tianhao Cheng",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b447",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b448",
                    "user": {
                        "_id": "63f37af60be81bdc5d92eebb",
                        "avatarUrl": "/avatars/b8dfdff4ab36988ec9a8643e82a3d2db.svg",
                        "isPro": false,
                        "fullname": "Huang",
                        "user": "Jinfa",
                        "type": "user"
                    },
                    "name": "Jinfa Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:54.002Z",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b449",
                    "name": "Dawei Zhu",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b44a",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b44b",
                    "name": "Kaiwen Xue",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b44c",
                    "name": "Xuanliang Zhang",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b44d",
                    "name": "Yong Shan",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b44e",
                    "name": "Tianle Cai",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b44f",
                    "name": "Taylor Kergan",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b450",
                    "name": "Assel Kembay",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b451",
                    "name": "Andrew Smith",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b452",
                    "name": "Chenghua Lin",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b453",
                    "name": "Binh Nguyen",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b454",
                    "name": "Yuqi Pan",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b455",
                    "name": "Yuhong Chou",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b456",
                    "name": "Zefan Cai",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b457",
                    "name": "Zhenhe Wu",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b458",
                    "name": "Yongchi Zhao",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b459",
                    "name": "Tianyu Liu",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b45a",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b45b",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b45c",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:58.017Z",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b45d",
                    "name": "Chongxuan Li",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b45e",
                    "name": "Yuyin Zhou",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b45f",
                    "name": "Zhoujun Li",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b460",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b461",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b462",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b463",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "686ddd7fcb5725779c60b464",
                    "user": {
                        "_id": "63047063bad6ce7fc02438c1",
                        "avatarUrl": "/avatars/8729cccbb15da682458d323eb8dc528b.svg",
                        "isPro": false,
                        "fullname": "Jason",
                        "user": "jeshragh",
                        "type": "user"
                    },
                    "name": "Jason Eshraghian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:47.907Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T17:29:07.000Z",
            "submittedOnDailyAt": "2025-07-09T01:45:08.087Z",
            "title": "A Survey on Latent Reasoning",
            "submittedOnDailyBy": {
                "_id": "63ff09f24852102d4871c19c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
                "isPro": false,
                "fullname": "Rui-Jie Zhu",
                "user": "ridger",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.",
            "upvotes": 58,
            "discussionId": "686ddd7fcb5725779c60b465",
            "githubRepo": "https://github.com/multimodal-art-projection/LatentCoT-Horizon/",
            "ai_summary": "Latent reasoning enhances large language models by performing multi-step inference in continuous hidden states, improving efficiency and expressiveness beyond token-level supervision.",
            "ai_keywords": [
                "chain-of-thought reasoning",
                "latent reasoning",
                "neural network layers",
                "hierarchical representations",
                "activation-based recurrence",
                "hidden state propagation",
                "fine-tuning strategies",
                "infinite-depth latent reasoning",
                "masked diffusion models",
                "globally consistent reasoning",
                "reversible reasoning processes"
            ],
            "githubStars": 52
        },
        "publishedAt": "2025-07-08T13:29:07.000Z",
        "title": "A Survey on Latent Reasoning",
        "summary": "Large Language Models (LLMs) have demonstrated impressive reasoning\ncapabilities, especially when guided by explicit chain-of-thought (CoT)\nreasoning that verbalizes intermediate steps. While CoT improves both\ninterpretability and accuracy, its dependence on natural language reasoning\nlimits the model's expressive bandwidth. Latent reasoning tackles this\nbottleneck by performing multi-step inference entirely in the model's\ncontinuous hidden state, eliminating token-level supervision. To advance latent\nreasoning research, this survey provides a comprehensive overview of the\nemerging field of latent reasoning. We begin by examining the foundational role\nof neural network layers as the computational substrate for reasoning,\nhighlighting how hierarchical representations support complex transformations.\nNext, we explore diverse latent reasoning methodologies, including\nactivation-based recurrence, hidden state propagation, and fine-tuning\nstrategies that compress or internalize explicit reasoning traces. Finally, we\ndiscuss advanced paradigms such as infinite-depth latent reasoning via masked\ndiffusion models, which enable globally consistent and reversible reasoning\nprocesses. By unifying these perspectives, we aim to clarify the conceptual\nlandscape of latent reasoning and chart future directions for research at the\nfrontier of LLM cognition. An associated GitHub repository collecting the\nlatest papers and repos is available at:\nhttps://github.com/multimodal-art-projection/LatentCoT-Horizon/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06203.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63ff09f24852102d4871c19c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
            "fullname": "Rui-Jie Zhu",
            "name": "ridger",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06229",
            "authors": [
                {
                    "_id": "686dc18acb5725779c60b326",
                    "user": {
                        "_id": "63357c608adfa81faf2ac180",
                        "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
                        "isPro": false,
                        "fullname": "Xiangru Tang",
                        "user": "RTT1",
                        "type": "user"
                    },
                    "name": "Xiangru Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:46.438Z",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b327",
                    "name": "Tianrui Qin",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b328",
                    "name": "Tianhao Peng",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b329",
                    "user": {
                        "_id": "67930201aad25d3eecab81cc",
                        "avatarUrl": "/avatars/afa8e19ccd5214979e405caf462d7a72.svg",
                        "isPro": false,
                        "fullname": "ZiyangZhou",
                        "user": "AzHouangy",
                        "type": "user"
                    },
                    "name": "Ziyang Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:48.624Z",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b32a",
                    "name": "Daniel Shao",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b32b",
                    "name": "Tingting Du",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b32c",
                    "name": "Xinming Wei",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b32d",
                    "user": {
                        "_id": "643e9ee6f6bb3c31a26e7bc4",
                        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                        "isPro": false,
                        "fullname": "Peng Xia",
                        "user": "richardxp888",
                        "type": "user"
                    },
                    "name": "Peng Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:44.151Z",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b32e",
                    "name": "Fang Wu",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b32f",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b330",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:41.208Z",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b331",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b332",
                    "name": "Xingyao Wang",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b333",
                    "name": "Sirui Hong",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b334",
                    "name": "Chenglin Wu",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b335",
                    "name": "Hao Cheng",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b336",
                    "name": "Chi Wang",
                    "hidden": false
                },
                {
                    "_id": "686dc18acb5725779c60b337",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T17:59:22.000Z",
            "submittedOnDailyAt": "2025-07-09T17:50:12.079Z",
            "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving",
            "submittedOnDailyBy": {
                "_id": "63357c608adfa81faf2ac180",
                "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
                "isPro": false,
                "fullname": "Xiangru Tang",
                "user": "RTT1",
                "type": "user"
            },
            "summary": "As language agents tackle increasingly complex tasks, they struggle with\neffective error correction and experience reuse across domains. We introduce\nAgent KB, a hierarchical experience framework that enables complex agentic\nproblem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses\na core limitation: agents traditionally cannot learn from each other's\nexperiences. By capturing both high-level strategies and detailed execution\nlogs, Agent KB creates a shared knowledge base that enables cross-agent\nknowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success\nrates by up to 16.28 percentage points. On the most challenging tasks, Claude-3\nimproves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on\nintermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to\nimprove from 41.33% to 53.33%. Our results suggest that Agent KB provides a\nmodular, framework-agnostic infrastructure for enabling agents to learn from\npast experiences and generalize successful strategies to new tasks.",
            "upvotes": 56,
            "discussionId": "686dc18acb5725779c60b338",
            "githubRepo": "https://github.com/OPPO-PersonalAI/Agent-KB",
            "githubStars": 36
        },
        "publishedAt": "2025-07-08T13:59:22.000Z",
        "title": "Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving",
        "summary": "As language agents tackle increasingly complex tasks, they struggle with\neffective error correction and experience reuse across domains. We introduce\nAgent KB, a hierarchical experience framework that enables complex agentic\nproblem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses\na core limitation: agents traditionally cannot learn from each other's\nexperiences. By capturing both high-level strategies and detailed execution\nlogs, Agent KB creates a shared knowledge base that enables cross-agent\nknowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success\nrates by up to 16.28 percentage points. On the most challenging tasks, Claude-3\nimproves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on\nintermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to\nimprove from 41.33% to 53.33%. Our results suggest that Agent KB provides a\nmodular, framework-agnostic infrastructure for enabling agents to learn from\npast experiences and generalize successful strategies to new tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06229.png",
        "numComments": 0,
        "submittedBy": {
            "_id": "63357c608adfa81faf2ac180",
            "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
            "fullname": "Xiangru Tang",
            "name": "RTT1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06165",
            "authors": [
                {
                    "_id": "686ddd5ccb5725779c60b430",
                    "name": "Yunhan Yang",
                    "hidden": false
                },
                {
                    "_id": "686ddd5ccb5725779c60b431",
                    "name": "Yufan Zhou",
                    "hidden": false
                },
                {
                    "_id": "686ddd5ccb5725779c60b432",
                    "name": "Yuan-Chen Guo",
                    "hidden": false
                },
                {
                    "_id": "686ddd5ccb5725779c60b433",
                    "name": "Zi-Xin Zou",
                    "hidden": false
                },
                {
                    "_id": "686ddd5ccb5725779c60b434",
                    "name": "Yukun Huang",
                    "hidden": false
                },
                {
                    "_id": "686ddd5ccb5725779c60b435",
                    "name": "Ying-Tian Liu",
                    "hidden": false
                },
                {
                    "_id": "686ddd5ccb5725779c60b436",
                    "name": "Hao Xu",
                    "hidden": false
                },
                {
                    "_id": "686ddd5ccb5725779c60b437",
                    "name": "Ding Liang",
                    "hidden": false
                },
                {
                    "_id": "686ddd5ccb5725779c60b438",
                    "name": "Yan-Pei Cao",
                    "hidden": false
                },
                {
                    "_id": "686ddd5ccb5725779c60b439",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6427e08288215cee63b1c44d/Y9wAFEXtYiDmbh7rRZDoz.mp4"
            ],
            "publishedAt": "2025-07-08T16:46:15.000Z",
            "submittedOnDailyAt": "2025-07-09T02:05:05.139Z",
            "title": "OmniPart: Part-Aware 3D Generation with Semantic Decoupling and\n  Structural Cohesion",
            "submittedOnDailyBy": {
                "_id": "6427e08288215cee63b1c44d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
                "isPro": false,
                "fullname": "yao teng",
                "user": "tytyt",
                "type": "user"
            },
            "summary": "The creation of 3D assets with explicit, editable part structures is crucial\nfor advancing interactive applications, yet most generative methods produce\nonly monolithic shapes, limiting their utility. We introduce OmniPart, a novel\nframework for part-aware 3D object generation designed to achieve high semantic\ndecoupling among components while maintaining robust structural cohesion.\nOmniPart uniquely decouples this complex task into two synergistic stages: (1)\nan autoregressive structure planning module generates a controllable,\nvariable-length sequence of 3D part bounding boxes, critically guided by\nflexible 2D part masks that allow for intuitive control over part decomposition\nwithout requiring direct correspondences or semantic labels; and (2) a\nspatially-conditioned rectified flow model, efficiently adapted from a\npre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and\nconsistently within the planned layout. Our approach supports user-defined part\ngranularity, precise localization, and enables diverse downstream applications.\nExtensive experiments demonstrate that OmniPart achieves state-of-the-art\nperformance, paving the way for more interpretable, editable, and versatile 3D\ncontent.",
            "upvotes": 44,
            "discussionId": "686ddd5ccb5725779c60b43a",
            "ai_summary": "OmniPart generates part-aware 3D objects with high semantic decoupling and robust structural cohesion using an autoregressive structure planning module and a spatially-conditioned rectified flow model.",
            "ai_keywords": [
                "autoregressive structure planning module",
                "3D part bounding boxes",
                "2D part masks",
                "spatially-conditioned rectified flow model",
                "holistic 3D generator"
            ]
        },
        "publishedAt": "2025-07-08T12:46:15.000Z",
        "title": "OmniPart: Part-Aware 3D Generation with Semantic Decoupling and\n  Structural Cohesion",
        "summary": "The creation of 3D assets with explicit, editable part structures is crucial\nfor advancing interactive applications, yet most generative methods produce\nonly monolithic shapes, limiting their utility. We introduce OmniPart, a novel\nframework for part-aware 3D object generation designed to achieve high semantic\ndecoupling among components while maintaining robust structural cohesion.\nOmniPart uniquely decouples this complex task into two synergistic stages: (1)\nan autoregressive structure planning module generates a controllable,\nvariable-length sequence of 3D part bounding boxes, critically guided by\nflexible 2D part masks that allow for intuitive control over part decomposition\nwithout requiring direct correspondences or semantic labels; and (2) a\nspatially-conditioned rectified flow model, efficiently adapted from a\npre-trained holistic 3D generator, synthesizes all 3D parts simultaneously and\nconsistently within the planned layout. Our approach supports user-defined part\ngranularity, precise localization, and enables diverse downstream applications.\nExtensive experiments demonstrate that OmniPart achieves state-of-the-art\nperformance, paving the way for more interpretable, editable, and versatile 3D\ncontent.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6427e08288215cee63b1c44d/Y9wAFEXtYiDmbh7rRZDoz.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06165.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6427e08288215cee63b1c44d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6427e08288215cee63b1c44d/rzaG978FF-ywzicWNl_xl.jpeg",
            "fullname": "yao teng",
            "name": "tytyt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.04103",
            "authors": [
                {
                    "_id": "686dbfe2cb5725779c60b314",
                    "user": {
                        "_id": "65fb7b1c876d4642e8340e95",
                        "avatarUrl": "/avatars/c258c4528315243b98c1e60ba398e98b.svg",
                        "isPro": false,
                        "fullname": "Dheeraj Vattikonda",
                        "user": "Dheeraj46329",
                        "type": "user"
                    },
                    "name": "Dheeraj Vattikonda",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T15:10:15.465Z",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b315",
                    "name": "Santhoshi Ravichandran",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b316",
                    "user": {
                        "_id": "66c4d0fd18743b108e4e260e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66c4d0fd18743b108e4e260e/xKLb9t6VZCxt464VCkQ0O.png",
                        "isPro": false,
                        "fullname": "Emiliano Penaloza",
                        "user": "ppEmiliano",
                        "type": "user"
                    },
                    "name": "Emiliano Penaloza",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:41:04.559Z",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b317",
                    "name": "Hadi Nekoei",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b318",
                    "name": "Megh Thakkar",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b319",
                    "name": "Thibault Le Sellier de Chezelles",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b31a",
                    "name": "Nicolas Gontier",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b31b",
                    "name": "Miguel Muñoz-Mármol",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b31c",
                    "name": "Sahar Omidi Shayegan",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b31d",
                    "name": "Stefania Raimondo",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b31e",
                    "name": "Xue Liu",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b31f",
                    "name": "Alexandre Drouin",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b320",
                    "name": "Laurent Charlin",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b321",
                    "name": "Alexandre Piché",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b322",
                    "name": "Alexandre Lacoste",
                    "hidden": false
                },
                {
                    "_id": "686dbfe2cb5725779c60b323",
                    "name": "Massimo Caccia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-05T17:12:33.000Z",
            "submittedOnDailyAt": "2025-07-09T02:00:38.643Z",
            "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
            "submittedOnDailyBy": {
                "_id": "5fa9ff3ea13e063b8b2b60cb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
                "isPro": false,
                "fullname": "Xing Han Lù",
                "user": "xhluca",
                "type": "user"
            },
            "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.",
            "upvotes": 36,
            "discussionId": "686dbfe2cb5725779c60b324",
            "ai_summary": "A study on compute allocation for post-training LLM-based web agents finds that combining supervised fine-tuning with on-policy reinforcement learning improves performance and reduces computational costs compared to either method alone.",
            "ai_keywords": [
                "LLM-based web agents",
                "supervised fine-tuning",
                "on-policy reinforcement learning",
                "hyperparameter choices",
                "bootstrapping",
                "WorkArena",
                "MiniWob++"
            ]
        },
        "publishedAt": "2025-07-05T13:12:33.000Z",
        "title": "How to Train Your LLM Web Agent: A Statistical Diagnosis",
        "summary": "LLM-based web agents have recently made significant progress, but much of it\nhas occurred in closed-source systems, widening the gap with open-source\nalternatives. Progress has been held back by two key challenges: first, a\nnarrow focus on single-step tasks that overlooks the complexity of multi-step\nweb interactions; and second, the high compute costs required to post-train\nLLM-based web agents. To address this, we present the first statistically\ngrounded study on compute allocation for LLM web-agent post-training. Our\napproach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate\na Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy\nreinforcement learning. We find this process highly sensitive to hyperparameter\nchoices, making exhaustive sweeps impractical. To spare others from expensive\ntrial-and-error, we sample 1,370 configurations and use bootstrapping to\nestimate effective hyperparameters. Our results show that combining SFT with\non-policy RL consistently outperforms either approach alone on both WorkArena\nand MiniWob++. Further, this strategy requires only 55% of the compute to match\nthe peak performance of pure SFT on MiniWob++, effectively pushing the\ncompute-performance Pareto frontier, and is the only strategy that can close\nthe gap with closed-source models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04103.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5fa9ff3ea13e063b8b2b60cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1633380224986-5fa9ff3ea13e063b8b2b60cb.jpeg",
            "fullname": "Xing Han Lù",
            "name": "xhluca",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06181",
            "authors": [
                {
                    "_id": "686dcc36cb5725779c60b393",
                    "user": {
                        "_id": "63299f93688ad82b783aaf20",
                        "avatarUrl": "/avatars/7c11e60e551ef1c62aa2862529e357f5.svg",
                        "isPro": false,
                        "fullname": "zhongyuan peng",
                        "user": "happzy2633",
                        "type": "user"
                    },
                    "name": "Zhongyuan Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:41:00.582Z",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b394",
                    "name": "Yifan Yao",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b395",
                    "name": "Kaijing Ma",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b396",
                    "name": "Shuyue Guo",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b397",
                    "name": "Yizhe Li",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b398",
                    "name": "Yichi Zhang",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b399",
                    "name": "Chenchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b39a",
                    "user": {
                        "_id": "647bf082aba7062fe5c51ca9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhang",
                        "user": "yifAI",
                        "type": "user"
                    },
                    "name": "Yifan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:21.463Z",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b39b",
                    "user": {
                        "_id": "62a80fe3ac97233f1625235a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62a80fe3ac97233f1625235a/_rGtpqdY7OEBz3pyqb6fE.jpeg",
                        "isPro": false,
                        "fullname": "Zhouliang Yu",
                        "user": "zhouliang",
                        "type": "user"
                    },
                    "name": "Zhouliang Yu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:19.408Z",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b39c",
                    "name": "Luming Li",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b39d",
                    "name": "Minghao Liu",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b39e",
                    "name": "Yihang Xia",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b39f",
                    "name": "Jiawei Shen",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b3a0",
                    "name": "Yuchen Wu",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b3a1",
                    "name": "Yixin Cao",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b3a2",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b3a3",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b3a4",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "686dcc36cb5725779c60b3a5",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:23.532Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T17:03:39.000Z",
            "submittedOnDailyAt": "2025-07-09T00:45:12.634Z",
            "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical\n  Formalization",
            "submittedOnDailyBy": {
                "_id": "63299f93688ad82b783aaf20",
                "avatarUrl": "/avatars/7c11e60e551ef1c62aa2862529e357f5.svg",
                "isPro": false,
                "fullname": "zhongyuan peng",
                "user": "happzy2633",
                "type": "user"
            },
            "summary": "Translating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to the critic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduce CriticLean, a novel critic-guided reinforcement learning framework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose the CriticLeanGPT, trained\nvia supervised fine-tuning and reinforcement learning, to rigorously assess the\nsemantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trained\nCriticLeanGPT models can significantly outperform strong open- and\nclosed-source baselines. Building on the CriticLean framework, we construct\nFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is essential for producing reliable formalizations, and we hope our\nCriticLean will provide valuable insights for future advances in formal\nmathematical reasoning.",
            "upvotes": 35,
            "discussionId": "686dcc36cb5725779c60b3a6",
            "githubRepo": "https://github.com/multimodal-art-projection/CriticLean",
            "ai_summary": "CriticLean, a reinforcement learning framework with CriticLeanGPT and CriticLeanBench, enhances semantic evaluation in automated theorem proving by actively learning to distinguish correct from incorrect formalizations.",
            "ai_keywords": [
                "critic phase",
                "reinforcement learning",
                "CriticLeanGPT",
                "supervised fine-tuning",
                "semantic fidelity",
                "Lean 4 formalizations",
                "CriticLeanBench",
                "FineLeanCorpus",
                "formal mathematical reasoning"
            ],
            "githubStars": 17
        },
        "publishedAt": "2025-07-08T13:03:39.000Z",
        "title": "CriticLean: Critic-Guided Reinforcement Learning for Mathematical\n  Formalization",
        "summary": "Translating natural language mathematical statements into formal, executable\ncode is a fundamental challenge in automated theorem proving. While prior work\nhas focused on generation and compilation success, little attention has been\npaid to the critic phase-the evaluation of whether generated formalizations\ntruly capture the semantic intent of the original problem. In this paper, we\nintroduce CriticLean, a novel critic-guided reinforcement learning framework\nthat elevates the role of the critic from a passive validator to an active\nlearning component. Specifically, first, we propose the CriticLeanGPT, trained\nvia supervised fine-tuning and reinforcement learning, to rigorously assess the\nsemantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench,\na benchmark designed to measure models' ability to distinguish semantically\ncorrect from incorrect formalizations, and demonstrate that our trained\nCriticLeanGPT models can significantly outperform strong open- and\nclosed-source baselines. Building on the CriticLean framework, we construct\nFineLeanCorpus, a dataset comprising over 285K problems that exhibits rich\ndomain diversity, broad difficulty coverage, and high correctness based on\nhuman evaluation. Overall, our findings highlight that optimizing the critic\nphase is essential for producing reliable formalizations, and we hope our\nCriticLean will provide valuable insights for future advances in formal\nmathematical reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06181.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63299f93688ad82b783aaf20",
            "avatarUrl": "/avatars/7c11e60e551ef1c62aa2862529e357f5.svg",
            "fullname": "zhongyuan peng",
            "name": "happzy2633",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.05240",
            "authors": [
                {
                    "_id": "686de9d4cb5725779c60b487",
                    "name": "Meng Wei",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b488",
                    "user": {
                        "_id": "66bb5e6573ce3e3ef046615a",
                        "avatarUrl": "/avatars/cbfb4b4114dc3afd0eb63b43a809ba09.svg",
                        "isPro": false,
                        "fullname": "Chenyang Wan",
                        "user": "cywan",
                        "type": "user"
                    },
                    "name": "Chenyang Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:45.781Z",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b489",
                    "name": "Xiqian Yu",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b48a",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b48b",
                    "name": "Yuqiang Yang",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b48c",
                    "name": "Xiaohan Mao",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b48d",
                    "name": "Chenming Zhu",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b48e",
                    "name": "Wenzhe Cai",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b48f",
                    "name": "Hanqing Wang",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b490",
                    "name": "Yilun Chen",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b491",
                    "name": "Xihui Liu",
                    "hidden": false
                },
                {
                    "_id": "686de9d4cb5725779c60b492",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/Vpc-LLN2y02mgIeZu43Mv.mp4"
            ],
            "publishedAt": "2025-07-07T17:49:41.000Z",
            "submittedOnDailyAt": "2025-07-09T03:18:43.255Z",
            "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
            "submittedOnDailyBy": {
                "_id": "64e6d9d229a548f66aff6e5b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
                "isPro": false,
                "fullname": "Tai Wang",
                "user": "taiwang",
                "type": "user"
            },
            "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\nhttps://streamvln.github.io/{https://streamvln.github.io/}.",
            "upvotes": 35,
            "discussionId": "686de9d4cb5725779c60b493",
            "projectPage": "https://streamvln.github.io/",
            "githubRepo": "https://github.com/OpenRobotLab/StreamVLN",
            "ai_summary": "StreamVLN, a streaming VLN framework, uses a hybrid slow-fast context modeling strategy to balance fine-grained visual understanding, long-term context modeling, and computational efficiency in real-world settings.",
            "ai_keywords": [
                "Video-LLMs",
                "StreamVLN",
                "hybrid slow-fast context modeling",
                "multi-modal reasoning",
                "fast-streaming dialogue context",
                "slow-updating memory context",
                "3D-aware token pruning",
                "KV cache reuse",
                "VLN-CE benchmarks"
            ],
            "githubStars": 76
        },
        "publishedAt": "2025-07-07T13:49:41.000Z",
        "title": "StreamVLN: Streaming Vision-and-Language Navigation via SlowFast Context\n  Modeling",
        "summary": "Vision-and-Language Navigation (VLN) in real-world settings requires agents\nto process continuous visual streams and generate actions with low latency\ngrounded in language instructions. While Video-based Large Language Models\n(Video-LLMs) have driven recent progress, current VLN methods based on\nVideo-LLM often face trade-offs among fine-grained visual understanding,\nlong-term context modeling and computational efficiency. We introduce\nStreamVLN, a streaming VLN framework that employs a hybrid slow-fast context\nmodeling strategy to support multi-modal reasoning over interleaved vision,\nlanguage and action inputs. The fast-streaming dialogue context facilitates\nresponsive action generation through a sliding-window of active dialogues,\nwhile the slow-updating memory context compresses historical visual states\nusing a 3D-aware token pruning strategy. With this slow-fast design, StreamVLN\nachieves coherent multi-turn dialogue through efficient KV cache reuse,\nsupporting long video streams with bounded context size and inference cost.\nExperiments on VLN-CE benchmarks demonstrate state-of-the-art performance with\nstable low latency, ensuring robustness and efficiency in real-world\ndeployment. The project page is:\nhttps://streamvln.github.io/{https://streamvln.github.io/}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64e6d9d229a548f66aff6e5b/Vpc-LLN2y02mgIeZu43Mv.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05240.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e6d9d229a548f66aff6e5b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6d9d229a548f66aff6e5b/yQ9E2TyzM4CfSjMPigcey.jpeg",
            "fullname": "Tai Wang",
            "name": "taiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.03112",
            "authors": [
                {
                    "_id": "686c8b1c364e2ad167eb53b4",
                    "user": {
                        "_id": "64be4408c05a0df0d2b6012e",
                        "avatarUrl": "/avatars/09d8427505a418090391dc5a3f8bfef2.svg",
                        "isPro": false,
                        "fullname": "PSWang",
                        "user": "CedarWang",
                        "type": "user"
                    },
                    "name": "Peisong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:51:33.965Z",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53b5",
                    "user": {
                        "_id": "648294b2eb4befee378951c1",
                        "avatarUrl": "/avatars/da5d8bf9d8662cc2ffa2c0de49bd66a3.svg",
                        "isPro": false,
                        "fullname": "Ruotian Ma",
                        "user": "vvibt",
                        "type": "user"
                    },
                    "name": "Ruotian Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:51:32.096Z",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53b6",
                    "name": "Bang Zhang",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53b7",
                    "name": "Xingyu Chen",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53b8",
                    "name": "Zhiwei He",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53b9",
                    "name": "Kang Luo",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53ba",
                    "name": "Qingsong Lv",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53bb",
                    "name": "Qingxuan Jiang",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53bc",
                    "name": "Zheng Xie",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53bd",
                    "name": "Shanyi Wang",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53be",
                    "name": "Yuan Li",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53bf",
                    "name": "Fanghua Ye",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53c0",
                    "name": "Jian Li",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53c1",
                    "name": "Yifan Yang",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53c2",
                    "name": "Zhaopeng Tu",
                    "hidden": false
                },
                {
                    "_id": "686c8b1c364e2ad167eb53c3",
                    "name": "Xiaolong Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-03T18:33:18.000Z",
            "submittedOnDailyAt": "2025-07-09T01:05:45.652Z",
            "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for\n  Empathetic Agents",
            "submittedOnDailyBy": {
                "_id": "61b859ddbdf1fac5ed499992",
                "avatarUrl": "/avatars/2387fb9b8a46840bfc75248462f0a410.svg",
                "isPro": false,
                "fullname": "Jiaqi Chen",
                "user": "judge",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel at logical and algorithmic reasoning, yet\ntheir emotional intelligence (EQ) still lags far behind their cognitive\nprowess. While reinforcement learning from verifiable rewards (RLVR) has\nadvanced in other domains, its application to dialogue-especially for emotional\nintelligence-remains underexplored. In this work, we introduce RLVER, the first\nend-to-end reinforcement learning framework that leverages verifiable emotion\nrewards from simulated users to cultivate higher-order empathetic abilities in\nLLMs. Within this framework, self-consistent affective simulated users engage\nin dialogue rollouts and produce deterministic emotion scores during\nconversations, serving as reward signals to guide the LLM's learning.\nFine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its\nSentient-Benchmark score from 13.3 to 79.2 while largely preserving\nmathematical and coding competence. Extensive experiments reveal that: (i)\nRLVER consistently improves multiple dialogue capabilities; (ii) Thinking and\nnon-thinking models show distinct trends--thinking models excel in empathy and\ninsight, while non-thinking models favor action; (iii) GRPO often yields stable\ngains, while PPO can push certain capabilities to a higher ceiling; (iv) More\nchallenging environments are not always better-moderate ones can yield stronger\noutcomes. Our results show that RLVER is a practical route toward emotionally\nintelligent and broadly capable language agents.",
            "upvotes": 27,
            "discussionId": "686c8b1c364e2ad167eb53c4",
            "ai_summary": "An end-to-end reinforcement learning framework using simulated user emotion rewards enhances emotional intelligence in large language models while maintaining cognitive skills.",
            "ai_keywords": [
                "large language models",
                "RLVR",
                "RLVER",
                "reinforcement learning",
                "affective simulated users",
                "dialogue rollouts",
                "deterministic emotion scores",
                "reward signals",
                "fine-tuning",
                "Qwen2.5-7B-Instruct",
                "PPO",
                "Sentient-Benchmark",
                "thinking models",
                "non-thinking models",
                "GRPO",
                "dialogue capabilities",
                "empathy",
                "insight",
                "action",
                "emotionally intelligent",
                "broadly capable language agents"
            ]
        },
        "publishedAt": "2025-07-03T14:33:18.000Z",
        "title": "RLVER: Reinforcement Learning with Verifiable Emotion Rewards for\n  Empathetic Agents",
        "summary": "Large language models (LLMs) excel at logical and algorithmic reasoning, yet\ntheir emotional intelligence (EQ) still lags far behind their cognitive\nprowess. While reinforcement learning from verifiable rewards (RLVR) has\nadvanced in other domains, its application to dialogue-especially for emotional\nintelligence-remains underexplored. In this work, we introduce RLVER, the first\nend-to-end reinforcement learning framework that leverages verifiable emotion\nrewards from simulated users to cultivate higher-order empathetic abilities in\nLLMs. Within this framework, self-consistent affective simulated users engage\nin dialogue rollouts and produce deterministic emotion scores during\nconversations, serving as reward signals to guide the LLM's learning.\nFine-tuning publicly available Qwen2.5-7B-Instruct model with PPO boosts its\nSentient-Benchmark score from 13.3 to 79.2 while largely preserving\nmathematical and coding competence. Extensive experiments reveal that: (i)\nRLVER consistently improves multiple dialogue capabilities; (ii) Thinking and\nnon-thinking models show distinct trends--thinking models excel in empathy and\ninsight, while non-thinking models favor action; (iii) GRPO often yields stable\ngains, while PPO can push certain capabilities to a higher ceiling; (iv) More\nchallenging environments are not always better-moderate ones can yield stronger\noutcomes. Our results show that RLVER is a practical route toward emotionally\nintelligent and broadly capable language agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03112.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61b859ddbdf1fac5ed499992",
            "avatarUrl": "/avatars/2387fb9b8a46840bfc75248462f0a410.svg",
            "fullname": "Jiaqi Chen",
            "name": "judge",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.05675",
            "authors": [
                {
                    "_id": "686ddcfacb5725779c60b427",
                    "user": {
                        "_id": "63ca949b04c979828315389d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca949b04c979828315389d/HS5xWNAYjjHeyAAwWJ11l.jpeg",
                        "isPro": false,
                        "fullname": "wangrongsheng",
                        "user": "wangrongsheng",
                        "type": "user"
                    },
                    "name": "Rongsheng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:04.039Z",
                    "hidden": false
                },
                {
                    "_id": "686ddcfacb5725779c60b428",
                    "name": "Junying Chen",
                    "hidden": false
                },
                {
                    "_id": "686ddcfacb5725779c60b429",
                    "name": "Ke Ji",
                    "hidden": false
                },
                {
                    "_id": "686ddcfacb5725779c60b42a",
                    "name": "Zhenyang Cai",
                    "hidden": false
                },
                {
                    "_id": "686ddcfacb5725779c60b42b",
                    "name": "Shunian Chen",
                    "hidden": false
                },
                {
                    "_id": "686ddcfacb5725779c60b42c",
                    "name": "Yunjin Yang",
                    "hidden": false
                },
                {
                    "_id": "686ddcfacb5725779c60b42d",
                    "name": "Benyou Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T04:58:36.000Z",
            "submittedOnDailyAt": "2025-07-09T01:40:25.246Z",
            "title": "MedGen: Unlocking Medical Video Generation by Scaling\n  Granularly-annotated Medical Videos",
            "submittedOnDailyBy": {
                "_id": "63ca949b04c979828315389d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca949b04c979828315389d/HS5xWNAYjjHeyAAwWJ11l.jpeg",
                "isPro": false,
                "fullname": "wangrongsheng",
                "user": "wangrongsheng",
                "type": "user"
            },
            "summary": "Recent advances in video generation have shown remarkable progress in\nopen-domain settings, yet medical video generation remains largely\nunderexplored. Medical videos are critical for applications such as clinical\ntraining, education, and simulation, requiring not only high visual fidelity\nbut also strict medical accuracy. However, current models often produce\nunrealistic or erroneous content when applied to medical prompts, largely due\nto the lack of large-scale, high-quality datasets tailored to the medical\ndomain. To address this gap, we introduce MedVideoCap-55K, the first\nlarge-scale, diverse, and caption-rich dataset for medical video generation. It\ncomprises over 55,000 curated clips spanning real-world medical scenarios,\nproviding a strong foundation for training generalist medical video generation\nmodels. Built upon this dataset, we develop MedGen, which achieves leading\nperformance among open-source models and rivals commercial systems across\nmultiple benchmarks in both visual quality and medical accuracy. We hope our\ndataset and model can serve as a valuable resource and help catalyze further\nresearch in medical video generation. Our code and data is available at\nhttps://github.com/FreedomIntelligence/MedGen",
            "upvotes": 24,
            "discussionId": "686ddcfbcb5725779c60b42e",
            "githubRepo": "https://github.com/FreedomIntelligence/MedGen",
            "ai_summary": "MedGen, a model trained on the large-scale MedVideoCap-55K dataset, achieves top performance in medical video generation by balancing visual quality and medical accuracy.",
            "ai_keywords": [
                "video generation",
                "medical video generation",
                "MedVideoCap-55K",
                "MedGen",
                "visual quality",
                "medical accuracy",
                "benchmarks"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-07-08T00:58:36.000Z",
        "title": "MedGen: Unlocking Medical Video Generation by Scaling\n  Granularly-annotated Medical Videos",
        "summary": "Recent advances in video generation have shown remarkable progress in\nopen-domain settings, yet medical video generation remains largely\nunderexplored. Medical videos are critical for applications such as clinical\ntraining, education, and simulation, requiring not only high visual fidelity\nbut also strict medical accuracy. However, current models often produce\nunrealistic or erroneous content when applied to medical prompts, largely due\nto the lack of large-scale, high-quality datasets tailored to the medical\ndomain. To address this gap, we introduce MedVideoCap-55K, the first\nlarge-scale, diverse, and caption-rich dataset for medical video generation. It\ncomprises over 55,000 curated clips spanning real-world medical scenarios,\nproviding a strong foundation for training generalist medical video generation\nmodels. Built upon this dataset, we develop MedGen, which achieves leading\nperformance among open-source models and rivals commercial systems across\nmultiple benchmarks in both visual quality and medical accuracy. We hope our\ndataset and model can serve as a valuable resource and help catalyze further\nresearch in medical video generation. Our code and data is available at\nhttps://github.com/FreedomIntelligence/MedGen",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05675.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63ca949b04c979828315389d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ca949b04c979828315389d/HS5xWNAYjjHeyAAwWJ11l.jpeg",
            "fullname": "wangrongsheng",
            "name": "wangrongsheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 60
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06219",
            "authors": [
                {
                    "_id": "686dcc7fcb5725779c60b3a8",
                    "user": {
                        "_id": "675b91e7a86e54985542f9ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/675b91e7a86e54985542f9ba/JVt4lmWplJTj8khQPThre.jpeg",
                        "isPro": false,
                        "fullname": "Modi Shi",
                        "user": "ModiShi",
                        "type": "user"
                    },
                    "name": "Modi Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:15.702Z",
                    "hidden": false
                },
                {
                    "_id": "686dcc7fcb5725779c60b3a9",
                    "name": "Li Chen",
                    "hidden": false
                },
                {
                    "_id": "686dcc7fcb5725779c60b3aa",
                    "name": "Jin Chen",
                    "hidden": false
                },
                {
                    "_id": "686dcc7fcb5725779c60b3ab",
                    "user": {
                        "_id": "64b8faeb8b53fb5dbdfecae5",
                        "avatarUrl": "/avatars/9f5919600ee69c38be896dd959bb8724.svg",
                        "isPro": false,
                        "fullname": "Yuxiang Lu",
                        "user": "yxlu0",
                        "type": "user"
                    },
                    "name": "Yuxiang Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:17.632Z",
                    "hidden": false
                },
                {
                    "_id": "686dcc7fcb5725779c60b3ac",
                    "name": "Chiming Liu",
                    "hidden": false
                },
                {
                    "_id": "686dcc7fcb5725779c60b3ad",
                    "user": {
                        "_id": "646ec9b135f55eb49e405faa",
                        "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
                        "isPro": false,
                        "fullname": "Guanghui Ren",
                        "user": "sundrops",
                        "type": "user"
                    },
                    "name": "Guanghui Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:13.590Z",
                    "hidden": false
                },
                {
                    "_id": "686dcc7fcb5725779c60b3ae",
                    "name": "Ping Luo",
                    "hidden": false
                },
                {
                    "_id": "686dcc7fcb5725779c60b3af",
                    "name": "Di Huang",
                    "hidden": false
                },
                {
                    "_id": "686dcc7fcb5725779c60b3b0",
                    "name": "Maoqing Yao",
                    "hidden": false
                },
                {
                    "_id": "686dcc7fcb5725779c60b3b1",
                    "name": "Hongyang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T17:52:44.000Z",
            "submittedOnDailyAt": "2025-07-09T00:29:16.132Z",
            "title": "Is Diversity All You Need for Scalable Robotic Manipulation?",
            "submittedOnDailyBy": {
                "_id": "64b8faeb8b53fb5dbdfecae5",
                "avatarUrl": "/avatars/9f5919600ee69c38be896dd959bb8724.svg",
                "isPro": false,
                "fullname": "Yuxiang Lu",
                "user": "yxlu0",
                "type": "user"
            },
            "summary": "Data scaling has driven remarkable success in foundation models for Natural\nLanguage Processing (NLP) and Computer Vision (CV), yet the principles of\neffective data scaling in robotic manipulation remain insufficiently\nunderstood. In this work, we investigate the nuanced role of data diversity in\nrobot learning by examining three critical dimensions-task (what to do),\nembodiment (which robot to use), and expert (who demonstrates)-challenging the\nconventional intuition of \"more diverse is better\". Throughout extensive\nexperiments on various robot platforms, we reveal that (1) task diversity\nproves more critical than per-task demonstration quantity, benefiting transfer\nfrom diverse pre-training tasks to novel downstream scenarios; (2)\nmulti-embodiment pre-training data is optional for cross-embodiment\ntransfer-models trained on high-quality single-embodiment data can efficiently\ntransfer to different platforms, showing more desirable scaling property during\nfine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,\narising from individual operational preferences and stochastic variations in\nhuman demonstrations, can be confounding to policy learning, with velocity\nmultimodality emerging as a key contributing factor. Based on this insight, we\npropose a distribution debiasing method to mitigate velocity ambiguity, the\nyielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to\nusing 2.5 times pre-training data. Collectively, these findings provide new\nperspectives and offer practical guidance on how to scale robotic manipulation\ndatasets effectively.",
            "upvotes": 17,
            "discussionId": "686dcc7fcb5725779c60b3b2",
            "projectPage": "https://agibot-world.com/",
            "githubRepo": "https://github.com/OpenDriveLab/AgiBot-World",
            "ai_summary": "Investigation into data diversity in robotic manipulation reveals that task diversity is crucial, multi-embodiment data is optional, and expert diversity can be confounding, leading to a distribution debiasing method for improved performance.",
            "ai_keywords": [
                "task diversity",
                "embodiment",
                "expert diversity",
                "multi-embodiment pre-training",
                "cross-embodiment transfer",
                "policy learning",
                "distribution debiasing"
            ],
            "githubStars": 2169
        },
        "publishedAt": "2025-07-08T13:52:44.000Z",
        "title": "Is Diversity All You Need for Scalable Robotic Manipulation?",
        "summary": "Data scaling has driven remarkable success in foundation models for Natural\nLanguage Processing (NLP) and Computer Vision (CV), yet the principles of\neffective data scaling in robotic manipulation remain insufficiently\nunderstood. In this work, we investigate the nuanced role of data diversity in\nrobot learning by examining three critical dimensions-task (what to do),\nembodiment (which robot to use), and expert (who demonstrates)-challenging the\nconventional intuition of \"more diverse is better\". Throughout extensive\nexperiments on various robot platforms, we reveal that (1) task diversity\nproves more critical than per-task demonstration quantity, benefiting transfer\nfrom diverse pre-training tasks to novel downstream scenarios; (2)\nmulti-embodiment pre-training data is optional for cross-embodiment\ntransfer-models trained on high-quality single-embodiment data can efficiently\ntransfer to different platforms, showing more desirable scaling property during\nfine-tuning than multi-embodiment pre-trained models; and (3) expert diversity,\narising from individual operational preferences and stochastic variations in\nhuman demonstrations, can be confounding to policy learning, with velocity\nmultimodality emerging as a key contributing factor. Based on this insight, we\npropose a distribution debiasing method to mitigate velocity ambiguity, the\nyielding GO-1-Pro achieves substantial performance gains of 15%, equivalent to\nusing 2.5 times pre-training data. Collectively, these findings provide new\nperspectives and offer practical guidance on how to scale robotic manipulation\ndatasets effectively.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06219.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64b8faeb8b53fb5dbdfecae5",
            "avatarUrl": "/avatars/9f5919600ee69c38be896dd959bb8724.svg",
            "fullname": "Yuxiang Lu",
            "name": "yxlu0",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.05791",
            "authors": [
                {
                    "_id": "686dcbe6cb5725779c60b37f",
                    "name": "Yan Yang",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b380",
                    "name": "Dongxu Li",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b381",
                    "name": "Yutong Dai",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b382",
                    "name": "Yuhao Yang",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b383",
                    "user": {
                        "_id": "6090ff099a8bcaa437b234a4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6090ff099a8bcaa437b234a4/iUvw7JXT-ngI7rGk1x-io.jpeg",
                        "isPro": false,
                        "fullname": "Ziyang Luo",
                        "user": "Ziyang",
                        "type": "user"
                    },
                    "name": "Ziyang Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:27.741Z",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b384",
                    "name": "Zirui Zhao",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b385",
                    "name": "Zhiyuan Hu",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b386",
                    "name": "Junzhe Huang",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b387",
                    "name": "Amrita Saha",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b388",
                    "name": "Zeyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b389",
                    "name": "Ran Xu",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b38a",
                    "name": "Liyuan Pan",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b38b",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "686dcbe6cb5725779c60b38c",
                    "name": "Junnan Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T08:52:18.000Z",
            "submittedOnDailyAt": "2025-07-09T00:25:26.046Z",
            "title": "GTA1: GUI Test-time Scaling Agent",
            "submittedOnDailyBy": {
                "_id": "655b813476e4fad5529f3256",
                "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
                "isPro": true,
                "fullname": "Yan Yang",
                "user": "HelloKKMe",
                "type": "user"
            },
            "summary": "Graphical user interface (GUI) agents autonomously operate across platforms\n(e.g., Linux) to complete tasks by interacting with visual elements.\nSpecifically, a user instruction is decomposed into a sequence of action\nproposals, each corresponding to an interaction with the GUI. After each\naction, the agent observes the updated GUI environment to plan the next step.\nHowever, two main challenges arise: i) resolving ambiguity in task planning\n(i.e., the action proposal sequence), where selecting an appropriate plan is\nnon-trivial, as many valid ones may exist; ii) accurately grounding actions in\ncomplex and high-resolution interfaces, i.e., precisely interacting with visual\ntargets.\n  This paper investigates the two aforementioned challenges with our GUI\nTest-time Scaling Agent, namely GTA1. First, to select the most appropriate\naction proposal, we introduce a test-time scaling method. At each step, we\nsample multiple candidate action proposals and leverage a judge model to\nevaluate and select the most suitable one. It trades off computation for better\ndecision quality by concurrent sampling, shortening task execution steps, and\nimproving overall performance. Second, we propose a model that achieves\nimproved accuracy when grounding the selected action proposal to its\ncorresponding visual elements. Our key insight is that reinforcement learning\n(RL) facilitates visual grounding through inherent objective alignments,\nrewarding successful clicks on interface elements.\n  Experimentally, our method establishes state-of-the-art performance across\ndiverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%\naccuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When\npaired with a planner applying our test-time scaling strategy, it exhibits\nstate-of-the-art agentic performance (e.g., 45.2% task success rate on\nOSWorld). We open-source our code and models here.",
            "upvotes": 16,
            "discussionId": "686dcbe6cb5725779c60b38d",
            "githubRepo": "https://github.com/Yan98/GTA1",
            "ai_summary": "GTA1 addresses task planning ambiguity and visual grounding in GUI interactions using test-time scaling and reinforcement learning, achieving state-of-the-art performance across benchmarks.",
            "ai_keywords": [
                "GUI",
                "test-time scaling",
                "action proposals",
                "judge model",
                "reinforcement learning",
                "visual grounding",
                "task planning",
                "state-of-the-art",
                "Screenspot-Pro",
                "Screenspot-V2",
                "OSWorld-G",
                "task success rate"
            ],
            "githubStars": 33
        },
        "publishedAt": "2025-07-08T04:52:18.000Z",
        "title": "GTA1: GUI Test-time Scaling Agent",
        "summary": "Graphical user interface (GUI) agents autonomously operate across platforms\n(e.g., Linux) to complete tasks by interacting with visual elements.\nSpecifically, a user instruction is decomposed into a sequence of action\nproposals, each corresponding to an interaction with the GUI. After each\naction, the agent observes the updated GUI environment to plan the next step.\nHowever, two main challenges arise: i) resolving ambiguity in task planning\n(i.e., the action proposal sequence), where selecting an appropriate plan is\nnon-trivial, as many valid ones may exist; ii) accurately grounding actions in\ncomplex and high-resolution interfaces, i.e., precisely interacting with visual\ntargets.\n  This paper investigates the two aforementioned challenges with our GUI\nTest-time Scaling Agent, namely GTA1. First, to select the most appropriate\naction proposal, we introduce a test-time scaling method. At each step, we\nsample multiple candidate action proposals and leverage a judge model to\nevaluate and select the most suitable one. It trades off computation for better\ndecision quality by concurrent sampling, shortening task execution steps, and\nimproving overall performance. Second, we propose a model that achieves\nimproved accuracy when grounding the selected action proposal to its\ncorresponding visual elements. Our key insight is that reinforcement learning\n(RL) facilitates visual grounding through inherent objective alignments,\nrewarding successful clicks on interface elements.\n  Experimentally, our method establishes state-of-the-art performance across\ndiverse benchmarks. For example, GTA1-7B achieves 50.1%, 92.4%, and 67.7%\naccuracies on Screenspot-Pro, Screenspot-V2, and OSWorld-G, respectively. When\npaired with a planner applying our test-time scaling strategy, it exhibits\nstate-of-the-art agentic performance (e.g., 45.2% task success rate on\nOSWorld). We open-source our code and models here.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05791.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "655b813476e4fad5529f3256",
            "avatarUrl": "/avatars/73d83e45d921531f9830a0ea80f76491.svg",
            "fullname": "Yan Yang",
            "name": "HelloKKMe",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.04569",
            "authors": [
                {
                    "_id": "686e2b0aa5f0f70d9de40c80",
                    "user": {
                        "_id": "6087e598e2b7cc3a117b0dc5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
                        "isPro": false,
                        "fullname": "Guokan Shang",
                        "user": "guokan-shang",
                        "type": "user"
                    },
                    "name": "Guokan Shang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:05.228Z",
                    "hidden": false
                },
                {
                    "_id": "686e2b0aa5f0f70d9de40c81",
                    "user": {
                        "_id": "66448b4fecac3bc79b26304f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66448b4fecac3bc79b26304f/aPH3UFbc20CL2Bz2yn7nH.jpeg",
                        "isPro": false,
                        "fullname": "Hadi Abdine",
                        "user": "habdine",
                        "type": "user"
                    },
                    "name": "Hadi Abdine",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:40:52.030Z",
                    "hidden": false
                },
                {
                    "_id": "686e2b0aa5f0f70d9de40c82",
                    "user": {
                        "_id": "6751b0caecaa275e389dd5eb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RI2Y_nnjg5xJnv5sJVx_v.png",
                        "isPro": false,
                        "fullname": "Ahmad Chamma",
                        "user": "AC-723",
                        "type": "user"
                    },
                    "name": "Ahmad Chamma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:40:47.916Z",
                    "hidden": false
                },
                {
                    "_id": "686e2b0aa5f0f70d9de40c83",
                    "user": {
                        "_id": "655efd24afee0e00788bb589",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655efd24afee0e00788bb589/22guLxIWNybbJR3jI-c4w.jpeg",
                        "isPro": false,
                        "fullname": "Amr Mohamed",
                        "user": "amr-mohamed",
                        "type": "user"
                    },
                    "name": "Amr Mohamed",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:40:50.004Z",
                    "hidden": false
                },
                {
                    "_id": "686e2b0aa5f0f70d9de40c84",
                    "user": {
                        "_id": "624331b8c927fd9d912b5819",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1648570759533-noauth.png",
                        "isPro": false,
                        "fullname": "Mohamed Anwar",
                        "user": "anwarvic",
                        "type": "user"
                    },
                    "name": "Mohamed Anwar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:40:44.048Z",
                    "hidden": false
                },
                {
                    "_id": "686e2b0aa5f0f70d9de40c85",
                    "user": {
                        "_id": "6380e53efb49cd1c12052c17",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6380e53efb49cd1c12052c17/b5CweexfrVn-W_xto2agR.jpeg",
                        "isPro": true,
                        "fullname": "Abdelaziz Bounhar",
                        "user": "BounharAbdelaziz",
                        "type": "user"
                    },
                    "name": "Abdelaziz Bounhar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:40:41.336Z",
                    "hidden": false
                },
                {
                    "_id": "686e2b0aa5f0f70d9de40c86",
                    "name": "Omar El Herraoui",
                    "hidden": false
                },
                {
                    "_id": "686e2b0aa5f0f70d9de40c87",
                    "user": {
                        "_id": "647f7eb25e1bc4753746bf9f",
                        "avatarUrl": "/avatars/cc9c6210fdc822d8a106937e747dff41.svg",
                        "isPro": false,
                        "fullname": "Preslav Nakov",
                        "user": "preslavnakov",
                        "type": "user"
                    },
                    "name": "Preslav Nakov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:40:46.092Z",
                    "hidden": false
                },
                {
                    "_id": "686e2b0aa5f0f70d9de40c88",
                    "name": "Michalis Vazirgiannis",
                    "hidden": false
                },
                {
                    "_id": "686e2b0aa5f0f70d9de40c89",
                    "name": "Eric Xing",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-06T22:53:41.000Z",
            "submittedOnDailyAt": "2025-07-09T07:14:54.453Z",
            "title": "Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts",
            "submittedOnDailyBy": {
                "_id": "6087e598e2b7cc3a117b0dc5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
                "isPro": false,
                "fullname": "Guokan Shang",
                "user": "guokan-shang",
                "type": "user"
            },
            "summary": "We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for\nEgyptian dialect, uniquely designed to understand and generate texts written in\nboth Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we\nintroduce a novel language adaptation approach by leveraging the\nBranch-Train-MiX strategy to merge script-specialized experts, into a single\nMoE model. Our Nile-Chat models significantly outperform leading multilingual\nand Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced\nEgyptian evaluation benchmarks, which span both understanding and generative\ntasks. Notably, our 12B model yields a 14.4% performance gain over\nQwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly\navailable. We believe this work presents a comprehensive methodology for\nadapting LLMs to dual-script languages, addressing an often overlooked aspect\nin modern LLM development.",
            "upvotes": 16,
            "discussionId": "686e2b0aa5f0f70d9de40c8a",
            "ai_summary": "Nile-Chat models, using Branch-Train-MiX strategy, outperform existing multilingual and Arabic LLMs on Egyptian dialect benchmarks in both Arabic and Latin scripts.",
            "ai_keywords": [
                "LLMs",
                "Egyptian dialect",
                "Arabic",
                "Latin scripts",
                "Branch-Train-MiX",
                "MoE model",
                "Qwen2.5-14B-Instruct",
                "dual-script languages"
            ]
        },
        "publishedAt": "2025-07-06T18:53:41.000Z",
        "title": "Nile-Chat: Egyptian Language Models for Arabic and Latin Scripts",
        "summary": "We introduce Nile-Chat-4B, 3x4B-A6B, and 12B, a collection of LLMs for\nEgyptian dialect, uniquely designed to understand and generate texts written in\nboth Arabic and Latin scripts. Specifically, with Nile-Chat-3x4B-A6B, we\nintroduce a novel language adaptation approach by leveraging the\nBranch-Train-MiX strategy to merge script-specialized experts, into a single\nMoE model. Our Nile-Chat models significantly outperform leading multilingual\nand Arabic LLMs, such as LLaMa, Jais, and ALLaM, on our newly introduced\nEgyptian evaluation benchmarks, which span both understanding and generative\ntasks. Notably, our 12B model yields a 14.4% performance gain over\nQwen2.5-14B-Instruct on Latin-script benchmarks. All our resources are publicly\navailable. We believe this work presents a comprehensive methodology for\nadapting LLMs to dual-script languages, addressing an often overlooked aspect\nin modern LLM development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04569.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6087e598e2b7cc3a117b0dc5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6087e598e2b7cc3a117b0dc5/Ctz_W-uo1gOQRBHXalD1P.png",
            "fullname": "Guokan Shang",
            "name": "guokan-shang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06138",
            "authors": [
                {
                    "_id": "686ddd6acb5725779c60b43c",
                    "name": "Taolin Zhang",
                    "hidden": false
                },
                {
                    "_id": "686ddd6acb5725779c60b43d",
                    "user": {
                        "_id": "677e869467f3bb8d8215eec6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
                        "isPro": false,
                        "fullname": "Zihan Ma",
                        "user": "MichaelErchi",
                        "type": "user"
                    },
                    "name": "Zihan Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:01.962Z",
                    "hidden": false
                },
                {
                    "_id": "686ddd6acb5725779c60b43e",
                    "name": "Maosong Cao",
                    "hidden": false
                },
                {
                    "_id": "686ddd6acb5725779c60b43f",
                    "user": {
                        "_id": "643d26979347842571bc9613",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
                        "isPro": false,
                        "fullname": "Junnan Liu",
                        "user": "jnanliu",
                        "type": "user"
                    },
                    "name": "Junnan Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:59.943Z",
                    "hidden": false
                },
                {
                    "_id": "686ddd6acb5725779c60b440",
                    "name": "Songyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "686ddd6acb5725779c60b441",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T16:20:43.000Z",
            "submittedOnDailyAt": "2025-07-09T01:39:34.405Z",
            "title": "Coding Triangle: How Does Large Language Model Understand Code?",
            "submittedOnDailyBy": {
                "_id": "630716d11801ecc7d2595021",
                "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
                "isPro": false,
                "fullname": "Songyang Zhang",
                "user": "zsytony",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.",
            "upvotes": 14,
            "discussionId": "686ddd6dcb5725779c60b442",
            "ai_summary": "The Code Triangle framework evaluates large language models across editorial analysis, code implementation, and test case generation, revealing limitations in diversity and robustness compared to human programmers and suggesting enhancements through human-generated content and model mixtures.",
            "ai_keywords": [
                "Code Triangle framework",
                "large language models",
                "editorial analysis",
                "code implementation",
                "test case generation",
                "competitive programming benchmarks",
                "self-consistent system",
                "distribution shift",
                "training data biases",
                "reasoning transfer",
                "human-generated editorials",
                "model mixtures",
                "self-reflection",
                "self-improvement"
            ]
        },
        "publishedAt": "2025-07-08T12:20:43.000Z",
        "title": "Coding Triangle: How Does Large Language Model Understand Code?",
        "summary": "Large language models (LLMs) have achieved remarkable progress in code\ngeneration, yet their true programming competence remains underexplored. We\nintroduce the Code Triangle framework, which systematically evaluates LLMs\nacross three fundamental dimensions: editorial analysis, code implementation,\nand test case generation. Through extensive experiments on competitive\nprogramming benchmarks, we reveal that while LLMs can form a self-consistent\nsystem across these dimensions, their solutions often lack the diversity and\nrobustness of human programmers. We identify a significant distribution shift\nbetween model cognition and human expertise, with model errors tending to\ncluster due to training data biases and limited reasoning transfer. Our study\ndemonstrates that incorporating human-generated editorials, solutions, and\ndiverse test cases, as well as leveraging model mixtures, can substantially\nenhance both the performance and robustness of LLMs. Furthermore, we reveal\nboth the consistency and inconsistency in the cognition of LLMs that may\nfacilitate self-reflection and self-improvement, providing a potential\ndirection for developing more powerful coding models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06138.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "630716d11801ecc7d2595021",
            "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
            "fullname": "Songyang Zhang",
            "name": "zsytony",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.05169",
            "authors": [
                {
                    "_id": "686e8b27d938c25d684419f7",
                    "name": "Eric Xing",
                    "hidden": false
                },
                {
                    "_id": "686e8b27d938c25d684419f8",
                    "name": "Mingkai Deng",
                    "hidden": false
                },
                {
                    "_id": "686e8b27d938c25d684419f9",
                    "name": "Jinyu Hou",
                    "hidden": false
                },
                {
                    "_id": "686e8b27d938c25d684419fa",
                    "name": "Zhiting Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T16:23:46.000Z",
            "submittedOnDailyAt": "2025-07-09T14:07:37.591Z",
            "title": "Critiques of World Models",
            "submittedOnDailyBy": {
                "_id": "61718f1eec6894b48c23eec9",
                "avatarUrl": "/avatars/9f426e0f1ca734fb428a22881d2e7b20.svg",
                "isPro": false,
                "fullname": "Mingkai Deng",
                "user": "mingkaid",
                "type": "user"
            },
            "summary": "World Model, the supposed algorithmic surrogate of the real-world environment\nwhich biological agents experience with and act upon, has been an emerging\ntopic in recent years because of the rising needs to develop virtual agents\nwith artificial (general) intelligence. There has been much debate on what a\nworld model really is, how to build it, how to use it, and how to evaluate it.\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\nand drawing inspiration from the concept of \"hypothetical thinking\" in\npsychology literature, we offer critiques of several schools of thoughts on\nworld modeling, and argue the primary goal of a world model to be simulating\nall actionable possibilities of the real world for purposeful reasoning and\nacting. Building on the critiques, we propose a new architecture for a\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\ncontinuous/discrete representations, and a generative and self-supervision\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\nAGI system enabled by such a model.",
            "upvotes": 12,
            "discussionId": "686e8b28d938c25d684419fb"
        },
        "publishedAt": "2025-07-07T12:23:46.000Z",
        "title": "Critiques of World Models",
        "summary": "World Model, the supposed algorithmic surrogate of the real-world environment\nwhich biological agents experience with and act upon, has been an emerging\ntopic in recent years because of the rising needs to develop virtual agents\nwith artificial (general) intelligence. There has been much debate on what a\nworld model really is, how to build it, how to use it, and how to evaluate it.\nIn this essay, starting from the imagination in the famed Sci-Fi classic Dune,\nand drawing inspiration from the concept of \"hypothetical thinking\" in\npsychology literature, we offer critiques of several schools of thoughts on\nworld modeling, and argue the primary goal of a world model to be simulating\nall actionable possibilities of the real world for purposeful reasoning and\nacting. Building on the critiques, we propose a new architecture for a\ngeneral-purpose world model, based on hierarchical, multi-level, and mixed\ncontinuous/discrete representations, and a generative and self-supervision\nlearning framework, with an outlook of a Physical, Agentic, and Nested (PAN)\nAGI system enabled by such a model.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05169.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61718f1eec6894b48c23eec9",
            "avatarUrl": "/avatars/9f426e0f1ca734fb428a22881d2e7b20.svg",
            "fullname": "Mingkai Deng",
            "name": "mingkaid",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06223",
            "authors": [
                {
                    "_id": "686dc72ccb5725779c60b356",
                    "name": "Zhiyuan Peng",
                    "hidden": false
                },
                {
                    "_id": "686dc72ccb5725779c60b357",
                    "name": "Ting-ruen Wei",
                    "hidden": false
                },
                {
                    "_id": "686dc72ccb5725779c60b358",
                    "user": {
                        "_id": "64dc29d9b5d625e0e9a6ecb9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
                        "isPro": false,
                        "fullname": "Tingyu Song",
                        "user": "songtingyu",
                        "type": "user"
                    },
                    "name": "Tingyu Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:31.763Z",
                    "hidden": false
                },
                {
                    "_id": "686dc72ccb5725779c60b359",
                    "user": {
                        "_id": "62f662bcc58915315c4eccea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62f662bcc58915315c4eccea/zOAQLONfMP88zr70sxHK-.jpeg",
                        "isPro": true,
                        "fullname": "Yilun Zhao",
                        "user": "yilunzhao",
                        "type": "user"
                    },
                    "name": "Yilun Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:33.878Z",
                    "hidden": false
                },
                {
                    "_id": "686dc72ccb5725779c60b35a",
                    "name": "Yi Fang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T17:56:28.000Z",
            "submittedOnDailyAt": "2025-07-09T00:20:14.472Z",
            "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
            "submittedOnDailyBy": {
                "_id": "64dc29d9b5d625e0e9a6ecb9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
                "isPro": false,
                "fullname": "Tingyu Song",
                "user": "songtingyu",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE2R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.",
            "upvotes": 11,
            "discussionId": "686dc72ccb5725779c60b35b",
            "ai_summary": "E\\textsuperscript{2}R-FLOPs evaluates LLM-based rerankers by measuring relevance and throughput per PetaFLOP, providing a hardware-agnostic metric for efficiency and effectiveness.",
            "ai_keywords": [
                "E\\textsuperscript{2}R-FLOPs",
                "ranking metrics per PetaFLOP",
                "queries per PetaFLOP",
                "FLOPs estimator",
                "LLM-based rerankers",
                "efficiency-effectiveness trade-off"
            ]
        },
        "publishedAt": "2025-07-08T13:56:28.000Z",
        "title": "Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers",
        "summary": "Large Language Models (LLMs) have recently been applied to reranking tasks in\ninformation retrieval, achieving strong performance. However, their high\ncomputational demands often hinder practical deployment. Existing studies\nevaluate the efficiency of LLM-based rerankers using proxy metrics such as\nlatency, the number of forward passes, input tokens, and output tokens.\nHowever, these metrics depend on hardware and running-time choices (\\eg\nparallel or not, batch size, etc), and often fail to account for model size,\nmaking it difficult to interpret and obscuring the evaluation of the\nefficiency-effectiveness tradeoff. To address this issue, we propose\nE2R-FLOPs, for LLM-based rerankers: ranking metrics per\nPetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for\nhardware-agnostic throughput. Companied with the new metrics, an interpretable\nFLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even\nwithout running any experiments. Based on the proposed metrics, we conduct\ncomprehensive experiments to evaluate a wide range of LLM-based rerankers with\ndifferent architecture, studying the efficiency-effectiveness trade-off and\nbringing this issue to the attention of the research community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06223.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64dc29d9b5d625e0e9a6ecb9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/QxGBsnk1cNsBEPqSx4ae-.jpeg",
            "fullname": "Tingyu Song",
            "name": "songtingyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.05101",
            "authors": [
                {
                    "_id": "686dc2d1cb5725779c60b342",
                    "user": {
                        "_id": "6682a6a75f3099b62d7aa802",
                        "avatarUrl": "/avatars/ca9d41a94d508f0afd3516b1805cd2a2.svg",
                        "isPro": false,
                        "fullname": "Xinzhe Zheng",
                        "user": "piaolaidangqu",
                        "type": "user"
                    },
                    "name": "Xinzhe Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:37.420Z",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b343",
                    "name": "Hao Du",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b344",
                    "name": "Fanding Xu",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b345",
                    "user": {
                        "_id": "67658bd7f7ac7e978ab6f957",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/c8VBgFckkZNUGeqUyotwq.png",
                        "isPro": false,
                        "fullname": "Jinzhe Li",
                        "user": "JinzheFudan",
                        "type": "user"
                    },
                    "name": "Jinzhe Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:35.772Z",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b346",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b347",
                    "name": "Wenkang Wang",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b348",
                    "name": "Tao Chen",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b349",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b34a",
                    "name": "Stan Z. Li",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b34b",
                    "name": "Yan Lu",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b34c",
                    "name": "Nanqing Dong",
                    "hidden": false
                },
                {
                    "_id": "686dc2d1cb5725779c60b34d",
                    "name": "Yang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T15:21:05.000Z",
            "submittedOnDailyAt": "2025-07-09T01:01:03.312Z",
            "title": "PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to\n  Graphs",
            "submittedOnDailyBy": {
                "_id": "6310a3cd531cc21f9e06de6a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
                "isPro": false,
                "fullname": "Zhiyuan Liu",
                "user": "acharkq",
                "type": "user"
            },
            "summary": "Deep learning-based computational methods have achieved promising results in\npredicting protein-protein interactions (PPIs). However, existing benchmarks\npredominantly focus on isolated pairwise evaluations, overlooking a model's\ncapability to reconstruct biologically meaningful PPI networks, which is\ncrucial for biology research. To address this gap, we introduce PRING, the\nfirst comprehensive benchmark that evaluates protein-protein interaction\nprediction from a graph-level perspective. PRING curates a high-quality,\nmulti-species PPI network dataset comprising 21,484 proteins and 186,818\ninteractions, with well-designed strategies to address both data redundancy and\nleakage. Building on this golden-standard dataset, we establish two\ncomplementary evaluation paradigms: (1) topology-oriented tasks, which assess\nintra and cross-species PPI network construction, and (2) function-oriented\ntasks, including protein complex pathway prediction, GO module analysis, and\nessential protein justification. These evaluations not only reflect the model's\ncapability to understand the network topology but also facilitate protein\nfunction annotation, biological module detection, and even disease mechanism\nanalysis. Extensive experiments on four representative model categories,\nconsisting of sequence similarity-based, naive sequence-based, protein language\nmodel-based, and structure-based approaches, demonstrate that current PPI\nmodels have potential limitations in recovering both structural and functional\nproperties of PPI networks, highlighting the gap in supporting real-world\nbiological applications. We believe PRING provides a reliable platform to guide\nthe development of more effective PPI prediction models for the community. The\ndataset and source code of PRING are available at\nhttps://github.com/SophieSarceau/PRING.",
            "upvotes": 10,
            "discussionId": "686dc2d1cb5725779c60b34e"
        },
        "publishedAt": "2025-07-07T11:21:05.000Z",
        "title": "PRING: Rethinking Protein-Protein Interaction Prediction from Pairs to\n  Graphs",
        "summary": "Deep learning-based computational methods have achieved promising results in\npredicting protein-protein interactions (PPIs). However, existing benchmarks\npredominantly focus on isolated pairwise evaluations, overlooking a model's\ncapability to reconstruct biologically meaningful PPI networks, which is\ncrucial for biology research. To address this gap, we introduce PRING, the\nfirst comprehensive benchmark that evaluates protein-protein interaction\nprediction from a graph-level perspective. PRING curates a high-quality,\nmulti-species PPI network dataset comprising 21,484 proteins and 186,818\ninteractions, with well-designed strategies to address both data redundancy and\nleakage. Building on this golden-standard dataset, we establish two\ncomplementary evaluation paradigms: (1) topology-oriented tasks, which assess\nintra and cross-species PPI network construction, and (2) function-oriented\ntasks, including protein complex pathway prediction, GO module analysis, and\nessential protein justification. These evaluations not only reflect the model's\ncapability to understand the network topology but also facilitate protein\nfunction annotation, biological module detection, and even disease mechanism\nanalysis. Extensive experiments on four representative model categories,\nconsisting of sequence similarity-based, naive sequence-based, protein language\nmodel-based, and structure-based approaches, demonstrate that current PPI\nmodels have potential limitations in recovering both structural and functional\nproperties of PPI networks, highlighting the gap in supporting real-world\nbiological applications. We believe PRING provides a reliable platform to guide\nthe development of more effective PPI prediction models for the community. The\ndataset and source code of PRING are available at\nhttps://github.com/SophieSarceau/PRING.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05101.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6310a3cd531cc21f9e06de6a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6310a3cd531cc21f9e06de6a/aTGMx3O41lUARK9s3dAik.jpeg",
            "fullname": "Zhiyuan Liu",
            "name": "acharkq",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.03698",
            "authors": [
                {
                    "_id": "686dc7ebcb5725779c60b35d",
                    "name": "Zhiling Yan",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b35e",
                    "name": "Sifan Song",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b35f",
                    "user": {
                        "_id": "619f01b8cc04eadf54fa5d5d",
                        "avatarUrl": "/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg",
                        "isPro": false,
                        "fullname": "Song Dingjie",
                        "user": "songdj",
                        "type": "user"
                    },
                    "name": "Dingjie Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:29.991Z",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b360",
                    "name": "Yiwei Li",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b361",
                    "name": "Rong Zhou",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b362",
                    "user": {
                        "_id": "6481a16f70ac5e1968a7bb97",
                        "avatarUrl": "/avatars/39fd34fd2bfab8057961642d0ed80f09.svg",
                        "isPro": false,
                        "fullname": "Weixiang Sun",
                        "user": "Sweson",
                        "type": "user"
                    },
                    "name": "Weixiang Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:41:02.920Z",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b363",
                    "name": "Zhennong Chen",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b364",
                    "name": "Sekeun Kim",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b365",
                    "name": "Hui Ren",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b366",
                    "name": "Tianming Liu",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b367",
                    "name": "Quanzheng Li",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b368",
                    "name": "Xiang Li",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b369",
                    "name": "Lifang He",
                    "hidden": false
                },
                {
                    "_id": "686dc7ebcb5725779c60b36a",
                    "name": "Lichao Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-04T16:30:38.000Z",
            "submittedOnDailyAt": "2025-07-09T00:08:40.667Z",
            "title": "SAMed-2: Selective Memory Enhanced Medical Segment Anything Model",
            "submittedOnDailyBy": {
                "_id": "619f01b8cc04eadf54fa5d5d",
                "avatarUrl": "/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg",
                "isPro": false,
                "fullname": "Song Dingjie",
                "user": "songdj",
                "type": "user"
            },
            "summary": "Recent \"segment anything\" efforts show promise by learning from large-scale\ndata, but adapting such models directly to medical images remains challenging\ndue to the complexity of medical data, noisy annotations, and continual\nlearning requirements across diverse modalities and anatomical structures. In\nthis work, we propose SAMed-2, a new foundation model for medical image\nsegmentation built upon the SAM-2 architecture. Specifically, we introduce a\ntemporal adapter into the image encoder to capture image correlations and a\nconfidence-driven memory mechanism to store high-certainty features for later\nretrieval. This memory-based strategy counters the pervasive noise in\nlarge-scale medical datasets and mitigates catastrophic forgetting when\nencountering new tasks or modalities. To train and evaluate SAMed-2, we curate\nMedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21\nmedical segmentation tasks. Our experiments on both internal benchmarks and 10\nexternal datasets demonstrate superior performance over state-of-the-art\nbaselines in multi-task scenarios. The code is available at:\nhttps://github.com/ZhilingYan/Medical-SAM-Bench.",
            "upvotes": 10,
            "discussionId": "686dc7eccb5725779c60b36b",
            "ai_summary": "SAMed-2, an adaptation of SAM-2 for medical image segmentation, incorporates a temporal adapter and confidence-driven memory to improve performance across diverse medical datasets and tasks.",
            "ai_keywords": [
                "SAM-2",
                "temporal adapter",
                "confidence-driven memory",
                "MedBank-100k",
                "medical segmentation",
                "catastrophic forgetting",
                "multi-task scenarios"
            ]
        },
        "publishedAt": "2025-07-04T12:30:38.000Z",
        "title": "SAMed-2: Selective Memory Enhanced Medical Segment Anything Model",
        "summary": "Recent \"segment anything\" efforts show promise by learning from large-scale\ndata, but adapting such models directly to medical images remains challenging\ndue to the complexity of medical data, noisy annotations, and continual\nlearning requirements across diverse modalities and anatomical structures. In\nthis work, we propose SAMed-2, a new foundation model for medical image\nsegmentation built upon the SAM-2 architecture. Specifically, we introduce a\ntemporal adapter into the image encoder to capture image correlations and a\nconfidence-driven memory mechanism to store high-certainty features for later\nretrieval. This memory-based strategy counters the pervasive noise in\nlarge-scale medical datasets and mitigates catastrophic forgetting when\nencountering new tasks or modalities. To train and evaluate SAMed-2, we curate\nMedBank-100k, a comprehensive dataset spanning seven imaging modalities and 21\nmedical segmentation tasks. Our experiments on both internal benchmarks and 10\nexternal datasets demonstrate superior performance over state-of-the-art\nbaselines in multi-task scenarios. The code is available at:\nhttps://github.com/ZhilingYan/Medical-SAM-Bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03698.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "619f01b8cc04eadf54fa5d5d",
            "avatarUrl": "/avatars/928f3d1a6146e2e1ae4860445d929d5c.svg",
            "fullname": "Song Dingjie",
            "name": "songdj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.05920",
            "authors": [
                {
                    "_id": "686deb33cb5725779c60b49c",
                    "name": "Xinyu Huang",
                    "hidden": false
                },
                {
                    "_id": "686deb33cb5725779c60b49d",
                    "name": "Yuhao Dong",
                    "hidden": false
                },
                {
                    "_id": "686deb33cb5725779c60b49e",
                    "name": "Weiwei Tian",
                    "hidden": false
                },
                {
                    "_id": "686deb33cb5725779c60b49f",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "686deb33cb5725779c60b4a0",
                    "name": "Rui Feng",
                    "hidden": false
                },
                {
                    "_id": "686deb33cb5725779c60b4a1",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T12:05:05.000Z",
            "submittedOnDailyAt": "2025-07-09T02:42:14.123Z",
            "title": "High-Resolution Visual Reasoning via Multi-Turn Grounding-Based\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "63bf7ba8da08ed0544ff20e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673493776367-63bf7ba8da08ed0544ff20e9.jpeg",
                "isPro": false,
                "fullname": "Xinyu Huang",
                "user": "xinyu1205",
                "type": "user"
            },
            "summary": "State-of-the-art large multi-modal models (LMMs) face challenges when\nprocessing high-resolution images, as these inputs are converted into enormous\nvisual tokens, many of which are irrelevant to the downstream task. In this\npaper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an\nend-to-end reinforcement learning (RL) framework that enables LMMs to\niteratively focus on key visual regions by automatically cropping sub-images,\nbased on model-predicted grounding coordinates within a multi-turn conversation\nframework. Compared to supervised fine-tuning (SFT), which requires costly\nadditional grounding annotations, our approach highlights that LMMs can emerge\nrobust grounding abilities during the RL training process, leveraging only a\nbinary reward function derived from the correctness of the final answer.\nAdditionally, we observe that LMMs struggle to autonomously trigger visual\ngrounding during the rollout process. To address this cold start problem, we\ndesign a multi-turn conversational template and restrict policy loss\ncomputation to model outputs generated across multiple dialogue rounds, thereby\npromoting stable optimization. Extensive experiments demonstrate that, when\ntrained on standard visual-question-short answering data without grounding\nannotations, MGPO effectively elicits stronger grounding capabilities compared\nto GRPO, leading to 5.4\\% improvement on in-distribution MME-Realworld and\n5.2\\% improvement on the challenging out-of-distribution (OOD) V* Bench.\nNotably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses\nOpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at\nhttps://github.com/EvolvingLMMs-Lab/MGPO.",
            "upvotes": 9,
            "discussionId": "686deb34cb5725779c60b4a2",
            "ai_summary": "MGPO, an end-to-end reinforcement learning framework, enhances large multi-modal models' ability to focus on key visual regions without requiring additional grounding annotations, improving performance on both in-distribution and out-of-distribution benchmarks.",
            "ai_keywords": [
                "Multi-turn Grounding-based Policy Optimization",
                "reinforcement learning",
                "LMMs",
                "large multi-modal models",
                "high-resolution images",
                "visual tokens",
                "grounding coordinates",
                "multi-turn conversation",
                "supervised fine-tuning",
                "binary reward function",
                "visual grounding",
                "policy loss",
                "multi-turn conversational template",
                "stable optimization",
                "MME-Realworld",
                "V* Bench",
                "Qwen2.5-VL-7B",
                "OpenAI's o1",
                "GPT-4o"
            ]
        },
        "publishedAt": "2025-07-08T08:05:05.000Z",
        "title": "High-Resolution Visual Reasoning via Multi-Turn Grounding-Based\n  Reinforcement Learning",
        "summary": "State-of-the-art large multi-modal models (LMMs) face challenges when\nprocessing high-resolution images, as these inputs are converted into enormous\nvisual tokens, many of which are irrelevant to the downstream task. In this\npaper, we propose Multi-turn Grounding-based Policy Optimization (MGPO), an\nend-to-end reinforcement learning (RL) framework that enables LMMs to\niteratively focus on key visual regions by automatically cropping sub-images,\nbased on model-predicted grounding coordinates within a multi-turn conversation\nframework. Compared to supervised fine-tuning (SFT), which requires costly\nadditional grounding annotations, our approach highlights that LMMs can emerge\nrobust grounding abilities during the RL training process, leveraging only a\nbinary reward function derived from the correctness of the final answer.\nAdditionally, we observe that LMMs struggle to autonomously trigger visual\ngrounding during the rollout process. To address this cold start problem, we\ndesign a multi-turn conversational template and restrict policy loss\ncomputation to model outputs generated across multiple dialogue rounds, thereby\npromoting stable optimization. Extensive experiments demonstrate that, when\ntrained on standard visual-question-short answering data without grounding\nannotations, MGPO effectively elicits stronger grounding capabilities compared\nto GRPO, leading to 5.4\\% improvement on in-distribution MME-Realworld and\n5.2\\% improvement on the challenging out-of-distribution (OOD) V* Bench.\nNotably, MGPO post-training on Qwen2.5-VL-7B with 21K samples surpasses\nOpenAI's o1 and GPT-4o models on the OOD V* Bench. Codes are available at\nhttps://github.com/EvolvingLMMs-Lab/MGPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05920.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63bf7ba8da08ed0544ff20e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673493776367-63bf7ba8da08ed0544ff20e9.jpeg",
            "fullname": "Xinyu Huang",
            "name": "xinyu1205",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 43
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.05963",
            "authors": [
                {
                    "_id": "686dde62cb5725779c60b467",
                    "name": "Zhenghao Zhang",
                    "hidden": false
                },
                {
                    "_id": "686dde62cb5725779c60b468",
                    "name": "Junchao Liao",
                    "hidden": false
                },
                {
                    "_id": "686dde62cb5725779c60b469",
                    "name": "Xiangyu Meng",
                    "hidden": false
                },
                {
                    "_id": "686dde62cb5725779c60b46a",
                    "name": "Long Qin",
                    "hidden": false
                },
                {
                    "_id": "686dde62cb5725779c60b46b",
                    "name": "Weizhi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T13:11:40.000Z",
            "submittedOnDailyAt": "2025-07-09T01:43:57.686Z",
            "title": "Tora2: Motion and Appearance Customized Diffusion Transformer for\n  Multi-Entity Video Generation",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "Recent advances in diffusion transformer models for motion-guided video\ngeneration, such as Tora, have shown significant progress. In this paper, we\npresent Tora2, an enhanced version of Tora, which introduces several design\nimprovements to expand its capabilities in both appearance and motion\ncustomization. Specifically, we introduce a decoupled personalization extractor\nthat generates comprehensive personalization embeddings for multiple open-set\nentities, better preserving fine-grained visual details compared to previous\nmethods. Building on this, we design a gated self-attention mechanism to\nintegrate trajectory, textual description, and visual information for each\nentity. This innovation significantly reduces misalignment in multimodal\nconditioning during training. Moreover, we introduce a contrastive loss that\njointly optimizes trajectory dynamics and entity consistency through explicit\nmapping between motion and personalization embeddings. Tora2 is, to our best\nknowledge, the first method to achieve simultaneous multi-entity customization\nof appearance and motion for video generation. Experimental results demonstrate\nthat Tora2 achieves competitive performance with state-of-the-art customization\nmethods while providing advanced motion control capabilities, which marks a\ncritical advancement in multi-condition video generation. Project page:\nhttps://github.com/alibaba/Tora .",
            "upvotes": 8,
            "discussionId": "686dde75cb5725779c60b46c",
            "ai_summary": "Tora2 enhances motion-guided video generation by introducing a decoupled personalization extractor, gated self-attention mechanism, and contrastive loss, enabling simultaneous multi-entity customization and advanced motion control.",
            "ai_keywords": [
                "diffusion transformer models",
                "Tora",
                "decoupled personalization extractor",
                "personalization embeddings",
                "gated self-attention mechanism",
                "trajectory dynamics",
                "entity consistency",
                "contrastive loss",
                "multi-entity customization",
                "motion control"
            ]
        },
        "publishedAt": "2025-07-08T09:11:40.000Z",
        "title": "Tora2: Motion and Appearance Customized Diffusion Transformer for\n  Multi-Entity Video Generation",
        "summary": "Recent advances in diffusion transformer models for motion-guided video\ngeneration, such as Tora, have shown significant progress. In this paper, we\npresent Tora2, an enhanced version of Tora, which introduces several design\nimprovements to expand its capabilities in both appearance and motion\ncustomization. Specifically, we introduce a decoupled personalization extractor\nthat generates comprehensive personalization embeddings for multiple open-set\nentities, better preserving fine-grained visual details compared to previous\nmethods. Building on this, we design a gated self-attention mechanism to\nintegrate trajectory, textual description, and visual information for each\nentity. This innovation significantly reduces misalignment in multimodal\nconditioning during training. Moreover, we introduce a contrastive loss that\njointly optimizes trajectory dynamics and entity consistency through explicit\nmapping between motion and personalization embeddings. Tora2 is, to our best\nknowledge, the first method to achieve simultaneous multi-entity customization\nof appearance and motion for video generation. Experimental results demonstrate\nthat Tora2 achieves competitive performance with state-of-the-art customization\nmethods while providing advanced motion control capabilities, which marks a\ncritical advancement in multi-condition video generation. Project page:\nhttps://github.com/alibaba/Tora .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05963.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 56
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.04723",
            "authors": [
                {
                    "_id": "686df5fecb5725779c60b4b8",
                    "name": "Zecheng Tang",
                    "hidden": false
                },
                {
                    "_id": "686df5fecb5725779c60b4b9",
                    "name": "Haitian Wang",
                    "hidden": false
                },
                {
                    "_id": "686df5fecb5725779c60b4ba",
                    "user": {
                        "_id": "6732fb1d24b316be87acaafe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6732fb1d24b316be87acaafe/BzD8HL4vhh3mfeSF3rm_1.jpeg",
                        "isPro": false,
                        "fullname": "Quantong Qiu",
                        "user": "QQTang1223",
                        "type": "user"
                    },
                    "name": "Quantong Qiu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:43.850Z",
                    "hidden": false
                },
                {
                    "_id": "686df5fecb5725779c60b4bb",
                    "name": "Baibei Ji",
                    "hidden": false
                },
                {
                    "_id": "686df5fecb5725779c60b4bc",
                    "name": "Ruoxi Sun",
                    "hidden": false
                },
                {
                    "_id": "686df5fecb5725779c60b4bd",
                    "name": "Keyan Zhou",
                    "hidden": false
                },
                {
                    "_id": "686df5fecb5725779c60b4be",
                    "name": "Juntao Li",
                    "hidden": false
                },
                {
                    "_id": "686df5fecb5725779c60b4bf",
                    "name": "Min Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T07:33:24.000Z",
            "submittedOnDailyAt": "2025-07-09T03:25:02.560Z",
            "title": "LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework",
            "submittedOnDailyBy": {
                "_id": "64096ef79e9f790c905b846d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
                "isPro": false,
                "fullname": "Zecheng Tang",
                "user": "ZetangForward",
                "type": "user"
            },
            "summary": "Long-context processing has become a fundamental capability for large\nlanguage models~(LLMs). To assess model's long-context performance, numerous\nlong-context evaluation benchmarks have been proposed. However, variations in\nevaluation settings across these benchmarks lead to inconsistent results,\nmaking it difficult to draw reliable comparisons. Besides, the high\ncomputational cost of long-context evaluation poses a significant barrier for\nthe community to conduct comprehensive assessments of long-context models. In\nthis paper, we propose LOOM-Scope, a comprehensive and efficient framework for\nlong-context evaluation. LOOM-Scope standardizes evaluation settings across\ndiverse benchmarks, supports deployment of efficient long-context inference\nacceleration methods, and introduces a holistic yet lightweight benchmark suite\nto evaluate models comprehensively. Homepage: https://loomscope.github.io",
            "upvotes": 8,
            "discussionId": "686df5fecb5725779c60b4c0",
            "projectPage": "https://loomscope.github.io/",
            "githubRepo": "https://github.com/LCM-Lab/LOOM-Scope",
            "githubStars": 11
        },
        "publishedAt": "2025-07-07T03:33:24.000Z",
        "title": "LOOM-Scope: a comprehensive and efficient LOng-cOntext Model evaluation\n  framework",
        "summary": "Long-context processing has become a fundamental capability for large\nlanguage models~(LLMs). To assess model's long-context performance, numerous\nlong-context evaluation benchmarks have been proposed. However, variations in\nevaluation settings across these benchmarks lead to inconsistent results,\nmaking it difficult to draw reliable comparisons. Besides, the high\ncomputational cost of long-context evaluation poses a significant barrier for\nthe community to conduct comprehensive assessments of long-context models. In\nthis paper, we propose LOOM-Scope, a comprehensive and efficient framework for\nlong-context evaluation. LOOM-Scope standardizes evaluation settings across\ndiverse benchmarks, supports deployment of efficient long-context inference\nacceleration methods, and introduces a holistic yet lightweight benchmark suite\nto evaluate models comprehensively. Homepage: https://loomscope.github.io",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04723.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64096ef79e9f790c905b846d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64096ef79e9f790c905b846d/hVzw656lXdzbCxTtnheud.jpeg",
            "fullname": "Zecheng Tang",
            "name": "ZetangForward",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06204",
            "authors": [
                {
                    "_id": "686e0c68cb5725779c60b4ed",
                    "name": "Nadav Schneider",
                    "hidden": false
                },
                {
                    "_id": "686e0c68cb5725779c60b4ee",
                    "name": "Itamar Zimerman",
                    "hidden": false
                },
                {
                    "_id": "686e0c68cb5725779c60b4ef",
                    "name": "Eliya Nachmani",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/D6ZhMrqrVT28Kh25ZySih.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/qqCHBOm_ZYidhb_WFlZ1-.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/kuXrDKc48bZzbeC1l1Si8.png",
                "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/e5wW0wTXLV1FOgG0pzEcl.png"
            ],
            "publishedAt": "2025-07-08T17:30:14.000Z",
            "submittedOnDailyAt": "2025-07-09T05:06:48.447Z",
            "title": "Differential Mamba",
            "submittedOnDailyBy": {
                "_id": "65ce0b4a03a8179f5da5d4ef",
                "avatarUrl": "/avatars/c0b68efb885486c5180d1d69c4e317ac.svg",
                "isPro": false,
                "fullname": "Nadav Schneider",
                "user": "nadavsc",
                "type": "user"
            },
            "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available.",
            "upvotes": 7,
            "discussionId": "686e0c68cb5725779c60b4f0",
            "ai_summary": "A novel differential mechanism for Mamba, a selective state-space layer architecture, improves retrieval capabilities and performance by addressing overallocation issues.",
            "ai_keywords": [
                "Transformers",
                "RNNs",
                "differential design",
                "Mamba",
                "selective state-space layers",
                "language modeling benchmarks",
                "ablation studies",
                "empirical analyses"
            ]
        },
        "publishedAt": "2025-07-08T13:30:14.000Z",
        "title": "Differential Mamba",
        "summary": "Sequence models like Transformers and RNNs often overallocate attention to\nirrelevant context, leading to noisy intermediate representations. This\ndegrades LLM capabilities by promoting hallucinations, weakening long-range and\nretrieval abilities, and reducing robustness. Recent work has shown that\ndifferential design can mitigate this issue in Transformers, improving their\neffectiveness across various applications. In this paper, we explore whether\nthese techniques, originally developed for Transformers, can be applied to\nMamba, a recent architecture based on selective state-space layers that\nachieves Transformer-level performance with greater efficiency. We show that a\nnaive adaptation of differential design to Mamba is insufficient and requires\ncareful architectural modifications. To address this, we introduce a novel\ndifferential mechanism for Mamba, empirically validated on language modeling\nbenchmarks, demonstrating improved retrieval capabilities and superior\nperformance over vanilla Mamba. Finally, we conduct extensive ablation studies\nand empirical analyses to justify our design choices and provide evidence that\nour approach effectively mitigates the overallocation problem in Mamba-based\nmodels. Our code is publicly available.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/D6ZhMrqrVT28Kh25ZySih.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/qqCHBOm_ZYidhb_WFlZ1-.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/kuXrDKc48bZzbeC1l1Si8.png",
            "https://cdn-uploads.huggingface.co/production/uploads/65ce0b4a03a8179f5da5d4ef/e5wW0wTXLV1FOgG0pzEcl.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06204.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65ce0b4a03a8179f5da5d4ef",
            "avatarUrl": "/avatars/c0b68efb885486c5180d1d69c4e317ac.svg",
            "fullname": "Nadav Schneider",
            "name": "nadavsc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.05578",
            "authors": [
                {
                    "_id": "686dca00cb5725779c60b379",
                    "name": "Alexander Xiong",
                    "hidden": false
                },
                {
                    "_id": "686dca00cb5725779c60b37a",
                    "name": "Xuandong Zhao",
                    "hidden": false
                },
                {
                    "_id": "686dca00cb5725779c60b37b",
                    "name": "Aneesh Pappu",
                    "hidden": false
                },
                {
                    "_id": "686dca00cb5725779c60b37c",
                    "name": "Dawn Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T01:30:46.000Z",
            "submittedOnDailyAt": "2025-07-09T00:17:12.585Z",
            "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and\n  Mitigation",
            "submittedOnDailyBy": {
                "_id": "6275a465597c70eb8949fce5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
                "isPro": false,
                "fullname": "Xuandong Zhao",
                "user": "Xuandong",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work.",
            "upvotes": 4,
            "discussionId": "686dca01cb5725779c60b37d",
            "ai_summary": "The paper reviews recent studies on memorization in Large Language Models, exploring factors that influence memorization, detection methodologies, and mitigation strategies, while addressing privacy and ethical implications.",
            "ai_keywords": [
                "Large Language Models",
                "memorization",
                "training data duplication",
                "training dynamics",
                "fine-tuning procedures",
                "prefix-based extraction",
                "membership inference",
                "adversarial prompting",
                "data cleaning",
                "differential privacy",
                "post-training unlearning"
            ]
        },
        "publishedAt": "2025-07-07T21:30:46.000Z",
        "title": "The Landscape of Memorization in LLMs: Mechanisms, Measurement, and\n  Mitigation",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\na wide range of tasks, yet they also exhibit memorization of their training\ndata. This phenomenon raises critical questions about model behavior, privacy\nrisks, and the boundary between learning and memorization. Addressing these\nconcerns, this paper synthesizes recent studies and investigates the landscape\nof memorization, the factors influencing it, and methods for its detection and\nmitigation. We explore key drivers, including training data duplication,\ntraining dynamics, and fine-tuning procedures that influence data memorization.\nIn addition, we examine methodologies such as prefix-based extraction,\nmembership inference, and adversarial prompting, assessing their effectiveness\nin detecting and measuring memorized content. Beyond technical analysis, we\nalso explore the broader implications of memorization, including the legal and\nethical implications. Finally, we discuss mitigation strategies, including data\ncleaning, differential privacy, and post-training unlearning, while\nhighlighting open challenges in balancing the minimization of harmful\nmemorization with utility. This paper provides a comprehensive overview of the\ncurrent state of research on LLM memorization across technical, privacy, and\nperformance dimensions, identifying critical directions for future work.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05578.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6275a465597c70eb8949fce5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
            "fullname": "Xuandong Zhao",
            "name": "Xuandong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.04610",
            "authors": [
                {
                    "_id": "686dcc0bcb5725779c60b38f",
                    "user": {
                        "_id": "63c9725ebedad7e2bf160bdc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c9725ebedad7e2bf160bdc/wzPuyhOXCYBNGwZDshbnL.jpeg",
                        "isPro": false,
                        "fullname": "Mostafa Elhoushi",
                        "user": "melhoushi",
                        "type": "user"
                    },
                    "name": "Mostafa Elhoushi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:50:25.543Z",
                    "hidden": false
                },
                {
                    "_id": "686dcc0bcb5725779c60b390",
                    "name": "Jeff Johnson",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T01:59:47.000Z",
            "submittedOnDailyAt": "2025-07-09T00:32:16.318Z",
            "title": "any4: Learned 4-bit Numeric Representation for LLMs",
            "submittedOnDailyBy": {
                "_id": "63c9725ebedad7e2bf160bdc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c9725ebedad7e2bf160bdc/wzPuyhOXCYBNGwZDshbnL.jpeg",
                "isPro": false,
                "fullname": "Mostafa Elhoushi",
                "user": "melhoushi",
                "type": "user"
            },
            "summary": "We present any4, a learned 4-bit weight quantization solution for large\nlanguage models (LLMs) providing arbitrary numeric representations without\nrequiring pre-processing of weights or activations. any4 yields higher accuracy\ncompared to other related 4-bit numeric representation types: int4, fp4 and\nnf4, as evaluated on a range of model sizes, generations and families (Llama 2,\nLlama 3, Mistral and Mixtral). While any4 does not require preprocessing of\nweights or activations, it is also competitive with orthogonal techniques that\nrequire such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3\nand any2 and show competitiveness at lower bits. Additionally, we show that we\ncan calibrate using a single curated diverse sample rather than hundreds of\nsamples from a dataset as done in most quantization approaches. We also open\nsource tinygemm, a latency optimized GPU matrix multiplication library for\nLLMs, that implements any4 using a GPU-efficient lookup table strategy along\nwith other common quantization methods. We open source our code at\nhttps://github.com/facebookresearch/any4 .",
            "upvotes": 4,
            "discussionId": "686dcc0ccb5725779c60b391",
            "githubRepo": "https://github.com/facebookresearch/any4",
            "ai_summary": "any4 is a learned 4-bit weight quantization method for LLMs that achieves high accuracy without preprocessing and uses a GPU-efficient lookup table strategy.",
            "ai_keywords": [
                "weight quantization",
                "LLMs",
                "int4",
                "fp4",
                "nf4",
                "AWQ",
                "GPTQ",
                "calibration",
                "GPU matrix multiplication",
                "lookup table strategy"
            ],
            "githubStars": 21
        },
        "publishedAt": "2025-07-06T21:59:47.000Z",
        "title": "any4: Learned 4-bit Numeric Representation for LLMs",
        "summary": "We present any4, a learned 4-bit weight quantization solution for large\nlanguage models (LLMs) providing arbitrary numeric representations without\nrequiring pre-processing of weights or activations. any4 yields higher accuracy\ncompared to other related 4-bit numeric representation types: int4, fp4 and\nnf4, as evaluated on a range of model sizes, generations and families (Llama 2,\nLlama 3, Mistral and Mixtral). While any4 does not require preprocessing of\nweights or activations, it is also competitive with orthogonal techniques that\nrequire such preprocessing (e.g., AWQ and GPTQ). We also experiment with any3\nand any2 and show competitiveness at lower bits. Additionally, we show that we\ncan calibrate using a single curated diverse sample rather than hundreds of\nsamples from a dataset as done in most quantization approaches. We also open\nsource tinygemm, a latency optimized GPU matrix multiplication library for\nLLMs, that implements any4 using a GPU-efficient lookup table strategy along\nwith other common quantization methods. We open source our code at\nhttps://github.com/facebookresearch/any4 .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04610.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63c9725ebedad7e2bf160bdc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c9725ebedad7e2bf160bdc/wzPuyhOXCYBNGwZDshbnL.jpeg",
            "fullname": "Mostafa Elhoushi",
            "name": "melhoushi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 34
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06230",
            "authors": [
                {
                    "_id": "686dfd05cb5725779c60b4c8",
                    "user": {
                        "_id": "685e6ef1882f117bc2b7d981",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/MU2Bgys-5qADLivaU__BX.jpeg",
                        "isPro": false,
                        "fullname": "Aleksandar Jevtić",
                        "user": "jev-aleks",
                        "type": "user"
                    },
                    "name": "Aleksandar Jevtić",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:39.517Z",
                    "hidden": false
                },
                {
                    "_id": "686dfd05cb5725779c60b4c9",
                    "user": {
                        "_id": "67f001a5f824e54a66563455",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AG6K_dOseLcc84tHP7V8D.png",
                        "isPro": false,
                        "fullname": "Christoph Reich",
                        "user": "ChristophReich1996",
                        "type": "user"
                    },
                    "name": "Christoph Reich",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:40:58.573Z",
                    "hidden": false
                },
                {
                    "_id": "686dfd05cb5725779c60b4ca",
                    "name": "Felix Wimbauer",
                    "hidden": false
                },
                {
                    "_id": "686dfd05cb5725779c60b4cb",
                    "user": {
                        "_id": "636020de88e41d249eccf68c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636020de88e41d249eccf68c/pMduOM5UNZo4JW2Ckk2jK.jpeg",
                        "isPro": false,
                        "fullname": "Oliver Hahn",
                        "user": "olvrhhn",
                        "type": "user"
                    },
                    "name": "Oliver Hahn",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:40:56.536Z",
                    "hidden": false
                },
                {
                    "_id": "686dfd05cb5725779c60b4cc",
                    "name": "Christian Rupprecht",
                    "hidden": false
                },
                {
                    "_id": "686dfd05cb5725779c60b4cd",
                    "name": "Stefan Roth",
                    "hidden": false
                },
                {
                    "_id": "686dfd05cb5725779c60b4ce",
                    "name": "Daniel Cremers",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67f001a5f824e54a66563455/tWGn_HDWzVq86RPLGUN0B.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/67f001a5f824e54a66563455/FmlTuOyyjNpx_MZk_i3ER.mp4"
            ],
            "publishedAt": "2025-07-08T17:59:50.000Z",
            "submittedOnDailyAt": "2025-07-09T10:50:51.310Z",
            "title": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion",
            "submittedOnDailyBy": {
                "_id": "67f001a5f824e54a66563455",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AG6K_dOseLcc84tHP7V8D.png",
                "isPro": false,
                "fullname": "Christoph Reich",
                "user": "ChristophReich1996",
                "type": "user"
            },
            "summary": "Semantic scene completion (SSC) aims to infer both the 3D geometry and\nsemantics of a scene from single images. In contrast to prior work on SSC that\nheavily relies on expensive ground-truth annotations, we approach SSC in an\nunsupervised setting. Our novel method, SceneDINO, adapts techniques from\nself-supervised representation learning and 2D unsupervised scene understanding\nto SSC. Our training exclusively utilizes multi-view consistency\nself-supervision without any form of semantic or geometric ground truth. Given\na single input image, SceneDINO infers the 3D geometry and expressive 3D DINO\nfeatures in a feed-forward manner. Through a novel 3D feature distillation\napproach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised\nscene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.\nLinear probing our 3D features matches the segmentation accuracy of a current\nsupervised SSC approach. Additionally, we showcase the domain generalization\nand multi-view consistency of SceneDINO, taking the first steps towards a\nstrong foundation for single image 3D scene understanding.",
            "upvotes": 3,
            "discussionId": "686dfd06cb5725779c60b4cf",
            "projectPage": "https://visinf.github.io/scenedino/",
            "githubRepo": "https://github.com/tum-vision/scenedino",
            "ai_summary": "SceneDINO achieves state-of-the-art segmentation accuracy in unsupervised semantic scene completion by leveraging self-supervised representation learning and 2D unsupervised scene understanding techniques.",
            "ai_keywords": [
                "self-supervised representation learning",
                "2D unsupervised scene understanding",
                "multi-view consistency",
                "feed-forward manner",
                "3D feature distillation",
                "domain generalization",
                "single image 3D scene understanding"
            ],
            "githubStars": 15
        },
        "publishedAt": "2025-07-08T13:59:50.000Z",
        "title": "Feed-Forward SceneDINO for Unsupervised Semantic Scene Completion",
        "summary": "Semantic scene completion (SSC) aims to infer both the 3D geometry and\nsemantics of a scene from single images. In contrast to prior work on SSC that\nheavily relies on expensive ground-truth annotations, we approach SSC in an\nunsupervised setting. Our novel method, SceneDINO, adapts techniques from\nself-supervised representation learning and 2D unsupervised scene understanding\nto SSC. Our training exclusively utilizes multi-view consistency\nself-supervision without any form of semantic or geometric ground truth. Given\na single input image, SceneDINO infers the 3D geometry and expressive 3D DINO\nfeatures in a feed-forward manner. Through a novel 3D feature distillation\napproach, we obtain unsupervised 3D semantics. In both 3D and 2D unsupervised\nscene understanding, SceneDINO reaches state-of-the-art segmentation accuracy.\nLinear probing our 3D features matches the segmentation accuracy of a current\nsupervised SSC approach. Additionally, we showcase the domain generalization\nand multi-view consistency of SceneDINO, taking the first steps towards a\nstrong foundation for single image 3D scene understanding.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67f001a5f824e54a66563455/tWGn_HDWzVq86RPLGUN0B.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/67f001a5f824e54a66563455/FmlTuOyyjNpx_MZk_i3ER.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06230.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67f001a5f824e54a66563455",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/AG6K_dOseLcc84tHP7V8D.png",
            "fullname": "Christoph Reich",
            "name": "ChristophReich1996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.05201",
            "authors": [
                {
                    "_id": "686dbc3bcb5725779c60b2b1",
                    "name": "Andrew Sellergren",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2b2",
                    "name": "Sahar Kazemzadeh",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2b3",
                    "name": "Tiam Jaroensri",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2b4",
                    "name": "Atilla Kiraly",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2b5",
                    "name": "Madeleine Traverse",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2b6",
                    "name": "Timo Kohlberger",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2b7",
                    "name": "Shawn Xu",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2b8",
                    "name": "Fayaz Jamil",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2b9",
                    "name": "Cían Hughes",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2ba",
                    "name": "Charles Lau",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2bb",
                    "name": "Justin Chen",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2bc",
                    "name": "Fereshteh Mahvar",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2bd",
                    "name": "Liron Yatziv",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2be",
                    "name": "Tiffany Chen",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2bf",
                    "name": "Bram Sterling",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c0",
                    "name": "Stefanie Anna Baby",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c1",
                    "name": "Susanna Maria Baby",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c2",
                    "name": "Jeremy Lai",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c3",
                    "name": "Samuel Schmidgall",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c4",
                    "name": "Lu Yang",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c5",
                    "name": "Kejia Chen",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c6",
                    "name": "Per Bjornsson",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c7",
                    "name": "Shashir Reddy",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c8",
                    "name": "Ryan Brush",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2c9",
                    "name": "Kenneth Philbrick",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2ca",
                    "name": "Howard Hu",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2cb",
                    "name": "Howard Yang",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2cc",
                    "name": "Richa Tiwari",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2cd",
                    "name": "Sunny Jansen",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2ce",
                    "name": "Preeti Singh",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2cf",
                    "name": "Yun Liu",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d0",
                    "name": "Shekoofeh Azizi",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d1",
                    "name": "Aishwarya Kamath",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d2",
                    "name": "Johan Ferret",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d3",
                    "name": "Shreya Pathak",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d4",
                    "name": "Nino Vieillard",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d5",
                    "name": "Ramona Merhej",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d6",
                    "name": "Sarah Perrin",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d7",
                    "name": "Tatiana Matejovicova",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d8",
                    "name": "Alexandre Ramé",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2d9",
                    "name": "Morgane Riviere",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2da",
                    "name": "Louis Rouillard",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2db",
                    "name": "Thomas Mesnard",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2dc",
                    "name": "Geoffrey Cideron",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2dd",
                    "name": "Jean-bastien Grill",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2de",
                    "name": "Sabela Ramos",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2df",
                    "name": "Edouard Yvinec",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e0",
                    "name": "Michelle Casbon",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e1",
                    "name": "Elena Buchatskaya",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e2",
                    "name": "Jean-Baptiste Alayrac",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e3",
                    "name": "Dmitry",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e4",
                    "name": "Lepikhin",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e5",
                    "name": "Vlad Feinberg",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e6",
                    "name": "Sebastian Borgeaud",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e7",
                    "name": "Alek Andreev",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e8",
                    "name": "Cassidy Hardin",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2e9",
                    "name": "Robert Dadashi",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2ea",
                    "name": "Léonard Hussenot",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2eb",
                    "name": "Armand Joulin",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2ec",
                    "name": "Olivier Bachem",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2ed",
                    "name": "Yossi Matias",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2ee",
                    "name": "Katherine Chou",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2ef",
                    "name": "Avinatan Hassidim",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f0",
                    "name": "Kavi Goel",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f1",
                    "name": "Clement Farabet",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f2",
                    "name": "Joelle Barral",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f3",
                    "name": "Tris Warkentin",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f4",
                    "name": "Jonathon Shlens",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f5",
                    "name": "David Fleet",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f6",
                    "name": "Victor Cotruta",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f7",
                    "name": "Omar Sanseviero",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f8",
                    "name": "Gus Martins",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2f9",
                    "name": "Phoebe Kirk",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2fa",
                    "name": "Anand Rao",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2fb",
                    "name": "Shravya Shetty",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2fc",
                    "name": "David F. Steiner",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2fd",
                    "name": "Can Kirmizibayrak",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2fe",
                    "name": "Rory Pilgrim",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b2ff",
                    "name": "Daniel Golden",
                    "hidden": false
                },
                {
                    "_id": "686dbc3bcb5725779c60b300",
                    "name": "Lin Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T17:01:44.000Z",
            "submittedOnDailyAt": "2025-07-09T13:04:01.109Z",
            "title": "MedGemma Technical Report",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.",
            "upvotes": 3,
            "discussionId": "686dbc3bcb5725779c60b301",
            "ai_summary": "MedGemma, a collection of medical vision-language foundation models, demonstrates advanced understanding and reasoning in healthcare applications, improving performance across various tasks and maintaining general capabilities.",
            "ai_keywords": [
                "medical vision-language foundation models",
                "Gemma 3",
                "medical multimodal question answering",
                "chest X-ray finding classification",
                "agentic evaluations",
                "fine-tuning",
                "electronic health record information retrieval",
                "pneumothorax classification",
                "histopathology patch classification",
                "MedSigLIP",
                "vision encoder",
                "SigLIP",
                "medical image encoders"
            ]
        },
        "publishedAt": "2025-07-07T13:01:44.000Z",
        "title": "MedGemma Technical Report",
        "summary": "Artificial intelligence (AI) has significant potential in healthcare\napplications, but its training and deployment faces challenges due to\nhealthcare's diverse data, complex tasks, and the need to preserve privacy.\nFoundation models that perform well on medical tasks and require less\ntask-specific tuning data are critical to accelerate the development of\nhealthcare AI applications. We introduce MedGemma, a collection of medical\nvision-language foundation models based on Gemma 3 4B and 27B. MedGemma\ndemonstrates advanced medical understanding and reasoning on images and text,\nsignificantly exceeding the performance of similar-sized generative models and\napproaching the performance of task-specific models, while maintaining the\ngeneral capabilities of the Gemma 3 base models. For out-of-distribution tasks,\nMedGemma achieves 2.6-10% improvement on medical multimodal question answering,\n15.5-18.1% improvement on chest X-ray finding classification, and 10.8%\nimprovement on agentic evaluations compared to the base models. Fine-tuning\nMedGemma further improves performance in subdomains, reducing errors in\nelectronic health record information retrieval by 50% and reaching comparable\nperformance to existing specialized state-of-the-art methods for pneumothorax\nclassification and histopathology patch classification. We additionally\nintroduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.\nMedSigLIP powers the visual understanding capabilities of MedGemma and as an\nencoder achieves comparable or better performance than specialized medical\nimage encoders. Taken together, the MedGemma collection provides a strong\nfoundation of medical image and text capabilities, with potential to\nsignificantly accelerate medical research and development of downstream\napplications. The MedGemma collection, including tutorials and model weights,\ncan be found at https://goo.gle/medgemma.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05201.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 911
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07102",
            "authors": [
                {
                    "_id": "686f1302d938c25d68441af3",
                    "name": "Arnas Uselis",
                    "hidden": false
                },
                {
                    "_id": "686f1302d938c25d68441af4",
                    "name": "Andrea Dittadi",
                    "hidden": false
                },
                {
                    "_id": "686f1302d938c25d68441af5",
                    "name": "Seong Joon Oh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T17:59:03.000Z",
            "submittedOnDailyAt": "2025-07-09T23:47:22.235Z",
            "title": "Does Data Scaling Lead to Visual Compositional Generalization?",
            "submittedOnDailyBy": {
                "_id": "6520898f7bf8cc2dd28b7a9c",
                "avatarUrl": "/avatars/87a29ba95b71ee2dce18e97aa85e17a1.svg",
                "isPro": false,
                "fullname": "Arnas Uselis",
                "user": "Gigglingface",
                "type": "user"
            },
            "summary": "Compositional understanding is crucial for human intelligence, yet it remains\nunclear whether contemporary vision models exhibit it. The dominant machine\nlearning paradigm is built on the premise that scaling data and model sizes\nwill improve out-of-distribution performance, including compositional\ngeneralization. We test this premise through controlled experiments that\nsystematically vary data scale, concept diversity, and combination coverage. We\nfind that compositional generalization is driven by data diversity, not mere\ndata scale. Increased combinatorial coverage forces models to discover a\nlinearly factored representational structure, where concepts decompose into\nadditive components. We prove this structure is key to efficiency, enabling\nperfect generalization from few observed combinations. Evaluating pretrained\nmodels (DINO, CLIP), we find above-random yet imperfect performance, suggesting\npartial presence of this structure. Our work motivates stronger emphasis on\nconstructing diverse datasets for compositional generalization, and considering\nthe importance of representational structure that enables efficient\ncompositional learning. Code available at\nhttps://github.com/oshapio/visual-compositional-generalization.",
            "upvotes": 1,
            "discussionId": "686f1303d938c25d68441af6"
        },
        "publishedAt": "2025-07-09T13:59:03.000Z",
        "title": "Does Data Scaling Lead to Visual Compositional Generalization?",
        "summary": "Compositional understanding is crucial for human intelligence, yet it remains\nunclear whether contemporary vision models exhibit it. The dominant machine\nlearning paradigm is built on the premise that scaling data and model sizes\nwill improve out-of-distribution performance, including compositional\ngeneralization. We test this premise through controlled experiments that\nsystematically vary data scale, concept diversity, and combination coverage. We\nfind that compositional generalization is driven by data diversity, not mere\ndata scale. Increased combinatorial coverage forces models to discover a\nlinearly factored representational structure, where concepts decompose into\nadditive components. We prove this structure is key to efficiency, enabling\nperfect generalization from few observed combinations. Evaluating pretrained\nmodels (DINO, CLIP), we find above-random yet imperfect performance, suggesting\npartial presence of this structure. Our work motivates stronger emphasis on\nconstructing diverse datasets for compositional generalization, and considering\nthe importance of representational structure that enables efficient\ncompositional learning. Code available at\nhttps://github.com/oshapio/visual-compositional-generalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07102.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6520898f7bf8cc2dd28b7a9c",
            "avatarUrl": "/avatars/87a29ba95b71ee2dce18e97aa85e17a1.svg",
            "fullname": "Arnas Uselis",
            "name": "Gigglingface",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.03728",
            "authors": [
                {
                    "_id": "686e2930a5f0f70d9de40c52",
                    "user": {
                        "_id": "65baa31607366d903890bcf4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65baa31607366d903890bcf4/6M9WaawnvJ-2h5wSUic1I.jpeg",
                        "isPro": false,
                        "fullname": "ABDENNACER BADAOUI",
                        "user": "badaoui",
                        "type": "user"
                    },
                    "name": "Abdennacer Badaoui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:07.301Z",
                    "hidden": false
                },
                {
                    "_id": "686e2930a5f0f70d9de40c53",
                    "user": {
                        "_id": "64ac127b6ded799c42a61ff0",
                        "avatarUrl": "/avatars/963bb5cf03cbac8e0f5e0762c38e65df.svg",
                        "isPro": false,
                        "fullname": "Oussama Kharouiche",
                        "user": "Eliot04",
                        "type": "user"
                    },
                    "name": "Oussama Kharouiche",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T14:40:54.408Z",
                    "hidden": false
                },
                {
                    "_id": "686e2930a5f0f70d9de40c54",
                    "name": "Hatim Mrabet",
                    "hidden": false
                },
                {
                    "_id": "686e2930a5f0f70d9de40c55",
                    "name": "Daniele Malitesta",
                    "hidden": false
                },
                {
                    "_id": "686e2930a5f0f70d9de40c56",
                    "name": "Fragkiskos D. Malliaros",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-04T17:31:41.000Z",
            "submittedOnDailyAt": "2025-07-09T08:04:43.439Z",
            "title": "FAROS: Fair Graph Generation via Attribute Switching Mechanisms",
            "submittedOnDailyBy": {
                "_id": "65baa31607366d903890bcf4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65baa31607366d903890bcf4/6M9WaawnvJ-2h5wSUic1I.jpeg",
                "isPro": false,
                "fullname": "ABDENNACER BADAOUI",
                "user": "badaoui",
                "type": "user"
            },
            "summary": "Recent advancements in graph diffusion models (GDMs) have enabled the\nsynthesis of realistic network structures, yet ensuring fairness in the\ngenerated data remains a critical challenge. Existing solutions attempt to\nmitigate bias by re-training the GDMs with ad-hoc fairness constraints.\nConversely, with this work, we propose FAROS, a novel FAir graph geneRatiOn\nframework leveraging attribute Switching mechanisms and directly running in the\ngeneration process of the pre-trained GDM. Technically, our approach works by\naltering nodes' sensitive attributes during the generation. To this end, FAROS\ncalculates the optimal fraction of switching nodes, and selects the diffusion\nstep to perform the switch by setting tailored multi-criteria constraints to\npreserve the node-topology profile from the original distribution (a proxy for\naccuracy) while ensuring the edge independence on the sensitive attributes for\nthe generated graph (a proxy for fairness). Our experiments on benchmark\ndatasets for link prediction demonstrate that the proposed approach effectively\nreduces fairness discrepancies while maintaining comparable (or even higher)\naccuracy performance to other similar baselines. Noteworthy, FAROS is also able\nto strike a better accuracy-fairness trade-off than other competitors in some\nof the tested settings under the Pareto optimality concept, demonstrating the\neffectiveness of the imposed multi-criteria constraints.",
            "upvotes": 1,
            "discussionId": "686e2931a5f0f70d9de40c57",
            "ai_summary": "FAROS is a framework that enhances fairness in graph diffusion models by strategically switching node attributes during generation to balance accuracy and fairness.",
            "ai_keywords": [
                "graph diffusion models",
                "FAir graph geneRatiOn",
                "attribute Switching",
                "node-topology profile",
                "edge independence",
                "link prediction",
                "Pareto optimality"
            ]
        },
        "publishedAt": "2025-07-04T13:31:41.000Z",
        "title": "FAROS: Fair Graph Generation via Attribute Switching Mechanisms",
        "summary": "Recent advancements in graph diffusion models (GDMs) have enabled the\nsynthesis of realistic network structures, yet ensuring fairness in the\ngenerated data remains a critical challenge. Existing solutions attempt to\nmitigate bias by re-training the GDMs with ad-hoc fairness constraints.\nConversely, with this work, we propose FAROS, a novel FAir graph geneRatiOn\nframework leveraging attribute Switching mechanisms and directly running in the\ngeneration process of the pre-trained GDM. Technically, our approach works by\naltering nodes' sensitive attributes during the generation. To this end, FAROS\ncalculates the optimal fraction of switching nodes, and selects the diffusion\nstep to perform the switch by setting tailored multi-criteria constraints to\npreserve the node-topology profile from the original distribution (a proxy for\naccuracy) while ensuring the edge independence on the sensitive attributes for\nthe generated graph (a proxy for fairness). Our experiments on benchmark\ndatasets for link prediction demonstrate that the proposed approach effectively\nreduces fairness discrepancies while maintaining comparable (or even higher)\naccuracy performance to other similar baselines. Noteworthy, FAROS is also able\nto strike a better accuracy-fairness trade-off than other competitors in some\nof the tested settings under the Pareto optimality concept, demonstrating the\neffectiveness of the imposed multi-criteria constraints.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.03728.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65baa31607366d903890bcf4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65baa31607366d903890bcf4/6M9WaawnvJ-2h5wSUic1I.jpeg",
            "fullname": "ABDENNACER BADAOUI",
            "name": "badaoui",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 40
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06137",
            "authors": [
                {
                    "_id": "686ec992d938c25d68441a4d",
                    "name": "Mohammad Mahdi Derakhshani",
                    "hidden": false
                },
                {
                    "_id": "686ec992d938c25d68441a4e",
                    "name": "Dheeraj Varghese",
                    "hidden": false
                },
                {
                    "_id": "686ec992d938c25d68441a4f",
                    "name": "Marzieh Fadaee",
                    "hidden": false
                },
                {
                    "_id": "686ec992d938c25d68441a50",
                    "name": "Cees G. M. Snoek",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T16:19:45.000Z",
            "submittedOnDailyAt": "2025-07-09T18:28:42.600Z",
            "title": "NeoBabel: A Multilingual Open Tower for Visual Generation",
            "submittedOnDailyBy": {
                "_id": "634faa7673f99acaa3fffa01",
                "avatarUrl": "/avatars/52f0d4cc4bd0d6af467fd06a7672fc85.svg",
                "isPro": false,
                "fullname": "Mohammad Mahdi Derakhshani",
                "user": "mderakhshani",
                "type": "user"
            },
            "summary": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI.",
            "upvotes": 0,
            "discussionId": "686ec993d938c25d68441a51",
            "ai_summary": "NeoBabel, a multilingual image generation framework, achieves state-of-the-art performance across six languages while maintaining efficiency and cultural alignment, outperforming existing multilingual models.",
            "ai_keywords": [
                "multilingual image generation",
                "large-scale multilingual pretraining",
                "high-resolution instruction tuning",
                "m-GenEval",
                "m-DPG",
                "multilingual alignment",
                "code-mixed prompts",
                "crosslingual generalization",
                "inclusive AI research",
                "multilingual text-image pairs",
                "standardized multilingual evaluation protocols"
            ]
        },
        "publishedAt": "2025-07-08T12:19:45.000Z",
        "title": "NeoBabel: A Multilingual Open Tower for Visual Generation",
        "summary": "Text-to-image generation advancements have been predominantly\nEnglish-centric, creating barriers for non-English speakers and perpetuating\ndigital inequities. While existing systems rely on translation pipelines, these\nintroduce semantic drift, computational overhead, and cultural misalignment. We\nintroduce NeoBabel, a novel multilingual image generation framework that sets a\nnew Pareto frontier in performance, efficiency and inclusivity, supporting six\nlanguages: English, Chinese, Dutch, French, Hindi, and Persian. The model is\ntrained using a combination of large-scale multilingual pretraining and\nhigh-resolution instruction tuning. To evaluate its capabilities, we expand two\nEnglish-only benchmarks to multilingual equivalents: m-GenEval and m-DPG.\nNeoBabel achieves state-of-the-art multilingual performance while retaining\nstrong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG.\nNotably, it performs on par with leading models on English tasks while\noutperforming them by +0.11 and +0.09 on multilingual benchmarks, even though\nthese models are built on multilingual base LLMs. This demonstrates the\neffectiveness of our targeted alignment training for preserving and extending\ncrosslingual generalization. We further introduce two new metrics to rigorously\nassess multilingual alignment and robustness to code-mixed prompts. Notably,\nNeoBabel matches or exceeds English-only models while being 2-4x smaller. We\nrelease an open toolkit, including all code, model checkpoints, a curated\ndataset of 124M multilingual text-image pairs, and standardized multilingual\nevaluation protocols, to advance inclusive AI research. Our work demonstrates\nthat multilingual capability is not a trade-off but a catalyst for improved\nrobustness, efficiency, and cultural fidelity in generative AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06137.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "634faa7673f99acaa3fffa01",
            "avatarUrl": "/avatars/52f0d4cc4bd0d6af467fd06a7672fc85.svg",
            "fullname": "Mohammad Mahdi Derakhshani",
            "name": "mderakhshani",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.05411",
            "authors": [
                {
                    "_id": "686e2a81a5f0f70d9de40c59",
                    "name": "Mark Lee",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c5a",
                    "name": "Tom Gunter",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c5b",
                    "name": "Chang Lan",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c5c",
                    "name": "John Peebles",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c5d",
                    "name": "Hanzhi Zhou",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c5e",
                    "name": "Kelvin Zou",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c5f",
                    "name": "Sneha Bangalore",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c60",
                    "name": "Chung-Cheng Chiu",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c61",
                    "name": "Nan Du",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c62",
                    "name": "Xianzhi Du",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c63",
                    "name": "Philipp Dufter",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c64",
                    "name": "Ruixuan Hou",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c65",
                    "name": "Haoshuo Huang",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c66",
                    "name": "Dongseong Hwang",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c67",
                    "name": "Xiang Kong",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c68",
                    "name": "Jinhao Lei",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c69",
                    "name": "Tao Lei",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c6a",
                    "name": "Meng Li",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c6b",
                    "name": "Li Li",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c6c",
                    "name": "Jiarui Lu",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c6d",
                    "name": "Zhiyun Lu",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c6e",
                    "name": "Yiping Ma",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c6f",
                    "name": "David Qiu",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c70",
                    "name": "Vivek Rathod",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c71",
                    "name": "Senyu Tong",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c72",
                    "name": "Zhucheng Tu",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c73",
                    "name": "Jianyu Wang",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c74",
                    "name": "Yongqiang Wang",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c75",
                    "name": "Zirui Wang",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c76",
                    "name": "Floris Weers",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c77",
                    "name": "Sam Wiseman",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c78",
                    "name": "Guoli Yin",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c79",
                    "name": "Bowen Zhang",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c7a",
                    "name": "Xiyou Zhou",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c7b",
                    "name": "Danyang Zhuo",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c7c",
                    "name": "Cheng Leong",
                    "hidden": false
                },
                {
                    "_id": "686e2a81a5f0f70d9de40c7d",
                    "name": "Ruoming Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T18:50:58.000Z",
            "submittedOnDailyAt": "2025-07-09T07:09:03.056Z",
            "title": "AXLearn: Modular Large Model Training on Heterogeneous Infrastructure",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "We design and implement AXLearn, a production deep learning system that\nfacilitates scalable and high-performance training of large deep learning\nmodels. Compared to other state-of-the-art deep learning systems, AXLearn has a\nunique focus on modularity and support for heterogeneous hardware\ninfrastructure. AXLearn's internal interfaces between software components\nfollow strict encapsulation, allowing different components to be assembled to\nfacilitate rapid model development and experimentation on heterogeneous compute\ninfrastructure. We introduce a novel method of quantifying modularity via\nLines-of-Code (LoC)-complexity, which demonstrates how our system maintains\nconstant complexity as we scale the components in the system, compared to\nlinear or quadratic complexity in other systems. This allows integrating\nfeatures such as Rotary Position Embeddings (RoPE) into AXLearn across hundred\nof modules with just 10 lines of code, compared to hundreds as required in\nother systems. At the same time, AXLearn maintains equivalent performance\ncompared to state-of-the-art training systems. Finally, we share our experience\nin the development and operation of AXLearn.",
            "upvotes": 0,
            "discussionId": "686e2a82a5f0f70d9de40c7e",
            "githubRepo": "https://github.com/apple/axlearn",
            "ai_summary": "AXLearn is a modular deep learning system designed for scalable training on heterogeneous hardware, maintaining performance and modularity through efficient code integration methods.",
            "ai_keywords": [
                "modularity",
                "heterogeneous hardware infrastructure",
                "encapsulation",
                "Lines-of-Code (LoC)-complexity",
                "Rotary Position Embeddings (RoPE)"
            ],
            "githubStars": 2121
        },
        "publishedAt": "2025-07-07T14:50:58.000Z",
        "title": "AXLearn: Modular Large Model Training on Heterogeneous Infrastructure",
        "summary": "We design and implement AXLearn, a production deep learning system that\nfacilitates scalable and high-performance training of large deep learning\nmodels. Compared to other state-of-the-art deep learning systems, AXLearn has a\nunique focus on modularity and support for heterogeneous hardware\ninfrastructure. AXLearn's internal interfaces between software components\nfollow strict encapsulation, allowing different components to be assembled to\nfacilitate rapid model development and experimentation on heterogeneous compute\ninfrastructure. We introduce a novel method of quantifying modularity via\nLines-of-Code (LoC)-complexity, which demonstrates how our system maintains\nconstant complexity as we scale the components in the system, compared to\nlinear or quadratic complexity in other systems. This allows integrating\nfeatures such as Rotary Position Embeddings (RoPE) into AXLearn across hundred\nof modules with just 10 lines of code, compared to hundreds as required in\nother systems. At the same time, AXLearn maintains equivalent performance\ncompared to state-of-the-art training systems. Finally, we share our experience\nin the development and operation of AXLearn.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05411.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 911
        },
        "isAuthorParticipating": false
    }
]