[
    {
        "paper": {
            "id": "2507.07105",
            "authors": [
                {
                    "_id": "686f5cbad938c25d68441bb2",
                    "user": {
                        "_id": "643e9efa2263cdc630f88f5c",
                        "avatarUrl": "/avatars/96cea51f17e7d41ffb6a4b438e05f5cb.svg",
                        "isPro": false,
                        "fullname": "Yushen Zuo",
                        "user": "YSZuo",
                        "type": "user"
                    },
                    "name": "Yushen Zuo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T09:12:43.813Z",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bb3",
                    "name": "Qi Zheng",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bb4",
                    "user": {
                        "_id": "6736d289c8a9bf8f86936201",
                        "avatarUrl": "/avatars/ca56298f9db458ba65c469b1baabda2c.svg",
                        "isPro": false,
                        "fullname": "MingyangWu",
                        "user": "mingyang-wu",
                        "type": "user"
                    },
                    "name": "Mingyang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T13:14:02.465Z",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bb5",
                    "name": "Xinrui Jiang",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bb6",
                    "name": "Renjie Li",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bb7",
                    "name": "Jian Wang",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bb8",
                    "name": "Yide Zhang",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bb9",
                    "name": "Gengchen Mai",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bba",
                    "name": "Lihong V. Wang",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bbb",
                    "name": "James Zou",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bbc",
                    "name": "Xiaoyu Wang",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bbd",
                    "name": "Ming-Hsuan Yang",
                    "hidden": false
                },
                {
                    "_id": "686f5cbad938c25d68441bbe",
                    "user": {
                        "_id": "62548d5fef3debb2ddf91217",
                        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
                        "isPro": false,
                        "fullname": "Zhengzhong Tu",
                        "user": "vztu",
                        "type": "user"
                    },
                    "name": "Zhengzhong Tu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T09:12:41.972Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ESnx_PS3_HUQoNDY5cSqI.png"
            ],
            "publishedAt": "2025-07-09T17:59:19.000Z",
            "submittedOnDailyAt": "2025-07-10T04:56:36.746Z",
            "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
            "submittedOnDailyBy": {
                "_id": "62548d5fef3debb2ddf91217",
                "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
                "isPro": false,
                "fullname": "Zhengzhong Tu",
                "user": "vztu",
                "type": "user"
            },
            "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.",
            "upvotes": 55,
            "discussionId": "686f5cbbd938c25d68441bbf",
            "projectPage": "https://4kagent.github.io/",
            "githubRepo": "https://github.com/taco-group/4KAgent",
            "ai_summary": "4KAgent, a unified agentic super-resolution system, enhances low-resolution images to 4K using profiling, perception, and restoration agents, achieving state-of-the-art performance across various imaging domains.",
            "ai_keywords": [
                "agentic super-resolution",
                "Profiling",
                "Perception Agent",
                "vision-language models",
                "image quality assessment",
                "Restoration Agent",
                "recursive execution-reflection",
                "quality-driven mixture-of-experts",
                "face restoration pipeline",
                "NIQE",
                "MUSIQ",
                "PSNR",
                "low-level vision tasks",
                "autonomous agents"
            ],
            "githubStars": 44
        },
        "publishedAt": "2025-07-09T13:59:19.000Z",
        "title": "4KAgent: Agentic Any Image to 4K Super-Resolution",
        "summary": "We present 4KAgent, a unified agentic super-resolution generalist system\ndesigned to universally upscale any image to 4K resolution (and even higher, if\napplied iteratively). Our system can transform images from extremely low\nresolutions with severe degradations, for example, highly distorted inputs at\n256x256, into crystal-clear, photorealistic 4K outputs. 4KAgent comprises three\ncore components: (1) Profiling, a module that customizes the 4KAgent pipeline\nbased on bespoke use cases; (2) A Perception Agent, which leverages\nvision-language models alongside image quality assessment experts to analyze\nthe input image and make a tailored restoration plan; and (3) A Restoration\nAgent, which executes the plan, following a recursive execution-reflection\nparadigm, guided by a quality-driven mixture-of-expert policy to select the\noptimal output for each step. Additionally, 4KAgent embeds a specialized face\nrestoration pipeline, significantly enhancing facial details in portrait and\nselfie photos. We rigorously evaluate our 4KAgent across 11 distinct task\ncategories encompassing a total of 26 diverse benchmarks, setting new\nstate-of-the-art on a broad spectrum of imaging domains. Our evaluations cover\nnatural images, portrait photos, AI-generated content, satellite imagery,\nfluorescence microscopy, and medical imaging like fundoscopy, ultrasound, and\nX-ray, demonstrating superior performance in terms of both perceptual (e.g.,\nNIQE, MUSIQ) and fidelity (e.g., PSNR) metrics. By establishing a novel agentic\nparadigm for low-level vision tasks, we aim to catalyze broader interest and\ninnovation within vision-centric autonomous agents across diverse research\ncommunities. We will release all the code, models, and results at:\nhttps://4kagent.github.io.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ESnx_PS3_HUQoNDY5cSqI.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07105.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62548d5fef3debb2ddf91217",
            "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
            "fullname": "Zhengzhong Tu",
            "name": "vztu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07095",
            "authors": [
                {
                    "_id": "686f2579d938c25d68441b43",
                    "name": "Ke Fan",
                    "hidden": false
                },
                {
                    "_id": "686f2579d938c25d68441b44",
                    "name": "Shunlin Lu",
                    "hidden": false
                },
                {
                    "_id": "686f2579d938c25d68441b45",
                    "user": {
                        "_id": "6853b71ec1be83a29eb5ba36",
                        "avatarUrl": "/avatars/ae205c2ec2c421a0d7851755b4f123a2.svg",
                        "isPro": false,
                        "fullname": "Minyue Dai",
                        "user": "Jixi111",
                        "type": "user"
                    },
                    "name": "Minyue Dai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T07:09:19.845Z",
                    "hidden": false
                },
                {
                    "_id": "686f2579d938c25d68441b46",
                    "name": "Runyi Yu",
                    "hidden": false
                },
                {
                    "_id": "686f2579d938c25d68441b47",
                    "name": "Lixing Xiao",
                    "hidden": false
                },
                {
                    "_id": "686f2579d938c25d68441b48",
                    "name": "Zhiyang Dou",
                    "hidden": false
                },
                {
                    "_id": "686f2579d938c25d68441b49",
                    "name": "Junting Dong",
                    "hidden": false
                },
                {
                    "_id": "686f2579d938c25d68441b4a",
                    "name": "Lizhuang Ma",
                    "hidden": false
                },
                {
                    "_id": "686f2579d938c25d68441b4b",
                    "name": "Jingbo Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T17:52:04.000Z",
            "submittedOnDailyAt": "2025-07-10T01:12:54.854Z",
            "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
            "submittedOnDailyBy": {
                "_id": "66d59dc9b005ad82ca6fc61d",
                "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
                "isPro": false,
                "fullname": "Runyi YU",
                "user": "IngridYU",
                "type": "user"
            },
            "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.",
            "upvotes": 40,
            "discussionId": "686f2579d938c25d68441b4c",
            "githubRepo": "https://github.com/VankouF/MotionMillion-Codes",
            "ai_summary": "A new dataset and evaluation framework improve zero-shot text-to-motion generation through a large-scale, high-quality dataset and a scalable model architecture.",
            "ai_keywords": [
                "MotionMillion",
                "MotionMillion-Eval",
                "zero-shot motion generation",
                "scalable architecture"
            ],
            "githubStars": 76
        },
        "publishedAt": "2025-07-09T13:52:04.000Z",
        "title": "Go to Zero: Towards Zero-shot Motion Generation with Million-scale Data",
        "summary": "Generating diverse and natural human motion sequences based on textual\ndescriptions constitutes a fundamental and challenging research area within the\ndomains of computer vision, graphics, and robotics. Despite significant\nadvancements in this field, current methodologies often face challenges\nregarding zero-shot generalization capabilities, largely attributable to the\nlimited size of training datasets. Moreover, the lack of a comprehensive\nevaluation framework impedes the advancement of this task by failing to\nidentify directions for improvement. In this work, we aim to push\ntext-to-motion into a new era, that is, to achieve the generalization ability\nof zero-shot. To this end, firstly, we develop an efficient annotation pipeline\nand introduce MotionMillion-the largest human motion dataset to date, featuring\nover 2,000 hours and 2 million high-quality motion sequences. Additionally, we\npropose MotionMillion-Eval, the most comprehensive benchmark for evaluating\nzero-shot motion generation. Leveraging a scalable architecture, we scale our\nmodel to 7B parameters and validate its performance on MotionMillion-Eval. Our\nresults demonstrate strong generalization to out-of-domain and complex\ncompositional motions, marking a significant step toward zero-shot human motion\ngeneration. The code is available at\nhttps://github.com/VankouF/MotionMillion-Codes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07095.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "66d59dc9b005ad82ca6fc61d",
            "avatarUrl": "/avatars/0ba424690afd1144a89665c5bacdfde7.svg",
            "fullname": "Runyi YU",
            "name": "IngridYU",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06448",
            "authors": [
                {
                    "_id": "686f326dd938c25d68441b6a",
                    "name": "Zhenhailong Wang",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b6b",
                    "name": "Xuehang Guo",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b6c",
                    "name": "Sofia Stoica",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b6d",
                    "user": {
                        "_id": "645b10e80c73ea27d13f7aca",
                        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
                        "isPro": false,
                        "fullname": "xuhaiyang",
                        "user": "xhyandwyy",
                        "type": "user"
                    },
                    "name": "Haiyang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T07:09:17.834Z",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b6e",
                    "name": "Hongru Wang",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b6f",
                    "name": "Hyeonjeong Ha",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b70",
                    "name": "Xiusi Chen",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b71",
                    "name": "Yangyi Chen",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b72",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b73",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "686f326dd938c25d68441b74",
                    "name": "Heng Ji",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T23:22:34.000Z",
            "submittedOnDailyAt": "2025-07-10T01:56:27.555Z",
            "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
            "submittedOnDailyBy": {
                "_id": "628d7265db4cd1d1717c884f",
                "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
                "isPro": false,
                "fullname": "Zhenhailong Wang",
                "user": "mikewang",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.",
            "upvotes": 34,
            "discussionId": "686f326dd938c25d68441b75",
            "projectPage": "https://mikewangwzhl.github.io/PAPO",
            "githubRepo": "https://github.com/MikeWangWZHL/PAPO",
            "ai_summary": "Perception-Aware Policy Optimization (PAPO) enhances reinforcement learning with verifiable rewards for multimodal reasoning by integrating implicit perception loss, improving visual perception and reasoning.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "Large Language Models (LLMs)",
                "multimodal reasoning tasks",
                "Perception-Aware Policy Optimization (PAPO)",
                "GRPO",
                "Implicit Perception Loss",
                "KL divergence",
                "Double Entropy Loss",
                "visually grounded reasoning"
            ],
            "githubStars": 24
        },
        "publishedAt": "2025-07-08T19:22:34.000Z",
        "title": "Perception-Aware Policy Optimization for Multimodal Reasoning",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be a\nhighly effective strategy for endowing Large Language Models (LLMs) with robust\nmulti-step reasoning abilities. However, its design and optimizations remain\ntailored to purely textual domains, resulting in suboptimal performance when\napplied to multimodal reasoning tasks. In particular, we observe that a major\nsource of error in current multimodal reasoning lies in the perception of\nvisual inputs. To address this bottleneck, we propose Perception-Aware Policy\nOptimization (PAPO), a simple yet effective extension of GRPO that encourages\nthe model to learn to perceive while learning to reason, entirely from internal\nsupervision signals. Notably, PAPO does not rely on additional data curation,\nexternal reward models, or proprietary models. Specifically, we introduce the\nImplicit Perception Loss in the form of a KL divergence term to the GRPO\nobjective, which, despite its simplicity, yields significant overall\nimprovements (4.4%) on diverse multimodal benchmarks. The improvements are more\npronounced, approaching 8.0%, on tasks with high vision dependency. We also\nobserve a substantial reduction (30.5%) in perception errors, indicating\nimproved perceptual capabilities with PAPO. We conduct comprehensive analysis\nof PAPO and identify a unique loss hacking issue, which we rigorously analyze\nand mitigate through a Double Entropy Loss. Overall, our work introduces a\ndeeper integration of perception-aware supervision into RLVR learning\nobjectives and lays the groundwork for a new RL framework that encourages\nvisually grounded reasoning. Project page: https://mikewangwzhl.github.io/PAPO.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06448.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "628d7265db4cd1d1717c884f",
            "avatarUrl": "/avatars/dff2a3dd10d84b4a73fa486402de7219.svg",
            "fullname": "Zhenhailong Wang",
            "name": "mikewang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07957",
            "authors": [
                {
                    "_id": "68706310c8391850d60977de",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "68706310c8391850d60977df",
                    "name": "Xi Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T17:40:11.000Z",
            "submittedOnDailyAt": "2025-07-10T23:37:19.840Z",
            "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
            "submittedOnDailyBy": {
                "_id": "63234809155b0e2c44f354d6",
                "avatarUrl": "/avatars/60d38f8f0e12363f3f5e0388e635d7b6.svg",
                "isPro": false,
                "fullname": "Yu Wang",
                "user": "YuWangX",
                "type": "user"
            },
            "summary": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.",
            "upvotes": 26,
            "discussionId": "68706311c8391850d60977e0",
            "projectPage": "https://mirix.io/",
            "githubRepo": "https://github.com/Mirix-AI/MIRIX",
            "ai_summary": "MIRIX, a modular multi-agent memory system, enhances AI memory capabilities by integrating diverse memory types and a dynamic framework, achieving superior performance in multimodal and long-form conversation benchmarks.",
            "ai_keywords": [
                "MIRIX",
                "modular",
                "multi-agent memory system",
                "Core Memory",
                "Episodic Memory",
                "Semantic Memory",
                "Procedural Memory",
                "Resource Memory",
                "Knowledge Vault",
                "ScreenshotVQA",
                "LOCOMO",
                "memory-augmented LLM agents"
            ]
        },
        "publishedAt": "2025-07-10T13:40:11.000Z",
        "title": "MIRIX: Multi-Agent Memory System for LLM-Based Agents",
        "summary": "Although memory capabilities of AI agents are gaining increasing attention,\nexisting solutions remain fundamentally limited. Most rely on flat, narrowly\nscoped memory components, constraining their ability to personalize, abstract,\nand reliably recall user-specific information over time. To this end, we\nintroduce MIRIX, a modular, multi-agent memory system that redefines the future\nof AI memory by solving the field's most critical challenge: enabling language\nmodels to truly remember. Unlike prior approaches, MIRIX transcends text to\nembrace rich visual and multimodal experiences, making memory genuinely useful\nin real-world scenarios. MIRIX consists of six distinct, carefully structured\nmemory types: Core, Episodic, Semantic, Procedural, Resource Memory, and\nKnowledge Vault, coupled with a multi-agent framework that dynamically controls\nand coordinates updates and retrieval. This design enables agents to persist,\nreason over, and accurately retrieve diverse, long-term user data at scale. We\nvalidate MIRIX in two demanding settings. First, on ScreenshotVQA, a\nchallenging multimodal benchmark comprising nearly 20,000 high-resolution\ncomputer screenshots per sequence, requiring deep contextual understanding and\nwhere no existing memory systems can be applied, MIRIX achieves 35% higher\naccuracy than the RAG baseline while reducing storage requirements by 99.9%.\nSecond, on LOCOMO, a long-form conversation benchmark with single-modal textual\ninput, MIRIX attains state-of-the-art performance of 85.4%, far surpassing\nexisting baselines. These results show that MIRIX sets a new performance\nstandard for memory-augmented LLM agents. To allow users to experience our\nmemory system, we provide a packaged application powered by MIRIX. It monitors\nthe screen in real time, builds a personalized memory base, and offers\nintuitive visualization and secure local storage to ensure privacy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07957.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63234809155b0e2c44f354d6",
            "avatarUrl": "/avatars/60d38f8f0e12363f3f5e0388e635d7b6.svg",
            "fullname": "Yu Wang",
            "name": "YuWangX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06920",
            "authors": [
                {
                    "_id": "686f1da1d938c25d68441b1b",
                    "user": {
                        "_id": "677e869467f3bb8d8215eec6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
                        "isPro": false,
                        "fullname": "Zihan Ma",
                        "user": "MichaelErchi",
                        "type": "user"
                    },
                    "name": "Zihan Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T07:09:33.413Z",
                    "hidden": false
                },
                {
                    "_id": "686f1da1d938c25d68441b1c",
                    "user": {
                        "_id": "64c8835a4cc48498134364be",
                        "avatarUrl": "/avatars/a26120216483c6bf3b830c0cf9c4008e.svg",
                        "isPro": false,
                        "fullname": "TaolinZhang",
                        "user": "iridescentttt",
                        "type": "user"
                    },
                    "name": "Taolin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T13:14:31.682Z",
                    "hidden": false
                },
                {
                    "_id": "686f1da1d938c25d68441b1d",
                    "name": "Maosong Cao",
                    "hidden": false
                },
                {
                    "_id": "686f1da1d938c25d68441b1e",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "686f1da1d938c25d68441b1f",
                    "name": "Minnan Luo",
                    "hidden": false
                },
                {
                    "_id": "686f1da1d938c25d68441b20",
                    "user": {
                        "_id": "630716d11801ecc7d2595021",
                        "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
                        "isPro": false,
                        "fullname": "Songyang Zhang",
                        "user": "zsytony",
                        "type": "user"
                    },
                    "name": "Songyang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T13:14:29.511Z",
                    "hidden": false
                },
                {
                    "_id": "686f1da1d938c25d68441b21",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T14:58:47.000Z",
            "submittedOnDailyAt": "2025-07-10T00:31:10.862Z",
            "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
            "submittedOnDailyBy": {
                "_id": "677e869467f3bb8d8215eec6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
                "isPro": false,
                "fullname": "Zihan Ma",
                "user": "MichaelErchi",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
            "upvotes": 23,
            "discussionId": "686f1da1d938c25d68441b22",
            "ai_summary": "A human-LLM collaborative method enhances code generation test case generation, improving reliability and detection rates in code evaluation benchmarks.",
            "ai_keywords": [
                "large language models",
                "code-generation",
                "HumanEval",
                "LiveCodeBench",
                "test-case generation",
                "multi-dimensional metrics",
                "human-LLM collaboration",
                "SAGA",
                "TCGBench",
                "reinforcement learning frameworks",
                "verifiable rewards",
                "RLVR",
                "verifier accuracy",
                "adversarial test synthesis",
                "adaptive benchmark integration"
            ]
        },
        "publishedAt": "2025-07-09T10:58:47.000Z",
        "title": "Rethinking Verification for LLM Code Generation: From Generation to\n  Testing",
        "summary": "Large language models (LLMs) have recently achieved notable success in\ncode-generation benchmarks such as HumanEval and LiveCodeBench. However, a\ndetailed examination reveals that these evaluation suites often comprise only a\nlimited number of homogeneous test cases, resulting in subtle faults going\nundetected. This not only artificially inflates measured performance but also\ncompromises accurate reward estimation in reinforcement learning frameworks\nutilizing verifiable rewards (RLVR). To address these critical shortcomings, we\nsystematically investigate the test-case generation (TCG) task by proposing\nmulti-dimensional metrics designed to rigorously quantify test-suite\nthoroughness. Furthermore, we introduce a human-LLM collaborative method\n(SAGA), leveraging human programming expertise with LLM reasoning capability,\naimed at significantly enhancing both the coverage and the quality of generated\ntest cases. In addition, we develop a TCGBench to facilitate the study of the\nTCG task. Experiments show that SAGA achieves a detection rate of 90.62% and a\nverifier accuracy of 32.58% on TCGBench. The Verifier Accuracy (Verifier Acc)\nof the code generation evaluation benchmark synthesized by SAGA is 10.78%\nhigher than that of LiveCodeBench-v6. These results demonstrate the\neffectiveness of our proposed method. We hope this work contributes to building\na scalable foundation for reliable LLM code evaluation, further advancing RLVR\nin code generation, and paving the way for automated adversarial test synthesis\nand adaptive benchmark integration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06920.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "677e869467f3bb8d8215eec6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/677e869467f3bb8d8215eec6/kEC6JOKObgLHA22jRcP4H.jpeg",
            "fullname": "Zihan Ma",
            "name": "MichaelErchi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06457",
            "authors": [
                {
                    "_id": "686f2371d938c25d68441b36",
                    "name": "Dustin Wang",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b37",
                    "user": {
                        "_id": "63ff09f24852102d4871c19c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
                        "isPro": false,
                        "fullname": "Rui-Jie Zhu",
                        "user": "ridger",
                        "type": "user"
                    },
                    "name": "Rui-Jie Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T07:09:30.737Z",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b38",
                    "name": "Steven Abreu",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b39",
                    "name": "Yong Shan",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b3a",
                    "name": "Taylor Kergan",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b3b",
                    "name": "Yuqi Pan",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b3c",
                    "name": "Yuhong Chou",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b3d",
                    "name": "Zheng Li",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b3e",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b3f",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "686f2371d938c25d68441b40",
                    "name": "Jason Eshraghian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T23:54:11.000Z",
            "submittedOnDailyAt": "2025-07-10T02:04:35.136Z",
            "title": "A Systematic Analysis of Hybrid Linear Attention",
            "submittedOnDailyBy": {
                "_id": "63ff09f24852102d4871c19c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
                "isPro": false,
                "fullname": "Rui-Jie Zhu",
                "user": "ridger",
                "type": "user"
            },
            "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
            "upvotes": 18,
            "discussionId": "686f2371d938c25d68441b41",
            "projectPage": "https://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e",
            "ai_summary": "Research evaluates various linear attention models and their integration with full attention in Transformers, identifying key mechanisms like selective gating and hierarchical recurrence for enhanced recall performance.",
            "ai_keywords": [
                "quadratic complexity",
                "linear attention mechanisms",
                "full attention layers",
                "hybrid architectures",
                "vector recurrences",
                "gating mechanisms",
                "recall performance",
                "language modeling",
                "recall tasks",
                "selective gating",
                "hierarchical recurrence",
                "controlled forgetting",
                "HGRN-2",
                "GatedDeltaNet"
            ]
        },
        "publishedAt": "2025-07-08T19:54:11.000Z",
        "title": "A Systematic Analysis of Hybrid Linear Attention",
        "summary": "Transformers face quadratic complexity and memory issues with long sequences,\nprompting the adoption of linear attention mechanisms using fixed-size hidden\nstates. However, linear models often suffer from limited recall performance,\nleading to hybrid architectures that combine linear and full attention layers.\nDespite extensive hybrid architecture research, the choice of linear attention\ncomponent has not been deeply explored. We systematically evaluate various\nlinear attention models across generations - vector recurrences to advanced\ngating mechanisms - both standalone and hybridized. To enable this\ncomprehensive analysis, we trained and open-sourced 72 models: 36 at 340M\nparameters (20B tokens) and 36 at 1.3B parameters (100B tokens), covering six\nlinear attention variants across five hybridization ratios. Benchmarking on\nstandard language modeling and recall tasks reveals that superior standalone\nlinear models do not necessarily excel in hybrids. While language modeling\nremains stable across linear-to-full attention ratios, recall significantly\nimproves with increased full attention layers, particularly below a 3:1 ratio.\nOur study highlights selective gating, hierarchical recurrence, and controlled\nforgetting as critical for effective hybrid models. We recommend architectures\nsuch as HGRN-2 or GatedDeltaNet with a linear-to-full ratio between 3:1 and 6:1\nto achieve Transformer-level recall efficiently. Our models are open-sourced at\nhttps://huggingface.co/collections/m-a-p/hybrid-linear-attention-research-686c488a63d609d2f20e2b1e.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06457.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63ff09f24852102d4871c19c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ff09f24852102d4871c19c/lyE3xemtZss3qebK5sEXw.png",
            "fullname": "Rui-Jie Zhu",
            "name": "ridger",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07017",
            "authors": [
                {
                    "_id": "686f39e8d938c25d68441b89",
                    "user": {
                        "_id": "64ab99dcb76bfd863eba64c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ab99dcb76bfd863eba64c1/UBXwDPx17X-gl-SzBPvrc.jpeg",
                        "isPro": false,
                        "fullname": "TY.Zheng",
                        "user": "aaabiao",
                        "type": "user"
                    },
                    "name": "Tianyu Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T07:09:04.799Z",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b8a",
                    "user": {
                        "_id": "65d2251f98b4a470bf6a26e3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d2251f98b4a470bf6a26e3/C4T0LHYGejrI9mu_k3M8p.jpeg",
                        "isPro": false,
                        "fullname": "xts",
                        "user": "xtsssss",
                        "type": "user"
                    },
                    "name": "Tianshun Xing",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T09:12:45.801Z",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b8b",
                    "name": "Qingshui Gu",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b8c",
                    "name": "Taoran Liang",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b8d",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b8e",
                    "name": "Xin Zhou",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b8f",
                    "name": "Yizhi Li",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b90",
                    "name": "Zhoufutu Wen",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b91",
                    "name": "Chenghua Lin",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b92",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b93",
                    "user": {
                        "_id": "612ee6a7b960e78c6d2319d4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
                        "isPro": false,
                        "fullname": "Qian Liu",
                        "user": "SivilTaram",
                        "type": "user"
                    },
                    "name": "Qian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T13:14:27.307Z",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b94",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "686f39e8d938c25d68441b95",
                    "name": "Zejun Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T16:45:48.000Z",
            "submittedOnDailyAt": "2025-07-10T02:29:04.666Z",
            "title": "First Return, Entropy-Eliciting Explore",
            "submittedOnDailyBy": {
                "_id": "638efcf4c67af472d316d424",
                "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                "isPro": false,
                "fullname": "Ge Zhang",
                "user": "zhangysk",
                "type": "user"
            },
            "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
            "upvotes": 15,
            "discussionId": "686f39e8d938c25d68441b96",
            "ai_summary": "FR3E enhances LLM reasoning by providing structured exploration through targeted rollouts at high-uncertainty points, leading to more stable training and accurate responses.",
            "ai_keywords": [
                "Reinforcement Learning from Verifiable Rewards",
                "RLVR",
                "Large Language Models",
                "LLMs",
                "FR3E",
                "First Return",
                "Entropy-Eliciting Explore",
                "structured exploration",
                "high-uncertainty decision points",
                "reasoning trajectories",
                "targeted rollouts",
                "semantically grounded intermediate feedback",
                "mathematical reasoning benchmarks",
                "AIME24",
                "stable training",
                "coherent responses",
                "fully correct trajectories"
            ]
        },
        "publishedAt": "2025-07-09T12:45:48.000Z",
        "title": "First Return, Entropy-Eliciting Explore",
        "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) improves the reasoning\nabilities of Large Language Models (LLMs) but it struggles with unstable\nexploration. We propose FR3E (First Return, Entropy-Eliciting Explore), a\nstructured exploration framework that identifies high-uncertainty decision\npoints in reasoning trajectories and performs targeted rollouts to construct\nsemantically grounded intermediate feedback. Our method provides targeted\nguidance without relying on dense supervision. Empirical results on\nmathematical reasoning benchmarks(AIME24) show that FR3E promotes more stable\ntraining, produces longer and more coherent responses, and increases the\nproportion of fully correct trajectories. These results highlight the\nframework's effectiveness in improving LLM reasoning through more robust and\nstructured exploration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07017.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "638efcf4c67af472d316d424",
            "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
            "fullname": "Ge Zhang",
            "name": "zhangysk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 50
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.05687",
            "authors": [
                {
                    "_id": "686e2c00a5f0f70d9de40c8c",
                    "name": "Shangzhan Li",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c8d",
                    "name": "Zefan Wang",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c8e",
                    "name": "Ye He",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c8f",
                    "name": "Yuxuan Li",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c90",
                    "user": {
                        "_id": "62ccd26d376917c022420a46",
                        "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
                        "isPro": false,
                        "fullname": "Qi Shi",
                        "user": "qshi",
                        "type": "user"
                    },
                    "name": "Qi Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:03.237Z",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c91",
                    "name": "Jianling Li",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c92",
                    "name": "Yonggang Hu",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c93",
                    "name": "Wanxiang Che",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c94",
                    "name": "Xu Han",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c95",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "686e2c00a5f0f70d9de40c96",
                    "name": "Maosong Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T05:38:24.000Z",
            "submittedOnDailyAt": "2025-07-10T00:44:20.590Z",
            "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "62ccd26d376917c022420a46",
                "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
                "isPro": false,
                "fullname": "Qi Shi",
                "user": "qshi",
                "type": "user"
            },
            "summary": "Kernel development in deep learning requires optimizing computational units\nacross hardware while balancing memory management, parallelism, and\nhardware-specific optimizations through extensive empirical tuning. Although\ndomain-specific languages like Triton simplify GPU programming by abstracting\nlow-level details, developers must still manually tune critical parameters such\nas tile sizes and memory access patterns through iterative experimentation,\ncreating substantial barriers to optimal performance and wider adoption. In\nthis work, we introduce AutoTriton, the first model dedicated to Triton\nprogramming powered by reinforcement learning (RL). AutoTriton performs\nsupervised fine-tuning (SFT) to be equipped with essential Triton programming\nexpertise using a high-quality data gathering pipeline, and conducts RL with\nGroup Relative Policy Optimization (GRPO) algorithm, combining a rule-based\nreward and an execution-based reward to further improve Triton programming\nability, sequentially. Experiments across five evaluation channels of\nTritonBench and KernelBench illustrate that our 8B model AutoTriton achieves\nperformance comparable to mainstream large models, including Claude-4-Sonnet\nand DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial\nrole of each module within AutoTriton, including the SFT stage, the RL stage,\nand the reward design strategy. These findings underscore the promise of RL for\nautomatically generating high-performance kernels, and since high-performance\nkernels are core components of AI systems, this breakthrough establishes an\nimportant foundation for building more efficient AI systems. The model and code\nwill be available at https://github.com/AI9Stars/AutoTriton.",
            "upvotes": 13,
            "discussionId": "686e2c00a5f0f70d9de40c97"
        },
        "publishedAt": "2025-07-08T01:38:24.000Z",
        "title": "AutoTriton: Automatic Triton Programming with Reinforcement Learning in\n  LLMs",
        "summary": "Kernel development in deep learning requires optimizing computational units\nacross hardware while balancing memory management, parallelism, and\nhardware-specific optimizations through extensive empirical tuning. Although\ndomain-specific languages like Triton simplify GPU programming by abstracting\nlow-level details, developers must still manually tune critical parameters such\nas tile sizes and memory access patterns through iterative experimentation,\ncreating substantial barriers to optimal performance and wider adoption. In\nthis work, we introduce AutoTriton, the first model dedicated to Triton\nprogramming powered by reinforcement learning (RL). AutoTriton performs\nsupervised fine-tuning (SFT) to be equipped with essential Triton programming\nexpertise using a high-quality data gathering pipeline, and conducts RL with\nGroup Relative Policy Optimization (GRPO) algorithm, combining a rule-based\nreward and an execution-based reward to further improve Triton programming\nability, sequentially. Experiments across five evaluation channels of\nTritonBench and KernelBench illustrate that our 8B model AutoTriton achieves\nperformance comparable to mainstream large models, including Claude-4-Sonnet\nand DeepSeek-R1-0528. Further experimental analysis demonstrates the crucial\nrole of each module within AutoTriton, including the SFT stage, the RL stage,\nand the reward design strategy. These findings underscore the promise of RL for\nautomatically generating high-performance kernels, and since high-performance\nkernels are core components of AI systems, this breakthrough establishes an\nimportant foundation for building more efficient AI systems. The model and code\nwill be available at https://github.com/AI9Stars/AutoTriton.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05687.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62ccd26d376917c022420a46",
            "avatarUrl": "/avatars/629858b1c7419dbcdbead3484a36abd1.svg",
            "fullname": "Qi Shi",
            "name": "qshi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06804",
            "authors": [
                {
                    "_id": "686f1f4dd938c25d68441b24",
                    "name": "Zhenwen Liang",
                    "hidden": false
                },
                {
                    "_id": "686f1f4dd938c25d68441b25",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "686f1f4dd938c25d68441b26",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "686f1f4dd938c25d68441b27",
                    "name": "Tao Yang",
                    "hidden": false
                },
                {
                    "_id": "686f1f4dd938c25d68441b28",
                    "name": "Feng Zhang",
                    "hidden": false
                },
                {
                    "_id": "686f1f4dd938c25d68441b29",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "686f1f4dd938c25d68441b2a",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T22:38:49.000Z",
            "submittedOnDailyAt": "2025-07-10T00:33:32.493Z",
            "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving",
            "submittedOnDailyBy": {
                "_id": "62ffa3f8311cad266f9af236",
                "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
                "isPro": false,
                "fullname": "Zhenwen Liang",
                "user": "invokerliang",
                "type": "user"
            },
            "summary": "Automated Theorem Proving (ATP) in formal languages is a foundational\nchallenge for AI. While Large Language Models (LLMs) have driven remarkable\nprogress, a significant gap remains between their powerful informal reasoning\ncapabilities and their weak formal proving performance. Recent studies show\nthat the informal accuracy exceeds 80% while formal success remains below 8% on\nbenchmarks like PutnamBench. We argue this gap persists because current\nstate-of-the-art provers, by tightly coupling reasoning and proving, are\ntrained with paradigms that inadvertently punish deep reasoning in favor of\nshallow, tactic-based strategies. To bridge this fundamental gap, we propose a\nnovel framework that decouples high-level reasoning from low-level proof\ngeneration. Our approach utilizes two distinct, specialized models: a powerful,\ngeneral-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an\nefficient Prover to rigorously verify them. This modular design liberates the\nmodel's full reasoning potential and bypasses the pitfalls of end-to-end\ntraining. We evaluate our method on a challenging set of post-2000 IMO\nproblems, a problem set on which no prior open-source prover has reported\nsuccess. Our decoupled framework successfully solves 5 of these problems,\ndemonstrating a significant step towards automated reasoning on exceptionally\ndifficult mathematical challenges. To foster future research, we release our\nfull dataset of generated and verified lemmas for a wide range of IMO problems,\navailable at https://tencent-imo.github.io/ .",
            "upvotes": 10,
            "discussionId": "686f1f4ed938c25d68441b2b",
            "ai_summary": "A novel framework decouples reasoning and proving in ATP to improve formal proving performance, achieving success on challenging IMO problems.",
            "ai_keywords": [
                "Automated Theorem Proving",
                "Large Language Models",
                "formal proving",
                "informal reasoning",
                "PutnamBench",
                "subgoal lemmas",
                "Reasoner",
                "Prover",
                "modular design",
                "end-to-end training",
                "IMO problems"
            ]
        },
        "publishedAt": "2025-07-07T18:38:49.000Z",
        "title": "Towards Solving More Challenging IMO Problems via Decoupled Reasoning\n  and Proving",
        "summary": "Automated Theorem Proving (ATP) in formal languages is a foundational\nchallenge for AI. While Large Language Models (LLMs) have driven remarkable\nprogress, a significant gap remains between their powerful informal reasoning\ncapabilities and their weak formal proving performance. Recent studies show\nthat the informal accuracy exceeds 80% while formal success remains below 8% on\nbenchmarks like PutnamBench. We argue this gap persists because current\nstate-of-the-art provers, by tightly coupling reasoning and proving, are\ntrained with paradigms that inadvertently punish deep reasoning in favor of\nshallow, tactic-based strategies. To bridge this fundamental gap, we propose a\nnovel framework that decouples high-level reasoning from low-level proof\ngeneration. Our approach utilizes two distinct, specialized models: a powerful,\ngeneral-purpose Reasoner to generate diverse, strategic subgoal lemmas, and an\nefficient Prover to rigorously verify them. This modular design liberates the\nmodel's full reasoning potential and bypasses the pitfalls of end-to-end\ntraining. We evaluate our method on a challenging set of post-2000 IMO\nproblems, a problem set on which no prior open-source prover has reported\nsuccess. Our decoupled framework successfully solves 5 of these problems,\ndemonstrating a significant step towards automated reasoning on exceptionally\ndifficult mathematical challenges. To foster future research, we release our\nfull dataset of generated and verified lemmas for a wide range of IMO problems,\navailable at https://tencent-imo.github.io/ .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06804.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62ffa3f8311cad266f9af236",
            "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
            "fullname": "Zhenwen Liang",
            "name": "invokerliang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2506.24044",
            "authors": [
                {
                    "_id": "68681d82213f123a1f88b973",
                    "user": {
                        "_id": "683daabc402acb18654e4674",
                        "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
                        "isPro": false,
                        "fullname": "Sicong Jiang",
                        "user": "Max2045",
                        "type": "user"
                    },
                    "name": "Sicong Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-05T07:52:40.893Z",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b974",
                    "name": "Zilin Huang",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b975",
                    "name": "Kangan Qian",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b976",
                    "name": "Ziang Luo",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b977",
                    "name": "Tianze Zhu",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b978",
                    "name": "Yang Zhong",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b979",
                    "name": "Yihong Tang",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b97a",
                    "name": "Menglin Kong",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b97b",
                    "name": "Yunlong Wang",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b97c",
                    "name": "Siwen Jiao",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b97d",
                    "name": "Hao Ye",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b97e",
                    "name": "Zihao Sheng",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b97f",
                    "name": "Xin Zhao",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b980",
                    "name": "Tuopu Wen",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b981",
                    "name": "Zheng Fu",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b982",
                    "name": "Sikai Chen",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b983",
                    "name": "Kun Jiang",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b984",
                    "name": "Diange Yang",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b985",
                    "name": "Seongjin Choi",
                    "hidden": false
                },
                {
                    "_id": "68681d82213f123a1f88b986",
                    "name": "Lijun Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-06-30T16:50:02.000Z",
            "submittedOnDailyAt": "2025-07-10T01:10:17.895Z",
            "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
            "submittedOnDailyBy": {
                "_id": "683daabc402acb18654e4674",
                "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
                "isPro": false,
                "fullname": "Sicong Jiang",
                "user": "Max2045",
                "type": "user"
            },
            "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\nhttps://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.",
            "upvotes": 8,
            "discussionId": "68681d82213f123a1f88b987",
            "githubRepo": "https://github.com/JohnsonJiang1996/Awesome-VLA4AD",
            "ai_summary": "This survey provides a comprehensive overview of Vision-Language-Action (VLA) paradigms and their adaptation for autonomous driving, detailing architectural components, evolution of models, datasets, and future challenges.",
            "ai_keywords": [
                "multimodal large language models",
                "Vision-Language-Action",
                "VLA",
                "VLA for Autonomous Driving",
                "VLA4AD",
                "explainer",
                "reasoning-centric models",
                "autonomous driving",
                "driving safety",
                "accuracy",
                "explanation quality",
                "robustness",
                "real-time efficiency",
                "formal verification",
                "interpretable socially aligned autonomous vehicles"
            ],
            "githubStars": 179
        },
        "publishedAt": "2025-06-30T12:50:02.000Z",
        "title": "A Survey on Vision-Language-Action Models for Autonomous Driving",
        "summary": "The rapid progress of multimodal large language models (MLLM) has paved the\nway for Vision-Language-Action (VLA) paradigms, which integrate visual\nperception, natural language understanding, and control within a single policy.\nResearchers in autonomous driving are actively adapting these methods to the\nvehicle domain. Such models promise autonomous vehicles that can interpret\nhigh-level instructions, reason about complex traffic scenes, and make their\nown decisions. However, the literature remains fragmented and is rapidly\nexpanding. This survey offers the first comprehensive overview of VLA for\nAutonomous Driving (VLA4AD). We (i) formalize the architectural building blocks\nshared across recent work, (ii) trace the evolution from early explainer to\nreasoning-centric VLA models, and (iii) compare over 20 representative models\naccording to VLA's progress in the autonomous driving domain. We also\nconsolidate existing datasets and benchmarks, highlighting protocols that\njointly measure driving safety, accuracy, and explanation quality. Finally, we\ndetail open challenges - robustness, real-time efficiency, and formal\nverification - and outline future directions of VLA4AD. This survey provides a\nconcise yet complete reference for advancing interpretable socially aligned\nautonomous vehicles. Github repo is available at\nhttps://github.com/JohnsonJiang1996/Awesome-VLA4AD{SicongJiang/Awesome-VLA4AD}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2506.24044.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "683daabc402acb18654e4674",
            "avatarUrl": "/avatars/9307b4c1c248e45b2daa8ffa1d74d4b4.svg",
            "fullname": "Sicong Jiang",
            "name": "Max2045",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06853",
            "authors": [
                {
                    "_id": "686f4142d938c25d68441b98",
                    "user": {
                        "_id": "64e84ec6d41a68b065bf78a7",
                        "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
                        "isPro": false,
                        "fullname": "Liang Wang",
                        "user": "AzureLeon1",
                        "type": "user"
                    },
                    "name": "Liang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T07:09:02.989Z",
                    "hidden": false
                },
                {
                    "_id": "686f4142d938c25d68441b99",
                    "user": {
                        "_id": "642eecbf9b2484d7d8526781",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642eecbf9b2484d7d8526781/4IvGbd66s49Wx5pZyZGHA.png",
                        "isPro": false,
                        "fullname": "Yu Rong",
                        "user": "Swrooy",
                        "type": "user"
                    },
                    "name": "Yu Rong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T13:14:04.210Z",
                    "hidden": false
                },
                {
                    "_id": "686f4142d938c25d68441b9a",
                    "name": "Tingyang Xu",
                    "hidden": false
                },
                {
                    "_id": "686f4142d938c25d68441b9b",
                    "name": "Zhenyi Zhong",
                    "hidden": false
                },
                {
                    "_id": "686f4142d938c25d68441b9c",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "686f4142d938c25d68441b9d",
                    "name": "Pengju Wang",
                    "hidden": false
                },
                {
                    "_id": "686f4142d938c25d68441b9e",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "686f4142d938c25d68441b9f",
                    "name": "Qiang Liu",
                    "hidden": false
                },
                {
                    "_id": "686f4142d938c25d68441ba0",
                    "name": "Shu Wu",
                    "hidden": false
                },
                {
                    "_id": "686f4142d938c25d68441ba1",
                    "name": "Liang Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/zoojDRIzYMaSv2bj_dh7R.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/Lc9mfYTjI12Vz0qsq_JWj.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/brRCb577NUJrWNDqDkcMJ.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/LV-8rRgSCOr0UmateiCPM.jpeg"
            ],
            "publishedAt": "2025-07-09T13:57:20.000Z",
            "submittedOnDailyAt": "2025-07-10T03:05:48.871Z",
            "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "64e84ec6d41a68b065bf78a7",
                "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
                "isPro": false,
                "fullname": "Liang Wang",
                "user": "AzureLeon1",
                "type": "user"
            },
            "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.",
            "upvotes": 4,
            "discussionId": "686f4142d938c25d68441ba2",
            "ai_summary": "DiffSpectra uses diffusion models with SE(3)-equivariant architecture and SpecFormer spectral encoder to accurately infer both 2D and 3D molecular structures from multi-modal spectral data.",
            "ai_keywords": [
                "diffusion models",
                "SE(3)-equivariant architecture",
                "Diffusion Molecule Transformer",
                "SpecFormer",
                "spectral encoder",
                "multi-modal spectral reasoning",
                "joint 2D/3D generative modeling",
                "de novo molecular structure elucidation"
            ]
        },
        "publishedAt": "2025-07-09T09:57:20.000Z",
        "title": "DiffSpectra: Molecular Structure Elucidation from Spectra using\n  Diffusion Models",
        "summary": "Molecular structure elucidation from spectra is a foundational problem in\nchemistry, with profound implications for compound identification, synthesis,\nand drug development. Traditional methods rely heavily on expert interpretation\nand lack scalability. Pioneering machine learning methods have introduced\nretrieval-based strategies, but their reliance on finite libraries limits\ngeneralization to novel molecules. Generative models offer a promising\nalternative, yet most adopt autoregressive SMILES-based architectures that\noverlook 3D geometry and struggle to integrate diverse spectral modalities. In\nthis work, we present DiffSpectra, a generative framework that directly infers\nboth 2D and 3D molecular structures from multi-modal spectral data using\ndiffusion models. DiffSpectra formulates structure elucidation as a conditional\ngeneration process. Its denoising network is parameterized by Diffusion\nMolecule Transformer, an SE(3)-equivariant architecture that integrates\ntopological and geometric information. Conditioning is provided by SpecFormer,\na transformer-based spectral encoder that captures intra- and inter-spectral\ndependencies from multi-modal spectra. Extensive experiments demonstrate that\nDiffSpectra achieves high accuracy in structure elucidation, recovering exact\nstructures with 16.01% top-1 accuracy and 96.86% top-20 accuracy through\nsampling. The model benefits significantly from 3D geometric modeling,\nSpecFormer pre-training, and multi-modal conditioning. These results highlight\nthe effectiveness of spectrum-conditioned diffusion modeling in addressing the\nchallenge of molecular structure elucidation. To our knowledge, DiffSpectra is\nthe first framework to unify multi-modal spectral reasoning and joint 2D/3D\ngenerative modeling for de novo molecular structure elucidation.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/zoojDRIzYMaSv2bj_dh7R.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/Lc9mfYTjI12Vz0qsq_JWj.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/brRCb577NUJrWNDqDkcMJ.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/64e84ec6d41a68b065bf78a7/LV-8rRgSCOr0UmateiCPM.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06853.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64e84ec6d41a68b065bf78a7",
            "avatarUrl": "/avatars/bae3c5e3210b40af6e4f113e85f3e206.svg",
            "fullname": "Liang Wang",
            "name": "AzureLeon1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06607",
            "authors": [
                {
                    "_id": "686f629bd938c25d68441be3",
                    "name": "Liliang Ren",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441be4",
                    "name": "Congcong Chen",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441be5",
                    "name": "Haoran Xu",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441be6",
                    "name": "Young Jin Kim",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441be7",
                    "name": "Adam Atkinson",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441be8",
                    "name": "Zheng Zhan",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441be9",
                    "name": "Jiankai Sun",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441bea",
                    "name": "Baolin Peng",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441beb",
                    "name": "Liyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441bec",
                    "name": "Shuohang Wang",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441bed",
                    "name": "Hao Cheng",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441bee",
                    "name": "Jianfeng Gao",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441bef",
                    "name": "Weizhu Chen",
                    "hidden": false
                },
                {
                    "_id": "686f629bd938c25d68441bf0",
                    "name": "Yelong Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T07:27:00.000Z",
            "submittedOnDailyAt": "2025-07-10T19:02:23.642Z",
            "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long\n  Generation",
            "submittedOnDailyBy": {
                "_id": "63815eff4761ddfa00903762",
                "avatarUrl": "/avatars/fa4fa92539b727e9450e675ac8aa938a.svg",
                "isPro": false,
                "fullname": "Liliang Ren",
                "user": "renll",
                "type": "user"
            },
            "summary": "Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.",
            "upvotes": 3,
            "discussionId": "686f629bd938c25d68441bf1",
            "projectPage": "https://huggingface.co/microsoft/Phi-4-mini-flash-reasoning",
            "githubRepo": "https://github.com/microsoft/ArchScale",
            "ai_summary": "Gated Memory Units improve memory sharing in hybrid decoder architectures, enhancing efficiency and performance in language modeling tasks.",
            "ai_keywords": [
                "State Space Models",
                "Samba",
                "YOCO",
                "Gated Memory Unit",
                "SambaY",
                "cross-decoder",
                "self-decoder",
                "positional encoding",
                "Differential Attention",
                "Phi4-mini-Flash-Reasoning",
                "Phi4-mini-Reasoning",
                "Math500",
                "AIME24/25",
                "GPQA Diamond",
                "vLLM inference framework"
            ],
            "githubStars": 17
        },
        "publishedAt": "2025-07-09T03:27:00.000Z",
        "title": "Decoder-Hybrid-Decoder Architecture for Efficient Reasoning with Long\n  Generation",
        "summary": "Recent advances in language modeling have demonstrated the effectiveness of\nState Space Models (SSMs) for efficient sequence modeling. While hybrid\narchitectures such as Samba and the decoder-decoder architecture, YOCO, have\nshown promising performance gains over Transformers, prior works have not\ninvestigated the efficiency potential of representation sharing between SSM\nlayers. In this paper, we introduce the Gated Memory Unit (GMU), a simple yet\neffective mechanism for efficient memory sharing across layers. We apply it to\ncreate SambaY, a decoder-hybrid-decoder architecture that incorporates GMUs in\nthe cross-decoder to share memory readout states from a Samba-based\nself-decoder. SambaY significantly enhances decoding efficiency, preserves\nlinear pre-filling time complexity, and boosts long-context performance, all\nwhile eliminating the need for explicit positional encoding. Through extensive\nscaling experiments, we demonstrate that our model exhibits a significantly\nlower irreducible loss compared to a strong YOCO baseline, indicating superior\nperformance scalability under large-scale compute regimes. Our largest model\nenhanced with Differential Attention, Phi4-mini-Flash-Reasoning, achieves\nsignificantly better performance than Phi4-mini-Reasoning on reasoning tasks\nsuch as Math500, AIME24/25, and GPQA Diamond without any reinforcement\nlearning, while delivering up to 10x higher decoding throughput on 2K-length\nprompts with 32K generation length under the vLLM inference framework. We\nrelease our training codebase on open-source data at\nhttps://github.com/microsoft/ArchScale.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06607.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63815eff4761ddfa00903762",
            "avatarUrl": "/avatars/fa4fa92539b727e9450e675ac8aa938a.svg",
            "fullname": "Liliang Ren",
            "name": "renll",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06485",
            "authors": [
                {
                    "_id": "686fe3461de169176ba58b4e",
                    "name": "Ziyang Wang",
                    "hidden": false
                },
                {
                    "_id": "686fe3461de169176ba58b4f",
                    "name": "Jaehong Yoon",
                    "hidden": false
                },
                {
                    "_id": "686fe3461de169176ba58b50",
                    "name": "Shoubin Yu",
                    "hidden": false
                },
                {
                    "_id": "686fe3461de169176ba58b51",
                    "name": "Md Mohaiminul Islam",
                    "hidden": false
                },
                {
                    "_id": "686fe3461de169176ba58b52",
                    "name": "Gedas Bertasius",
                    "hidden": false
                },
                {
                    "_id": "686fe3461de169176ba58b53",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T02:06:13.000Z",
            "submittedOnDailyAt": "2025-07-10T14:29:19.465Z",
            "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning",
            "submittedOnDailyBy": {
                "_id": "63ee8fb05f1300034de097fd",
                "avatarUrl": "/avatars/bceedb88927d8633948266503c2dd0b1.svg",
                "isPro": true,
                "fullname": "Yu",
                "user": "Shoubin",
                "type": "user"
            },
            "summary": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and finetuning remain significant\nchallenges. These methods often rely on large-scale supervised fine-tuning\n(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,\nmaking them costly and hard to scale. To address this, we present Video-RTS, a\nnew approach to improve video reasoning capability with drastically improved\ndata efficiency by combining data-efficient RL with a video-adaptive test-time\nscaling (TTS) strategy. Based on observations about the data scaling of RL\nsamples, we skip the resource-intensive SFT step and employ efficient pure-RL\ntraining with output-based rewards, requiring no additional annotations or\nextensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by an average of 2.4% in accuracy\nusing only 3.6% training samples. For example, Video-RTS achieves a 4.2%\nimprovement on Video-Holmes, a recent and challenging video reasoning\nbenchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and\nadaptive video TTS offer complementary strengths, enabling Video-RTS's strong\nreasoning performance.",
            "upvotes": 3,
            "discussionId": "686fe3461de169176ba58b54",
            "ai_summary": "Video-RTS enhances video reasoning efficiency and accuracy through pure RL training and adaptive test-time scaling, reducing data and computational costs.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "data-efficient RL",
                "video-adaptive test-time scaling",
                "output-based rewards",
                "sparse-to-dense video TTS",
                "video reasoning benchmarks",
                "Video-Holmes",
                "MMVU"
            ]
        },
        "publishedAt": "2025-07-08T22:06:13.000Z",
        "title": "Video-RTS: Rethinking Reinforcement Learning and Test-Time Scaling for\n  Efficient and Enhanced Video Reasoning",
        "summary": "Despite advances in reinforcement learning (RL)-based video reasoning with\nlarge language models (LLMs), data collection and finetuning remain significant\nchallenges. These methods often rely on large-scale supervised fine-tuning\n(SFT) with extensive video data and long Chain-of-Thought (CoT) annotations,\nmaking them costly and hard to scale. To address this, we present Video-RTS, a\nnew approach to improve video reasoning capability with drastically improved\ndata efficiency by combining data-efficient RL with a video-adaptive test-time\nscaling (TTS) strategy. Based on observations about the data scaling of RL\nsamples, we skip the resource-intensive SFT step and employ efficient pure-RL\ntraining with output-based rewards, requiring no additional annotations or\nextensive fine-tuning. Furthermore, to utilize computational resources more\nefficiently, we introduce a sparse-to-dense video TTS strategy that improves\ninference by iteratively adding frames based on output consistency. We validate\nour approach on multiple video reasoning benchmarks, showing that Video-RTS\nsurpasses existing video reasoning models by an average of 2.4% in accuracy\nusing only 3.6% training samples. For example, Video-RTS achieves a 4.2%\nimprovement on Video-Holmes, a recent and challenging video reasoning\nbenchmark, and a 2.6% improvement on MMVU. Notably, our pure RL training and\nadaptive video TTS offer complementary strengths, enabling Video-RTS's strong\nreasoning performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06485.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63ee8fb05f1300034de097fd",
            "avatarUrl": "/avatars/bceedb88927d8633948266503c2dd0b1.svg",
            "fullname": "Yu",
            "name": "Shoubin",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06167",
            "authors": [
                {
                    "_id": "686dc8b5cb5725779c60b36d",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "686dc8b5cb5725779c60b36e",
                    "name": "Jiangbo Pei",
                    "hidden": false
                },
                {
                    "_id": "686dc8b5cb5725779c60b36f",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "686dc8b5cb5725779c60b370",
                    "name": "Xuchen Song",
                    "hidden": false
                },
                {
                    "_id": "686dc8b5cb5725779c60b371",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "686dc8b5cb5725779c60b372",
                    "name": "Jian Peng",
                    "hidden": false
                },
                {
                    "_id": "686dc8b5cb5725779c60b373",
                    "name": "Haofeng Sun",
                    "hidden": false
                },
                {
                    "_id": "686dc8b5cb5725779c60b374",
                    "name": "Yunzhuo Hao",
                    "hidden": false
                },
                {
                    "_id": "686dc8b5cb5725779c60b375",
                    "name": "Peiyu Wang",
                    "hidden": false
                },
                {
                    "_id": "686dc8b5cb5725779c60b376",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62308ae8be7833fa98620372/SC5ZtSuzovqe_L4DXD4r7.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/62308ae8be7833fa98620372/R37-cC3pIdtrkn4rVUhrD.png"
            ],
            "publishedAt": "2025-07-08T16:47:16.000Z",
            "submittedOnDailyAt": "2025-07-10T23:42:45.430Z",
            "title": "Skywork-R1V3 Technical Report",
            "submittedOnDailyBy": {
                "_id": "62308ae8be7833fa98620372",
                "avatarUrl": "/avatars/220ed57376202c61ec1afbf5a4f45b67.svg",
                "isPro": false,
                "fullname": "random",
                "user": "fakerbaby",
                "type": "user"
            },
            "summary": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.",
            "upvotes": 3,
            "discussionId": "686dc8b5cb5725779c60b377",
            "ai_summary": "Skywork-R1V3, an open-source vision-language model, enhances visual reasoning through post-training reinforcement learning, achieving state-of-the-art performance on MMMU and rivaling top closed-source models.",
            "ai_keywords": [
                "vision-language model",
                "visual reasoning",
                "Large Language Models",
                "post-training RL framework",
                "connector module",
                "cross-modal alignment",
                "multimodal reasoning models",
                "entropy of critical reasoning tokens",
                "checkpoint selection",
                "RL-powered post-training",
                "curriculum learning",
                "reinforcement finetuning strategies"
            ]
        },
        "publishedAt": "2025-07-08T12:47:16.000Z",
        "title": "Skywork-R1V3 Technical Report",
        "summary": "We introduce Skywork-R1V3, an advanced, open-source vision-language model\n(VLM) that pioneers a new approach to visual reasoning. Its key innovation lies\nin effectively transferring reasoning skills from text-only Large Language\nModels (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily\nstems from our elaborate post-training RL framework, which effectively\nactivates and enhances the model's reasoning ability, without the need for\nadditional continue pre-training. Through this framework, we further uncover\nthe fundamental role of the connector module in achieving robust cross-modal\nalignment for multimodal reasoning models. In addition, we introduce a unique\nindicator of reasoning capability, the entropy of critical reasoning tokens,\nwhich has proven highly effective for checkpoint selection during RL training.\nSkywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving\nfrom 64.3% to 76.0%. This performance matches entry-level human capabilities.\nRemarkably, our RL-powered post-training approach enables even the 38B\nparameter model to rival top closed-source VLMs. The implementation\nsuccessfully transfers mathematical reasoning to other subject-related\nreasoning tasks. We also include an analysis of curriculum learning and\nreinforcement finetuning strategies, along with a broader discussion on\nmultimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal\nreasoning, showcasing RL as a powerful engine for advancing open-source VLM\ncapabilities.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62308ae8be7833fa98620372/SC5ZtSuzovqe_L4DXD4r7.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/62308ae8be7833fa98620372/R37-cC3pIdtrkn4rVUhrD.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06167.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62308ae8be7833fa98620372",
            "avatarUrl": "/avatars/220ed57376202c61ec1afbf5a4f45b67.svg",
            "fullname": "random",
            "name": "fakerbaby",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.05455",
            "authors": [
                {
                    "_id": "686f5ec6d938c25d68441bc1",
                    "user": {
                        "_id": "628c29a54c5a62a1d216c560",
                        "avatarUrl": "/avatars/d21b4da766f87f47228112958666643b.svg",
                        "isPro": false,
                        "fullname": "Ashima Suvarna",
                        "user": "Ashima",
                        "type": "user"
                    },
                    "name": "Ashima Suvarna",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T09:12:39.834Z",
                    "hidden": false
                },
                {
                    "_id": "686f5ec6d938c25d68441bc2",
                    "user": {
                        "_id": "6543d269326cb9a32bd58e40",
                        "avatarUrl": "/avatars/ddabbd80844e3dff676a8c2a182d920c.svg",
                        "isPro": false,
                        "fullname": "Christina",
                        "user": "christinachance",
                        "type": "user"
                    },
                    "name": "Christina Chance",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-10T09:16:25.997Z",
                    "hidden": false
                },
                {
                    "_id": "686f5ec6d938c25d68441bc3",
                    "name": "Karolina Naranjo",
                    "hidden": false
                },
                {
                    "_id": "686f5ec6d938c25d68441bc4",
                    "user": {
                        "_id": "6189ddd0992df2640e3e7d40",
                        "avatarUrl": "/avatars/925f2308fc412ae352b57a1b71815028.svg",
                        "isPro": false,
                        "fullname": "Hamid Palangi",
                        "user": "hamidpalangi",
                        "type": "user"
                    },
                    "name": "Hamid Palangi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-10T09:16:36.140Z",
                    "hidden": false
                },
                {
                    "_id": "686f5ec6d938c25d68441bc5",
                    "user": {
                        "_id": "65d9787bec3dc9ccf7344f6f",
                        "avatarUrl": "/avatars/ab361193ed2287158f803fc792bb2df5.svg",
                        "isPro": false,
                        "fullname": "Sophie Hao",
                        "user": "notaphonologist",
                        "type": "user"
                    },
                    "name": "Sophie Hao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-10T09:16:42.125Z",
                    "hidden": false
                },
                {
                    "_id": "686f5ec6d938c25d68441bc6",
                    "name": "Thomas Hartvigsen",
                    "hidden": false
                },
                {
                    "_id": "686f5ec6d938c25d68441bc7",
                    "name": "Saadia Gabriel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T20:15:18.000Z",
            "submittedOnDailyAt": "2025-07-10T05:04:25.096Z",
            "title": "ModelCitizens: Representing Community Voices in Online Safety",
            "submittedOnDailyBy": {
                "_id": "61c5c25705aa54027c52f7b3",
                "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
                "isPro": false,
                "fullname": "Hritik Bansal",
                "user": "hbXNov",
                "type": "user"
            },
            "summary": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation. The data, models and code are available at\nhttps://github.com/asuvarna31/modelcitizens.",
            "upvotes": 3,
            "discussionId": "686f5ec7d938c25d68441bc8",
            "ai_summary": "A new dataset and models for toxic language detection incorporate diverse community perspectives and conversational context, improving accuracy over existing tools.",
            "ai_keywords": [
                "MODELCITIZENS",
                "LLM-generated conversational scenarios",
                "OpenAI Moderation API",
                "GPT-o4-mini",
                "LLAMACITIZEN-8B",
                "GEMMACITIZEN-12B",
                "LLaMA-based models",
                "Gemma-based models",
                "in-distribution evaluations",
                "community-informed annotation",
                "inclusive content moderation"
            ]
        },
        "publishedAt": "2025-07-07T16:15:18.000Z",
        "title": "ModelCitizens: Representing Community Voices in Online Safety",
        "summary": "Automatic toxic language detection is critical for creating safe, inclusive\nonline spaces. However, it is a highly subjective task, with perceptions of\ntoxic language shaped by community norms and lived experience. Existing\ntoxicity detection models are typically trained on annotations that collapse\ndiverse annotator perspectives into a single ground truth, erasing important\ncontext-specific notions of toxicity such as reclaimed language. To address\nthis, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K\ntoxicity annotations across diverse identity groups. To capture the role of\nconversational context on toxicity, typical of social media posts, we augment\nMODELCITIZENS posts with LLM-generated conversational scenarios.\nState-of-the-art toxicity detection tools (e.g. OpenAI Moderation API,\nGPT-o4-mini) underperform on MODELCITIZENS, with further degradation on\ncontext-augmented posts. Finally, we release LLAMACITIZEN-8B and\nGEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS,\nwhich outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our\nfindings highlight the importance of community-informed annotation and modeling\nfor inclusive content moderation. The data, models and code are available at\nhttps://github.com/asuvarna31/modelcitizens.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05455.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "61c5c25705aa54027c52f7b3",
            "avatarUrl": "/avatars/8a89e040dc331b7a83d9a704c4fc29d2.svg",
            "fullname": "Hritik Bansal",
            "name": "hbXNov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07024",
            "authors": [
                {
                    "_id": "686f6171d938c25d68441bca",
                    "name": "Weijia Shi",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bcb",
                    "name": "Akshita Bhagia",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bcc",
                    "name": "Kevin Farhat",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bcd",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bce",
                    "name": "Pete Walsh",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bcf",
                    "name": "Jacob Morrison",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd0",
                    "name": "Dustin Schwenk",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd1",
                    "name": "Shayne Longpre",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd2",
                    "name": "Jake Poznanski",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd3",
                    "name": "Allyson Ettinger",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd4",
                    "name": "Daogao Liu",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd5",
                    "name": "Margaret Li",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd6",
                    "name": "Dirk Groeneveld",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd7",
                    "name": "Mike Lewis",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd8",
                    "name": "Wen-tau Yih",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bd9",
                    "name": "Luca Soldaini",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bda",
                    "name": "Kyle Lo",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bdb",
                    "name": "Noah A. Smith",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bdc",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bdd",
                    "name": "Pang Wei Koh",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bde",
                    "name": "Hannaneh Hajishirzi",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441bdf",
                    "name": "Ali Farhadi",
                    "hidden": false
                },
                {
                    "_id": "686f6171d938c25d68441be0",
                    "name": "Sewon Min",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T16:54:21.000Z",
            "submittedOnDailyAt": "2025-07-10T22:21:48.759Z",
            "title": "FlexOlmo: Open Language Models for Flexible Data Use",
            "submittedOnDailyBy": {
                "_id": "5f1eb362eec0ad2a071ad6e2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
                "isPro": false,
                "fullname": "Niklas Muennighoff",
                "user": "Muennighoff",
                "type": "user"
            },
            "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.",
            "upvotes": 2,
            "discussionId": "686f6171d938c25d68441be1",
            "ai_summary": "FlexOlmo, a distributed and data-flexible language model using a mixture-of-experts architecture, achieves significant improvements in performance across diverse tasks while respecting data privacy and ownership.",
            "ai_keywords": [
                "mixture-of-experts",
                "MoE",
                "domain-informed routing",
                "FlexMix",
                "parameter-efficient fine-tuning",
                "data-flexible inference",
                "distributed training",
                "closed datasets",
                "data licensing",
                "data owners' preferences",
                "parameter-efficient fine-tuning",
                "FLOPs"
            ]
        },
        "publishedAt": "2025-07-09T12:54:21.000Z",
        "title": "FlexOlmo: Open Language Models for Flexible Data Use",
        "summary": "We introduce FlexOlmo, a new class of language models (LMs) that supports (1)\ndistributed training without data sharing, where different model parameters are\nindependently trained on closed datasets, and (2) data-flexible inference,\nwhere these parameters along with their associated data can be flexibly\nincluded or excluded from model inferences with no further training. FlexOlmo\nemploys a mixture-of-experts (MoE) architecture where each expert is trained\nindependently on closed datasets and later integrated through a new\ndomain-informed routing without any joint training. FlexOlmo is trained on\nFlexMix, a corpus we curate comprising publicly available datasets alongside\nseven domain-specific sets, representing realistic approximations of closed\nsets. We evaluate models with up to 37 billion parameters (20 billion active)\non 31 diverse downstream tasks. We show that a general expert trained on public\ndata can be effectively combined with independently trained experts from other\ndata owners, leading to an average 41% relative improvement while allowing\nusers to opt out of certain data based on data licensing or permission\nrequirements. Our approach also outperforms prior model merging methods by\n10.1% on average and surpasses the standard MoE trained without data\nrestrictions using the same training FLOPs. Altogether, this research presents\na solution for both data owners and researchers in regulated industries with\nsensitive or protected data. FlexOlmo enables benefiting from closed data while\nrespecting data owners' preferences by keeping their data local and supporting\nfine-grained control of data access during inference.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07024.png",
        "numComments": 0,
        "submittedBy": {
            "_id": "5f1eb362eec0ad2a071ad6e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
            "fullname": "Niklas Muennighoff",
            "name": "Muennighoff",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 140
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06415",
            "authors": [
                {
                    "_id": "686fe7d21de169176ba58b6d",
                    "name": "Zeming Chen",
                    "hidden": false
                },
                {
                    "_id": "686fe7d21de169176ba58b6e",
                    "name": "Angelika Romanou",
                    "hidden": false
                },
                {
                    "_id": "686fe7d21de169176ba58b6f",
                    "name": "Gail Weiss",
                    "hidden": false
                },
                {
                    "_id": "686fe7d21de169176ba58b70",
                    "name": "Antoine Bosselut",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/654b92086a49f6f6e0edbcd9/ARHv9Ju14CxLhwlR7l0W3.png"
            ],
            "publishedAt": "2025-07-08T21:38:45.000Z",
            "submittedOnDailyAt": "2025-07-10T17:12:20.274Z",
            "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning",
            "submittedOnDailyBy": {
                "_id": "654b92086a49f6f6e0edbcd9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654b92086a49f6f6e0edbcd9/1bWswk7EcL04mWwgx1TpA.jpeg",
                "isPro": true,
                "fullname": "Zeming Chen - Eric",
                "user": "zechen-nlp",
                "type": "user"
            },
            "summary": "Long-context reasoning requires accurately identifying relevant information\nin extensive, noisy input contexts. Previous research shows that using\ntest-time learning to encode context directly into model parameters can\neffectively enable reasoning over noisy information. However, meta-learning\nmethods for enabling test-time learning are prohibitively memory-intensive,\npreventing their application to long context settings. In this work, we propose\nPERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for\nlearning to encode long input contexts using gradient updates to a lightweight\nmodel adapter at test time. Specifically, PERK employs two nested optimization\nloops in a meta-training phase. The inner loop rapidly encodes contexts into a\nlow-rank adapter (LoRA) that serves as a parameter-efficient memory module for\nthe base model. Concurrently, the outer loop learns to use the updated adapter\nto accurately recall and reason over relevant information from the encoded long\ncontext. Our evaluations on several long-context reasoning tasks show that PERK\nsignificantly outperforms the standard prompt-based long-context baseline,\nachieving average absolute performance gains of up to 90% for smaller models\n(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In\ngeneral, PERK is more robust to reasoning complexity, length extrapolation, and\nthe locations of relevant information in contexts. Finally, we show that while\nPERK is memory-intensive during training, it scales more efficiently at\ninference time than prompt-based long-context inference.",
            "upvotes": 2,
            "discussionId": "686fe7d31de169176ba58b71",
            "projectPage": "https://perk-long-context.web.app/",
            "githubRepo": "https://github.com/eric11eca/perk",
            "ai_summary": "PERK, a scalable approach using parameter-efficient adapters, enhances long-context reasoning by encoding contexts into a lightweight model at test time, achieving significant performance improvements over prompt-based methods.",
            "ai_keywords": [
                "long-context reasoning",
                "test-time learning",
                "meta-learning",
                "parameter-efficient reasoning",
                "PERK",
                "gradient updates",
                "model adapter",
                "nested optimization loops",
                "low-rank adapter",
                "LoRA",
                "prompt-based baseline",
                "GPT-2",
                "Qwen-2.5-0.5B",
                "reasoning complexity",
                "length extrapolation",
                "memory-intensive",
                "inference time"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-07-08T17:38:45.000Z",
        "title": "PERK: Long-Context Reasoning as Parameter-Efficient Test-Time Learning",
        "summary": "Long-context reasoning requires accurately identifying relevant information\nin extensive, noisy input contexts. Previous research shows that using\ntest-time learning to encode context directly into model parameters can\neffectively enable reasoning over noisy information. However, meta-learning\nmethods for enabling test-time learning are prohibitively memory-intensive,\npreventing their application to long context settings. In this work, we propose\nPERK (Parameter Efficient Reasoning over Knowledge), a scalable approach for\nlearning to encode long input contexts using gradient updates to a lightweight\nmodel adapter at test time. Specifically, PERK employs two nested optimization\nloops in a meta-training phase. The inner loop rapidly encodes contexts into a\nlow-rank adapter (LoRA) that serves as a parameter-efficient memory module for\nthe base model. Concurrently, the outer loop learns to use the updated adapter\nto accurately recall and reason over relevant information from the encoded long\ncontext. Our evaluations on several long-context reasoning tasks show that PERK\nsignificantly outperforms the standard prompt-based long-context baseline,\nachieving average absolute performance gains of up to 90% for smaller models\n(GPT-2) and up to 27% for our largest evaluated model, Qwen-2.5-0.5B. In\ngeneral, PERK is more robust to reasoning complexity, length extrapolation, and\nthe locations of relevant information in contexts. Finally, we show that while\nPERK is memory-intensive during training, it scales more efficiently at\ninference time than prompt-based long-context inference.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/654b92086a49f6f6e0edbcd9/ARHv9Ju14CxLhwlR7l0W3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06415.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "654b92086a49f6f6e0edbcd9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654b92086a49f6f6e0edbcd9/1bWswk7EcL04mWwgx1TpA.jpeg",
            "fullname": "Zeming Chen - Eric",
            "name": "zechen-nlp",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.06260",
            "authors": [
                {
                    "_id": "686f14e3d938c25d68441af8",
                    "name": "Satyapriya Krishna",
                    "hidden": false
                },
                {
                    "_id": "686f14e3d938c25d68441af9",
                    "name": "Ninareh Mehrabi",
                    "hidden": false
                },
                {
                    "_id": "686f14e3d938c25d68441afa",
                    "name": "Abhinav Mohanty",
                    "hidden": false
                },
                {
                    "_id": "686f14e3d938c25d68441afb",
                    "name": "Matteo Memelli",
                    "hidden": false
                },
                {
                    "_id": "686f14e3d938c25d68441afc",
                    "name": "Vincent Ponzo",
                    "hidden": false
                },
                {
                    "_id": "686f14e3d938c25d68441afd",
                    "name": "Payal Motwani",
                    "hidden": false
                },
                {
                    "_id": "686f14e3d938c25d68441afe",
                    "name": "Rahul Gupta",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T13:33:35.000Z",
            "submittedOnDailyAt": "2025-07-10T00:10:50.485Z",
            "title": "Evaluating the Critical Risks of Amazon's Nova Premier under the\n  Frontier Model Safety Framework",
            "submittedOnDailyBy": {
                "_id": "6186fef1b1085ab638324e7f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
                "isPro": false,
                "fullname": "Satya",
                "user": "skrishna",
                "type": "user"
            },
            "summary": "Nova Premier is Amazon's most capable multimodal foundation model and teacher\nfor model distillation. It processes text, images, and video with a\none-million-token context window, enabling analysis of large codebases,\n400-page documents, and 90-minute videos in a single prompt. We present the\nfirst comprehensive evaluation of Nova Premier's critical risk profile under\nthe Frontier Model Safety Framework. Evaluations target three high-risk domains\n-- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber\nOperations, and Automated AI R&D -- and combine automated benchmarks, expert\nred-teaming, and uplift studies to determine whether the model exceeds release\nthresholds. We summarize our methodology and report core findings. Based on\nthis evaluation, we find that Nova Premier is safe for public release as per\nour commitments made at the 2025 Paris AI Safety Summit. We will continue to\nenhance our safety evaluation and mitigation pipelines as new risks and\ncapabilities associated with frontier models are identified.",
            "upvotes": 2,
            "discussionId": "686f14e4d938c25d68441aff"
        },
        "publishedAt": "2025-07-07T09:33:35.000Z",
        "title": "Evaluating the Critical Risks of Amazon's Nova Premier under the\n  Frontier Model Safety Framework",
        "summary": "Nova Premier is Amazon's most capable multimodal foundation model and teacher\nfor model distillation. It processes text, images, and video with a\none-million-token context window, enabling analysis of large codebases,\n400-page documents, and 90-minute videos in a single prompt. We present the\nfirst comprehensive evaluation of Nova Premier's critical risk profile under\nthe Frontier Model Safety Framework. Evaluations target three high-risk domains\n-- Chemical, Biological, Radiological & Nuclear (CBRN), Offensive Cyber\nOperations, and Automated AI R&D -- and combine automated benchmarks, expert\nred-teaming, and uplift studies to determine whether the model exceeds release\nthresholds. We summarize our methodology and report core findings. Based on\nthis evaluation, we find that Nova Premier is safe for public release as per\nour commitments made at the 2025 Paris AI Safety Summit. We will continue to\nenhance our safety evaluation and mitigation pipelines as new risks and\ncapabilities associated with frontier models are identified.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06260.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6186fef1b1085ab638324e7f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6186fef1b1085ab638324e7f/BL6_WJCkxB-BatBUBilT8.jpeg",
            "fullname": "Satya",
            "name": "skrishna",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2505.10251",
            "authors": [
                {
                    "_id": "686fb55f6626f47a7a1f8259",
                    "name": "Ji Woong Kim",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f825a",
                    "name": "Juo-Tung Chen",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f825b",
                    "name": "Pascal Hansen",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f825c",
                    "name": "Lucy X. Shi",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f825d",
                    "name": "Antony Goldenberg",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f825e",
                    "name": "Samuel Schmidgall",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f825f",
                    "name": "Paul Maria Scheikl",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f8260",
                    "name": "Anton Deguet",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f8261",
                    "name": "Brandon M. White",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f8262",
                    "name": "De Ru Tsai",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f8263",
                    "name": "Richard Cha",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f8264",
                    "name": "Jeffrey Jopling",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f8265",
                    "name": "Chelsea Finn",
                    "hidden": false
                },
                {
                    "_id": "686fb55f6626f47a7a1f8266",
                    "name": "Axel Krieger",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-05-15T13:04:53.000Z",
            "submittedOnDailyAt": "2025-07-10T11:15:46.422Z",
            "title": "SRT-H: A Hierarchical Framework for Autonomous Surgery via Language\n  Conditioned Imitation Learning",
            "submittedOnDailyBy": {
                "_id": "60f1abe7544c2adfd699860c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
                "isPro": true,
                "fullname": "AK",
                "user": "akhaliq",
                "type": "user"
            },
            "summary": "Research on autonomous surgery has largely focused on simple task automation\nin controlled environments. However, real-world surgical applications demand\ndexterous manipulation over extended durations and generalization to the\ninherent variability of human tissue. These challenges remain difficult to\naddress using existing logic-based or conventional end-to-end learning\napproaches. To address this gap, we propose a hierarchical framework for\nperforming dexterous, long-horizon surgical steps. Our approach utilizes a\nhigh-level policy for task planning and a low-level policy for generating robot\ntrajectories. The high-level planner plans in language space, generating\ntask-level or corrective instructions that guide the robot through the\nlong-horizon steps and correct for the low-level policy's errors. We validate\nour framework through ex vivo experiments on cholecystectomy, a\ncommonly-practiced minimally invasive procedure, and conduct ablation studies\nto evaluate key components of the system. Our method achieves a 100\\% success\nrate across eight unseen ex vivo gallbladders, operating fully autonomously\nwithout human intervention. This work demonstrates step-level autonomy in a\nsurgical procedure, marking a milestone toward clinical deployment of\nautonomous surgical systems.",
            "upvotes": 2,
            "discussionId": "686fb55f6626f47a7a1f8267",
            "ai_summary": "A hierarchical framework combining high-level task planning and low-level trajectory generation enables autonomous surgical procedures with high success rates in ex vivo experiments.",
            "ai_keywords": [
                "hierarchical framework",
                "high-level policy",
                "low-level policy",
                "task planning",
                "robot trajectories",
                "language space",
                "task-level instructions",
                "corrective instructions",
                "ex vivo experiments",
                "cholecystectomy",
                "minimally invasive procedure",
                "step-level autonomy",
                "clinical deployment"
            ]
        },
        "publishedAt": "2025-05-15T09:04:53.000Z",
        "title": "SRT-H: A Hierarchical Framework for Autonomous Surgery via Language\n  Conditioned Imitation Learning",
        "summary": "Research on autonomous surgery has largely focused on simple task automation\nin controlled environments. However, real-world surgical applications demand\ndexterous manipulation over extended durations and generalization to the\ninherent variability of human tissue. These challenges remain difficult to\naddress using existing logic-based or conventional end-to-end learning\napproaches. To address this gap, we propose a hierarchical framework for\nperforming dexterous, long-horizon surgical steps. Our approach utilizes a\nhigh-level policy for task planning and a low-level policy for generating robot\ntrajectories. The high-level planner plans in language space, generating\ntask-level or corrective instructions that guide the robot through the\nlong-horizon steps and correct for the low-level policy's errors. We validate\nour framework through ex vivo experiments on cholecystectomy, a\ncommonly-practiced minimally invasive procedure, and conduct ablation studies\nto evaluate key components of the system. Our method achieves a 100\\% success\nrate across eight unseen ex vivo gallbladders, operating fully autonomously\nwithout human intervention. This work demonstrates step-level autonomy in a\nsurgical procedure, marking a milestone toward clinical deployment of\nautonomous surgical systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2505.10251.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f1abe7544c2adfd699860c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1674929746905-60f1abe7544c2adfd699860c.jpeg",
            "fullname": "AK",
            "name": "akhaliq",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7305
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07106",
            "authors": [
                {
                    "_id": "686f4265d938c25d68441ba4",
                    "user": {
                        "_id": "653213eaa0dffcea236ca422",
                        "avatarUrl": "/avatars/69d30ce923edcf99e177669c2f8bf9a5.svg",
                        "isPro": false,
                        "fullname": "Vatsal Agarwal",
                        "user": "vatsalag",
                        "type": "user"
                    },
                    "name": "Vatsal Agarwal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-10T07:09:01.062Z",
                    "hidden": false
                },
                {
                    "_id": "686f4265d938c25d68441ba5",
                    "name": "Matthew Gwilliam",
                    "hidden": false
                },
                {
                    "_id": "686f4265d938c25d68441ba6",
                    "name": "Gefen Kohavi",
                    "hidden": false
                },
                {
                    "_id": "686f4265d938c25d68441ba7",
                    "name": "Eshan Verma",
                    "hidden": false
                },
                {
                    "_id": "686f4265d938c25d68441ba8",
                    "name": "Daniel Ulbricht",
                    "hidden": false
                },
                {
                    "_id": "686f4265d938c25d68441ba9",
                    "name": "Abhinav Shrivastava",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T17:59:47.000Z",
            "submittedOnDailyAt": "2025-07-10T13:24:46.036Z",
            "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor",
            "submittedOnDailyBy": {
                "_id": "653213eaa0dffcea236ca422",
                "avatarUrl": "/avatars/69d30ce923edcf99e177669c2f8bf9a5.svg",
                "isPro": false,
                "fullname": "Vatsal Agarwal",
                "user": "vatsalag",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/.",
            "upvotes": 1,
            "discussionId": "686f4265d938c25d68441baa",
            "ai_summary": "Text-to-image diffusion models enhance image-based question-answering by providing semantically rich and instruction-aware visual encodings, complementing CLIP and improving spatial and compositional reasoning.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "image-based question-answering",
                "CLIP",
                "visual encoder",
                "diffusion models",
                "instruction-aware",
                "internal representations",
                "image-text alignment",
                "text conditioning",
                "leakage phenomenon",
                "fusion strategy",
                "VQA",
                "vision-centric tasks",
                "spatial reasoning",
                "compositional reasoning"
            ]
        },
        "publishedAt": "2025-07-09T13:59:47.000Z",
        "title": "Towards Multimodal Understanding via Stable Diffusion as a Task-Aware\n  Feature Extractor",
        "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nimage-based question-answering capabilities. However, a key limitation is the\nuse of CLIP as the visual encoder; while it can capture coarse global\ninformation, it often can miss fine-grained details that are relevant to the\ninput query. To address these shortcomings, this work studies whether\npre-trained text-to-image diffusion models can serve as instruction-aware\nvisual encoders. Through an analysis of their internal representations, we find\ndiffusion features are both rich in semantics and can encode strong image-text\nalignment. Moreover, we find that we can leverage text conditioning to focus\nthe model on regions relevant to the input question. We then investigate how to\nalign these features with large language models and uncover a leakage\nphenomenon, where the LLM can inadvertently recover information from the\noriginal diffusion prompt. We analyze the causes of this leakage and propose a\nmitigation strategy. Based on these insights, we explore a simple fusion\nstrategy that utilizes both CLIP and conditional diffusion features. We\nevaluate our approach on both general VQA and specialized MLLM benchmarks,\ndemonstrating the promise of diffusion models for visual understanding,\nparticularly in vision-centric tasks that require spatial and compositional\nreasoning. Our project page can be found\nhttps://vatsalag99.github.io/mustafar/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07106.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "653213eaa0dffcea236ca422",
            "avatarUrl": "/avatars/69d30ce923edcf99e177669c2f8bf9a5.svg",
            "fullname": "Vatsal Agarwal",
            "name": "vatsalag",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.05980",
            "authors": [
                {
                    "_id": "686dfb25cb5725779c60b4c2",
                    "user": {
                        "_id": "655f4ff710e5c5fbef30fd97",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f4ff710e5c5fbef30fd97/bI_KQnFof50HRqYBWpyu2.jpeg",
                        "isPro": true,
                        "fullname": "Gabriel C",
                        "user": "gabrielchua",
                        "type": "user"
                    },
                    "name": "Gabriel Chua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-09T08:49:41.950Z",
                    "hidden": false
                },
                {
                    "_id": "686dfb25cb5725779c60b4c3",
                    "name": "Leanne Tan",
                    "hidden": false
                },
                {
                    "_id": "686dfb25cb5725779c60b4c4",
                    "name": "Ziyu Ge",
                    "hidden": false
                },
                {
                    "_id": "686dfb25cb5725779c60b4c5",
                    "name": "Roy Ka-Wei Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T13:37:25.000Z",
            "submittedOnDailyAt": "2025-07-10T23:20:28.681Z",
            "title": "RabakBench: Scaling Human Annotations to Construct Localized\n  Multilingual Safety Benchmarks for Low-Resource Languages",
            "submittedOnDailyBy": {
                "_id": "655f4ff710e5c5fbef30fd97",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f4ff710e5c5fbef30fd97/bI_KQnFof50HRqYBWpyu2.jpeg",
                "isPro": true,
                "fullname": "Gabriel C",
                "user": "gabrielchua",
                "type": "user"
            },
            "summary": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available.",
            "upvotes": 1,
            "discussionId": "686dfb25cb5725779c60b4c6",
            "projectPage": "https://go.gov.sg/rabakbench-blog",
            "githubRepo": "https://github.com/govtech-responsibleai/rabakbench",
            "githubStars": 0
        },
        "publishedAt": "2025-07-08T09:37:25.000Z",
        "title": "RabakBench: Scaling Human Annotations to Construct Localized\n  Multilingual Safety Benchmarks for Low-Resource Languages",
        "summary": "Large language models (LLMs) and their safety classifiers often perform\npoorly on low-resource languages due to limited training data and evaluation\nbenchmarks. This paper introduces RabakBench, a new multilingual safety\nbenchmark localized to Singapore's unique linguistic context, covering\nSinglish, Chinese, Malay, and Tamil. RabakBench is constructed through a\nscalable three-stage pipeline: (i) Generate - adversarial example generation by\naugmenting real Singlish web content with LLM-driven red teaming; (ii) Label -\nsemi-automated multi-label safety annotation using majority-voted LLM labelers\naligned with human judgments; and (iii) Translate - high-fidelity translation\npreserving linguistic nuance and toxicity across languages. The final dataset\ncomprises over 5,000 safety-labeled examples across four languages and six\nfine-grained safety categories with severity levels. Evaluations of 11 popular\nopen-source and closed-source guardrail classifiers reveal significant\nperformance degradation. RabakBench not only enables robust safety evaluation\nin Southeast Asian multilingual settings but also offers a reproducible\nframework for building localized safety datasets in low-resource environments.\nThe benchmark dataset, including the human-verified translations, and\nevaluation code are publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05980.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "655f4ff710e5c5fbef30fd97",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655f4ff710e5c5fbef30fd97/bI_KQnFof50HRqYBWpyu2.jpeg",
            "fullname": "Gabriel C",
            "name": "gabrielchua",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 104
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.01702",
            "authors": [
                {
                    "_id": "686f7ef5d938c25d6844295f",
                    "name": "Zixin Chen",
                    "hidden": false
                },
                {
                    "_id": "686f7ef5d938c25d68442960",
                    "name": "Hongzhan Lin",
                    "hidden": false
                },
                {
                    "_id": "686f7ef5d938c25d68442961",
                    "name": "Kaixin Li",
                    "hidden": false
                },
                {
                    "_id": "686f7ef5d938c25d68442962",
                    "name": "Ziyang Luo",
                    "hidden": false
                },
                {
                    "_id": "686f7ef5d938c25d68442963",
                    "name": "Zhen Ye",
                    "hidden": false
                },
                {
                    "_id": "686f7ef5d938c25d68442964",
                    "name": "Guang Chen",
                    "hidden": false
                },
                {
                    "_id": "686f7ef5d938c25d68442965",
                    "name": "Zhiyong Huang",
                    "hidden": false
                },
                {
                    "_id": "686f7ef5d938c25d68442966",
                    "name": "Jing Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-02T13:32:30.000Z",
            "submittedOnDailyAt": "2025-07-10T07:21:57.693Z",
            "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large\n  Language Models on Harmfulness",
            "submittedOnDailyBy": {
                "_id": "6499466c7d1edf7cb612a9a6",
                "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
                "isPro": false,
                "fullname": "Hongzhan Lin",
                "user": "danielhzlin",
                "type": "user"
            },
            "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.",
            "upvotes": 1,
            "discussionId": "686f7ef5d938c25d68442967",
            "ai_summary": "AdamMeme, an adaptive agent-based framework, evaluates multimodal Large Language Models' understanding of harmful memes through iterative updates and multi-agent collaboration, revealing model-specific weaknesses.",
            "ai_keywords": [
                "multimodal Large Language Models",
                "meme harmfulness",
                "agent-based evaluation framework",
                "multi-agent collaboration",
                "meme data updates",
                "model-specific weaknesses"
            ]
        },
        "publishedAt": "2025-07-02T09:32:30.000Z",
        "title": "AdamMeme: Adaptively Probe the Reasoning Capacity of Multimodal Large\n  Language Models on Harmfulness",
        "summary": "The proliferation of multimodal memes in the social media era demands that\nmultimodal Large Language Models (mLLMs) effectively understand meme\nharmfulness. Existing benchmarks for assessing mLLMs on harmful meme\nunderstanding rely on accuracy-based, model-agnostic evaluations using static\ndatasets. These benchmarks are limited in their ability to provide up-to-date\nand thorough assessments, as online memes evolve dynamically. To address this,\nwe propose AdamMeme, a flexible, agent-based evaluation framework that\nadaptively probes the reasoning capabilities of mLLMs in deciphering meme\nharmfulness. Through multi-agent collaboration, AdamMeme provides comprehensive\nevaluations by iteratively updating the meme data with challenging samples,\nthereby exposing specific limitations in how mLLMs interpret harmfulness.\nExtensive experiments show that our framework systematically reveals the\nvarying performance of different target mLLMs, offering in-depth, fine-grained\nanalyses of model-specific weaknesses. Our code is available at\nhttps://github.com/Lbotirx/AdamMeme.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.01702.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6499466c7d1edf7cb612a9a6",
            "avatarUrl": "/avatars/c2e18594aa0879db8226f2a04496fb0b.svg",
            "fullname": "Hongzhan Lin",
            "name": "danielhzlin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    }
]