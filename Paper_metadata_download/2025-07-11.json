[
    {
        "paper": {
            "id": "2507.07966",
            "authors": [
                {
                    "_id": "68706bdcc8391850d60977eb",
                    "name": "Yukang Chen",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977ec",
                    "name": "Wei Huang",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977ed",
                    "name": "Baifeng Shi",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977ee",
                    "name": "Qinghao Hu",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977ef",
                    "name": "Hanrong Ye",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977f0",
                    "name": "Ligeng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977f1",
                    "name": "Zhijian Liu",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977f2",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977f3",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977f4",
                    "name": "Xiaojuan Qi",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977f5",
                    "name": "Sifei Liu",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977f6",
                    "name": "Hongxu Yin",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977f7",
                    "name": "Yao Lu",
                    "hidden": false
                },
                {
                    "_id": "68706bdcc8391850d60977f8",
                    "name": "Song Han",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/Tu__SuWZeWyCBPoK8YUyh.mp4"
            ],
            "publishedAt": "2025-07-10T17:47:40.000Z",
            "submittedOnDailyAt": "2025-07-11T00:13:53.988Z",
            "title": "Scaling RL to Long Videos",
            "submittedOnDailyBy": {
                "_id": "62919485a29097b211bc7b83",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653710384819-62919485a29097b211bc7b83.png",
                "isPro": false,
                "fullname": "YukangChen",
                "user": "Yukang",
                "type": "user"
            },
            "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
            "upvotes": 94,
            "discussionId": "68706bdcc8391850d60977f9",
            "projectPage": "https://github.com/NVlabs/Long-RL",
            "githubRepo": "https://github.com/NVlabs/Long-RL",
            "ai_summary": "A framework for scaling vision-language models to long videos using reinforcement learning, achieving strong performance on various reasoning tasks with a specialized training infrastructure.",
            "ai_keywords": [
                "vision-language models",
                "reinforcement learning",
                "chain-of-thought supervised fine-tuning",
                "CoT-SFT",
                "Multi-modal Reinforcement Sequence Parallelism",
                "MR-SP",
                "sequence parallelism",
                "vLLM",
                "long video QA",
                "VideoMME",
                "LongVideo-Reason-eval",
                "temporal reasoning",
                "goal and purpose reasoning",
                "spatial reasoning",
                "plot reasoning",
                "RL training",
                "image and video generation models"
            ],
            "githubStars": 283
        },
        "publishedAt": "2025-07-10T13:47:40.000Z",
        "title": "Scaling RL to Long Videos",
        "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 52K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In experiments, LongVILA-R1-7B achieves\nstrong performance on long video QA benchmarks such as VideoMME. It also\noutperforms Video-R1-7B and even matches Gemini-1.5-Pro across temporal\nreasoning, goal and purpose reasoning, spatial reasoning, and plot reasoning on\nour LongVideo-Reason-eval benchmark. Notably, our MR-SP system achieves up to\n2.1x speedup on long video RL training. LongVILA-R1 demonstrates consistent\nperformance gains as the number of input video frames scales. LongVILA-R1 marks\na firm step towards long video reasoning in VLMs. In addition, we release our\ntraining system for public availability that supports RL training on various\nmodalities (video, text, and audio), various models (VILA and Qwen series), and\neven image and video generation models. On a single A100 node (8 GPUs), it\nsupports RL training on hour-long videos (e.g., 3,600 frames / around 256k\ntokens).",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62919485a29097b211bc7b83/Tu__SuWZeWyCBPoK8YUyh.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07966.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62919485a29097b211bc7b83",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1653710384819-62919485a29097b211bc7b83.png",
            "fullname": "YukangChen",
            "name": "Yukang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 61
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.05964",
            "authors": [
                {
                    "_id": "6870b8b5c8391850d60978e0",
                    "user": {
                        "_id": "6385d420c12615765caa00b2",
                        "avatarUrl": "/avatars/c125b2d61423908d453231b046fb6a28.svg",
                        "isPro": false,
                        "fullname": "Vera Soboleva",
                        "user": "verasobol",
                        "type": "user"
                    },
                    "name": "Vera Soboleva",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T16:31:53.439Z",
                    "hidden": false
                },
                {
                    "_id": "6870b8b5c8391850d60978e1",
                    "user": {
                        "_id": "66680c6451545a8b46c6fd21",
                        "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
                        "isPro": false,
                        "fullname": "Aibek Alanov",
                        "user": "ai-alanov",
                        "type": "user"
                    },
                    "name": "Aibek Alanov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T07:59:56.813Z",
                    "hidden": false
                },
                {
                    "_id": "6870b8b5c8391850d60978e2",
                    "user": {
                        "_id": "643984dceb7c5616ef3f5d54",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643984dceb7c5616ef3f5d54/10JRkblrRIEVci6UJwvPz.jpeg",
                        "isPro": false,
                        "fullname": "Andrey Kuznetsov",
                        "user": "kuznetsoffandrey",
                        "type": "user"
                    },
                    "name": "Andrey Kuznetsov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T13:12:36.562Z",
                    "hidden": false
                },
                {
                    "_id": "6870b8b5c8391850d60978e3",
                    "user": {
                        "_id": "6388ba34ec1f539adc092b56",
                        "avatarUrl": "/avatars/1d87a657dd81e6ca025cc020f3205525.svg",
                        "isPro": false,
                        "fullname": "Konstantin Sobolev",
                        "user": "k-sobolev",
                        "type": "user"
                    },
                    "name": "Konstantin Sobolev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T16:31:55.163Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T13:14:10.000Z",
            "submittedOnDailyAt": "2025-07-11T05:42:42.057Z",
            "title": "T-LoRA: Single Image Diffusion Model Customization Without Overfitting",
            "submittedOnDailyBy": {
                "_id": "66680c6451545a8b46c6fd21",
                "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
                "isPro": false,
                "fullname": "Aibek Alanov",
                "user": "ai-alanov",
                "type": "user"
            },
            "summary": "While diffusion model fine-tuning offers a powerful approach for customizing\npre-trained models to generate specific objects, it frequently suffers from\noverfitting when training samples are limited, compromising both generalization\ncapability and output diversity. This paper tackles the challenging yet most\nimpactful task of adapting a diffusion model using just a single concept image,\nas single-image customization holds the greatest practical potential. We\nintroduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework\nspecifically designed for diffusion model personalization. In our work we show\nthat higher diffusion timesteps are more prone to overfitting than lower ones,\nnecessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates\ntwo key innovations: (1) a dynamic fine-tuning strategy that adjusts\nrank-constrained updates based on diffusion timesteps, and (2) a weight\nparametrization technique that ensures independence between adapter components\nthrough orthogonal initialization. Extensive experiments show that T-LoRA and\nits individual components outperform standard LoRA and other diffusion model\npersonalization techniques. They achieve a superior balance between concept\nfidelity and text alignment, highlighting the potential of T-LoRA in\ndata-limited and resource-constrained scenarios. Code is available at\nhttps://github.com/ControlGenAI/T-LoRA.",
            "upvotes": 83,
            "discussionId": "6870b8b5c8391850d60978e4",
            "githubRepo": "https://github.com/ControlGenAI/T-LoRA",
            "ai_summary": "T-LoRA, a timestep-dependent low-rank adaptation framework, enhances diffusion model personalization with a dynamic fine-tuning strategy and orthogonal initialization, achieving better concept fidelity and text alignment in data-limited settings.",
            "ai_keywords": [
                "diffusion model fine-tuning",
                "overfitting",
                "generalization capability",
                "output diversity",
                "single-image customization",
                "T-LoRA",
                "timestep-dependent low-rank adaptation",
                "dynamic fine-tuning strategy",
                "rank-constrained updates",
                "diffusion timesteps",
                "weight parametrization",
                "orthogonal initialization",
                "concept fidelity",
                "text alignment",
                "data-limited scenarios"
            ],
            "githubStars": 39
        },
        "publishedAt": "2025-07-08T09:14:10.000Z",
        "title": "T-LoRA: Single Image Diffusion Model Customization Without Overfitting",
        "summary": "While diffusion model fine-tuning offers a powerful approach for customizing\npre-trained models to generate specific objects, it frequently suffers from\noverfitting when training samples are limited, compromising both generalization\ncapability and output diversity. This paper tackles the challenging yet most\nimpactful task of adapting a diffusion model using just a single concept image,\nas single-image customization holds the greatest practical potential. We\nintroduce T-LoRA, a Timestep-Dependent Low-Rank Adaptation framework\nspecifically designed for diffusion model personalization. In our work we show\nthat higher diffusion timesteps are more prone to overfitting than lower ones,\nnecessitating a timestep-sensitive fine-tuning strategy. T-LoRA incorporates\ntwo key innovations: (1) a dynamic fine-tuning strategy that adjusts\nrank-constrained updates based on diffusion timesteps, and (2) a weight\nparametrization technique that ensures independence between adapter components\nthrough orthogonal initialization. Extensive experiments show that T-LoRA and\nits individual components outperform standard LoRA and other diffusion model\npersonalization techniques. They achieve a superior balance between concept\nfidelity and text alignment, highlighting the potential of T-LoRA in\ndata-limited and resource-constrained scenarios. Code is available at\nhttps://github.com/ControlGenAI/T-LoRA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05964.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66680c6451545a8b46c6fd21",
            "avatarUrl": "/avatars/03707f5ea4e2aa8dc825a9782b00ed85.svg",
            "fullname": "Aibek Alanov",
            "name": "ai-alanov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07999",
            "authors": [
                {
                    "_id": "68706dcdc8391850d60977fb",
                    "user": {
                        "_id": "6499809cf19fc795e7724e43",
                        "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
                        "isPro": false,
                        "fullname": "HaochenWang",
                        "user": "HaochenWang",
                        "type": "user"
                    },
                    "name": "Haochen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:30.986Z",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d60977fc",
                    "user": {
                        "_id": "63958b4414513eaf9029ebf1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/U1g5H071pWRswGAG9UTpo.png",
                        "isPro": false,
                        "fullname": "Xiangtai Li",
                        "user": "LXT",
                        "type": "user"
                    },
                    "name": "Xiangtai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:28.856Z",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d60977fd",
                    "name": "Zilong Huang",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d60977fe",
                    "name": "Anran Wang",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d60977ff",
                    "user": {
                        "_id": "64d201b1c2bd235422fb1d14",
                        "avatarUrl": "/avatars/e50581aa66391cedae94e116e759b9ec.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "stormthunder",
                        "type": "user"
                    },
                    "name": "Jiacong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:26.581Z",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d6097800",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d6097801",
                    "user": {
                        "_id": "64531f631a57e1179c203e6b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64531f631a57e1179c203e6b/C_J7pXFLqoJoHYPPhK3J9.jpeg",
                        "isPro": false,
                        "fullname": "zjn",
                        "user": "garlicisnotmyfavor",
                        "type": "user"
                    },
                    "name": "Jiani Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:33.749Z",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d6097802",
                    "name": "Sule Bai",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d6097803",
                    "name": "Zijian Kang",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d6097804",
                    "name": "Jiashi Feng",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d6097805",
                    "name": "Zhuochen Wang",
                    "hidden": false
                },
                {
                    "_id": "68706dcdc8391850d6097806",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T17:59:58.000Z",
            "submittedOnDailyAt": "2025-07-11T00:29:18.339Z",
            "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology",
            "submittedOnDailyBy": {
                "_id": "6499809cf19fc795e7724e43",
                "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
                "isPro": false,
                "fullname": "HaochenWang",
                "user": "HaochenWang",
                "type": "user"
            },
            "summary": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.",
            "upvotes": 37,
            "discussionId": "68706dcec8391850d6097807",
            "projectPage": "https://github.com/Haochen-Wang409/TreeVGR",
            "githubRepo": "https://github.com/Haochen-Wang409/TreeVGR",
            "ai_summary": "TreeBench evaluates visual grounded reasoning through subtle target detection, traceable evidence, and second-order reasoning, while TreeVGR enhances this with joint localization and reasoning using reinforcement learning.",
            "ai_keywords": [
                "visual grounded reasoning",
                "bounding box evaluation",
                "second-order reasoning",
                "TreeBench",
                "TreeVGR",
                "reinforcement learning",
                "localization",
                "reasoning pathways",
                "V* Bench",
                "MME-RealWorld"
            ],
            "githubStars": 25
        },
        "publishedAt": "2025-07-10T13:59:58.000Z",
        "title": "Traceable Evidence Enhanced Visual Grounded Reasoning: Evaluation and\n  Methodology",
        "summary": "Models like OpenAI-o3 pioneer visual grounded reasoning by dynamically\nreferencing visual regions, just like human \"thinking with images\". However, no\nbenchmark exists to evaluate these capabilities holistically. To bridge this\ngap, we propose TreeBench (Traceable Evidence Evaluation Benchmark), a\ndiagnostic benchmark built on three principles: (1) focused visual perception\nof subtle targets in complex scenes, (2) traceable evidence via bounding box\nevaluation, and (3) second-order reasoning to test object interactions and\nspatial hierarchies beyond simple object localization. Prioritizing images with\ndense objects, we initially sample 1K high-quality images from SA-1B, and\nincorporate eight LMM experts to manually annotate questions, candidate\noptions, and answers for each image. After three stages of quality control,\nTreeBench consists of 405 challenging visual question-answering pairs, even the\nmost advanced models struggle with this benchmark, where none of them reach 60%\naccuracy, e.g., OpenAI-o3 scores only 54.87. Furthermore, we introduce TreeVGR\n(Traceable Evidence Enhanced Visual Grounded Reasoning), a training paradigm to\nsupervise localization and reasoning jointly with reinforcement learning,\nenabling accurate localizations and explainable reasoning pathways. Initialized\nfrom Qwen2.5-VL-7B, it improves V* Bench (+16.8), MME-RealWorld (+12.6), and\nTreeBench (+13.4), proving traceability is key to advancing vision-grounded\nreasoning. The code is available at https://github.com/Haochen-Wang409/TreeVGR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07999.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6499809cf19fc795e7724e43",
            "avatarUrl": "/avatars/4b3adce8c85e2f3ef05318ded6c89c3e.svg",
            "fullname": "HaochenWang",
            "name": "HaochenWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07984",
            "authors": [
                {
                    "_id": "687088a6c8391850d6097874",
                    "name": "JingLi Lin",
                    "hidden": false
                },
                {
                    "_id": "687088a6c8391850d6097875",
                    "user": {
                        "_id": "6433aba4546e16f17a0f19f6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433aba4546e16f17a0f19f6/3hlNy4_Suy0bO__Qa9amL.jpeg",
                        "isPro": false,
                        "fullname": "Chenming Zhu",
                        "user": "ChaimZhu",
                        "type": "user"
                    },
                    "name": "Chenming Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T13:12:39.911Z",
                    "hidden": false
                },
                {
                    "_id": "687088a6c8391850d6097876",
                    "name": "Runsen Xu",
                    "hidden": false
                },
                {
                    "_id": "687088a6c8391850d6097877",
                    "name": "Xiaohan Mao",
                    "hidden": false
                },
                {
                    "_id": "687088a6c8391850d6097878",
                    "name": "Xihui Liu",
                    "hidden": false
                },
                {
                    "_id": "687088a6c8391850d6097879",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "687088a6c8391850d609787a",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6433aba4546e16f17a0f19f6/pNjo9cepo_BTSUgc_ZLsg.mp4"
            ],
            "publishedAt": "2025-07-10T17:56:07.000Z",
            "submittedOnDailyAt": "2025-07-11T04:12:08.963Z",
            "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding",
            "submittedOnDailyBy": {
                "_id": "6433aba4546e16f17a0f19f6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433aba4546e16f17a0f19f6/3hlNy4_Suy0bO__Qa9amL.jpeg",
                "isPro": false,
                "fullname": "Chenming Zhu",
                "user": "ChaimZhu",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/",
            "upvotes": 29,
            "discussionId": "687088a6c8391850d609787b",
            "projectPage": "https://rbler1234.github.io/OSTBench.github.io/",
            "githubRepo": "https://github.com/OpenRobotLab/OST-Bench",
            "ai_summary": "OST-Bench evaluates multimodal large language models in online spatio-temporal reasoning tasks, revealing challenges in handling complex spatial cues and long-term memory in real-world scenarios.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "OST-Bench",
                "Online Spatio-Temporal understanding",
                "ScanNet",
                "Matterport3D",
                "ARKitScenes",
                "complex spatio-temporal reasoning",
                "long-term memory retrieval"
            ],
            "githubStars": 40
        },
        "publishedAt": "2025-07-10T13:56:07.000Z",
        "title": "OST-Bench: Evaluating the Capabilities of MLLMs in Online\n  Spatio-temporal Scene Understanding",
        "summary": "Recent advances in multimodal large language models (MLLMs) have shown\nremarkable capabilities in integrating vision and language for complex\nreasoning. While most existing benchmarks evaluate models under offline\nsettings with a fixed set of pre-recorded inputs, we introduce OST-Bench, a\nbenchmark designed to evaluate Online Spatio-Temporal understanding from the\nperspective of an agent actively exploring a scene. The Online aspect\nemphasizes the need to process and reason over incrementally acquired\nobservations, while the Spatio-Temporal component requires integrating current\nvisual inputs with historical memory to support dynamic spatial reasoning.\nOST-Bench better reflects the challenges of real-world embodied perception.\nBuilt on an efficient data collection pipeline, OST-Bench consists of 1.4k\nscenes and 10k question-answer pairs collected from ScanNet, Matterport3D, and\nARKitScenes. We evaluate several leading MLLMs on OST-Bench and observe that\nthey fall short on tasks requiring complex spatio-temporal reasoning. Under the\nonline setting, their accuracy declines as the exploration horizon extends and\nthe memory grows. Through further experimental analysis, we identify common\nerror patterns across models and find that both complex clue-based spatial\nreasoning demands and long-term memory retrieval requirements significantly\ndrop model performance along two separate axes, highlighting the core\nchallenges that must be addressed to improve online embodied reasoning. To\nfoster further research and development in the field, our codes, dataset, and\nbenchmark are available. Our project page is:\nhttps://rbler1234.github.io/OSTBench.github.io/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6433aba4546e16f17a0f19f6/pNjo9cepo_BTSUgc_ZLsg.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07984.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6433aba4546e16f17a0f19f6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6433aba4546e16f17a0f19f6/3hlNy4_Suy0bO__Qa9amL.jpeg",
            "fullname": "Chenming Zhu",
            "name": "ChaimZhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07990",
            "authors": [
                {
                    "_id": "68708156c8391850d6097869",
                    "user": {
                        "_id": "6513030fb3a463e17df56edd",
                        "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
                        "isPro": false,
                        "fullname": "Hyun, Jeongseok",
                        "user": "js-hyun",
                        "type": "user"
                    },
                    "name": "Jeongseok Hyun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:01.307Z",
                    "hidden": false
                },
                {
                    "_id": "68708156c8391850d609786a",
                    "name": "Sukjun Hwang",
                    "hidden": false
                },
                {
                    "_id": "68708156c8391850d609786b",
                    "name": "Su Ho Han",
                    "hidden": false
                },
                {
                    "_id": "68708156c8391850d609786c",
                    "name": "Taeoh Kim",
                    "hidden": false
                },
                {
                    "_id": "68708156c8391850d609786d",
                    "name": "Inwoong Lee",
                    "hidden": false
                },
                {
                    "_id": "68708156c8391850d609786e",
                    "name": "Dongyoon Wee",
                    "hidden": false
                },
                {
                    "_id": "68708156c8391850d609786f",
                    "name": "Joon-Young Lee",
                    "hidden": false
                },
                {
                    "_id": "68708156c8391850d6097870",
                    "name": "Seon Joo Kim",
                    "hidden": false
                },
                {
                    "_id": "68708156c8391850d6097871",
                    "name": "Minho Shim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T17:59:02.000Z",
            "submittedOnDailyAt": "2025-07-11T05:04:15.008Z",
            "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
            "submittedOnDailyBy": {
                "_id": "6513030fb3a463e17df56edd",
                "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
                "isPro": false,
                "fullname": "Hyun, Jeongseok",
                "user": "js-hyun",
                "type": "user"
            },
            "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
            "upvotes": 24,
            "discussionId": "68708157c8391850d6097872",
            "projectPage": "https://www.jshyun.me/projects/sttm",
            "githubRepo": "https://github.com/HYUNJS/STTM",
            "ai_summary": "A spatio-temporal token merging method improves video LLM efficiency by exploiting redundancy, achieving significant speed-ups with minimal accuracy loss.",
            "ai_keywords": [
                "spatio-temporal tokens",
                "quadratic computational scaling",
                "token merging method",
                "STTM",
                "multi-granular spatial tokens",
                "quadtree structure",
                "directed pairwise merging",
                "video QA benchmarks",
                "token budget",
                "query-agnostic",
                "KV cache reuse"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-07-10T13:59:02.000Z",
        "title": "Multi-Granular Spatio-Temporal Token Merging for Training-Free\n  Acceleration of Video LLMs",
        "summary": "Video large language models (LLMs) achieve strong video understanding by\nleveraging a large number of spatio-temporal tokens, but suffer from quadratic\ncomputational scaling with token count. To address this, we propose a\ntraining-free spatio-temporal token merging method, named STTM. Our key insight\nis to exploit local spatial and temporal redundancy in video data which has\nbeen overlooked in prior work. STTM first transforms each frame into\nmulti-granular spatial tokens using a coarse-to-fine search over a quadtree\nstructure, then performs directed pairwise merging across the temporal\ndimension. This decomposed merging approach outperforms existing token\nreduction methods across six video QA benchmarks. Notably, STTM achieves a\n2times speed-up with only a 0.5% accuracy drop under a 50% token budget, and\na 3times speed-up with just a 2% drop under a 30% budget. Moreover, STTM is\nquery-agnostic, allowing KV cache reuse across different questions for the same\nvideo. The project page is available at https://www.jshyun.me/projects/sttm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07990.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6513030fb3a463e17df56edd",
            "avatarUrl": "/avatars/867bd4316b2de758654ad3a84ea868c1.svg",
            "fullname": "Hyun, Jeongseok",
            "name": "js-hyun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07998",
            "authors": [
                {
                    "_id": "687068dec8391850d60977e2",
                    "user": {
                        "_id": "62c66504031996c36c86976a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
                        "isPro": true,
                        "fullname": "steve z",
                        "user": "stzhao",
                        "type": "user"
                    },
                    "name": "Shitian Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:02:29.862Z",
                    "hidden": false
                },
                {
                    "_id": "687068dec8391850d60977e3",
                    "user": {
                        "_id": "67ff7f687351095d4b606b84",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ff7f687351095d4b606b84/KhNPmbBC3zghuP5h1MK-c.png",
                        "isPro": false,
                        "fullname": "Haoquan Zhang",
                        "user": "haoquan03",
                        "type": "user"
                    },
                    "name": "Haoquan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:02:26.572Z",
                    "hidden": false
                },
                {
                    "_id": "687068dec8391850d60977e4",
                    "name": "Shaoheng Lin",
                    "hidden": false
                },
                {
                    "_id": "687068dec8391850d60977e5",
                    "user": {
                        "_id": "6794cd79b72b1721ea69f4f2",
                        "avatarUrl": "/avatars/4e4fb9e9e127a0c031131ace705687cd.svg",
                        "isPro": false,
                        "fullname": "Ming Li",
                        "user": "afdsafas",
                        "type": "user"
                    },
                    "name": "Ming Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T13:12:43.797Z",
                    "hidden": false
                },
                {
                    "_id": "687068dec8391850d60977e6",
                    "user": {
                        "_id": "64379d79fac5ea753f1c10f3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64379d79fac5ea753f1c10f3/clfjIaMTVDTG9K04dRud_.png",
                        "isPro": false,
                        "fullname": "Jerry Wu",
                        "user": "QJerry",
                        "type": "user"
                    },
                    "name": "Qilong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T13:12:42.192Z",
                    "hidden": false
                },
                {
                    "_id": "687068dec8391850d60977e7",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "687068dec8391850d60977e8",
                    "name": "Chen Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T17:59:55.000Z",
            "submittedOnDailyAt": "2025-07-11T00:33:22.135Z",
            "title": "PyVision: Agentic Vision with Dynamic Tooling",
            "submittedOnDailyBy": {
                "_id": "62c66504031996c36c86976a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
                "isPro": true,
                "fullname": "steve z",
                "user": "stzhao",
                "type": "user"
            },
            "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.",
            "upvotes": 22,
            "discussionId": "687068dec8391850d60977e9",
            "projectPage": "https://agent-x.space/pyvision/",
            "githubRepo": "https://github.com/agents-x-project/PyVision",
            "ai_summary": "PyVision, an interactive framework, enables LLMs to autonomously create and refine Python-based tools for visual reasoning, achieving significant performance improvements across benchmarks.",
            "ai_keywords": [
                "LLMs",
                "agents",
                "visual reasoning",
                "predefined workflows",
                "static toolsets",
                "interactive framework",
                "multi-turn framework",
                "autonomously generate",
                "execute",
                "refine",
                "Python-based tools",
                "taxonomy",
                "benchmarks",
                "GPT-4.1",
                "Claude-4.0-Sonnet",
                "V*",
                "VLMsAreBlind-mini",
                "dynamic tooling",
                "agentic visual reasoning"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-07-10T13:59:55.000Z",
        "title": "PyVision: Agentic Vision with Dynamic Tooling",
        "summary": "LLMs are increasingly deployed as agents, systems capable of planning,\nreasoning, and dynamically calling external tools. However, in visual\nreasoning, prior approaches largely remain limited by predefined workflows and\nstatic toolsets. In this report, we present PyVision, an interactive,\nmulti-turn framework that enables MLLMs to autonomously generate, execute, and\nrefine Python-based tools tailored to the task at hand, unlocking flexible and\ninterpretable problem-solving. We develop a taxonomy of the tools created by\nPyVision and analyze their usage across a diverse set of benchmarks.\nQuantitatively, PyVision achieves consistent performance gains, boosting\nGPT-4.1 by +7.8% on V* and Claude-4.0-Sonnet by +31.1% on VLMsAreBlind-mini.\nThese results point to a broader shift: dynamic tooling allows models not just\nto use tools, but to invent them, advancing toward more agentic visual\nreasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07998.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62c66504031996c36c86976a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62c66504031996c36c86976a/wIq0YJhkWnEhlzsh-TGYO.png",
            "fullname": "steve z",
            "name": "stzhao",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07982",
            "authors": [
                {
                    "_id": "68707ef2c8391850d6097860",
                    "user": {
                        "_id": "6590f7880c993129053a2344",
                        "avatarUrl": "/avatars/d08049493234edb8e23f1c1531e386d3.svg",
                        "isPro": false,
                        "fullname": "Haoyu wu",
                        "user": "Haoyuwu",
                        "type": "user"
                    },
                    "name": "Haoyu Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:05.153Z",
                    "hidden": false
                },
                {
                    "_id": "68707ef2c8391850d6097861",
                    "user": {
                        "_id": "6577fba2eb02736add6377f5",
                        "avatarUrl": "/avatars/3e486dd36021feaa4a0259cd89c1eee9.svg",
                        "isPro": false,
                        "fullname": "Wu",
                        "user": "Diankun",
                        "type": "user"
                    },
                    "name": "Diankun Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T16:31:56.924Z",
                    "hidden": false
                },
                {
                    "_id": "68707ef2c8391850d6097862",
                    "user": {
                        "_id": "619b7b1cab4c7b7f16a7d59e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619b7b1cab4c7b7f16a7d59e/6TvXaAqBghAMYO1-j5l4v.jpeg",
                        "isPro": false,
                        "fullname": "Tianyu He",
                        "user": "deeptimhe",
                        "type": "user"
                    },
                    "name": "Tianyu He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:03.023Z",
                    "hidden": false
                },
                {
                    "_id": "68707ef2c8391850d6097863",
                    "name": "Junliang Guo",
                    "hidden": false
                },
                {
                    "_id": "68707ef2c8391850d6097864",
                    "name": "Yang Ye",
                    "hidden": false
                },
                {
                    "_id": "68707ef2c8391850d6097865",
                    "name": "Yueqi Duan",
                    "hidden": false
                },
                {
                    "_id": "68707ef2c8391850d6097866",
                    "name": "Jiang Bian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T17:55:08.000Z",
            "submittedOnDailyAt": "2025-07-11T02:06:14.101Z",
            "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
            "submittedOnDailyBy": {
                "_id": "6577fba2eb02736add6377f5",
                "avatarUrl": "/avatars/3e486dd36021feaa4a0259cd89c1eee9.svg",
                "isPro": false,
                "fullname": "Wu",
                "user": "Diankun",
                "type": "user"
            },
            "summary": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.",
            "upvotes": 21,
            "discussionId": "68707ef2c8391850d6097867",
            "projectPage": "https://geometryforcing.github.io/"
        },
        "publishedAt": "2025-07-10T13:55:08.000Z",
        "title": "Geometry Forcing: Marrying Video Diffusion and 3D Representation for\n  Consistent World Modeling",
        "summary": "Videos inherently represent 2D projections of a dynamic 3D world. However,\nour analysis suggests that video diffusion models trained solely on raw video\ndata often fail to capture meaningful geometric-aware structure in their\nlearned representations. To bridge this gap between video diffusion models and\nthe underlying 3D nature of the physical world, we propose Geometry Forcing, a\nsimple yet effective method that encourages video diffusion models to\ninternalize latent 3D representations. Our key insight is to guide the model's\nintermediate representations toward geometry-aware structure by aligning them\nwith features from a pretrained geometric foundation model. To this end, we\nintroduce two complementary alignment objectives: Angular Alignment, which\nenforces directional consistency via cosine similarity, and Scale Alignment,\nwhich preserves scale-related information by regressing unnormalized geometric\nfeatures from normalized diffusion representation. We evaluate Geometry Forcing\non both camera view-conditioned and action-conditioned video generation tasks.\nExperimental results demonstrate that our method substantially improves visual\nquality and 3D consistency over the baseline methods. Project page:\nhttps://GeometryForcing.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07982.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6577fba2eb02736add6377f5",
            "avatarUrl": "/avatars/3e486dd36021feaa4a0259cd89c1eee9.svg",
            "fullname": "Wu",
            "name": "Diankun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07136",
            "authors": [
                {
                    "_id": "68708cb9c8391850d609788f",
                    "name": "Wanhua Li",
                    "hidden": false
                },
                {
                    "_id": "68708cb9c8391850d6097890",
                    "name": "Yujie Zhao",
                    "hidden": false
                },
                {
                    "_id": "68708cb9c8391850d6097891",
                    "name": "Minghan Qin",
                    "hidden": false
                },
                {
                    "_id": "68708cb9c8391850d6097892",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "68708cb9c8391850d6097893",
                    "name": "Yuanhao Cai",
                    "hidden": false
                },
                {
                    "_id": "68708cb9c8391850d6097894",
                    "name": "Chuang Gan",
                    "hidden": false
                },
                {
                    "_id": "68708cb9c8391850d6097895",
                    "name": "Hanspeter Pfister",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/tfzmPoFtikY2cpVZVd5Rt.mp4"
            ],
            "publishedAt": "2025-07-09T00:19:58.000Z",
            "submittedOnDailyAt": "2025-07-11T02:43:05.954Z",
            "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS",
            "submittedOnDailyBy": {
                "_id": "658bb7e47459b6e471b9d2e6",
                "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
                "isPro": false,
                "fullname": "Wanhua Li",
                "user": "EthanTaylor",
                "type": "user"
            },
            "summary": "In this paper, we introduce LangSplatV2, which achieves high-dimensional\nfeature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6\nFPS for high-resolution images, providing a 42 times speedup and a 47\ntimes boost over LangSplat respectively, along with improved query accuracy.\nLangSplat employs Gaussian Splatting to embed 2D CLIP language features into\n3D, significantly enhancing speed and learning a precise 3D language field with\nSAM semantics. Such advancements in 3D language fields are crucial for\napplications that require language interaction within complex scenes. However,\nLangSplat does not yet achieve real-time inference performance (8.2 FPS), even\nwith advanced A100 GPUs, severely limiting its broader application. In this\npaper, we first conduct a detailed time analysis of LangSplat, identifying the\nheavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2\nassumes that each Gaussian acts as a sparse code within a global dictionary,\nleading to the learning of a 3D sparse coefficient field that entirely\neliminates the need for a heavyweight decoder. By leveraging this sparsity, we\nfurther propose an efficient sparse coefficient splatting method with CUDA\noptimization, rendering high-dimensional feature maps at high quality while\nincurring only the time cost of splatting an ultra-low-dimensional feature. Our\nexperimental results demonstrate that LangSplatV2 not only achieves better or\ncompetitive query accuracy but is also significantly faster. Codes and demos\nare available at our project page: https://langsplat-v2.github.io.",
            "upvotes": 19,
            "discussionId": "68708cbac8391850d6097896",
            "ai_summary": "LangSplatV2 enhances 3D text querying speed and accuracy by replacing the heavyweight decoder with a sparse coefficient field and efficient CUDA optimization.",
            "ai_keywords": [
                "Gaussian Splatting",
                "CLIP language features",
                "3D language field",
                "SAM semantics",
                "sparse code",
                "sparse coefficient field",
                "sparse coefficient splatting",
                "CUDA optimization"
            ]
        },
        "publishedAt": "2025-07-08T20:19:58.000Z",
        "title": "LangSplatV2: High-dimensional 3D Language Gaussian Splatting with 450+\n  FPS",
        "summary": "In this paper, we introduce LangSplatV2, which achieves high-dimensional\nfeature splatting at 476.2 FPS and 3D open-vocabulary text querying at 384.6\nFPS for high-resolution images, providing a 42 times speedup and a 47\ntimes boost over LangSplat respectively, along with improved query accuracy.\nLangSplat employs Gaussian Splatting to embed 2D CLIP language features into\n3D, significantly enhancing speed and learning a precise 3D language field with\nSAM semantics. Such advancements in 3D language fields are crucial for\napplications that require language interaction within complex scenes. However,\nLangSplat does not yet achieve real-time inference performance (8.2 FPS), even\nwith advanced A100 GPUs, severely limiting its broader application. In this\npaper, we first conduct a detailed time analysis of LangSplat, identifying the\nheavyweight decoder as the primary speed bottleneck. Our solution, LangSplatV2\nassumes that each Gaussian acts as a sparse code within a global dictionary,\nleading to the learning of a 3D sparse coefficient field that entirely\neliminates the need for a heavyweight decoder. By leveraging this sparsity, we\nfurther propose an efficient sparse coefficient splatting method with CUDA\noptimization, rendering high-dimensional feature maps at high quality while\nincurring only the time cost of splatting an ultra-low-dimensional feature. Our\nexperimental results demonstrate that LangSplatV2 not only achieves better or\ncompetitive query accuracy but is also significantly faster. Codes and demos\nare available at our project page: https://langsplat-v2.github.io.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/658bb7e47459b6e471b9d2e6/tfzmPoFtikY2cpVZVd5Rt.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07136.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "658bb7e47459b6e471b9d2e6",
            "avatarUrl": "/avatars/efd8051b468b4dbcb5d149479de67c58.svg",
            "fullname": "Wanhua Li",
            "name": "EthanTaylor",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07202",
            "authors": [
                {
                    "_id": "68707dfbc8391850d6097841",
                    "user": {
                        "_id": "659d164f4b29e5948c66b9f6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/659d164f4b29e5948c66b9f6/h42oa-B6MIW7Dhblt5dO7.jpeg",
                        "isPro": false,
                        "fullname": "Mohamed Elmoghany",
                        "user": "elmoghany",
                        "type": "user"
                    },
                    "name": "Mohamed Elmoghany",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:12.603Z",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097842",
                    "name": "Ryan Rossi",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097843",
                    "name": "Seunghyun Yoon",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097844",
                    "name": "Subhojyoti Mukherjee",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097845",
                    "name": "Eslam Bakr",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097846",
                    "name": "Puneet Mathur",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097847",
                    "name": "Gang Wu",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097848",
                    "name": "Viet Dac Lai",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097849",
                    "name": "Nedim Lipka",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609784a",
                    "name": "Ruiyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609784b",
                    "name": "Varun Manjunatha",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609784c",
                    "name": "Chien Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609784d",
                    "name": "Daksh Dangi",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609784e",
                    "name": "Abel Salinas",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609784f",
                    "user": {
                        "_id": "6039478ab3ecf716b1a5fd4d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                        "isPro": true,
                        "fullname": "taesiri",
                        "user": "taesiri",
                        "type": "user"
                    },
                    "name": "Mohammad Taesiri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:08.440Z",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097850",
                    "name": "Hongjie Chen",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097851",
                    "name": "Xiaolei Huang",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097852",
                    "name": "Joe Barrow",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097853",
                    "name": "Nesreen Ahmed",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097854",
                    "name": "Hoda Eldardiry",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097855",
                    "name": "Namyong Park",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097856",
                    "name": "Yu Wang",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097857",
                    "user": {
                        "_id": "5ffe32d8942cf3533d364449",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
                        "isPro": false,
                        "fullname": "Jaemin Cho",
                        "user": "j-min",
                        "type": "user"
                    },
                    "name": "Jaemin Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T16:31:58.647Z",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097858",
                    "name": "Anh Totti Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d6097859",
                    "name": "Zhengzhong Tu",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609785a",
                    "name": "Thien Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609785b",
                    "name": "Dinesh Manocha",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609785c",
                    "name": "Mohamed Elhoseiny",
                    "hidden": false
                },
                {
                    "_id": "68707dfbc8391850d609785d",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:00:10.400Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T18:20:33.000Z",
            "submittedOnDailyAt": "2025-07-11T01:30:02.686Z",
            "title": "A Survey on Long-Video Storytelling Generation: Architectures,\n  Consistency, and Cinematic Quality",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.",
            "upvotes": 17,
            "discussionId": "68707dfcc8391850d609785e"
        },
        "publishedAt": "2025-07-09T14:20:33.000Z",
        "title": "A Survey on Long-Video Storytelling Generation: Architectures,\n  Consistency, and Cinematic Quality",
        "summary": "Despite the significant progress that has been made in video generative\nmodels, existing state-of-the-art methods can only produce videos lasting 5-16\nseconds, often labeled \"long-form videos\". Furthermore, videos exceeding 16\nseconds struggle to maintain consistent character appearances and scene layouts\nthroughout the narrative. In particular, multi-subject long videos still fail\nto preserve character consistency and motion coherence. While some methods can\ngenerate videos up to 150 seconds long, they often suffer from frame redundancy\nand low temporal diversity. Recent work has attempted to produce long-form\nvideos featuring multiple characters, narrative coherence, and high-fidelity\ndetail. We comprehensively studied 32 papers on video generation to identify\nkey architectural components and training strategies that consistently yield\nthese qualities. We also construct a comprehensive novel taxonomy of existing\nmethods and present comparative tables that categorize papers by their\narchitectural designs and performance characteristics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07202.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07996",
            "authors": [
                {
                    "_id": "68709572c8391850d60978b7",
                    "name": "Ziyue Li",
                    "hidden": false
                },
                {
                    "_id": "68709572c8391850d60978b8",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68709572c8391850d60978b9",
                    "user": {
                        "_id": "647f5af5b0e96764589f3b2a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                        "isPro": false,
                        "fullname": "Tianyi Zhou",
                        "user": "zhoutianyi",
                        "type": "user"
                    },
                    "name": "Tianyi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T07:59:58.984Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T17:59:53.000Z",
            "submittedOnDailyAt": "2025-07-11T03:54:10.527Z",
            "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
            "submittedOnDailyBy": {
                "_id": "647f5af5b0e96764589f3b2a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
                "isPro": false,
                "fullname": "Tianyi Zhou",
                "user": "zhoutianyi",
                "type": "user"
            },
            "summary": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.",
            "upvotes": 14,
            "discussionId": "68709573c8391850d60978ba",
            "ai_summary": "A method using chain-of-layers (CoLa) derived from a pretrained large language model allows for dynamic architecture adaptation, improving efficiency and accuracy across diverse tasks through selective layer manipulation and Monte Carlo Tree Search optimization.",
            "ai_keywords": [
                "pretrained large language model (LLM)",
                "chain-of-layers (CoLa)",
                "Monte Carlo Tree Search (MCTS)",
                "looped/recurrent pretrained modules",
                "layer pruning",
                "early-exit networks",
                "test-time depth adaptation"
            ]
        },
        "publishedAt": "2025-07-10T13:59:53.000Z",
        "title": "Skip a Layer or Loop it? Test-Time Depth Adaptation of Pretrained LLMs",
        "summary": "Can a pretrained neural network adapt its architecture to different inputs\nwithout any finetuning? Do we need all layers for simple tasks, and are they\nadequate for challenging tasks? We found that the layers of a pretrained large\nlanguage model (LLM) can be manipulated as separate modules to build a better\nand even shallower model customized for each test sample. In particular, each\nlayer from the pretrained model can be skipped/pruned or repeated multiple\ntimes as recurrent neural networks (RNN), and stacked with others in arbitrary\norders, yielding a chain-of-layers (CoLa) per sample. This compositional space\ngreatly expands the scope of existing works on looped/recurrent pretrained\nmodules, layer pruning, or early-exit networks. We develop a Monte Carlo Tree\nSearch (MCTS) protocol to explore and identify the optimal CoLa for each sample\nfrom math and commonsense reasoning benchmarks. Compared to a static model of a\nfixed depth, CoLa allows shortcut paths (fast thinking), recurrence of the same\nlayer(s) (slow thinking), and combining both, offering more flexible, dynamic\narchitectures for different inputs. We conduct an extensive analysis of the\nMCTS-optimized CoLa, which leads to two key findings: (1) For >75% of samples\nwith correct predictions by the original LLM, we can find shorter CoLa,\nsuggesting a large space for improving inference efficiency; (2) For >60% of\nsamples with originally incorrect predictions, we can identify CoLa achieving\ncorrect predictions, suggesting a large space of performance enhancement. Our\nresults highlight the shortcomings of using a fixed architecture of pre-trained\nLLMs for inference on different samples and pave the way to unlock the\ngeneralization power of test-time depth adaptation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07996.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "647f5af5b0e96764589f3b2a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/VJ4cDyjp5M3V5WmI5gPIU.jpeg",
            "fullname": "Tianyi Zhou",
            "name": "zhoutianyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.06543",
            "authors": [
                {
                    "_id": "686f9aad706a6ea465418a08",
                    "user": {
                        "_id": "67c6a1e75e2443d7d5f85cb3",
                        "avatarUrl": "/avatars/0569b368520411ab828d46725bc3896a.svg",
                        "isPro": false,
                        "fullname": "Taekyung Kim",
                        "user": "taekyung-k",
                        "type": "user"
                    },
                    "name": "Taekyung Kim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:03:22.401Z",
                    "hidden": false
                },
                {
                    "_id": "686f9aad706a6ea465418a09",
                    "user": {
                        "_id": "660f8cc1a61244f3df3d4426",
                        "avatarUrl": "/avatars/45d59766122bb3482f6dd7f9d98aa87a.svg",
                        "isPro": false,
                        "fullname": "Dongyoon Han",
                        "user": "calintz",
                        "type": "user"
                    },
                    "name": "Dongyoon Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:03:14.509Z",
                    "hidden": false
                },
                {
                    "_id": "686f9aad706a6ea465418a0a",
                    "user": {
                        "_id": "64b9feed96676e40d0fa89a7",
                        "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
                        "isPro": false,
                        "fullname": "Byeongho Heo",
                        "user": "bhheo",
                        "type": "user"
                    },
                    "name": "Byeongho Heo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T08:03:16.434Z",
                    "hidden": false
                },
                {
                    "_id": "686f9aad706a6ea465418a0b",
                    "name": "Jeongeun Park",
                    "hidden": false
                },
                {
                    "_id": "686f9aad706a6ea465418a0c",
                    "name": "Sangdoo Yun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T04:57:29.000Z",
            "submittedOnDailyAt": "2025-07-11T04:21:34.394Z",
            "title": "Token Bottleneck: One Token to Remember Dynamics",
            "submittedOnDailyBy": {
                "_id": "64b9feed96676e40d0fa89a7",
                "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
                "isPro": false,
                "fullname": "Byeongho Heo",
                "user": "bhheo",
                "type": "user"
            },
            "summary": "Deriving compact and temporally aware visual representations from dynamic\nscenes is essential for successful execution of sequential scene understanding\ntasks such as visual tracking and robotic manipulation. In this paper, we\nintroduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised\nlearning pipeline that squeezes a scene into a bottleneck token and predicts\nthe subsequent scene using minimal patches as hints. The ToBo pipeline\nfacilitates the learning of sequential scene representations by conservatively\nencoding the reference scene into a compact bottleneck token during the squeeze\nstep. In the expansion step, we guide the model to capture temporal dynamics by\npredicting the target scene using the bottleneck token along with few target\npatches as hints. This design encourages the vision backbone to embed temporal\ndependencies, thereby enabling understanding of dynamic transitions across\nscenes. Extensive experiments in diverse sequential tasks, including video\nlabel propagation and robot manipulation in simulated environments demonstrate\nthe superiority of ToBo over baselines. Moreover, deploying our pre-trained\nmodel on physical robots confirms its robustness and effectiveness in\nreal-world environments. We further validate the scalability of ToBo across\ndifferent model scales.",
            "upvotes": 11,
            "discussionId": "686f9aae706a6ea465418a0d",
            "projectPage": "https://token-bottleneck.github.io",
            "githubRepo": "https://github.com/naver-ai/tobo",
            "ai_summary": "ToBo is a self-supervised learning method that creates compact, temporally aware visual representations for sequential scene understanding tasks, outperforming baselines in both simulated and real-world environments.",
            "ai_keywords": [
                "Token Bottleneck",
                "self-supervised learning",
                "bottleneck token",
                "sequential scene representations",
                "temporal dependencies",
                "video label propagation",
                "robot manipulation"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-07-09T00:57:29.000Z",
        "title": "Token Bottleneck: One Token to Remember Dynamics",
        "summary": "Deriving compact and temporally aware visual representations from dynamic\nscenes is essential for successful execution of sequential scene understanding\ntasks such as visual tracking and robotic manipulation. In this paper, we\nintroduce Token Bottleneck (ToBo), a simple yet intuitive self-supervised\nlearning pipeline that squeezes a scene into a bottleneck token and predicts\nthe subsequent scene using minimal patches as hints. The ToBo pipeline\nfacilitates the learning of sequential scene representations by conservatively\nencoding the reference scene into a compact bottleneck token during the squeeze\nstep. In the expansion step, we guide the model to capture temporal dynamics by\npredicting the target scene using the bottleneck token along with few target\npatches as hints. This design encourages the vision backbone to embed temporal\ndependencies, thereby enabling understanding of dynamic transitions across\nscenes. Extensive experiments in diverse sequential tasks, including video\nlabel propagation and robot manipulation in simulated environments demonstrate\nthe superiority of ToBo over baselines. Moreover, deploying our pre-trained\nmodel on physical robots confirms its robustness and effectiveness in\nreal-world environments. We further validate the scalability of ToBo across\ndifferent model scales.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.06543.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b9feed96676e40d0fa89a7",
            "avatarUrl": "/avatars/2154a6ceb87677ad2c9d9620de5b18ec.svg",
            "fullname": "Byeongho Heo",
            "name": "bhheo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07484",
            "authors": [
                {
                    "_id": "68708d7ac8391850d609789f",
                    "name": "Kaiqu Liang",
                    "hidden": false
                },
                {
                    "_id": "68708d7ac8391850d60978a0",
                    "name": "Haimin Hu",
                    "hidden": false
                },
                {
                    "_id": "68708d7ac8391850d60978a1",
                    "name": "Xuandong Zhao",
                    "hidden": false
                },
                {
                    "_id": "68708d7ac8391850d60978a2",
                    "name": "Dawn Song",
                    "hidden": false
                },
                {
                    "_id": "68708d7ac8391850d60978a3",
                    "name": "Thomas L. Griffiths",
                    "hidden": false
                },
                {
                    "_id": "68708d7ac8391850d60978a4",
                    "name": "Jaime Fernndez Fisac",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T07:11:57.000Z",
            "submittedOnDailyAt": "2025-07-11T02:36:07.699Z",
            "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6275a465597c70eb8949fce5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
                "isPro": false,
                "fullname": "Xuandong Zhao",
                "user": "Xuandong",
                "type": "user"
            },
            "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.",
            "upvotes": 5,
            "discussionId": "68708d7ac8391850d60978a5",
            "ai_summary": "Machine bullshit, characterized by LLMs' indifference to truth, is quantified and analyzed through a new framework, revealing that RLHF and CoT prompting exacerbate certain bullshit forms.",
            "ai_keywords": [
                "Bullshit Index",
                "reinforcement learning from human feedback (RLHF)",
                "chain-of-thought (CoT) prompting",
                "empty rhetoric",
                "paltering",
                "weasel words",
                "unverified claims",
                "AI alignment"
            ]
        },
        "publishedAt": "2025-07-10T03:11:57.000Z",
        "title": "Machine Bullshit: Characterizing the Emergent Disregard for Truth in\n  Large Language Models",
        "summary": "Bullshit, as conceptualized by philosopher Harry Frankfurt, refers to\nstatements made without regard to their truth value. While previous work has\nexplored large language model (LLM) hallucination and sycophancy, we propose\nmachine bullshit as an overarching conceptual framework that can allow\nresearchers to characterize the broader phenomenon of emergent loss of\ntruthfulness in LLMs and shed light on its underlying mechanisms. We introduce\nthe Bullshit Index, a novel metric quantifying LLMs' indifference to truth, and\npropose a complementary taxonomy analyzing four qualitative forms of bullshit:\nempty rhetoric, paltering, weasel words, and unverified claims. We conduct\nempirical evaluations on the Marketplace dataset, the Political Neutrality\ndataset, and our new BullshitEval benchmark (2,400 scenarios spanning 100 AI\nassistants) explicitly designed to evaluate machine bullshit. Our results\ndemonstrate that model fine-tuning with reinforcement learning from human\nfeedback (RLHF) significantly exacerbates bullshit and inference-time\nchain-of-thought (CoT) prompting notably amplify specific bullshit forms,\nparticularly empty rhetoric and paltering. We also observe prevalent machine\nbullshit in political contexts, with weasel words as the dominant strategy. Our\nfindings highlight systematic challenges in AI alignment and provide new\ninsights toward more truthful LLM behavior.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07484.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6275a465597c70eb8949fce5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6275a465597c70eb8949fce5/ph4UogqMurMB0hSXZC38w.png",
            "fullname": "Xuandong Zhao",
            "name": "Xuandong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07955",
            "authors": [
                {
                    "_id": "6871627f257d4f0435370075",
                    "name": "Sukjun Hwang",
                    "hidden": false
                },
                {
                    "_id": "6871627f257d4f0435370076",
                    "name": "Brandon Wang",
                    "hidden": false
                },
                {
                    "_id": "6871627f257d4f0435370077",
                    "name": "Albert Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T17:39:37.000Z",
            "submittedOnDailyAt": "2025-07-11T17:45:08.790Z",
            "title": "Dynamic Chunking for End-to-End Hierarchical Sequence Modeling",
            "submittedOnDailyBy": {
                "_id": "60eeedbf50b60c406afc1291",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649111275459-60eeedbf50b60c406afc1291.png",
                "isPro": false,
                "fullname": "Samuel Arcadinho",
                "user": "SSamDav",
                "type": "user"
            },
            "summary": "Despite incredible progress in language models (LMs) in recent years, largely\nresulting from moving away from specialized models designed for specific tasks\nto general models based on powerful architectures (e.g. the Transformer) that\nlearn everything from raw data, pre-processing steps such as tokenization\nremain a barrier to true end-to-end foundation models. We introduce a\ncollection of new techniques that enable a dynamic chunking mechanism which\nautomatically learns content -- and context -- dependent segmentation\nstrategies learned jointly with the rest of the model. Incorporating this into\nan explicit hierarchical network (H-Net) allows replacing the (implicitly\nhierarchical) tokenization-LM-detokenization pipeline with a single model\nlearned fully end-to-end. When compute- and data- matched, an H-Net with one\nstage of hierarchy operating at the byte level outperforms a strong Transformer\nlanguage model operating over BPE tokens. Iterating the hierarchy to multiple\nstages further increases its performance by modeling multiple levels of\nabstraction, demonstrating significantly better scaling with data and matching\na token-based Transformer of twice its size. H-Nets pretrained on English show\nsignificantly increased character-level robustness, and qualitatively learn\nmeaningful data-dependent chunking strategies without any heuristics or\nexplicit supervision. Finally, the H-Net's improvement over tokenized pipelines\nis further increased in languages and modalities with weaker tokenization\nheuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement\nin data efficiency over baselines), showing the potential of true end-to-end\nmodels that learn and scale better from unprocessed data.",
            "upvotes": 4,
            "discussionId": "6871627f257d4f0435370078",
            "ai_summary": "Hierarchical networks replace traditional tokenization pipelines by dynamically learning segmentation strategies, achieving better performance and scalability across various languages and modalities.",
            "ai_keywords": [
                "Transformer",
                "dynamic chunking mechanism",
                "hierarchical network (H-Net)",
                "tokenization-LM-detokenization pipeline",
                "byte level",
                "multiple stages of hierarchy",
                "character-level robustness",
                "data-dependent chunking strategies",
                "data efficiency"
            ]
        },
        "publishedAt": "2025-07-10T13:39:37.000Z",
        "title": "Dynamic Chunking for End-to-End Hierarchical Sequence Modeling",
        "summary": "Despite incredible progress in language models (LMs) in recent years, largely\nresulting from moving away from specialized models designed for specific tasks\nto general models based on powerful architectures (e.g. the Transformer) that\nlearn everything from raw data, pre-processing steps such as tokenization\nremain a barrier to true end-to-end foundation models. We introduce a\ncollection of new techniques that enable a dynamic chunking mechanism which\nautomatically learns content -- and context -- dependent segmentation\nstrategies learned jointly with the rest of the model. Incorporating this into\nan explicit hierarchical network (H-Net) allows replacing the (implicitly\nhierarchical) tokenization-LM-detokenization pipeline with a single model\nlearned fully end-to-end. When compute- and data- matched, an H-Net with one\nstage of hierarchy operating at the byte level outperforms a strong Transformer\nlanguage model operating over BPE tokens. Iterating the hierarchy to multiple\nstages further increases its performance by modeling multiple levels of\nabstraction, demonstrating significantly better scaling with data and matching\na token-based Transformer of twice its size. H-Nets pretrained on English show\nsignificantly increased character-level robustness, and qualitatively learn\nmeaningful data-dependent chunking strategies without any heuristics or\nexplicit supervision. Finally, the H-Net's improvement over tokenized pipelines\nis further increased in languages and modalities with weaker tokenization\nheuristics, such as Chinese and code, or DNA sequences (nearly 4x improvement\nin data efficiency over baselines), showing the potential of true end-to-end\nmodels that learn and scale better from unprocessed data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07955.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60eeedbf50b60c406afc1291",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1649111275459-60eeedbf50b60c406afc1291.png",
            "fullname": "Samuel Arcadinho",
            "name": "SSamDav",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07574",
            "authors": [
                {
                    "_id": "6870c2d3c8391850d60978e6",
                    "user": {
                        "_id": "63aadfe9a4bdd629b7ea7692",
                        "avatarUrl": "/avatars/9cc4eb5d4090ce84a590ec195b70e545.svg",
                        "isPro": false,
                        "fullname": "Enrico",
                        "user": "envomp",
                        "type": "user"
                    },
                    "name": "Enrico Vompa",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T07:59:54.331Z",
                    "hidden": false
                },
                {
                    "_id": "6870c2d3c8391850d60978e7",
                    "name": "Tanel Tammet",
                    "hidden": false
                },
                {
                    "_id": "6870c2d3c8391850d60978e8",
                    "name": "Mohit Vaishnav",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T09:23:32.000Z",
            "submittedOnDailyAt": "2025-07-11T06:38:11.205Z",
            "title": "Beyond the Linear Separability Ceiling",
            "submittedOnDailyBy": {
                "_id": "63aadfe9a4bdd629b7ea7692",
                "avatarUrl": "/avatars/9cc4eb5d4090ce84a590ec195b70e545.svg",
                "isPro": false,
                "fullname": "Enrico",
                "user": "envomp",
                "type": "user"
            },
            "summary": "Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by\nthe linear separabilty of their visual embeddings on abstract reasoning tasks.\nThis work investigates this \"linear reasoning bottleneck\" by introducing the\nLinear Separability Ceiling (LSC), the performance of a simple linear\nclassifier on a VLM's visual embeddings. We find this bottleneck is widespread\nand stems not from poor perception, but from failures in the language model's\nreasoning pathways. We demonstrate this is a solvable alignment issue. The\nrequired intervention, however, is task-dependent: activating existing pathways\nsuffices for semantic concepts, while complex relational reasoning requires\nadapting core model weights. Using postfix tuning as a methodological control,\nwe find strong evidence for powerful, dormant reasoning pathways within VLMs.\nHowever, for complex relational tasks requiring deeper adaptation, explicitly\nimproving representation quality causes the model to fail on new prompt formats\ndespite its embeddings remaining well separated. Ultimately, this work provides\na new lens for VLM analysis, showing that robust reasoning is a matter of\ntargeted alignment, not simply improved representation learning.",
            "upvotes": 3,
            "discussionId": "6870c2d4c8391850d60978e9",
            "githubRepo": "https://github.com/envomp/Beyond-the-Linear-Separability-Ceiling",
            "ai_summary": "The study identifies a linear reasoning bottleneck in Visual-Language Models and proposes the Linear Separability Ceiling as a metric to evaluate it, suggesting targeted alignment rather than improved representation learning as a solution.",
            "ai_keywords": [
                "Visual-Language Models",
                "VLMs",
                "linear separability",
                "Linear Separability Ceiling",
                "linear classifier",
                "visual embeddings",
                "abstract reasoning tasks",
                "reasoning pathways",
                "postfix tuning",
                "representation quality",
                "prompt formats"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-07-10T05:23:32.000Z",
        "title": "Beyond the Linear Separability Ceiling",
        "summary": "Most state-of-the-art Visual-Language Models (VLMs) are seemingly limited by\nthe linear separabilty of their visual embeddings on abstract reasoning tasks.\nThis work investigates this \"linear reasoning bottleneck\" by introducing the\nLinear Separability Ceiling (LSC), the performance of a simple linear\nclassifier on a VLM's visual embeddings. We find this bottleneck is widespread\nand stems not from poor perception, but from failures in the language model's\nreasoning pathways. We demonstrate this is a solvable alignment issue. The\nrequired intervention, however, is task-dependent: activating existing pathways\nsuffices for semantic concepts, while complex relational reasoning requires\nadapting core model weights. Using postfix tuning as a methodological control,\nwe find strong evidence for powerful, dormant reasoning pathways within VLMs.\nHowever, for complex relational tasks requiring deeper adaptation, explicitly\nimproving representation quality causes the model to fail on new prompt formats\ndespite its embeddings remaining well separated. Ultimately, this work provides\na new lens for VLM analysis, showing that robust reasoning is a matter of\ntargeted alignment, not simply improved representation learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07574.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63aadfe9a4bdd629b7ea7692",
            "avatarUrl": "/avatars/9cc4eb5d4090ce84a590ec195b70e545.svg",
            "fullname": "Enrico",
            "name": "envomp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07867",
            "authors": [
                {
                    "_id": "6871281ec8391850d60979dd",
                    "name": "Dimitrios Bralios",
                    "hidden": false
                },
                {
                    "_id": "6871281ec8391850d60979de",
                    "name": "Jonah Casebeer",
                    "hidden": false
                },
                {
                    "_id": "6871281ec8391850d60979df",
                    "name": "Paris Smaragdis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T15:47:43.000Z",
            "submittedOnDailyAt": "2025-07-11T13:37:26.158Z",
            "title": "Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders",
            "submittedOnDailyBy": {
                "_id": "660372877bbe436ee9cd3ba6",
                "avatarUrl": "/avatars/b2ac6707c537d5ae4447455e942f858b.svg",
                "isPro": false,
                "fullname": "Dimitrios Bralios",
                "user": "dbralios",
                "type": "user"
            },
            "summary": "Neural audio codecs and autoencoders have emerged as versatile models for\naudio compression, transmission, feature-extraction, and latent-space\ngeneration. However, a key limitation is that most are trained to maximize\nreconstruction fidelity, often neglecting the specific latent structure\nnecessary for optimal performance in diverse downstream applications. We\npropose a simple, post-hoc framework to address this by modifying the\nbottleneck of a pre-trained autoencoder. Our method introduces a\n\"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space\nlosses to instill user-defined structure. We demonstrate the framework's\neffectiveness in three experiments. First, we enforce an ordering on latent\nchannels without sacrificing reconstruction quality. Second, we align latents\nwith semantic embeddings, analyzing the impact on downstream diffusion\nmodeling. Third, we introduce equivariance, ensuring that a filtering operation\non the input waveform directly corresponds to a specific transformation in the\nlatent space. Ultimately, our Re-Bottleneck framework offers a flexible and\nefficient way to tailor representations of neural audio models, enabling them\nto seamlessly meet the varied demands of different applications with minimal\nadditional training.",
            "upvotes": 2,
            "discussionId": "6871281ec8391850d60979e0",
            "ai_summary": "A Re-Bottleneck framework modifies pre-trained autoencoders to enforce specific latent structures, improving performance in diverse downstream applications.",
            "ai_keywords": [
                "neural audio codecs",
                "autoencoders",
                "audio compression",
                "feature-extraction",
                "latent-space generation",
                "bottleneck",
                "Re-Bottleneck",
                "latent space losses",
                "latent channels",
                "semantic embeddings",
                "diffusion modeling",
                "equivariance"
            ]
        },
        "publishedAt": "2025-07-10T11:47:43.000Z",
        "title": "Re-Bottleneck: Latent Re-Structuring for Neural Audio Autoencoders",
        "summary": "Neural audio codecs and autoencoders have emerged as versatile models for\naudio compression, transmission, feature-extraction, and latent-space\ngeneration. However, a key limitation is that most are trained to maximize\nreconstruction fidelity, often neglecting the specific latent structure\nnecessary for optimal performance in diverse downstream applications. We\npropose a simple, post-hoc framework to address this by modifying the\nbottleneck of a pre-trained autoencoder. Our method introduces a\n\"Re-Bottleneck\", an inner bottleneck trained exclusively through latent space\nlosses to instill user-defined structure. We demonstrate the framework's\neffectiveness in three experiments. First, we enforce an ordering on latent\nchannels without sacrificing reconstruction quality. Second, we align latents\nwith semantic embeddings, analyzing the impact on downstream diffusion\nmodeling. Third, we introduce equivariance, ensuring that a filtering operation\non the input waveform directly corresponds to a specific transformation in the\nlatent space. Ultimately, our Re-Bottleneck framework offers a flexible and\nefficient way to tailor representations of neural audio models, enabling them\nto seamlessly meet the varied demands of different applications with minimal\nadditional training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07867.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "660372877bbe436ee9cd3ba6",
            "avatarUrl": "/avatars/b2ac6707c537d5ae4447455e942f858b.svg",
            "fullname": "Dimitrios Bralios",
            "name": "dbralios",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07129",
            "authors": [
                {
                    "_id": "6870d30fc8391850d60978fe",
                    "user": {
                        "_id": "661d507b4fe8c7a85ab96e3e",
                        "avatarUrl": "/avatars/6e7564331d0f1696e94377784a4b0e9b.svg",
                        "isPro": false,
                        "fullname": "Andrey",
                        "user": "Bochkov",
                        "type": "user"
                    },
                    "name": "A. Bochkov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-11T13:12:34.561Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-08T20:01:15.000Z",
            "submittedOnDailyAt": "2025-07-11T12:22:37.979Z",
            "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a\n  Frozen Substrate",
            "submittedOnDailyBy": {
                "_id": "661d507b4fe8c7a85ab96e3e",
                "avatarUrl": "/avatars/6e7564331d0f1696e94377784a4b0e9b.svg",
                "isPro": false,
                "fullname": "Andrey",
                "user": "Bochkov",
                "type": "user"
            },
            "summary": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive approach to model\ndevelopment, built upon the foundation of non-trainable, deterministic input\nembeddings. In prior [1], we established that high-level semantic reasoning can\nemerge in Transformers using frozen embeddings derived from the visual\nstructure of Unicode glyphs. Here, we demonstrate that this fixed\nrepresentational substrate acts as a universal \"docking port,\" enabling two\npowerful and efficient scaling paradigms: seamless modular composition and\nprogressive layer-wise growth.\n  First, we show that specialist models trained on disparate datasets (e.g.,\nRussian and Chinese text) can be merged into a single, more capable\nMixture-of-Experts (MoE) model, post-training, with zero architectural\nmodification. This is achieved by simply averaging their output logits. The\nresulting MoE model exhibits immediate performance improvements on reasoning\nbenchmarks like MMLU, surpassing its constituent experts without catastrophic\nforgetting. Second, we introduce a layer-wise constructive training\nmethodology, where a deep Transformer is \"grown\" by progressively stacking and\ntraining one layer at a time. This method demonstrates stable convergence and a\nclear correlation between model depth and the emergence of complex reasoning\nabilities, such as those required for SQuAD.\n  Our findings suggest a paradigm shift from monolithic optimization towards a\nmore biological or constructive model of AI development, where complexity is\nbuilt incrementally and modules can be composed freely. This opens new avenues\nfor resource-efficient scaling, continual learning, and a more democratized\necosystem for building powerful AI systems. We release all code and models to\nfacilitate further research.",
            "upvotes": 2,
            "discussionId": "6870d310c8391850d60978ff",
            "ai_summary": "A novel approach to scaling large language models through modular composition and layer-wise growth using fixed embeddings enhances performance and flexibility.",
            "ai_keywords": [
                "Transformers",
                "frozen embeddings",
                "Unicode glyphs",
                "Mixture-of-Experts (MoE)",
                "output logits",
                "MMLU",
                "reasoning benchmarks",
                "SQuAD",
                "constructive training methodology",
                "layer-wise growth",
                "monolithic optimization",
                "biological AI development",
                "continual learning"
            ]
        },
        "publishedAt": "2025-07-08T16:01:15.000Z",
        "title": "Growing Transformers: Modular Composition and Layer-wise Expansion on a\n  Frozen Substrate",
        "summary": "The prevailing paradigm for scaling large language models (LLMs) involves\nmonolithic, end-to-end training, a resource-intensive process that lacks\nflexibility. This paper explores an alternative, constructive approach to model\ndevelopment, built upon the foundation of non-trainable, deterministic input\nembeddings. In prior [1], we established that high-level semantic reasoning can\nemerge in Transformers using frozen embeddings derived from the visual\nstructure of Unicode glyphs. Here, we demonstrate that this fixed\nrepresentational substrate acts as a universal \"docking port,\" enabling two\npowerful and efficient scaling paradigms: seamless modular composition and\nprogressive layer-wise growth.\n  First, we show that specialist models trained on disparate datasets (e.g.,\nRussian and Chinese text) can be merged into a single, more capable\nMixture-of-Experts (MoE) model, post-training, with zero architectural\nmodification. This is achieved by simply averaging their output logits. The\nresulting MoE model exhibits immediate performance improvements on reasoning\nbenchmarks like MMLU, surpassing its constituent experts without catastrophic\nforgetting. Second, we introduce a layer-wise constructive training\nmethodology, where a deep Transformer is \"grown\" by progressively stacking and\ntraining one layer at a time. This method demonstrates stable convergence and a\nclear correlation between model depth and the emergence of complex reasoning\nabilities, such as those required for SQuAD.\n  Our findings suggest a paradigm shift from monolithic optimization towards a\nmore biological or constructive model of AI development, where complexity is\nbuilt incrementally and modules can be composed freely. This opens new avenues\nfor resource-efficient scaling, continual learning, and a more democratized\necosystem for building powerful AI systems. We release all code and models to\nfacilitate further research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07129.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661d507b4fe8c7a85ab96e3e",
            "avatarUrl": "/avatars/6e7564331d0f1696e94377784a4b0e9b.svg",
            "fullname": "Andrey",
            "name": "Bochkov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.05241",
            "authors": [
                {
                    "_id": "6870a7c6c8391850d60978ca",
                    "name": "Jingyi Chai",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978cb",
                    "name": "Shuo Tang",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978cc",
                    "name": "Rui Ye",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978cd",
                    "name": "Yuwen Du",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978ce",
                    "name": "Xinyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978cf",
                    "name": "Mengcheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978d0",
                    "name": "Yanfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978d1",
                    "name": "Weinan E",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978d2",
                    "name": "Yuzhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978d3",
                    "name": "Linfeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6870a7c6c8391850d60978d4",
                    "name": "Siheng Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T17:50:52.000Z",
            "submittedOnDailyAt": "2025-07-11T04:29:26.187Z",
            "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
            "submittedOnDailyBy": {
                "_id": "62d22496c58f969c152bcefd",
                "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
                "isPro": false,
                "fullname": "Tiezhen WANG",
                "user": "xianbao",
                "type": "user"
            },
            "summary": "The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training.",
            "upvotes": 2,
            "discussionId": "6870a7c6c8391850d60978d5"
        },
        "publishedAt": "2025-07-07T13:50:52.000Z",
        "title": "SciMaster: Towards General-Purpose Scientific AI Agents, Part I.\n  X-Master as Foundation: Can We Lead on Humanity's Last Exam?",
        "summary": "The rapid advancements of AI agents have ignited the long-held ambition of\nleveraging them to accelerate scientific discovery. Achieving this goal\nrequires a deep understanding of the frontiers of human knowledge. As such,\nHumanity's Last Exam (HLE) provides an exceptionally challenging touchstone for\nevaluating scientific AI agents. In this work, we aim to construct the\nfoundational architecture for general-purpose agents and validate the\ncapabilities through leading performance on HLE. To achieve this, we introduce\nX-Master, a tool-augmented reasoning agent designed to emulate human\nresearchers by interacting flexibly with external tools during its reasoning\nprocess. This agent, guided by the conceptualization of code as an interaction\nlanguage, can flexibly leverage built-in Python libraries and our customized\ntools to augment the reasoning. We further scale its capabilities through\nX-Masters, a scattered-and-stacked agentic workflow that systematically\nenhances breadth and depth of reasoning. Our open-source solution, X-Masters,\nsets a new state-of-the-art record on HLE with a score of 32.1%, surpassing\nOpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to\nexceed the 30% threshold. This work allows us to gain a deeper understanding of\ncomplex task-solving and accumulates valuable experience that can inform future\nadvancements, guiding subsequent model training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05241.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62d22496c58f969c152bcefd",
            "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
            "fullname": "Tiezhen WANG",
            "name": "xianbao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 120
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.04886",
            "authors": [
                {
                    "_id": "686ccebb364e2ad167eb54ed",
                    "user": {
                        "_id": "661d507b4fe8c7a85ab96e3e",
                        "avatarUrl": "/avatars/6e7564331d0f1696e94377784a4b0e9b.svg",
                        "isPro": false,
                        "fullname": "Andrey",
                        "user": "Bochkov",
                        "type": "user"
                    },
                    "name": "A. Bochkov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-08T12:40:28.551Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T11:17:32.000Z",
            "submittedOnDailyAt": "2025-07-11T12:28:53.664Z",
            "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations",
            "submittedOnDailyBy": {
                "_id": "661d507b4fe8c7a85ab96e3e",
                "avatarUrl": "/avatars/6e7564331d0f1696e94377784a4b0e9b.svg",
                "isPro": false,
                "fullname": "Andrey",
                "user": "Bochkov",
                "type": "user"
            },
            "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.",
            "upvotes": 1,
            "discussionId": "686ccebb364e2ad167eb54ee",
            "githubRepo": "https://github.com/AVBochkov/Embeddings",
            "ai_summary": "Transformer models equipped with fixed, visually derived embeddings outperform those with trainable embeddings on a reasoning benchmark, challenging the traditional role of embeddings in LLMs.",
            "ai_keywords": [
                "Transformer models",
                "input embeddings",
                "meaning vectors",
                "visual embeddings",
                "Unicode glyphs",
                "tokenizer",
                "MMLU reasoning benchmark",
                "representational interference",
                "compositional architecture",
                "data scale",
                "structural primitives"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-07-07T07:17:32.000Z",
        "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations",
        "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.04886.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "661d507b4fe8c7a85ab96e3e",
            "avatarUrl": "/avatars/6e7564331d0f1696e94377784a4b0e9b.svg",
            "fullname": "Andrey",
            "name": "Bochkov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]