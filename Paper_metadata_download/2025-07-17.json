[
    {
        "paper": {
            "id": "2507.09477",
            "authors": [
                {
                    "_id": "68787030001546c83aa4f9ae",
                    "name": "Yangning Li",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9af",
                    "user": {
                        "_id": "6667e801fd95ddf66cac84ff",
                        "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
                        "isPro": false,
                        "fullname": "Weizhi Zhang",
                        "user": "WZDavid",
                        "type": "user"
                    },
                    "name": "Weizhi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:22:48.379Z",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b0",
                    "name": "Yuyao Yang",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b1",
                    "name": "Wei-Chieh Huang",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b2",
                    "name": "Yaozu Wu",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b3",
                    "name": "Junyu Luo",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b4",
                    "name": "Yuanchen Bei",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b5",
                    "user": {
                        "_id": "633f112013e836a0fc4fa567",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1665077519962-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Henry Peng Zou",
                        "user": "TreeForest",
                        "type": "user"
                    },
                    "name": "Henry Peng Zou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:22:46.466Z",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b6",
                    "name": "Xiao Luo",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b7",
                    "name": "Yusheng Zhao",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b8",
                    "name": "Chunkit Chan",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9b9",
                    "name": "Yankai Chen",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9ba",
                    "name": "Zhongfen Deng",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9bb",
                    "name": "Yinghui Li",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9bc",
                    "name": "Hai-Tao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9bd",
                    "name": "Dongyuan Li",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9be",
                    "name": "Renhe Jiang",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9bf",
                    "name": "Ming Zhang",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9c0",
                    "name": "Yangqiu Song",
                    "hidden": false
                },
                {
                    "_id": "68787030001546c83aa4f9c1",
                    "name": "Philip S. Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-13T03:29:41.000Z",
            "submittedOnDailyAt": "2025-07-17T02:27:52.541Z",
            "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
            "submittedOnDailyBy": {
                "_id": "6667e801fd95ddf66cac84ff",
                "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
                "isPro": false,
                "fullname": "Weizhi Zhang",
                "user": "WZDavid",
                "type": "user"
            },
            "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
            "upvotes": 50,
            "discussionId": "68787031001546c83aa4f9c2",
            "githubRepo": "https://github.com/DavidZWZ/Awesome-RAG-Reasoning",
            "ai_summary": "This survey integrates reasoning and retrieval in Large Language Models to improve factuality and multi-step inference, highlighting Synergized RAG-Reasoning frameworks and outlining future research directions.",
            "ai_keywords": [
                "Retrieval-Augmented Generation",
                "Large Language Models",
                "Reasoning-Enhanced RAG",
                "RAG-Enhanced Reasoning",
                "Synergized RAG-Reasoning",
                "knowledge-intensive benchmarks",
                "multimodally-adaptive",
                "trustworthy",
                "human-centric"
            ],
            "githubStars": 75
        },
        "publishedAt": "2025-07-12T23:29:41.000Z",
        "title": "Towards Agentic RAG with Deep Reasoning: A Survey of RAG-Reasoning\n  Systems in LLMs",
        "summary": "Retrieval-Augmented Generation (RAG) lifts the factuality of Large Language\nModels (LLMs) by injecting external knowledge, yet it falls short on problems\nthat demand multi-step inference; conversely, purely reasoning-oriented\napproaches often hallucinate or mis-ground facts. This survey synthesizes both\nstrands under a unified reasoning-retrieval perspective. We first map how\nadvanced reasoning optimizes each stage of RAG (Reasoning-Enhanced RAG). Then,\nwe show how retrieved knowledge of different type supply missing premises and\nexpand context for complex inference (RAG-Enhanced Reasoning). Finally, we\nspotlight emerging Synergized RAG-Reasoning frameworks, where (agentic) LLMs\niteratively interleave search and reasoning to achieve state-of-the-art\nperformance across knowledge-intensive benchmarks. We categorize methods,\ndatasets, and open challenges, and outline research avenues toward deeper\nRAG-Reasoning systems that are more effective, multimodally-adaptive,\ntrustworthy, and human-centric. The collection is available at\nhttps://github.com/DavidZWZ/Awesome-RAG-Reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09477.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6667e801fd95ddf66cac84ff",
            "avatarUrl": "/avatars/b75f6253160c81f558e6b40ed2ca1755.svg",
            "fullname": "Weizhi Zhang",
            "name": "WZDavid",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.12465",
            "authors": [
                {
                    "_id": "6878635e001546c83aa4f979",
                    "user": {
                        "_id": "65af6f6b52e1b2aae437af2e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
                        "isPro": false,
                        "fullname": "Ziang Cao",
                        "user": "Caoza",
                        "type": "user"
                    },
                    "name": "Ziang Cao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:22:44.605Z",
                    "hidden": false
                },
                {
                    "_id": "6878635e001546c83aa4f97a",
                    "user": {
                        "_id": "62fc8cf7ee999004b5a8b982",
                        "avatarUrl": "/avatars/6c5dda9e58747054a989f077a078f3dc.svg",
                        "isPro": false,
                        "fullname": "Zhaoxi Chen",
                        "user": "FrozenBurning",
                        "type": "user"
                    },
                    "name": "Zhaoxi Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:22:11.615Z",
                    "hidden": false
                },
                {
                    "_id": "6878635e001546c83aa4f97b",
                    "name": "Linag Pan",
                    "hidden": false
                },
                {
                    "_id": "6878635e001546c83aa4f97c",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:21:56.595Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
            ],
            "publishedAt": "2025-07-16T17:59:35.000Z",
            "submittedOnDailyAt": "2025-07-17T01:26:11.001Z",
            "title": "PhysX: Physical-Grounded 3D Asset Generation",
            "submittedOnDailyBy": {
                "_id": "65af6f6b52e1b2aae437af2e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
                "isPro": false,
                "fullname": "Ziang Cao",
                "user": "Caoza",
                "type": "user"
            },
            "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose PhysX, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose PhysXGen, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.",
            "upvotes": 20,
            "discussionId": "6878635e001546c83aa4f97d",
            "projectPage": "https://physx-3d.github.io/",
            "githubRepo": "https://github.com/ziangcao0312/PhysX",
            "ai_summary": "PhysX addresses the lack of physical properties in 3D generative models by introducing PhysXNet, a physics-annotated dataset, and PhysXGen, a framework that integrates physical knowledge into 3D asset generation.",
            "ai_keywords": [
                "physics-grounded 3D asset generation",
                "PhysXNet",
                "physics-annotated 3D datasets",
                "vision-language models",
                "human-in-the-loop annotation pipeline",
                "PhysXGen",
                "feed-forward framework",
                "dual-branch architecture",
                "latent correlations",
                "physical predictions",
                "geometry quality"
            ],
            "githubStars": 72
        },
        "publishedAt": "2025-07-16T13:59:35.000Z",
        "title": "PhysX: Physical-Grounded 3D Asset Generation",
        "summary": "3D modeling is moving from virtual to physical. Existing 3D generation\nprimarily emphasizes geometries and textures while neglecting physical-grounded\nmodeling. Consequently, despite the rapid development of 3D generative models,\nthe synthesized 3D assets often overlook rich and important physical\nproperties, hampering their real-world application in physical domains like\nsimulation and embodied AI. As an initial attempt to address this challenge, we\npropose PhysX, an end-to-end paradigm for physical-grounded 3D asset\ngeneration. 1) To bridge the critical gap in physics-annotated 3D datasets, we\npresent PhysXNet - the first physics-grounded 3D dataset systematically\nannotated across five foundational dimensions: absolute scale, material,\naffordance, kinematics, and function description. In particular, we devise a\nscalable human-in-the-loop annotation pipeline based on vision-language models,\nwhich enables efficient creation of physics-first assets from raw 3D assets.2)\nFurthermore, we propose PhysXGen, a feed-forward framework for\nphysics-grounded image-to-3D asset generation, injecting physical knowledge\ninto the pre-trained 3D structural space. Specifically, PhysXGen employs a\ndual-branch architecture to explicitly model the latent correlations between 3D\nstructures and physical properties, thereby producing 3D assets with plausible\nphysical predictions while preserving the native geometry quality. Extensive\nexperiments validate the superior performance and promising generalization\ncapability of our framework. All the code, data, and models will be released to\nfacilitate future research in generative physical AI.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65af6f6b52e1b2aae437af2e/nrL7wGaZ1Z5FZWuu0GTbg.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12465.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65af6f6b52e1b2aae437af2e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65af6f6b52e1b2aae437af2e/sFC98zLL_ZPS9fvZFi01W.jpeg",
            "fullname": "Ziang Cao",
            "name": "Caoza",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.12415",
            "authors": [
                {
                    "_id": "68788b9b001546c83aa4f9ed",
                    "user": {
                        "_id": "65008c1b7aa4d88f207bedc5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65008c1b7aa4d88f207bedc5/e4YN_z1UyV0M47nezcFOx.jpeg",
                        "isPro": false,
                        "fullname": "Xinyi He",
                        "user": "betty12",
                        "type": "user"
                    },
                    "name": "Xinyi He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T18:43:15.979Z",
                    "hidden": false
                },
                {
                    "_id": "68788b9b001546c83aa4f9ee",
                    "user": {
                        "_id": "612ee6a7b960e78c6d2319d4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
                        "isPro": false,
                        "fullname": "Qian Liu",
                        "user": "SivilTaram",
                        "type": "user"
                    },
                    "name": "Qian Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:17:46.375Z",
                    "hidden": false
                },
                {
                    "_id": "68788b9b001546c83aa4f9ef",
                    "user": {
                        "_id": "61711f02e0b1ddb56eb9b526",
                        "avatarUrl": "/avatars/3e2fdf774f5bc1f73b450486d6da42d4.svg",
                        "isPro": true,
                        "fullname": "Mingzhe Du",
                        "user": "Elfsong",
                        "type": "user"
                    },
                    "name": "Mingzhe Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T09:08:30.259Z",
                    "hidden": false
                },
                {
                    "_id": "68788b9b001546c83aa4f9f0",
                    "name": "Lin Yan",
                    "hidden": false
                },
                {
                    "_id": "68788b9b001546c83aa4f9f1",
                    "name": "Zhijie Fan",
                    "hidden": false
                },
                {
                    "_id": "68788b9b001546c83aa4f9f2",
                    "name": "Yiming Huang",
                    "hidden": false
                },
                {
                    "_id": "68788b9b001546c83aa4f9f3",
                    "name": "Zejian Yuan",
                    "hidden": false
                },
                {
                    "_id": "68788b9b001546c83aa4f9f4",
                    "name": "Zejun Ma",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
            ],
            "publishedAt": "2025-07-16T17:05:17.000Z",
            "submittedOnDailyAt": "2025-07-17T04:10:30.408Z",
            "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
            "submittedOnDailyBy": {
                "_id": "612ee6a7b960e78c6d2319d4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
                "isPro": false,
                "fullname": "Qian Liu",
                "user": "SivilTaram",
                "type": "user"
            },
            "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.",
            "upvotes": 19,
            "discussionId": "68788b9b001546c83aa4f9f5",
            "projectPage": "https://swe-perf.github.io/",
            "githubRepo": "https://github.com/SWE-Perf/SWE-Perf",
            "ai_summary": "SWE-Perf is a benchmark for evaluating Large Language Models in code performance optimization using real-world repository data.",
            "ai_keywords": [
                "Large Language Models",
                "code performance optimization",
                "benchmark",
                "performance-improving pull requests",
                "codebase",
                "target functions",
                "performance-related tests",
                "expert-authored patches",
                "executable environments",
                "Agentless",
                "OpenHands"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-07-16T13:05:17.000Z",
        "title": "SWE-Perf: Can Language Models Optimize Code Performance on Real-World\n  Repositories?",
        "summary": "Code performance optimization is paramount in real-world software engineering\nand critical for production-level systems. While Large Language Models (LLMs)\nhave demonstrated impressive capabilities in code generation and bug fixing,\ntheir proficiency in enhancing code performance at the repository level remains\nlargely unexplored. To address this gap, we introduce SWE-Perf, the first\nbenchmark specifically designed to systematically evaluate LLMs on code\nperformance optimization tasks within authentic repository contexts. SWE-Perf\ncomprises 140 carefully curated instances, each derived from\nperformance-improving pull requests from popular GitHub repositories. Each\nbenchmark instance includes the relevant codebase, target functions,\nperformance-related tests, expert-authored patches, and executable\nenvironments. Through a comprehensive evaluation of representative methods that\nspan file-level and repo-level approaches (e.g., Agentless and OpenHands), we\nreveal a substantial capability gap between existing LLMs and expert-level\noptimization performance, highlighting critical research opportunities in this\nemerging field.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/612ee6a7b960e78c6d2319d4/_XkK-c8Xm-G1Ui5AwtGdu.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12415.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "612ee6a7b960e78c6d2319d4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/612ee6a7b960e78c6d2319d4/2Hu9BaAyXbyh1vt0v1Qui.jpeg",
            "fullname": "Qian Liu",
            "name": "SivilTaram",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 85
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.12463",
            "authors": [
                {
                    "_id": "68788789001546c83aa4f9e4",
                    "user": {
                        "_id": "686fe2365998cd8af12fc8ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GQbxskSw-98mdjbM3szaC.png",
                        "isPro": false,
                        "fullname": "Renjie Li",
                        "user": "renjie-li",
                        "type": "user"
                    },
                    "name": "Renjie Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T14:59:16.875Z",
                    "hidden": false
                },
                {
                    "_id": "68788789001546c83aa4f9e5",
                    "user": {
                        "_id": "6825fc0e58cf56d164cb339d",
                        "avatarUrl": "/avatars/4ade4a5bbb0a805a92a83bfb233f805f.svg",
                        "isPro": false,
                        "fullname": "Ruijie Ye",
                        "user": "jerryye0110",
                        "type": "user"
                    },
                    "name": "Ruijie Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:17:53.937Z",
                    "hidden": false
                },
                {
                    "_id": "68788789001546c83aa4f9e6",
                    "user": {
                        "_id": "6736d289c8a9bf8f86936201",
                        "avatarUrl": "/avatars/ca56298f9db458ba65c469b1baabda2c.svg",
                        "isPro": false,
                        "fullname": "MingyangWu",
                        "user": "mingyang-wu",
                        "type": "user"
                    },
                    "name": "Mingyang Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T18:43:17.791Z",
                    "hidden": false
                },
                {
                    "_id": "68788789001546c83aa4f9e7",
                    "name": "Hao Frank Yang",
                    "hidden": false
                },
                {
                    "_id": "68788789001546c83aa4f9e8",
                    "user": {
                        "_id": "63b354bb7091e602f1a0e2e8",
                        "avatarUrl": "/avatars/a388d93c0af2f57eadb6fa60d6789041.svg",
                        "isPro": false,
                        "fullname": "wayne",
                        "user": "waynefan",
                        "type": "user"
                    },
                    "name": "Zhiwen Fan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:23:18.102Z",
                    "hidden": false
                },
                {
                    "_id": "68788789001546c83aa4f9e9",
                    "name": "Hezhen Hu",
                    "hidden": false
                },
                {
                    "_id": "68788789001546c83aa4f9ea",
                    "user": {
                        "_id": "62548d5fef3debb2ddf91217",
                        "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
                        "isPro": false,
                        "fullname": "Zhengzhong Tu",
                        "user": "vztu",
                        "type": "user"
                    },
                    "name": "Zhengzhong Tu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:23:16.047Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
            ],
            "publishedAt": "2025-07-16T17:59:30.000Z",
            "submittedOnDailyAt": "2025-07-17T03:48:41.381Z",
            "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "62548d5fef3debb2ddf91217",
                "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
                "isPro": false,
                "fullname": "Zhengzhong Tu",
                "user": "vztu",
                "type": "user"
            },
            "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behaviorx2014such as motion, trajectories, and\nintentionx2014a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose MMHU, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasksx2014ranging from motion prediction to motion\ngeneration and human behavior question answeringx2014thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.",
            "upvotes": 17,
            "discussionId": "6878878a001546c83aa4f9eb",
            "projectPage": "https://mmhu-benchmark.github.io/",
            "ai_summary": "A large-scale benchmark, MMHU, is proposed for human behavior analysis in autonomous driving, featuring rich annotations and diverse data sources, and benchmarking multiple tasks including motion prediction and behavior question answering.",
            "ai_keywords": [
                "human behavior analysis",
                "motion prediction",
                "motion generation",
                "human behavior question answering",
                "human-in-the-loop annotation",
                "Waymo",
                "YouTube",
                "self-collected data"
            ]
        },
        "publishedAt": "2025-07-16T13:59:30.000Z",
        "title": "MMHU: A Massive-Scale Multimodal Benchmark for Human Behavior\n  Understanding",
        "summary": "Humans are integral components of the transportation ecosystem, and\nunderstanding their behaviors is crucial to facilitating the development of\nsafe driving systems. Although recent progress has explored various aspects of\nhuman behaviorx2014such as motion, trajectories, and\nintentionx2014a comprehensive benchmark for evaluating human\nbehavior understanding in autonomous driving remains unavailable. In this work,\nwe propose MMHU, a large-scale benchmark for human behavior analysis\nfeaturing rich annotations, such as human motion and trajectories, text\ndescription for human motions, human intention, and critical behavior labels\nrelevant to driving safety. Our dataset encompasses 57k human motion clips and\n1.73M frames gathered from diverse sources, including established driving\ndatasets such as Waymo, in-the-wild videos from YouTube, and self-collected\ndata. A human-in-the-loop annotation pipeline is developed to generate rich\nbehavior captions. We provide a thorough dataset analysis and benchmark\nmultiple tasksx2014ranging from motion prediction to motion\ngeneration and human behavior question answeringx2014thereby\noffering a broad evaluation suite. Project page :\nhttps://MMHU-Benchmark.github.io.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62548d5fef3debb2ddf91217/ARblfCBCEmNaS2zzbTJLm.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12463.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62548d5fef3debb2ddf91217",
            "avatarUrl": "/avatars/14975b45568f9c399c92c3986b6ce83e.svg",
            "fullname": "Zhengzhong Tu",
            "name": "vztu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.11949",
            "authors": [
                {
                    "_id": "687872c3001546c83aa4f9cf",
                    "user": {
                        "_id": "683d94e5ba11bab2cc848aab",
                        "avatarUrl": "/avatars/4a1915ad48c78b3a71733b3282f2f93c.svg",
                        "isPro": false,
                        "fullname": "Shuyang Xu",
                        "user": "JimSYXu",
                        "type": "user"
                    },
                    "name": "Shuyang Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:22:22.253Z",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d0",
                    "user": {
                        "_id": "645223fb01d7bd9555ea399a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
                        "isPro": false,
                        "fullname": "Zhiyang Dou",
                        "user": "frankzydou",
                        "type": "user"
                    },
                    "name": "Zhiyang Dou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:22:59.825Z",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d1",
                    "name": "Mingyi Shi",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d2",
                    "name": "Liang Pan",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d3",
                    "name": "Leo Ho",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d4",
                    "name": "Jingbo Wang",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d5",
                    "name": "Yuan Liu",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d6",
                    "name": "Cheng Lin",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d7",
                    "name": "Yuexin Ma",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d8",
                    "name": "Wenping Wang",
                    "hidden": false
                },
                {
                    "_id": "687872c3001546c83aa4f9d9",
                    "name": "Taku Komura",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
            ],
            "publishedAt": "2025-07-16T06:33:11.000Z",
            "submittedOnDailyAt": "2025-07-17T02:48:15.112Z",
            "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
            "submittedOnDailyBy": {
                "_id": "645223fb01d7bd9555ea399a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
                "isPro": false,
                "fullname": "Zhiyang Dou",
                "user": "frankzydou",
                "type": "user"
            },
            "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.",
            "upvotes": 13,
            "discussionId": "687872c9001546c83aa4f9da",
            "ai_summary": "A diffusion-based generative framework, MOSPA, is introduced to model human motion in response to spatial audio, achieving state-of-the-art performance using the newly created SAM dataset.",
            "ai_keywords": [
                "diffusion-based generative framework",
                "spatial audio",
                "human motion",
                "SAM dataset",
                "MOSPA",
                "spatial features",
                "perceptual modeling",
                "motion synthesis"
            ]
        },
        "publishedAt": "2025-07-16T02:33:11.000Z",
        "title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
        "summary": "Enabling virtual humans to dynamically and realistically respond to diverse\nauditory stimuli remains a key challenge in character animation, demanding the\nintegration of perceptual modeling and motion synthesis. Despite its\nsignificance, this task remains largely unexplored. Most previous works have\nprimarily focused on mapping modalities like speech, audio, and music to\ngenerate human motion. As of yet, these models typically overlook the impact of\nspatial features encoded in spatial audio signals on human motion. To bridge\nthis gap and enable high-quality modeling of human movements in response to\nspatial audio, we introduce the first comprehensive Spatial Audio-Driven Human\nMotion (SAM) dataset, which contains diverse and high-quality spatial audio and\nmotion data. For benchmarking, we develop a simple yet effective\ndiffusion-based generative framework for human MOtion generation driven by\nSPatial Audio, termed MOSPA, which faithfully captures the relationship between\nbody motion and spatial audio through an effective fusion mechanism. Once\ntrained, MOSPA could generate diverse realistic human motions conditioned on\nvarying spatial audio inputs. We perform a thorough investigation of the\nproposed dataset and conduct extensive experiments for benchmarking, where our\nmethod achieves state-of-the-art performance on this task. Our model and\ndataset will be open-sourced upon acceptance. Please refer to our supplementary\nvideo for more details.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/645223fb01d7bd9555ea399a/KoPBJkGwyft7hfVmM8ikZ.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11949.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "645223fb01d7bd9555ea399a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645223fb01d7bd9555ea399a/fVR7XmGg6pMSRKx9sEdvT.png",
            "fullname": "Zhiyang Dou",
            "name": "frankzydou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.11527",
            "authors": [
                {
                    "_id": "68785ee1001546c83aa4f967",
                    "user": {
                        "_id": "65c950ebd908bd52a4477116",
                        "avatarUrl": "/avatars/bc6ba0dd2903c7bea37f7b9c40857718.svg",
                        "isPro": false,
                        "fullname": "Yinsheng Li",
                        "user": "Eason666",
                        "type": "user"
                    },
                    "name": "Yinsheng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:22:42.470Z",
                    "hidden": false
                },
                {
                    "_id": "68785ee1001546c83aa4f968",
                    "user": {
                        "_id": "643ba2f725681c3afaa8f05e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
                        "isPro": false,
                        "fullname": "Zhen Dong",
                        "user": "zhendongucb",
                        "type": "user"
                    },
                    "name": "Zhen Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:22:37.102Z",
                    "hidden": false
                },
                {
                    "_id": "68785ee1001546c83aa4f969",
                    "name": "Yi Shao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
            ],
            "publishedAt": "2025-07-15T17:56:04.000Z",
            "submittedOnDailyAt": "2025-07-17T01:30:58.515Z",
            "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
            "submittedOnDailyBy": {
                "_id": "643ba2f725681c3afaa8f05e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
                "isPro": false,
                "fullname": "Zhen Dong",
                "user": "zhendongucb",
                "type": "user"
            },
            "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.",
            "upvotes": 12,
            "discussionId": "68785ee1001546c83aa4f96a",
            "githubRepo": "https://github.com/Eason-Li-AIS/DrafterBench",
            "ai_summary": "DrafterBench is an open-source benchmark for evaluating LLM agents in technical drawing revision, assessing their capabilities in structured data comprehension, function execution, instruction following, and critical reasoning.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "technical drawing revision",
                "structured data comprehension",
                "function execution",
                "instruction following",
                "critical reasoning",
                "benchmark",
                "open-source",
                "implicit policy awareness"
            ],
            "githubStars": 29
        },
        "publishedAt": "2025-07-15T13:56:04.000Z",
        "title": "DrafterBench: Benchmarking Large Language Models for Tasks Automation in\n  Civil Engineering",
        "summary": "Large Language Model (LLM) agents have shown great potential for solving\nreal-world problems and promise to be a solution for tasks automation in\nindustry. However, more benchmarks are needed to systematically evaluate\nautomation agents from an industrial perspective, for example, in Civil\nEngineering. Therefore, we propose DrafterBench for the comprehensive\nevaluation of LLM agents in the context of technical drawing revision, a\nrepresentation task in civil engineering. DrafterBench contains twelve types of\ntasks summarized from real-world drawing files, with 46 customized\nfunctions/tools and 1920 tasks in total. DrafterBench is an open-source\nbenchmark to rigorously test AI agents' proficiency in interpreting intricate\nand long-context instructions, leveraging prior knowledge, and adapting to\ndynamic instruction quality via implicit policy awareness. The toolkit\ncomprehensively assesses distinct capabilities in structured data\ncomprehension, function execution, instruction following, and critical\nreasoning. DrafterBench offers detailed analysis of task accuracy and error\nstatistics, aiming to provide deeper insight into agent capabilities and\nidentify improvement targets for integrating LLMs in engineering applications.\nOur benchmark is available at https://github.com/Eason-Li-AIS/DrafterBench,\nwith the test set hosted at\nhttps://huggingface.co/datasets/Eason666/DrafterBench.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643ba2f725681c3afaa8f05e/2T1jxRrhqMGVLESbEDwJT.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11527.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "643ba2f725681c3afaa8f05e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643ba2f725681c3afaa8f05e/2RnOdmbBM8WHYhiTGG-Cd.jpeg",
            "fullname": "Zhen Dong",
            "name": "zhendongucb",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.11412",
            "authors": [
                {
                    "_id": "6877521f257d4f043537084f",
                    "name": "Orion Weller",
                    "hidden": false
                },
                {
                    "_id": "6877521f257d4f0435370850",
                    "name": "Kathryn Ricci",
                    "hidden": false
                },
                {
                    "_id": "6877521f257d4f0435370851",
                    "user": {
                        "_id": "63e410e88083f19a218be964",
                        "avatarUrl": "/avatars/ce271d1686f6e5ee1f5b2d429cb60de6.svg",
                        "isPro": false,
                        "fullname": "Marc Marone",
                        "user": "mmarone",
                        "type": "user"
                    },
                    "name": "Marc Marone",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T18:43:19.468Z",
                    "hidden": false
                },
                {
                    "_id": "6877521f257d4f0435370852",
                    "user": {
                        "_id": "609bbe2f4932693ca2009d6a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1620819560688-609bbe2f4932693ca2009d6a.jpeg",
                        "isPro": false,
                        "fullname": "Antoine Chaffin",
                        "user": "NohTow",
                        "type": "user"
                    },
                    "name": "Antoine Chaffin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-16T15:16:06.045Z",
                    "hidden": false
                },
                {
                    "_id": "6877521f257d4f0435370853",
                    "name": "Dawn Lawrie",
                    "hidden": false
                },
                {
                    "_id": "6877521f257d4f0435370854",
                    "name": "Benjamin Van Durme",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-15T15:31:51.000Z",
            "submittedOnDailyAt": "2025-07-17T11:43:56.880Z",
            "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders",
            "submittedOnDailyBy": {
                "_id": "609bbe2f4932693ca2009d6a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1620819560688-609bbe2f4932693ca2009d6a.jpeg",
                "isPro": false,
                "fullname": "Antoine Chaffin",
                "user": "NohTow",
                "type": "user"
            },
            "summary": "The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.",
            "upvotes": 11,
            "discussionId": "68775220257d4f0435370855",
            "githubRepo": "https://github.com/jhu-clsp/ettin-encoder-vs-decoder",
            "githubStars": 20
        },
        "publishedAt": "2025-07-15T11:31:51.000Z",
        "title": "Seq vs Seq: An Open Suite of Paired Encoders and Decoders",
        "summary": "The large language model (LLM) community focuses almost exclusively on\ndecoder-only language models, since they are easier to use for text generation.\nHowever, a large subset of the community still uses encoder-only models for\ntasks such as classification or retrieval. Previous work has attempted to\ncompare these architectures, but is forced to make comparisons with models that\nhave different numbers of parameters, training techniques, and datasets. We\nintroduce the SOTA open-data Ettin suite of models: paired encoder-only and\ndecoder-only models ranging from 17 million parameters to 1 billion, trained on\nup to 2 trillion tokens. Using the same recipe for both encoder-only and\ndecoder-only models produces SOTA recipes in both categories for their\nrespective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as\ndecoders. Like previous work, we find that encoder-only models excel at\nclassification and retrieval tasks while decoders excel at generative tasks.\nHowever, we show that adapting a decoder model to encoder tasks (and vice\nversa) through continued training is subpar compared to using only the reverse\nobjective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa\nfor generative tasks). We open-source all artifacts of this study including\ntraining data, training order segmented by checkpoint, and 200+ checkpoints to\nallow future work to analyze or extend all aspects of training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11412.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "609bbe2f4932693ca2009d6a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1620819560688-609bbe2f4932693ca2009d6a.jpeg",
            "fullname": "Antoine Chaffin",
            "name": "NohTow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 37
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.02857",
            "authors": [
                {
                    "_id": "68788cd9001546c83aa4f9f7",
                    "user": {
                        "_id": "66aef8691dd7d0a8c6584724",
                        "avatarUrl": "/avatars/df9c2a56f3d0746cf64a330137a105b4.svg",
                        "isPro": false,
                        "fullname": "Ziye Li",
                        "user": "TribeRinb",
                        "type": "user"
                    },
                    "name": "Ziye Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:48:49.437Z",
                    "hidden": false
                },
                {
                    "_id": "68788cd9001546c83aa4f9f8",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "68788cd9001546c83aa4f9f9",
                    "user": {
                        "_id": "6335c7fa2db86a181cca723f",
                        "avatarUrl": "/avatars/13f04af01914f8473f9939f49b4eecd4.svg",
                        "isPro": false,
                        "fullname": "XC",
                        "user": "XinchengShuai",
                        "type": "user"
                    },
                    "name": "Xincheng Shuai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:48:34.400Z",
                    "hidden": false
                },
                {
                    "_id": "68788cd9001546c83aa4f9fa",
                    "user": {
                        "_id": "67ff29ecbf6889a333c69c7a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
                        "isPro": false,
                        "fullname": "Henghui Ding",
                        "user": "HenghuiDing",
                        "type": "user"
                    },
                    "name": "Henghui Ding",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:48:26.454Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-03T17:59:02.000Z",
            "submittedOnDailyAt": "2025-07-17T04:11:55.501Z",
            "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
            "submittedOnDailyBy": {
                "_id": "67ff29ecbf6889a333c69c7a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
                "isPro": false,
                "fullname": "Henghui Ding",
                "user": "HenghuiDing",
                "type": "user"
            },
            "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.",
            "upvotes": 7,
            "discussionId": "68788cda001546c83aa4f9fb",
            "projectPage": "https://henghuiding.com/AnyI2V/",
            "githubRepo": "https://github.com/FudanCVL/AnyI2V",
            "ai_summary": "AnyI2V is a training-free framework that animates conditional images with user-defined motion trajectories, supporting various data types and enabling flexible video generation.",
            "ai_keywords": [
                "diffusion models",
                "text-to-video",
                "image-to-video",
                "ControlNet",
                "motion trajectories",
                "conditional images",
                "meshes",
                "point clouds",
                "mixed conditional inputs",
                "style transfer",
                "LoRA",
                "text prompts"
            ],
            "githubStars": 86
        },
        "publishedAt": "2025-07-03T13:59:02.000Z",
        "title": "AnyI2V: Animating Any Conditional Image with Motion Control",
        "summary": "Recent advancements in video generation, particularly in diffusion models,\nhave driven notable progress in text-to-video (T2V) and image-to-video (I2V)\nsynthesis. However, challenges remain in effectively integrating dynamic motion\nsignals and flexible spatial constraints. Existing T2V methods typically rely\non text prompts, which inherently lack precise control over the spatial layout\nof generated content. In contrast, I2V methods are limited by their dependence\non real images, which restricts the editability of the synthesized content.\nAlthough some methods incorporate ControlNet to introduce image-based\nconditioning, they often lack explicit motion control and require\ncomputationally expensive training. To address these limitations, we propose\nAnyI2V, a training-free framework that animates any conditional images with\nuser-defined motion trajectories. AnyI2V supports a broader range of modalities\nas the conditional image, including data types such as meshes and point clouds\nthat are not supported by ControlNet, enabling more flexible and versatile\nvideo generation. Additionally, it supports mixed conditional inputs and\nenables style transfer and editing via LoRA and text prompts. Extensive\nexperiments demonstrate that the proposed AnyI2V achieves superior performance\nand provides a new perspective in spatial- and motion-controlled video\ngeneration. Code is available at https://henghuiding.com/AnyI2V/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.02857.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "fullname": "Henghui Ding",
            "name": "HenghuiDing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.09025",
            "authors": [
                {
                    "_id": "68786e45001546c83aa4f9a0",
                    "name": "Chien Van Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9a1",
                    "name": "Ruiyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9a2",
                    "user": {
                        "_id": "652767bfbdcf00b9b9ac9a74",
                        "avatarUrl": "/avatars/2cc8e9167562f364f0c25410f13a9d62.svg",
                        "isPro": false,
                        "fullname": "Hanieh Deilamsalehy",
                        "user": "haniehds",
                        "type": "user"
                    },
                    "name": "Hanieh Deilamsalehy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:36:52.637Z",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9a3",
                    "name": "Puneet Mathur",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9a4",
                    "name": "Viet Dac Lai",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9a5",
                    "name": "Haoliang Wang",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9a6",
                    "user": {
                        "_id": "66d32a678819c81cce2052f4",
                        "avatarUrl": "/avatars/3a5b40ef9e9e73512743756d1c24ab6c.svg",
                        "isPro": false,
                        "fullname": "Jayakumar Subramanian",
                        "user": "jasubram",
                        "type": "user"
                    },
                    "name": "Jayakumar Subramanian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:37:03.196Z",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9a7",
                    "name": "Ryan A. Rossi",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9a8",
                    "user": {
                        "_id": "67f1035155fe17a33dc71f23",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ohlRrzUI8VwvyYYxFXJuY.png",
                        "isPro": false,
                        "fullname": "Trung Bui",
                        "user": "TrungBui1111",
                        "type": "user"
                    },
                    "name": "Trung Bui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:37:13.349Z",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9a9",
                    "user": {
                        "_id": "675346f0ab1d47a36ca60b89",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mqWUUxAuFn3DsgIqoF_ah.png",
                        "isPro": false,
                        "fullname": "Nikos Vlassis",
                        "user": "Nikosapa",
                        "type": "user"
                    },
                    "name": "Nikos Vlassis",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:36:17.107Z",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9aa",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:14:23.916Z",
                    "hidden": false
                },
                {
                    "_id": "68786e45001546c83aa4f9ab",
                    "user": {
                        "_id": "64804fad8c6a3b8f11f73912",
                        "avatarUrl": "/avatars/61e37a91d4bba35fda9bf52aadd87745.svg",
                        "isPro": false,
                        "fullname": "Thien Huu Nguyen",
                        "user": "anoperson",
                        "type": "user"
                    },
                    "name": "Thien Huu Nguyen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:36:11.546Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-11T21:19:18.000Z",
            "submittedOnDailyAt": "2025-07-17T02:00:25.332Z",
            "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
            "upvotes": 6,
            "discussionId": "68786e45001546c83aa4f9ac",
            "ai_summary": "Lizard is a linearization framework that transforms Transformer-based LLMs into subquadratic architectures for efficient infinite-context generation, using a hybrid attention mechanism and hardware-aware training.",
            "ai_keywords": [
                "Transformer-based LLMs",
                "subquadratic architectures",
                "softmax attention",
                "key-value (KV) cache",
                "subquadratic attention mechanism",
                "gating module",
                "adaptive memory control",
                "constant-memory inference",
                "length generalization",
                "gated linear attention",
                "sliding window attention",
                "meta memory",
                "hardware-aware algorithm",
                "MMLU benchmark",
                "associative recall tasks"
            ]
        },
        "publishedAt": "2025-07-11T17:19:18.000Z",
        "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
        "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.09025.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.12462",
            "authors": [
                {
                    "_id": "68785eb6001546c83aa4f95b",
                    "name": "Yuxi Xiao",
                    "hidden": false
                },
                {
                    "_id": "68785eb6001546c83aa4f95c",
                    "user": {
                        "_id": "649bf403fd9cea8366d669ad",
                        "avatarUrl": "/avatars/27bd8ca9a948ec38fee950b64f669ce3.svg",
                        "isPro": false,
                        "fullname": "Jianyuan Wang",
                        "user": "JianyuanWang",
                        "type": "user"
                    },
                    "name": "Jianyuan Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:37:32.434Z",
                    "hidden": false
                },
                {
                    "_id": "68785eb6001546c83aa4f95d",
                    "user": {
                        "_id": "6485ce5ec7f19728a49df17a",
                        "avatarUrl": "/avatars/e83966e6906c1d0f151300981e30f85a.svg",
                        "isPro": true,
                        "fullname": "Nan",
                        "user": "cherubicxn",
                        "type": "user"
                    },
                    "name": "Nan Xue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:15:41.525Z",
                    "hidden": false
                },
                {
                    "_id": "68785eb6001546c83aa4f95e",
                    "user": {
                        "_id": "6393a73584c565d2c3416cb9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6393a73584c565d2c3416cb9/OGtX-i-_MLg5--qA054ti.jpeg",
                        "isPro": true,
                        "fullname": "Nikita Karaev",
                        "user": "nikkar",
                        "type": "user"
                    },
                    "name": "Nikita Karaev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:40:30.784Z",
                    "hidden": false
                },
                {
                    "_id": "68785eb6001546c83aa4f95f",
                    "name": "Yuri Makarov",
                    "hidden": false
                },
                {
                    "_id": "68785eb6001546c83aa4f960",
                    "user": {
                        "_id": "647b5fef6a79fbf5e996c47c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
                        "isPro": false,
                        "fullname": "Bingyi Kang",
                        "user": "bykang",
                        "type": "user"
                    },
                    "name": "Bingyi Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:40:39.614Z",
                    "hidden": false
                },
                {
                    "_id": "68785eb6001546c83aa4f961",
                    "name": "Xing Zhu",
                    "hidden": false
                },
                {
                    "_id": "68785eb6001546c83aa4f962",
                    "name": "Hujun Bao",
                    "hidden": false
                },
                {
                    "_id": "68785eb6001546c83aa4f963",
                    "name": "Yujun Shen",
                    "hidden": false
                },
                {
                    "_id": "68785eb6001546c83aa4f964",
                    "name": "Xiaowei Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-16T17:59:03.000Z",
            "submittedOnDailyAt": "2025-07-17T06:55:04.439Z",
            "title": "SpatialTrackerV2: 3D Point Tracking Made Easy",
            "submittedOnDailyBy": {
                "_id": "6688a8f30bf195d6e53ac28d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
                "isPro": true,
                "fullname": "Yuxi Xiao",
                "user": "Yuxihenry",
                "type": "user"
            },
            "summary": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50times faster.",
            "upvotes": 4,
            "discussionId": "68785eb7001546c83aa4f965",
            "projectPage": "https://spatialtracker.github.io",
            "githubRepo": "https://github.com/henry123-boy/SpaTrackerV2",
            "ai_summary": "SpatialTrackerV2 is a feed-forward 3D point tracking method for monocular videos that integrates point tracking, monocular depth, and camera pose estimation into a unified, end-to-end architecture, achieving high performance and speed.",
            "ai_keywords": [
                "feed-forward",
                "3D point tracking",
                "monocular videos",
                "intrinsic connections",
                "monocular depth",
                "camera pose estimation",
                "fully differentiable",
                "end-to-end architecture",
                "scene geometry",
                "camera ego-motion",
                "pixel-wise object motion",
                "synthetic sequences",
                "posed RGB-D videos",
                "unlabeled in-the-wild footage",
                "dynamic 3D reconstruction"
            ],
            "githubStars": 496
        },
        "publishedAt": "2025-07-16T13:59:03.000Z",
        "title": "SpatialTrackerV2: 3D Point Tracking Made Easy",
        "summary": "We present SpatialTrackerV2, a feed-forward 3D point tracking method for\nmonocular videos. Going beyond modular pipelines built on off-the-shelf\ncomponents for 3D tracking, our approach unifies the intrinsic connections\nbetween point tracking, monocular depth, and camera pose estimation into a\nhigh-performing and feedforward 3D point tracker. It decomposes world-space 3D\nmotion into scene geometry, camera ego-motion, and pixel-wise object motion,\nwith a fully differentiable and end-to-end architecture, allowing scalable\ntraining across a wide range of datasets, including synthetic sequences, posed\nRGB-D videos, and unlabeled in-the-wild footage. By learning geometry and\nmotion jointly from such heterogeneous data, SpatialTrackerV2 outperforms\nexisting 3D tracking methods by 30%, and matches the accuracy of leading\ndynamic 3D reconstruction approaches while running 50times faster.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12462.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6688a8f30bf195d6e53ac28d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6688a8f30bf195d6e53ac28d/5izbVmdCWWwA1wBjcxZPB.jpeg",
            "fullname": "Yuxi Xiao",
            "name": "Yuxihenry",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.05065",
            "authors": [
                {
                    "_id": "68776e57ff8f47a7f86442bd",
                    "user": {
                        "_id": "669e707ec517d804cfce91c5",
                        "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
                        "isPro": false,
                        "fullname": "Corrado Rainone",
                        "user": "crainone",
                        "type": "user"
                    },
                    "name": "Corrado Rainone",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-16T15:05:55.722Z",
                    "hidden": false
                },
                {
                    "_id": "68776e57ff8f47a7f86442be",
                    "name": "Tim Bakker",
                    "hidden": false
                },
                {
                    "_id": "68776e57ff8f47a7f86442bf",
                    "name": "Roland Memisevic",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-07T14:49:18.000Z",
            "submittedOnDailyAt": "2025-07-17T06:25:52.334Z",
            "title": "Replacing thinking with tool usage enables reasoning in small language\n  models",
            "submittedOnDailyBy": {
                "_id": "669e707ec517d804cfce91c5",
                "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
                "isPro": false,
                "fullname": "Corrado Rainone",
                "user": "crainone",
                "type": "user"
            },
            "summary": "Recent advances have established a new machine learning paradigm based on\nscaling up compute at inference time as well as at training time. In that line\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\nused for training Large Language Models to expend extra compute during\ninference in the form of \"thoughts\" expressed in natural language. In this\npaper, we propose to instead format these tokens as a multi-turn interaction\ntrace with a stateful tool. At each turn, the new state of the tool is appended\nto the context of the model, whose job is to generate the tokens necessary to\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\nrepairing malfunctioning Python code, and show that this constrained setup\nallows for faster sampling of experience and a denser reward signal, allowing\neven models of size up to 3B parameters to learn how to proficiently expend\nadditional compute on the task.",
            "upvotes": 3,
            "discussionId": "68776e58ff8f47a7f86442c0",
            "ai_summary": "A new approach formats tokens as a multi-turn interaction trace with a stateful tool for training Large Language Models, enabling faster sampling and denser reward signals for tasks like repairing Python code.",
            "ai_keywords": [
                "Supervised Fine-Tuning",
                "Reinforcement Learning with Verifiable Rewards",
                "Large Language Models",
                "multi-turn interaction trace",
                "stateful tool",
                "custom DSL"
            ]
        },
        "publishedAt": "2025-07-07T10:49:18.000Z",
        "title": "Replacing thinking with tool usage enables reasoning in small language\n  models",
        "summary": "Recent advances have established a new machine learning paradigm based on\nscaling up compute at inference time as well as at training time. In that line\nof work, a combination of Supervised Fine-Tuning (SFT) on synthetic\ndemonstrations and Reinforcement Learning with Verifiable Rewards (RLVR) is\nused for training Large Language Models to expend extra compute during\ninference in the form of \"thoughts\" expressed in natural language. In this\npaper, we propose to instead format these tokens as a multi-turn interaction\ntrace with a stateful tool. At each turn, the new state of the tool is appended\nto the context of the model, whose job is to generate the tokens necessary to\ncontrol the tool via a custom DSL. We benchmark this approach on the problem of\nrepairing malfunctioning Python code, and show that this constrained setup\nallows for faster sampling of experience and a denser reward signal, allowing\neven models of size up to 3B parameters to learn how to proficiently expend\nadditional compute on the task.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.05065.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "669e707ec517d804cfce91c5",
            "avatarUrl": "/avatars/fc18e64e100cbf28763db04b44242747.svg",
            "fullname": "Corrado Rainone",
            "name": "crainone",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.11764",
            "authors": [
                {
                    "_id": "6878a0ee001546c83aa4fa25",
                    "user": {
                        "_id": "65c4aa3f08a42345687aaa3a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c4aa3f08a42345687aaa3a/8HZT84z58rp6YMRyX_zha.jpeg",
                        "isPro": false,
                        "fullname": "MatteoFasulo",
                        "user": "MatteoFasulo",
                        "type": "user"
                    },
                    "name": "Matteo Fasulo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:17:28.898Z",
                    "hidden": false
                },
                {
                    "_id": "6878a0ee001546c83aa4fa26",
                    "user": {
                        "_id": "64cf700c749587dbe0172d39",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64cf700c749587dbe0172d39/VoZYUTI9SwfSCZJD64UtH.jpeg",
                        "isPro": false,
                        "fullname": "Luca",
                        "user": "ElectroDuck",
                        "type": "user"
                    },
                    "name": "Luca Babboni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:17:26.160Z",
                    "hidden": false
                },
                {
                    "_id": "6878a0ee001546c83aa4fa27",
                    "user": {
                        "_id": "647e3bab11084fb5831c4c51",
                        "avatarUrl": "/avatars/8126f7406f1516cbedfaff16b2617c6e.svg",
                        "isPro": false,
                        "fullname": "Luca Tedeschini ",
                        "user": "Luca289",
                        "type": "user"
                    },
                    "name": "Luca Tedeschini",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:17:31.749Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-15T22:10:20.000Z",
            "submittedOnDailyAt": "2025-07-17T11:10:51.050Z",
            "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings\n  with Sentiment for Subjectivity Detection in News Articles",
            "submittedOnDailyBy": {
                "_id": "65c4aa3f08a42345687aaa3a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c4aa3f08a42345687aaa3a/8HZT84z58rp6YMRyX_zha.jpeg",
                "isPro": false,
                "fullname": "MatteoFasulo",
                "user": "MatteoFasulo",
                "type": "user"
            },
            "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).",
            "upvotes": 2,
            "discussionId": "6878a0f0001546c83aa4fa28",
            "githubRepo": "https://github.com/MatteoFasulo/clef2025-checkthat",
            "ai_summary": "Sentiment-augmented transformer-based classifiers improve subjectivity detection in multilingual and zero-shot settings, achieving high performance and ranking first for Greek.",
            "ai_keywords": [
                "transformer-based classifiers",
                "sentiment scores",
                "mDeBERTaV3-base",
                "ModernBERT-base",
                "Llama3.2-1B",
                "decision threshold calibration",
                "class imbalance",
                "subjective F1 score",
                "Macro F1"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-07-15T18:10:20.000Z",
        "title": "AI Wizards at CheckThat! 2025: Enhancing Transformer-Based Embeddings\n  with Sentiment for Subjectivity Detection in News Articles",
        "summary": "This paper presents AI Wizards' participation in the CLEF 2025 CheckThat! Lab\nTask 1: Subjectivity Detection in News Articles, classifying sentences as\nsubjective/objective in monolingual, multilingual, and zero-shot settings.\nTraining/development datasets were provided for Arabic, German, English,\nItalian, and Bulgarian; final evaluation included additional unseen languages\n(e.g., Greek, Romanian, Polish, Ukrainian) to assess generalization. Our\nprimary strategy enhanced transformer-based classifiers by integrating\nsentiment scores, derived from an auxiliary model, with sentence\nrepresentations, aiming to improve upon standard fine-tuning. We explored this\nsentiment-augmented architecture with mDeBERTaV3-base, ModernBERT-base\n(English), and Llama3.2-1B. To address class imbalance, prevalent across\nlanguages, we employed decision threshold calibration optimized on the\ndevelopment set. Our experiments show sentiment feature integration\nsignificantly boosts performance, especially subjective F1 score. This\nframework led to high rankings, notably 1st for Greek (Macro F1 = 0.51).",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11764.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65c4aa3f08a42345687aaa3a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65c4aa3f08a42345687aaa3a/8HZT84z58rp6YMRyX_zha.jpeg",
            "fullname": "MatteoFasulo",
            "name": "MatteoFasulo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.07451",
            "authors": [
                {
                    "_id": "68747204257d4f04353702de",
                    "user": {
                        "_id": "6474b290d815855e4ef59b05",
                        "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
                        "isPro": false,
                        "fullname": "Hongzhi Zhang",
                        "user": "hongzhizhang",
                        "type": "user"
                    },
                    "name": "Hongzhi Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T08:21:31.114Z",
                    "hidden": false
                },
                {
                    "_id": "68747204257d4f04353702df",
                    "name": "Jia Fu",
                    "hidden": false
                },
                {
                    "_id": "68747204257d4f04353702e0",
                    "name": "Jingyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68747204257d4f04353702e1",
                    "name": "Kai Fu",
                    "hidden": false
                },
                {
                    "_id": "68747204257d4f04353702e2",
                    "name": "Qi Wang",
                    "hidden": false
                },
                {
                    "_id": "68747204257d4f04353702e3",
                    "user": {
                        "_id": "67c5945da1661d5fa6f29adb",
                        "avatarUrl": "/avatars/62561f3875c0c251cae949acc38d72dc.svg",
                        "isPro": false,
                        "fullname": "Fuzheng Zhang",
                        "user": "Edrex",
                        "type": "user"
                    },
                    "name": "Fuzheng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:35:13.823Z",
                    "hidden": false
                },
                {
                    "_id": "68747204257d4f04353702e4",
                    "user": {
                        "_id": "67c6c570cf87e2d2ebfc81aa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67c6c570cf87e2d2ebfc81aa/7qAstZtIT86Uwrz3u_anv.jpeg",
                        "isPro": false,
                        "fullname": "Guorui Zhou",
                        "user": "GuoruiZhou",
                        "type": "user"
                    },
                    "name": "Guorui Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-17T09:34:55.007Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-10T05:58:55.000Z",
            "submittedOnDailyAt": "2025-07-17T07:36:19.087Z",
            "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "6474b290d815855e4ef59b05",
                "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
                "isPro": false,
                "fullname": "Hongzhi Zhang",
                "user": "hongzhizhang",
                "type": "user"
            },
            "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present RLEP\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.",
            "upvotes": 2,
            "discussionId": "68747204257d4f04353702e5",
            "githubRepo": "https://github.com/Kwai-Klear/RLEP",
            "ai_summary": "RLEP, a reinforcement learning framework with experience replay, enhances large language model training by focusing on high-quality examples, leading to faster convergence and improved performance on math-related benchmarks.",
            "ai_keywords": [
                "reinforcement learning",
                "experience replay",
                "trajectories",
                "rollouts",
                "policy optimization",
                "convergence",
                "performance",
                "Qwen2.5-Math-7B",
                "AIME-2024",
                "AIME-2025",
                "AMC-2023"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-07-10T01:58:55.000Z",
        "title": "RLEP: Reinforcement Learning with Experience Replay for LLM Reasoning",
        "summary": "Reinforcement learning (RL) for large language models is an energy-intensive\nendeavor: training can be unstable, and the policy may gradually drift away\nfrom its pretrained weights. We present RLEP\\, -- \\,Reinforcement\nLearning with Experience rePlay\\, -- \\,a two-phase framework that first\ncollects verified trajectories and then replays them during subsequent\ntraining. At every update step, the policy is optimized on mini-batches that\nblend newly generated rollouts with these replayed successes. By replaying\nhigh-quality examples, RLEP steers the model away from fruitless exploration,\nfocuses learning on promising reasoning paths, and delivers both faster\nconvergence and stronger final performance. On the Qwen2.5-Math-7B base model,\nRLEP reaches baseline peak accuracy with substantially fewer updates and\nultimately surpasses it, improving accuracy on AIME-2024 from 38.2% to 39.9%,\non AIME-2025 from 19.8% to 22.3%, and on AMC-2023 from 77.0% to 82.2%. Our\ncode, datasets, and checkpoints are publicly available at\nhttps://github.com/Kwai-Klear/RLEP to facilitate reproducibility and further\nresearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07451.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6474b290d815855e4ef59b05",
            "avatarUrl": "/avatars/decf3cfe8a5b2203f0520cdb3d26ee40.svg",
            "fullname": "Hongzhi Zhang",
            "name": "hongzhizhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.12367",
            "authors": [
                {
                    "_id": "6878bb18001546c83aa4fa92",
                    "name": "Diganta Misra",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa93",
                    "name": "Nizar Islah",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa94",
                    "name": "Victor May",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa95",
                    "name": "Brice Rauby",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa96",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa97",
                    "name": "Justine Gehring",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa98",
                    "name": "Antonio Orvieto",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa99",
                    "name": "Muawiz Chaudhary",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa9a",
                    "name": "Eilif B. Muller",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa9b",
                    "name": "Irina Rish",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa9c",
                    "name": "Samira Ebrahimi Kahou",
                    "hidden": false
                },
                {
                    "_id": "6878bb18001546c83aa4fa9d",
                    "name": "Massimo Caccia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-16T16:10:42.000Z",
            "submittedOnDailyAt": "2025-07-17T22:31:38.577Z",
            "title": "GitChameleon: Evaluating AI Code Generation Against Python Library\n  Version Incompatibilities",
            "submittedOnDailyBy": {
                "_id": "643006f01572f43a481766a9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643006f01572f43a481766a9/egLlzmsWxHovmuvqAPnBO.jpeg",
                "isPro": false,
                "fullname": "_",
                "user": "Xa9aX",
                "type": "user"
            },
            "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.",
            "upvotes": 0,
            "discussionId": "6878bb19001546c83aa4fa9e",
            "ai_summary": "GitChameleon is a dataset for evaluating version-conditioned code generation by large language models, LLM-powered agents, code assistants, and RAG systems using execution-based tests.",
            "ai_keywords": [
                "GitChameleon",
                "large language models",
                "LLM-powered agents",
                "code assistants",
                "RAG systems",
                "code completion",
                "library versions",
                "executable unit tests",
                "version-conditioned code generation",
                "functional accuracy"
            ]
        },
        "publishedAt": "2025-07-16T12:10:42.000Z",
        "title": "GitChameleon: Evaluating AI Code Generation Against Python Library\n  Version Incompatibilities",
        "summary": "The rapid evolution of software libraries poses a considerable hurdle for\ncode generation, necessitating continuous adaptation to frequent version\nupdates while preserving backward compatibility. While existing code evolution\nbenchmarks provide valuable insights, they typically lack execution-based\nevaluation for generating code compliant with specific library versions. To\naddress this, we introduce GitChameleon, a novel, meticulously curated dataset\ncomprising 328 Python code completion problems, each conditioned on specific\nlibrary versions and accompanied by executable unit tests. GitChameleon\nrigorously evaluates the capacity of contemporary large language models (LLMs),\nLLM-powered agents, code assistants, and RAG systems to perform\nversion-conditioned code generation that demonstrates functional accuracy\nthrough execution. Our extensive evaluations indicate that state-of-the-art\nsystems encounter significant challenges with this task; enterprise models\nachieving baseline success rates in the 48-51\\% range, underscoring the\nintricacy of the problem. By offering an execution-based benchmark emphasizing\nthe dynamic nature of code libraries, GitChameleon enables a clearer\nunderstanding of this challenge and helps guide the development of more\nadaptable and dependable AI code generation methods. We make the dataset and\nevaluation code publicly available at\nhttps://github.com/mrcabbage972/GitChameleonBenchmark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.12367.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "643006f01572f43a481766a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643006f01572f43a481766a9/egLlzmsWxHovmuvqAPnBO.jpeg",
            "fullname": "_",
            "name": "Xa9aX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.10015",
            "authors": [
                {
                    "_id": "68798f2021b37e676c8e4050",
                    "name": "Jaisidh Singh",
                    "hidden": false
                },
                {
                    "_id": "68798f2021b37e676c8e4051",
                    "name": "Diganta Misra",
                    "hidden": false
                },
                {
                    "_id": "68798f2021b37e676c8e4052",
                    "name": "Boris Knyazev",
                    "hidden": false
                },
                {
                    "_id": "68798f2021b37e676c8e4053",
                    "name": "Antonio Orvieto",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-14T07:51:01.000Z",
            "submittedOnDailyAt": "2025-07-17T22:34:01.134Z",
            "title": "(Almost) Free Modality Stitching of Foundation Models",
            "submittedOnDailyBy": {
                "_id": "643006f01572f43a481766a9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643006f01572f43a481766a9/egLlzmsWxHovmuvqAPnBO.jpeg",
                "isPro": false,
                "fullname": "_",
                "user": "Xa9aX",
                "type": "user"
            },
            "summary": "Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\ntext model. This stitching process is performed by training a connector module\nthat aims to align the representation spaces of these uni-modal models towards\na multi-modal objective. However, given the complexity of training such\nconnectors on large scale web-based datasets coupled with the ever-increasing\nnumber of available pretrained uni-modal models, the task of uni-modal models\nselection and subsequent connector module training becomes computationally\ndemanding. To address this under-studied critical problem, we propose\nHypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal\nuni-modal model selection and connector training by leveraging hypernetworks.\nSpecifically, our framework utilizes the parameter prediction capability of a\nhypernetwork to obtain jointly trained connector modules for N times M\ncombinations of uni-modal models. In our experiments, Hyma reduces the cost of\nsearching for the best performing uni-modal model pair by 10times, while\nmatching the ranking and trained connector performance obtained via grid search\nacross a suite of diverse multi-modal benchmarks.",
            "upvotes": 0,
            "discussionId": "68798f2021b37e676c8e4054",
            "ai_summary": "Hypernetwork Model Alignment (Hyma) optimizes uni-modal model selection and connector training for multi-modal models, reducing search costs while maintaining performance.",
            "ai_keywords": [
                "Hypernetwork Model Alignment",
                "Hyma",
                "hypernetworks",
                "parameter prediction",
                "connector modules",
                "uni-modal models",
                "multi-modal models",
                "grid search",
                "multi-modal benchmarks"
            ]
        },
        "publishedAt": "2025-07-14T03:51:01.000Z",
        "title": "(Almost) Free Modality Stitching of Foundation Models",
        "summary": "Foundation multi-modal models are often designed by stitching of multiple\nexisting pretrained uni-modal models: for example, an image classifier with an\ntext model. This stitching process is performed by training a connector module\nthat aims to align the representation spaces of these uni-modal models towards\na multi-modal objective. However, given the complexity of training such\nconnectors on large scale web-based datasets coupled with the ever-increasing\nnumber of available pretrained uni-modal models, the task of uni-modal models\nselection and subsequent connector module training becomes computationally\ndemanding. To address this under-studied critical problem, we propose\nHypernetwork Model Alignment (Hyma), a novel all-in-one solution for optimal\nuni-modal model selection and connector training by leveraging hypernetworks.\nSpecifically, our framework utilizes the parameter prediction capability of a\nhypernetwork to obtain jointly trained connector modules for N times M\ncombinations of uni-modal models. In our experiments, Hyma reduces the cost of\nsearching for the best performing uni-modal model pair by 10times, while\nmatching the ranking and trained connector performance obtained via grid search\nacross a suite of diverse multi-modal benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10015.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "643006f01572f43a481766a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643006f01572f43a481766a9/egLlzmsWxHovmuvqAPnBO.jpeg",
            "fullname": "_",
            "name": "Xa9aX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.07015",
            "authors": [
                {
                    "_id": "687918adcc15e42a72b01ae6",
                    "user": {
                        "_id": "683fb1132548830f5d65fc6f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/683fb1132548830f5d65fc6f/Xjasc-yrB0BdZpGKP4TVq.png",
                        "isPro": false,
                        "fullname": "Hui Li",
                        "user": "Gray1y",
                        "type": "user"
                    },
                    "name": "Hui Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-17T18:43:11.149Z",
                    "hidden": false
                },
                {
                    "_id": "687918adcc15e42a72b01ae7",
                    "name": "Pengfei Yang",
                    "hidden": false
                },
                {
                    "_id": "687918adcc15e42a72b01ae8",
                    "name": "Juanyang Chen",
                    "hidden": false
                },
                {
                    "_id": "687918adcc15e42a72b01ae9",
                    "name": "Le Dong",
                    "hidden": false
                },
                {
                    "_id": "687918adcc15e42a72b01aea",
                    "name": "Yanxin Chen",
                    "hidden": false
                },
                {
                    "_id": "687918adcc15e42a72b01aeb",
                    "name": "Quan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-09T16:45:28.000Z",
            "submittedOnDailyAt": "2025-07-17T23:10:16.974Z",
            "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge\n  Distillation",
            "submittedOnDailyBy": {
                "_id": "683fb1132548830f5d65fc6f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/683fb1132548830f5d65fc6f/Xjasc-yrB0BdZpGKP4TVq.png",
                "isPro": false,
                "fullname": "Hui Li",
                "user": "Gray1y",
                "type": "user"
            },
            "summary": "Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill.",
            "upvotes": 0,
            "discussionId": "687918adcc15e42a72b01aec",
            "githubRepo": "https://github.com/Gray-OREO/MST-Distill",
            "ai_summary": "MST-Distill, a novel cross-modal knowledge distillation framework, uses a mixture of specialized teachers and an instance-level routing network to address distillation path selection and knowledge drift, outperforming existing methods across multimodal datasets.",
            "ai_keywords": [
                "knowledge distillation",
                "cross-modal",
                "multimodal",
                "teacher models",
                "instance-level routing network",
                "knowledge drift",
                "masking module"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-07-09T12:45:28.000Z",
        "title": "MST-Distill: Mixture of Specialized Teachers for Cross-Modal Knowledge\n  Distillation",
        "summary": "Knowledge distillation as an efficient knowledge transfer technique, has\nachieved remarkable success in unimodal scenarios. However, in cross-modal\nsettings, conventional distillation methods encounter significant challenges\ndue to data and statistical heterogeneities, failing to leverage the\ncomplementary prior knowledge embedded in cross-modal teacher models. This\npaper empirically reveals two critical issues in existing approaches:\ndistillation path selection and knowledge drift. To address these limitations,\nwe propose MST-Distill, a novel cross-modal knowledge distillation framework\nfeaturing a mixture of specialized teachers. Our approach employs a diverse\nensemble of teacher models across both cross-modal and multimodal\nconfigurations, integrated with an instance-level routing network that\nfacilitates adaptive and dynamic distillation. This architecture effectively\ntranscends the constraints of traditional methods that rely on monotonous and\nstatic teacher models. Additionally, we introduce a plug-in masking module,\nindependently trained to suppress modality-specific discrepancies and\nreconstruct teacher representations, thereby mitigating knowledge drift and\nenhancing transfer effectiveness. Extensive experiments across five diverse\nmultimodal datasets, spanning visual, audio, and text, demonstrate that our\nmethod significantly outperforms existing state-of-the-art knowledge\ndistillation methods in cross-modal distillation tasks. The source code is\navailable at https://github.com/Gray-OREO/MST-Distill.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.07015.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "683fb1132548830f5d65fc6f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/683fb1132548830f5d65fc6f/Xjasc-yrB0BdZpGKP4TVq.png",
            "fullname": "Hui Li",
            "name": "Gray1y",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]