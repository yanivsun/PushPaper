[
    {
        "paper": {
            "id": "2507.16863",
            "authors": [
                {
                    "_id": "6881be07df7c5aafaf37f0e1",
                    "name": "Hongcheng Gao",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0e2",
                    "name": "Zihao Huang",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0e3",
                    "name": "Lin Xu",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0e4",
                    "name": "Jingyi Tang",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0e5",
                    "name": "Xinhao Li",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0e6",
                    "name": "Yue Liu",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0e7",
                    "name": "Haoyang Li",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0e8",
                    "name": "Taihang Hu",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0e9",
                    "name": "Minhua Lin",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0ea",
                    "name": "Xinlong Yang",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0eb",
                    "name": "Ge Wu",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0ec",
                    "name": "Balong Bi",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0ed",
                    "name": "Hongyu Chen",
                    "hidden": false
                },
                {
                    "_id": "6881be07df7c5aafaf37f0ee",
                    "name": "Wentao Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-21T21:50:16.000Z",
            "submittedOnDailyAt": "2025-07-24T03:31:48.243Z",
            "title": "Pixels, Patterns, but No Poetry: To See The World like Humans",
            "submittedOnDailyBy": {
                "_id": "62728f4f6253fe2068da1021",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
                "isPro": false,
                "fullname": "Hongcheng Gao",
                "user": "HongchengGao",
                "type": "user"
            },
            "summary": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.",
            "upvotes": 48,
            "discussionId": "6881be08df7c5aafaf37f0ef",
            "projectPage": "https://turingeyetest.github.io/",
            "ai_summary": "The Turing Eye Test evaluates MLLMs' perceptual abilities through synthetic images, revealing that vision tower generalization is a significant gap compared to human perception.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Turing Eye Test",
                "in-context learning",
                "fine-tuning",
                "vision tower",
                "visual generalization"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-07-21T17:50:16.000Z",
        "title": "Pixels, Patterns, but No Poetry: To See The World like Humans",
        "summary": "Achieving human-like perception and reasoning in Multimodal Large Language\nModels (MLLMs) remains a central challenge in artificial intelligence. While\nrecent research has primarily focused on enhancing reasoning capabilities in\nMLLMs, a fundamental question persists: Can Multimodal Large Language Models\ntruly perceive the world as humans do? This paper shifts focus from reasoning\nto perception. Rather than constructing benchmarks specifically for reasoning,\nwe introduce the Turing Eye Test (TET), a challenging perception-oriented\nbenchmark comprising four diagnostic tasks that evaluate MLLMs' performance on\nsynthetic images that humans process intuitively. Our findings reveal that\nstate-of-the-art MLLMs exhibit catastrophic failures on our perceptual tasks\ntrivial for humans. Both in-context learning and training on language\nbackbone-effective for previous benchmarks-fail to improve performance on our\ntasks, while fine-tuning the vision tower enables rapid adaptation, suggesting\nthat our benchmark poses challenges for vision tower generalization rather than\nfor the knowledge and reasoning capabilities of the language backbone-a key gap\nbetween current MLLMs and human perception. We release a representative subset\nof TET tasks in this version, and will introduce more diverse tasks and methods\nto enhance visual generalization in future work.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16863.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "62728f4f6253fe2068da1021",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62728f4f6253fe2068da1021/KZ65X0EH98AF3zXemPiap.jpeg",
            "fullname": "Hongcheng Gao",
            "name": "HongchengGao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.17744",
            "authors": [
                {
                    "_id": "6881b7d1df7c5aafaf37f0d5",
                    "name": "Xiaofeng Mao",
                    "hidden": false
                },
                {
                    "_id": "6881b7d1df7c5aafaf37f0d6",
                    "name": "Shaoheng Lin",
                    "hidden": false
                },
                {
                    "_id": "6881b7d1df7c5aafaf37f0d7",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "6881b7d1df7c5aafaf37f0d8",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "6881b7d1df7c5aafaf37f0d9",
                    "name": "Wenshuo Peng",
                    "hidden": false
                },
                {
                    "_id": "6881b7d1df7c5aafaf37f0da",
                    "name": "Tong He",
                    "hidden": false
                },
                {
                    "_id": "6881b7d1df7c5aafaf37f0db",
                    "user": {
                        "_id": "65783ee6ee33d547aecc3ffc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                        "isPro": false,
                        "fullname": "Jiangmiao Pang",
                        "user": "Jiangmiao",
                        "type": "user"
                    },
                    "name": "Jiangmiao Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:00:40.580Z",
                    "hidden": false
                },
                {
                    "_id": "6881b7d1df7c5aafaf37f0dc",
                    "name": "Mingmin Chi",
                    "hidden": false
                },
                {
                    "_id": "6881b7d1df7c5aafaf37f0dd",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6881b7d1df7c5aafaf37f0de",
                    "user": {
                        "_id": "63527f4e7d071f23d085ad45",
                        "avatarUrl": "/avatars/99a51adef5673b3ac1a8c02eb47759c4.svg",
                        "isPro": false,
                        "fullname": "KAIPENG ZHANG",
                        "user": "kpzhang",
                        "type": "user"
                    },
                    "name": "Kaipeng Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:00:46.757Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/l3pzwBZRLJ2_T_7faBEK6.mp4"
            ],
            "publishedAt": "2025-07-23T17:57:09.000Z",
            "submittedOnDailyAt": "2025-07-24T04:38:47.770Z",
            "title": "Yume: An Interactive World Generation Model",
            "submittedOnDailyBy": {
                "_id": "65f1713552c38a91e0a445e8",
                "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
                "isPro": false,
                "fullname": "kaipeng",
                "user": "kpzhang996",
                "type": "user"
            },
            "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
            "upvotes": 45,
            "discussionId": "6881b7d1df7c5aafaf37f0df",
            "projectPage": "https://stdstu12.github.io/YUME-Project/",
            "githubRepo": "https://github.com/stdstu12/YUME",
            "ai_summary": "A framework for generating and exploring interactive video worlds from images using Masked Video Diffusion Transformer, Anti-Artifact Mechanism, Time Travel Sampling, and model acceleration techniques.",
            "ai_keywords": [
                "camera motion quantization",
                "Masked Video Diffusion Transformer",
                "MVDT",
                "memory module",
                "infinite video generation",
                "autoregressive",
                "Anti-Artifact Mechanism",
                "AAM",
                "Time Travel Sampling",
                "TTS-SDE",
                "stochastic differential equations",
                "model acceleration",
                "adversarial distillation",
                "caching mechanisms"
            ],
            "githubStars": 84
        },
        "publishedAt": "2025-07-23T13:57:09.000Z",
        "title": "Yume: An Interactive World Generation Model",
        "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65f1713552c38a91e0a445e8/l3pzwBZRLJ2_T_7faBEK6.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17744.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65f1713552c38a91e0a445e8",
            "avatarUrl": "/avatars/47ab3ada51c9b9976ac1cd0c4301c373.svg",
            "fullname": "kaipeng",
            "name": "kpzhang996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.17202",
            "authors": [
                {
                    "_id": "6881cb17df7c5aafaf37f0f1",
                    "user": {
                        "_id": "6369f693bf21b20c5692937b",
                        "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg",
                        "isPro": false,
                        "fullname": "Jooyeol Yun",
                        "user": "YeolJoo",
                        "type": "user"
                    },
                    "name": "Jooyeol Yun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-24T09:13:01.588Z",
                    "hidden": false
                },
                {
                    "_id": "6881cb17df7c5aafaf37f0f2",
                    "name": "Heng Wang",
                    "hidden": false
                },
                {
                    "_id": "6881cb17df7c5aafaf37f0f3",
                    "name": "Yotaro Shimose",
                    "hidden": false
                },
                {
                    "_id": "6881cb17df7c5aafaf37f0f4",
                    "name": "Jaegul Choo",
                    "hidden": false
                },
                {
                    "_id": "6881cb17df7c5aafaf37f0f5",
                    "name": "Shingo Takamatsu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-23T04:49:48.000Z",
            "submittedOnDailyAt": "2025-07-24T04:33:49.250Z",
            "title": "DesignLab: Designing Slides Through Iterative Detection and Correction",
            "submittedOnDailyBy": {
                "_id": "6369f693bf21b20c5692937b",
                "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg",
                "isPro": false,
                "fullname": "Jooyeol Yun",
                "user": "YeolJoo",
                "type": "user"
            },
            "summary": "Designing high-quality presentation slides can be challenging for non-experts\ndue to the complexity involved in navigating various design choices. Numerous\nautomated tools can suggest layouts and color schemes, yet often lack the\nability to refine their own output, which is a key aspect in real-world\nworkflows. We propose DesignLab, which separates the design process into two\nroles, the design reviewer, who identifies design-related issues, and the\ndesign contributor who corrects them. This decomposition enables an iterative\nloop where the reviewer continuously detects issues and the contributor\ncorrects them, allowing a draft to be further polished with each iteration,\nreaching qualities that were unattainable. We fine-tune large language models\nfor these roles and simulate intermediate drafts by introducing controlled\nperturbations, enabling the design reviewer learn design errors and the\ncontributor learn how to fix them. Our experiments show that DesignLab\noutperforms existing design-generation methods, including a commercial tool, by\nembracing the iterative nature of designing which can result in polished,\nprofessional slides.",
            "upvotes": 33,
            "discussionId": "6881cb17df7c5aafaf37f0f6",
            "projectPage": "https://yeolj00.github.io/personal-projects/designlab/",
            "ai_summary": "DesignLab uses fine-tuned large language models to iteratively improve presentation slides through a design reviewer and contributor system, outperforming existing tools.",
            "ai_keywords": [
                "large language models",
                "design reviewer",
                "design contributor",
                "iterative loop",
                "controlled perturbations",
                "design errors"
            ]
        },
        "publishedAt": "2025-07-23T00:49:48.000Z",
        "title": "DesignLab: Designing Slides Through Iterative Detection and Correction",
        "summary": "Designing high-quality presentation slides can be challenging for non-experts\ndue to the complexity involved in navigating various design choices. Numerous\nautomated tools can suggest layouts and color schemes, yet often lack the\nability to refine their own output, which is a key aspect in real-world\nworkflows. We propose DesignLab, which separates the design process into two\nroles, the design reviewer, who identifies design-related issues, and the\ndesign contributor who corrects them. This decomposition enables an iterative\nloop where the reviewer continuously detects issues and the contributor\ncorrects them, allowing a draft to be further polished with each iteration,\nreaching qualities that were unattainable. We fine-tune large language models\nfor these roles and simulate intermediate drafts by introducing controlled\nperturbations, enabling the design reviewer learn design errors and the\ncontributor learn how to fix them. Our experiments show that DesignLab\noutperforms existing design-generation methods, including a commercial tool, by\nembracing the iterative nature of designing which can result in polished,\nprofessional slides.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17202.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6369f693bf21b20c5692937b",
            "avatarUrl": "/avatars/e937dc8234b3e456149882bfce34841f.svg",
            "fullname": "Jooyeol Yun",
            "name": "YeolJoo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.17512",
            "authors": [
                {
                    "_id": "6881a669df7c5aafaf37f0bc",
                    "user": {
                        "_id": "671b852aa4fa4f8f5fb5404c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/671b852aa4fa4f8f5fb5404c/TDLsgP8WgKW-qaA8Ys-iJ.jpeg",
                        "isPro": false,
                        "fullname": "YU LI",
                        "user": "yu0226",
                        "type": "user"
                    },
                    "name": "Yu Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:02:03.468Z",
                    "hidden": false
                },
                {
                    "_id": "6881a669df7c5aafaf37f0bd",
                    "name": "Zhuoshi Pan",
                    "hidden": false
                },
                {
                    "_id": "6881a669df7c5aafaf37f0be",
                    "name": "Honglin Lin",
                    "hidden": false
                },
                {
                    "_id": "6881a669df7c5aafaf37f0bf",
                    "user": {
                        "_id": "67ad790c2b28204981be8e24",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67ad790c2b28204981be8e24/KstE5e5bUXXIvgPJqMO2B.jpeg",
                        "isPro": false,
                        "fullname": "Mengyuan Sun",
                        "user": "blue01223",
                        "type": "user"
                    },
                    "name": "Mengyuan Sun",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:01:39.741Z",
                    "hidden": false
                },
                {
                    "_id": "6881a669df7c5aafaf37f0c0",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "6881a669df7c5aafaf37f0c1",
                    "name": "Lijun Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-23T13:51:04.000Z",
            "submittedOnDailyAt": "2025-07-24T01:51:17.747Z",
            "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "671b852aa4fa4f8f5fb5404c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/671b852aa4fa4f8f5fb5404c/TDLsgP8WgKW-qaA8Ys-iJ.jpeg",
                "isPro": false,
                "fullname": "YU LI",
                "user": "yu0226",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.",
            "upvotes": 25,
            "discussionId": "6881a669df7c5aafaf37f0c2"
        },
        "publishedAt": "2025-07-23T09:51:04.000Z",
        "title": "Can One Domain Help Others? A Data-Centric Study on Multi-Domain\n  Reasoning via Reinforcement Learning",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing the reasoning capabilities of LLMs. Existing\nresearch has predominantly concentrated on isolated reasoning domains such as\nmathematical problem-solving, coding tasks, or logical reasoning. However, real\nworld reasoning scenarios inherently demand an integrated application of\nmultiple cognitive skills. Despite this, the interplay among these reasoning\nskills under reinforcement learning remains poorly understood. To bridge this\ngap, we present a systematic investigation of multi-domain reasoning within the\nRLVR framework, explicitly focusing on three primary domains: mathematical\nreasoning, code generation, and logical puzzle solving. We conduct a\ncomprehensive study comprising four key components: (1) Leveraging the GRPO\nalgorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the\nmodels' in-domain improvements and cross-domain generalization capabilities\nwhen trained on single-domain datasets. (2) Additionally, we examine the\nintricate interactions including mutual enhancements and conflicts that emerge\nduring combined cross-domain training. (3) To further understand the influence\nof SFT on RL, we also analyze and compare performance differences between base\nand instruct models under identical RL configurations. (4) Furthermore, we\ndelve into critical RL training details, systematically exploring the impacts\nof curriculum learning strategies, variations in reward design, and\nlanguage-specific factors. Through extensive experiments, our results offer\nsignificant insights into the dynamics governing domain interactions, revealing\nkey factors influencing both specialized and generalizable reasoning\nperformance. These findings provide valuable guidance for optimizing RL\nmethodologies to foster comprehensive, multi-domain reasoning capabilities in\nLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17512.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "671b852aa4fa4f8f5fb5404c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/671b852aa4fa4f8f5fb5404c/TDLsgP8WgKW-qaA8Ys-iJ.jpeg",
            "fullname": "YU LI",
            "name": "yu0226",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.16725",
            "authors": [
                {
                    "_id": "68807c5900f4b5a05f2fbeab",
                    "user": {
                        "_id": "643a587fe2b979ae6141b193",
                        "avatarUrl": "/avatars/1726b6a1629d800795f9bdf6d03ad190.svg",
                        "isPro": false,
                        "fullname": "yilong xu",
                        "user": "sapphirex",
                        "type": "user"
                    },
                    "name": "Yilong Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:08:21.488Z",
                    "hidden": false
                },
                {
                    "_id": "68807c5900f4b5a05f2fbeac",
                    "name": "Xiang Long",
                    "hidden": false
                },
                {
                    "_id": "68807c5900f4b5a05f2fbead",
                    "name": "Zhi Zheng",
                    "hidden": false
                },
                {
                    "_id": "68807c5900f4b5a05f2fbeae",
                    "name": "Jinhua Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-22T16:08:12.000Z",
            "submittedOnDailyAt": "2025-07-24T08:54:42.339Z",
            "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
            "submittedOnDailyBy": {
                "_id": "643a587fe2b979ae6141b193",
                "avatarUrl": "/avatars/1726b6a1629d800795f9bdf6d03ad190.svg",
                "isPro": false,
                "fullname": "yilong xu",
                "user": "sapphirex",
                "type": "user"
            },
            "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.",
            "upvotes": 25,
            "discussionId": "68807c5900f4b5a05f2fbeaf",
            "githubRepo": "https://github.com/SwordFaith/RAVine",
            "ai_summary": "A new evaluation framework called RAVine is proposed to assess agentic search systems by focusing on realistic queries, accurate ground truth, and iterative process efficiency.",
            "ai_keywords": [
                "agentic search",
                "retrieval augmentation",
                "intelligent search systems",
                "evaluation frameworks",
                "complex queries",
                "ground truth extraction",
                "end-to-end evaluations",
                "multi-point queries",
                "long-form answers",
                "user intents",
                "attributable ground truth",
                "search tools",
                "iterative process",
                "efficiency"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-07-22T12:08:12.000Z",
        "title": "RAVine: Reality-Aligned Evaluation for Agentic Search",
        "summary": "Agentic search, as a more autonomous and adaptive paradigm of retrieval\naugmentation, is driving the evolution of intelligent search systems. However,\nexisting evaluation frameworks fail to align well with the goals of agentic\nsearch. First, the complex queries commonly used in current benchmarks often\ndeviate from realistic user search scenarios. Second, prior approaches tend to\nintroduce noise when extracting ground truth for end-to-end evaluations,\nleading to distorted assessments at a fine-grained level. Third, most current\nframeworks focus solely on the quality of final answers, neglecting the\nevaluation of the iterative process inherent to agentic search. To address\nthese limitations, we propose RAVine -- a Reality-Aligned eValuation framework\nfor agentic LLMs with search. RAVine targets multi-point queries and long-form\nanswers that better reflect user intents, and introduces an attributable ground\ntruth construction strategy to enhance the accuracy of fine-grained evaluation.\nMoreover, RAVine examines model's interaction with search tools throughout the\niterative process, and accounts for factors of efficiency. We benchmark a\nseries of models using RAVine and derive several insights, which we hope will\ncontribute to advancing the development of agentic search systems. The code and\ndatasets are available at https://github.com/SwordFaith/RAVine.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16725.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "643a587fe2b979ae6141b193",
            "avatarUrl": "/avatars/1726b6a1629d800795f9bdf6d03ad190.svg",
            "fullname": "yilong xu",
            "name": "sapphirex",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.16331",
            "authors": [
                {
                    "_id": "6881d9d8df7c5aafaf37f117",
                    "name": "Chuanhao Yan",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f118",
                    "name": "Fengdi Che",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f119",
                    "name": "Xuhan Huang",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f11a",
                    "name": "Xu Xu",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f11b",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f11c",
                    "name": "Yizhi Li",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f11d",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f11e",
                    "name": "Jingzhe Shi",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f11f",
                    "name": "Zhuangzhuang He",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f120",
                    "name": "Chenghua Lin",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f121",
                    "name": "Yaodong Yang",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f122",
                    "name": "Binhang Yuan",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f123",
                    "name": "Hang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f124",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f125",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "6881d9d8df7c5aafaf37f126",
                    "name": "Jie Fu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-22T08:13:01.000Z",
            "submittedOnDailyAt": "2025-07-24T05:31:08.422Z",
            "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny",
            "submittedOnDailyBy": {
                "_id": "641a6895fb5ffff5ac79d593",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a6895fb5ffff5ac79d593/vxvwsto3llOEWGqQKGMYx.jpeg",
                "isPro": false,
                "fullname": "Jie Fu",
                "user": "bigaidream",
                "type": "user"
            },
            "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark.",
            "upvotes": 15,
            "discussionId": "6881d9d8df7c5aafaf37f127",
            "projectPage": "https://veri-code.github.io/ReForm-page/",
            "githubRepo": "https://github.com/Veri-Code/ReForm",
            "ai_summary": "Formal language-based reasoning and automatic verification improve the reliability and scalability of Large Language Models for generating verifiable programs.",
            "ai_keywords": [
                "Reinforcement Learning",
                "formal language-based reasoning",
                "formal systems",
                "formal language spaces",
                "generative models",
                "automatic verification",
                "chain-of-thought",
                "human priors",
                "data curation pipeline",
                "supervised fine-tuning",
                "DafnyComp",
                "compositional formal programs",
                "auto-formalized specifications",
                "specification reasoning",
                "syntactically valid",
                "verifiable Dafny code",
                "regularization",
                "generalization",
                "out-of-domain tasks"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-07-22T04:13:01.000Z",
        "title": "Re:Form -- Reducing Human Priors in Scalable Formal Software\n  Verification with RL in LLMs: A Preliminary Study on Dafny",
        "summary": "Existing informal language-based (e.g., human language) Large Language Models\n(LLMs) trained with Reinforcement Learning (RL) face a significant challenge:\ntheir verification processes, which provide crucial training signals, are\nneither reliable nor scalable. In fact, the prevalent large proprietary models\ncould hardly generate verifiable programs. A promising yet largely uncharted\nalternative is formal language-based reasoning. Grounding LLMs in rigorous\nformal systems where generative models operate in formal language spaces (e.g.,\nDafny) enables the automatic and mathematically provable verification of their\nreasoning processes and outcomes. This capability is pivotal for achieving\nlarge-scale, reliable formal software verification. It is a common practice to\nemploy human-annotated chain-of-thought and other human priors to induce the\nreasoning and coding capabilities of LLMs. Unfortunately, it becomes\nunacceptably all-consuming to provide such priors for supervising complex\nprogramming tasks. In this work, we systematically explore ways to reduce human\npriors with the formal language, Dafny, as the main environment for our pilot\nstudy. Our pipeline mainly relies on introducing an automatic and scalable data\ncuration pipeline, and careful RL designs integrated with feedback from the\nformal language verifier. We introduce DafnyComp, a benchmark of compositional\nformal programs with auto-formalized specifications for specification\nreasoning. Our supervised fine-tuning (SFT) stage enables even small models\n(e.g., 0.5B) to generate syntactically valid and verifiable Dafny code,\nsurpassing proprietary models. RL with regularization further improves\nperformance, achieving stronger generalization to out-of-domain tasks and\noutperforming all strong baselines on the challenging DafnyComp benchmark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16331.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "641a6895fb5ffff5ac79d593",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641a6895fb5ffff5ac79d593/vxvwsto3llOEWGqQKGMYx.jpeg",
            "fullname": "Jie Fu",
            "name": "bigaidream",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.17745",
            "authors": [
                {
                    "_id": "6881b243df7c5aafaf37f0c4",
                    "name": "Yiwen Chen",
                    "hidden": false
                },
                {
                    "_id": "6881b243df7c5aafaf37f0c5",
                    "name": "Zhihao Li",
                    "hidden": false
                },
                {
                    "_id": "6881b243df7c5aafaf37f0c6",
                    "name": "Yikai Wang",
                    "hidden": false
                },
                {
                    "_id": "6881b243df7c5aafaf37f0c7",
                    "name": "Hu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6881b243df7c5aafaf37f0c8",
                    "name": "Qin Li",
                    "hidden": false
                },
                {
                    "_id": "6881b243df7c5aafaf37f0c9",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6881b243df7c5aafaf37f0ca",
                    "name": "Guosheng Lin",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64e45b1c2fbf570ac70dc6e8/pEcsL-6nuzyxL0kQxvdFJ.png"
            ],
            "publishedAt": "2025-07-23T17:57:16.000Z",
            "submittedOnDailyAt": "2025-07-24T02:42:32.620Z",
            "title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention",
            "submittedOnDailyBy": {
                "_id": "64e45b1c2fbf570ac70dc6e8",
                "avatarUrl": "/avatars/8dd65f75f2c1b92abf2f189f6d8466a2.svg",
                "isPro": true,
                "fullname": "Chen Yiwen",
                "user": "Yiwen-ntu",
                "type": "user"
            },
            "summary": "Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.",
            "upvotes": 11,
            "discussionId": "6881b243df7c5aafaf37f0cb",
            "ai_summary": "Ultra3D uses VecSet and Part Attention to accelerate 3D voxel generation while maintaining high quality and resolution.",
            "ai_keywords": [
                "sparse voxel representations",
                "diffusion pipelines",
                "VecSet",
                "Part Attention",
                "geometry-aware localized attention",
                "semantic consistency",
                "part-labeled sparse voxels"
            ]
        },
        "publishedAt": "2025-07-23T13:57:16.000Z",
        "title": "Ultra3D: Efficient and High-Fidelity 3D Generation with Part Attention",
        "summary": "Recent advances in sparse voxel representations have significantly improved\nthe quality of 3D content generation, enabling high-resolution modeling with\nfine-grained geometry. However, existing frameworks suffer from severe\ncomputational inefficiencies due to the quadratic complexity of attention\nmechanisms in their two-stage diffusion pipelines. In this work, we propose\nUltra3D, an efficient 3D generation framework that significantly accelerates\nsparse voxel modeling without compromising quality. Our method leverages the\ncompact VecSet representation to efficiently generate a coarse object layout in\nthe first stage, reducing token count and accelerating voxel coordinate\nprediction. To refine per-voxel latent features in the second stage, we\nintroduce Part Attention, a geometry-aware localized attention mechanism that\nrestricts attention computation within semantically consistent part regions.\nThis design preserves structural continuity while avoiding unnecessary global\nattention, achieving up to 6.7x speed-up in latent generation. To support this\nmechanism, we construct a scalable part annotation pipeline that converts raw\nmeshes into part-labeled sparse voxels. Extensive experiments demonstrate that\nUltra3D supports high-resolution 3D generation at 1024 resolution and achieves\nstate-of-the-art performance in both visual fidelity and user preference.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64e45b1c2fbf570ac70dc6e8/pEcsL-6nuzyxL0kQxvdFJ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17745.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64e45b1c2fbf570ac70dc6e8",
            "avatarUrl": "/avatars/8dd65f75f2c1b92abf2f189f6d8466a2.svg",
            "fullname": "Chen Yiwen",
            "name": "Yiwen-ntu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 23
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.11465",
            "authors": [
                {
                    "_id": "6881b78bdf7c5aafaf37f0cd",
                    "user": {
                        "_id": "63106aac95c34b954078ef67",
                        "avatarUrl": "/avatars/0e627c74d301d65f3b364e8229dd78ab.svg",
                        "isPro": false,
                        "fullname": "Nuri Ryu",
                        "user": "terryryu",
                        "type": "user"
                    },
                    "name": "Nuri Ryu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:07:05.498Z",
                    "hidden": false
                },
                {
                    "_id": "6881b78bdf7c5aafaf37f0ce",
                    "name": "Jiyun Won",
                    "hidden": false
                },
                {
                    "_id": "6881b78bdf7c5aafaf37f0cf",
                    "name": "Jooeun Son",
                    "hidden": false
                },
                {
                    "_id": "6881b78bdf7c5aafaf37f0d0",
                    "user": {
                        "_id": "6412f01b251f9368f690a536",
                        "avatarUrl": "/avatars/32addb8b2d4cf5babf7ca2284497c847.svg",
                        "isPro": false,
                        "fullname": "Minsu Gong",
                        "user": "gongms",
                        "type": "user"
                    },
                    "name": "Minsu Gong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:07:21.378Z",
                    "hidden": false
                },
                {
                    "_id": "6881b78bdf7c5aafaf37f0d1",
                    "user": {
                        "_id": "62d562635c29ac61fec7ae38",
                        "avatarUrl": "/avatars/816ec718442fa541a21a02b29070e08a.svg",
                        "isPro": false,
                        "fullname": "Joo-Haeng Lee",
                        "user": "joohaeng",
                        "type": "user"
                    },
                    "name": "Joo-Haeng Lee",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:07:26.786Z",
                    "hidden": false
                },
                {
                    "_id": "6881b78bdf7c5aafaf37f0d2",
                    "user": {
                        "_id": "675d3d94036615f450fe818f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Oc4-UMIHyVe0naZ0XGQmj.png",
                        "isPro": false,
                        "fullname": "sunghyun cho",
                        "user": "chosh1110",
                        "type": "user"
                    },
                    "name": "Sunghyun Cho",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:07:33.499Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-15T16:36:20.000Z",
            "submittedOnDailyAt": "2025-07-24T03:03:53.801Z",
            "title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a\n  Low-Quality Model",
            "submittedOnDailyBy": {
                "_id": "63106aac95c34b954078ef67",
                "avatarUrl": "/avatars/0e627c74d301d65f3b364e8229dd78ab.svg",
                "isPro": false,
                "fullname": "Nuri Ryu",
                "user": "terryryu",
                "type": "user"
            },
            "summary": "High-quality 3D assets are essential for various applications in computer\ngraphics and 3D vision but remain scarce due to significant acquisition costs.\nTo address this shortage, we introduce Elevate3D, a novel framework that\ntransforms readily accessible low-quality 3D assets into higher quality. At the\ncore of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that\nsignificantly improves texture quality while preserving the appearance and\ngeometry while fixing its degradations. Furthermore, Elevate3D operates in a\nview-by-view manner, alternating between texture and geometry refinement.\nUnlike previous methods that have largely overlooked geometry refinement, our\nframework leverages geometric cues from images refined with HFS-SDEdit by\nemploying state-of-the-art monocular geometry predictors. This approach ensures\ndetailed and accurate geometry that aligns seamlessly with the enhanced\ntexture. Elevate3D outperforms recent competitors by achieving state-of-the-art\nquality in 3D model refinement, effectively addressing the scarcity of\nhigh-quality open-source 3D assets.",
            "upvotes": 6,
            "discussionId": "6881b78cdf7c5aafaf37f0d3",
            "projectPage": "https://cg.postech.ac.kr/research/Elevate3D/",
            "githubRepo": "https://github.com/ryunuri/Elevate3D",
            "ai_summary": "Elevate3D enhances both texture and geometry of low-quality 3D assets using HFS-SDEdit and monocular geometry predictors, achieving superior refinement quality.",
            "ai_keywords": [
                "HFS-SDEdit",
                "monocular geometry predictors"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-07-15T12:36:20.000Z",
        "title": "Elevating 3D Models: High-Quality Texture and Geometry Refinement from a\n  Low-Quality Model",
        "summary": "High-quality 3D assets are essential for various applications in computer\ngraphics and 3D vision but remain scarce due to significant acquisition costs.\nTo address this shortage, we introduce Elevate3D, a novel framework that\ntransforms readily accessible low-quality 3D assets into higher quality. At the\ncore of Elevate3D is HFS-SDEdit, a specialized texture enhancement method that\nsignificantly improves texture quality while preserving the appearance and\ngeometry while fixing its degradations. Furthermore, Elevate3D operates in a\nview-by-view manner, alternating between texture and geometry refinement.\nUnlike previous methods that have largely overlooked geometry refinement, our\nframework leverages geometric cues from images refined with HFS-SDEdit by\nemploying state-of-the-art monocular geometry predictors. This approach ensures\ndetailed and accurate geometry that aligns seamlessly with the enhanced\ntexture. Elevate3D outperforms recent competitors by achieving state-of-the-art\nquality in 3D model refinement, effectively addressing the scarcity of\nhigh-quality open-source 3D assets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.11465.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63106aac95c34b954078ef67",
            "avatarUrl": "/avatars/0e627c74d301d65f3b364e8229dd78ab.svg",
            "fullname": "Nuri Ryu",
            "name": "terryryu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.16880",
            "authors": [
                {
                    "_id": "6881d12ddf7c5aafaf37f109",
                    "user": {
                        "_id": "62f6a0b0ffd6a0853ee51b5c",
                        "avatarUrl": "/avatars/5e0dbe8e122b3106b7799e226cb9a9c7.svg",
                        "isPro": false,
                        "fullname": "Antoni Kowalczuk",
                        "user": "antoniaaa",
                        "type": "user"
                    },
                    "name": "Antoni Kowalczuk",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:07:46.084Z",
                    "hidden": false
                },
                {
                    "_id": "6881d12ddf7c5aafaf37f10a",
                    "name": "Dominik Hintersdorf",
                    "hidden": false
                },
                {
                    "_id": "6881d12ddf7c5aafaf37f10b",
                    "user": {
                        "_id": "6305ca82d37ce67e0e48aadb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6305ca82d37ce67e0e48aadb/gyTcyIURKmXSFXtr07FGu.jpeg",
                        "isPro": false,
                        "fullname": "Lukas Struppek",
                        "user": "lukas-struppek",
                        "type": "user"
                    },
                    "name": "Lukas Struppek",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-24T12:07:55.382Z",
                    "hidden": false
                },
                {
                    "_id": "6881d12ddf7c5aafaf37f10c",
                    "name": "Kristian Kersting",
                    "hidden": false
                },
                {
                    "_id": "6881d12ddf7c5aafaf37f10d",
                    "name": "Adam Dziedzic",
                    "hidden": false
                },
                {
                    "_id": "6881d12ddf7c5aafaf37f10e",
                    "name": "Franziska Boenisch",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-22T15:02:38.000Z",
            "submittedOnDailyAt": "2025-07-24T07:26:37.511Z",
            "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less\n  Local Than Assumed",
            "submittedOnDailyBy": {
                "_id": "6305ca82d37ce67e0e48aadb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6305ca82d37ce67e0e48aadb/gyTcyIURKmXSFXtr07FGu.jpeg",
                "isPro": false,
                "fullname": "Lukas Struppek",
                "user": "lukas-struppek",
                "type": "user"
            },
            "summary": "Text-to-image diffusion models (DMs) have achieved remarkable success in\nimage generation. However, concerns about data privacy and intellectual\nproperty remain due to their potential to inadvertently memorize and replicate\ntraining data. Recent mitigation efforts have focused on identifying and\npruning weights responsible for triggering replication, based on the assumption\nthat memorization can be localized. Our research assesses the robustness of\nthese pruning-based approaches. We demonstrate that even after pruning, minor\nadjustments to text embeddings of input prompts are sufficient to re-trigger\ndata replication, highlighting the fragility of these defenses. Furthermore, we\nchallenge the fundamental assumption of memorization locality, by showing that\nreplication can be triggered from diverse locations within the text embedding\nspace, and follows different paths in the model. Our findings indicate that\nexisting mitigation strategies are insufficient and underscore the need for\nmethods that truly remove memorized content, rather than attempting to suppress\nits retrieval. As a first step in this direction, we introduce a novel\nadversarial fine-tuning method that iteratively searches for replication\ntriggers and updates the model to increase robustness. Through our research, we\nprovide fresh insights into the nature of memorization in text-to-image DMs and\na foundation for building more trustworthy and compliant generative AI.",
            "upvotes": 3,
            "discussionId": "6881d12ddf7c5aafaf37f10f",
            "ai_summary": "Pruning-based defenses in text-to-image diffusion models are ineffective as minor adjustments to text embeddings can re-trigger data replication, necessitating methods that truly remove memorized content.",
            "ai_keywords": [
                "text-to-image diffusion models",
                "DMs",
                "data privacy",
                "intellectual property",
                "pruning",
                "text embeddings",
                "memorization locality",
                "adversarial fine-tuning"
            ]
        },
        "publishedAt": "2025-07-22T11:02:38.000Z",
        "title": "Finding Dori: Memorization in Text-to-Image Diffusion Models Is Less\n  Local Than Assumed",
        "summary": "Text-to-image diffusion models (DMs) have achieved remarkable success in\nimage generation. However, concerns about data privacy and intellectual\nproperty remain due to their potential to inadvertently memorize and replicate\ntraining data. Recent mitigation efforts have focused on identifying and\npruning weights responsible for triggering replication, based on the assumption\nthat memorization can be localized. Our research assesses the robustness of\nthese pruning-based approaches. We demonstrate that even after pruning, minor\nadjustments to text embeddings of input prompts are sufficient to re-trigger\ndata replication, highlighting the fragility of these defenses. Furthermore, we\nchallenge the fundamental assumption of memorization locality, by showing that\nreplication can be triggered from diverse locations within the text embedding\nspace, and follows different paths in the model. Our findings indicate that\nexisting mitigation strategies are insufficient and underscore the need for\nmethods that truly remove memorized content, rather than attempting to suppress\nits retrieval. As a first step in this direction, we introduce a novel\nadversarial fine-tuning method that iteratively searches for replication\ntriggers and updates the model to increase robustness. Through our research, we\nprovide fresh insights into the nature of memorization in text-to-image DMs and\na foundation for building more trustworthy and compliant generative AI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16880.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6305ca82d37ce67e0e48aadb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6305ca82d37ce67e0e48aadb/gyTcyIURKmXSFXtr07FGu.jpeg",
            "fullname": "Lukas Struppek",
            "name": "lukas-struppek",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.16116",
            "authors": [
                {
                    "_id": "688035ff00f4b5a05f2fbd9b",
                    "name": "Yaofang Liu",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbd9c",
                    "name": "Yumeng Ren",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbd9d",
                    "name": "Aitor Artola",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbd9e",
                    "name": "Yuxuan Hu",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbd9f",
                    "name": "Xiaodong Cun",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbda0",
                    "name": "Xiaotong Zhao",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbda1",
                    "name": "Alan Zhao",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbda2",
                    "name": "Raymond H. Chan",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbda3",
                    "name": "Suiyun Zhang",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbda4",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbda5",
                    "name": "Dandan Tu",
                    "hidden": false
                },
                {
                    "_id": "688035ff00f4b5a05f2fbda6",
                    "name": "Jean-Michel Morel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-22T00:09:37.000Z",
            "submittedOnDailyAt": "2025-07-24T13:16:35.391Z",
            "title": "PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized\n  Timestep Adaptation",
            "submittedOnDailyBy": {
                "_id": "658d46649a1397992a4d657f",
                "avatarUrl": "/avatars/66d23095b8d0368ea862778dbf123a67.svg",
                "isPro": false,
                "fullname": "Raphael Liu",
                "user": "RaphaelLiu",
                "type": "user"
            },
            "summary": "The rapid advancement of video diffusion models has been hindered by\nfundamental limitations in temporal modeling, particularly the rigid\nsynchronization of frame evolution imposed by conventional scalar timestep\nvariables. While task-specific adaptations and autoregressive models have\nsought to address these challenges, they remain constrained by computational\ninefficiency, catastrophic forgetting, or narrow applicability. In this work,\nwe present Pusa, a groundbreaking paradigm that leverages vectorized timestep\nadaptation (VTA) to enable fine-grained temporal control within a unified video\ndiffusion framework. Besides, VTA is a non-destructive adaptation, which means\nit fully preserves the capabilities of the base model. By finetuning the SOTA\nWan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --\nsurpassing the performance of Wan-I2V-14B with leq 1/200 of the training\ncost (\\500 vs. \\geq 100,000) and leq 1/2500 of the dataset size (4K vs.\ngeq 10M samples). Pusa not only sets a new standard for image-to-video (I2V)\ngeneration, achieving a VBench-I2V total score of 87.32\\% (vs. 86.86\\% of\nWan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as\nstart-end frames and video extension -- all without task-specific training.\nMeanwhile, Pusa can still perform text-to-video generation. Mechanistic\nanalyses reveal that our approach preserves the foundation model's generative\npriors while surgically injecting temporal dynamics, avoiding the combinatorial\nexplosion inherent to vectorized timesteps. This work establishes a scalable,\nefficient, and versatile paradigm for next-generation video synthesis,\ndemocratizing high-fidelity video generation for research and industry alike.\nCode is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen",
            "upvotes": 2,
            "discussionId": "688035ff00f4b5a05f2fbda7",
            "projectPage": "https://yaofang-liu.github.io/Pusa_Web/",
            "githubRepo": "https://github.com/Yaofang-Liu/Pusa-VidGen",
            "ai_summary": "Pusa, a vectorized timestep adaptation approach, enhances video diffusion models for efficient and versatile video generation, improving performance and reducing costs.",
            "ai_keywords": [
                "video diffusion models",
                "temporal modeling",
                "scalar timestep variables",
                "autoregressive models",
                "vectorized timestep adaptation",
                "fine-grained temporal control",
                "Wan2.1-T2V-14B",
                "Wan-I2V-14B",
                "I2V generation",
                "VBench-I2V",
                "zero-shot multi-task capabilities",
                "start-end frames",
                "video extension",
                "text-to-video generation",
                "generative priors",
                "temporal dynamics"
            ],
            "githubStars": 508
        },
        "publishedAt": "2025-07-21T20:09:37.000Z",
        "title": "PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized\n  Timestep Adaptation",
        "summary": "The rapid advancement of video diffusion models has been hindered by\nfundamental limitations in temporal modeling, particularly the rigid\nsynchronization of frame evolution imposed by conventional scalar timestep\nvariables. While task-specific adaptations and autoregressive models have\nsought to address these challenges, they remain constrained by computational\ninefficiency, catastrophic forgetting, or narrow applicability. In this work,\nwe present Pusa, a groundbreaking paradigm that leverages vectorized timestep\nadaptation (VTA) to enable fine-grained temporal control within a unified video\ndiffusion framework. Besides, VTA is a non-destructive adaptation, which means\nit fully preserves the capabilities of the base model. By finetuning the SOTA\nWan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency --\nsurpassing the performance of Wan-I2V-14B with leq 1/200 of the training\ncost (\\500 vs. \\geq 100,000) and leq 1/2500 of the dataset size (4K vs.\ngeq 10M samples). Pusa not only sets a new standard for image-to-video (I2V)\ngeneration, achieving a VBench-I2V total score of 87.32\\% (vs. 86.86\\% of\nWan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as\nstart-end frames and video extension -- all without task-specific training.\nMeanwhile, Pusa can still perform text-to-video generation. Mechanistic\nanalyses reveal that our approach preserves the foundation model's generative\npriors while surgically injecting temporal dynamics, avoiding the combinatorial\nexplosion inherent to vectorized timesteps. This work establishes a scalable,\nefficient, and versatile paradigm for next-generation video synthesis,\ndemocratizing high-fidelity video generation for research and industry alike.\nCode is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16116.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "658d46649a1397992a4d657f",
            "avatarUrl": "/avatars/66d23095b8d0368ea862778dbf123a67.svg",
            "fullname": "Raphael Liu",
            "name": "RaphaelLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 28
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.14241",
            "authors": [
                {
                    "_id": "6882978a6a54dd1e77daa9c2",
                    "name": "Rithesh Murthy",
                    "hidden": false
                },
                {
                    "_id": "6882978a6a54dd1e77daa9c3",
                    "name": "Ming Zhu",
                    "hidden": false
                },
                {
                    "_id": "6882978a6a54dd1e77daa9c4",
                    "name": "Liangwei Yang",
                    "hidden": false
                },
                {
                    "_id": "6882978a6a54dd1e77daa9c5",
                    "name": "Jielin Qiu",
                    "hidden": false
                },
                {
                    "_id": "6882978a6a54dd1e77daa9c6",
                    "name": "Juntao Tan",
                    "hidden": false
                },
                {
                    "_id": "6882978a6a54dd1e77daa9c7",
                    "name": "Shelby Heinecke",
                    "hidden": false
                },
                {
                    "_id": "6882978a6a54dd1e77daa9c8",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "6882978a6a54dd1e77daa9c9",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "6882978a6a54dd1e77daa9ca",
                    "name": "Huan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-17T18:18:20.000Z",
            "submittedOnDailyAt": "2025-07-24T18:59:59.029Z",
            "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "60381555c58b41eeda0d6ccf",
                "avatarUrl": "/avatars/2c2c4b75dd0199478dc0fbc72f9bcadc.svg",
                "isPro": false,
                "fullname": "rmurthy",
                "user": "rmurthy",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.",
            "upvotes": 1,
            "discussionId": "6882978a6a54dd1e77daa9cb",
            "githubRepo": "https://github.com/SalesforceAIResearch/promptomatix",
            "githubStars": 14
        },
        "publishedAt": "2025-07-17T14:18:20.000Z",
        "title": "Promptomatix: An Automatic Prompt Optimization Framework for Large\n  Language Models",
        "summary": "Large Language Models (LLMs) perform best with well-crafted prompts, yet\nprompt engineering remains manual, inconsistent, and inaccessible to\nnon-experts. We introduce Promptomatix, an automatic prompt optimization\nframework that transforms natural language task descriptions into high-quality\nprompts without requiring manual tuning or domain expertise. Promptomatix\nsupports both a lightweight meta-prompt-based optimizer and a DSPy-powered\ncompiler, with modular design enabling future extension to more advanced\nframeworks. The system analyzes user intent, generates synthetic training data,\nselects prompting strategies, and refines prompts using cost-aware objectives.\nEvaluated across 5 task categories, Promptomatix achieves competitive or\nsuperior performance compared to existing libraries, while reducing prompt\nlength and computational overhead making prompt optimization scalable and\nefficient.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14241.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60381555c58b41eeda0d6ccf",
            "avatarUrl": "/avatars/2c2c4b75dd0199478dc0fbc72f9bcadc.svg",
            "fullname": "rmurthy",
            "name": "rmurthy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]