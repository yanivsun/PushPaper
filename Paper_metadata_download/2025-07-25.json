[
    {
        "paper": {
            "id": "2507.13546",
            "authors": [
                {
                    "_id": "6883427a846d2e78b7548bce",
                    "name": "Dmitrii Mikhailov",
                    "hidden": false
                },
                {
                    "_id": "6883427a846d2e78b7548bcf",
                    "name": "Aleksey Letunovskiy",
                    "hidden": false
                },
                {
                    "_id": "6883427a846d2e78b7548bd0",
                    "name": "Maria Kovaleva",
                    "hidden": false
                },
                {
                    "_id": "6883427a846d2e78b7548bd1",
                    "name": "Vladimir Arkhipkin",
                    "hidden": false
                },
                {
                    "_id": "6883427a846d2e78b7548bd2",
                    "user": {
                        "_id": "67bcb1012906865678a11f91",
                        "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg",
                        "isPro": false,
                        "fullname": "Vladimir Korviakov",
                        "user": "korviakov",
                        "type": "user"
                    },
                    "name": "Vladimir Korviakov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T13:36:18.497Z",
                    "hidden": false
                },
                {
                    "_id": "6883427a846d2e78b7548bd3",
                    "name": "Vladimir Polovnikov",
                    "hidden": false
                },
                {
                    "_id": "6883427a846d2e78b7548bd4",
                    "name": "Viacheslav Vasilev",
                    "hidden": false
                },
                {
                    "_id": "6883427a846d2e78b7548bd5",
                    "name": "Evelina Sidorova",
                    "hidden": false
                },
                {
                    "_id": "6883427a846d2e78b7548bd6",
                    "user": {
                        "_id": "6669a678465d1d802181e456",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6669a678465d1d802181e456/ZCthBBhDFQnh0bBkgUQUU.png",
                        "isPro": false,
                        "fullname": "Denis Dimitrov",
                        "user": "dendimitrov",
                        "type": "user"
                    },
                    "name": "Denis Dimitrov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T13:36:20.344Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-17T21:36:36.000Z",
            "submittedOnDailyAt": "2025-07-25T12:40:28.103Z",
            "title": "nablaNABLA: Neighborhood Adaptive Block-Level Attention",
            "submittedOnDailyBy": {
                "_id": "67bcb1012906865678a11f91",
                "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg",
                "isPro": false,
                "fullname": "Vladimir Korviakov",
                "user": "korviakov",
                "type": "user"
            },
            "summary": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA",
            "upvotes": 81,
            "discussionId": "6883427a846d2e78b7548bd7",
            "githubRepo": "https://github.com/gen-ai-team/Wan2.1-NABLA",
            "ai_summary": "NABLA, a dynamic block-level attention mechanism, improves video diffusion transformers by enhancing computational efficiency without sacrificing generative quality.",
            "ai_keywords": [
                "transformer-based architectures",
                "full attention mechanisms",
                "quadratic complexity",
                "Neighborhood Adaptive Block-Level Attention",
                "NABLA",
                "sparsity patterns",
                "video diffusion transformers",
                "DiTs",
                "block-wise attention",
                "adaptive sparsity-driven threshold",
                "CLIP score",
                "VBench score",
                "human evaluation score"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-07-17T17:36:36.000Z",
        "title": "nablaNABLA: Neighborhood Adaptive Block-Level Attention",
        "summary": "Recent progress in transformer-based architectures has demonstrated\nremarkable success in video generation tasks. However, the quadratic complexity\nof full attention mechanisms remains a critical bottleneck, particularly for\nhigh-resolution and long-duration video sequences. In this paper, we propose\nNABLA, a novel Neighborhood Adaptive Block-Level Attention mechanism that\ndynamically adapts to sparsity patterns in video diffusion transformers (DiTs).\nBy leveraging block-wise attention with adaptive sparsity-driven threshold,\nNABLA reduces computational overhead while preserving generative quality. Our\nmethod does not require custom low-level operator design and can be seamlessly\nintegrated with PyTorch's Flex Attention operator. Experiments demonstrate that\nNABLA achieves up to 2.7x faster training and inference compared to baseline\nalmost without compromising quantitative metrics (CLIP score, VBench score,\nhuman evaluation score) and visual quality drop. The code and model weights are\navailable here: https://github.com/gen-ai-team/Wan2.1-NABLA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.13546.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "67bcb1012906865678a11f91",
            "avatarUrl": "/avatars/80fb0cc24f0d16c4740f9115b680df0f.svg",
            "fullname": "Vladimir Korviakov",
            "name": "korviakov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.18071",
            "authors": [
                {
                    "_id": "68831fe6846d2e78b7548ac8",
                    "user": {
                        "_id": "610b70452719facd4ea85e28",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                        "isPro": false,
                        "fullname": "Chujie Zheng",
                        "user": "chujiezheng",
                        "type": "user"
                    },
                    "name": "Chujie Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T13:36:28.634Z",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548ac9",
                    "name": "Shixuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548aca",
                    "name": "Mingze Li",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548acb",
                    "user": {
                        "_id": "63f30b870a16587ea970edfe",
                        "avatarUrl": "/avatars/059491b33fecec69032e6d481229ee31.svg",
                        "isPro": false,
                        "fullname": "Xiong-Hui Chen",
                        "user": "xionghuichen",
                        "type": "user"
                    },
                    "name": "Xiong-Hui Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T13:36:26.841Z",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548acc",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548acd",
                    "name": "Chang Gao",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548ace",
                    "name": "Kai Dang",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548acf",
                    "name": "Yuqiong Liu",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548ad0",
                    "name": "Rui Men",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548ad1",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548ad2",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "68831fe6846d2e78b7548ad3",
                    "name": "Junyang Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T03:50:32.000Z",
            "submittedOnDailyAt": "2025-07-25T08:54:57.629Z",
            "title": "Group Sequence Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "610b70452719facd4ea85e28",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
                "isPro": false,
                "fullname": "Chujie Zheng",
                "user": "chujiezheng",
                "type": "user"
            },
            "summary": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable,\nefficient, and performant reinforcement learning algorithm for training large\nlanguage models. Unlike previous algorithms that adopt token-level importance\nratios, GSPO defines the importance ratio based on sequence likelihood and\nperforms sequence-level clipping, rewarding, and optimization. We demonstrate\nthat GSPO achieves superior training efficiency and performance compared to the\nGRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and\nhas the potential for simplifying the design of RL infrastructure. These merits\nof GSPO have contributed to the remarkable improvements in the latest Qwen3\nmodels.",
            "upvotes": 55,
            "discussionId": "68831fe7846d2e78b7548ad4"
        },
        "publishedAt": "2025-07-23T23:50:32.000Z",
        "title": "Group Sequence Policy Optimization",
        "summary": "This paper introduces Group Sequence Policy Optimization (GSPO), our stable,\nefficient, and performant reinforcement learning algorithm for training large\nlanguage models. Unlike previous algorithms that adopt token-level importance\nratios, GSPO defines the importance ratio based on sequence likelihood and\nperforms sequence-level clipping, rewarding, and optimization. We demonstrate\nthat GSPO achieves superior training efficiency and performance compared to the\nGRPO algorithm, notably stabilizes Mixture-of-Experts (MoE) RL training, and\nhas the potential for simplifying the design of RL infrastructure. These merits\nof GSPO have contributed to the remarkable improvements in the latest Qwen3\nmodels.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18071.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "610b70452719facd4ea85e28",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/610b70452719facd4ea85e28/S7nMy7D0Rxq0VIVblhYDG.jpeg",
            "fullname": "Chujie Zheng",
            "name": "chujiezheng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 48
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.14958",
            "authors": [
                {
                    "_id": "688332d5846d2e78b7548b6f",
                    "name": "Hang Yan",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b70",
                    "name": "Fangzhi Xu",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b71",
                    "name": "Rongman Xu",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b72",
                    "name": "Yifei Li",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b73",
                    "name": "Jian Zhang",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b74",
                    "name": "Haoran Luo",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b75",
                    "name": "Xiaobao Wu",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b76",
                    "name": "Luu Anh Tuan",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b77",
                    "name": "Haiteng Zhao",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b78",
                    "name": "Qika Lin",
                    "hidden": false
                },
                {
                    "_id": "688332d5846d2e78b7548b79",
                    "name": "Jun Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-20T13:36:19.000Z",
            "submittedOnDailyAt": "2025-07-25T06:03:00.614Z",
            "title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64e6cf78ecce34cb442dc889",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
                "isPro": false,
                "fullname": "Fangzhi Xu",
                "user": "xufangzhi",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have achieved impressive performance on\nreasoning-intensive tasks, yet optimizing their reasoning efficiency remains an\nopen challenge. While Test-Time Scaling (TTS) improves reasoning quality, it\noften leads to overthinking, wasting tokens on redundant computations. This\nwork investigates how to efficiently and adaptively guide LLM test-time scaling\nwithout additional training. Inspired by the concept of momentum in physics, we\npropose Momentum Uncertainty-guided Reasoning (MUR), which dynamically\nallocates thinking budgets to critical reasoning steps by tracking and\naggregating stepwise uncertainty over time. To support flexible inference-time\ncontrol, we introduce gamma-control, a simple mechanism that tunes the\nreasoning budget via a single hyperparameter. We provide in-depth theoretical\nproof to support the superiority of MUR in terms of stability and biases. MUR\nis comprehensively evaluated against various TTS methods across four\nchallenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using\ndifferent sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate\nthat MUR reduces computation by over 50% on average while improving accuracy by\n0.62-3.37%.",
            "upvotes": 31,
            "discussionId": "688332d6846d2e78b7548b7a",
            "projectPage": "https://github.com/yayayacc/MUR",
            "githubRepo": "https://github.com/yayayacc/MUR",
            "ai_summary": "Momentum Uncertainty-guided Reasoning (MUR) dynamically optimizes reasoning budgets in Large Language Models during inference, reducing computation and enhancing accuracy.",
            "ai_keywords": [
                "Large Language Models",
                "Test-Time Scaling",
                "Momentum Uncertainty-guided Reasoning",
                "reasoning budgets",
                "stepwise uncertainty",
                "gamma-control",
                "MATH-500",
                "AIME24",
                "AIME25",
                "GPQA-diamond",
                "Qwen3"
            ],
            "githubStars": 26
        },
        "publishedAt": "2025-07-20T09:36:19.000Z",
        "title": "MUR: Momentum Uncertainty guided Reasoning for Large Language Models",
        "summary": "Large Language Models (LLMs) have achieved impressive performance on\nreasoning-intensive tasks, yet optimizing their reasoning efficiency remains an\nopen challenge. While Test-Time Scaling (TTS) improves reasoning quality, it\noften leads to overthinking, wasting tokens on redundant computations. This\nwork investigates how to efficiently and adaptively guide LLM test-time scaling\nwithout additional training. Inspired by the concept of momentum in physics, we\npropose Momentum Uncertainty-guided Reasoning (MUR), which dynamically\nallocates thinking budgets to critical reasoning steps by tracking and\naggregating stepwise uncertainty over time. To support flexible inference-time\ncontrol, we introduce gamma-control, a simple mechanism that tunes the\nreasoning budget via a single hyperparameter. We provide in-depth theoretical\nproof to support the superiority of MUR in terms of stability and biases. MUR\nis comprehensively evaluated against various TTS methods across four\nchallenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using\ndifferent sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate\nthat MUR reduces computation by over 50% on average while improving accuracy by\n0.62-3.37%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14958.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64e6cf78ecce34cb442dc889",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e6cf78ecce34cb442dc889/qVZFiUEpBpSkmH8SQeinm.jpeg",
            "fullname": "Fangzhi Xu",
            "name": "xufangzhi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.15758",
            "authors": [
                {
                    "_id": "687ef94833947f780d9b4ac7",
                    "user": {
                        "_id": "674d103d2d93658f850151a6",
                        "avatarUrl": "/avatars/e221d43d3d8601d94cb6a44610054e23.svg",
                        "isPro": false,
                        "fullname": "wuxingyu",
                        "user": "wuxingyu",
                        "type": "user"
                    },
                    "name": "Xingyu Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T13:36:33.856Z",
                    "hidden": false
                },
                {
                    "_id": "687ef94833947f780d9b4ac8",
                    "user": {
                        "_id": "64098738342c26884c792c93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                        "isPro": false,
                        "fullname": "Yuchen Yan",
                        "user": "yanyc",
                        "type": "user"
                    },
                    "name": "Yuchen Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-22T07:49:07.023Z",
                    "hidden": false
                },
                {
                    "_id": "687ef94833947f780d9b4ac9",
                    "name": "Shangke Lyu",
                    "hidden": false
                },
                {
                    "_id": "687ef94833947f780d9b4aca",
                    "name": "Linjuan Wu",
                    "hidden": false
                },
                {
                    "_id": "687ef94833947f780d9b4acb",
                    "name": "Yiwen Qiu",
                    "hidden": false
                },
                {
                    "_id": "687ef94833947f780d9b4acc",
                    "user": {
                        "_id": "5e1058e9fcf41d740b69966d",
                        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                        "isPro": false,
                        "fullname": "Yongliang Shen",
                        "user": "tricktreat",
                        "type": "user"
                    },
                    "name": "Yongliang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-22T07:49:05.190Z",
                    "hidden": false
                },
                {
                    "_id": "687ef94833947f780d9b4acd",
                    "name": "Weiming Lu",
                    "hidden": false
                },
                {
                    "_id": "687ef94833947f780d9b4ace",
                    "name": "Jian Shao",
                    "hidden": false
                },
                {
                    "_id": "687ef94833947f780d9b4acf",
                    "name": "Jun Xiao",
                    "hidden": false
                },
                {
                    "_id": "687ef94833947f780d9b4ad0",
                    "name": "Yueting Zhuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-21T16:14:41.000Z",
            "submittedOnDailyAt": "2025-07-25T02:07:25.946Z",
            "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization",
            "submittedOnDailyBy": {
                "_id": "5e1058e9fcf41d740b69966d",
                "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                "isPro": false,
                "fullname": "Yongliang Shen",
                "user": "tricktreat",
                "type": "user"
            },
            "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.",
            "upvotes": 25,
            "discussionId": "687ef94833947f780d9b4ad1",
            "githubRepo": "https://github.com/ZJU-REAL/HBPO",
            "githubStars": 14
        },
        "publishedAt": "2025-07-21T12:14:41.000Z",
        "title": "LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy\n  Optimization",
        "summary": "Large reasoning models have achieved remarkable performance through extended\nchain-of-thought sequences, yet this computational freedom leads to excessive\ntoken generation even for simple problems. We present Length-Adaptive Policy\nOptimization (LAPO), a novel framework that transforms reasoning length control\nfrom an external constraint into an intrinsic model capability. Unlike existing\napproaches that impose rigid limits or rely on post-hoc interventions, LAPO\nenables models to internalize an understanding of appropriate reasoning depth\nthrough a two-stage reinforcement learning process. In the first stage, models\nlearn natural reasoning patterns by discovering the statistical distribution of\nsuccessful solution lengths. The second stage leverages these patterns as\nmeta-cognitive guidance, embedding them directly within the model's reasoning\ncontext to ensure inference-time flexibility. Experiments on mathematical\nreasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\\%\nwhile improving accuracy by 2.3\\%. Our analysis reveals that models trained\nwith LAPO develop emergent abilities to allocate computational resources based\non problem complexity, achieving efficient reasoning without sacrificing\nquality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15758.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "fullname": "Yongliang Shen",
            "name": "tricktreat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 25
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.18634",
            "authors": [
                {
                    "_id": "6882f472846d2e78b7548a58",
                    "name": "Junfei Xiao",
                    "hidden": false
                },
                {
                    "_id": "6882f472846d2e78b7548a59",
                    "name": "Ceyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "6882f472846d2e78b7548a5a",
                    "name": "Lvmin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6882f472846d2e78b7548a5b",
                    "name": "Shengqu Cai",
                    "hidden": false
                },
                {
                    "_id": "6882f472846d2e78b7548a5c",
                    "name": "Yang Zhao",
                    "hidden": false
                },
                {
                    "_id": "6882f472846d2e78b7548a5d",
                    "name": "Yuwei Guo",
                    "hidden": false
                },
                {
                    "_id": "6882f472846d2e78b7548a5e",
                    "name": "Gordon Wetzstein",
                    "hidden": false
                },
                {
                    "_id": "6882f472846d2e78b7548a5f",
                    "name": "Maneesh Agrawala",
                    "hidden": false
                },
                {
                    "_id": "6882f472846d2e78b7548a60",
                    "name": "Alan Yuille",
                    "hidden": false
                },
                {
                    "_id": "6882f472846d2e78b7548a61",
                    "name": "Lu Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T17:59:56.000Z",
            "submittedOnDailyAt": "2025-07-25T01:35:44.053Z",
            "title": "Captain Cinema: Towards Short Movie Generation",
            "submittedOnDailyBy": {
                "_id": "63468720dd6d90d82ccf3450",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
                "isPro": false,
                "fullname": "YSH",
                "user": "BestWishYsh",
                "type": "user"
            },
            "summary": "We present Captain Cinema, a generation framework for short movie generation.\nGiven a detailed textual description of a movie storyline, our approach firstly\ngenerates a sequence of keyframes that outline the entire narrative, which\nensures long-range coherence in both the storyline and visual appearance (e.g.,\nscenes and characters). We refer to this step as top-down keyframe planning.\nThese keyframes then serve as conditioning signals for a video synthesis model,\nwhich supports long context learning, to produce the spatio-temporal dynamics\nbetween them. This step is referred to as bottom-up video synthesis. To support\nstable and efficient generation of multi-scene long narrative cinematic works,\nwe introduce an interleaved training strategy for Multimodal Diffusion\nTransformers (MM-DiT), specifically adapted for long-context video data. Our\nmodel is trained on a specially curated cinematic dataset consisting of\ninterleaved data pairs. Our experiments demonstrate that Captain Cinema\nperforms favorably in the automated creation of visually coherent and narrative\nconsistent short movies in high quality and efficiency. Project page:\nhttps://thecinema.ai",
            "upvotes": 19,
            "discussionId": "6882f473846d2e78b7548a62",
            "projectPage": "https://thecinema.ai/",
            "ai_summary": "Captain Cinema generates high-quality short movies from textual descriptions using top-down keyframe planning and bottom-up video synthesis with interleaved training of Multimodal Diffusion Transformers.",
            "ai_keywords": [
                "top-down keyframe planning",
                "bottom-up video synthesis",
                "Multimodal Diffusion Transformers",
                "MM-DiT",
                "long-context learning",
                "interleaved training strategy",
                "cinematic dataset",
                "visually coherent",
                "narrative consistent"
            ]
        },
        "publishedAt": "2025-07-24T13:59:56.000Z",
        "title": "Captain Cinema: Towards Short Movie Generation",
        "summary": "We present Captain Cinema, a generation framework for short movie generation.\nGiven a detailed textual description of a movie storyline, our approach firstly\ngenerates a sequence of keyframes that outline the entire narrative, which\nensures long-range coherence in both the storyline and visual appearance (e.g.,\nscenes and characters). We refer to this step as top-down keyframe planning.\nThese keyframes then serve as conditioning signals for a video synthesis model,\nwhich supports long context learning, to produce the spatio-temporal dynamics\nbetween them. This step is referred to as bottom-up video synthesis. To support\nstable and efficient generation of multi-scene long narrative cinematic works,\nwe introduce an interleaved training strategy for Multimodal Diffusion\nTransformers (MM-DiT), specifically adapted for long-context video data. Our\nmodel is trained on a specially curated cinematic dataset consisting of\ninterleaved data pairs. Our experiments demonstrate that Captain Cinema\nperforms favorably in the automated creation of visually coherent and narrative\nconsistent short movies in high quality and efficiency. Project page:\nhttps://thecinema.ai",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18634.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63468720dd6d90d82ccf3450",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63468720dd6d90d82ccf3450/tVBFlmZNz8FRMkOrDaDID.jpeg",
            "fullname": "YSH",
            "name": "BestWishYsh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 56
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.18537",
            "authors": [
                {
                    "_id": "6882ea76846d2e78b7548a40",
                    "name": "Zhekai Chen",
                    "hidden": false
                },
                {
                    "_id": "6882ea76846d2e78b7548a41",
                    "name": "Ruihang Chu",
                    "hidden": false
                },
                {
                    "_id": "6882ea76846d2e78b7548a42",
                    "name": "Yukang Chen",
                    "hidden": false
                },
                {
                    "_id": "6882ea76846d2e78b7548a43",
                    "name": "Shiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6882ea76846d2e78b7548a44",
                    "name": "Yujie Wei",
                    "hidden": false
                },
                {
                    "_id": "6882ea76846d2e78b7548a45",
                    "name": "Yingya Zhang",
                    "hidden": false
                },
                {
                    "_id": "6882ea76846d2e78b7548a46",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T16:04:55.000Z",
            "submittedOnDailyAt": "2025-07-25T01:24:35.060Z",
            "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive\n  Generation",
            "submittedOnDailyBy": {
                "_id": "62d812e143df7719860d05d1",
                "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg",
                "isPro": false,
                "fullname": "zhekai chen",
                "user": "Azily",
                "type": "user"
            },
            "summary": "Scaling visual generation models is essential for real-world content\ncreation, yet requires substantial training and computational expenses.\nAlternatively, test-time scaling has garnered growing attention due to resource\nefficiency and promising performance. In this work, we present TTS-VAR, the\nfirst general test-time scaling framework for visual auto-regressive (VAR)\nmodels, modeling the generation process as a path searching problem. To\ndynamically balance computational efficiency with exploration capacity, we\nfirst introduce an adaptive descending batch size schedule throughout the\ncausal generation process. Besides, inspired by VAR's hierarchical\ncoarse-to-fine multi-scale generation, our framework integrates two key\ncomponents: (i) At coarse scales, we observe that generated tokens are hard for\nevaluation, possibly leading to erroneous acceptance of inferior samples or\nrejection of superior samples. Noticing that the coarse scales contain\nsufficient structural information, we propose clustering-based diversity\nsearch. It preserves structural variety through semantic feature clustering,\nenabling later selection on samples with higher potential. (ii) In fine scales,\nresampling-based potential selection prioritizes promising candidates using\npotential scores, which are defined as reward functions incorporating\nmulti-scale generation history. Experiments on the powerful VAR model Infinity\nshow a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights\nreveal that early-stage structural features effectively influence final\nquality, and resampling efficacy varies across generation scales. Code is\navailable at https://github.com/ali-vilab/TTS-VAR.",
            "upvotes": 11,
            "discussionId": "6882ea76846d2e78b7548a47",
            "githubRepo": "https://github.com/ali-vilab/TTS-VAR",
            "ai_summary": "TTS-VAR, a test-time scaling framework for visual auto-regressive models, improves generation quality by dynamically adjusting batch sizes and using clustering and resampling techniques.",
            "ai_keywords": [
                "test-time scaling",
                "visual auto-regressive models",
                "path searching problem",
                "adaptive descending batch size schedule",
                "causal generation process",
                "clustering-based diversity search",
                "resampling-based potential selection",
                "potential scores",
                "reward functions",
                "multi-scale generation history",
                "Infinity model",
                "GenEval score"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-07-24T12:04:55.000Z",
        "title": "TTS-VAR: A Test-Time Scaling Framework for Visual Auto-Regressive\n  Generation",
        "summary": "Scaling visual generation models is essential for real-world content\ncreation, yet requires substantial training and computational expenses.\nAlternatively, test-time scaling has garnered growing attention due to resource\nefficiency and promising performance. In this work, we present TTS-VAR, the\nfirst general test-time scaling framework for visual auto-regressive (VAR)\nmodels, modeling the generation process as a path searching problem. To\ndynamically balance computational efficiency with exploration capacity, we\nfirst introduce an adaptive descending batch size schedule throughout the\ncausal generation process. Besides, inspired by VAR's hierarchical\ncoarse-to-fine multi-scale generation, our framework integrates two key\ncomponents: (i) At coarse scales, we observe that generated tokens are hard for\nevaluation, possibly leading to erroneous acceptance of inferior samples or\nrejection of superior samples. Noticing that the coarse scales contain\nsufficient structural information, we propose clustering-based diversity\nsearch. It preserves structural variety through semantic feature clustering,\nenabling later selection on samples with higher potential. (ii) In fine scales,\nresampling-based potential selection prioritizes promising candidates using\npotential scores, which are defined as reward functions incorporating\nmulti-scale generation history. Experiments on the powerful VAR model Infinity\nshow a notable 8.7% GenEval score improvement (from 0.69 to 0.75). Key insights\nreveal that early-stage structural features effectively influence final\nquality, and resampling efficacy varies across generation scales. Code is\navailable at https://github.com/ali-vilab/TTS-VAR.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18537.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d812e143df7719860d05d1",
            "avatarUrl": "/avatars/412f7ec5c9f54990f4b562652d3e2c59.svg",
            "fullname": "zhekai chen",
            "name": "Azily",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.15844",
            "authors": [
                {
                    "_id": "687ef9a833947f780d9b4ad3",
                    "user": {
                        "_id": "6572a479b3d8dd7b92212a4e",
                        "avatarUrl": "/avatars/fc6d60211504547113a6e14e15ddb4fb.svg",
                        "isPro": false,
                        "fullname": "lvshangke",
                        "user": "paradox122",
                        "type": "user"
                    },
                    "name": "Shangke Lyu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T08:39:24.100Z",
                    "hidden": false
                },
                {
                    "_id": "687ef9a833947f780d9b4ad4",
                    "name": "Linjuan Wu",
                    "hidden": false
                },
                {
                    "_id": "687ef9a833947f780d9b4ad5",
                    "user": {
                        "_id": "64098738342c26884c792c93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                        "isPro": false,
                        "fullname": "Yuchen Yan",
                        "user": "yanyc",
                        "type": "user"
                    },
                    "name": "Yuchen Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-22T07:49:03.191Z",
                    "hidden": false
                },
                {
                    "_id": "687ef9a833947f780d9b4ad6",
                    "user": {
                        "_id": "674d103d2d93658f850151a6",
                        "avatarUrl": "/avatars/e221d43d3d8601d94cb6a44610054e23.svg",
                        "isPro": false,
                        "fullname": "wuxingyu",
                        "user": "wuxingyu",
                        "type": "user"
                    },
                    "name": "Xingyu Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T13:36:32.198Z",
                    "hidden": false
                },
                {
                    "_id": "687ef9a833947f780d9b4ad7",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "687ef9a833947f780d9b4ad8",
                    "user": {
                        "_id": "5e1058e9fcf41d740b69966d",
                        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                        "isPro": false,
                        "fullname": "Yongliang Shen",
                        "user": "tricktreat",
                        "type": "user"
                    },
                    "name": "Yongliang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-22T07:48:59.928Z",
                    "hidden": false
                },
                {
                    "_id": "687ef9a833947f780d9b4ad9",
                    "name": "Peisheng Jiang",
                    "hidden": false
                },
                {
                    "_id": "687ef9a833947f780d9b4ada",
                    "name": "Weiming Lu",
                    "hidden": false
                },
                {
                    "_id": "687ef9a833947f780d9b4adb",
                    "name": "Jun Xiao",
                    "hidden": false
                },
                {
                    "_id": "687ef9a833947f780d9b4adc",
                    "name": "Yueting Zhuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-21T17:52:34.000Z",
            "submittedOnDailyAt": "2025-07-25T02:09:28.395Z",
            "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
            "submittedOnDailyBy": {
                "_id": "5e1058e9fcf41d740b69966d",
                "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                "isPro": false,
                "fullname": "Yongliang Shen",
                "user": "tricktreat",
                "type": "user"
            },
            "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.",
            "upvotes": 11,
            "discussionId": "687ef9a933947f780d9b4add",
            "githubRepo": "https://github.com/zju-real/hbpo",
            "githubStars": 14
        },
        "publishedAt": "2025-07-21T13:52:34.000Z",
        "title": "Hierarchical Budget Policy Optimization for Adaptive Reasoning",
        "summary": "Large reasoning models achieve remarkable performance through extensive\nchain-of-thought generation, yet exhibit significant computational inefficiency\nby applying uniform reasoning strategies regardless of problem complexity. We\npresent Hierarchical Budget Policy Optimization (HBPO), a reinforcement\nlearning framework that enables models to learn problem-specific reasoning\ndepths without sacrificing capability. HBPO addresses the fundamental challenge\nof exploration space collapse in efficiency-oriented training, where penalties\non long output length systematically bias models away from necessary long\nreasoning paths. Through hierarchical budget exploration, our approach\npartitions rollout samples into multiple subgroups with distinct token budgets,\naiming to enable efficient resource allocation while preventing degradation of\ncapability. We introduce differentiated reward mechanisms that create\nbudget-aware incentives aligned with the complexity of the problem, allowing\nmodels to discover natural correspondences between task requirements and\ncomputational effort. Extensive experiments demonstrate that HBPO reduces\naverage token usage by up to 60.6% while improving accuracy by 3.14% across\nfour reasoning benchmarks. Unlike existing methods that impose external\nconstraints or rely on discrete mode selection, HBPO exhibits emergent adaptive\nbehavior where models automatically adjust reasoning depth based on problem\ncomplexity. Our results suggest that reasoning efficiency and capability are\nnot inherently conflicting, and can be simultaneously optimized through\nappropriately structured hierarchical training that preserves exploration\ndiversity.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15844.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5e1058e9fcf41d740b69966d",
            "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
            "fullname": "Yongliang Shen",
            "name": "tricktreat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 25
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.16535",
            "authors": [
                {
                    "_id": "688237c76a54dd1e77daa95c",
                    "user": {
                        "_id": "64de108815b7ebabdfa928d2",
                        "avatarUrl": "/avatars/63fa034ab4c427f195cc743bfa4e2299.svg",
                        "isPro": false,
                        "fullname": "Liu",
                        "user": "ShuYaoLiu",
                        "type": "user"
                    },
                    "name": "Shang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-24T14:11:16.075Z",
                    "hidden": false
                },
                {
                    "_id": "688237c76a54dd1e77daa95d",
                    "name": "Chenjie Cao",
                    "hidden": false
                },
                {
                    "_id": "688237c76a54dd1e77daa95e",
                    "name": "Chaohui Yu",
                    "hidden": false
                },
                {
                    "_id": "688237c76a54dd1e77daa95f",
                    "name": "Wen Qian",
                    "hidden": false
                },
                {
                    "_id": "688237c76a54dd1e77daa960",
                    "name": "Jing Wang",
                    "hidden": false
                },
                {
                    "_id": "688237c76a54dd1e77daa961",
                    "name": "Fan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-22T12:46:48.000Z",
            "submittedOnDailyAt": "2025-07-25T00:36:12.372Z",
            "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion",
            "submittedOnDailyBy": {
                "_id": "64de108815b7ebabdfa928d2",
                "avatarUrl": "/avatars/63fa034ab4c427f195cc743bfa4e2299.svg",
                "isPro": false,
                "fullname": "Liu",
                "user": "ShuYaoLiu",
                "type": "user"
            },
            "summary": "Despite the remarkable developments achieved by recent 3D generation works,\nscaling these methods to geographic extents, such as modeling thousands of\nsquare kilometers of Earth's surface, remains an open challenge. We address\nthis through a dual innovation in data infrastructure and model architecture.\nFirst, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,\nconsisting of 50k curated scenes (each measuring 600m x 600m) captured across\nthe U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene\nprovides pose-annotated multi-view images, depth maps, normals, semantic\nsegmentation, and camera poses, with explicit quality control to ensure terrain\ndiversity. Building on this foundation, we propose EarthCrafter, a tailored\nframework for large-scale 3D Earth generation via sparse-decoupled latent\ndiffusion. Our architecture separates structural and textural generation: 1)\nDual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D\nGaussian Splats (2DGS) into compact latent spaces, largely alleviating the\ncostly computation suffering from vast geographic scales while preserving\ncritical information. 2) We propose condition-aware flow matching models\ntrained on mixed inputs (semantics, images, or neither) to flexibly model\nlatent geometry and texture features independently. Extensive experiments\ndemonstrate that EarthCrafter performs substantially better in extremely\nlarge-scale generation. The framework further supports versatile applications,\nfrom semantic-guided urban layout generation to unconditional terrain\nsynthesis, while maintaining geographic plausibility through our rich data\npriors from Aerial-Earth3D. Our project page is available at\nhttps://whiteinblue.github.io/earthcrafter/",
            "upvotes": 8,
            "discussionId": "688237c76a54dd1e77daa962"
        },
        "publishedAt": "2025-07-22T08:46:48.000Z",
        "title": "EarthCrafter: Scalable 3D Earth Generation via Dual-Sparse Latent\n  Diffusion",
        "summary": "Despite the remarkable developments achieved by recent 3D generation works,\nscaling these methods to geographic extents, such as modeling thousands of\nsquare kilometers of Earth's surface, remains an open challenge. We address\nthis through a dual innovation in data infrastructure and model architecture.\nFirst, we introduce Aerial-Earth3D, the largest 3D aerial dataset to date,\nconsisting of 50k curated scenes (each measuring 600m x 600m) captured across\nthe U.S. mainland, comprising 45M multi-view Google Earth frames. Each scene\nprovides pose-annotated multi-view images, depth maps, normals, semantic\nsegmentation, and camera poses, with explicit quality control to ensure terrain\ndiversity. Building on this foundation, we propose EarthCrafter, a tailored\nframework for large-scale 3D Earth generation via sparse-decoupled latent\ndiffusion. Our architecture separates structural and textural generation: 1)\nDual sparse 3D-VAEs compress high-resolution geometric voxels and textural 2D\nGaussian Splats (2DGS) into compact latent spaces, largely alleviating the\ncostly computation suffering from vast geographic scales while preserving\ncritical information. 2) We propose condition-aware flow matching models\ntrained on mixed inputs (semantics, images, or neither) to flexibly model\nlatent geometry and texture features independently. Extensive experiments\ndemonstrate that EarthCrafter performs substantially better in extremely\nlarge-scale generation. The framework further supports versatile applications,\nfrom semantic-guided urban layout generation to unconditional terrain\nsynthesis, while maintaining geographic plausibility through our rich data\npriors from Aerial-Earth3D. Our project page is available at\nhttps://whiteinblue.github.io/earthcrafter/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16535.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64de108815b7ebabdfa928d2",
            "avatarUrl": "/avatars/63fa034ab4c427f195cc743bfa4e2299.svg",
            "fullname": "Liu",
            "name": "ShuYaoLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.18546",
            "authors": [
                {
                    "_id": "688335fe846d2e78b7548ba7",
                    "name": "Urchade Zaratiana",
                    "hidden": false
                },
                {
                    "_id": "688335fe846d2e78b7548ba8",
                    "name": "Gil Pasternak",
                    "hidden": false
                },
                {
                    "_id": "688335fe846d2e78b7548ba9",
                    "name": "Oliver Boyd",
                    "hidden": false
                },
                {
                    "_id": "688335fe846d2e78b7548baa",
                    "name": "George Hurn-Maloney",
                    "hidden": false
                },
                {
                    "_id": "688335fe846d2e78b7548bab",
                    "name": "Ash Lewis",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/LkSJWb7jx2Js7OVwSPV9g.png"
            ],
            "publishedAt": "2025-07-24T16:11:14.000Z",
            "submittedOnDailyAt": "2025-07-25T06:20:20.273Z",
            "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface",
            "submittedOnDailyBy": {
                "_id": "5e6a3d4ea9afd5125d9ec064",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                "isPro": true,
                "fullname": "Stefan Schweter",
                "user": "stefan-it",
                "type": "user"
            },
            "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.",
            "upvotes": 7,
            "discussionId": "688335fe846d2e78b7548bac",
            "ai_summary": "GLiNER2 is a unified framework that supports multiple NLP tasks using a single efficient transformer model, improving deployment accessibility over large language models.",
            "ai_keywords": [
                "transformer encoder",
                "named entity recognition",
                "text classification",
                "hierarchical structured data extraction",
                "multi-task composition",
                "schema-based interface"
            ]
        },
        "publishedAt": "2025-07-24T12:11:14.000Z",
        "title": "GLiNER2: An Efficient Multi-Task Information Extraction System with\n  Schema-Driven Interface",
        "summary": "Information extraction (IE) is fundamental to numerous NLP applications, yet\nexisting solutions often require specialized models for different tasks or rely\non computationally expensive large language models. We present GLiNER2, a\nunified framework that enhances the original GLiNER architecture to support\nnamed entity recognition, text classification, and hierarchical structured data\nextraction within a single efficient model. Built pretrained transformer\nencoder architecture, GLiNER2 maintains CPU efficiency and compact size while\nintroducing multi-task composition through an intuitive schema-based interface.\nOur experiments demonstrate competitive performance across extraction and\nclassification tasks with substantial improvements in deployment accessibility\ncompared to LLM-based alternatives. We release GLiNER2 as an open-source\npip-installable library with pre-trained models and documentation at\nhttps://github.com/fastino-ai/GLiNER2.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/LkSJWb7jx2Js7OVwSPV9g.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18546.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "5e6a3d4ea9afd5125d9ec064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2981
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.18464",
            "authors": [
                {
                    "_id": "68833d33846d2e78b7548bb3",
                    "name": "Miguel Aspis",
                    "hidden": false
                },
                {
                    "_id": "68833d33846d2e78b7548bb4",
                    "user": {
                        "_id": "628ddf04986ae70e823298f7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
                        "isPro": false,
                        "fullname": "Sebastin Andres Cajas Ordez",
                        "user": "sebasmos",
                        "type": "user"
                    },
                    "name": "Sebastin A. Cajas Ordnez",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T08:38:10.028Z",
                    "hidden": false
                },
                {
                    "_id": "68833d33846d2e78b7548bb5",
                    "user": {
                        "_id": "674f4a1689fcf1699e6f1713",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/dMrPBgCzZBLcmW2Xue94n.jpeg",
                        "isPro": false,
                        "fullname": "Andres L. Suarez Cetrulo",
                        "user": "suarezcetrulo",
                        "type": "user"
                    },
                    "name": "Andrs L. Surez-Cetrulo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T08:38:44.525Z",
                    "hidden": false
                },
                {
                    "_id": "68833d33846d2e78b7548bb6",
                    "name": "Ricardo Simn Carbajo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T14:39:20.000Z",
            "submittedOnDailyAt": "2025-07-25T06:47:43.241Z",
            "title": "DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts",
            "submittedOnDailyBy": {
                "_id": "628ddf04986ae70e823298f7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
                "isPro": false,
                "fullname": "Sebastin Andres Cajas Ordez",
                "user": "sebasmos",
                "type": "user"
            },
            "summary": "Learning from non-stationary data streams subject to concept drift requires\nmodels that can adapt on-the-fly while remaining resource-efficient. Existing\nadaptive ensemble methods often rely on coarse-grained adaptation mechanisms or\nsimple voting schemes that fail to optimally leverage specialized knowledge.\nThis paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture\nthat addresses these limitations through a novel co-training framework.\nDriftMoE features a compact neural router that is co-trained alongside a pool\nof incremental Hoeffding tree experts. The key innovation lies in a symbiotic\nlearning loop that enables expert specialization: the router selects the most\nsuitable expert for prediction, the relevant experts update incrementally with\nthe true label, and the router refines its parameters using a multi-hot\ncorrectness mask that reinforces every accurate expert. This feedback loop\nprovides the router with a clear training signal while accelerating expert\nspecialization. We evaluate DriftMoE's performance across nine state-of-the-art\ndata stream learning benchmarks spanning abrupt, gradual, and real-world drifts\ntesting two distinct configurations: one where experts specialize on data\nregimes (multi-class variant), and another where they focus on single-class\nspecialization (task-based variant). Our results demonstrate that DriftMoE\nachieves competitive results with state-of-the-art stream learning adaptive\nensembles, offering a principled and efficient approach to concept drift\nadaptation. All code, data pipelines, and reproducibility scripts are available\nin our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.",
            "upvotes": 7,
            "discussionId": "68833d33846d2e78b7548bb7",
            "githubRepo": "https://github.com/miguel-ceadar/drift-moe",
            "ai_summary": "DriftMoE, an online Mixture-of-Experts architecture with a compact neural router, achieves competitive results in adapting to concept drift in data streams through a symbiotic learning loop.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "MoE",
                "neural router",
                "Hoeffding tree experts",
                "incremental learning",
                "co-training framework",
                "multi-hot correctness mask",
                "expert specialization",
                "concept drift",
                "data stream learning",
                "abrupt drift",
                "gradual drift",
                "real-world drifts"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-07-24T10:39:20.000Z",
        "title": "DriftMoE: A Mixture of Experts Approach to Handle Concept Drifts",
        "summary": "Learning from non-stationary data streams subject to concept drift requires\nmodels that can adapt on-the-fly while remaining resource-efficient. Existing\nadaptive ensemble methods often rely on coarse-grained adaptation mechanisms or\nsimple voting schemes that fail to optimally leverage specialized knowledge.\nThis paper introduces DriftMoE, an online Mixture-of-Experts (MoE) architecture\nthat addresses these limitations through a novel co-training framework.\nDriftMoE features a compact neural router that is co-trained alongside a pool\nof incremental Hoeffding tree experts. The key innovation lies in a symbiotic\nlearning loop that enables expert specialization: the router selects the most\nsuitable expert for prediction, the relevant experts update incrementally with\nthe true label, and the router refines its parameters using a multi-hot\ncorrectness mask that reinforces every accurate expert. This feedback loop\nprovides the router with a clear training signal while accelerating expert\nspecialization. We evaluate DriftMoE's performance across nine state-of-the-art\ndata stream learning benchmarks spanning abrupt, gradual, and real-world drifts\ntesting two distinct configurations: one where experts specialize on data\nregimes (multi-class variant), and another where they focus on single-class\nspecialization (task-based variant). Our results demonstrate that DriftMoE\nachieves competitive results with state-of-the-art stream learning adaptive\nensembles, offering a principled and efficient approach to concept drift\nadaptation. All code, data pipelines, and reproducibility scripts are available\nin our public GitHub repository: https://github.com/miguel-ceadar/drift-moe.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18464.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628ddf04986ae70e823298f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
            "fullname": "Sebastin Andres Cajas Ordez",
            "name": "sebasmos",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.14988",
            "authors": [
                {
                    "_id": "6882ea91846d2e78b7548a49",
                    "name": "Yinghao Aaron Li",
                    "hidden": false
                },
                {
                    "_id": "6882ea91846d2e78b7548a4a",
                    "name": "Xilin Jiang",
                    "hidden": false
                },
                {
                    "_id": "6882ea91846d2e78b7548a4b",
                    "name": "Fei Tao",
                    "hidden": false
                },
                {
                    "_id": "6882ea91846d2e78b7548a4c",
                    "name": "Cheng Niu",
                    "hidden": false
                },
                {
                    "_id": "6882ea91846d2e78b7548a4d",
                    "name": "Kaifeng Xu",
                    "hidden": false
                },
                {
                    "_id": "6882ea91846d2e78b7548a4e",
                    "name": "Juntong Song",
                    "hidden": false
                },
                {
                    "_id": "6882ea91846d2e78b7548a4f",
                    "name": "Nima Mesgarani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-20T14:48:48.000Z",
            "submittedOnDailyAt": "2025-07-25T01:01:10.523Z",
            "title": "DMOSpeech 2: Reinforcement Learning for Duration Prediction in\n  Metric-Optimized Speech Synthesis",
            "submittedOnDailyBy": {
                "_id": "6531a65daed617662c7f1007",
                "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
                "isPro": false,
                "fullname": "Xilin Jiang",
                "user": "xi-j",
                "type": "user"
            },
            "summary": "Diffusion-based text-to-speech (TTS) systems have made remarkable progress in\nzero-shot speech synthesis, yet optimizing all components for perceptual\nmetrics remains challenging. Prior work with DMOSpeech demonstrated direct\nmetric optimization for speech generation components, but duration prediction\nremained unoptimized. This paper presents DMOSpeech 2, which extends metric\noptimization to the duration predictor through a reinforcement learning\napproach. The proposed system implements a novel duration policy framework\nusing group relative preference optimization (GRPO) with speaker similarity and\nword error rate as reward signals. By optimizing this previously unoptimized\ncomponent, DMOSpeech 2 creates a more complete metric-optimized synthesis\npipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid\napproach leveraging a teacher model for initial denoising steps before\ntransitioning to the student model, significantly improving output diversity\nwhile maintaining efficiency. Comprehensive evaluations demonstrate superior\nperformance across all metrics compared to previous systems, while reducing\nsampling steps by half without quality degradation. These advances represent a\nsignificant step toward speech synthesis systems with metric optimization\nacross multiple components. The audio samples, code and pre-trained models are\navailable at https://dmospeech2.github.io/.",
            "upvotes": 6,
            "discussionId": "6882ea92846d2e78b7548a50",
            "ai_summary": "DMOSpeech 2 optimizes duration prediction and introduces teacher-guided sampling to enhance speech synthesis performance and diversity.",
            "ai_keywords": [
                "diffusion-based text-to-speech",
                "DMOSpeech",
                "metric optimization",
                "duration prediction",
                "reinforcement learning",
                "group relative preference optimization",
                "GRPO",
                "speaker similarity",
                "word error rate",
                "teacher-guided sampling",
                "speech synthesis",
                "output diversity"
            ]
        },
        "publishedAt": "2025-07-20T10:48:48.000Z",
        "title": "DMOSpeech 2: Reinforcement Learning for Duration Prediction in\n  Metric-Optimized Speech Synthesis",
        "summary": "Diffusion-based text-to-speech (TTS) systems have made remarkable progress in\nzero-shot speech synthesis, yet optimizing all components for perceptual\nmetrics remains challenging. Prior work with DMOSpeech demonstrated direct\nmetric optimization for speech generation components, but duration prediction\nremained unoptimized. This paper presents DMOSpeech 2, which extends metric\noptimization to the duration predictor through a reinforcement learning\napproach. The proposed system implements a novel duration policy framework\nusing group relative preference optimization (GRPO) with speaker similarity and\nword error rate as reward signals. By optimizing this previously unoptimized\ncomponent, DMOSpeech 2 creates a more complete metric-optimized synthesis\npipeline. Additionally, this paper introduces teacher-guided sampling, a hybrid\napproach leveraging a teacher model for initial denoising steps before\ntransitioning to the student model, significantly improving output diversity\nwhile maintaining efficiency. Comprehensive evaluations demonstrate superior\nperformance across all metrics compared to previous systems, while reducing\nsampling steps by half without quality degradation. These advances represent a\nsignificant step toward speech synthesis systems with metric optimization\nacross multiple components. The audio samples, code and pre-trained models are\navailable at https://dmospeech2.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.14988.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6531a65daed617662c7f1007",
            "avatarUrl": "/avatars/ea2e504780dc40719f7501ab2c7d9c91.svg",
            "fullname": "Xilin Jiang",
            "name": "xi-j",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.18103",
            "authors": [
                {
                    "_id": "68832a97846d2e78b7548ad6",
                    "name": "Riley Carlson",
                    "hidden": false
                },
                {
                    "_id": "68832a97846d2e78b7548ad7",
                    "name": "John Bauer",
                    "hidden": false
                },
                {
                    "_id": "68832a97846d2e78b7548ad8",
                    "name": "Christopher D. Manning",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T05:29:18.000Z",
            "submittedOnDailyAt": "2025-07-25T05:26:55.424Z",
            "title": "A New Pair of GloVes",
            "submittedOnDailyBy": {
                "_id": "5e6a3d4ea9afd5125d9ec064",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                "isPro": true,
                "fullname": "Stefan Schweter",
                "user": "stefan-it",
                "type": "user"
            },
            "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.",
            "upvotes": 5,
            "discussionId": "68832a97846d2e78b7548ad9",
            "ai_summary": "New 2024 GloVe models improve upon 2014 versions by incorporating updated datasets and demonstrating enhanced performance on culturally and temporally relevant Named Entity Recognition tasks.",
            "ai_keywords": [
                "GloVe",
                "Global Vectors for Word Representation",
                "word embeddings",
                "Wikipedia",
                "Gigaword",
                "Dolma",
                "vocabulary comparison",
                "analogy",
                "similarity",
                "Named Entity Recognition",
                "NER"
            ]
        },
        "publishedAt": "2025-07-24T01:29:18.000Z",
        "title": "A New Pair of GloVes",
        "summary": "This report documents, describes, and evaluates new 2024 English GloVe\n(Global Vectors for Word Representation) models. While the original GloVe\nmodels built in 2014 have been widely used and found useful, languages and the\nworld continue to evolve and we thought that current usage could benefit from\nupdated models. Moreover, the 2014 models were not carefully documented as to\nthe exact data versions and preprocessing that were used, and we rectify this\nby documenting these new models. We trained two sets of word embeddings using\nWikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary\ncomparison, direct testing, and NER tasks shows that the 2024 vectors\nincorporate new culturally and linguistically relevant words, perform\ncomparably on structural tasks like analogy and similarity, and demonstrate\nimproved performance on recent, temporally dependent NER datasets such as\nnon-Western newswire data.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18103.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e6a3d4ea9afd5125d9ec064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2981
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.18013",
            "authors": [
                {
                    "_id": "68834762846d2e78b7548bfb",
                    "user": {
                        "_id": "640ad946b871d7117d5eead7",
                        "avatarUrl": "/avatars/4cd90f6926a24c58485ef364eae50ef1.svg",
                        "isPro": false,
                        "fullname": "Zihan Wang",
                        "user": "ZihanWang99",
                        "type": "user"
                    },
                    "name": "Zihan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T13:36:16.549Z",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548bfc",
                    "name": "Xinzhang Liu",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548bfd",
                    "name": "Yitong Yao",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548bfe",
                    "name": "Chao Wang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548bff",
                    "name": "Yu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c00",
                    "name": "Zhihao Yang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c01",
                    "name": "Wenmin Deng",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c02",
                    "name": "Kaipeng Jia",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c03",
                    "name": "Jiaxin Peng",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c04",
                    "name": "Yuyao Huang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c05",
                    "name": "Sishi Xiong",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c06",
                    "name": "Zhuo Jiang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c07",
                    "name": "Kaidong Yu",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c08",
                    "name": "Xiaohui Hu",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c09",
                    "name": "Fubei Yao",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c0a",
                    "name": "Ruiyu Fang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c0b",
                    "name": "Zhuoru Jiang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c0c",
                    "name": "Ruiting Song",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c0d",
                    "name": "Qiyi Xie",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c0e",
                    "name": "Rui Xue",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c0f",
                    "name": "Xuewei He",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c10",
                    "name": "Yanlei Xue",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c11",
                    "name": "Zhu Yuan",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c12",
                    "name": "Zhaoxi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c13",
                    "name": "Zilu Huang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c14",
                    "name": "Shiquan Wang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c15",
                    "name": "Xin Wang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c16",
                    "name": "Hanming Wu",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c17",
                    "name": "Mingyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c18",
                    "name": "Xufeng Zhan",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c19",
                    "name": "Yuhan Sun",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c1a",
                    "name": "Zhaohu Xing",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c1b",
                    "name": "Yuhao Jiang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c1c",
                    "name": "Bingkai Yang",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c1d",
                    "name": "Shuangyong Song",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c1e",
                    "name": "Yongxiang Li",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c1f",
                    "name": "Zhongjiang He",
                    "hidden": false
                },
                {
                    "_id": "68834762846d2e78b7548c20",
                    "name": "Xuelong Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T01:00:48.000Z",
            "submittedOnDailyAt": "2025-07-25T07:32:10.810Z",
            "title": "Technical Report of TeleChat2, TeleChat2.5 and T1",
            "submittedOnDailyBy": {
                "_id": "640ad946b871d7117d5eead7",
                "avatarUrl": "/avatars/4cd90f6926a24c58485ef364eae50ef1.svg",
                "isPro": false,
                "fullname": "Zihan Wang",
                "user": "ZihanWang99",
                "type": "user"
            },
            "summary": "We introduce the latest series of TeleChat models: TeleChat2,\nTeleChat2.5, and T1, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith TeleChat2, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. TeleChat2.5 and T1 expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The T1 variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, TeleChat2.5 prioritizes speed, delivering rapid\ninference. Both flagship models of T1 and TeleChat2.5 are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, T1-115B outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release TeleChat2,\nTeleChat2.5 and T1, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications.",
            "upvotes": 5,
            "discussionId": "68834762846d2e78b7548c21",
            "ai_summary": "The TeleChat2, TeleChat2.5, and T1 models enhance language capabilities through advanced training strategies, including Supervised Fine-Tuning, Direct Preference Optimization, and reinforcement learning, achieving superior performance in reasoning and speed compared to previous models.",
            "ai_keywords": [
                "Supervised Fine-Tuning",
                "Direct Preference Optimization",
                "reinforcement learning",
                "Chain-of-Thought",
                "Transformer-based architectures"
            ]
        },
        "publishedAt": "2025-07-23T21:00:48.000Z",
        "title": "Technical Report of TeleChat2, TeleChat2.5 and T1",
        "summary": "We introduce the latest series of TeleChat models: TeleChat2,\nTeleChat2.5, and T1, offering a significant upgrade over\ntheir predecessor, TeleChat. Despite minimal changes to the model architecture,\nthe new series achieves substantial performance gains through enhanced training\nstrategies in both pre-training and post-training stages. The series begins\nwith TeleChat2, which undergoes pretraining on 10 trillion\nhigh-quality and diverse tokens. This is followed by Supervised Fine-Tuning\n(SFT) and Direct Preference Optimization (DPO) to further enhance its\ncapabilities. TeleChat2.5 and T1 expand the pipeline by\nincorporating a continual pretraining phase with domain-specific datasets,\ncombined with reinforcement learning (RL) to improve performance in code\ngeneration and mathematical reasoning tasks. The T1 variant is\ndesigned for complex reasoning, supporting long Chain-of-Thought (CoT)\nreasoning and demonstrating substantial improvements in mathematics and coding.\nIn contrast, TeleChat2.5 prioritizes speed, delivering rapid\ninference. Both flagship models of T1 and TeleChat2.5 are\ndense Transformer-based architectures with 115B parameters, showcasing\nsignificant advancements in reasoning and general task performance compared to\nthe original TeleChat. Notably, T1-115B outperform proprietary models\nsuch as OpenAI's o1-mini and GPT-4o. We publicly release TeleChat2,\nTeleChat2.5 and T1, including post-trained versions with 35B\nand 115B parameters, to empower developers and researchers with\nstate-of-the-art language models tailored for diverse applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18013.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640ad946b871d7117d5eead7",
            "avatarUrl": "/avatars/4cd90f6926a24c58485ef364eae50ef1.svg",
            "fullname": "Zihan Wang",
            "name": "ZihanWang99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.15595",
            "authors": [
                {
                    "_id": "68833580846d2e78b7548b98",
                    "user": {
                        "_id": "63e7493fdb40d9e67feaccdd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
                        "isPro": false,
                        "fullname": "Salah Eddine Bekhouche",
                        "user": "Bekhouche",
                        "type": "user"
                    },
                    "name": "Salah Eddine Bekhouche",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T08:38:18.314Z",
                    "hidden": false
                },
                {
                    "_id": "68833580846d2e78b7548b99",
                    "name": "Gaby Maroun",
                    "hidden": false
                },
                {
                    "_id": "68833580846d2e78b7548b9a",
                    "name": "Fadi Dornaika",
                    "hidden": false
                },
                {
                    "_id": "68833580846d2e78b7548b9b",
                    "name": "Abdenour Hadid",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/cDOcgi2KvA5oqyayMqsZh.png"
            ],
            "publishedAt": "2025-07-21T13:18:05.000Z",
            "submittedOnDailyAt": "2025-07-25T06:17:55.434Z",
            "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical\n  Imaging",
            "submittedOnDailyBy": {
                "_id": "63e7493fdb40d9e67feaccdd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
                "isPro": false,
                "fullname": "Salah Eddine Bekhouche",
                "user": "Bekhouche",
                "type": "user"
            },
            "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\nhttps://github.com/Bekhouche/SegDT{GitHub}.",
            "upvotes": 4,
            "discussionId": "68833580846d2e78b7548b9c",
            "githubRepo": "https://github.com/Bekhouche/SegDT/",
            "ai_summary": "SegDT, a diffusion transformer-based segmentation model, achieves state-of-the-art results in skin lesion segmentation with fast inference speeds, making it suitable for real-world medical applications.",
            "ai_keywords": [
                "diffusion transformer",
                "Rectified Flow",
                "medical image segmentation",
                "skin lesions",
                "skin cancer",
                "benchmarking datasets",
                "inference speeds"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-07-21T09:18:05.000Z",
        "title": "SegDT: A Diffusion Transformer-Based Segmentation Model for Medical\n  Imaging",
        "summary": "Medical image segmentation is crucial for many healthcare tasks, including\ndisease diagnosis and treatment planning. One key area is the segmentation of\nskin lesions, which is vital for diagnosing skin cancer and monitoring\npatients. In this context, this paper introduces SegDT, a new segmentation\nmodel based on diffusion transformer (DiT). SegDT is designed to work on\nlow-cost hardware and incorporates Rectified Flow, which improves the\ngeneration quality at reduced inference steps and maintains the flexibility of\nstandard diffusion models. Our method is evaluated on three benchmarking\ndatasets and compared against several existing works, achieving\nstate-of-the-art results while maintaining fast inference speeds. This makes\nthe proposed model appealing for real-world medical applications. This work\nadvances the performance and capabilities of deep learning models in medical\nimage analysis, enabling faster, more accurate diagnostic tools for healthcare\nprofessionals. The code is made publicly available at\nhttps://github.com/Bekhouche/SegDT{GitHub}.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63e7493fdb40d9e67feaccdd/cDOcgi2KvA5oqyayMqsZh.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15595.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63e7493fdb40d9e67feaccdd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63e7493fdb40d9e67feaccdd/9d1iTreC4GlxSiWvLT0dP.jpeg",
            "fullname": "Salah Eddine Bekhouche",
            "name": "Bekhouche",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.18192",
            "authors": [
                {
                    "_id": "6882db3f846d2e78b7548a37",
                    "name": "Minghao Fu",
                    "hidden": false
                },
                {
                    "_id": "6882db3f846d2e78b7548a38",
                    "user": {
                        "_id": "636f4c6b5d2050767e4a1491",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
                        "isPro": false,
                        "fullname": "Guo-Hua Wang",
                        "user": "Flourish",
                        "type": "user"
                    },
                    "name": "Guo-Hua Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T08:38:39.947Z",
                    "hidden": false
                },
                {
                    "_id": "6882db3f846d2e78b7548a39",
                    "name": "Xiaohao Chen",
                    "hidden": false
                },
                {
                    "_id": "6882db3f846d2e78b7548a3a",
                    "name": "Qing-Guo Chen",
                    "hidden": false
                },
                {
                    "_id": "6882db3f846d2e78b7548a3b",
                    "name": "Zhao Xu",
                    "hidden": false
                },
                {
                    "_id": "6882db3f846d2e78b7548a3c",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "6882db3f846d2e78b7548a3d",
                    "name": "Kaifu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T08:45:40.000Z",
            "submittedOnDailyAt": "2025-07-25T02:04:07.946Z",
            "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
            "submittedOnDailyBy": {
                "_id": "636f4c6b5d2050767e4a1491",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
                "isPro": false,
                "fullname": "Guo-Hua Wang",
                "user": "Flourish",
                "type": "user"
            },
            "summary": "Recent advances in text-to-image synthesis largely benefit from sophisticated\nsampling strategies and classifier-free guidance (CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (Text\nEmbeddings Fusion), a novel and efficient distillation method\nthat directly incorporates the guidance magnitude into the text embeddings and\ndistills the teacher model's complex sampling strategy. By simply fusing\nconditional and unconditional text embeddings using linear operations,\nTeEFusion reconstructs the desired guidance without adding extra parameters,\nsimultaneously enabling the student model to learn from the teacher's output\nproduced via its sophisticated sampling approach. Extensive experiments on\nstate-of-the-art models such as SD3 demonstrate that our method allows the\nstudent to closely mimic the teacher's performance with a far simpler and more\nefficient sampling strategy. Consequently, the student model achieves inference\nspeeds up to 6times faster than the teacher model, while maintaining image\nquality at levels comparable to those obtained through the teacher's complex\nsampling approach. The code is publicly available at\nhttps://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}.",
            "upvotes": 3,
            "discussionId": "6882db40846d2e78b7548a3e",
            "githubRepo": "https://github.com/AIDC-AI/TeEFusion",
            "ai_summary": "TeEFusion enhances text-to-image synthesis by efficiently incorporating classifier-free guidance into text embeddings, reducing inference costs without sacrificing image quality.",
            "ai_keywords": [
                "sampling strategies",
                "classifier-free guidance",
                "TeEFusion",
                "text embeddings",
                "distillation method",
                "linear operations",
                "student model",
                "teacher model",
                "inference speeds",
                "image quality",
                "SD3"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-07-24T04:45:40.000Z",
        "title": "TeEFusion: Blending Text Embeddings to Distill Classifier-Free Guidance",
        "summary": "Recent advances in text-to-image synthesis largely benefit from sophisticated\nsampling strategies and classifier-free guidance (CFG) to ensure high-quality\ngeneration. However, CFG's reliance on two forward passes, especially when\ncombined with intricate sampling algorithms, results in prohibitively high\ninference costs. To address this, we introduce TeEFusion (Text\nEmbeddings Fusion), a novel and efficient distillation method\nthat directly incorporates the guidance magnitude into the text embeddings and\ndistills the teacher model's complex sampling strategy. By simply fusing\nconditional and unconditional text embeddings using linear operations,\nTeEFusion reconstructs the desired guidance without adding extra parameters,\nsimultaneously enabling the student model to learn from the teacher's output\nproduced via its sophisticated sampling approach. Extensive experiments on\nstate-of-the-art models such as SD3 demonstrate that our method allows the\nstudent to closely mimic the teacher's performance with a far simpler and more\nefficient sampling strategy. Consequently, the student model achieves inference\nspeeds up to 6times faster than the teacher model, while maintaining image\nquality at levels comparable to those obtained through the teacher's complex\nsampling approach. The code is publicly available at\nhttps://github.com/AIDC-AI/TeEFusion{github.com/AIDC-AI/TeEFusion}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18192.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636f4c6b5d2050767e4a1491",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/636f4c6b5d2050767e4a1491/qErBLQnYzL35EkbDAhvqN.jpeg",
            "fullname": "Guo-Hua Wang",
            "name": "Flourish",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.16038",
            "authors": [
                {
                    "_id": "68833e9e846d2e78b7548bbf",
                    "name": "Rahul Venkatesh",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc0",
                    "name": "Klemen Kotar",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc1",
                    "name": "Lilian Naing Chen",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc2",
                    "name": "Seungwoo Kim",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc3",
                    "name": "Luca Thomas Wheeler",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc4",
                    "name": "Jared Watrous",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc5",
                    "name": "Ashley Xu",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc6",
                    "name": "Gia Ancone",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc7",
                    "name": "Wanhee Lee",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc8",
                    "name": "Honglin Chen",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bc9",
                    "name": "Daniel Bear",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bca",
                    "name": "Stefan Stojanov",
                    "hidden": false
                },
                {
                    "_id": "68833e9e846d2e78b7548bcb",
                    "name": "Daniel Yamins",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/658223f521aa786b76a71d15/NF94odl9OrcIYDIbfdKz3.png"
            ],
            "publishedAt": "2025-07-21T20:11:57.000Z",
            "submittedOnDailyAt": "2025-07-25T06:58:59.296Z",
            "title": "Discovering and using Spelke segments",
            "submittedOnDailyBy": {
                "_id": "658223f521aa786b76a71d15",
                "avatarUrl": "/avatars/3b99b7e35453b9d3f1865344ffdbf183.svg",
                "isPro": false,
                "fullname": "Rahul Venkatesh",
                "user": "rmvenkat",
                "type": "user"
            },
            "summary": "Segments in computer vision are often defined by semantic considerations and\nare highly dependent on category-specific conventions. In contrast,\ndevelopmental psychology suggests that humans perceive the world in terms of\nSpelke objects--groupings of physical things that reliably move together when\nacted on by physical forces. Spelke objects thus operate on category-agnostic\ncausal motion relationships which potentially better support tasks like\nmanipulation and planning. In this paper, we first benchmark the Spelke object\nconcept, introducing the SpelkeBench dataset that contains a wide variety of\nwell-defined Spelke segments in natural images. Next, to extract Spelke\nsegments from images algorithmically, we build SpelkeNet, a class of visual\nworld models trained to predict distributions over future motions. SpelkeNet\nsupports estimation of two key concepts for Spelke object discovery: (1) the\nmotion affordance map, identifying regions likely to move under a poke, and (2)\nthe expected-displacement map, capturing how the rest of the scene will move.\nThese concepts are used for \"statistical counterfactual probing\", where diverse\n\"virtual pokes\" are applied on regions of high motion-affordance, and the\nresultant expected displacement maps are used define Spelke segments as\nstatistical aggregates of correlated motion statistics. We find that SpelkeNet\noutperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench.\nFinally, we show that the Spelke concept is practically useful for downstream\napplications, yielding superior performance on the 3DEditBench benchmark for\nphysical object manipulation when used in a variety of off-the-shelf object\nmanipulation models.",
            "upvotes": 3,
            "discussionId": "68833e9e846d2e78b7548bcc",
            "projectPage": "https://neuroailab.github.io/spelke_net/",
            "githubRepo": "https://github.com/neuroailab/SpelkeNet/tree/main",
            "ai_summary": "A visual world model called SpelkeNet outperforms existing methods in identifying Spelke objects in images, improving performance in tasks like physical object manipulation.",
            "ai_keywords": [
                "Spelke objects",
                "SpelkeBench",
                "SpelkeNet",
                "motion affordance map",
                "expected-displacement map",
                "statistical counterfactual probing",
                "SegmentAnything",
                "3DEditBench"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-07-21T16:11:57.000Z",
        "title": "Discovering and using Spelke segments",
        "summary": "Segments in computer vision are often defined by semantic considerations and\nare highly dependent on category-specific conventions. In contrast,\ndevelopmental psychology suggests that humans perceive the world in terms of\nSpelke objects--groupings of physical things that reliably move together when\nacted on by physical forces. Spelke objects thus operate on category-agnostic\ncausal motion relationships which potentially better support tasks like\nmanipulation and planning. In this paper, we first benchmark the Spelke object\nconcept, introducing the SpelkeBench dataset that contains a wide variety of\nwell-defined Spelke segments in natural images. Next, to extract Spelke\nsegments from images algorithmically, we build SpelkeNet, a class of visual\nworld models trained to predict distributions over future motions. SpelkeNet\nsupports estimation of two key concepts for Spelke object discovery: (1) the\nmotion affordance map, identifying regions likely to move under a poke, and (2)\nthe expected-displacement map, capturing how the rest of the scene will move.\nThese concepts are used for \"statistical counterfactual probing\", where diverse\n\"virtual pokes\" are applied on regions of high motion-affordance, and the\nresultant expected displacement maps are used define Spelke segments as\nstatistical aggregates of correlated motion statistics. We find that SpelkeNet\noutperforms supervised baselines like SegmentAnything (SAM) on SpelkeBench.\nFinally, we show that the Spelke concept is practically useful for downstream\napplications, yielding superior performance on the 3DEditBench benchmark for\nphysical object manipulation when used in a variety of off-the-shelf object\nmanipulation models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/658223f521aa786b76a71d15/NF94odl9OrcIYDIbfdKz3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16038.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "658223f521aa786b76a71d15",
            "avatarUrl": "/avatars/3b99b7e35453b9d3f1865344ffdbf183.svg",
            "fullname": "Rahul Venkatesh",
            "name": "rmvenkat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.18405",
            "authors": [
                {
                    "_id": "6883591b6d99b948a364d1bc",
                    "user": {
                        "_id": "6562baa7516893ddeb9ad518",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6562baa7516893ddeb9ad518/xHyyR-F-x3UxTr3Zw2pIj.jpeg",
                        "isPro": false,
                        "fullname": "Simin Huo",
                        "user": "cominder",
                        "type": "user"
                    },
                    "name": "Simin Huo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-07-25T15:09:55.852Z",
                    "hidden": false
                },
                {
                    "_id": "6883591b6d99b948a364d1bd",
                    "name": "Ning Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T13:45:48.000Z",
            "submittedOnDailyAt": "2025-07-25T13:43:27.425Z",
            "title": "Iwin Transformer: Hierarchical Vision Transformer using Interleaved\n  Windows",
            "submittedOnDailyBy": {
                "_id": "6562baa7516893ddeb9ad518",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6562baa7516893ddeb9ad518/xHyyR-F-x3UxTr3Zw2pIj.jpeg",
                "isPro": false,
                "fullname": "Simin Huo",
                "user": "cominder",
                "type": "user"
            },
            "summary": "We introduce Iwin Transformer, a novel position-embedding-free hierarchical\nvision transformer, which can be fine-tuned directly from low to high\nresolution, through the collaboration of innovative interleaved window\nattention and depthwise separable convolution. This approach uses attention to\nconnect distant tokens and applies convolution to link neighboring tokens,\nenabling global information exchange within a single module, overcoming Swin\nTransformer's limitation of requiring two consecutive blocks to approximate\nglobal attention. Extensive experiments on visual benchmarks demonstrate that\nIwin Transformer exhibits strong competitiveness in tasks such as image\nclassification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and\nvideo action recognition. We also validate the effectiveness of the core\ncomponent in Iwin as a standalone module that can seamlessly replace the\nself-attention module in class-conditional image generation. The concepts and\nmethods introduced by the Iwin Transformer have the potential to inspire future\nresearch, like Iwin 3D Attention in video generation. The code and models are\navailable at https://github.com/cominder/Iwin-Transformer.",
            "upvotes": 2,
            "discussionId": "6883591b6d99b948a364d1be",
            "githubRepo": "https://github.com/Cominder/Iwin-Transformer",
            "ai_summary": "Iwin Transformer, a hierarchical vision transformer without position embeddings, combines interleaved window attention and depthwise separable convolution for efficient global information exchange, achieving competitive performance in image classification, semantic segmentation, and video action recognition.",
            "ai_keywords": [
                "position-embedding-free",
                "hierarchical vision transformer",
                "interleaved window attention",
                "depthwise separable convolution",
                "global attention",
                "Swin Transformer",
                "image classification",
                "semantic segmentation",
                "video action recognition",
                "class-conditional image generation",
                "Iwin 3D Attention"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-07-24T09:45:48.000Z",
        "title": "Iwin Transformer: Hierarchical Vision Transformer using Interleaved\n  Windows",
        "summary": "We introduce Iwin Transformer, a novel position-embedding-free hierarchical\nvision transformer, which can be fine-tuned directly from low to high\nresolution, through the collaboration of innovative interleaved window\nattention and depthwise separable convolution. This approach uses attention to\nconnect distant tokens and applies convolution to link neighboring tokens,\nenabling global information exchange within a single module, overcoming Swin\nTransformer's limitation of requiring two consecutive blocks to approximate\nglobal attention. Extensive experiments on visual benchmarks demonstrate that\nIwin Transformer exhibits strong competitiveness in tasks such as image\nclassification (87.4 top-1 accuracy on ImageNet-1K), semantic segmentation and\nvideo action recognition. We also validate the effectiveness of the core\ncomponent in Iwin as a standalone module that can seamlessly replace the\nself-attention module in class-conditional image generation. The concepts and\nmethods introduced by the Iwin Transformer have the potential to inspire future\nresearch, like Iwin 3D Attention in video generation. The code and models are\navailable at https://github.com/cominder/Iwin-Transformer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18405.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6562baa7516893ddeb9ad518",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6562baa7516893ddeb9ad518/xHyyR-F-x3UxTr3Zw2pIj.jpeg",
            "fullname": "Simin Huo",
            "name": "cominder",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.16802",
            "authors": [
                {
                    "_id": "688358306d99b948a364d1a2",
                    "name": "Yanjun Zheng",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1a3",
                    "name": "Xiyang Du",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1a4",
                    "name": "Longfei Liao",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1a5",
                    "name": "Xiaoke Zhao",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1a6",
                    "name": "Zhaowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1a7",
                    "name": "Jingze Song",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1a8",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1a9",
                    "name": "Jiawei Liu",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1aa",
                    "name": "Xiang Qi",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1ab",
                    "name": "Zhe Li",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1ac",
                    "name": "Zhiqiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1ad",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "688358306d99b948a364d1ae",
                    "name": "Peng Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/xiYbRb5V1RWOpZJz3yzrC.png"
            ],
            "publishedAt": "2025-07-22T17:52:16.000Z",
            "submittedOnDailyAt": "2025-07-25T09:23:09.672Z",
            "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.",
            "upvotes": 2,
            "discussionId": "688358306d99b948a364d1af",
            "ai_summary": "The Agentar-Fin-R1 series of financial large language models enhances reasoning, reliability, and domain specialization through a trustworthiness assurance framework and achieves state-of-the-art performance on financial and general reasoning tasks.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "Qwen3",
                "financial large language models",
                "reasoning capabilities",
                "trustworthiness criteria",
                "domain-specific requirements",
                "high-quality financial task label system",
                "multi-layered trustworthiness assurance framework",
                "trustworthy knowledge engineering",
                "multi-agent trustworthy data synthesis",
                "rigorous data validation governance",
                "label-guided automated difficulty-aware optimization",
                "tow-stage training pipeline",
                "dynamic attribution systems",
                "Fineva",
                "FinEval",
                "FinanceIQ",
                "MATH-500",
                "GPQA-diamond",
                "Finova evaluation benchmark",
                "agent-level financial reasoning",
                "compliance verification"
            ]
        },
        "publishedAt": "2025-07-22T13:52:16.000Z",
        "title": "Agentar-Fin-R1: Enhancing Financial Intelligence through Domain\n  Expertise, Training Efficiency, and Advanced Reasoning",
        "summary": "Large Language Models (LLMs) exhibit considerable promise in financial\napplications; however, prevailing models frequently demonstrate limitations\nwhen confronted with scenarios that necessitate sophisticated reasoning\ncapabilities, stringent trustworthiness criteria, and efficient adaptation to\ndomain-specific requirements. We introduce the Agentar-Fin-R1 series of\nfinancial large language models (8B and 32B parameters), specifically\nengineered based on the Qwen3 foundation model to enhance reasoning\ncapabilities, reliability, and domain specialization for financial\napplications. Our optimization approach integrates a high-quality, systematic\nfinancial task label system with a comprehensive multi-layered trustworthiness\nassurance framework. This framework encompasses high-quality trustworthy\nknowledge engineering, multi-agent trustworthy data synthesis, and rigorous\ndata validation governance. Through label-guided automated difficulty-aware\noptimization, tow-stage training pipeline, and dynamic attribution systems, we\nachieve substantial improvements in training efficiency. Our models undergo\ncomprehensive evaluation on mainstream financial benchmarks including Fineva,\nFinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500\nand GPQA-diamond. To thoroughly assess real-world deployment capabilities, we\ninnovatively propose the Finova evaluation benchmark, which focuses on\nagent-level financial reasoning and compliance verification. Experimental\nresults demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art\nperformance on financial tasks but also exhibits exceptional general reasoning\ncapabilities, validating its effectiveness as a trustworthy solution for\nhigh-stakes financial applications. The Finova bench is available at\nhttps://github.com/antgroup/Finova.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63a369d98c0c89dcae3b8329/xiYbRb5V1RWOpZJz3yzrC.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16802.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1064
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.18565",
            "authors": [
                {
                    "_id": "688332fe846d2e78b7548b7c",
                    "user": {
                        "_id": "659d5355d09adee457236d53",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/y1W6Co4jIYB95Cx6Tjrsd.jpeg",
                        "isPro": true,
                        "fullname": "Muhammad Imran Zaman",
                        "user": "ImranzamanML",
                        "type": "user"
                    },
                    "name": "Muhammad Imran Zaman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T08:38:23.842Z",
                    "hidden": false
                },
                {
                    "_id": "688332fe846d2e78b7548b7d",
                    "name": "Nisar Ahmed",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T16:41:26.000Z",
            "submittedOnDailyAt": "2025-07-25T06:52:42.179Z",
            "title": "Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age\n  Estimation and Gender Classification for Targeted Advertisement",
            "submittedOnDailyBy": {
                "_id": "659d5355d09adee457236d53",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/y1W6Co4jIYB95Cx6Tjrsd.jpeg",
                "isPro": true,
                "fullname": "Muhammad Imran Zaman",
                "user": "ImranzamanML",
                "type": "user"
            },
            "summary": "This paper presents a novel deep learning-based approach for simultaneous age\nand gender classification from facial images, designed to enhance the\neffectiveness of targeted advertising campaigns. We propose a custom\nConvolutional Neural Network (CNN) architecture, optimized for both tasks,\nwhich leverages the inherent correlation between age and gender information\npresent in facial features. Unlike existing methods that often treat these\ntasks independently, our model learns shared representations, leading to\nimproved performance. The network is trained on a large, diverse dataset of\nfacial images, carefully pre-processed to ensure robustness against variations\nin lighting, pose, and image quality. Our experimental results demonstrate a\nsignificant improvement in gender classification accuracy, achieving 95%, and a\ncompetitive mean absolute error of 5.77 years for age estimation. Critically,\nwe analyze the performance across different age groups, identifying specific\nchallenges in accurately estimating the age of younger individuals. This\nanalysis reveals the need for targeted data augmentation and model refinement\nto address these biases. Furthermore, we explore the impact of different CNN\narchitectures and hyperparameter settings on the overall performance, providing\nvaluable insights for future research.",
            "upvotes": 1,
            "discussionId": "688332ff846d2e78b7548b7e",
            "ai_summary": "A custom CNN architecture simultaneously classifies age and gender from facial images, improving performance by learning shared representations and achieving high accuracy and low mean absolute error.",
            "ai_keywords": [
                "Convolutional Neural Network",
                "CNN",
                "age classification",
                "gender classification",
                "shared representations",
                "facial features",
                "data augmentation",
                "hyperparameter settings"
            ]
        },
        "publishedAt": "2025-07-24T12:41:26.000Z",
        "title": "Deep Learning-Based Age Estimation and Gender Deep Learning-Based Age\n  Estimation and Gender Classification for Targeted Advertisement",
        "summary": "This paper presents a novel deep learning-based approach for simultaneous age\nand gender classification from facial images, designed to enhance the\neffectiveness of targeted advertising campaigns. We propose a custom\nConvolutional Neural Network (CNN) architecture, optimized for both tasks,\nwhich leverages the inherent correlation between age and gender information\npresent in facial features. Unlike existing methods that often treat these\ntasks independently, our model learns shared representations, leading to\nimproved performance. The network is trained on a large, diverse dataset of\nfacial images, carefully pre-processed to ensure robustness against variations\nin lighting, pose, and image quality. Our experimental results demonstrate a\nsignificant improvement in gender classification accuracy, achieving 95%, and a\ncompetitive mean absolute error of 5.77 years for age estimation. Critically,\nwe analyze the performance across different age groups, identifying specific\nchallenges in accurately estimating the age of younger individuals. This\nanalysis reveals the need for targeted data augmentation and model refinement\nto address these biases. Furthermore, we explore the impact of different CNN\narchitectures and hyperparameter settings on the overall performance, providing\nvaluable insights for future research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18565.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "659d5355d09adee457236d53",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/y1W6Co4jIYB95Cx6Tjrsd.jpeg",
            "fullname": "Muhammad Imran Zaman",
            "name": "ImranzamanML",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 62
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.17402",
            "authors": [
                {
                    "_id": "688358b06d99b948a364d1b1",
                    "user": {
                        "_id": "688206eb4f947b9971eacc7a",
                        "avatarUrl": "/avatars/4b9238add0bef154aef9401b88b4a231.svg",
                        "isPro": false,
                        "fullname": "Jun Li",
                        "user": "JunLi2005",
                        "type": "user"
                    },
                    "name": "Li Jun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-25T13:36:14.709Z",
                    "hidden": false
                },
                {
                    "_id": "688358b06d99b948a364d1b2",
                    "name": "Wang Jinpeng",
                    "hidden": false
                },
                {
                    "_id": "688358b06d99b948a364d1b3",
                    "name": "Tan Chaolei",
                    "hidden": false
                },
                {
                    "_id": "688358b06d99b948a364d1b4",
                    "name": "Lian Niu",
                    "hidden": false
                },
                {
                    "_id": "688358b06d99b948a364d1b5",
                    "name": "Chen Long",
                    "hidden": false
                },
                {
                    "_id": "688358b06d99b948a364d1b6",
                    "name": "Zhang Min",
                    "hidden": false
                },
                {
                    "_id": "688358b06d99b948a364d1b7",
                    "name": "Wang Yaowei",
                    "hidden": false
                },
                {
                    "_id": "688358b06d99b948a364d1b8",
                    "name": "Xia Shu-Tao",
                    "hidden": false
                },
                {
                    "_id": "688358b06d99b948a364d1b9",
                    "name": "Chen Bin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-23T10:59:46.000Z",
            "submittedOnDailyAt": "2025-07-25T15:02:44.289Z",
            "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic\n  Learning",
            "submittedOnDailyBy": {
                "_id": "688206eb4f947b9971eacc7a",
                "avatarUrl": "/avatars/4b9238add0bef154aef9401b88b4a231.svg",
                "isPro": false,
                "fullname": "Jun Li",
                "user": "JunLi2005",
                "type": "user"
            },
            "summary": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.",
            "upvotes": 1,
            "discussionId": "688358b06d99b948a364d1ba",
            "projectPage": "https://github.com/lijun2005/ICCV25-HLFormer",
            "githubRepo": "https://github.com/lijun2005/ICCV25-HLFormer",
            "ai_summary": "HLFormer uses a hyperbolic modeling framework with Lorentz and Euclidean attention blocks to improve video-text retrieval by addressing hierarchical and partial relevance issues.",
            "ai_keywords": [
                "hyperbolic modeling",
                "HLFormer",
                "Lorentz Attention Block",
                "Euclidean Attention Block",
                "Mean-Guided Adaptive Interaction Module",
                "Partial Order Preservation Loss",
                "Lorentzian cone constraints",
                "cross-modal matching"
            ],
            "githubStars": 19
        },
        "publishedAt": "2025-07-23T06:59:46.000Z",
        "title": "HLFormer: Enhancing Partially Relevant Video Retrieval with Hyperbolic\n  Learning",
        "summary": "Partially Relevant Video Retrieval (PRVR) addresses the critical challenge of\nmatching untrimmed videos with text queries describing only partial content.\nExisting methods suffer from geometric distortion in Euclidean space that\nsometimes misrepresents the intrinsic hierarchical structure of videos and\noverlooks certain hierarchical semantics, ultimately leading to suboptimal\ntemporal modeling. To address this issue, we propose the first hyperbolic\nmodeling framework for PRVR, namely HLFormer, which leverages hyperbolic space\nlearning to compensate for the suboptimal hierarchical modeling capabilities of\nEuclidean space. Specifically, HLFormer integrates the Lorentz Attention Block\nand Euclidean Attention Block to encode video embeddings in hybrid spaces,\nusing the Mean-Guided Adaptive Interaction Module to dynamically fuse features.\nAdditionally, we introduce a Partial Order Preservation Loss to enforce \"text <\nvideo\" hierarchy through Lorentzian cone constraints. This approach further\nenhances cross-modal matching by reinforcing partial relevance between video\ncontent and text queries. Extensive experiments show that HLFormer outperforms\nstate-of-the-art methods. Code is released at\nhttps://github.com/lijun2005/ICCV25-HLFormer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17402.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "688206eb4f947b9971eacc7a",
            "avatarUrl": "/avatars/4b9238add0bef154aef9401b88b4a231.svg",
            "fullname": "Jun Li",
            "name": "JunLi2005",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.15807",
            "authors": [
                {
                    "_id": "6883fc897d7a19a208cdef55",
                    "name": "Shuo Chen",
                    "hidden": false
                },
                {
                    "_id": "6883fc897d7a19a208cdef56",
                    "name": "Jianzhe Liu",
                    "hidden": false
                },
                {
                    "_id": "6883fc897d7a19a208cdef57",
                    "name": "Zhen Han",
                    "hidden": false
                },
                {
                    "_id": "6883fc897d7a19a208cdef58",
                    "name": "Yan Xia",
                    "hidden": false
                },
                {
                    "_id": "6883fc897d7a19a208cdef59",
                    "name": "Daniel Cremers",
                    "hidden": false
                },
                {
                    "_id": "6883fc897d7a19a208cdef5a",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "6883fc897d7a19a208cdef5b",
                    "name": "Volker Tresp",
                    "hidden": false
                },
                {
                    "_id": "6883fc897d7a19a208cdef5c",
                    "name": "Jindong Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-21T17:08:18.000Z",
            "submittedOnDailyAt": "2025-07-25T20:23:36.567Z",
            "title": "True Multimodal In-Context Learning Needs Attention to the Visual\n  Context",
            "submittedOnDailyBy": {
                "_id": "648cbea3dee03837c823cbf2",
                "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
                "isPro": false,
                "fullname": "Shuo Chen",
                "user": "ShuoChen99",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .",
            "upvotes": 1,
            "discussionId": "6883fc897d7a19a208cdef5d"
        },
        "publishedAt": "2025-07-21T13:08:18.000Z",
        "title": "True Multimodal In-Context Learning Needs Attention to the Visual\n  Context",
        "summary": "Multimodal Large Language Models (MLLMs), built on powerful language\nbackbones, have enabled Multimodal In-Context Learning (MICL)-adapting to new\ntasks from a few multimodal demonstrations consisting of images, questions, and\nanswers. Despite showing noticeable improvement on standard vision-language\ndatasets, current MLLMs struggle to leverage visual information in the\ndemonstrations. Specifically, they tend to neglect visual cues and over-rely on\ntextual patterns, leading to mere text imitation rather than genuine multimodal\nadaptation. This behavior makes MICL still unimodal and largely restricts its\npractical utility. More importantly, this limitation is often concealed by the\nimproved performance on tasks that do not require understanding the visual\ncontext. As a result, how to effectively enhance MICL ability and reliably\nevaluate the MICL performance remains underexplored. To address these issues,\nwe first introduce Dynamic Attention Reallocation (DARA), an efficient\nfine-tuning strategy that encourages models to attend to the visual context by\nrebalancing attention across visual and textual tokens. In addition, we present\nTrueMICL, an MICL-dedicated dataset with both support and test sets that\nexplicitly requires the integration of multimodal information-particularly\nvisual content-for correct task completion. Extensive experiments demonstrate\nthe effectiveness of our holistic solution, showcasing substantial improvements\nin the true multimodal in-context learning capabilities. Code and datasets are\navailable at https://chenxshuo.github.io/true-micl-colm .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.15807.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "648cbea3dee03837c823cbf2",
            "avatarUrl": "/avatars/3f8c36436a5cbff2948df099ae604418.svg",
            "fullname": "Shuo Chen",
            "name": "ShuoChen99",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]