[
    {
        "paper": {
            "id": "2507.16075",
            "authors": [
                {
                    "_id": "688501d57d7a19a208cdf03a",
                    "name": "Rujun Han",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03b",
                    "name": "Yanfei Chen",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03c",
                    "name": "Zoey CuiZhu",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03d",
                    "name": "Lesly Miculicich",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03e",
                    "name": "Guan Sun",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf03f",
                    "name": "Yuanjun Bi",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf040",
                    "name": "Weiming Wen",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf041",
                    "name": "Hui Wan",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf042",
                    "name": "Chunfeng Wen",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf043",
                    "name": "Solène Maître",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf044",
                    "name": "George Lee",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf045",
                    "name": "Vishy Tirumalashetty",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf046",
                    "name": "Emily Xue",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf047",
                    "name": "Zizhao Zhang",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf048",
                    "name": "Salem Haykal",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf049",
                    "name": "Burak Gokturk",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf04a",
                    "name": "Tomas Pfister",
                    "hidden": false
                },
                {
                    "_id": "688501d57d7a19a208cdf04b",
                    "name": "Chen-Yu Lee",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-21T21:23:21.000Z",
            "submittedOnDailyAt": "2025-07-28T05:49:28.861Z",
            "title": "Deep Researcher with Test-Time Diffusion",
            "submittedOnDailyBy": {
                "_id": "62f32eab52ad88c930bb3f3b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
                "isPro": true,
                "fullname": "Asankhaya Sharma",
                "user": "codelion",
                "type": "user"
            },
            "summary": "Deep research agents, powered by Large Language Models (LLMs), are rapidly\nadvancing; yet, their performance often plateaus when generating complex,\nlong-form research reports using generic test-time scaling algorithms. Drawing\ninspiration from the iterative nature of human research, which involves cycles\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\nResearcher (TTD-DR). This novel framework conceptualizes research report\ngeneration as a diffusion process. TTD-DR initiates this process with a\npreliminary draft, an updatable skeleton that serves as an evolving foundation\nto guide the research direction. The draft is then iteratively refined through\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\nthat incorporates external information at each step. The core process is\nfurther enhanced by a self-evolutionary algorithm applied to each component of\nthe agentic workflow, ensuring the generation of high-quality context for the\ndiffusion process. This draft-centric design makes the report writing process\nmore timely and coherent while reducing information loss during the iterative\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\nresults on a wide array of benchmarks that require intensive search and\nmulti-hop reasoning, significantly outperforming existing deep research agents.",
            "upvotes": 25,
            "discussionId": "688501d67d7a19a208cdf04c",
            "githubRepo": "https://github.com/codelion/optillm/tree/main/optillm/plugins/deep_research",
            "ai_summary": "The Test-Time Diffusion Deep Researcher (TTD-DR) framework uses a diffusion process with iterative refinement and external information retrieval to generate high-quality research reports, outperforming existing methods.",
            "ai_keywords": [
                "Test-Time Diffusion Deep Researcher",
                "TTD-DR",
                "diffusion process",
                "preliminary draft",
                "denoising process",
                "retrieval mechanism",
                "self-evolutionary algorithm",
                "agentic workflow",
                "iterative search",
                "multi-hop reasoning"
            ],
            "githubStars": 2666
        },
        "publishedAt": "2025-07-21T17:23:21.000Z",
        "title": "Deep Researcher with Test-Time Diffusion",
        "summary": "Deep research agents, powered by Large Language Models (LLMs), are rapidly\nadvancing; yet, their performance often plateaus when generating complex,\nlong-form research reports using generic test-time scaling algorithms. Drawing\ninspiration from the iterative nature of human research, which involves cycles\nof searching, reasoning, and revision, we propose the Test-Time Diffusion Deep\nResearcher (TTD-DR). This novel framework conceptualizes research report\ngeneration as a diffusion process. TTD-DR initiates this process with a\npreliminary draft, an updatable skeleton that serves as an evolving foundation\nto guide the research direction. The draft is then iteratively refined through\na \"denoising\" process, which is dynamically informed by a retrieval mechanism\nthat incorporates external information at each step. The core process is\nfurther enhanced by a self-evolutionary algorithm applied to each component of\nthe agentic workflow, ensuring the generation of high-quality context for the\ndiffusion process. This draft-centric design makes the report writing process\nmore timely and coherent while reducing information loss during the iterative\nsearch process. We demonstrate that our TTD-DR achieves state-of-the-art\nresults on a wide array of benchmarks that require intensive search and\nmulti-hop reasoning, significantly outperforming existing deep research agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16075.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f32eab52ad88c930bb3f3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677134945205-62f32eab52ad88c930bb3f3b.png",
            "fullname": "Asankhaya Sharma",
            "name": "codelion",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 154
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.18553",
            "authors": [
                {
                    "_id": "688389bb5d77c97374117c30",
                    "user": {
                        "_id": "6298d8dab58e71e2ac9e2967",
                        "avatarUrl": "/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg",
                        "isPro": false,
                        "fullname": "Jiale Chen",
                        "user": "softmax",
                        "type": "user"
                    },
                    "name": "Jiale Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-28T08:45:38.736Z",
                    "hidden": false
                },
                {
                    "_id": "688389bb5d77c97374117c31",
                    "name": "Torsten Hoefler",
                    "hidden": false
                },
                {
                    "_id": "688389bb5d77c97374117c32",
                    "name": "Dan Alistarh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T16:22:18.000Z",
            "submittedOnDailyAt": "2025-07-28T08:41:14.809Z",
            "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm",
            "submittedOnDailyBy": {
                "_id": "6298d8dab58e71e2ac9e2967",
                "avatarUrl": "/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg",
                "isPro": false,
                "fullname": "Jiale Chen",
                "user": "softmax",
                "type": "user"
            },
            "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models.",
            "upvotes": 19,
            "discussionId": "688389bb5d77c97374117c33"
        },
        "publishedAt": "2025-07-24T12:22:18.000Z",
        "title": "The Geometry of LLM Quantization: GPTQ as Babai's Nearest Plane\n  Algorithm",
        "summary": "Quantizing the weights of large language models (LLMs) from 16-bit to lower\nbitwidth is the de facto approach to deploy massive transformers onto more\naffordable accelerators. GPTQ emerged as one of the standard methods for\none-shot post-training quantization at LLM scale. Yet, its inner workings are\ndescribed as a sequence of ad-hoc algebraic updates that obscure any geometric\nmeaning or worst-case guarantees. In this work, we show that, when executed\nback-to-front (from the last to first dimension) for a linear layer, GPTQ is\nmathematically identical to Babai's nearest plane algorithm for the classical\nclosest vector problem (CVP) on a lattice defined by the Hessian matrix of the\nlayer's inputs. This equivalence is based on a sophisticated mathematical\nargument, and has two analytical consequences: (i) the GPTQ error propagation\nstep gains an intuitive geometric interpretation; (ii) GPTQ inherits the error\nupper bound of Babai's algorithm under the no-clipping condition. Taken\ntogether, these results place GPTQ on firm theoretical footing and open the\ndoor to importing decades of progress in lattice algorithms towards the design\nof future quantization algorithms for billion-parameter models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18553.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6298d8dab58e71e2ac9e2967",
            "avatarUrl": "/avatars/99fec4ba78ab61c5952f51e6ebf03ffa.svg",
            "fullname": "Jiale Chen",
            "name": "softmax",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.19478",
            "authors": [
                {
                    "_id": "68877a6bc8db48c925156eec",
                    "name": "Xuehui Wang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156eed",
                    "name": "Zhenyu Wu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156eee",
                    "name": "JingJing Xie",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156eef",
                    "name": "Zichen Ding",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef0",
                    "name": "Bowen Yang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef1",
                    "name": "Zehao Li",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef2",
                    "name": "Zhaoyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef3",
                    "name": "Qingyun Li",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef4",
                    "name": "Xuan Dong",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef5",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef6",
                    "name": "Weiyun Wang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef7",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef8",
                    "name": "Jixuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156ef9",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efa",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efb",
                    "name": "Chenyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efc",
                    "name": "Shiqian Su",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efd",
                    "name": "Yue Yu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156efe",
                    "name": "Yuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156eff",
                    "name": "Yiqian Liu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f00",
                    "name": "Xiao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f01",
                    "name": "Yanting Zhang",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f02",
                    "name": "Xiangyu Yue",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f03",
                    "name": "Weijie Su",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f04",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f05",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f06",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "68877a6bc8db48c925156f07",
                    "name": "Wenhai Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-25T17:59:26.000Z",
            "submittedOnDailyAt": "2025-07-28T12:29:10.972Z",
            "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI\n  Agents",
            "submittedOnDailyBy": {
                "_id": "649cf4ecdd87dd9ef76fe020",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/M7RpD_AcNewA2xADhhyCB.jpeg",
                "isPro": false,
                "fullname": "Xuehui Wang",
                "user": "huiserwang",
                "type": "user"
            },
            "summary": "We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\nplatforms. It comprises four levels: GUI Content Understanding, Element\nGrounding, Task Automation, and Task Collaboration, covering essential skills\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\nmetric to assess GUI agent execution efficiency in online automation scenarios.\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\ndeterminant of overall task success, emphasizing the substantial benefits of\nmodular frameworks that integrate specialized grounding modules. Furthermore,\nto achieve reliable GUI automation, an agent requires strong task planning and\ncross-platform generalization abilities, with long-context memory, a broad\naction space, and long-term reasoning playing a critical role. More important,\ntask efficiency remains a critically underexplored dimension, and all models\nsuffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration of precise localization,\neffective planning, and early stopping strategies is indispensable to enable\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\ndata, and running environment will be publicly available at\nhttps://github.com/open-compass/MMBench-GUI.",
            "upvotes": 14,
            "discussionId": "68877a6bc8db48c925156f08",
            "ai_summary": "A hierarchical benchmark evaluates GUI automation agents across multiple platforms, emphasizing key skills such as visual grounding, task planning, and efficiency.",
            "ai_keywords": [
                "GUI Content Understanding",
                "Element Grounding",
                "Task Automation",
                "Task Collaboration",
                "Efficiency-Quality Area",
                "EQA",
                "modular frameworks",
                "specialized grounding modules",
                "long-context memory",
                "broad action space",
                "long-term reasoning",
                "precise localization",
                "effective planning",
                "early stopping strategies"
            ]
        },
        "publishedAt": "2025-07-25T13:59:26.000Z",
        "title": "MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI\n  Agents",
        "summary": "We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI\nautomation agents across Windows, macOS, Linux, iOS, Android, and Web\nplatforms. It comprises four levels: GUI Content Understanding, Element\nGrounding, Task Automation, and Task Collaboration, covering essential skills\nfor GUI agents. In addition, we propose a novel Efficiency-Quality Area (EQA)\nmetric to assess GUI agent execution efficiency in online automation scenarios.\nThrough MMBench-GUI, we identify accurate visual grounding as a critical\ndeterminant of overall task success, emphasizing the substantial benefits of\nmodular frameworks that integrate specialized grounding modules. Furthermore,\nto achieve reliable GUI automation, an agent requires strong task planning and\ncross-platform generalization abilities, with long-context memory, a broad\naction space, and long-term reasoning playing a critical role. More important,\ntask efficiency remains a critically underexplored dimension, and all models\nsuffer from substantial inefficiencies, with excessive redundant steps even\nwhen tasks are ultimately completed. The integration of precise localization,\neffective planning, and early stopping strategies is indispensable to enable\ntruly efficient and scalable GUI automation. Our benchmark code, evaluation\ndata, and running environment will be publicly available at\nhttps://github.com/open-compass/MMBench-GUI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.19478.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649cf4ecdd87dd9ef76fe020",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/M7RpD_AcNewA2xADhhyCB.jpeg",
            "fullname": "Xuehui Wang",
            "name": "huiserwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.18392",
            "authors": [
                {
                    "_id": "68878f837e66e00ef8c1cbd3",
                    "name": "Asaf Yehudai",
                    "hidden": false
                },
                {
                    "_id": "68878f837e66e00ef8c1cbd4",
                    "name": "Lilach Eden",
                    "hidden": false
                },
                {
                    "_id": "68878f837e66e00ef8c1cbd5",
                    "name": "Yotam Perlitz",
                    "hidden": false
                },
                {
                    "_id": "68878f837e66e00ef8c1cbd6",
                    "name": "Roy Bar-Haim",
                    "hidden": false
                },
                {
                    "_id": "68878f837e66e00ef8c1cbd7",
                    "name": "Michal Shmueli-Scheuer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T13:15:21.000Z",
            "submittedOnDailyAt": "2025-07-28T13:26:30.022Z",
            "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
            "submittedOnDailyBy": {
                "_id": "638324f862badff43269e588",
                "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
                "isPro": false,
                "fullname": "Asaf Yehudai",
                "user": "Asaf-Yehudai",
                "type": "user"
            },
            "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.",
            "upvotes": 8,
            "discussionId": "68878f837e66e00ef8c1cbd8",
            "githubRepo": "https://github.com/IBM/CLEAR",
            "ai_summary": "CLEAR is an interactive open-source package that provides detailed error analysis by generating per-instance feedback and system-level error issues, aiding in understanding and improving the performance of Large Language Models.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "CLEAR",
                "error analysis",
                "per-instance feedback",
                "system-level error issues",
                "interactive dashboard",
                "aggregate visualizations",
                "interactive filters",
                "RAG benchmarks",
                "Math benchmarks"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-07-24T09:15:21.000Z",
        "title": "CLEAR: Error Analysis via LLM-as-a-Judge Made Easy",
        "summary": "The evaluation of Large Language Models (LLMs) increasingly relies on other\nLLMs acting as judges. However, current evaluation paradigms typically yield a\nsingle score or ranking, answering which model is better but not why. While\nessential for benchmarking, these top-level scores obscure the specific,\nactionable reasons behind a model's performance. To bridge this gap, we\nintroduce CLEAR, an interactive, open-source package for LLM-based error\nanalysis. CLEAR first generates per-instance textual feedback, then it creates\na set of system-level error issues, and quantifies the prevalence of each\nidentified issue. Our package also provides users with an interactive dashboard\nthat allows for a comprehensive error analysis through aggregate\nvisualizations, applies interactive filters to isolate specific issues or score\nranges, and drills down to the individual instances that exemplify a particular\nbehavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,\nand showcase its utility through a user case study.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18392.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638324f862badff43269e588",
            "avatarUrl": "/avatars/907a39a9b44fc8b7f3fad35858b01fb7.svg",
            "fullname": "Asaf Yehudai",
            "name": "Asaf-Yehudai",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.20198",
            "authors": [
                {
                    "_id": "68881faaaf872d625c10c58b",
                    "name": "Kele Shao",
                    "hidden": false
                },
                {
                    "_id": "68881faaaf872d625c10c58c",
                    "name": "Keda Tao",
                    "hidden": false
                },
                {
                    "_id": "68881faaaf872d625c10c58d",
                    "name": "Kejia Zhang",
                    "hidden": false
                },
                {
                    "_id": "68881faaaf872d625c10c58e",
                    "name": "Sicheng Feng",
                    "hidden": false
                },
                {
                    "_id": "68881faaaf872d625c10c58f",
                    "name": "Mu Cai",
                    "hidden": false
                },
                {
                    "_id": "68881faaaf872d625c10c590",
                    "name": "Yuzhang Shang",
                    "hidden": false
                },
                {
                    "_id": "68881faaaf872d625c10c591",
                    "name": "Haoxuan You",
                    "hidden": false
                },
                {
                    "_id": "68881faaaf872d625c10c592",
                    "name": "Can Qin",
                    "hidden": false
                },
                {
                    "_id": "68881faaaf872d625c10c593",
                    "name": "Yang Sui",
                    "hidden": false
                },
                {
                    "_id": "68881faaaf872d625c10c594",
                    "name": "Huan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-27T09:33:56.000Z",
            "submittedOnDailyAt": "2025-07-28T23:44:18.131Z",
            "title": "When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token\n  Compression across Images, Videos, and Audios",
            "submittedOnDailyBy": {
                "_id": "65ce28c6340c3e914285aa58",
                "avatarUrl": "/avatars/ffaa6d6ce92274bff960f8ea229a37f8.svg",
                "isPro": false,
                "fullname": "Keda TAO",
                "user": "KD-TAO",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have made remarkable strides,\nlargely driven by their ability to process increasingly long and complex\ncontexts, such as high-resolution images, extended video sequences, and lengthy\naudio input. While this ability significantly enhances MLLM capabilities, it\nintroduces substantial computational challenges, primarily due to the quadratic\ncomplexity of self-attention mechanisms with numerous input tokens. To mitigate\nthese bottlenecks, token compression has emerged as an auspicious and critical\napproach, efficiently reducing the number of tokens during both training and\ninference. In this paper, we present the first systematic survey and synthesis\nof the burgeoning field of multimodal long context token compression.\nRecognizing that effective compression strategies are deeply tied to the unique\ncharacteristics and redundancies of each modality, we categorize existing\napproaches by their primary data focus, enabling researchers to quickly access\nand learn methods tailored to their specific area of interest: (1)\nimage-centric compression, which addresses spatial redundancy in visual data;\n(2) video-centric compression, which tackles spatio-temporal redundancy in\ndynamic sequences; and (3) audio-centric compression, which handles temporal\nand spectral redundancy in acoustic signals. Beyond this modality-driven\ncategorization, we further dissect methods based on their underlying\nmechanisms, including transformation-based, similarity-based, attention-based,\nand query-based approaches. By providing a comprehensive and structured\noverview, this survey aims to consolidate current progress, identify key\nchallenges, and inspire future research directions in this rapidly evolving\ndomain. We also maintain a public repository to continuously track and update\nthe latest advances in this promising area.",
            "upvotes": 5,
            "discussionId": "68881faaaf872d625c10c595",
            "ai_summary": "A survey examines methodologies for compressing tokens in multimodal large language models to manage computational demands associated with long contexts in images, videos, and audio.",
            "ai_keywords": [
                "multimodal large language models",
                "self-attention mechanisms",
                "token compression",
                "image-centric compression",
                "video-centric compression",
                "audio-centric compression",
                "transformation-based",
                "similarity-based",
                "attention-based",
                "query-based"
            ]
        },
        "publishedAt": "2025-07-27T05:33:56.000Z",
        "title": "When Tokens Talk Too Much: A Survey of Multimodal Long-Context Token\n  Compression across Images, Videos, and Audios",
        "summary": "Multimodal large language models (MLLMs) have made remarkable strides,\nlargely driven by their ability to process increasingly long and complex\ncontexts, such as high-resolution images, extended video sequences, and lengthy\naudio input. While this ability significantly enhances MLLM capabilities, it\nintroduces substantial computational challenges, primarily due to the quadratic\ncomplexity of self-attention mechanisms with numerous input tokens. To mitigate\nthese bottlenecks, token compression has emerged as an auspicious and critical\napproach, efficiently reducing the number of tokens during both training and\ninference. In this paper, we present the first systematic survey and synthesis\nof the burgeoning field of multimodal long context token compression.\nRecognizing that effective compression strategies are deeply tied to the unique\ncharacteristics and redundancies of each modality, we categorize existing\napproaches by their primary data focus, enabling researchers to quickly access\nand learn methods tailored to their specific area of interest: (1)\nimage-centric compression, which addresses spatial redundancy in visual data;\n(2) video-centric compression, which tackles spatio-temporal redundancy in\ndynamic sequences; and (3) audio-centric compression, which handles temporal\nand spectral redundancy in acoustic signals. Beyond this modality-driven\ncategorization, we further dissect methods based on their underlying\nmechanisms, including transformation-based, similarity-based, attention-based,\nand query-based approaches. By providing a comprehensive and structured\noverview, this survey aims to consolidate current progress, identify key\nchallenges, and inspire future research directions in this rapidly evolving\ndomain. We also maintain a public repository to continuously track and update\nthe latest advances in this promising area.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.20198.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65ce28c6340c3e914285aa58",
            "avatarUrl": "/avatars/ffaa6d6ce92274bff960f8ea229a37f8.svg",
            "fullname": "Keda TAO",
            "name": "KD-TAO",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.17596",
            "authors": [
                {
                    "_id": "688223086a54dd1e77daa947",
                    "user": {
                        "_id": "641862440956be7233a1788f",
                        "avatarUrl": "/avatars/8710a44806a98f44d36b60814144186a.svg",
                        "isPro": false,
                        "fullname": "Maciej Wozniak",
                        "user": "maciejw94",
                        "type": "user"
                    },
                    "name": "Maciej K. Wozniak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-24T14:11:19.043Z",
                    "hidden": false
                },
                {
                    "_id": "688223086a54dd1e77daa948",
                    "name": "Lianhang Liu",
                    "hidden": false
                },
                {
                    "_id": "688223086a54dd1e77daa949",
                    "name": "Yixi Cai",
                    "hidden": false
                },
                {
                    "_id": "688223086a54dd1e77daa94a",
                    "name": "Patric Jensfelt",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-23T15:28:23.000Z",
            "submittedOnDailyAt": "2025-07-28T05:33:52.120Z",
            "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
            "submittedOnDailyBy": {
                "_id": "641862440956be7233a1788f",
                "avatarUrl": "/avatars/8710a44806a98f44d36b60814144186a.svg",
                "isPro": false,
                "fullname": "Maciej Wozniak",
                "user": "maciejw94",
                "type": "user"
            },
            "summary": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.",
            "upvotes": 3,
            "discussionId": "688223086a54dd1e77daa94b",
            "ai_summary": "PRIX, an end-to-end driving architecture using only camera data, achieves state-of-the-art performance with a Context-aware Recalibration Transformer, outperforming larger multimodal planners in efficiency and scalability.",
            "ai_keywords": [
                "end-to-end driving architecture",
                "Context-aware Recalibration Transformer",
                "CaRT",
                "NavSim",
                "nuScenes",
                "BEV feature representations",
                "LiDAR",
                "visual feature extractor",
                "generative planning head",
                "safe trajectories",
                "raw pixel inputs"
            ]
        },
        "publishedAt": "2025-07-23T11:28:23.000Z",
        "title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
        "summary": "While end-to-end autonomous driving models show promising results, their\npractical deployment is often hindered by large model sizes, a reliance on\nexpensive LiDAR sensors and computationally intensive BEV feature\nrepresentations. This limits their scalability, especially for mass-market\nvehicles equipped only with cameras. To address these challenges, we propose\nPRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving\narchitecture operates using only camera data, without explicit BEV\nrepresentation and forgoing the need for LiDAR. PRIX leverages a visual feature\nextractor coupled with a generative planning head to predict safe trajectories\nfrom raw pixel inputs directly. A core component of our architecture is the\nContext-aware Recalibration Transformer (CaRT), a novel module designed to\neffectively enhance multi-level visual features for more robust planning. We\ndemonstrate through comprehensive experiments that PRIX achieves\nstate-of-the-art performance on the NavSim and nuScenes benchmarks, matching\nthe capabilities of larger, multimodal diffusion planners while being\nsignificantly more efficient in terms of inference speed and model size, making\nit a practical solution for real-world deployment. Our work is open-source and\nthe code will be at https://maxiuw.github.io/prix.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17596.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641862440956be7233a1788f",
            "avatarUrl": "/avatars/8710a44806a98f44d36b60814144186a.svg",
            "fullname": "Maciej Wozniak",
            "name": "maciejw94",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.19457",
            "authors": [
                {
                    "_id": "6887e6b5af872d625c10c570",
                    "name": "Lakshya A Agrawal",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c571",
                    "name": "Shangyin Tan",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c572",
                    "name": "Dilara Soylu",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c573",
                    "name": "Noah Ziems",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c574",
                    "name": "Rishi Khare",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c575",
                    "name": "Krista Opsahl-Ong",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c576",
                    "name": "Arnav Singhvi",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c577",
                    "name": "Herumb Shandilya",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c578",
                    "name": "Michael J Ryan",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c579",
                    "name": "Meng Jiang",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c57a",
                    "name": "Christopher Potts",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c57b",
                    "name": "Koushik Sen",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c57c",
                    "name": "Alexandros G. Dimakis",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c57d",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c57e",
                    "name": "Dan Klein",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c57f",
                    "name": "Matei Zaharia",
                    "hidden": false
                },
                {
                    "_id": "6887e6b5af872d625c10c580",
                    "name": "Omar Khattab",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63bf0cbb24e75d1efdfdf8bd/WXZuTFrFSYd5KOvn6HkT5.png"
            ],
            "publishedAt": "2025-07-25T17:42:32.000Z",
            "submittedOnDailyAt": "2025-07-28T19:38:29.783Z",
            "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "63bf0cbb24e75d1efdfdf8bd",
                "avatarUrl": "/avatars/3bb13e989db31820734cbe6b13be130a.svg",
                "isPro": false,
                "fullname": "Lakshya A Agrawal",
                "user": "LakshyAAAgrawal",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.",
            "upvotes": 2,
            "discussionId": "6887e6b6af872d625c10c581",
            "ai_summary": "GEPA, an RL-based prompt optimizer using natural language reflection, achieves superior performance compared to GRPO and MIPROv2, requiring fewer rollouts and showing promise in code optimization.",
            "ai_keywords": [
                "Group Relative Policy Optimization",
                "GRPO",
                "Genetic-Pareto",
                "GEPA",
                "prompt optimizer",
                "system-level trajectories",
                "natural language reflection",
                "Pareto frontier",
                "reinforcement learning",
                "Large language models",
                "LLMs",
                "code optimization"
            ]
        },
        "publishedAt": "2025-07-25T13:42:32.000Z",
        "title": "GEPA: Reflective Prompt Evolution Can Outperform Reinforcement Learning",
        "summary": "Large language models (LLMs) are increasingly adapted to downstream tasks via\nreinforcement learning (RL) methods like Group Relative Policy Optimization\n(GRPO), which often require thousands of rollouts to learn new tasks. We argue\nthat the interpretable nature of language can often provide a much richer\nlearning medium for LLMs, compared with policy gradients derived from sparse,\nscalar rewards. To test this, we introduce GEPA (Genetic-Pareto), a prompt\noptimizer that thoroughly incorporates natural language reflection to learn\nhigh-level rules from trial and error. Given any AI system containing one or\nmore LLM prompts, GEPA samples system-level trajectories (e.g., reasoning, tool\ncalls, and tool outputs) and reflects on them in natural language to diagnose\nproblems, propose and test prompt updates, and combine complementary lessons\nfrom the Pareto frontier of its own attempts. As a result of GEPA's design, it\ncan often turn even just a few rollouts into a large quality gain. Across four\ntasks, GEPA outperforms GRPO by 10% on average and by up to 20%, while using up\nto 35x fewer rollouts. GEPA also outperforms the leading prompt optimizer,\nMIPROv2, by over 10% across two LLMs, and demonstrates promising results as an\ninference-time search strategy for code optimization.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63bf0cbb24e75d1efdfdf8bd/WXZuTFrFSYd5KOvn6HkT5.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.19457.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63bf0cbb24e75d1efdfdf8bd",
            "avatarUrl": "/avatars/3bb13e989db31820734cbe6b13be130a.svg",
            "fullname": "Lakshya A Agrawal",
            "name": "LakshyAAAgrawal",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.18742",
            "authors": [
                {
                    "_id": "688728d6a5a841b139945452",
                    "user": {
                        "_id": "5fad8602b8423e1d80b8a965",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
                        "isPro": false,
                        "fullname": "Victor Gallego",
                        "user": "vicgalle",
                        "type": "user"
                    },
                    "name": "Víctor Gallego",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-28T08:44:19.745Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T18:44:28.000Z",
            "submittedOnDailyAt": "2025-07-28T06:09:19.276Z",
            "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking\n  Through Test-Time Refinement",
            "submittedOnDailyBy": {
                "_id": "5fad8602b8423e1d80b8a965",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
                "isPro": false,
                "fullname": "Victor Gallego",
                "user": "vicgalle",
                "type": "user"
            },
            "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .",
            "upvotes": 2,
            "discussionId": "688728d7a5a841b139945453",
            "githubRepo": "https://github.com/vicgalle/specification-self-correction",
            "ai_summary": "A new framework called Specification Self-Correction allows language models to dynamically correct flawed instructions during inference, reducing reward hacking vulnerabilities.",
            "ai_keywords": [
                "language models",
                "reward hacking",
                "Specification Self-Correction",
                "multi-step inference",
                "self-corrected specification"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-07-24T14:44:28.000Z",
        "title": "Specification Self-Correction: Mitigating In-Context Reward Hacking\n  Through Test-Time Refinement",
        "summary": "Language models (LMs) are susceptible to in-context reward hacking, where\nthey exploit flaws in tainted or faulty written specifications or rubrics to\nachieve high scores without fulfilling the user's true intent. We introduce\nSpecification Self-Correction (SSC), a novel, test-time framework that enables\nan LM to identify and correct flaws within its own guiding specification. SSC\nemploys a multi-step inference process where the model first generates a\nresponse based on a potentially tainted specification, critiques its output,\nand then revises the specification itself to remove the exploitable loophole. A\nfinal, more robust response is then generated using this self-corrected\nspecification. Across experiments spanning creative writing and agentic coding\ntasks with several LMs, we demonstrate that while models initially game tainted\nspecifications in 50-70\\% of cases, the SSC process reduces this vulnerability\nby over 90\\%. This dynamic repair occurs at inference time, requires no weight\nmodification, and leads to more robustly aligned model behavior. Code at\nhttps://github.com/vicgalle/specification-self-correction .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.18742.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5fad8602b8423e1d80b8a965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fad8602b8423e1d80b8a965/tRqTwcZmrGka8c1vFq2wX.jpeg",
            "fullname": "Victor Gallego",
            "name": "vicgalle",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 134
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.10510",
            "authors": [
                {
                    "_id": "6886fab77d7a19a208cdf212",
                    "user": {
                        "_id": "67e43f9b29d49781111e4013",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e43f9b29d49781111e4013/UXMNGH3-PsVEE3oazmGx2.jpeg",
                        "isPro": false,
                        "fullname": "Jiangkai",
                        "user": "keyonN",
                        "type": "user"
                    },
                    "name": "Jiangkai Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-28T08:44:35.831Z",
                    "hidden": false
                },
                {
                    "_id": "6886fab77d7a19a208cdf213",
                    "name": "Zhiyuan Ren",
                    "hidden": false
                },
                {
                    "_id": "6886fab77d7a19a208cdf214",
                    "name": "Liming Liu",
                    "hidden": false
                },
                {
                    "_id": "6886fab77d7a19a208cdf215",
                    "name": "Xinggong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-14T17:34:49.000Z",
            "submittedOnDailyAt": "2025-07-28T02:51:56.268Z",
            "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from\n  Human to AI",
            "submittedOnDailyBy": {
                "_id": "67e43f9b29d49781111e4013",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e43f9b29d49781111e4013/UXMNGH3-PsVEE3oazmGx2.jpeg",
                "isPro": false,
                "fullname": "Jiangkai",
                "user": "keyonN",
                "type": "user"
            },
            "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.",
            "upvotes": 2,
            "discussionId": "6886fab77d7a19a208cdf216",
            "ai_summary": "Artic addresses latency issues in AI Video Chat by optimizing video streaming and frame rate adaptation to enhance MLLM accuracy and reduce bitrate.",
            "ai_keywords": [
                "Multimodal Large Language Model",
                "MLLM",
                "Context-Aware Video Streaming",
                "Loss-Resilient Adaptive Frame Rate",
                "Degraded Video Understanding Benchmark",
                "DeViBench"
            ]
        },
        "publishedAt": "2025-07-14T13:34:49.000Z",
        "title": "Chat with AI: The Surprising Turn of Real-time Video Communication from\n  Human to AI",
        "summary": "AI Video Chat emerges as a new paradigm for Real-time Communication (RTC),\nwhere one peer is not a human, but a Multimodal Large Language Model (MLLM).\nThis makes interaction between humans and AI more intuitive, as if chatting\nface-to-face with a real person. However, this poses significant challenges to\nlatency, because the MLLM inference takes up most of the response time, leaving\nvery little time for video streaming. Due to network uncertainty and\ninstability, transmission latency becomes a critical bottleneck preventing AI\nfrom being like a real person. To address this, we propose Artic, an\nAI-oriented Real-time Communication framework, exploring the network\nrequirement shift from \"humans watching video\" to \"AI understanding video\". To\nreduce bitrate dramatically while maintaining MLLM accuracy, we propose\nContext-Aware Video Streaming that recognizes the importance of each video\nregion for chat and allocates bitrate almost exclusively to chat-important\nregions. To avoid packet retransmission, we propose Loss-Resilient Adaptive\nFrame Rate that leverages previous frames to substitute for lost/delayed frames\nwhile avoiding bitrate waste. To evaluate the impact of video streaming quality\non MLLM accuracy, we build the first benchmark, named Degraded Video\nUnderstanding Benchmark (DeViBench). Finally, we discuss some open questions\nand ongoing solutions for AI Video Chat.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.10510.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67e43f9b29d49781111e4013",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67e43f9b29d49781111e4013/UXMNGH3-PsVEE3oazmGx2.jpeg",
            "fullname": "Jiangkai",
            "name": "keyonN",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.16534",
            "authors": [
                {
                    "_id": "6887df72af872d625c10c548",
                    "name": "Shanghai AI Lab",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c54a",
                    "name": "Xiaoyang Chen",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c54b",
                    "name": "Yunhao Chen",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c54c",
                    "name": "Zeren Chen",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c54d",
                    "name": "Zhiyun Chen",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c54e",
                    "name": "Hanyun Cui",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c54f",
                    "name": "Yawen Duan",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c550",
                    "name": "Jiaxuan Guo",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c551",
                    "name": "Qi Guo",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c552",
                    "name": "Xuhao Hu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c553",
                    "name": "Hong Huang",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c554",
                    "name": "Lige Huang",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c555",
                    "name": "Chunxiao Li",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c556",
                    "name": "Juncheng Li",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c557",
                    "name": "Qihao Lin",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c558",
                    "name": "Dongrui Liu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c559",
                    "name": "Xinmin Liu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c55a",
                    "name": "Zicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c55b",
                    "name": "Chaochao Lu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c55c",
                    "name": "Xiaoya Lu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c55d",
                    "name": "Jingjing Qu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c55e",
                    "name": "Qibing Ren",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c55f",
                    "name": "Jing Shao",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c560",
                    "name": "Jingwei Shi",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c561",
                    "name": "Jingwei Sun",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c562",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c563",
                    "name": "Weibing Wang",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c564",
                    "name": "Jia Xu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c565",
                    "name": "Lewen Yan",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c566",
                    "name": "Xiao Yu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c567",
                    "name": "Yi Yu",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c568",
                    "name": "Boxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c569",
                    "name": "Jie Zhang",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c56a",
                    "name": "Weichen Zhang",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c56b",
                    "name": "Zhijie Zheng",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c56c",
                    "name": "Tianyi Zhou",
                    "hidden": false
                },
                {
                    "_id": "6887df72af872d625c10c56d",
                    "name": "Bowen Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-22T12:44:38.000Z",
            "submittedOnDailyAt": "2025-07-28T19:18:06.661Z",
            "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis\n  Technical Report",
            "submittedOnDailyBy": {
                "_id": "63a369d98c0c89dcae3b8329",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
                "isPro": true,
                "fullname": "Adina Yakefu",
                "user": "AdinaY",
                "type": "user"
            },
            "summary": "To understand and identify the unprecedented risks posed by rapidly advancing\nartificial intelligence (AI) models, this report presents a comprehensive\nassessment of their frontier risks. Drawing on the E-T-C analysis (deployment\nenvironment, threat source, enabling capability) from the Frontier AI Risk\nManagement Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks\nin seven areas: cyber offense, biological and chemical risks, persuasion and\nmanipulation, uncontrolled autonomous AI R\\&D, strategic deception and\nscheming, self-replication, and collusion. Guided by the \"AI-45^circ Law,\"\nwe evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow\nlines\" (early warning indicators) to define risk zones: green (manageable risk\nfor routine deployment and continuous monitoring), yellow (requiring\nstrengthened mitigations and controlled deployment), and red (necessitating\nsuspension of development and/or deployment). Experimental results show that\nall recent frontier AI models reside in green and yellow zones, without\ncrossing red lines. Specifically, no evaluated models cross the yellow line for\ncyber offense or uncontrolled AI R\\&D risks. For self-replication, and\nstrategic deception and scheming, most models remain in the green zone, except\nfor certain reasoning models in the yellow zone. In persuasion and\nmanipulation, most models are in the yellow zone due to their effective\ninfluence on humans. For biological and chemical risks, we are unable to rule\nout the possibility of most models residing in the yellow zone, although\ndetailed threat modeling and in-depth assessment are required to make further\nclaims. This work reflects our current understanding of AI frontier risks and\nurges collective action to mitigate these challenges.",
            "upvotes": 1,
            "discussionId": "6887df73af872d625c10c56e"
        },
        "publishedAt": "2025-07-22T08:44:38.000Z",
        "title": "Frontier AI Risk Management Framework in Practice: A Risk Analysis\n  Technical Report",
        "summary": "To understand and identify the unprecedented risks posed by rapidly advancing\nartificial intelligence (AI) models, this report presents a comprehensive\nassessment of their frontier risks. Drawing on the E-T-C analysis (deployment\nenvironment, threat source, enabling capability) from the Frontier AI Risk\nManagement Framework (v1.0) (SafeWork-F1-Framework), we identify critical risks\nin seven areas: cyber offense, biological and chemical risks, persuasion and\nmanipulation, uncontrolled autonomous AI R\\&D, strategic deception and\nscheming, self-replication, and collusion. Guided by the \"AI-45^circ Law,\"\nwe evaluate these risks using \"red lines\" (intolerable thresholds) and \"yellow\nlines\" (early warning indicators) to define risk zones: green (manageable risk\nfor routine deployment and continuous monitoring), yellow (requiring\nstrengthened mitigations and controlled deployment), and red (necessitating\nsuspension of development and/or deployment). Experimental results show that\nall recent frontier AI models reside in green and yellow zones, without\ncrossing red lines. Specifically, no evaluated models cross the yellow line for\ncyber offense or uncontrolled AI R\\&D risks. For self-replication, and\nstrategic deception and scheming, most models remain in the green zone, except\nfor certain reasoning models in the yellow zone. In persuasion and\nmanipulation, most models are in the yellow zone due to their effective\ninfluence on humans. For biological and chemical risks, we are unable to rule\nout the possibility of most models residing in the yellow zone, although\ndetailed threat modeling and in-depth assessment are required to make further\nclaims. This work reflects our current understanding of AI frontier risks and\nurges collective action to mitigate these challenges.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16534.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a369d98c0c89dcae3b8329",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a369d98c0c89dcae3b8329/AiH2zjy1cnt9OADAAZMLD.jpeg",
            "fullname": "Adina Yakefu",
            "name": "AdinaY",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1071
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.17957",
            "authors": [
                {
                    "_id": "6883c1b27d7a19a208cdef24",
                    "user": {
                        "_id": "6883c09516f1ec82fa56f2a7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/pozHaETGZj-a8bITgG78r.png",
                        "isPro": false,
                        "fullname": "Md. Al-Masrur Khan",
                        "user": "Masrur02",
                        "type": "user"
                    },
                    "name": "Md. Al-Masrur Khan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-07-28T08:45:27.007Z",
                    "hidden": false
                },
                {
                    "_id": "6883c1b27d7a19a208cdef25",
                    "name": "Durgakant Pushp",
                    "hidden": false
                },
                {
                    "_id": "6883c1b27d7a19a208cdef26",
                    "name": "Lantao Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-23T22:02:17.000Z",
            "submittedOnDailyAt": "2025-07-28T23:56:09.242Z",
            "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic\n  Segmentation",
            "submittedOnDailyBy": {
                "_id": "6883c09516f1ec82fa56f2a7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/pozHaETGZj-a8bITgG78r.png",
                "isPro": false,
                "fullname": "Md. Al-Masrur Khan",
                "user": "Masrur02",
                "type": "user"
            },
            "summary": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is\ntrained on labeled source domain data (e.g., synthetic images) and adapted to\nan unlabeled target domain (e.g., real-world images) without access to target\nannotations. Existing UDA-SS methods often struggle to balance fine-grained\nlocal details with global contextual information, leading to segmentation\nerrors in complex regions. To address this, we introduce the Adaptive Feature\nRefinement (AFR) module, which enhances segmentation accuracy by refining\nhighresolution features using semantic priors from low-resolution logits. AFR\nalso integrates high-frequency components, which capture fine-grained\nstructures and provide crucial boundary information, improving object\ndelineation. Additionally, AFR adaptively balances local and global information\nthrough uncertaintydriven attention, reducing misclassifications. Its\nlightweight design allows seamless integration into HRDA-based UDA methods,\nleading to state-of-the-art segmentation performance. Our approach improves\nexisting UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on\nSynthia-->Cityscapes. The implementation of our framework is available at:\nhttps://github.com/Masrur02/AFRDA",
            "upvotes": 0,
            "discussionId": "6883c1b27d7a19a208cdef27",
            "githubRepo": "https://github.com/Masrur02/AFRDA",
            "ai_summary": "The Adaptive Feature Refinement (AFR) module enhances unsupervised domain adaptive semantic segmentation by refining high-resolution features with low-resolution logits and integrating high-frequency components, leading to improved segmentation performance.",
            "ai_keywords": [
                "Unsupervised Domain Adaptive Semantic Segmentation",
                "UDA-SS",
                "Adaptive Feature Refinement",
                "AFR",
                "high-resolution features",
                "low-resolution logits",
                "high-frequency components",
                "uncertainty-driven attention",
                "HRDA-based UDA methods",
                "mIoU",
                "GTA V",
                "Cityscapes",
                "Synthia"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-07-23T18:02:17.000Z",
        "title": "AFRDA: Attentive Feature Refinement for Domain Adaptive Semantic\n  Segmentation",
        "summary": "In Unsupervised Domain Adaptive Semantic Segmentation (UDA-SS), a model is\ntrained on labeled source domain data (e.g., synthetic images) and adapted to\nan unlabeled target domain (e.g., real-world images) without access to target\nannotations. Existing UDA-SS methods often struggle to balance fine-grained\nlocal details with global contextual information, leading to segmentation\nerrors in complex regions. To address this, we introduce the Adaptive Feature\nRefinement (AFR) module, which enhances segmentation accuracy by refining\nhighresolution features using semantic priors from low-resolution logits. AFR\nalso integrates high-frequency components, which capture fine-grained\nstructures and provide crucial boundary information, improving object\ndelineation. Additionally, AFR adaptively balances local and global information\nthrough uncertaintydriven attention, reducing misclassifications. Its\nlightweight design allows seamless integration into HRDA-based UDA methods,\nleading to state-of-the-art segmentation performance. Our approach improves\nexisting UDA-SS methods by 1.05% mIoU on GTA V --> Cityscapes and 1.04% mIoU on\nSynthia-->Cityscapes. The implementation of our framework is available at:\nhttps://github.com/Masrur02/AFRDA",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17957.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6883c09516f1ec82fa56f2a7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/pozHaETGZj-a8bITgG78r.png",
            "fullname": "Md. Al-Masrur Khan",
            "name": "Masrur02",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]