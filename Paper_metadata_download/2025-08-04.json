[
    {
        "paper": {
            "id": "2508.00819",
            "authors": [
                {
                    "_id": "689020e10a411b3b8d28d67f",
                    "name": "Jinsong Li",
                    "hidden": false
                },
                {
                    "_id": "689020e10a411b3b8d28d680",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "689020e10a411b3b8d28d681",
                    "name": "Yuhang Zang",
                    "hidden": false
                },
                {
                    "_id": "689020e10a411b3b8d28d682",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "689020e10a411b3b8d28d683",
                    "name": "Jiaqi Wang",
                    "hidden": false
                },
                {
                    "_id": "689020e10a411b3b8d28d684",
                    "name": "Dahua Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T17:56:07.000Z",
            "submittedOnDailyAt": "2025-08-04T01:25:12.035Z",
            "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "64b4eec4faa3181a5eab9c46",
                "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
                "isPro": true,
                "fullname": "Jiaqi Wang",
                "user": "myownskyW7",
                "type": "user"
            },
            "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
            "upvotes": 38,
            "discussionId": "689020e20a411b3b8d28d685",
            "githubRepo": "https://github.com/Li-Jinsong/DAEDAL",
            "ai_summary": "DAEDAL, a novel training-free denoising strategy, enables dynamic length adaptation in Diffusion Large Language Models, improving performance and computational efficiency.",
            "ai_keywords": [
                "Diffusion Large Language Models",
                "DLLMs",
                "Autoregressive Large Language Models",
                "denoising strategy",
                "Dynamic Adaptive Length Expansion",
                "sequence completion metric",
                "mask token insertion",
                "effective token ratio"
            ],
            "githubStars": 51
        },
        "publishedAt": "2025-08-01T13:56:07.000Z",
        "title": "Beyond Fixed: Variable-Length Denoising for Diffusion Large Language\n  Models",
        "summary": "Diffusion Large Language Models (DLLMs) are emerging as a powerful\nalternative to the dominant Autoregressive Large Language Models, offering\nefficient parallel generation and capable global context modeling. However, the\npractical application of DLLMs is hindered by a critical architectural\nconstraint: the need for a statically predefined generation length. This static\nlength allocation leads to a problematic trade-off: insufficient lengths\ncripple performance on complex tasks, while excessive lengths incur significant\ncomputational overhead and sometimes result in performance degradation. While\nthe inference framework is rigid, we observe that the model itself possesses\ninternal signals that correlate with the optimal response length for a given\ntask. To bridge this gap, we leverage these latent signals and introduce\nDAEDAL, a novel training-free denoising strategy that enables Dynamic Adaptive\nLength Expansion for Diffusion Large Language Models. DAEDAL operates in two\nphases: 1) Before the denoising process, DAEDAL starts from a short initial\nlength and iteratively expands it to a coarse task-appropriate length, guided\nby a sequence completion metric. 2) During the denoising process, DAEDAL\ndynamically intervenes by pinpointing and expanding insufficient generation\nregions through mask token insertion, ensuring the final output is fully\ndeveloped. Extensive experiments on DLLMs demonstrate that DAEDAL achieves\nperformance comparable, and in some cases superior, to meticulously tuned\nfixed-length baselines, while simultaneously enhancing computational efficiency\nby achieving a higher effective token ratio. By resolving the static length\nconstraint, DAEDAL unlocks new potential for DLLMs, bridging a critical gap\nwith their Autoregressive counterparts and paving the way for more efficient\nand capable generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00819.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b4eec4faa3181a5eab9c46",
            "avatarUrl": "/avatars/bcc9bf5cbf67546ad2b4c9ec8b96ac96.svg",
            "fullname": "Jiaqi Wang",
            "name": "myownskyW7",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 21
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.23268",
            "authors": [
                {
                    "_id": "688c1ec68c434640078cc386",
                    "name": "Shuai Wang",
                    "hidden": false
                },
                {
                    "_id": "688c1ec68c434640078cc387",
                    "name": "Ziteng Gao",
                    "hidden": false
                },
                {
                    "_id": "688c1ec68c434640078cc388",
                    "name": "Chenhui Zhu",
                    "hidden": false
                },
                {
                    "_id": "688c1ec68c434640078cc389",
                    "name": "Weilin Huang",
                    "hidden": false
                },
                {
                    "_id": "688c1ec68c434640078cc38a",
                    "name": "Limin Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T06:07:20.000Z",
            "submittedOnDailyAt": "2025-08-04T00:45:14.702Z",
            "title": "PixNerd: Pixel Neural Field Diffusion",
            "submittedOnDailyBy": {
                "_id": "66615c855fd9d736e670e0a9",
                "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
                "isPro": false,
                "fullname": "wangshuai",
                "user": "wangsssssss",
                "type": "user"
            },
            "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID\non ImageNet 512times512 without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.",
            "upvotes": 31,
            "discussionId": "688c1ec68c434640078cc38b",
            "projectPage": "https://huggingface.co/spaces/MCG-NJU/PixNerd",
            "githubRepo": "https://github.com/MCG-NJU/PixNerd",
            "ai_summary": "Pixel Neural Field Diffusion (PixNerd) achieves high-quality image generation in a single-scale, single-stage process without VAEs or complex pipelines, and extends to text-to-image applications with competitive performance.",
            "ai_keywords": [
                "diffusion transformers",
                "compressed latent space",
                "pre-trained variational autoencoder",
                "pixel space",
                "patch-wise decoding",
                "neural field",
                "single-scale",
                "single-stage",
                "end-to-end solution",
                "pixel neural field diffusion",
                "PixNerd",
                "FID",
                "ImageNet",
                "text-to-image",
                "GenEval benchmark",
                "DPG benchmark"
            ],
            "githubStars": 30
        },
        "publishedAt": "2025-07-31T02:07:20.000Z",
        "title": "PixNerd: Pixel Neural Field Diffusion",
        "summary": "The current success of diffusion transformers heavily depends on the\ncompressed latent space shaped by the pre-trained variational autoencoder(VAE).\nHowever, this two-stage training paradigm inevitably introduces accumulated\nerrors and decoding artifacts. To address the aforementioned problems,\nresearchers return to pixel space at the cost of complicated cascade pipelines\nand increased token complexity. In contrast to their efforts, we propose to\nmodel the patch-wise decoding with neural field and present a single-scale,\nsingle-stage, efficient, end-to-end solution, coined as pixel neural field\ndiffusion~(PixelNerd). Thanks to the efficient neural field representation in\nPixNerd, we directly achieved 2.15 FID on ImageNet 256times256 and 2.84 FID\non ImageNet 512times512 without any complex cascade pipeline or VAE. We also\nextend our PixNerd framework to text-to-image applications. Our PixNerd-XXL/16\nachieved a competitive 0.73 overall score on the GenEval benchmark and 80.9\noverall score on the DPG benchmark.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23268.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "66615c855fd9d736e670e0a9",
            "avatarUrl": "/avatars/0ff3127b513552432a7c651e21d7f283.svg",
            "fullname": "wangshuai",
            "name": "wangsssssss",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.00414",
            "authors": [
                {
                    "_id": "6890eddff01a094725f833f8",
                    "name": "Tianqing Fang",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f833f9",
                    "name": "Zhisong Zhang",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f833fa",
                    "name": "Xiaoyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f833fb",
                    "name": "Rui Wang",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f833fc",
                    "name": "Can Qin",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f833fd",
                    "name": "Yuxuan Wan",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f833fe",
                    "name": "Jun-Yu Ma",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f833ff",
                    "name": "Ce Zhang",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f83400",
                    "name": "Jiaqi Chen",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f83401",
                    "name": "Xiyun Li",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f83402",
                    "name": "Hongming Zhang",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f83403",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "6890eddff01a094725f83404",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T08:11:31.000Z",
            "submittedOnDailyAt": "2025-08-04T16:01:44.753Z",
            "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent\n  Foundation Models Training",
            "submittedOnDailyBy": {
                "_id": "657cd228138b7e391444a65d",
                "avatarUrl": "/avatars/c7c984ae483144fab627aa2c54d91d0f.svg",
                "isPro": false,
                "fullname": "Xiaoyang Wang",
                "user": "xywang1",
                "type": "user"
            },
            "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present Cognitive Kernel-Pro, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro",
            "upvotes": 17,
            "discussionId": "6890eddff01a094725f83405",
            "githubRepo": "https://github.com/Tencent/CognitiveKernel-Pro",
            "ai_summary": "Cognitive Kernel-Pro is an open-source multi-module agent framework that enhances AI agent robustness and performance through data curation and novel test-time strategies, achieving state-of-the-art results.",
            "ai_keywords": [
                "Agent Foundation Models",
                "queries",
                "trajectories",
                "verifiable answers",
                "agent test-time reflection",
                "agent voting",
                "GAIA",
                "WebDancer",
                "WebSailor"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-08-01T04:11:31.000Z",
        "title": "Cognitive Kernel-Pro: A Framework for Deep Research Agents and Agent\n  Foundation Models Training",
        "summary": "General AI Agents are increasingly recognized as foundational frameworks for\nthe next generation of artificial intelligence, enabling complex reasoning, web\ninteraction, coding, and autonomous research capabilities. However, current\nagent systems are either closed-source or heavily reliant on a variety of paid\nAPIs and proprietary tools, limiting accessibility and reproducibility for the\nresearch community. In this work, we present Cognitive Kernel-Pro, a\nfully open-source and (to the maximum extent) free multi-module agent framework\ndesigned to democratize the development and evaluation of advanced AI agents.\nWithin Cognitive Kernel-Pro, we systematically investigate the curation of\nhigh-quality training data for Agent Foundation Models, focusing on the\nconstruction of queries, trajectories, and verifiable answers across four key\ndomains: web, file, code, and general reasoning. Furthermore, we explore novel\nstrategies for agent test-time reflection and voting to enhance agent\nrobustness and performance. We evaluate Cognitive Kernel-Pro on GAIA, achieving\nstate-of-the-art results among open-source and free agents. Notably, our\n8B-parameter open-source model surpasses previous leading systems such as\nWebDancer and WebSailor, establishing a new performance standard for\naccessible, high-capability AI agents. Code is available at\nhttps://github.com/Tencent/CognitiveKernel-Pro",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00414.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "657cd228138b7e391444a65d",
            "avatarUrl": "/avatars/c7c984ae483144fab627aa2c54d91d0f.svg",
            "fullname": "Xiaoyang Wang",
            "name": "xywang1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.23478",
            "authors": [
                {
                    "_id": "6890356a0a411b3b8d28d6c3",
                    "name": "Ting Huang",
                    "hidden": false
                },
                {
                    "_id": "6890356a0a411b3b8d28d6c4",
                    "name": "Zeyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6890356a0a411b3b8d28d6c5",
                    "name": "Hao Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T11:59:06.000Z",
            "submittedOnDailyAt": "2025-08-04T02:53:23.346Z",
            "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding",
            "submittedOnDailyBy": {
                "_id": "64ec877bb93654d4ca5c92e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                "isPro": false,
                "fullname": "Zeyu Zhang",
                "user": "SteveZeyuZhang",
                "type": "user"
            },
            "summary": "Large vision-language models (VLMs) have made significant strides in 2D\nvisual understanding tasks, sparking interest in extending these capabilities\nto 3D scene understanding. However, current 3D VLMs often struggle with robust\nreasoning and generalization due to limitations in high-quality spatial data\nand the static nature of viewpoint assumptions. To address these challenges, we\npropose 3D-R1, a foundation model that enhances the reasoning capabilities of\n3D VLMs. Specifically, we first construct a high-quality synthetic dataset with\nCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine\nbased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.\nMoreover, we leverage RLHF policy such as GRPO in the reinforcement learning\ntraining process to enhance reasoning capabilities and introduce three reward\nfunctions: a perception reward, a semantic similarity reward and a format\nreward to maintain detection accuracy and answer semantic precision.\nFurthermore, we introduce a dynamic view selection strategy that adaptively\nchooses the most informative perspectives for 3D scene understanding. Extensive\nexperiments demonstrate that 3D-R1 delivers an average improvement of 10%\nacross various 3D scene benchmarks, highlighting its effectiveness in enhancing\nreasoning and generalization in 3D scene understanding. Code:\nhttps://github.com/AIGeeksGroup/3D-R1. Website:\nhttps://aigeeksgroup.github.io/3D-R1.",
            "upvotes": 7,
            "discussionId": "6890356b0a411b3b8d28d6c6",
            "projectPage": "https://aigeeksgroup.github.io/3D-R1",
            "githubRepo": "https://github.com/AIGeeksGroup/3D-R1",
            "ai_summary": "3D-R1 enhances 3D scene understanding through a high-quality synthetic dataset, reinforcement learning with GRPO, and dynamic view selection, achieving significant improvements in reasoning and generalization.",
            "ai_keywords": [
                "3D-R1",
                "VLMs",
                "3D scene understanding",
                "Scene-30K",
                "Gemini 2.5 Pro",
                "RLHF policy",
                "GRPO",
                "perception reward",
                "semantic similarity reward",
                "format reward",
                "dynamic view selection"
            ],
            "githubStars": 54
        },
        "publishedAt": "2025-07-31T07:59:06.000Z",
        "title": "3D-R1: Enhancing Reasoning in 3D VLMs for Unified Scene Understanding",
        "summary": "Large vision-language models (VLMs) have made significant strides in 2D\nvisual understanding tasks, sparking interest in extending these capabilities\nto 3D scene understanding. However, current 3D VLMs often struggle with robust\nreasoning and generalization due to limitations in high-quality spatial data\nand the static nature of viewpoint assumptions. To address these challenges, we\npropose 3D-R1, a foundation model that enhances the reasoning capabilities of\n3D VLMs. Specifically, we first construct a high-quality synthetic dataset with\nCoT, named Scene-30K, leveraging existing 3D-VL datasets and a data engine\nbased on Gemini 2.5 Pro. It serves as cold-start initialization data for 3D-R1.\nMoreover, we leverage RLHF policy such as GRPO in the reinforcement learning\ntraining process to enhance reasoning capabilities and introduce three reward\nfunctions: a perception reward, a semantic similarity reward and a format\nreward to maintain detection accuracy and answer semantic precision.\nFurthermore, we introduce a dynamic view selection strategy that adaptively\nchooses the most informative perspectives for 3D scene understanding. Extensive\nexperiments demonstrate that 3D-R1 delivers an average improvement of 10%\nacross various 3D scene benchmarks, highlighting its effectiveness in enhancing\nreasoning and generalization in 3D scene understanding. Code:\nhttps://github.com/AIGeeksGroup/3D-R1. Website:\nhttps://aigeeksgroup.github.io/3D-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23478.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "fullname": "Zeyu Zhang",
            "name": "SteveZeyuZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.00265",
            "authors": [
                {
                    "_id": "6890159b0a411b3b8d28d650",
                    "name": "Henghui Ding",
                    "hidden": false
                },
                {
                    "_id": "6890159b0a411b3b8d28d651",
                    "name": "Song Tang",
                    "hidden": false
                },
                {
                    "_id": "6890159b0a411b3b8d28d652",
                    "name": "Shuting He",
                    "hidden": false
                },
                {
                    "_id": "6890159b0a411b3b8d28d653",
                    "name": "Chang Liu",
                    "hidden": false
                },
                {
                    "_id": "6890159b0a411b3b8d28d654",
                    "name": "Zuxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "6890159b0a411b3b8d28d655",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T02:14:00.000Z",
            "submittedOnDailyAt": "2025-08-04T00:37:52.118Z",
            "title": "Multimodal Referring Segmentation: A Survey",
            "submittedOnDailyBy": {
                "_id": "67ff29ecbf6889a333c69c7a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
                "isPro": false,
                "fullname": "Henghui Ding",
                "user": "HenghuiDing",
                "type": "user"
            },
            "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
            "upvotes": 6,
            "discussionId": "6890159b0a411b3b8d28d656",
            "ai_summary": "A survey of multimodal referring segmentation techniques, covering advancements in convolutional neural networks, transformers, and large language models for segmenting objects in images, videos, and 3D scenes based on text or audio instructions.",
            "ai_keywords": [
                "convolutional neural networks",
                "transformers",
                "large language models",
                "multimodal referring segmentation",
                "Generalized Referring Expression (GREx)"
            ]
        },
        "publishedAt": "2025-07-31T22:14:00.000Z",
        "title": "Multimodal Referring Segmentation: A Survey",
        "summary": "Multimodal referring segmentation aims to segment target objects in visual\nscenes, such as images, videos, and 3D scenes, based on referring expressions\nin text or audio format. This task plays a crucial role in practical\napplications requiring accurate object perception based on user instructions.\nOver the past decade, it has gained significant attention in the multimodal\ncommunity, driven by advances in convolutional neural networks, transformers,\nand large language models, all of which have substantially improved multimodal\nperception capabilities. This paper provides a comprehensive survey of\nmultimodal referring segmentation. We begin by introducing this field's\nbackground, including problem definitions and commonly used datasets. Next, we\nsummarize a unified meta architecture for referring segmentation and review\nrepresentative methods across three primary visual scenes, including images,\nvideos, and 3D scenes. We further discuss Generalized Referring Expression\n(GREx) methods to address the challenges of real-world complexity, along with\nrelated tasks and practical applications. Extensive performance comparisons on\nstandard benchmarks are also provided. We continually track related works at\nhttps://github.com/henghuiding/Awesome-Multimodal-Referring-Segmentation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00265.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "67ff29ecbf6889a333c69c7a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/fYIJFtF0uciB-1wb0lmTY.png",
            "fullname": "Henghui Ding",
            "name": "HenghuiDing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.23361",
            "authors": [
                {
                    "_id": "688c53f38c434640078cc47c",
                    "name": "Silin Chen",
                    "hidden": false
                },
                {
                    "_id": "688c53f38c434640078cc47d",
                    "name": "Shaoxin Lin",
                    "hidden": false
                },
                {
                    "_id": "688c53f38c434640078cc47e",
                    "name": "Xiaodong Gu",
                    "hidden": false
                },
                {
                    "_id": "688c53f38c434640078cc47f",
                    "name": "Yuling Shi",
                    "hidden": false
                },
                {
                    "_id": "688c53f38c434640078cc480",
                    "name": "Heng Lian",
                    "hidden": false
                },
                {
                    "_id": "688c53f38c434640078cc481",
                    "name": "Longfei Yun",
                    "hidden": false
                },
                {
                    "_id": "688c53f38c434640078cc482",
                    "name": "Dong Chen",
                    "hidden": false
                },
                {
                    "_id": "688c53f38c434640078cc483",
                    "name": "Weiguo Sun",
                    "hidden": false
                },
                {
                    "_id": "688c53f38c434640078cc484",
                    "name": "Lin Cao",
                    "hidden": false
                },
                {
                    "_id": "688c53f38c434640078cc485",
                    "name": "Qianxiang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T09:13:42.000Z",
            "submittedOnDailyAt": "2025-08-04T01:12:58.702Z",
            "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
            "submittedOnDailyBy": {
                "_id": "645b0c3ec35da9c7afd95421",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                "isPro": false,
                "fullname": "Yuling",
                "user": "YerbaPage",
                "type": "user"
            },
            "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.",
            "upvotes": 6,
            "discussionId": "688c53f38c434640078cc486",
            "githubRepo": "https://github.com/YerbaPage/SWE-Exp",
            "ai_summary": "SWE-Exp enhances software issue resolution by systematically accumulating and leveraging repair expertise from past agent experiences, improving resolution rates.",
            "ai_keywords": [
                "large language model (LLM)",
                "multi-agent collaboration",
                "Monte Carlo Tree Search (MCTS)",
                "experience bank",
                "issue resolution knowledge",
                "SWE-bench-Verified",
                "open-source agent frameworks"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-07-31T05:13:42.000Z",
        "title": "SWE-Exp: Experience-Driven Software Issue Resolution",
        "summary": "Recent advances in large language model (LLM) agents have shown remarkable\nprogress in software issue resolution, leveraging advanced techniques such as\nmulti-agent collaboration and Monte Carlo Tree Search (MCTS). However, current\nagents act as memoryless explorers - treating each problem separately without\nretaining or reusing knowledge from previous repair experiences. This leads to\nredundant exploration of failed trajectories and missed chances to adapt\nsuccessful issue resolution methods to similar problems. To address this\nproblem, we introduce SWE-Exp, an experience - enhanced approach that distills\nconcise and actionable experience from prior agent trajectories, enabling\ncontinuous learning across issues. Our method introduces a multi-faceted\nexperience bank that captures both successful and failed repair attempts.\nSpecifically, it extracts reusable issue resolution knowledge at different\nlevels - from high-level problem comprehension to specific code changes.\nExperiments show that SWE-Exp achieves state-of-the-art resolution rate (41.6%\nPass@1) on SWE-bench-Verified under open-source agent frameworks. Our approach\nestablishes a new paradigm in which automated software engineering agents\nsystematically accumulate and leverage repair expertise, fundamentally shifting\nfrom trial-and-error exploration to strategic, experience-driven issue\nresolution.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23361.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "fullname": "Yuling",
            "name": "YerbaPage",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 275
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.00454",
            "authors": [
                {
                    "_id": "689022980a411b3b8d28d69e",
                    "name": "Yuqi Tang",
                    "hidden": false
                },
                {
                    "_id": "689022980a411b3b8d28d69f",
                    "name": "Kehua Feng",
                    "hidden": false
                },
                {
                    "_id": "689022980a411b3b8d28d6a0",
                    "name": "Yunfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "689022980a411b3b8d28d6a1",
                    "name": "Zhiwen Chen",
                    "hidden": false
                },
                {
                    "_id": "689022980a411b3b8d28d6a2",
                    "name": "Chengfei Lv",
                    "hidden": false
                },
                {
                    "_id": "689022980a411b3b8d28d6a3",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "689022980a411b3b8d28d6a4",
                    "name": "Qiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "689022980a411b3b8d28d6a5",
                    "name": "Keyan Ding",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6479e038a26759caa62ea433/Fz7YpJV6QnC1THsv7jjSt.png"
            ],
            "publishedAt": "2025-08-01T09:26:01.000Z",
            "submittedOnDailyAt": "2025-08-04T01:39:47.788Z",
            "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
            "submittedOnDailyBy": {
                "_id": "6479e038a26759caa62ea433",
                "avatarUrl": "/avatars/7a9a023b1fae802eedbe90ec40c791d1.svg",
                "isPro": false,
                "fullname": "Feng",
                "user": "Kehua",
                "type": "user"
            },
            "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
            "upvotes": 4,
            "discussionId": "689022980a411b3b8d28d6a6",
            "githubRepo": "https://github.com/James-TYQ/MTDEval",
            "ai_summary": "An efficient multi-turn dialogue evaluator aggregates multiple LLM judgments into a single model to assess dialogue quality with reduced computational cost.",
            "ai_keywords": [
                "LLM-as-a-judge",
                "multi-judge approach",
                "preference knowledge",
                "multi-turn dialogue evaluator",
                "dialogue evaluation benchmarks"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-08-01T05:26:01.000Z",
        "title": "Learning an Efficient Multi-Turn Dialogue Evaluator from Multiple Judges",
        "summary": "Evaluating the conversational abilities of large language models (LLMs)\nremains a challenging task. Current mainstream approaches primarily rely on the\n``LLM-as-a-judge\" paradigm, where an LLM is prompted to serve as an evaluator\nto assess dialogue quality. However, such methods often suffer from various\nbiases, which undermine the reliability and consistency of the evaluation\nresults. To mitigate these biases, recent methods employ multiple LLMs as\njudges and aggregate their judgments to select the optimal assessment. Although\neffective, this multi-judge approach incurs significant computational overhead\nduring inference. In this paper, we propose an efficient multi-turn dialogue\nevaluator that captures the collective wisdom of multiple LLM judges by\naggregating their preference knowledge into a single model. Our approach\npreserves the advantages of diverse multi-judge feedback while drastically\nreducing the evaluation cost, enabling fast and flexible dialogue quality\nassessment. Extensive experiments on seven single rating and pairwise\ncomparison dialogue evaluation benchmarks demonstrate that our method\noutperforms existing baselines across diverse scenarios, showcasing its\nefficiency and robustness.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6479e038a26759caa62ea433/Fz7YpJV6QnC1THsv7jjSt.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00454.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6479e038a26759caa62ea433",
            "avatarUrl": "/avatars/7a9a023b1fae802eedbe90ec40c791d1.svg",
            "fullname": "Feng",
            "name": "Kehua",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.23348",
            "authors": [
                {
                    "_id": "688c53d88c434640078cc471",
                    "name": "Han Li",
                    "hidden": false
                },
                {
                    "_id": "688c53d88c434640078cc472",
                    "name": "Yuling Shi",
                    "hidden": false
                },
                {
                    "_id": "688c53d88c434640078cc473",
                    "name": "Shaoxin Lin",
                    "hidden": false
                },
                {
                    "_id": "688c53d88c434640078cc474",
                    "name": "Xiaodong Gu",
                    "hidden": false
                },
                {
                    "_id": "688c53d88c434640078cc475",
                    "name": "Heng Lian",
                    "hidden": false
                },
                {
                    "_id": "688c53d88c434640078cc476",
                    "name": "Xin Wang",
                    "hidden": false
                },
                {
                    "_id": "688c53d88c434640078cc477",
                    "name": "Yantao Jia",
                    "hidden": false
                },
                {
                    "_id": "688c53d88c434640078cc478",
                    "name": "Tao Huang",
                    "hidden": false
                },
                {
                    "_id": "688c53d88c434640078cc479",
                    "name": "Qianxiang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T08:54:46.000Z",
            "submittedOnDailyAt": "2025-08-04T01:11:57.584Z",
            "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
            "submittedOnDailyBy": {
                "_id": "645b0c3ec35da9c7afd95421",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                "isPro": false,
                "fullname": "Yuling",
                "user": "YerbaPage",
                "type": "user"
            },
            "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.",
            "upvotes": 4,
            "discussionId": "688c53d98c434640078cc47a",
            "githubRepo": "https://github.com/YerbaPage/SWE-Debate",
            "ai_summary": "SWE-Debate, a competitive multi-agent framework, enhances issue resolution in software engineering by promoting diverse reasoning and achieving better issue localization and fix planning.",
            "ai_keywords": [
                "large language models",
                "agent-based frameworks",
                "SWE-agent",
                "autonomous agents",
                "tool-using agents",
                "software engineering tasks",
                "local solutions",
                "issue patterns",
                "codebase",
                "competitive multi-agent debate",
                "fault propagation traces",
                "code dependency graph",
                "specialized agents",
                "reasoning perspectives",
                "structured competition",
                "consolidated fix plan",
                "MCTS-based code modification agent",
                "SWE-bench benchmark"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-07-31T04:54:46.000Z",
        "title": "SWE-Debate: Competitive Multi-Agent Debate for Software Issue Resolution",
        "summary": "Issue resolution has made remarkable progress thanks to the advanced\nreasoning capabilities of large language models (LLMs). Recently, agent-based\nframeworks such as SWE-agent have further advanced this progress by enabling\nautonomous, tool-using agents to tackle complex software engineering tasks.\nWhile existing agent-based issue resolution approaches are primarily based on\nagents' independent explorations, they often get stuck in local solutions and\nfail to identify issue patterns that span across different parts of the\ncodebase. To address this limitation, we propose SWE-Debate, a competitive\nmulti-agent debate framework that encourages diverse reasoning paths and\nachieves more consolidated issue localization. SWE-Debate first creates\nmultiple fault propagation traces as localization proposals by traversing a\ncode dependency graph. Then, it organizes a three-round debate among\nspecialized agents, each embodying distinct reasoning perspectives along the\nfault propagation trace. This structured competition enables agents to\ncollaboratively converge on a consolidated fix plan. Finally, this consolidated\nfix plan is integrated into an MCTS-based code modification agent for patch\ngeneration. Experiments on the SWE-bench benchmark show that SWE-Debate\nachieves new state-of-the-art results in open-source agent frameworks and\noutperforms baselines by a large margin.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23348.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "fullname": "Yuling",
            "name": "YerbaPage",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 275
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02124",
            "authors": [
                {
                    "_id": "68915e20f01a094725f8344f",
                    "name": "Jingze Shi",
                    "hidden": false
                },
                {
                    "_id": "68915e20f01a094725f83450",
                    "name": "Yifan Wu",
                    "hidden": false
                },
                {
                    "_id": "68915e20f01a094725f83451",
                    "name": "Bingheng Wu",
                    "hidden": false
                },
                {
                    "_id": "68915e20f01a094725f83452",
                    "name": "Yiran Peng",
                    "hidden": false
                },
                {
                    "_id": "68915e20f01a094725f83453",
                    "name": "Liangdong Wang",
                    "hidden": false
                },
                {
                    "_id": "68915e20f01a094725f83454",
                    "name": "Guang Liu",
                    "hidden": false
                },
                {
                    "_id": "68915e20f01a094725f83455",
                    "name": "Yuyu Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T07:05:15.000Z",
            "submittedOnDailyAt": "2025-08-04T23:58:28.069Z",
            "title": "Trainable Dynamic Mask Sparse Attention",
            "submittedOnDailyBy": {
                "_id": "673ab3647afcea17eb4378fd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png",
                "isPro": false,
                "fullname": "Loser Cheems",
                "user": "JingzeShi",
                "type": "user"
            },
            "summary": "In large language models, the demand for modeling long contexts is constantly\nincreasing, but the quadratic complexity of the standard self-attention\nmechanism often becomes a bottleneck. Although existing sparse attention\nmechanisms have improved efficiency, they may still encounter issues such as\nstatic patterns or information loss. We introduce a trainable dynamic mask\nsparse attention mechanism, Dynamic Mask Attention, which effectively utilizes\ncontent-aware and position-aware sparsity. DMA achieves this through two key\ninnovations: First, it dynamically generates content-aware sparse masks from\nvalue representations, enabling the model to identify and focus on critical\ninformation adaptively. Second, it implements position-aware sparse attention\ncomputation that effectively skips unnecessary calculation regions. This\ndual-sparsity design allows the model to significantly reduce the computational\ncomplexity of important information while retaining complete information,\nachieving an excellent balance between information fidelity and computational\nefficiency. We have verified the performance of DMA through comprehensive\nexperiments. Comparative studies show that DMA outperforms multi-head\nattention, sliding window attention, multi-head latent attention, and native\nsparse attention in terms of perplexity under Chinchilla Scaling Law settings.\nMoreover, in challenging multi-query associative recall tasks, DMA also\ndemonstrates superior performance and efficiency compared to these methods.\nCrucially, in the evaluation of a 1.7B parameter model, DMA significantly\noutperforms multi-head attention in both standard benchmark performance and the\nchallenging needle-in-a-haystack task. These experimental results highlight its\ncapability to balance model efficiency and long-context modeling ability\neffectively.",
            "upvotes": 3,
            "discussionId": "68915e21f01a094725f83456",
            "githubRepo": "https://github.com/SmallDoges/flash-dmattn",
            "ai_summary": "A dynamic mask sparse attention mechanism, DMA, improves long-context modeling in large language models by reducing computational complexity while maintaining information fidelity.",
            "ai_keywords": [
                "self-attention",
                "sparse attention",
                "content-aware",
                "position-aware",
                "dynamic mask",
                "Chinchilla Scaling Law",
                "perplexity",
                "multi-query associative recall",
                "needle-in-a-haystack task"
            ]
        },
        "publishedAt": "2025-08-04T03:05:15.000Z",
        "title": "Trainable Dynamic Mask Sparse Attention",
        "summary": "In large language models, the demand for modeling long contexts is constantly\nincreasing, but the quadratic complexity of the standard self-attention\nmechanism often becomes a bottleneck. Although existing sparse attention\nmechanisms have improved efficiency, they may still encounter issues such as\nstatic patterns or information loss. We introduce a trainable dynamic mask\nsparse attention mechanism, Dynamic Mask Attention, which effectively utilizes\ncontent-aware and position-aware sparsity. DMA achieves this through two key\ninnovations: First, it dynamically generates content-aware sparse masks from\nvalue representations, enabling the model to identify and focus on critical\ninformation adaptively. Second, it implements position-aware sparse attention\ncomputation that effectively skips unnecessary calculation regions. This\ndual-sparsity design allows the model to significantly reduce the computational\ncomplexity of important information while retaining complete information,\nachieving an excellent balance between information fidelity and computational\nefficiency. We have verified the performance of DMA through comprehensive\nexperiments. Comparative studies show that DMA outperforms multi-head\nattention, sliding window attention, multi-head latent attention, and native\nsparse attention in terms of perplexity under Chinchilla Scaling Law settings.\nMoreover, in challenging multi-query associative recall tasks, DMA also\ndemonstrates superior performance and efficiency compared to these methods.\nCrucially, in the evaluation of a 1.7B parameter model, DMA significantly\noutperforms multi-head attention in both standard benchmark performance and the\nchallenging needle-in-a-haystack task. These experimental results highlight its\ncapability to balance model efficiency and long-context modeling ability\neffectively.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02124.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "673ab3647afcea17eb4378fd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/673ab3647afcea17eb4378fd/YQB6zSH1LPxBMUYayIURi.png",
            "fullname": "Loser Cheems",
            "name": "JingzeShi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 35
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.00782",
            "authors": [
                {
                    "_id": "689073e2e8049ec50047e173",
                    "name": "Kien T. Pham",
                    "hidden": false
                },
                {
                    "_id": "689073e2e8049ec50047e174",
                    "name": "Yingqing He",
                    "hidden": false
                },
                {
                    "_id": "689073e2e8049ec50047e175",
                    "name": "Yazhou Xing",
                    "hidden": false
                },
                {
                    "_id": "689073e2e8049ec50047e176",
                    "name": "Qifeng Chen",
                    "hidden": false
                },
                {
                    "_id": "689073e2e8049ec50047e177",
                    "name": "Long Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65899fc5ce38d143c4638da4/9ultJKgVDvZreGl0VHqEw.png"
            ],
            "publishedAt": "2025-08-01T17:05:04.000Z",
            "submittedOnDailyAt": "2025-08-04T07:37:25.197Z",
            "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware\n  Video Generation",
            "submittedOnDailyBy": {
                "_id": "65899fc5ce38d143c4638da4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65899fc5ce38d143c4638da4/TE-CGpM08JWsbHTjwKcaK.jpeg",
                "isPro": false,
                "fullname": "PHAM Trung Kien",
                "user": "TK3105",
                "type": "user"
            },
            "summary": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.",
            "upvotes": 3,
            "discussionId": "689073e3e8049ec50047e178",
            "ai_summary": "SpA2V generates realistic videos aligned with input audio by leveraging spatial auditory cues and integrating them into diffusion models through video scene layouts.",
            "ai_keywords": [
                "MLLM",
                "Video Scene Layouts (VSLs)",
                "diffusion models",
                "audio-guided video planning",
                "layout-grounded video generation"
            ]
        },
        "publishedAt": "2025-08-01T13:05:04.000Z",
        "title": "SpA2V: Harnessing Spatial Auditory Cues for Audio-driven Spatially-aware\n  Video Generation",
        "summary": "Audio-driven video generation aims to synthesize realistic videos that align\nwith input audio recordings, akin to the human ability to visualize scenes from\nauditory input. However, existing approaches predominantly focus on exploring\nsemantic information, such as the classes of sounding sources present in the\naudio, limiting their ability to generate videos with accurate content and\nspatial composition. In contrast, we humans can not only naturally identify the\nsemantic categories of sounding sources but also determine their deeply encoded\nspatial attributes, including locations and movement directions. This useful\ninformation can be elucidated by considering specific spatial indicators\nderived from the inherent physical properties of sound, such as loudness or\nfrequency. As prior methods largely ignore this factor, we present SpA2V, the\nfirst framework explicitly exploits these spatial auditory cues from audios to\ngenerate videos with high semantic and spatial correspondence. SpA2V decomposes\nthe generation process into two stages: 1) Audio-guided Video Planning: We\nmeticulously adapt a state-of-the-art MLLM for a novel task of harnessing\nspatial and semantic cues from input audio to construct Video Scene Layouts\n(VSLs). This serves as an intermediate representation to bridge the gap between\nthe audio and video modalities. 2) Layout-grounded Video Generation: We develop\nan efficient and effective approach to seamlessly integrate VSLs as conditional\nguidance into pre-trained diffusion models, enabling VSL-grounded video\ngeneration in a training-free manner. Extensive experiments demonstrate that\nSpA2V excels in generating realistic videos with semantic and spatial alignment\nto the input audios.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65899fc5ce38d143c4638da4/9ultJKgVDvZreGl0VHqEw.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00782.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65899fc5ce38d143c4638da4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65899fc5ce38d143c4638da4/TE-CGpM08JWsbHTjwKcaK.jpeg",
            "fullname": "PHAM Trung Kien",
            "name": "TK3105",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.22720",
            "authors": [
                {
                    "_id": "689036f00a411b3b8d28d6d1",
                    "name": "Amit Das",
                    "hidden": false
                },
                {
                    "_id": "689036f00a411b3b8d28d6d2",
                    "name": "Md. Najib Hasan",
                    "hidden": false
                },
                {
                    "_id": "689036f00a411b3b8d28d6d3",
                    "name": "Souvika Sarkar",
                    "hidden": false
                },
                {
                    "_id": "689036f00a411b3b8d28d6d4",
                    "name": "Zheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "689036f00a411b3b8d28d6d5",
                    "name": "Fatemeh Jamshidi",
                    "hidden": false
                },
                {
                    "_id": "689036f00a411b3b8d28d6d6",
                    "name": "Tathagata Bhattacharya",
                    "hidden": false
                },
                {
                    "_id": "689036f00a411b3b8d28d6d7",
                    "name": "Nilanjana Raychawdhury",
                    "hidden": false
                },
                {
                    "_id": "689036f00a411b3b8d28d6d8",
                    "name": "Dongji Feng",
                    "hidden": false
                },
                {
                    "_id": "689036f00a411b3b8d28d6d9",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "689036f00a411b3b8d28d6da",
                    "name": "Aman Chadha",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-30T14:39:51.000Z",
            "submittedOnDailyAt": "2025-08-04T02:59:25.205Z",
            "title": "Investigating Hallucination in Conversations for Low Resource Languages",
            "submittedOnDailyBy": {
                "_id": "63a4754927f1f64ed7238dac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                "isPro": false,
                "fullname": "Aman Chadha",
                "user": "amanchadha",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.",
            "upvotes": 3,
            "discussionId": "689036f00a411b3b8d28d6db",
            "ai_summary": "LLMs generate fewer hallucinations in Mandarin compared to Hindi and Farsi across multiple models.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "hallucination",
                "GPT-3.5",
                "GPT-4o",
                "Llama-3.1",
                "Gemma-2.0",
                "DeepSeek-R1",
                "Qwen-3"
            ]
        },
        "publishedAt": "2025-07-30T10:39:51.000Z",
        "title": "Investigating Hallucination in Conversations for Low Resource Languages",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable proficiency in\ngenerating text that closely resemble human writing. However, they often\ngenerate factually incorrect statements, a problem typically referred to as\n'hallucination'. Addressing hallucination is crucial for enhancing the\nreliability and effectiveness of LLMs. While much research has focused on\nhallucinations in English, our study extends this investigation to\nconversational data in three languages: Hindi, Farsi, and Mandarin. We offer a\ncomprehensive analysis of a dataset to examine both factual and linguistic\nerrors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0,\nDeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated\nresponses in Mandarin but generate a significantly higher number of\nhallucinations in Hindi and Farsi.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22720.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.19634",
            "authors": [
                {
                    "_id": "6890e6d6f01a094725f833d9",
                    "name": "Sara Papi",
                    "hidden": false
                },
                {
                    "_id": "6890e6d6f01a094725f833da",
                    "name": "Maike Zfle",
                    "hidden": false
                },
                {
                    "_id": "6890e6d6f01a094725f833db",
                    "name": "Marco Gaido",
                    "hidden": false
                },
                {
                    "_id": "6890e6d6f01a094725f833dc",
                    "name": "Beatrice Savoldi",
                    "hidden": false
                },
                {
                    "_id": "6890e6d6f01a094725f833dd",
                    "name": "Danni Liu",
                    "hidden": false
                },
                {
                    "_id": "6890e6d6f01a094725f833de",
                    "name": "Ioannis Douros",
                    "hidden": false
                },
                {
                    "_id": "6890e6d6f01a094725f833df",
                    "name": "Luisa Bentivogli",
                    "hidden": false
                },
                {
                    "_id": "6890e6d6f01a094725f833e0",
                    "name": "Jan Niehues",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-25T19:00:51.000Z",
            "submittedOnDailyAt": "2025-08-04T16:13:56.434Z",
            "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from\n  Scientific Talks",
            "submittedOnDailyBy": {
                "_id": "66309b3833ccd9e68c5d5171",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
                "isPro": false,
                "fullname": "Sara Papi",
                "user": "spapi",
                "type": "user"
            },
            "summary": "Recent advances in large language models have catalyzed the development of\nmultimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As MLLMs evolve from narrow, monolingual, task-specific systems to\ngeneral-purpose instruction-following models, a key frontier lies in evaluating\ntheir multilingual and multimodal capabilities over both long and short\ncontexts. However, existing benchmarks fall short in evaluating these\ndimensions jointly: they are often limited to English, mostly focus on one\nsingle modality at a time, rely on short-form contexts, or lack human\nannotations -- hindering comprehensive assessment of model performance across\nlanguages, modalities, and task complexity. To address these gaps, we introduce\nMCIF (Multimodal Crosslingual Instruction Following), the first multilingual\nhuman-annotated benchmark based on scientific talks that is designed to\nevaluate instruction-following in crosslingual, multimodal settings over both\nshort- and long-form inputs. MCIF spans three core modalities -- speech,\nvision, and text -- and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret\ninstructions across languages and combine them with multimodal contextual\ninformation. MCIF is released under a CC-BY 4.0 license to encourage open\nresearch and progress in MLLMs development.",
            "upvotes": 3,
            "discussionId": "6890e6d6f01a094725f833e1",
            "ai_summary": "MCIF is a multilingual, human-annotated benchmark for evaluating instruction-following in crosslingual, multimodal settings using scientific talks.",
            "ai_keywords": [
                "multimodal LLMs",
                "MLLMs",
                "instruction-following",
                "multilingual",
                "multimodal",
                "scientific talks",
                "speech",
                "vision",
                "text",
                "human-annotated benchmark",
                "CC-BY 4.0 license"
            ]
        },
        "publishedAt": "2025-07-25T15:00:51.000Z",
        "title": "MCIF: Multimodal Crosslingual Instruction-Following Benchmark from\n  Scientific Talks",
        "summary": "Recent advances in large language models have catalyzed the development of\nmultimodal LLMs (MLLMs) that integrate text, speech, and vision within unified\nframeworks. As MLLMs evolve from narrow, monolingual, task-specific systems to\ngeneral-purpose instruction-following models, a key frontier lies in evaluating\ntheir multilingual and multimodal capabilities over both long and short\ncontexts. However, existing benchmarks fall short in evaluating these\ndimensions jointly: they are often limited to English, mostly focus on one\nsingle modality at a time, rely on short-form contexts, or lack human\nannotations -- hindering comprehensive assessment of model performance across\nlanguages, modalities, and task complexity. To address these gaps, we introduce\nMCIF (Multimodal Crosslingual Instruction Following), the first multilingual\nhuman-annotated benchmark based on scientific talks that is designed to\nevaluate instruction-following in crosslingual, multimodal settings over both\nshort- and long-form inputs. MCIF spans three core modalities -- speech,\nvision, and text -- and four diverse languages (English, German, Italian, and\nChinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret\ninstructions across languages and combine them with multimodal contextual\ninformation. MCIF is released under a CC-BY 4.0 license to encourage open\nresearch and progress in MLLMs development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.19634.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66309b3833ccd9e68c5d5171",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66309b3833ccd9e68c5d5171/UGt7pZazJHhtwjg0iiyBu.jpeg",
            "fullname": "Sara Papi",
            "name": "spapi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.00632",
            "authors": [
                {
                    "_id": "689094bca31da8bfabf98492",
                    "name": "Alexia Jolicoeur-Martineau",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T13:45:13.000Z",
            "submittedOnDailyAt": "2025-08-04T12:29:45.646Z",
            "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings",
            "submittedOnDailyBy": {
                "_id": "62f8ea1177b722f186611e8e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660479589894-noauth.jpeg",
                "isPro": false,
                "fullname": "Alexia Jolicoeur-Martineau",
                "user": "AlexiaJM",
                "type": "user"
            },
            "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.",
            "upvotes": 2,
            "discussionId": "689094bca31da8bfabf98493",
            "projectPage": "https://alexiajm.github.io/2025/03/15/avr.html",
            "githubRepo": "https://github.com/SamsungSAILMontreal/AVR-Eval-Agent",
            "ai_summary": "A multi-agent system using an omni-modal evaluation metric improves JavaScript game and animation generation but struggles with custom assets and audio-visual feedback.",
            "ai_keywords": [
                "AVR-Eval",
                "multi-agent system",
                "omni-modal model",
                "text model",
                "Audio-Visual Recordings (AVRs)",
                "AVR-Agent",
                "JavaScript code",
                "multimedia assets",
                "audio",
                "images",
                "3D models",
                "win rate",
                "one-shot generation"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-08-01T09:45:13.000Z",
        "title": "Multi-Agent Game Generation and Evaluation via Audio-Visual Recordings",
        "summary": "While AI excels at generating text, audio, images, and videos, creating\ninteractive audio-visual content such as video games remains challenging.\nCurrent LLMs can generate JavaScript games and animations, but lack automated\nevaluation metrics and struggle with complex content that normally requires\nteams of humans working for many months (multi-shot, multi-agents) using assets\nmade by artists. To tackle these issues, we built a new metric and a\nmulti-agent system.\n  We propose AVR-Eval, a relative metric for multimedia content quality using\nAudio-Visual Recordings (AVRs). An omni-modal model (processing text, video,\nand audio) compares the AVRs of two contents, with a text model reviewing\nevaluations to determine superiority. We show that AVR-Eval properly identifies\ngood from broken or mismatched content.\n  We built AVR-Agent, a multi-agent system generating JavaScript code from a\nbank of multimedia assets (audio, images, 3D models). The coding agent selects\nrelevant assets, generates multiple initial codes, uses AVR-Eval to identify\nthe best version, and iteratively improves it through omni-modal agent feedback\nfrom the AVR.\n  We run experiments on games and animations with AVR-Eval (win rate of content\nA against B). We find that content generated by AVR-Agent has a significantly\nhigher win rate against content made through one-shot generation. However,\nmodels struggle to leverage custom assets and AVR feedback effectively, showing\nno higher win rate. This reveals a critical gap: while humans benefit from\nhigh-quality assets and audio-visual feedback, current coding models do not\nseem to utilize these resources as effectively, highlighting fundamental\ndifferences between human and machine content creation approaches.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00632.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62f8ea1177b722f186611e8e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660479589894-noauth.jpeg",
            "fullname": "Alexia Jolicoeur-Martineau",
            "name": "AlexiaJM",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.00823",
            "authors": [
                {
                    "_id": "68902cbc0a411b3b8d28d6ba",
                    "name": "Wenxuan Guo",
                    "hidden": false
                },
                {
                    "_id": "68902cbc0a411b3b8d28d6bb",
                    "name": "Xiuwei Xu",
                    "hidden": false
                },
                {
                    "_id": "68902cbc0a411b3b8d28d6bc",
                    "name": "Hang Yin",
                    "hidden": false
                },
                {
                    "_id": "68902cbc0a411b3b8d28d6bd",
                    "name": "Ziwei Wang",
                    "hidden": false
                },
                {
                    "_id": "68902cbc0a411b3b8d28d6be",
                    "name": "Jianjiang Feng",
                    "hidden": false
                },
                {
                    "_id": "68902cbc0a411b3b8d28d6bf",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68902cbc0a411b3b8d28d6c0",
                    "name": "Jiwen Lu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T17:59:56.000Z",
            "submittedOnDailyAt": "2025-08-04T02:17:55.656Z",
            "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
            "submittedOnDailyBy": {
                "_id": "67b2cf648a276e7b4856e307",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHepLDwyYeQC4IQhVKgz6.png",
                "isPro": false,
                "fullname": "Wenxuan Guo",
                "user": "gwx22",
                "type": "user"
            },
            "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.",
            "upvotes": 1,
            "discussionId": "68902cbc0a411b3b8d28d6c1",
            "projectPage": "https://gwxuan.github.io/IGL-Nav/",
            "ai_summary": "IGL-Nav uses an incremental 3D Gaussian representation for efficient and accurate image-goal navigation in 3D space, outperforming existing methods and applicable in real-world settings.",
            "ai_keywords": [
                "renderable 3D gaussian",
                "3DGS",
                "6-DoF camera pose",
                "feed-forward monocular prediction",
                "discrete space matching",
                "differentiable rendering",
                "IGL-Nav",
                "image-goal navigation",
                "free-view image-goal setting"
            ]
        },
        "publishedAt": "2025-08-01T13:59:56.000Z",
        "title": "IGL-Nav: Incremental 3D Gaussian Localization for Image-goal Navigation",
        "summary": "Visual navigation with an image as goal is a fundamental and challenging\nproblem. Conventional methods either rely on end-to-end RL learning or\nmodular-based policy with topological graph or BEV map as memory, which cannot\nfully model the geometric relationship between the explored 3D environment and\nthe goal image. In order to efficiently and accurately localize the goal image\nin 3D space, we build our navigation system upon the renderable 3D gaussian\n(3DGS) representation. However, due to the computational intensity of 3DGS\noptimization and the large search space of 6-DoF camera pose, directly\nleveraging 3DGS for image localization during agent exploration process is\nprohibitively inefficient. To this end, we propose IGL-Nav, an Incremental 3D\nGaussian Localization framework for efficient and 3D-aware image-goal\nnavigation. Specifically, we incrementally update the scene representation as\nnew images arrive with feed-forward monocular prediction. Then we coarsely\nlocalize the goal by leveraging the geometric information for discrete space\nmatching, which can be equivalent to efficient 3D convolution. When the agent\nis close to the goal, we finally solve the fine target pose with optimization\nvia differentiable rendering. The proposed IGL-Nav outperforms existing\nstate-of-the-art methods by a large margin across diverse experimental\nconfigurations. It can also handle the more challenging free-view image-goal\nsetting and be deployed on real-world robotic platform using a cellphone to\ncapture goal image at arbitrary pose. Project page:\nhttps://gwxuan.github.io/IGL-Nav/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00823.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67b2cf648a276e7b4856e307",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/lHepLDwyYeQC4IQhVKgz6.png",
            "fullname": "Wenxuan Guo",
            "name": "gwx22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]