[
    {
        "paper": {
            "id": "2508.02324",
            "authors": [
                {
                    "_id": "68919a57f01a094725f83598",
                    "name": "Chenfei Wu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f83599",
                    "name": "Jiahao Li",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359a",
                    "name": "Jingren Zhou",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359b",
                    "name": "Junyang Lin",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359c",
                    "name": "Kaiyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359d",
                    "name": "Kun Yan",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359e",
                    "name": "Sheng-ming Yin",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f8359f",
                    "name": "Shuai Bai",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a0",
                    "name": "Xiao Xu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a1",
                    "name": "Yilei Chen",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a2",
                    "name": "Yuxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a3",
                    "name": "Zecheng Tang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a4",
                    "name": "Zekai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a5",
                    "name": "Zhengyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a6",
                    "name": "An Yang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a7",
                    "name": "Bowen Yu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a8",
                    "name": "Chen Cheng",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835a9",
                    "name": "Dayiheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835aa",
                    "name": "Deqing Li",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ab",
                    "name": "Hang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ac",
                    "name": "Hao Meng",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ad",
                    "name": "Hu Wei",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ae",
                    "name": "Jingyuan Ni",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835af",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b0",
                    "name": "Kuan Cao",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b1",
                    "name": "Liang Peng",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b2",
                    "name": "Lin Qu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b3",
                    "name": "Minggang Wu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b4",
                    "name": "Peng Wang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b5",
                    "name": "Shuting Yu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b6",
                    "name": "Tingkun Wen",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b7",
                    "name": "Wensen Feng",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b8",
                    "name": "Xiaoxiao Xu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835b9",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835ba",
                    "name": "Yichang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835bb",
                    "name": "Yongqiang Zhu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835bc",
                    "name": "Yujia Wu",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835bd",
                    "name": "Yuxuan Cai",
                    "hidden": false
                },
                {
                    "_id": "68919a57f01a094725f835be",
                    "name": "Zenan Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/IGGtaa2qHZfwfhjAJfV-Q.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/wBudoV6xlkMnSdkPjkC4C.png"
            ],
            "publishedAt": "2025-08-04T11:49:20.000Z",
            "submittedOnDailyAt": "2025-08-05T04:39:37.929Z",
            "title": "Qwen-Image Technical Report",
            "submittedOnDailyBy": {
                "_id": "616fb788e2ad27af26561b1a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675485317568-616fb788e2ad27af26561b1a.jpeg",
                "isPro": false,
                "fullname": "Xiao Xu",
                "user": "LooperXX",
                "type": "user"
            },
            "summary": "We present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.",
            "upvotes": 87,
            "discussionId": "68919a57f01a094725f835bf",
            "githubRepo": "https://github.com/QwenLM/Qwen-Image",
            "ai_summary": "Qwen-Image, an image generation model, advances text rendering and image editing through a comprehensive data pipeline, progressive training, and dual-encoding mechanism.",
            "ai_keywords": [
                "data pipeline",
                "progressive training",
                "curriculum learning",
                "text-to-image",
                "text-image-to-image",
                "image-to-image",
                "latent representations",
                "dual-encoding mechanism",
                "semantic consistency",
                "visual fidelity"
            ],
            "githubStars": 1572
        },
        "publishedAt": "2025-08-04T07:49:20.000Z",
        "title": "Qwen-Image Technical Report",
        "summary": "We present Qwen-Image, an image generation foundation model in the Qwen\nseries that achieves significant advances in complex text rendering and precise\nimage editing. To address the challenges of complex text rendering, we design a\ncomprehensive data pipeline that includes large-scale data collection,\nfiltering, annotation, synthesis, and balancing. Moreover, we adopt a\nprogressive training strategy that starts with non-text-to-text rendering,\nevolves from simple to complex textual inputs, and gradually scales up to\nparagraph-level descriptions. This curriculum learning approach substantially\nenhances the model's native text rendering capabilities. As a result,\nQwen-Image not only performs exceptionally well in alphabetic languages such as\nEnglish, but also achieves remarkable progress on more challenging logographic\nlanguages like Chinese. To enhance image editing consistency, we introduce an\nimproved multi-task training paradigm that incorporates not only traditional\ntext-to-image (T2I) and text-image-to-image (TI2I) tasks but also\nimage-to-image (I2I) reconstruction, effectively aligning the latent\nrepresentations between Qwen2.5-VL and MMDiT. Furthermore, we separately feed\nthe original image into Qwen2.5-VL and the VAE encoder to obtain semantic and\nreconstructive representations, respectively. This dual-encoding mechanism\nenables the editing module to strike a balance between preserving semantic\nconsistency and maintaining visual fidelity. Qwen-Image achieves\nstate-of-the-art performance, demonstrating its strong capabilities in both\nimage generation and editing across multiple benchmarks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/IGGtaa2qHZfwfhjAJfV-Q.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/616fb788e2ad27af26561b1a/wBudoV6xlkMnSdkPjkC4C.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02324.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "616fb788e2ad27af26561b1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1675485317568-616fb788e2ad27af26561b1a.jpeg",
            "fullname": "Xiao Xu",
            "name": "LooperXX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01959",
            "authors": [
                {
                    "_id": "68919172f01a094725f8355d",
                    "name": "Junjie Wu",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f8355e",
                    "name": "Jiangnan Li",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f8355f",
                    "name": "Yuqing Li",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83560",
                    "name": "Lemao Liu",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83561",
                    "name": "Liyan Xu",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83562",
                    "name": "Jiwei Li",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83563",
                    "name": "Dit-Yan Yeung",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83564",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68919172f01a094725f83565",
                    "name": "Mo Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-03T23:59:31.000Z",
            "submittedOnDailyAt": "2025-08-05T03:38:04.443Z",
            "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension",
            "submittedOnDailyBy": {
                "_id": "64d36ca3036ae0b3756588e3",
                "avatarUrl": "/avatars/0d7dfbd681b1157a38d0f0a86f19b702.svg",
                "isPro": false,
                "fullname": "Junjie Wu",
                "user": "junjiewu",
                "type": "user"
            },
            "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.",
            "upvotes": 39,
            "discussionId": "68919173f01a094725f83566",
            "projectPage": "https://huggingface.co/SituatedEmbedding",
            "ai_summary": "A new training paradigm and situated embedding models (SitEmb) enhance retrieval performance by conditioning short text chunks on broader context windows, outperforming state-of-the-art models with fewer parameters.",
            "ai_keywords": [
                "Retrieval-augmented generation (RAG)",
                "context window",
                "embedding models",
                "situated context",
                "situated embedding models (SitEmb)",
                "BGE-M3",
                "downstream applications"
            ]
        },
        "publishedAt": "2025-08-03T19:59:31.000Z",
        "title": "SitEmb-v1.5: Improved Context-Aware Dense Retrieval for Semantic\n  Association and Long Story Comprehension",
        "summary": "Retrieval-augmented generation (RAG) over long documents typically involves\nsplitting the text into smaller chunks, which serve as the basic units for\nretrieval. However, due to dependencies across the original document,\ncontextual information is often essential for accurately interpreting each\nchunk. To address this, prior work has explored encoding longer context windows\nto produce embeddings for longer chunks. Despite these efforts, gains in\nretrieval and downstream tasks remain limited. This is because (1) longer\nchunks strain the capacity of embedding models due to the increased amount of\ninformation they must encode, and (2) many real-world applications still\nrequire returning localized evidence due to constraints on model or human\nbandwidth.\n  We propose an alternative approach to this challenge by representing short\nchunks in a way that is conditioned on a broader context window to enhance\nretrieval performance -- i.e., situating a chunk's meaning within its context.\nWe further show that existing embedding models are not well-equipped to encode\nsuch situated context effectively, and thus introduce a new training paradigm\nand develop the situated embedding models (SitEmb). To evaluate our method, we\ncurate a book-plot retrieval dataset specifically designed to assess situated\nretrieval capabilities. On this benchmark, our SitEmb-v1 model based on BGE-M3\nsubstantially outperforms state-of-the-art embedding models, including several\nwith up to 7-8B parameters, with only 1B parameters. Our 8B SitEmb-v1.5 model\nfurther improves performance by over 10% and shows strong results across\ndifferent languages and several downstream applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01959.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d36ca3036ae0b3756588e3",
            "avatarUrl": "/avatars/0d7dfbd681b1157a38d0f0a86f19b702.svg",
            "fullname": "Junjie Wu",
            "name": "junjiewu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02276",
            "authors": [
                {
                    "_id": "68917696f01a094725f834b1",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b2",
                    "name": "Zhuoyun Yu",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b3",
                    "name": "Jiapeng Chen",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b4",
                    "name": "Yan Cui",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b5",
                    "name": "Daniel Shao",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b6",
                    "name": "Weixu Wang",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b7",
                    "name": "Fang Wu",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b8",
                    "name": "Yuchen Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834b9",
                    "name": "Wenqi Shi",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834ba",
                    "name": "Zhi Huang",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834bb",
                    "name": "Arman Cohan",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834bc",
                    "name": "Xihong Lin",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834bd",
                    "name": "Fabian Theis",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834be",
                    "name": "Smita Krishnaswamy",
                    "hidden": false
                },
                {
                    "_id": "68917696f01a094725f834bf",
                    "name": "Mark Gerstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T10:43:31.000Z",
            "submittedOnDailyAt": "2025-08-05T01:42:34.345Z",
            "title": "CellForge: Agentic Design of Virtual Cell Models",
            "submittedOnDailyBy": {
                "_id": "63357c608adfa81faf2ac180",
                "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
                "isPro": false,
                "fullname": "Xiangru Tang",
                "user": "RTT1",
                "type": "user"
            },
            "summary": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
            "upvotes": 29,
            "discussionId": "68917697f01a094725f834c2",
            "ai_summary": "CellForge, an agentic system using a multi-agent framework, transforms raw single-cell multi-omics data into optimized computational models for virtual cells, outperforming state-of-the-art methods in single-cell perturbation prediction.",
            "ai_keywords": [
                "multi-agent framework",
                "single-cell multi-omics data",
                "task analysis",
                "method design",
                "experiment execution",
                "LLM agents",
                "gene knockouts",
                "drug treatments",
                "cytokine stimulations"
            ]
        },
        "publishedAt": "2025-08-04T06:43:31.000Z",
        "title": "CellForge: Agentic Design of Virtual Cell Models",
        "summary": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02276.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63357c608adfa81faf2ac180",
            "avatarUrl": "/avatars/ae0314c644f882251baf59b9134fd36f.svg",
            "fullname": "Xiangru Tang",
            "name": "RTT1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02150",
            "authors": [
                {
                    "_id": "6891804cf01a094725f8350e",
                    "name": "Qingyu Ren",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f8350f",
                    "name": "Qianyu He",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83510",
                    "name": "Bowei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83511",
                    "name": "Jie Zeng",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83512",
                    "name": "Jiaqing Liang",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83513",
                    "name": "Yanghua Xiao",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83514",
                    "name": "Weikang Zhou",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83515",
                    "name": "Zeye Sun",
                    "hidden": false
                },
                {
                    "_id": "6891804cf01a094725f83516",
                    "name": "Fei Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T07:48:59.000Z",
            "submittedOnDailyAt": "2025-08-05T04:44:57.940Z",
            "title": "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following",
            "submittedOnDailyBy": {
                "_id": "636b36351340f879a2ec2bb1",
                "avatarUrl": "/avatars/260a1c15f9c14c967125469072020946.svg",
                "isPro": false,
                "fullname": "QianyuHe",
                "user": "Abbey4799",
                "type": "user"
            },
            "summary": "Reasoning models excel in complex problem solving but exhibit a concerning\ntrade off between reasoning capabilities and instruction following abilities.\nExisting approaches for improving instruction following rely on stronger\nexternal models, creating methodological bottlenecks and practical limitations\nincluding increased costs and accessibility constraints. We propose a\nself-supervised RL framework that leverages reasoning models' own internal\nsignals to improve instruction following capabilities without external\nsupervision. Extensive experiments demonstrate that our framework significantly\nimproves instruction following capabilities while maintaining reasoning\nperformance, offering a scalable and cost-effective approach to enhance\ninstruction following in reasoning models. The data and code are publicly\navailable at https://github.com/Rainier-rq/verl-if.",
            "upvotes": 25,
            "discussionId": "6891804df01a094725f83517",
            "githubRepo": "https://github.com/Rainier-rq/verl-if",
            "ai_summary": "A self-supervised RL framework enhances instruction following in reasoning models without external supervision, maintaining reasoning performance and offering scalability and cost-effectiveness.",
            "ai_keywords": [
                "self-supervised RL",
                "reasoning models",
                "instruction following",
                "internal signals",
                "scalability",
                "cost-effectiveness"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-08-04T03:48:59.000Z",
        "title": "Beyond the Trade-off: Self-Supervised Reinforcement Learning for\n  Reasoning Models' Instruction Following",
        "summary": "Reasoning models excel in complex problem solving but exhibit a concerning\ntrade off between reasoning capabilities and instruction following abilities.\nExisting approaches for improving instruction following rely on stronger\nexternal models, creating methodological bottlenecks and practical limitations\nincluding increased costs and accessibility constraints. We propose a\nself-supervised RL framework that leverages reasoning models' own internal\nsignals to improve instruction following capabilities without external\nsupervision. Extensive experiments demonstrate that our framework significantly\nimproves instruction following capabilities while maintaining reasoning\nperformance, offering a scalable and cost-effective approach to enhance\ninstruction following in reasoning models. The data and code are publicly\navailable at https://github.com/Rainier-rq/verl-if.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02150.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636b36351340f879a2ec2bb1",
            "avatarUrl": "/avatars/260a1c15f9c14c967125469072020946.svg",
            "fullname": "QianyuHe",
            "name": "Abbey4799",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01059",
            "authors": [
                {
                    "_id": "68916c3af01a094725f83460",
                    "name": "Sajana Weerawardhena",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83461",
                    "name": "Paul Kassianik",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83462",
                    "name": "Blaine Nelson",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83463",
                    "name": "Baturay Saglam",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83464",
                    "name": "Anu Vellore",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83465",
                    "name": "Aman Priyanshu",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83466",
                    "name": "Supriti Vijay",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83467",
                    "name": "Massimo Aufiero",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83468",
                    "name": "Arthur Goldblatt",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83469",
                    "name": "Fraser Burch",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346a",
                    "name": "Ed Li",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346b",
                    "name": "Jianliang He",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346c",
                    "name": "Dhruv Kedia",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346d",
                    "name": "Kojin Oshiba",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346e",
                    "name": "Zhouran Yang",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f8346f",
                    "name": "Yaron Singer",
                    "hidden": false
                },
                {
                    "_id": "68916c3af01a094725f83470",
                    "name": "Amin Karbasi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T20:25:57.000Z",
            "submittedOnDailyAt": "2025-08-05T01:01:25.435Z",
            "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report",
            "submittedOnDailyBy": {
                "_id": "620042b28c2eb991da50d34e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
                "isPro": true,
                "fullname": "Aman Priyanshu",
                "user": "AmanPriyanshu",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have shown remarkable success across many\ndomains, yet their integration into cybersecurity applications remains limited\ndue to a lack of general-purpose cybersecurity data, representational\ncomplexity, and safety and regulatory concerns. To address this gap, we\npreviously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable\nfor fine-tuning on downstream tasks. That model, however, was not designed for\nchat-style interactions or instruction-following. In this report, we release\nFoundation-Sec-8B-Instruct: a model specifically trained for general-purpose\ncybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific\nknowledge with instruction-following, conversational capabilities, and\nalignment with human preferences to produce high-quality, relevant responses.\nComprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms\nLlama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its\ninstruction-following performance. It is also competitive with GPT-4o-mini on\ncyber threat intelligence and instruction-following tasks. We envision\nFoundation-Sec-8B-Instruct becoming an indispensable assistant in the daily\nworkflows of cybersecurity professionals. We release the model publicly at\nhttps://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.",
            "upvotes": 21,
            "discussionId": "68916c3af01a094725f83471",
            "ai_summary": "Foundation-Sec-8B-Instruct is a cybersecurity-focused LLM designed for chat-style interactions and instruction-following, outperforming other models in cybersecurity tasks while matching their instruction-following capabilities.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "cybersecurity",
                "Foundation-Sec-8B",
                "fine-tuning",
                "instruction-following",
                "conversational capabilities",
                "human preferences",
                "cybersecurity dialogue",
                "Llama 3.1-8B-Instruct",
                "GPT-4o-mini",
                "cyber threat intelligence"
            ]
        },
        "publishedAt": "2025-08-01T16:25:57.000Z",
        "title": "Llama-3.1-FoundationAI-SecurityLLM-8B-Instruct Technical Report",
        "summary": "Large language models (LLMs) have shown remarkable success across many\ndomains, yet their integration into cybersecurity applications remains limited\ndue to a lack of general-purpose cybersecurity data, representational\ncomplexity, and safety and regulatory concerns. To address this gap, we\npreviously introduced Foundation-Sec-8B, a cybersecurity-focused LLM suitable\nfor fine-tuning on downstream tasks. That model, however, was not designed for\nchat-style interactions or instruction-following. In this report, we release\nFoundation-Sec-8B-Instruct: a model specifically trained for general-purpose\ncybersecurity dialogue. Built on Foundation-Sec-8B, it combines domain-specific\nknowledge with instruction-following, conversational capabilities, and\nalignment with human preferences to produce high-quality, relevant responses.\nComprehensive evaluations show that Foundation-Sec-8B-Instruct outperforms\nLlama 3.1-8B-Instruct on a range of cybersecurity tasks while matching its\ninstruction-following performance. It is also competitive with GPT-4o-mini on\ncyber threat intelligence and instruction-following tasks. We envision\nFoundation-Sec-8B-Instruct becoming an indispensable assistant in the daily\nworkflows of cybersecurity professionals. We release the model publicly at\nhttps://huggingface.co/fdtn-ai/Foundation-Sec-8B-Instruct.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01059.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620042b28c2eb991da50d34e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620042b28c2eb991da50d34e/Q5cj1GIq3XnKj-K64Mtyd.jpeg",
            "fullname": "Aman Priyanshu",
            "name": "AmanPriyanshu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02317",
            "authors": [
                {
                    "_id": "68917ae7f01a094725f834cd",
                    "name": "Qianli Ma",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834ce",
                    "name": "Yaowei Zheng",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834cf",
                    "name": "Zhelun Shi",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834d0",
                    "name": "Zhongkai Zhao",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834d1",
                    "name": "Bin Jia",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834d2",
                    "name": "Ziyue Huang",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834d3",
                    "name": "Zhiqi Lin",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834d4",
                    "name": "Youjie Li",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834d5",
                    "name": "Jiacheng Yang",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834d6",
                    "name": "Yanghua Peng",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834d7",
                    "name": "Zhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68917ae7f01a094725f834d8",
                    "name": "Xin Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T11:33:04.000Z",
            "submittedOnDailyAt": "2025-08-05T02:01:45.888Z",
            "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo",
            "submittedOnDailyBy": {
                "_id": "642fef28a043f0ac7defa8a9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
                "isPro": false,
                "fullname": "Yaowei Zheng",
                "user": "hiyouga",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
            "upvotes": 8,
            "discussionId": "68917ae8f01a094725f834d9",
            "githubRepo": "https://github.com/ByteDance-Seed/VeOmni",
            "ai_summary": "A modular training framework accelerates the development of omni-modal LLMs through efficient 3D parallelism and flexible configuration.",
            "ai_keywords": [
                "large language models",
                "omni-modal understanding",
                "omni-modal generation",
                "heterogeneous model architectures",
                "parallel logic",
                "model-centric distributed recipes",
                "communication",
                "computation",
                "3D parallelism",
                "flexible configuration interface",
                "mixture-of-experts",
                "tokens/sec/GPU throughput",
                "context lengths"
            ],
            "githubStars": 412
        },
        "publishedAt": "2025-08-04T07:33:04.000Z",
        "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo",
        "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. % We present \\veomni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. \\veomni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. \\veomni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. % Using \\veomni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02317.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642fef28a043f0ac7defa8a9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642fef28a043f0ac7defa8a9/RwOEkuj3fOnOA54tGR7Ea.png",
            "fullname": "Yaowei Zheng",
            "name": "hiyouga",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2462
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02137",
            "authors": [
                {
                    "_id": "689225bd9dcf0e7ac9a66ec4",
                    "name": "Zhongyue Zhang",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ec5",
                    "name": "Jiahua Rao",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ec6",
                    "name": "Jie Zhong",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ec7",
                    "name": "Weiqiang Bai",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ec8",
                    "name": "Dongxue Wang",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ec9",
                    "name": "Shaobo Ning",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66eca",
                    "name": "Lifeng Qiao",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ecb",
                    "name": "Sheng Xu",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ecc",
                    "name": "Runze Ma",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ecd",
                    "name": "Will Hua",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ece",
                    "name": "Jack Xiaoyu Chen",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ecf",
                    "name": "Odin Zhang",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed0",
                    "name": "Wei Lu",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed1",
                    "name": "Hanyi Feng",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed2",
                    "name": "He Yang",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed3",
                    "name": "Xinchao Shi",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed4",
                    "name": "Rui Li",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed5",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed6",
                    "name": "Xinzhu Ma",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed7",
                    "name": "Jiahao Wang",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed8",
                    "name": "Jixian Zhang",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66ed9",
                    "name": "Jia Duan",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66eda",
                    "name": "Siqi Sun",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66edb",
                    "name": "Jian Zhang",
                    "hidden": false
                },
                {
                    "_id": "689225bd9dcf0e7ac9a66edc",
                    "name": "Shuangjia Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T07:34:48.000Z",
            "submittedOnDailyAt": "2025-08-05T15:00:31.734Z",
            "title": "Fitness aligned structural modeling enables scalable virtual screening\n  with AuroBind",
            "submittedOnDailyBy": {
                "_id": "67d3a1a5943a965360fcae51",
                "avatarUrl": "/avatars/165ed684b0750e7f57b9f2babfb47a8c.svg",
                "isPro": false,
                "fullname": "Siqi Sun",
                "user": "siqisun",
                "type": "user"
            },
            "summary": "Most human proteins remain undrugged, over 96% of human proteins remain\nunexploited by approved therapeutics. While structure-based virtual screening\npromises to expand the druggable proteome, existing methods lack atomic-level\nprecision and fail to predict binding fitness, limiting translational impact.\nWe present AuroBind, a scalable virtual screening framework that fine-tunes a\ncustom atomic-level structural model on million-scale chemogenomic data.\nAuroBind integrates direct preference optimization, self-distillation from\nhigh-confidence complexes, and a teacher-student acceleration strategy to\njointly predict ligand-bound structures and binding fitness. The proposed\nmodels outperform state-of-the-art models on structural and functional\nbenchmarks while enabling 100,000-fold faster screening across ultra-large\ncompound libraries. In a prospective screen across ten disease-relevant\ntargets, AuroBind achieved experimental hit rates of 7-69%, with top compounds\nreaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and\nGPR160, AuroBind identified both agonists and antagonists with success rates of\n16-30%, and functional assays confirmed GPR160 modulation in liver and prostate\ncancer models. AuroBind offers a generalizable framework for structure-function\nlearning and high-throughput molecular screening, bridging the gap between\nstructure prediction and therapeutic discovery.",
            "upvotes": 8,
            "discussionId": "689225bd9dcf0e7ac9a66edd",
            "ai_summary": "AuroBind is a scalable virtual screening framework that fine-tunes atomic-level structural models to predict ligand-bound structures and binding fitness, achieving high hit rates in prospective screens across disease-relevant targets.",
            "ai_keywords": [
                "structure-based virtual screening",
                "druggable proteome",
                "atomic-level structural model",
                "direct preference optimization",
                "self-distillation",
                "teacher-student acceleration strategy",
                "ligand-bound structures",
                "binding fitness",
                "structural benchmarks",
                "functional benchmarks",
                "high-throughput molecular screening",
                "structure-function learning",
                "therapeutic discovery"
            ]
        },
        "publishedAt": "2025-08-04T03:34:48.000Z",
        "title": "Fitness aligned structural modeling enables scalable virtual screening\n  with AuroBind",
        "summary": "Most human proteins remain undrugged, over 96% of human proteins remain\nunexploited by approved therapeutics. While structure-based virtual screening\npromises to expand the druggable proteome, existing methods lack atomic-level\nprecision and fail to predict binding fitness, limiting translational impact.\nWe present AuroBind, a scalable virtual screening framework that fine-tunes a\ncustom atomic-level structural model on million-scale chemogenomic data.\nAuroBind integrates direct preference optimization, self-distillation from\nhigh-confidence complexes, and a teacher-student acceleration strategy to\njointly predict ligand-bound structures and binding fitness. The proposed\nmodels outperform state-of-the-art models on structural and functional\nbenchmarks while enabling 100,000-fold faster screening across ultra-large\ncompound libraries. In a prospective screen across ten disease-relevant\ntargets, AuroBind achieved experimental hit rates of 7-69%, with top compounds\nreaching sub-nanomolar to picomolar potency. For the orphan GPCRs GPR151 and\nGPR160, AuroBind identified both agonists and antagonists with success rates of\n16-30%, and functional assays confirmed GPR160 modulation in liver and prostate\ncancer models. AuroBind offers a generalizable framework for structure-function\nlearning and high-throughput molecular screening, bridging the gap between\nstructure prediction and therapeutic discovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02137.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d3a1a5943a965360fcae51",
            "avatarUrl": "/avatars/165ed684b0750e7f57b9f2babfb47a8c.svg",
            "fullname": "Siqi Sun",
            "name": "siqisun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.17520",
            "authors": [
                {
                    "_id": "68917181f01a094725f8348d",
                    "name": "Shuai Yang",
                    "hidden": false
                },
                {
                    "_id": "68917181f01a094725f8348e",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "68917181f01a094725f8348f",
                    "name": "Yilun Chen",
                    "hidden": false
                },
                {
                    "_id": "68917181f01a094725f83490",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "68917181f01a094725f83491",
                    "name": "Yang Tian",
                    "hidden": false
                },
                {
                    "_id": "68917181f01a094725f83492",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "68917181f01a094725f83493",
                    "name": "Hanqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68917181f01a094725f83494",
                    "name": "Feng Zhao",
                    "hidden": false
                },
                {
                    "_id": "68917181f01a094725f83495",
                    "name": "Yiyi Liao",
                    "hidden": false
                },
                {
                    "_id": "68917181f01a094725f83496",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-23T13:57:06.000Z",
            "submittedOnDailyAt": "2025-08-05T01:24:22.392Z",
            "title": "InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation",
            "submittedOnDailyBy": {
                "_id": "64548f6c363bb3aaf9cba136",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64548f6c363bb3aaf9cba136/HqJL9HQ5CWVOJCsHyuMOm.jpeg",
                "isPro": false,
                "fullname": "Shuai Yang",
                "user": "ShuaiYang03",
                "type": "user"
            },
            "summary": "To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.",
            "upvotes": 8,
            "discussionId": "68917182f01a094725f83497",
            "ai_summary": "InstructVLA is an end-to-end vision-language-action model that enhances manipulation performance while preserving vision-language reasoning through multimodal training and mixture-of-experts adaptation.",
            "ai_keywords": [
                "vision-language-action (VLA) models",
                "multimodal reasoning",
                "precise action generation",
                "catastrophic forgetting",
                "large vision-language models (VLMs)",
                "Vision-Language-Action Instruction Tuning (VLA-IT)",
                "mixture-of-experts adaptation",
                "SimplerEnv tasks",
                "SimplerEnv-Instruct",
                "OpenVLA",
                "GPT-4o",
                "multimodal tasks",
                "inference-time scaling",
                "policy learning"
            ]
        },
        "publishedAt": "2025-07-23T09:57:06.000Z",
        "title": "InstructVLA: Vision-Language-Action Instruction Tuning from\n  Understanding to Manipulation",
        "summary": "To operate effectively in the real world, robots must integrate multimodal\nreasoning with precise action generation. However, existing\nvision-language-action (VLA) models often sacrifice one for the other, narrow\ntheir abilities to task-specific manipulation data, and suffer catastrophic\nforgetting of pre-trained vision-language capabilities. To bridge this gap, we\nintroduce InstructVLA, an end-to-end VLA model that preserves the flexible\nreasoning of large vision-language models (VLMs) while delivering leading\nmanipulation performance. InstructVLA introduces a novel training paradigm,\nVision-Language-Action Instruction Tuning (VLA-IT), which employs multimodal\ntraining with mixture-of-experts adaptation to jointly optimize textual\nreasoning and action generation on both standard VLM corpora and a curated\n650K-sample VLA-IT dataset. On in-domain SimplerEnv tasks, InstructVLA achieves\n30.5% improvement over SpatialVLA. To evaluate generalization, we introduce\nSimplerEnv-Instruct, an 80-task benchmark requiring closed-loop control and\nhigh-level instruction understanding, where it outperforms a fine-tuned OpenVLA\nby 92% and an action expert aided by GPT-4o by 29%. Additionally, InstructVLA\nsurpasses baseline VLMs on multimodal tasks and exhibits inference-time scaling\nby leveraging textual reasoning to boost manipulation performance in both\nsimulated and real-world settings. These results demonstrate InstructVLA's\npotential for bridging intuitive and steerable human-robot interaction with\nefficient policy learning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.17520.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64548f6c363bb3aaf9cba136",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64548f6c363bb3aaf9cba136/HqJL9HQ5CWVOJCsHyuMOm.jpeg",
            "fullname": "Shuai Yang",
            "name": "ShuaiYang03",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02271",
            "authors": [
                {
                    "_id": "6891f8409373cd501b6a2d82",
                    "name": "Kenneth Enevoldsen",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d83",
                    "name": "Kristian Nrgaard Jensen",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d84",
                    "name": "Jan Kostkan",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d85",
                    "name": "Balzs Szab",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d86",
                    "name": "Mrton Kardos",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d87",
                    "name": "Kirten Vad",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d88",
                    "name": "Andrea Blasi Nez",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d89",
                    "name": "Gianluca Barmina",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d8a",
                    "name": "Jacob Nielsen",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d8b",
                    "name": "Rasmus Larsen",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d8c",
                    "name": "Peter Vahlstrup",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d8d",
                    "name": "Per Mldrup Dalum",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d8e",
                    "name": "Desmond Elliott",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d8f",
                    "name": "Lukas Galke",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d90",
                    "name": "Peter Schneider-Kamp",
                    "hidden": false
                },
                {
                    "_id": "6891f8409373cd501b6a2d91",
                    "name": "Kristoffer Nielbo",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/uKwjTi4ugIjry2f4QrSp6.png",
                "https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/-t7eoSuOlVvFv_eaqVMPP.png",
                "https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/poPB0FzjojDZZWTh3wTfU.png"
            ],
            "publishedAt": "2025-08-04T10:30:42.000Z",
            "submittedOnDailyAt": "2025-08-05T18:32:26.107Z",
            "title": "Dynaword: From One-shot to Continuously Developed Datasets",
            "submittedOnDailyBy": {
                "_id": "5ff5943752c26e9bc240bada",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5ff5943752c26e9bc240bada/Exyzf3C_gJ2KdsL4K5_cq.png",
                "isPro": false,
                "fullname": "Kenneth C. Enevoldsen",
                "user": "KennethEnevoldsen",
                "type": "user"
            },
            "summary": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution.",
            "upvotes": 7,
            "discussionId": "6891f8409373cd501b6a2d92",
            "ai_summary": "A framework called Dynaword and its implementation Danish Dynaword enable community-driven, open, and continuously updated large-scale natural language datasets.",
            "ai_keywords": [
                "Dynaword",
                "Danish Dynaword"
            ]
        },
        "publishedAt": "2025-08-04T06:30:42.000Z",
        "title": "Dynaword: From One-shot to Continuously Developed Datasets",
        "summary": "Large-scale datasets are foundational for research and development in natural\nlanguage processing. However, current approaches face three key challenges: (1)\nreliance on ambiguously licensed sources restricting use, sharing, and\nderivative works; (2) static dataset releases that prevent community\ncontributions and diminish longevity; and (3) quality assurance processes\nrestricted to publishing teams rather than leveraging community expertise.\n  To address these limitations, we introduce two contributions: the Dynaword\napproach and Danish Dynaword. The Dynaword approach is a framework for creating\nlarge-scale, open datasets that can be continuously updated through community\ncollaboration. Danish Dynaword is a concrete implementation that validates this\napproach and demonstrates its potential. Danish Dynaword contains over four\ntimes as many tokens as comparable releases, is exclusively openly licensed,\nand has received multiple contributions across industry and research. The\nrepository includes light-weight tests to ensure data formatting, quality, and\ndocumentation, establishing a sustainable framework for ongoing community\ncontributions and dataset evolution.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/uKwjTi4ugIjry2f4QrSp6.png",
            "https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/-t7eoSuOlVvFv_eaqVMPP.png",
            "https://cdn-uploads.huggingface.co/production/uploads/5ff5943752c26e9bc240bada/poPB0FzjojDZZWTh3wTfU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02271.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5ff5943752c26e9bc240bada",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5ff5943752c26e9bc240bada/Exyzf3C_gJ2KdsL4K5_cq.png",
            "fullname": "Kenneth C. Enevoldsen",
            "name": "KennethEnevoldsen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 38
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01548",
            "authors": [
                {
                    "_id": "689186aef01a094725f83537",
                    "name": "Quan-Sheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "689186aef01a094725f83538",
                    "name": "Yunheng Li",
                    "hidden": false
                },
                {
                    "_id": "689186aef01a094725f83539",
                    "name": "Qilong Wang",
                    "hidden": false
                },
                {
                    "_id": "689186aef01a094725f8353a",
                    "name": "Peng-Tao Jiang",
                    "hidden": false
                },
                {
                    "_id": "689186aef01a094725f8353b",
                    "name": "Zuxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "689186aef01a094725f8353c",
                    "name": "Ming-Ming Cheng",
                    "hidden": false
                },
                {
                    "_id": "689186aef01a094725f8353d",
                    "name": "Qibin Hou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-03T02:15:43.000Z",
            "submittedOnDailyAt": "2025-08-05T05:07:36.709Z",
            "title": "A Glimpse to Compress: Dynamic Visual Token Pruning for Large\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "66ef2611fcc1c455f8dce832",
                "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
                "isPro": false,
                "fullname": "Boyuan Sun",
                "user": "BBBBCHAN",
                "type": "user"
            },
            "summary": "Visual token compression is critical for Large Vision-Language Models (LVLMs)\nto efficiently process high-resolution inputs. Existing methods that typically\nadopt fixed compression ratios cannot adapt to scenes of varying complexity,\noften causing imprecise pruning that discards informative visual tokens and\nresults in degraded model performance. To address this issue, we introduce a\ndynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes\na data-driven ''glimpse'' and prunes irrelevant visual tokens in a single\nforward pass before answer generation. This approach prunes 92.6% of visual\ntokens while on average fully retaining the baseline performance on free-form\nVQA tasks. The reduced computational cost also enables more effective\nfine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline\nperformance while maintaining a similarly high pruning rate. Our work paves a\nnew way for building more powerful and efficient LVLMs.",
            "upvotes": 7,
            "discussionId": "689186aef01a094725f8353e",
            "githubRepo": "https://github.com/HVision-NKU/GlimpsePrune",
            "ai_summary": "A dynamic pruning framework, GlimpsePrune, improves efficiency in Large Vision-Language Models by adaptively removing irrelevant visual tokens without degrading performance.",
            "ai_keywords": [
                "Visual token compression",
                "Large Vision-Language Models",
                "dynamic pruning",
                "GlimpsePrune",
                "free-form VQA tasks",
                "parameter-efficient fine-tuning"
            ],
            "githubStars": 28
        },
        "publishedAt": "2025-08-02T22:15:43.000Z",
        "title": "A Glimpse to Compress: Dynamic Visual Token Pruning for Large\n  Vision-Language Models",
        "summary": "Visual token compression is critical for Large Vision-Language Models (LVLMs)\nto efficiently process high-resolution inputs. Existing methods that typically\nadopt fixed compression ratios cannot adapt to scenes of varying complexity,\noften causing imprecise pruning that discards informative visual tokens and\nresults in degraded model performance. To address this issue, we introduce a\ndynamic pruning framework, GlimpsePrune, inspired by human cognition. It takes\na data-driven ''glimpse'' and prunes irrelevant visual tokens in a single\nforward pass before answer generation. This approach prunes 92.6% of visual\ntokens while on average fully retaining the baseline performance on free-form\nVQA tasks. The reduced computational cost also enables more effective\nfine-tuning: an enhanced GlimpsePrune+ achieves 110% of the baseline\nperformance while maintaining a similarly high pruning rate. Our work paves a\nnew way for building more powerful and efficient LVLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01548.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66ef2611fcc1c455f8dce832",
            "avatarUrl": "/avatars/c73ef2dfcd1e6ec8414a31226ad38e3b.svg",
            "fullname": "Boyuan Sun",
            "name": "BBBBCHAN",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01691",
            "authors": [
                {
                    "_id": "6891c8a6c99fe2df02fc8543",
                    "name": "Tiantian Feng",
                    "hidden": false
                },
                {
                    "_id": "6891c8a6c99fe2df02fc8544",
                    "name": "Kevin Huang",
                    "hidden": false
                },
                {
                    "_id": "6891c8a6c99fe2df02fc8545",
                    "name": "Anfeng Xu",
                    "hidden": false
                },
                {
                    "_id": "6891c8a6c99fe2df02fc8546",
                    "name": "Xuan Shi",
                    "hidden": false
                },
                {
                    "_id": "6891c8a6c99fe2df02fc8547",
                    "name": "Thanathai Lertpetchpun",
                    "hidden": false
                },
                {
                    "_id": "6891c8a6c99fe2df02fc8548",
                    "name": "Jihwan Lee",
                    "hidden": false
                },
                {
                    "_id": "6891c8a6c99fe2df02fc8549",
                    "name": "Yoonjeong Lee",
                    "hidden": false
                },
                {
                    "_id": "6891c8a6c99fe2df02fc854a",
                    "name": "Dani Byrd",
                    "hidden": false
                },
                {
                    "_id": "6891c8a6c99fe2df02fc854b",
                    "name": "Shrikanth Narayanan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-03T09:51:28.000Z",
            "submittedOnDailyAt": "2025-08-05T10:09:37.035Z",
            "title": "Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and\n  Regional Languages Around the Globe",
            "submittedOnDailyBy": {
                "_id": "64092a1ab6a334f53e278b3b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64092a1ab6a334f53e278b3b/tcueLWyyDL6WMUTw3Or4t.jpeg",
                "isPro": false,
                "fullname": "Tiantian Feng",
                "user": "tiantiaf",
                "type": "user"
            },
            "summary": "We present Voxlect, a novel benchmark for modeling dialects and regional\nlanguages worldwide using speech foundation models. Specifically, we report\ncomprehensive benchmark evaluations on dialects and regional language varieties\nin English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,\nSpanish, French, German, Brazilian Portuguese, and Italian. Our study used over\n2 million training utterances from 30 publicly available speech corpora that\nare provided with dialectal information. We evaluate the performance of several\nwidely used speech foundation models in classifying speech dialects. We assess\nthe robustness of the dialectal models under noisy conditions and present an\nerror analysis that highlights modeling results aligned with geographic\ncontinuity. In addition to benchmarking dialect classification, we demonstrate\nseveral downstream applications enabled by Voxlect. Specifically, we show that\nVoxlect can be applied to augment existing speech recognition datasets with\ndialect information, enabling a more detailed analysis of ASR performance\nacross dialectal variations. Voxlect is also used as a tool to evaluate the\nperformance of speech generation systems. Voxlect is publicly available with\nthe license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.",
            "upvotes": 6,
            "discussionId": "6891c8a7c99fe2df02fc854c",
            "githubRepo": "https://github.com/tiantiaf0627/voxlect",
            "ai_summary": "Voxlect is a benchmark for evaluating speech foundation models on dialect classification and downstream applications across multiple languages and dialects.",
            "ai_keywords": [
                "speech foundation models",
                "dialect classification",
                "speech corpora",
                "dialectal information",
                "ASR performance",
                "speech generation systems"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-08-03T05:51:28.000Z",
        "title": "Voxlect: A Speech Foundation Model Benchmark for Modeling Dialects and\n  Regional Languages Around the Globe",
        "summary": "We present Voxlect, a novel benchmark for modeling dialects and regional\nlanguages worldwide using speech foundation models. Specifically, we report\ncomprehensive benchmark evaluations on dialects and regional language varieties\nin English, Arabic, Mandarin and Cantonese, Tibetan, Indic languages, Thai,\nSpanish, French, German, Brazilian Portuguese, and Italian. Our study used over\n2 million training utterances from 30 publicly available speech corpora that\nare provided with dialectal information. We evaluate the performance of several\nwidely used speech foundation models in classifying speech dialects. We assess\nthe robustness of the dialectal models under noisy conditions and present an\nerror analysis that highlights modeling results aligned with geographic\ncontinuity. In addition to benchmarking dialect classification, we demonstrate\nseveral downstream applications enabled by Voxlect. Specifically, we show that\nVoxlect can be applied to augment existing speech recognition datasets with\ndialect information, enabling a more detailed analysis of ASR performance\nacross dialectal variations. Voxlect is also used as a tool to evaluate the\nperformance of speech generation systems. Voxlect is publicly available with\nthe license of the RAIL family at: https://github.com/tiantiaf0627/voxlect.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01691.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64092a1ab6a334f53e278b3b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64092a1ab6a334f53e278b3b/tcueLWyyDL6WMUTw3Or4t.jpeg",
            "fullname": "Tiantian Feng",
            "name": "tiantiaf",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01151",
            "authors": [
                {
                    "_id": "68917c7af01a094725f834db",
                    "name": "Yu Lei",
                    "hidden": false
                },
                {
                    "_id": "68917c7af01a094725f834dc",
                    "name": "Jinbin Bai",
                    "hidden": false
                },
                {
                    "_id": "68917c7af01a094725f834dd",
                    "name": "Qingyu Shi",
                    "hidden": false
                },
                {
                    "_id": "68917c7af01a094725f834de",
                    "name": "Aosong Feng",
                    "hidden": false
                },
                {
                    "_id": "68917c7af01a094725f834df",
                    "name": "Kaidong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T02:23:20.000Z",
            "submittedOnDailyAt": "2025-08-05T02:08:49.581Z",
            "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "63fccdac93b993a4ebd7789a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
                "isPro": false,
                "fullname": "Jinbin Bai",
                "user": "BryanW",
                "type": "user"
            },
            "summary": "Text-to-image diffusion models have revolutionized visual content generation,\nbut current safety mechanisms apply uniform standards that often fail to\naccount for individual user preferences. These models overlook the diverse\nsafety boundaries shaped by factors like age, mental health, and personal\nbeliefs. To address this, we propose Personalized Safety Alignment (PSA), a\nframework that allows user-specific control over safety behaviors in generative\nmodels. PSA integrates personalized user profiles into the diffusion process,\nadjusting the model's behavior to match individual safety preferences while\npreserving image quality. We introduce a new dataset, Sage, which captures\nuser-specific safety preferences and incorporates these profiles through a\ncross-attention mechanism. Experiments show that PSA outperforms existing\nmethods in harmful content suppression and aligns generated content better with\nuser constraints, achieving higher Win Rate and Pass Rate scores. Our code,\ndata, and models are publicly available at\nhttps://torpedo2648.github.io/PSAlign/.",
            "upvotes": 6,
            "discussionId": "68917c7af01a094725f834e0",
            "ai_summary": "A personalized safety alignment framework integrates user-specific profiles into text-to-image diffusion models to better align generated content with individual safety preferences.",
            "ai_keywords": [
                "text-to-image diffusion models",
                "Personalized Safety Alignment (PSA)",
                "user-specific profiles",
                "cross-attention mechanism",
                "harmful content suppression",
                "Win Rate",
                "Pass Rate"
            ]
        },
        "publishedAt": "2025-08-01T22:23:20.000Z",
        "title": "Personalized Safety Alignment for Text-to-Image Diffusion Models",
        "summary": "Text-to-image diffusion models have revolutionized visual content generation,\nbut current safety mechanisms apply uniform standards that often fail to\naccount for individual user preferences. These models overlook the diverse\nsafety boundaries shaped by factors like age, mental health, and personal\nbeliefs. To address this, we propose Personalized Safety Alignment (PSA), a\nframework that allows user-specific control over safety behaviors in generative\nmodels. PSA integrates personalized user profiles into the diffusion process,\nadjusting the model's behavior to match individual safety preferences while\npreserving image quality. We introduce a new dataset, Sage, which captures\nuser-specific safety preferences and incorporates these profiles through a\ncross-attention mechanism. Experiments show that PSA outperforms existing\nmethods in harmful content suppression and aligns generated content better with\nuser constraints, achieving higher Win Rate and Pass Rate scores. Our code,\ndata, and models are publicly available at\nhttps://torpedo2648.github.io/PSAlign/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01151.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fccdac93b993a4ebd7789a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63fccdac93b993a4ebd7789a/KRx8vpdoDjsZBRw0j8Vg8.jpeg",
            "fullname": "Jinbin Bai",
            "name": "BryanW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02558",
            "authors": [
                {
                    "_id": "689229189dcf0e7ac9a66edf",
                    "name": "Yuerong Song",
                    "hidden": false
                },
                {
                    "_id": "689229189dcf0e7ac9a66ee0",
                    "name": "Xiaoran Liu",
                    "hidden": false
                },
                {
                    "_id": "689229189dcf0e7ac9a66ee1",
                    "name": "Ruixiao Li",
                    "hidden": false
                },
                {
                    "_id": "689229189dcf0e7ac9a66ee2",
                    "name": "Zhigeng Liu",
                    "hidden": false
                },
                {
                    "_id": "689229189dcf0e7ac9a66ee3",
                    "name": "Zengfeng Huang",
                    "hidden": false
                },
                {
                    "_id": "689229189dcf0e7ac9a66ee4",
                    "name": "Qipeng Guo",
                    "hidden": false
                },
                {
                    "_id": "689229189dcf0e7ac9a66ee5",
                    "name": "Ziwei He",
                    "hidden": false
                },
                {
                    "_id": "689229189dcf0e7ac9a66ee6",
                    "name": "Xipeng Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T16:14:03.000Z",
            "submittedOnDailyAt": "2025-08-05T14:25:20.913Z",
            "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
            "submittedOnDailyBy": {
                "_id": "64f033ef82c6eea604c4da8b",
                "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
                "isPro": false,
                "fullname": "Liu Xiaoran",
                "user": "LiuXR",
                "type": "user"
            },
            "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10times higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
            "upvotes": 5,
            "discussionId": "689229189dcf0e7ac9a66ee7",
            "ai_summary": "Sparse-dLLM improves the efficiency of diffusion large language models by implementing dynamic cache eviction and sparse attention, enhancing throughput without compromising performance.",
            "ai_keywords": [
                "diffusion large language models",
                "dLLMs",
                "computational complexity",
                "memory overhead",
                "caching techniques",
                "attention patterns",
                "cross-layer sparsity",
                "pivotal tokens",
                "low-relevance tokens",
                "selective cache eviction",
                "sparse attention",
                "delayed bidirectional sparse caching",
                "token saliency",
                "throughput",
                "peak memory costs"
            ]
        },
        "publishedAt": "2025-08-04T12:14:03.000Z",
        "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
        "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10times higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02558.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f033ef82c6eea604c4da8b",
            "avatarUrl": "/avatars/51b93fea7fd68b4274ee03701245dcca.svg",
            "fullname": "Liu Xiaoran",
            "name": "LiuXR",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01415",
            "authors": [
                {
                    "_id": "689183e9f01a094725f83527",
                    "name": "Mingcong Lei",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f83528",
                    "name": "Honghao Cai",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f83529",
                    "name": "Zezhou Cui",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f8352a",
                    "name": "Liangchen Tan",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f8352b",
                    "name": "Junkun Hong",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f8352c",
                    "name": "Gehan Hu",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f8352d",
                    "name": "Shuangyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f8352e",
                    "name": "Yimou Wu",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f8352f",
                    "name": "Shaohan Jiang",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f83530",
                    "name": "Ge Wang",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f83531",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f83532",
                    "name": "Shuguang Cui",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f83533",
                    "name": "Yiming Zhao",
                    "hidden": false
                },
                {
                    "_id": "689183e9f01a094725f83534",
                    "name": "Yatong Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T15:39:42.000Z",
            "submittedOnDailyAt": "2025-08-05T02:39:59.148Z",
            "title": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems",
            "submittedOnDailyBy": {
                "_id": "6628c6107751d297d7025a71",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
                "isPro": false,
                "fullname": "Lei Mingcong",
                "user": "SP4595",
                "type": "user"
            },
            "summary": "We present RoboMemory, a brain-inspired multi-memory framework for lifelong\nlearning in physical embodied systems, addressing critical challenges in\nreal-world environments: continuous learning, multi-module memory latency, task\ncorrelation capture, and infinite-loop mitigation in closed-loop planning.\nGrounded in cognitive neuroscience, it integrates four core modules: the\nInformation Preprocessor (thalamus-like), the Lifelong Embodied Memory System\n(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and\nthe Low-Level Executer (cerebellum-like) to enable long-term planning and\ncumulative learning. The Lifelong Embodied Memory System, central to the\nframework, alleviates inference speed issues in complex memory frameworks via\nparallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic\nsubmodules. It incorporates a dynamic Knowledge Graph (KG) and consistent\narchitectural design to enhance memory consistency and scalability. Evaluations\non EmbodiedBench show RoboMemory outperforms the open-source baseline\n(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the\nclosed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing\nnew SOTA. Ablation studies validate key components (critic, spatial memory,\nlong-term memory), while real-world deployment confirms its lifelong learning\ncapability with significantly improved success rates across repeated tasks.\nRoboMemory alleviates high latency challenges with scalability, serving as a\nfoundational reference for integrating multi-modal memory systems in physical\nrobots.",
            "upvotes": 4,
            "discussionId": "689183eaf01a094725f83535",
            "ai_summary": "RoboMemory, a brain-inspired multi-memory framework, enhances lifelong learning in physical robots by integrating cognitive neuroscience principles and achieving state-of-the-art performance in real-world tasks.",
            "ai_keywords": [
                "Information Preprocessor",
                "Lifelong Embodied Memory System",
                "Closed-Loop Planning Module",
                "Low-Level Executer",
                "Spatial submodule",
                "Temporal submodule",
                "Episodic submodule",
                "Semantic submodule",
                "Knowledge Graph",
                "dynamic Knowledge Graph",
                "consistent architectural design",
                "memory consistency",
                "memory scalability",
                "lifelong learning",
                "ablation studies",
                "real-world deployment",
                "EmbodiedBench",
                "Qwen2.5-VL-72B-Ins",
                "Claude3.5-Sonnet",
                "state-of-the-art",
                "SOTA",
                "critic",
                "spatial memory",
                "long-term memory"
            ]
        },
        "publishedAt": "2025-08-02T11:39:42.000Z",
        "title": "RoboMemory: A Brain-inspired Multi-memory Agentic Framework for Lifelong\n  Learning in Physical Embodied Systems",
        "summary": "We present RoboMemory, a brain-inspired multi-memory framework for lifelong\nlearning in physical embodied systems, addressing critical challenges in\nreal-world environments: continuous learning, multi-module memory latency, task\ncorrelation capture, and infinite-loop mitigation in closed-loop planning.\nGrounded in cognitive neuroscience, it integrates four core modules: the\nInformation Preprocessor (thalamus-like), the Lifelong Embodied Memory System\n(hippocampus-like), the Closed-Loop Planning Module (prefrontal lobe-like), and\nthe Low-Level Executer (cerebellum-like) to enable long-term planning and\ncumulative learning. The Lifelong Embodied Memory System, central to the\nframework, alleviates inference speed issues in complex memory frameworks via\nparallelized updates/retrieval across Spatial, Temporal, Episodic, and Semantic\nsubmodules. It incorporates a dynamic Knowledge Graph (KG) and consistent\narchitectural design to enhance memory consistency and scalability. Evaluations\non EmbodiedBench show RoboMemory outperforms the open-source baseline\n(Qwen2.5-VL-72B-Ins) by 25% in average success rate and surpasses the\nclosed-source State-of-the-Art (SOTA) (Claude3.5-Sonnet) by 5%, establishing\nnew SOTA. Ablation studies validate key components (critic, spatial memory,\nlong-term memory), while real-world deployment confirms its lifelong learning\ncapability with significantly improved success rates across repeated tasks.\nRoboMemory alleviates high latency challenges with scalability, serving as a\nfoundational reference for integrating multi-modal memory systems in physical\nrobots.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01415.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6628c6107751d297d7025a71",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6628c6107751d297d7025a71/S1rm5VIwV2Uxfv8GetKMU.jpeg",
            "fullname": "Lei Mingcong",
            "name": "SP4595",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01408",
            "authors": [
                {
                    "_id": "6891f2119373cd501b6a2d6c",
                    "name": "Tarian Fu",
                    "hidden": false
                },
                {
                    "_id": "6891f2119373cd501b6a2d6d",
                    "name": "Javier Conde",
                    "hidden": false
                },
                {
                    "_id": "6891f2119373cd501b6a2d6e",
                    "name": "Gonzalo Martnez",
                    "hidden": false
                },
                {
                    "_id": "6891f2119373cd501b6a2d6f",
                    "name": "Pedro Reviriego",
                    "hidden": false
                },
                {
                    "_id": "6891f2119373cd501b6a2d70",
                    "name": "Elena Merino-Gmez",
                    "hidden": false
                },
                {
                    "_id": "6891f2119373cd501b6a2d71",
                    "name": "Fernando Moral",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T15:27:31.000Z",
            "submittedOnDailyAt": "2025-08-05T10:31:08.261Z",
            "title": "Artificial Intelligence and Misinformation in Art: Can Vision Language\n  Models Judge the Hand or the Machine Behind the Canvas?",
            "submittedOnDailyBy": {
                "_id": "6574f66b06fdcd4ca9491299",
                "avatarUrl": "/avatars/e31821949d75efb750ab2d9ebe12b9a8.svg",
                "isPro": false,
                "fullname": "pedro reviriego",
                "user": "reviriego",
                "type": "user"
            },
            "summary": "The attribution of artworks in general and of paintings in particular has\nalways been an issue in art. The advent of powerful artificial intelligence\nmodels that can generate and analyze images creates new challenges for painting\nattribution. On the one hand, AI models can create images that mimic the style\nof a painter, which can be incorrectly attributed, for example, by other AI\nmodels. On the other hand, AI models may not be able to correctly identify the\nartist for real paintings, inducing users to incorrectly attribute paintings.\nIn this paper, both problems are experimentally studied using state-of-the-art\nAI models for image generation and analysis on a large dataset with close to\n40,000 paintings from 128 artists. The results show that vision language models\nhave limited capabilities to: 1) perform canvas attribution and 2) to identify\nAI generated images. As users increasingly rely on queries to AI models to get\ninformation, these results show the need to improve the capabilities of VLMs to\nreliably perform artist attribution and detection of AI generated images to\nprevent the spread of incorrect information.",
            "upvotes": 4,
            "discussionId": "6891f2129373cd501b6a2d72",
            "ai_summary": "State-of-the-art vision language models struggle with accurately attributing artists and distinguishing AI-generated images, highlighting the need for improvement to prevent misinformation.",
            "ai_keywords": [
                "vision language models",
                "canvas attribution",
                "AI generated images"
            ]
        },
        "publishedAt": "2025-08-02T11:27:31.000Z",
        "title": "Artificial Intelligence and Misinformation in Art: Can Vision Language\n  Models Judge the Hand or the Machine Behind the Canvas?",
        "summary": "The attribution of artworks in general and of paintings in particular has\nalways been an issue in art. The advent of powerful artificial intelligence\nmodels that can generate and analyze images creates new challenges for painting\nattribution. On the one hand, AI models can create images that mimic the style\nof a painter, which can be incorrectly attributed, for example, by other AI\nmodels. On the other hand, AI models may not be able to correctly identify the\nartist for real paintings, inducing users to incorrectly attribute paintings.\nIn this paper, both problems are experimentally studied using state-of-the-art\nAI models for image generation and analysis on a large dataset with close to\n40,000 paintings from 128 artists. The results show that vision language models\nhave limited capabilities to: 1) perform canvas attribution and 2) to identify\nAI generated images. As users increasingly rely on queries to AI models to get\ninformation, these results show the need to improve the capabilities of VLMs to\nreliably perform artist attribution and detection of AI generated images to\nprevent the spread of incorrect information.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01408.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6574f66b06fdcd4ca9491299",
            "avatarUrl": "/avatars/e31821949d75efb750ab2d9ebe12b9a8.svg",
            "fullname": "pedro reviriego",
            "name": "reviriego",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01287",
            "authors": [
                {
                    "_id": "6891995ff01a094725f83594",
                    "name": "Micah Rentschler",
                    "hidden": false
                },
                {
                    "_id": "6891995ff01a094725f83595",
                    "name": "Jesse Roberts",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T09:42:59.000Z",
            "submittedOnDailyAt": "2025-08-05T04:13:13.413Z",
            "title": "Exploitation Is All You Need... for Exploration",
            "submittedOnDailyBy": {
                "_id": "66fffe1b3ec4cc293d40f2d5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fffe1b3ec4cc293d40f2d5/-6Ff7rSQ_fvqBmTQRTlB4.png",
                "isPro": false,
                "fullname": "Micah Rentschler",
                "user": "micahr234",
                "type": "user"
            },
            "summary": "Ensuring sufficient exploration is a central challenge when training\nmeta-reinforcement learning (meta-RL) agents to solve novel environments.\nConventional solutions to the exploration-exploitation dilemma inject explicit\nincentives such as randomization, uncertainty bonuses, or intrinsic rewards to\nencourage exploration. In this work, we hypothesize that an agent trained\nsolely to maximize a greedy (exploitation-only) objective can nonetheless\nexhibit emergent exploratory behavior, provided three conditions are met: (1)\nRecurring Environmental Structure, where the environment features repeatable\nregularities that allow past experience to inform future choices; (2) Agent\nMemory, enabling the agent to retain and utilize historical interaction data;\nand (3) Long-Horizon Credit Assignment, where learning propagates returns over\na time frame sufficient for the delayed benefits of exploration to inform\ncurrent decisions. Through experiments in stochastic multi-armed bandits and\ntemporally extended gridworlds, we observe that, when both structure and memory\nare present, a policy trained on a strictly greedy objective exhibits\ninformation-seeking exploratory behavior. We further demonstrate, through\ncontrolled ablations, that emergent exploration vanishes if either\nenvironmental structure or agent memory is absent (Conditions 1 & 2).\nSurprisingly, removing long-horizon credit assignment (Condition 3) does not\nalways prevent emergent exploration-a result we attribute to the\npseudo-Thompson Sampling effect. These findings suggest that, under the right\nprerequisites, exploration and exploitation need not be treated as orthogonal\nobjectives but can emerge from a unified reward-maximization process.",
            "upvotes": 3,
            "discussionId": "68919960f01a094725f83596",
            "ai_summary": "Meta-reinforcement learning agents can exhibit exploratory behavior when trained with a greedy objective, provided the environment has recurring structure, the agent has memory, and long-horizon credit assignment is possible.",
            "ai_keywords": [
                "meta-reinforcement learning",
                "meta-RL",
                "exploration-exploitation dilemma",
                "intrinsic rewards",
                "greedy objective",
                "emergent exploratory behavior",
                "Recurring Environmental Structure",
                "Agent Memory",
                "Long-Horizon Credit Assignment",
                "stochastic multi-armed bandits",
                "temporally extended gridworlds",
                "pseudo-Thompson Sampling"
            ]
        },
        "publishedAt": "2025-08-02T05:42:59.000Z",
        "title": "Exploitation Is All You Need... for Exploration",
        "summary": "Ensuring sufficient exploration is a central challenge when training\nmeta-reinforcement learning (meta-RL) agents to solve novel environments.\nConventional solutions to the exploration-exploitation dilemma inject explicit\nincentives such as randomization, uncertainty bonuses, or intrinsic rewards to\nencourage exploration. In this work, we hypothesize that an agent trained\nsolely to maximize a greedy (exploitation-only) objective can nonetheless\nexhibit emergent exploratory behavior, provided three conditions are met: (1)\nRecurring Environmental Structure, where the environment features repeatable\nregularities that allow past experience to inform future choices; (2) Agent\nMemory, enabling the agent to retain and utilize historical interaction data;\nand (3) Long-Horizon Credit Assignment, where learning propagates returns over\na time frame sufficient for the delayed benefits of exploration to inform\ncurrent decisions. Through experiments in stochastic multi-armed bandits and\ntemporally extended gridworlds, we observe that, when both structure and memory\nare present, a policy trained on a strictly greedy objective exhibits\ninformation-seeking exploratory behavior. We further demonstrate, through\ncontrolled ablations, that emergent exploration vanishes if either\nenvironmental structure or agent memory is absent (Conditions 1 & 2).\nSurprisingly, removing long-horizon credit assignment (Condition 3) does not\nalways prevent emergent exploration-a result we attribute to the\npseudo-Thompson Sampling effect. These findings suggest that, under the right\nprerequisites, exploration and exploitation need not be treated as orthogonal\nobjectives but can emerge from a unified reward-maximization process.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01287.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66fffe1b3ec4cc293d40f2d5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66fffe1b3ec4cc293d40f2d5/-6Ff7rSQ_fvqBmTQRTlB4.png",
            "fullname": "Micah Rentschler",
            "name": "micahr234",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.00910",
            "authors": [
                {
                    "_id": "68917e31f01a094725f834f9",
                    "name": "Terry Yue Zhuo",
                    "hidden": false
                },
                {
                    "_id": "68917e31f01a094725f834fa",
                    "name": "Dingmin Wang",
                    "hidden": false
                },
                {
                    "_id": "68917e31f01a094725f834fb",
                    "name": "Hantian Ding",
                    "hidden": false
                },
                {
                    "_id": "68917e31f01a094725f834fc",
                    "name": "Varun Kumar",
                    "hidden": false
                },
                {
                    "_id": "68917e31f01a094725f834fd",
                    "name": "Zijian Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-29T18:10:37.000Z",
            "submittedOnDailyAt": "2025-08-05T02:18:02.073Z",
            "title": "Cyber-Zero: Training Cybersecurity Agents without Runtime",
            "submittedOnDailyBy": {
                "_id": "62b7fb545233925f253531c8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg",
                "isPro": false,
                "fullname": "Terry Yue Zhuo",
                "user": "terryyz",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have achieved remarkable success in software\nengineering tasks when trained with executable runtime environments,\nparticularly in resolving GitHub issues. However, such runtime environments are\noften unavailable in other domains, especially cybersecurity, where challenge\nconfigurations and execution contexts are ephemeral or restricted. We present\nCyber-Zero, the first runtime-free framework for synthesizing high-quality\nagent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly\navailable CTF writeups and employs persona-driven LLM simulation to\nreverse-engineer runtime behaviors and generate realistic, long-horizon\ninteraction sequences without actual environments. Using trajectories\nsynthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%\nabsolute performance gains over baseline models on three prominent CTF\nbenchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,\nCyber-Zero-32B, establishes new state-of-the-art performance among open-weight\nmodels, matching the capabilities of proprietary systems like DeepSeek-V3-0324\nand Claude-3.5-Sonnet while offering superior cost-effectiveness, and\ndemonstrating that runtime-free trajectory synthesis can effectively\ndemocratize the development of state-of-the-art cybersecurity agents.",
            "upvotes": 3,
            "discussionId": "68917e31f01a094725f834fe",
            "projectPage": "https://github.com/amazon-science/Cyber-Zero",
            "githubRepo": "https://github.com/amazon-science/Cyber-Zero",
            "ai_summary": "Cyber-Zero synthesizes agent trajectories from CTF writeups to train runtime-free cybersecurity LLMs, achieving state-of-the-art performance on benchmarks.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "Cyber-Zero",
                "CTF writeups",
                "persona-driven LLM simulation",
                "agent trajectories",
                "InterCode-CTF",
                "NYU CTF Bench",
                "Cybench",
                "Cyber-Zero-32B",
                "DeepSeek-V3-0324",
                "Claude-3.5-Sonnet"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-07-29T14:10:37.000Z",
        "title": "Cyber-Zero: Training Cybersecurity Agents without Runtime",
        "summary": "Large Language Models (LLMs) have achieved remarkable success in software\nengineering tasks when trained with executable runtime environments,\nparticularly in resolving GitHub issues. However, such runtime environments are\noften unavailable in other domains, especially cybersecurity, where challenge\nconfigurations and execution contexts are ephemeral or restricted. We present\nCyber-Zero, the first runtime-free framework for synthesizing high-quality\nagent trajectories to train cybersecurity LLMs. Cyber-Zero leverages publicly\navailable CTF writeups and employs persona-driven LLM simulation to\nreverse-engineer runtime behaviors and generate realistic, long-horizon\ninteraction sequences without actual environments. Using trajectories\nsynthesized by Cyber-Zero, we train LLM-based agents that achieve up to 13.1%\nabsolute performance gains over baseline models on three prominent CTF\nbenchmarks: InterCode-CTF, NYU CTF Bench, and Cybench. Our best model,\nCyber-Zero-32B, establishes new state-of-the-art performance among open-weight\nmodels, matching the capabilities of proprietary systems like DeepSeek-V3-0324\nand Claude-3.5-Sonnet while offering superior cost-effectiveness, and\ndemonstrating that runtime-free trajectory synthesis can effectively\ndemocratize the development of state-of-the-art cybersecurity agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00910.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62b7fb545233925f253531c8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62b7fb545233925f253531c8/W50u2G1HK3EtUKHRU189V.jpeg",
            "fullname": "Terry Yue Zhuo",
            "name": "terryyz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.00024",
            "authors": [
                {
                    "_id": "6891ef0e9373cd501b6a2d64",
                    "name": "Sebastin Andrs Cajas Ordez",
                    "hidden": false
                },
                {
                    "_id": "6891ef0e9373cd501b6a2d65",
                    "name": "Luis Fernando Torres Torres",
                    "hidden": false
                },
                {
                    "_id": "6891ef0e9373cd501b6a2d66",
                    "name": "Mario Bifulco",
                    "hidden": false
                },
                {
                    "_id": "6891ef0e9373cd501b6a2d67",
                    "name": "Carlos Andrs Durn",
                    "hidden": false
                },
                {
                    "_id": "6891ef0e9373cd501b6a2d68",
                    "name": "Cristian Bosch",
                    "hidden": false
                },
                {
                    "_id": "6891ef0e9373cd501b6a2d69",
                    "name": "Ricardo Simn Carbajo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-28T21:23:51.000Z",
            "submittedOnDailyAt": "2025-08-05T10:22:36.512Z",
            "title": "Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine\n  Learning",
            "submittedOnDailyBy": {
                "_id": "628ddf04986ae70e823298f7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
                "isPro": false,
                "fullname": "Sebastin Andres Cajas Ordez",
                "user": "sebasmos",
                "type": "user"
            },
            "summary": "Quantum Support Vector Machines face scalability challenges due to\nhigh-dimensional quantum states and hardware limitations. We propose an\nembedding-aware quantum-classical pipeline combining class-balanced k-means\ndistillation with pretrained Vision Transformer embeddings. Our key finding:\nViT embeddings uniquely enable quantum advantage, achieving up to 8.02%\naccuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,\nwhile CNN features show performance degradation. Using 16-qubit tensor network\nsimulation via cuTensorNet, we provide the first systematic evidence that\nquantum kernel advantage depends critically on embedding choice, revealing\nfundamental synergy between transformer attention and quantum feature spaces.\nThis provides a practical pathway for scalable quantum machine learning that\nleverages modern neural architectures.",
            "upvotes": 2,
            "discussionId": "6891ef0e9373cd501b6a2d6a",
            "githubRepo": "https://github.com/sebasmos/QuantumVE",
            "ai_summary": "Combining Vision Transformer embeddings with quantum-classical pipelines achieves quantum advantage in classification tasks, demonstrating the importance of embedding choice in quantum machine learning.",
            "ai_keywords": [
                "quantum support vector machines",
                "high-dimensional quantum states",
                "class-balanced k-means distillation",
                "pretrained Vision Transformer embeddings",
                "quantum advantage",
                "accuracy improvements",
                "Fashion-MNIST",
                "MNIST",
                "CNN features",
                "16-qubit tensor network simulation",
                "cuTensorNet",
                "quantum kernel advantage",
                "transformer attention",
                "quantum feature spaces"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-07-28T17:23:51.000Z",
        "title": "Embedding-Aware Quantum-Classical SVMs for Scalable Quantum Machine\n  Learning",
        "summary": "Quantum Support Vector Machines face scalability challenges due to\nhigh-dimensional quantum states and hardware limitations. We propose an\nembedding-aware quantum-classical pipeline combining class-balanced k-means\ndistillation with pretrained Vision Transformer embeddings. Our key finding:\nViT embeddings uniquely enable quantum advantage, achieving up to 8.02%\naccuracy improvements over classical SVMs on Fashion-MNIST and 4.42% on MNIST,\nwhile CNN features show performance degradation. Using 16-qubit tensor network\nsimulation via cuTensorNet, we provide the first systematic evidence that\nquantum kernel advantage depends critically on embedding choice, revealing\nfundamental synergy between transformer attention and quantum feature spaces.\nThis provides a practical pathway for scalable quantum machine learning that\nleverages modern neural architectures.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00024.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628ddf04986ae70e823298f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
            "fullname": "Sebastin Andres Cajas Ordez",
            "name": "sebasmos",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.00890",
            "authors": [
                {
                    "_id": "68919443f01a094725f83574",
                    "name": "Fali Wang",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f83575",
                    "name": "Hui Liu",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f83576",
                    "name": "Zhenwei Dai",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f83577",
                    "name": "Jingying Zeng",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f83578",
                    "name": "Zhiwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f83579",
                    "name": "Zongyu Wu",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f8357a",
                    "name": "Chen Luo",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f8357b",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f8357c",
                    "name": "Xianfeng Tang",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f8357d",
                    "name": "Qi He",
                    "hidden": false
                },
                {
                    "_id": "68919443f01a094725f8357e",
                    "name": "Suhang Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-26T19:21:18.000Z",
            "submittedOnDailyAt": "2025-08-05T03:49:48.335Z",
            "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal\n  Scaling Strategy in Complex Tasks",
            "submittedOnDailyBy": {
                "_id": "644a8ca97c5c68c7762906a0",
                "avatarUrl": "/avatars/c2f6507fa7dcf00fe0151462533f1c2c.svg",
                "isPro": false,
                "fullname": "Fali Wang",
                "user": "FairyFali",
                "type": "user"
            },
            "summary": "Test-time scaling (TTS) enhances the performance of large language models\n(LLMs) by allocating additional compute resources during inference. However,\nexisting research primarily investigates TTS in single-stage tasks; while many\nreal-world problems are multi-stage complex tasks, composed of a sequence of\nheterogeneous subtasks with each subtask requires LLM of specific capability.\nTherefore, we study a novel problem: the test-time compute-optimal scaling in\nmulti-stage complex tasks, aiming to select suitable models and allocate\nbudgets per subtask to maximize overall performance. TTS in multi-stage tasks\nintroduces two fundamental challenges: (i) The combinatorial search space of\nmodel and budget allocations, combined with the high cost of inference, makes\nbrute-force search impractical. (ii) The optimal model and budget allocations\nacross subtasks are interdependent, increasing the complexity of the\ncompute-optimal search. To address this gap, we conduct extensive pilot\nexperiments on four tasks across six datasets, deriving three empirical\ninsights characterizing the behavior of LLMs in multi-stage complex tasks.\nInformed by these insights, we propose AgentTTS, an LLM-agent-based framework\nthat autonomously searches for compute-optimal allocations through iterative\nfeedback-driven interactions with the execution environment. Experimental\nresults demonstrate that AgentTTS significantly outperforms traditional and\nother LLM-based baselines in search efficiency, and shows improved robustness\nto varying training set sizes and enhanced interpretability.",
            "upvotes": 2,
            "discussionId": "68919443f01a094725f8357f",
            "ai_summary": "AgentTTS, an LLM-agent-based framework, optimizes compute allocation for multi-stage complex tasks, improving performance and robustness compared to traditional methods.",
            "ai_keywords": [
                "test-time scaling",
                "large language models",
                "multi-stage complex tasks",
                "combinatorial search space",
                "inference cost",
                "compute-optimal scaling",
                "AgentTTS",
                "iterative feedback-driven interactions"
            ]
        },
        "publishedAt": "2025-07-26T15:21:18.000Z",
        "title": "AgentTTS: Large Language Model Agent for Test-time Compute-optimal\n  Scaling Strategy in Complex Tasks",
        "summary": "Test-time scaling (TTS) enhances the performance of large language models\n(LLMs) by allocating additional compute resources during inference. However,\nexisting research primarily investigates TTS in single-stage tasks; while many\nreal-world problems are multi-stage complex tasks, composed of a sequence of\nheterogeneous subtasks with each subtask requires LLM of specific capability.\nTherefore, we study a novel problem: the test-time compute-optimal scaling in\nmulti-stage complex tasks, aiming to select suitable models and allocate\nbudgets per subtask to maximize overall performance. TTS in multi-stage tasks\nintroduces two fundamental challenges: (i) The combinatorial search space of\nmodel and budget allocations, combined with the high cost of inference, makes\nbrute-force search impractical. (ii) The optimal model and budget allocations\nacross subtasks are interdependent, increasing the complexity of the\ncompute-optimal search. To address this gap, we conduct extensive pilot\nexperiments on four tasks across six datasets, deriving three empirical\ninsights characterizing the behavior of LLMs in multi-stage complex tasks.\nInformed by these insights, we propose AgentTTS, an LLM-agent-based framework\nthat autonomously searches for compute-optimal allocations through iterative\nfeedback-driven interactions with the execution environment. Experimental\nresults demonstrate that AgentTTS significantly outperforms traditional and\nother LLM-based baselines in search efficiency, and shows improved robustness\nto varying training set sizes and enhanced interpretability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00890.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "644a8ca97c5c68c7762906a0",
            "avatarUrl": "/avatars/c2f6507fa7dcf00fe0151462533f1c2c.svg",
            "fullname": "Fali Wang",
            "name": "FairyFali",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02605",
            "authors": [
                {
                    "_id": "6891de219373cd501b6a2d32",
                    "name": "Zhengdao Li",
                    "hidden": false
                },
                {
                    "_id": "6891de219373cd501b6a2d33",
                    "name": "Siheng Wang",
                    "hidden": false
                },
                {
                    "_id": "6891de219373cd501b6a2d34",
                    "name": "Zeyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6891de219373cd501b6a2d35",
                    "name": "Hao Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T16:56:35.000Z",
            "submittedOnDailyAt": "2025-08-05T10:12:10.677Z",
            "title": "ReMoMask: Retrieval-Augmented Masked Motion Generation",
            "submittedOnDailyBy": {
                "_id": "64ec877bb93654d4ca5c92e9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                "isPro": false,
                "fullname": "Zeyu Zhang",
                "user": "SteveZeyuZhang",
                "type": "user"
            },
            "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic and semantically\naligned human motion sequences from natural language descriptions. However,\ncurrent approaches face dual challenges: Generative models (e.g., diffusion\nmodels) suffer from limited diversity, error accumulation, and physical\nimplausibility, while Retrieval-Augmented Generation (RAG) methods exhibit\ndiffusion inertia, partial-mode collapse, and asynchronous artifacts. To\naddress these limitations, we propose ReMoMask, a unified framework integrating\nthree key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples\nnegative sample scale from batch size via momentum queues, substantially\nimproving cross-modal retrieval precision; 2) A Semantic Spatio-temporal\nAttention mechanism enforces biomechanical constraints during part-level fusion\nto eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates\nminor unconditional generation to enhance generalization. Built upon MoMask's\nRVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal\nsteps. Extensive experiments on standard benchmarks demonstrate the\nstate-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%\nimprovement in FID scores on HumanML3D and KIT-ML, respectively, compared to\nthe previous SOTA method RAG-T2M. Code:\nhttps://github.com/AIGeeksGroup/ReMoMask. Website:\nhttps://aigeeksgroup.github.io/ReMoMask.",
            "upvotes": 1,
            "discussionId": "6891de219373cd501b6a2d36",
            "projectPage": "https://aigeeksgroup.github.io/ReMoMask/",
            "githubRepo": "https://github.com/AIGeeksGroup/ReMoMask",
            "ai_summary": "ReMoMask, a unified framework, addresses limitations in text-to-motion generation by integrating a Bidirectional Momentum Text-Motion Model, Semantic Spatio-temporal Attention, and RAG-Classier-Free Guidance, achieving state-of-the-art performance on HumanML3D and KIT-ML benchmarks.",
            "ai_keywords": [
                "diffusion models",
                "Retrieval-Augmented Generation (RAG)",
                "Bidirectional Momentum Text-Motion Model",
                "momentum queues",
                "Semantic Spatio-temporal Attention",
                "biomechanical constraints",
                "RAG-Classier-Free Guidance",
                "RVQ-VAE",
                "temporally coherent motions",
                "FID scores"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-08-04T12:56:35.000Z",
        "title": "ReMoMask: Retrieval-Augmented Masked Motion Generation",
        "summary": "Text-to-Motion (T2M) generation aims to synthesize realistic and semantically\naligned human motion sequences from natural language descriptions. However,\ncurrent approaches face dual challenges: Generative models (e.g., diffusion\nmodels) suffer from limited diversity, error accumulation, and physical\nimplausibility, while Retrieval-Augmented Generation (RAG) methods exhibit\ndiffusion inertia, partial-mode collapse, and asynchronous artifacts. To\naddress these limitations, we propose ReMoMask, a unified framework integrating\nthree key innovations: 1) A Bidirectional Momentum Text-Motion Model decouples\nnegative sample scale from batch size via momentum queues, substantially\nimproving cross-modal retrieval precision; 2) A Semantic Spatio-temporal\nAttention mechanism enforces biomechanical constraints during part-level fusion\nto eliminate asynchronous artifacts; 3) RAG-Classier-Free Guidance incorporates\nminor unconditional generation to enhance generalization. Built upon MoMask's\nRVQ-VAE, ReMoMask efficiently generates temporally coherent motions in minimal\nsteps. Extensive experiments on standard benchmarks demonstrate the\nstate-of-the-art performance of ReMoMask, achieving a 3.88% and 10.97%\nimprovement in FID scores on HumanML3D and KIT-ML, respectively, compared to\nthe previous SOTA method RAG-T2M. Code:\nhttps://github.com/AIGeeksGroup/ReMoMask. Website:\nhttps://aigeeksgroup.github.io/ReMoMask.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02605.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ec877bb93654d4ca5c92e9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
            "fullname": "Zeyu Zhang",
            "name": "SteveZeyuZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02268",
            "authors": [
                {
                    "_id": "68922b1a9dcf0e7ac9a66ee9",
                    "name": "Serry Sibaee",
                    "hidden": false
                },
                {
                    "_id": "68922b1a9dcf0e7ac9a66eea",
                    "name": "Omer Nacar",
                    "hidden": false
                },
                {
                    "_id": "68922b1a9dcf0e7ac9a66eeb",
                    "name": "Yasser Al-Habashi",
                    "hidden": false
                },
                {
                    "_id": "68922b1a9dcf0e7ac9a66eec",
                    "name": "Adel Ammar",
                    "hidden": false
                },
                {
                    "_id": "68922b1a9dcf0e7ac9a66eed",
                    "name": "Wadii Boulila",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T10:21:11.000Z",
            "submittedOnDailyAt": "2025-08-05T14:33:33.231Z",
            "title": "SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic\n  Bidirectional Machine Translation System",
            "submittedOnDailyBy": {
                "_id": "628f7a71dd993507cfcbe587",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
                "isPro": true,
                "fullname": "Omartificial Intelligence Space",
                "user": "Omartificial-Intelligence-Space",
                "type": "user"
            },
            "summary": "The rich linguistic landscape of the Arab world is characterized by a\nsignificant gap between Modern Standard Arabic (MSA), the language of formal\ncommunication, and the diverse regional dialects used in everyday life. This\ndiglossia presents a formidable challenge for natural language processing,\nparticularly machine translation. This paper introduces SHAMI-MT, a\nbidirectional machine translation system specifically engineered to bridge the\ncommunication gap between MSA and the Syrian dialect. We present two\nspecialized models, one for MSA-to-Shami and another for Shami-to-MSA\ntranslation, both built upon the state-of-the-art AraT5v2-base-1024\narchitecture. The models were fine-tuned on the comprehensive Nabra dataset and\nrigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami\nmodel achieved an outstanding average quality score of 4.01 out of 5.0\nwhen judged by OPENAI model GPT-4.1, demonstrating its ability to produce\ntranslations that are not only accurate but also dialectally authentic. This\nwork provides a crucial, high-fidelity tool for a previously underserved\nlanguage pair, advancing the field of dialectal Arabic translation and offering\nsignificant applications in content localization, cultural heritage, and\nintercultural communication.",
            "upvotes": 1,
            "discussionId": "68922b1a9dcf0e7ac9a66eee",
            "ai_summary": "A bidirectional machine translation system, SHAMI-MT, bridges the gap between Modern Standard Arabic and the Syrian dialect using AraT5v2-base-1024 architecture, achieving high-quality translations.",
            "ai_keywords": [
                "bidirectional machine translation",
                "AraT5v2-base-1024",
                "Nabra dataset",
                "MADAR corpus",
                "GPT-4.1",
                "dialectal Arabic translation"
            ]
        },
        "publishedAt": "2025-08-04T06:21:11.000Z",
        "title": "SHAMI-MT: A Syrian Arabic Dialect to Modern Standard Arabic\n  Bidirectional Machine Translation System",
        "summary": "The rich linguistic landscape of the Arab world is characterized by a\nsignificant gap between Modern Standard Arabic (MSA), the language of formal\ncommunication, and the diverse regional dialects used in everyday life. This\ndiglossia presents a formidable challenge for natural language processing,\nparticularly machine translation. This paper introduces SHAMI-MT, a\nbidirectional machine translation system specifically engineered to bridge the\ncommunication gap between MSA and the Syrian dialect. We present two\nspecialized models, one for MSA-to-Shami and another for Shami-to-MSA\ntranslation, both built upon the state-of-the-art AraT5v2-base-1024\narchitecture. The models were fine-tuned on the comprehensive Nabra dataset and\nrigorously evaluated on unseen data from the MADAR corpus. Our MSA-to-Shami\nmodel achieved an outstanding average quality score of 4.01 out of 5.0\nwhen judged by OPENAI model GPT-4.1, demonstrating its ability to produce\ntranslations that are not only accurate but also dialectally authentic. This\nwork provides a crucial, high-fidelity tool for a previously underserved\nlanguage pair, advancing the field of dialectal Arabic translation and offering\nsignificant applications in content localization, cultural heritage, and\nintercultural communication.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02268.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628f7a71dd993507cfcbe587",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628f7a71dd993507cfcbe587/3frAcMgfBCx-OJsrdAhnb.png",
            "fullname": "Omartificial Intelligence Space",
            "name": "Omartificial-Intelligence-Space",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 121
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01109",
            "authors": [
                {
                    "_id": "68921fffc07ad472081610bb",
                    "name": "Satiyabooshan Murugaboopathy",
                    "hidden": false
                },
                {
                    "_id": "68921fffc07ad472081610bc",
                    "name": "Connor T. Jerzak",
                    "hidden": false
                },
                {
                    "_id": "68921fffc07ad472081610bd",
                    "name": "Adel Daoud",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T23:07:16.000Z",
            "submittedOnDailyAt": "2025-08-05T13:47:15.917Z",
            "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language\n  Codes or Agent-Induced Novelty?",
            "submittedOnDailyBy": {
                "_id": "642c374b8f90c557f742db66",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642c374b8f90c557f742db66/7Hw_z7_nY9gr5gdzYyygs.jpeg",
                "isPro": false,
                "fullname": "Connor T. Jerzak",
                "user": "cjerzak",
                "type": "user"
            },
            "summary": "We investigate whether socio-economic indicators like household wealth leave\nrecoverable imprints in satellite imagery (capturing physical features) and\nInternet-sourced text (reflecting historical/economic narratives). Using\nDemographic and Health Survey (DHS) data from African neighborhoods, we pair\nLandsat images with LLM-generated textual descriptions conditioned on\nlocation/year and text retrieved by an AI search agent from web sources. We\ndevelop a multimodal framework predicting household wealth (International\nWealth Index) through five pipelines: (i) vision model on satellite images,\n(ii) LLM using only location/year, (iii) AI agent searching/synthesizing web\ntext, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework\nyields three contributions. First, fusing vision and agent/LLM text outperforms\nvision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on\nout-of-sample splits), with LLM-internal knowledge proving more effective than\nagent-retrieved text, improving robustness to out-of-country and out-of-time\ngeneralization. Second, we find partial representational convergence: fused\nembeddings from vision/language modalities correlate moderately (median cosine\nsimilarity of 0.60 after alignment), suggesting a shared latent code of\nmaterial well-being while retaining complementary details, consistent with the\nPlatonic Representation Hypothesis. Although LLM-only text outperforms\nagent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest\ngains from combining agent data in some splits weakly support the notion that\nagent-gathered information introduces unique representational structures not\nfully captured by static LLM knowledge. Third, we release a large-scale\nmultimodal dataset comprising more than 60,000 DHS clusters linked to satellite\nimages, LLM-generated descriptions, and agent-retrieved texts.",
            "upvotes": 1,
            "discussionId": "68921fffc07ad472081610be",
            "projectPage": "https://aidevlab.org/platonic/",
            "ai_summary": "A multimodal framework using satellite imagery and text data outperforms vision-only models in predicting household wealth, with LLM-generated text proving more effective than agent-retrieved text.",
            "ai_keywords": [
                "vision model",
                "LLM",
                "AI agent",
                "joint image-text encoder",
                "ensemble",
                "International Wealth Index",
                "R-squared",
                "cosine similarity",
                "Platonic Representation Hypothesis",
                "Agent-Induced Novelty Hypothesis"
            ]
        },
        "publishedAt": "2025-08-01T19:07:16.000Z",
        "title": "Platonic Representations for Poverty Mapping: Unified Vision-Language\n  Codes or Agent-Induced Novelty?",
        "summary": "We investigate whether socio-economic indicators like household wealth leave\nrecoverable imprints in satellite imagery (capturing physical features) and\nInternet-sourced text (reflecting historical/economic narratives). Using\nDemographic and Health Survey (DHS) data from African neighborhoods, we pair\nLandsat images with LLM-generated textual descriptions conditioned on\nlocation/year and text retrieved by an AI search agent from web sources. We\ndevelop a multimodal framework predicting household wealth (International\nWealth Index) through five pipelines: (i) vision model on satellite images,\n(ii) LLM using only location/year, (iii) AI agent searching/synthesizing web\ntext, (iv) joint image-text encoder, (v) ensemble of all signals. Our framework\nyields three contributions. First, fusing vision and agent/LLM text outperforms\nvision-only baselines in wealth prediction (e.g., R-squared of 0.77 vs. 0.63 on\nout-of-sample splits), with LLM-internal knowledge proving more effective than\nagent-retrieved text, improving robustness to out-of-country and out-of-time\ngeneralization. Second, we find partial representational convergence: fused\nembeddings from vision/language modalities correlate moderately (median cosine\nsimilarity of 0.60 after alignment), suggesting a shared latent code of\nmaterial well-being while retaining complementary details, consistent with the\nPlatonic Representation Hypothesis. Although LLM-only text outperforms\nagent-retrieved data, challenging our Agent-Induced Novelty Hypothesis, modest\ngains from combining agent data in some splits weakly support the notion that\nagent-gathered information introduces unique representational structures not\nfully captured by static LLM knowledge. Third, we release a large-scale\nmultimodal dataset comprising more than 60,000 DHS clusters linked to satellite\nimages, LLM-generated descriptions, and agent-retrieved texts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01109.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642c374b8f90c557f742db66",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642c374b8f90c557f742db66/7Hw_z7_nY9gr5gdzYyygs.jpeg",
            "fullname": "Connor T. Jerzak",
            "name": "cjerzak",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 67
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01773",
            "authors": [
                {
                    "_id": "6891fbb39373cd501b6a2d94",
                    "name": "Jiuzhou Han",
                    "hidden": false
                },
                {
                    "_id": "6891fbb39373cd501b6a2d95",
                    "name": "Wray Buntine",
                    "hidden": false
                },
                {
                    "_id": "6891fbb39373cd501b6a2d96",
                    "name": "Ehsan Shareghi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-03T14:14:13.000Z",
            "submittedOnDailyAt": "2025-08-05T11:11:39.295Z",
            "title": "Uncertainty-Based Methods for Automated Process Reward Data Construction\n  and Output Aggregation in Mathematical Reasoning",
            "submittedOnDailyBy": {
                "_id": "63b0e5a7f2eb87a4d695398a",
                "avatarUrl": "/avatars/3a03fbb3edbb4aad848cd63c0bce6853.svg",
                "isPro": false,
                "fullname": "Jiuzhou Han",
                "user": "Jiuzhouh",
                "type": "user"
            },
            "summary": "Large language models have demonstrated remarkable capabilities in complex\nmathematical reasoning tasks, but they inevitably generate errors throughout\nmulti-step solutions. Process-level Reward Models (PRMs) have shown great\npromise by providing supervision and evaluation at each intermediate step,\nthereby effectively improving the models' reasoning abilities. However,\ntraining effective PRMs requires high-quality process reward data, yet existing\nmethods for constructing such data are often labour-intensive or inefficient.\nIn this paper, we propose an uncertainty-driven framework for automated process\nreward data construction, encompassing both data generation and annotation\nprocesses for PRMs. Additionally, we identify the limitations of both majority\nvote and PRMs, and introduce two generic uncertainty-aware output aggregation\nmethods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which\ncombine the strengths of majority vote with PRMs. Extensive experiments on\nProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the\nproposed PRM data construction framework, and demonstrate that the two output\naggregation methods further improve the mathematical reasoning abilities across\ndiverse PRMs. The code and data will be publicly available at\nhttps://github.com/Jiuzhouh/UnPRM.",
            "upvotes": 0,
            "discussionId": "6891fbb49373cd501b6a2d97",
            "ai_summary": "An uncertainty-driven framework for automated process reward data construction and aggregation methods improves the effectiveness and efficiency of Process-Level Reward Models in mathematical reasoning tasks.",
            "ai_keywords": [
                "Process-Level Reward Models",
                "PRMs",
                "uncertainty-driven framework",
                "data generation",
                "annotation processes",
                "Hybrid Majority Reward Vote",
                "Weighted Reward Frequency Vote",
                "ProcessBench",
                "MATH",
                "GSMPlus"
            ]
        },
        "publishedAt": "2025-08-03T10:14:13.000Z",
        "title": "Uncertainty-Based Methods for Automated Process Reward Data Construction\n  and Output Aggregation in Mathematical Reasoning",
        "summary": "Large language models have demonstrated remarkable capabilities in complex\nmathematical reasoning tasks, but they inevitably generate errors throughout\nmulti-step solutions. Process-level Reward Models (PRMs) have shown great\npromise by providing supervision and evaluation at each intermediate step,\nthereby effectively improving the models' reasoning abilities. However,\ntraining effective PRMs requires high-quality process reward data, yet existing\nmethods for constructing such data are often labour-intensive or inefficient.\nIn this paper, we propose an uncertainty-driven framework for automated process\nreward data construction, encompassing both data generation and annotation\nprocesses for PRMs. Additionally, we identify the limitations of both majority\nvote and PRMs, and introduce two generic uncertainty-aware output aggregation\nmethods: Hybrid Majority Reward Vote and Weighted Reward Frequency Vote, which\ncombine the strengths of majority vote with PRMs. Extensive experiments on\nProcessBench, MATH, and GSMPlus show the effectiveness and efficiency of the\nproposed PRM data construction framework, and demonstrate that the two output\naggregation methods further improve the mathematical reasoning abilities across\ndiverse PRMs. The code and data will be publicly available at\nhttps://github.com/Jiuzhouh/UnPRM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01773.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b0e5a7f2eb87a4d695398a",
            "avatarUrl": "/avatars/3a03fbb3edbb4aad848cd63c0bce6853.svg",
            "fullname": "Jiuzhou Han",
            "name": "Jiuzhouh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.16290",
            "authors": [
                {
                    "_id": "689225269dcf0e7ac9a66eb7",
                    "name": "Xianze Fang",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66eb8",
                    "name": "Jingnan Gao",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66eb9",
                    "name": "Zhe Wang",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66eba",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66ebb",
                    "name": "Xingyu Ren",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66ebc",
                    "name": "Jiangjing Lyu",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66ebd",
                    "name": "Qiaomu Ren",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66ebe",
                    "name": "Zhonglei Yang",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66ebf",
                    "name": "Xiaokang Yang",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66ec0",
                    "name": "Yichao Yan",
                    "hidden": false
                },
                {
                    "_id": "689225269dcf0e7ac9a66ec1",
                    "name": "Chengfei Lyu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-22T07:22:30.000Z",
            "submittedOnDailyAt": "2025-08-05T14:10:11.737Z",
            "title": "Dens3R: A Foundation Model for 3D Geometry Prediction",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Recent advances in dense 3D reconstruction have led to significant progress,\nyet achieving accurate unified geometric prediction remains a major challenge.\nMost existing methods are limited to predicting a single geometry quantity from\ninput images. However, geometric quantities such as depth, surface normals, and\npoint maps are inherently correlated, and estimating them in isolation often\nfails to ensure consistency, thereby limiting both accuracy and practical\napplicability. This motivates us to explore a unified framework that explicitly\nmodels the structural coupling among different geometric properties to enable\njoint regression. In this paper, we present Dens3R, a 3D foundation model\ndesigned for joint geometric dense prediction and adaptable to a wide range of\ndownstream tasks. Dens3R adopts a two-stage training framework to progressively\nbuild a pointmap representation that is both generalizable and intrinsically\ninvariant. Specifically, we design a lightweight shared encoder-decoder\nbackbone and introduce position-interpolated rotary positional encoding to\nmaintain expressive power while enhancing robustness to high-resolution inputs.\nBy integrating image-pair matching features with intrinsic invariance modeling,\nDens3R accurately regresses multiple geometric quantities such as surface\nnormals and depth, achieving consistent geometry perception from single-view to\nmulti-view inputs. Additionally, we propose a post-processing pipeline that\nsupports geometrically consistent multi-view inference. Extensive experiments\ndemonstrate the superior performance of Dens3R across various dense 3D\nprediction tasks and highlight its potential for broader applications.",
            "upvotes": 0,
            "discussionId": "689225269dcf0e7ac9a66ec2",
            "ai_summary": "Dens3R is a 3D foundation model that jointly predicts multiple geometric quantities using a two-stage training framework, enhancing consistency and performance in dense 3D reconstruction tasks.",
            "ai_keywords": [
                "3D reconstruction",
                "dense 3D prediction",
                "geometric quantities",
                "depth",
                "surface normals",
                "point maps",
                "unified framework",
                "structural coupling",
                "joint regression",
                "3D foundation model",
                "two-stage training",
                "pointmap representation",
                "lightweight shared encoder-decoder",
                "position-interpolated rotary positional encoding",
                "image-pair matching",
                "intrinsic invariance",
                "multi-view inference"
            ]
        },
        "publishedAt": "2025-07-22T03:22:30.000Z",
        "title": "Dens3R: A Foundation Model for 3D Geometry Prediction",
        "summary": "Recent advances in dense 3D reconstruction have led to significant progress,\nyet achieving accurate unified geometric prediction remains a major challenge.\nMost existing methods are limited to predicting a single geometry quantity from\ninput images. However, geometric quantities such as depth, surface normals, and\npoint maps are inherently correlated, and estimating them in isolation often\nfails to ensure consistency, thereby limiting both accuracy and practical\napplicability. This motivates us to explore a unified framework that explicitly\nmodels the structural coupling among different geometric properties to enable\njoint regression. In this paper, we present Dens3R, a 3D foundation model\ndesigned for joint geometric dense prediction and adaptable to a wide range of\ndownstream tasks. Dens3R adopts a two-stage training framework to progressively\nbuild a pointmap representation that is both generalizable and intrinsically\ninvariant. Specifically, we design a lightweight shared encoder-decoder\nbackbone and introduce position-interpolated rotary positional encoding to\nmaintain expressive power while enhancing robustness to high-resolution inputs.\nBy integrating image-pair matching features with intrinsic invariance modeling,\nDens3R accurately regresses multiple geometric quantities such as surface\nnormals and depth, achieving consistent geometry perception from single-view to\nmulti-view inputs. Additionally, we propose a post-processing pipeline that\nsupports geometrically consistent multi-view inference. Extensive experiments\ndemonstrate the superior performance of Dens3R across various dense 3D\nprediction tasks and highlight its potential for broader applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.16290.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 934
        },
        "isAuthorParticipating": false
    }
]