[
    {
        "paper": {
            "id": "2508.02193",
            "authors": [
                {
                    "_id": "6892c77d8da45ffb0a2b2471",
                    "user": {
                        "_id": "66275b53d138af2d2eeb9326",
                        "avatarUrl": "/avatars/1103f68b978238d5bce604294d467e00.svg",
                        "isPro": false,
                        "fullname": "Yuxuan Song",
                        "user": "yxsong",
                        "type": "user"
                    },
                    "name": "Yuxuan Song",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:16:45.791Z",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2472",
                    "name": "Zheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2473",
                    "user": {
                        "_id": "65150e4c91aa56a7b10630b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65150e4c91aa56a7b10630b5/vWcnwhSdfej_V5zih3fxv.jpeg",
                        "isPro": false,
                        "fullname": "Luo Cheng",
                        "user": "Hmdlc",
                        "type": "user"
                    },
                    "name": "Cheng Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:16:41.735Z",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2474",
                    "name": "Pengyang Gao",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2475",
                    "name": "Fan Xia",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2476",
                    "name": "Hao Luo",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2477",
                    "name": "Zheng Li",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2478",
                    "name": "Yuehang Yang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2479",
                    "name": "Hongli Yu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247a",
                    "name": "Xingwei Qu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247b",
                    "name": "Yuwei Fu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247c",
                    "name": "Jing Su",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247d",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247e",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b247f",
                    "name": "Mingxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2480",
                    "name": "Lin Yan",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2481",
                    "name": "Xiaoying Jia",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2482",
                    "name": "Jingjing Liu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2483",
                    "name": "Wei-Ying Ma",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2484",
                    "name": "Ya-Qin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2485",
                    "name": "Yonghui Wu",
                    "hidden": false
                },
                {
                    "_id": "6892c77d8da45ffb0a2b2486",
                    "name": "Hao Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66275b53d138af2d2eeb9326/dFE-umzBWqWmGHWbKItOo.png"
            ],
            "publishedAt": "2025-08-04T08:43:01.000Z",
            "submittedOnDailyAt": "2025-08-06T02:15:50.872Z",
            "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference",
            "submittedOnDailyBy": {
                "_id": "66275b53d138af2d2eeb9326",
                "avatarUrl": "/avatars/1103f68b978238d5bce604294d467e00.svg",
                "isPro": false,
                "fullname": "Yuxuan Song",
                "user": "yxsong",
                "type": "user"
            },
            "summary": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.",
            "upvotes": 68,
            "discussionId": "6892c77d8da45ffb0a2b2487",
            "projectPage": "https://seed.bytedance.com/en/seed_diffusion",
            "ai_summary": "Seed Diffusion Preview, a discrete-state diffusion language model, achieves fast inference speeds through parallel generation, outperforming Mercury and Gemini Diffusion in speed and quality.",
            "ai_keywords": [
                "discrete-state diffusion",
                "non-sequential",
                "parallel generation",
                "token-by-token decoding",
                "Seed Diffusion Preview",
                "H20 GPUs",
                "code evaluation benchmarks",
                "speed-quality Pareto frontier"
            ]
        },
        "publishedAt": "2025-08-04T04:43:01.000Z",
        "title": "Seed Diffusion: A Large-Scale Diffusion Language Model with High-Speed\n  Inference",
        "summary": "We present Seed Diffusion Preview, a large-scale language model based on\ndiscrete-state diffusion, offering remarkably fast inference speed. Thanks to\nnon-sequential, parallel generation, discrete diffusion models provide a\nnotable speedup to mitigate the inherent latency of token-by-token decoding, as\ndemonstrated recently (e.g., Mercury Coder, Gemini Diffusion). Seed Diffusion\nPreview achieves an inference speed of 2,146 token/s over H20 GPUs while\nmaintaining competitive performance across a sweep of standard code evaluation\nbenchmarks, significantly faster than contemporary Mercury and Gemini\nDiffusion, establishing new state of the art on the speed-quality Pareto\nfrontier for code models.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66275b53d138af2d2eeb9326/dFE-umzBWqWmGHWbKItOo.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02193.png",
        "numComments": 9,
        "submittedBy": {
            "_id": "66275b53d138af2d2eeb9326",
            "avatarUrl": "/avatars/1103f68b978238d5bce604294d467e00.svg",
            "fullname": "Yuxuan Song",
            "name": "yxsong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03320",
            "authors": [
                {
                    "_id": "6892bd5c8da45ffb0a2b23fe",
                    "user": {
                        "_id": "620f5a1c3f76c50e6458a9b6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
                        "isPro": false,
                        "fullname": "Peiyu Wang",
                        "user": "OrlandoHugBot",
                        "type": "user"
                    },
                    "name": "Peiyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:17:26.595Z",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b23ff",
                    "name": "Yi Peng",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2400",
                    "name": "Yimeng Gan",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2401",
                    "name": "Liang Hu",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2402",
                    "name": "Tianyidan Xie",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2403",
                    "name": "Xiaokun Wang",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2404",
                    "name": "Yichen Wei",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2405",
                    "name": "Chuanxin Tang",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2406",
                    "name": "Bo Zhu",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2407",
                    "name": "Changshi Li",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2408",
                    "name": "Hongyang Wei",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b2409",
                    "name": "Eric Li",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b240a",
                    "name": "Xuchen Song",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b240b",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "6892bd5c8da45ffb0a2b240c",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T10:59:01.000Z",
            "submittedOnDailyAt": "2025-08-06T01:57:00.887Z",
            "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation",
            "submittedOnDailyBy": {
                "_id": "620f5a1c3f76c50e6458a9b6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
                "isPro": false,
                "fullname": "Peiyu Wang",
                "user": "OrlandoHugBot",
                "type": "user"
            },
            "summary": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model\nthat unifies image understanding, text-to-image generation, and image editing\nwithin a single architecture-eliminating the need for task-specific adapters or\ninter-module connectors-and demonstrate that compact multimodal systems can\nachieve state-of-the-art performance on commodity hardware. Skywork UniPic\nachieves a GenEval score of 0.86, surpassing most existing unified models; sets\na new DPG-Bench complex-generation record of 85.5; attains 5.83 on\nGEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x\n1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled\nencoding strategy that leverages a masked autoregressive encoder for synthesis\nand a SigLIP2 encoder for understanding, all feeding a shared autoregressive\ndecoder; (2) a progressive, resolution-aware training schedule scaling from 256\nx 256 to 1024 x 1024 while dynamically unfreezing parameters to balance\ncapacity and stability; and (3) meticulously curated, 100 million-scale\ndatasets augmented with task-specific reward models to refine generation and\nediting objectives. By demonstrating that high-fidelity multimodal integration\nneed not incur prohibitive resource demands, Skywork UniPic establishes a\npractical paradigm for deployable, high-fidelity multimodal AI. Code and\nweights are publicly available at\nhttps://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
            "upvotes": 44,
            "discussionId": "6892bd5c8da45ffb0a2b240d",
            "githubRepo": "https://github.com/SkyworkAI/UniPic",
            "ai_summary": "Skywork UniPic, a 1.5 billion-parameter autoregressive model, unifies image understanding, text-to-image generation, and image editing with state-of-the-art performance on commodity hardware.",
            "ai_keywords": [
                "autoregressive model",
                "image understanding",
                "text-to-image generation",
                "image editing",
                "GenEval",
                "DPG-Bench",
                "GEditBench-EN",
                "ImgEdit-Bench",
                "decoupled encoding strategy",
                "masked autoregressive encoder",
                "SigLIP2 encoder",
                "shared autoregressive decoder",
                "progressive training schedule",
                "resolution-aware training",
                "parameter unfreezing",
                "high-fidelity multimodal integration"
            ],
            "githubStars": 380
        },
        "publishedAt": "2025-08-05T06:59:01.000Z",
        "title": "Skywork UniPic: Unified Autoregressive Modeling for Visual Understanding\n  and Generation",
        "summary": "We introduce Skywork UniPic, a 1.5 billion-parameter autoregressive model\nthat unifies image understanding, text-to-image generation, and image editing\nwithin a single architecture-eliminating the need for task-specific adapters or\ninter-module connectors-and demonstrate that compact multimodal systems can\nachieve state-of-the-art performance on commodity hardware. Skywork UniPic\nachieves a GenEval score of 0.86, surpassing most existing unified models; sets\na new DPG-Bench complex-generation record of 85.5; attains 5.83 on\nGEditBench-EN and 3.49 on ImgEdit-Bench for image editing; and generates 1024 x\n1024 images with under 15 GB of GPU memory (e.g., RTX 4090). (1) a decoupled\nencoding strategy that leverages a masked autoregressive encoder for synthesis\nand a SigLIP2 encoder for understanding, all feeding a shared autoregressive\ndecoder; (2) a progressive, resolution-aware training schedule scaling from 256\nx 256 to 1024 x 1024 while dynamically unfreezing parameters to balance\ncapacity and stability; and (3) meticulously curated, 100 million-scale\ndatasets augmented with task-specific reward models to refine generation and\nediting objectives. By demonstrating that high-fidelity multimodal integration\nneed not incur prohibitive resource demands, Skywork UniPic establishes a\npractical paradigm for deployable, high-fidelity multimodal AI. Code and\nweights are publicly available at\nhttps://huggingface.co/Skywork/Skywork-UniPic-1.5B.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03320.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620f5a1c3f76c50e6458a9b6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/620f5a1c3f76c50e6458a9b6/pXh_f5F0UvufxuUa-eS-v.jpeg",
            "fullname": "Peiyu Wang",
            "name": "OrlandoHugBot",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03694",
            "authors": [
                {
                    "_id": "6892b64d8da45ffb0a2b23d4",
                    "user": {
                        "_id": "643815c4961bb61e463c5896",
                        "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg",
                        "isPro": false,
                        "fullname": "Jianxiong Gao",
                        "user": "Jianxiong",
                        "type": "user"
                    },
                    "name": "Jianxiong Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:17:37.121Z",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d5",
                    "name": "Zhaoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d6",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d7",
                    "name": "Jianfeng Feng",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d8",
                    "name": "Chenyang Si",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23d9",
                    "name": "Yanwei Fu",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23da",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "6892b64d8da45ffb0a2b23db",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T17:59:58.000Z",
            "submittedOnDailyAt": "2025-08-06T00:30:01.624Z",
            "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation",
            "submittedOnDailyBy": {
                "_id": "643815c4961bb61e463c5896",
                "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg",
                "isPro": false,
                "fullname": "Jianxiong Gao",
                "user": "Jianxiong",
                "type": "user"
            },
            "summary": "Controllable ultra-long video generation is a fundamental yet challenging\ntask. Although existing methods are effective for short clips, they struggle to\nscale due to issues such as temporal inconsistency and visual degradation. In\nthis paper, we initially investigate and identify three key factors: separate\nnoise initialization, independent control signal normalization, and the\nlimitations of single-modality guidance. To address these issues, we propose\nLongVie, an end-to-end autoregressive framework for controllable long video\ngeneration. LongVie introduces two core designs to ensure temporal consistency:\n1) a unified noise initialization strategy that maintains consistent generation\nacross clips, and 2) global control signal normalization that enforces\nalignment in the control space throughout the entire video. To mitigate visual\ndegradation, LongVie employs 3) a multi-modal control framework that integrates\nboth dense (e.g., depth maps) and sparse (e.g., keypoints) control signals,\ncomplemented by 4) a degradation-aware training strategy that adaptively\nbalances modality contributions over time to preserve visual quality. We also\nintroduce LongVGenBench, a comprehensive benchmark consisting of 100\nhigh-resolution videos spanning diverse real-world and synthetic environments,\neach lasting over one minute. Extensive experiments show that LongVie achieves\nstate-of-the-art performance in long-range controllability, consistency, and\nquality.",
            "upvotes": 32,
            "discussionId": "6892b64d8da45ffb0a2b23dc",
            "projectPage": "https://vchitect.github.io/LongVie-project/",
            "githubRepo": "https://github.com/Vchitect/LongVie",
            "ai_summary": "LongVie, an end-to-end autoregressive framework, addresses temporal consistency and visual degradation in ultra-long video generation through unified noise initialization, global control signal normalization, multi-modal control, and degradation-aware training.",
            "ai_keywords": [
                "autoregressive framework",
                "temporal consistency",
                "visual degradation",
                "unified noise initialization",
                "global control signal normalization",
                "multi-modal control",
                "degradation-aware training",
                "LongVGenBench"
            ],
            "githubStars": 25
        },
        "publishedAt": "2025-08-05T13:59:58.000Z",
        "title": "LongVie: Multimodal-Guided Controllable Ultra-Long Video Generation",
        "summary": "Controllable ultra-long video generation is a fundamental yet challenging\ntask. Although existing methods are effective for short clips, they struggle to\nscale due to issues such as temporal inconsistency and visual degradation. In\nthis paper, we initially investigate and identify three key factors: separate\nnoise initialization, independent control signal normalization, and the\nlimitations of single-modality guidance. To address these issues, we propose\nLongVie, an end-to-end autoregressive framework for controllable long video\ngeneration. LongVie introduces two core designs to ensure temporal consistency:\n1) a unified noise initialization strategy that maintains consistent generation\nacross clips, and 2) global control signal normalization that enforces\nalignment in the control space throughout the entire video. To mitigate visual\ndegradation, LongVie employs 3) a multi-modal control framework that integrates\nboth dense (e.g., depth maps) and sparse (e.g., keypoints) control signals,\ncomplemented by 4) a degradation-aware training strategy that adaptively\nbalances modality contributions over time to preserve visual quality. We also\nintroduce LongVGenBench, a comprehensive benchmark consisting of 100\nhigh-resolution videos spanning diverse real-world and synthetic environments,\neach lasting over one minute. Extensive experiments show that LongVie achieves\nstate-of-the-art performance in long-range controllability, consistency, and\nquality.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03694.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "643815c4961bb61e463c5896",
            "avatarUrl": "/avatars/3b44592472f16c56105bff8c314d9939.svg",
            "fullname": "Jianxiong Gao",
            "name": "Jianxiong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03686",
            "authors": [
                {
                    "_id": "6892f2468da45ffb0a2b2500",
                    "user": {
                        "_id": "654ce87af0b05673196a9f45",
                        "avatarUrl": "/avatars/7b9c854eb98e487e3057479b1c7860ac.svg",
                        "isPro": false,
                        "fullname": "Shudong Liu",
                        "user": "Sudanl",
                        "type": "user"
                    },
                    "name": "Shudong Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:16:08.000Z",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2501",
                    "name": "Hongwei Liu",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2502",
                    "user": {
                        "_id": "643d26979347842571bc9613",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
                        "isPro": false,
                        "fullname": "Junnan Liu",
                        "user": "jnanliu",
                        "type": "user"
                    },
                    "name": "Junnan Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:16:12.064Z",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2503",
                    "name": "Linchen Xiao",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2504",
                    "name": "Songyang Gao",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2505",
                    "name": "Chengqi Lyu",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2506",
                    "name": "Yuzhe Gu",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2507",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2508",
                    "name": "Derek F. Wong",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b2509",
                    "name": "Songyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892f2468da45ffb0a2b250a",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T17:55:24.000Z",
            "submittedOnDailyAt": "2025-08-06T04:43:01.913Z",
            "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
            "submittedOnDailyBy": {
                "_id": "630716d11801ecc7d2595021",
                "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
                "isPro": false,
                "fullname": "Songyang Zhang",
                "user": "zsytony",
                "type": "user"
            },
            "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.",
            "upvotes": 24,
            "discussionId": "6892f2468da45ffb0a2b250b",
            "githubRepo": "https://github.com/open-compass/CompassVerifier",
            "ai_summary": "CompassVerifier is a lightweight, robust model for verifying LLM outputs across various domains, supported by VerifierBench, a comprehensive benchmark dataset.",
            "ai_keywords": [
                "LLMs",
                "answer verification",
                "reward model",
                "evaluation frameworks",
                "regex rules",
                "evaluation prompts",
                "benchmarks",
                "verifier development",
                "multi-domain competency",
                "math",
                "knowledge",
                "reasoning tasks",
                "multi-subproblems",
                "formulas",
                "sequence answers",
                "abnormal/invalid responses",
                "VerifierBench"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-08-05T13:55:24.000Z",
        "title": "CompassVerifier: A Unified and Robust Verifier for LLMs Evaluation and\n  Outcome Reward",
        "summary": "Answer verification is crucial not only for evaluating large language models\n(LLMs) by matching their unstructured outputs against standard answers, but\nalso serves as the reward model to guide LLM optimization. Most evaluation\nframeworks rely on regularized matching or employ general LLMs for answer\nverification, which demands extensive, repetitive customization for regex rules\nor evaluation prompts. Two fundamental limitations persist in current\nmethodologies: 1) the absence of comprehensive benchmarks that systematically\nevaluate verification capabilities across different LLMs; and 2) the nascent\nstage of verifier development, where existing approaches lack both the\nrobustness to handle complex edge cases and the generalizability across\ndifferent domains. In this work, we develop CompassVerifier, an accurate and\nrobust lightweight verifier model for evaluation and outcome reward. It\ndemonstrates multi-domain competency spanning math, knowledge, and diverse\nreasoning tasks, with the capability to process various answer types, including\nmulti-subproblems, formulas, and sequence answers, while effectively\nidentifying abnormal/invalid responses. We introduce VerifierBench benchmark\ncomprising model outputs collected from multiple data sources, augmented\nthrough manual analysis of metaerror patterns to enhance CompassVerifier. We\nanticipate that CompassVerifier and VerifierBench will facilitate answer\nverification, evaluation protocols, and reinforcement learning research. Code\nand dataset are available at https://github.com/open-compass/CompassVerifier.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03686.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "630716d11801ecc7d2595021",
            "avatarUrl": "/avatars/2d36a880ce4a3cf7efc5ff3987dbeaf3.svg",
            "fullname": "Songyang Zhang",
            "name": "zsytony",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03012",
            "authors": [
                {
                    "_id": "6892c65c8da45ffb0a2b245e",
                    "user": {
                        "_id": "654da66fb36f85a025bc24b6",
                        "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
                        "isPro": false,
                        "fullname": "Zexiong Ma",
                        "user": "mizersy",
                        "type": "user"
                    },
                    "name": "Zexiong Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:17:05.996Z",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b245f",
                    "user": {
                        "_id": "64425a502f4abae43fc0446c",
                        "avatarUrl": "/avatars/7448a8d024813d8a20e09c162a189304.svg",
                        "isPro": false,
                        "fullname": "Chao Peng",
                        "user": "pengchao",
                        "type": "user"
                    },
                    "name": "Chao Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:17:09.600Z",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b2460",
                    "name": "Qunhong Zeng",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b2461",
                    "name": "Pengfei Gao",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b2462",
                    "name": "Yanzhen Zou",
                    "hidden": false
                },
                {
                    "_id": "6892c65c8da45ffb0a2b2463",
                    "name": "Bing Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T02:44:21.000Z",
            "submittedOnDailyAt": "2025-08-06T01:36:11.665Z",
            "title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
            "submittedOnDailyBy": {
                "_id": "654da66fb36f85a025bc24b6",
                "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
                "isPro": false,
                "fullname": "Zexiong Ma",
                "user": "mizersy",
                "type": "user"
            },
            "summary": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.",
            "upvotes": 10,
            "discussionId": "6892c65d8da45ffb0a2b2464",
            "githubRepo": "https://github.com/Mizersy/RepoDeepSearch",
            "ai_summary": "ToolTrain, a two-stage training framework combining supervised fine-tuning and reinforcement learning, enhances LLMs for issue localization by integrating repository retrieval tools, achieving state-of-the-art performance.",
            "ai_keywords": [
                "LLM-based agents",
                "Repo Deep Search",
                "rejection-sampled supervised fine-tuning",
                "tool-integrated reinforcement learning",
                "function-level localization",
                "Claude-3.7",
                "automated software development"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-08-04T22:44:21.000Z",
        "title": "Tool-integrated Reinforcement Learning for Repo Deep Search",
        "summary": "Issue localization, the process of identifying code locations that need\nmodification to resolve software issues, is a critical yet challenging task in\nsoftware development. The semantic gap between natural language issue\ndescriptions and faulty code requires complex multi-hop reasoning through code\ndependencies. Existing LLM-based agents attempt to address this by integrating\nrepository retrieval tools. However, this transforms issue localization into a\ndemanding task we call Repo Deep Search, which requires the LLM to effectively\nutilize various repository retrieval tools throughout a multi-step reasoning\nand navigation process. To tackle this challenge, we present ToolTrain, a\ntwo-stage tool-integrated training framework combining rejection-sampled\nsupervised fine-tuning and tool-integrated reinforcement learning to enhance\nLLMs' ability to use retrieval tools for issue localization. Experimental\nresults show that ToolTrain-trained models achieve state-of-the-art\nperformance, with our 32B model even surpassing Claude-3.7 on function-level\nlocalization. The results also show that improved localization performance\ntranslates to better end-to-end issue resolution performance. This further\ndemonstrates that training for issue localization is a viable and effective\nstrategy for improving automated software development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03012.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654da66fb36f85a025bc24b6",
            "avatarUrl": "/avatars/e5542856ab4bf1845e8f546b5f17cd99.svg",
            "fullname": "Zexiong Ma",
            "name": "mizersy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.00367",
            "authors": [
                {
                    "_id": "68935c555645843c67bf4e5e",
                    "name": "Joonmyung Choi",
                    "hidden": false
                },
                {
                    "_id": "68935c555645843c67bf4e5f",
                    "name": "Sanghyeok Lee",
                    "hidden": false
                },
                {
                    "_id": "68935c555645843c67bf4e60",
                    "name": "Byungoh Ko",
                    "hidden": false
                },
                {
                    "_id": "68935c555645843c67bf4e61",
                    "name": "Eunseo Kim",
                    "hidden": false
                },
                {
                    "_id": "68935c555645843c67bf4e62",
                    "name": "Jihyung Kil",
                    "hidden": false
                },
                {
                    "_id": "68935c555645843c67bf4e63",
                    "name": "Hyunwoo J. Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T06:53:55.000Z",
            "submittedOnDailyAt": "2025-08-06T12:16:24.657Z",
            "title": "Representation Shift: Unifying Token Compression with FlashAttention",
            "submittedOnDailyBy": {
                "_id": "661748796b30e903469fac66",
                "avatarUrl": "/avatars/5cb8b0e9ee6c6cdfd350b42378e65ac7.svg",
                "isPro": false,
                "fullname": "Sanghyeok Lee",
                "user": "SanghyeokLee",
                "type": "user"
            },
            "summary": "Transformers have demonstrated remarkable success across vision, language,\nand video. Yet, increasing task complexity has led to larger models and more\ntokens, raising the quadratic cost of self-attention and the overhead of GPU\nmemory access. To reduce the computation cost of self-attention, prior work has\nproposed token compression techniques that drop redundant or less informative\ntokens. Meanwhile, fused attention kernels such as FlashAttention have been\ndeveloped to alleviate memory overhead by avoiding attention map construction\nand its associated I/O to HBM. This, however, makes it incompatible with most\ntraining-free token compression methods, which rely on attention maps to\ndetermine token importance. Here, we propose Representation Shift, a\ntraining-free, model-agnostic metric that measures the degree of change in each\ntoken's representation. This seamlessly integrates token compression with\nFlashAttention, without attention maps or retraining. Our method further\ngeneralizes beyond Transformers to CNNs and state space models. Extensive\nexperiments show that Representation Shift enables effective token compression\ncompatible with FlashAttention, yielding significant speedups of up to 5.5% and\n4.4% in video-text retrieval and video QA, respectively. Code is available at\nhttps://github.com/mlvlab/Representation-Shift.",
            "upvotes": 8,
            "discussionId": "68935c565645843c67bf4e64",
            "projectPage": "https://github.com/mlvlab/Representation-Shift",
            "githubRepo": "https://github.com/mlvlab/Representation-Shift",
            "ai_summary": "Representation Shift is a training-free, model-agnostic metric that integrates token compression with FlashAttention, enabling significant speedups in video-text retrieval and video QA.",
            "ai_keywords": [
                "Transformers",
                "self-attention",
                "token compression",
                "FlashAttention",
                "token importance",
                "Representation Shift",
                "CNNs",
                "state space models",
                "video-text retrieval",
                "video QA"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-08-01T02:53:55.000Z",
        "title": "Representation Shift: Unifying Token Compression with FlashAttention",
        "summary": "Transformers have demonstrated remarkable success across vision, language,\nand video. Yet, increasing task complexity has led to larger models and more\ntokens, raising the quadratic cost of self-attention and the overhead of GPU\nmemory access. To reduce the computation cost of self-attention, prior work has\nproposed token compression techniques that drop redundant or less informative\ntokens. Meanwhile, fused attention kernels such as FlashAttention have been\ndeveloped to alleviate memory overhead by avoiding attention map construction\nand its associated I/O to HBM. This, however, makes it incompatible with most\ntraining-free token compression methods, which rely on attention maps to\ndetermine token importance. Here, we propose Representation Shift, a\ntraining-free, model-agnostic metric that measures the degree of change in each\ntoken's representation. This seamlessly integrates token compression with\nFlashAttention, without attention maps or retraining. Our method further\ngeneralizes beyond Transformers to CNNs and state space models. Extensive\nexperiments show that Representation Shift enables effective token compression\ncompatible with FlashAttention, yielding significant speedups of up to 5.5% and\n4.4% in video-text retrieval and video QA, respectively. Code is available at\nhttps://github.com/mlvlab/Representation-Shift.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00367.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661748796b30e903469fac66",
            "avatarUrl": "/avatars/5cb8b0e9ee6c6cdfd350b42378e65ac7.svg",
            "fullname": "Sanghyeok Lee",
            "name": "SanghyeokLee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02091",
            "authors": [
                {
                    "_id": "6892ef808da45ffb0a2b24e8",
                    "name": "Xiaoya Li",
                    "hidden": false
                },
                {
                    "_id": "6892ef808da45ffb0a2b24e9",
                    "name": "Xiaofei Sun",
                    "hidden": false
                },
                {
                    "_id": "6892ef808da45ffb0a2b24ea",
                    "name": "Albert Wang",
                    "hidden": false
                },
                {
                    "_id": "6892ef808da45ffb0a2b24eb",
                    "name": "Chris Shum",
                    "hidden": false
                },
                {
                    "_id": "6892ef808da45ffb0a2b24ec",
                    "name": "Jiwei Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6803ddda5c044d396d02f03a/HjerqHcTmj_DCsAFjU5w3.png"
            ],
            "publishedAt": "2025-08-04T05:57:46.000Z",
            "submittedOnDailyAt": "2025-08-06T04:32:14.488Z",
            "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search",
            "submittedOnDailyBy": {
                "_id": "6803ddda5c044d396d02f03a",
                "avatarUrl": "/avatars/ec583f6560d3d82fbd3bf95fb83c9356.svg",
                "isPro": false,
                "fullname": "Xiaoya Li",
                "user": "xxiaoyali",
                "type": "user"
            },
            "summary": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
            "upvotes": 7,
            "discussionId": "6892ef808da45ffb0a2b24ed",
            "ai_summary": "CRINN, a reinforcement learning-based approach, optimizes approximate nearest-neighbor search algorithms for speed while maintaining accuracy, outperforming state-of-the-art methods on several benchmarks.",
            "ai_keywords": [
                "approximate nearest-neighbor search",
                "ANNS",
                "retrieval-augmented generation",
                "RAG",
                "agent-based LLM",
                "reinforcement learning",
                "execution speed",
                "reward signal",
                "NNS benchmark datasets",
                "GIST-960-Euclidean",
                "MNIST-784-Euclidean",
                "GloVe-25-angular",
                "SIFT-128-Euclidean"
            ]
        },
        "publishedAt": "2025-08-04T01:57:46.000Z",
        "title": "CRINN: Contrastive Reinforcement Learning for Approximate Nearest\n  Neighbor Search",
        "summary": "Approximate nearest-neighbor search (ANNS) algorithms have become\nincreasingly critical for recent AI applications, particularly in\nretrieval-augmented generation (RAG) and agent-based LLM applications. In this\npaper, we present CRINN, a new paradigm for ANNS algorithms. CRINN treats ANNS\noptimization as a reinforcement learning problem where execution speed serves\nas the reward signal. This approach enables the automatic generation of\nprogressively faster ANNS implementations while maintaining accuracy\nconstraints. Our experimental evaluation demonstrates CRINN's effectiveness\nacross six widely-used NNS benchmark datasets. When compared against\nstate-of-the-art open-source ANNS algorithms, CRINN achieves best performance\non three of them (GIST-960-Euclidean, MNIST-784-Euclidean, and\nGloVe-25-angular), and tied for first place on two of them (SIFT-128-Euclidean\nand GloVe-25-angular). The implications of CRINN's success reach well beyond\nANNS optimization: It validates that LLMs augmented with reinforcement learning\ncan function as an effective tool for automating sophisticated algorithmic\noptimizations that demand specialized knowledge and labor-intensive manual\nrefinement.Code can be found at https://github.com/deepreinforce-ai/CRINN",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6803ddda5c044d396d02f03a/HjerqHcTmj_DCsAFjU5w3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02091.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6803ddda5c044d396d02f03a",
            "avatarUrl": "/avatars/ec583f6560d3d82fbd3bf95fb83c9356.svg",
            "fullname": "Xiaoya Li",
            "name": "xxiaoyali",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.03050",
            "authors": [
                {
                    "_id": "6892e5018da45ffb0a2b24dc",
                    "user": {
                        "_id": "66c45954ab8f09b10b7ab6a8",
                        "avatarUrl": "/avatars/f9946c775c4d70b8e044865ac34ef121.svg",
                        "isPro": false,
                        "fullname": "Zhu",
                        "user": "ZaynZhu",
                        "type": "user"
                    },
                    "name": "Zeyu Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:16:18.558Z",
                    "hidden": false
                },
                {
                    "_id": "6892e5018da45ffb0a2b24dd",
                    "name": "Weijia Wu",
                    "hidden": false
                },
                {
                    "_id": "6892e5018da45ffb0a2b24de",
                    "name": "Mike Zheng Shou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T03:54:18.000Z",
            "submittedOnDailyAt": "2025-08-06T03:46:25.473Z",
            "title": "Multi-human Interactive Talking Dataset",
            "submittedOnDailyBy": {
                "_id": "6345a93afe134dfd7a0cfabd",
                "avatarUrl": "/avatars/65130ce06b1c72ab1066678419731d88.svg",
                "isPro": false,
                "fullname": "wu weijia",
                "user": "weijiawu",
                "type": "user"
            },
            "summary": "Existing studies on talking video generation have predominantly focused on\nsingle-person monologues or isolated facial animations, limiting their\napplicability to realistic multi-human interactions. To bridge this gap, we\nintroduce MIT, a large-scale dataset specifically designed for multi-human\ntalking video generation. To this end, we develop an automatic pipeline that\ncollects and annotates multi-person conversational videos. The resulting\ndataset comprises 12 hours of high-resolution footage, each featuring two to\nfour speakers, with fine-grained annotations of body poses and speech\ninteractions. It captures natural conversational dynamics in multi-speaker\nscenario, offering a rich resource for studying interactive visual behaviors.\nTo demonstrate the potential of MIT, we furthur propose CovOG, a baseline model\nfor this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle\nvarying numbers of speakers by aggregating individual pose embeddings, and an\nInteractive Audio Driver (IAD) to modulate head dynamics based on\nspeaker-specific audio features. Together, these components showcase the\nfeasibility and challenges of generating realistic multi-human talking videos,\nestablishing MIT as a valuable benchmark for future research. The code is\navalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.",
            "upvotes": 6,
            "discussionId": "6892e5018da45ffb0a2b24df",
            "ai_summary": "MIT, a large-scale dataset for multi-human talking video generation, includes fine-grained annotations and is used to demonstrate CovOG, a baseline model integrating a Multi-Human Pose Encoder and an Interactive Audio Driver.",
            "ai_keywords": [
                "Multi-Human Pose Encoder",
                "Interactive Audio Driver",
                "multi-human talking video generation",
                "fine-grained annotations",
                "body poses",
                "speech interactions",
                "conversational dynamics",
                "multi-speaker scenario"
            ]
        },
        "publishedAt": "2025-08-04T23:54:18.000Z",
        "title": "Multi-human Interactive Talking Dataset",
        "summary": "Existing studies on talking video generation have predominantly focused on\nsingle-person monologues or isolated facial animations, limiting their\napplicability to realistic multi-human interactions. To bridge this gap, we\nintroduce MIT, a large-scale dataset specifically designed for multi-human\ntalking video generation. To this end, we develop an automatic pipeline that\ncollects and annotates multi-person conversational videos. The resulting\ndataset comprises 12 hours of high-resolution footage, each featuring two to\nfour speakers, with fine-grained annotations of body poses and speech\ninteractions. It captures natural conversational dynamics in multi-speaker\nscenario, offering a rich resource for studying interactive visual behaviors.\nTo demonstrate the potential of MIT, we furthur propose CovOG, a baseline model\nfor this novel task. It integrates a Multi-Human Pose Encoder (MPE) to handle\nvarying numbers of speakers by aggregating individual pose embeddings, and an\nInteractive Audio Driver (IAD) to modulate head dynamics based on\nspeaker-specific audio features. Together, these components showcase the\nfeasibility and challenges of generating realistic multi-human talking videos,\nestablishing MIT as a valuable benchmark for future research. The code is\navalibale at: https://github.com/showlab/Multi-human-Talking-Video-Dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03050.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6345a93afe134dfd7a0cfabd",
            "avatarUrl": "/avatars/65130ce06b1c72ab1066678419731d88.svg",
            "fullname": "wu weijia",
            "name": "weijiawu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01119",
            "authors": [
                {
                    "_id": "689293748da45ffb0a2b238f",
                    "user": {
                        "_id": "67a10af0f79b3b8da5334183",
                        "avatarUrl": "/avatars/1e867d56714fc6fd26dbd55b171393f3.svg",
                        "isPro": false,
                        "fullname": "Saba Ahmadi",
                        "user": "sabaa96",
                        "type": "user"
                    },
                    "name": "Saba Ahmadi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:17:50.666Z",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2390",
                    "name": "Rabiul Awal",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2391",
                    "user": {
                        "_id": "62fc31f5f60cc3ae073f6c90",
                        "avatarUrl": "/avatars/5e2dcbdc069f956c7694142f94a466b2.svg",
                        "isPro": true,
                        "fullname": "Ankur Sikarwar",
                        "user": "sikarwarank",
                        "type": "user"
                    },
                    "name": "Ankur Sikarwar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:17:54.210Z",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2392",
                    "name": "Amirhossein Kazemnejad",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2393",
                    "name": "Ge Ya Luo",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2394",
                    "name": "Juan A. Rodriguez",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2395",
                    "name": "Sai Rajeswar",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2396",
                    "name": "Siva Reddy",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2397",
                    "name": "Christopher Pal",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2398",
                    "name": "Benno Krojer",
                    "hidden": false
                },
                {
                    "_id": "689293748da45ffb0a2b2399",
                    "name": "Aishwarya Agrawal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T23:47:29.000Z",
            "submittedOnDailyAt": "2025-08-06T17:51:15.459Z",
            "title": "The Promise of RL for Autoregressive Image Editing",
            "submittedOnDailyBy": {
                "_id": "62fc31f5f60cc3ae073f6c90",
                "avatarUrl": "/avatars/5e2dcbdc069f956c7694142f94a466b2.svg",
                "isPro": true,
                "fullname": "Ankur Sikarwar",
                "user": "sikarwarank",
                "type": "user"
            },
            "summary": "We explore three strategies to enhance performance on a wide range of image\nediting tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and\nChain-of-Thought (CoT) reasoning. In order to study all these components in one\nconsistent framework, we adopt an autoregressive multimodal model that\nprocesses textual and visual tokens in a unified manner. We find RL combined\nwith a large multi-modal LLM verifier to be the most effective of these\nstrategies. As a result, we release EARL: Editing with Autoregression and RL, a\nstrong RL-based image editing model that performs competitively on a diverse\nrange of edits compared to strong baselines, despite using much less training\ndata. Thus, EARL pushes the frontier of autoregressive multimodal models on\nimage editing. We release our code, training data, and trained models at\nhttps://github.com/mair-lab/EARL.",
            "upvotes": 6,
            "discussionId": "689293748da45ffb0a2b239a",
            "githubRepo": "https://github.com/mair-lab/EARL",
            "ai_summary": "Reinforcement learning combined with a large multimodal language model verifier enhances image editing performance in an autoregressive multimodal framework.",
            "ai_keywords": [
                "supervised fine-tuning",
                "reinforcement learning",
                "Chain-of-Thought reasoning",
                "autoregressive multimodal model",
                "textual tokens",
                "visual tokens",
                "large multi-modal LLM verifier",
                "EARL",
                "Editing with Autoregression and RL"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-08-01T19:47:29.000Z",
        "title": "The Promise of RL for Autoregressive Image Editing",
        "summary": "We explore three strategies to enhance performance on a wide range of image\nediting tasks: supervised fine-tuning (SFT), reinforcement learning (RL), and\nChain-of-Thought (CoT) reasoning. In order to study all these components in one\nconsistent framework, we adopt an autoregressive multimodal model that\nprocesses textual and visual tokens in a unified manner. We find RL combined\nwith a large multi-modal LLM verifier to be the most effective of these\nstrategies. As a result, we release EARL: Editing with Autoregression and RL, a\nstrong RL-based image editing model that performs competitively on a diverse\nrange of edits compared to strong baselines, despite using much less training\ndata. Thus, EARL pushes the frontier of autoregressive multimodal models on\nimage editing. We release our code, training data, and trained models at\nhttps://github.com/mair-lab/EARL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01119.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62fc31f5f60cc3ae073f6c90",
            "avatarUrl": "/avatars/5e2dcbdc069f956c7694142f94a466b2.svg",
            "fullname": "Ankur Sikarwar",
            "name": "sikarwarank",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03613",
            "authors": [
                {
                    "_id": "6892cb908da45ffb0a2b2490",
                    "name": "Yong Lin",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b2491",
                    "name": "Shange Tang",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b2492",
                    "user": {
                        "_id": "650267e7e751d03da933a24a",
                        "avatarUrl": "/avatars/f047a047d1de304cd97027463541bdf3.svg",
                        "isPro": false,
                        "fullname": "Bohan22",
                        "user": "Bohan22",
                        "type": "user"
                    },
                    "name": "Bohan Lyu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:16:36.323Z",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b2493",
                    "name": "Ziran Yang",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b2494",
                    "name": "Jui-Hui Chung",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b2495",
                    "name": "Haoyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b2496",
                    "name": "Lai Jiang",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b2497",
                    "name": "Yihan Geng",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b2498",
                    "name": "Jiawei Ge",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b2499",
                    "name": "Jingruo Sun",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b249a",
                    "name": "Jiayun Wu",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b249b",
                    "name": "Jiri Gesi",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b249c",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b249d",
                    "name": "David Acuna",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b249e",
                    "name": "Kaiyu Yang",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b249f",
                    "name": "Hongzhou Lin",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b24a0",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b24a1",
                    "name": "Danqi Chen",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b24a2",
                    "name": "Sanjeev Arora",
                    "hidden": false
                },
                {
                    "_id": "6892cb908da45ffb0a2b24a3",
                    "name": "Chi Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T16:28:22.000Z",
            "submittedOnDailyAt": "2025-08-06T07:01:12.666Z",
            "title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction",
            "submittedOnDailyBy": {
                "_id": "650267e7e751d03da933a24a",
                "avatarUrl": "/avatars/f047a047d1de304cd97027463541bdf3.svg",
                "isPro": false,
                "fullname": "Bohan22",
                "user": "Bohan22",
                "type": "user"
            },
            "summary": "We introduce Goedel-Prover-V2, a series of open-source language models that\nset a new state-of-the-art in automated theorem proving. Built on the standard\nexpert iteration and reinforcement learning pipeline, our approach incorporates\nthree key innovations: (1) Scaffolded data synthesis: We generate synthetic\ntasks of increasing difficulty to train the model to master increasingly\ncomplex theorems; (2) Verifier-guided self-correction: We enable the model to\niteratively revise its proofs by leveraging feedback from the Lean compiler;\n(3) Model averaging: We merge model checkpoints to mitigate the decrease in\nmodel output diversity in later stages of training. Our small model,\nGoedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms\nDeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our\nflagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in\nstandard mode and 90.4% in self-correction mode, outperforming prior SOTA by a\nlarge margin. Additionally, our flagship model solves 86 problems on\nPutnamBench at pass@184, securing the first place among open-source models on\nthe leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47\nproblems by pass@1024 with a significantly smaller model size and compute\nbudget. At the time of its release (July-August 2025), Goedel-Prover-V2\nachieves the strongest overall performance among all open-source theorem\nprovers. It also ranks among the top-performing models--including closed-source\nsystems with publicly reported performance--under a constrained test-time\ncompute budget. Our models, code, and data are released at\nhttps://github.com/Goedel-LM/Goedel-Prover-V2.",
            "upvotes": 5,
            "discussionId": "6892cb908da45ffb0a2b24a8",
            "ai_summary": "Goedel-Prover-V2, a series of open-source language models, achieves state-of-the-art performance in automated theorem proving through scaffolded data synthesis, verifier-guided self-correction, and model averaging.",
            "ai_keywords": [
                "scaffolded data synthesis",
                "verifier-guided self-correction",
                "model averaging",
                "automated theorem proving",
                "MiniF2F",
                "PutnamBench",
                "pass@32",
                "pass@184",
                "pass@1024"
            ]
        },
        "publishedAt": "2025-08-05T12:28:22.000Z",
        "title": "Goedel-Prover-V2: Scaling Formal Theorem Proving with Scaffolded Data\n  Synthesis and Self-Correction",
        "summary": "We introduce Goedel-Prover-V2, a series of open-source language models that\nset a new state-of-the-art in automated theorem proving. Built on the standard\nexpert iteration and reinforcement learning pipeline, our approach incorporates\nthree key innovations: (1) Scaffolded data synthesis: We generate synthetic\ntasks of increasing difficulty to train the model to master increasingly\ncomplex theorems; (2) Verifier-guided self-correction: We enable the model to\niteratively revise its proofs by leveraging feedback from the Lean compiler;\n(3) Model averaging: We merge model checkpoints to mitigate the decrease in\nmodel output diversity in later stages of training. Our small model,\nGoedel-Prover-V2-8B, reaches 84.6% pass@32 on MiniF2F and outperforms\nDeepSeek-Prover-V2-671B under the same metric, despite being 80X smaller. Our\nflagship model, Goedel-Prover-V2-32B, achieves 88.1% on MiniF2F at pass@32 in\nstandard mode and 90.4% in self-correction mode, outperforming prior SOTA by a\nlarge margin. Additionally, our flagship model solves 86 problems on\nPutnamBench at pass@184, securing the first place among open-source models on\nthe leaderboard, surpassing DeepSeek-Prover-V2-671B's record of solving 47\nproblems by pass@1024 with a significantly smaller model size and compute\nbudget. At the time of its release (July-August 2025), Goedel-Prover-V2\nachieves the strongest overall performance among all open-source theorem\nprovers. It also ranks among the top-performing models--including closed-source\nsystems with publicly reported performance--under a constrained test-time\ncompute budget. Our models, code, and data are released at\nhttps://github.com/Goedel-LM/Goedel-Prover-V2.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03613.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650267e7e751d03da933a24a",
            "avatarUrl": "/avatars/f047a047d1de304cd97027463541bdf3.svg",
            "fullname": "Bohan22",
            "name": "Bohan22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.01780",
            "authors": [
                {
                    "_id": "689197dcf01a094725f83589",
                    "user": {
                        "_id": "664cd01f4f959a203dcf97a8",
                        "avatarUrl": "/avatars/f2216e0697e29c424818438f8cf0427d.svg",
                        "isPro": false,
                        "fullname": "mo guozhao",
                        "user": "hysdhlx",
                        "type": "user"
                    },
                    "name": "Guozhao Mo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:21:23.008Z",
                    "hidden": false
                },
                {
                    "_id": "689197dcf01a094725f8358a",
                    "name": "Wenliang Zhong",
                    "hidden": false
                },
                {
                    "_id": "689197dcf01a094725f8358b",
                    "user": {
                        "_id": "654c7fbe6b51714c2a6ff590",
                        "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
                        "isPro": false,
                        "fullname": "Jiawei Chen",
                        "user": "jiawei-ucas",
                        "type": "user"
                    },
                    "name": "Jiawei Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:21:17.511Z",
                    "hidden": false
                },
                {
                    "_id": "689197dcf01a094725f8358c",
                    "name": "Xuanang Chen",
                    "hidden": false
                },
                {
                    "_id": "689197dcf01a094725f8358d",
                    "name": "Yaojie Lu",
                    "hidden": false
                },
                {
                    "_id": "689197dcf01a094725f8358e",
                    "name": "Hongyu Lin",
                    "hidden": false
                },
                {
                    "_id": "689197dcf01a094725f8358f",
                    "name": "Ben He",
                    "hidden": false
                },
                {
                    "_id": "689197dcf01a094725f83590",
                    "name": "Xianpei Han",
                    "hidden": false
                },
                {
                    "_id": "689197dcf01a094725f83591",
                    "name": "Le Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-03T14:36:42.000Z",
            "submittedOnDailyAt": "2025-08-06T01:39:40.186Z",
            "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
            "submittedOnDailyBy": {
                "_id": "654c7fbe6b51714c2a6ff590",
                "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
                "isPro": false,
                "fullname": "Jiawei Chen",
                "user": "jiawei-ucas",
                "type": "user"
            },
            "summary": "With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench.",
            "upvotes": 5,
            "discussionId": "689197ddf01a094725f83592",
            "ai_summary": "LiveMCPBench provides a comprehensive benchmark for evaluating LLM agents across a diverse set of real-world tasks in the MCP ecosystem, using a scalable evaluation pipeline and adaptive judging framework.",
            "ai_keywords": [
                "Model Context Protocol",
                "MCP",
                "LiveMCPBench",
                "LLM agents",
                "real-world tasks",
                "LiveMCPTool",
                "LiveMCPEval",
                "LLM-as-a-Judge",
                "MCP Copilot Agent",
                "API interaction",
                "dynamic task environments",
                "human reviewers",
                "performance variance"
            ]
        },
        "publishedAt": "2025-08-03T10:36:42.000Z",
        "title": "LiveMCPBench: Can Agents Navigate an Ocean of MCP Tools?",
        "summary": "With the rapid development of Model Context Protocol (MCP), the number of MCP\nservers has surpassed 10,000. However, existing MCP benchmarks are limited to\nsingle-server settings with only a few tools, hindering effective evaluation of\nagent capabilities in large-scale, real-world scenarios. To address this\nlimitation, we present LiveMCPBench, the first comprehensive benchmark\ncomprising 95 real-world tasks grounded in the MCP ecosystem, designed to\nevaluate LLM agents at scale across diverse servers. To support a scalable and\nreproducible evaluation pipeline in large-scale MCP environments, we curate\nLiveMCPTool, a diverse and readily deployable collection of 70 MCP servers and\n527 tools. Furthermore, we introduce LiveMCPEval, an LLM-as-a-Judge framework\nthat enables automated and adaptive evaluation in dynamic, time-varying task\nenvironments, achieving 81% agreement with human reviewers. Finally, we propose\nthe MCP Copilot Agent, a multi-step agent that routes tools for dynamic\nplanning and executes tools for API interaction across the entire LiveMCPTool\nsuite. Our evaluation covers 10 leading models, with the best-performing model\n(Claude-Sonnet-4) reaching a 78.95% success rate. However, we observe large\nperformance variance across models, and several widely-used models perform\npoorly in LiveMCPBench's complex, tool-rich environments. Overall, LiveMCPBench\noffers the first unified framework for benchmarking LLM agents in realistic,\ntool-rich, and dynamic MCP environments, laying a solid foundation for scalable\nand reproducible research on agent capabilities. Our code and data will be\npublicly available at https://icip-cas.github.io/LiveMCPBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01780.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654c7fbe6b51714c2a6ff590",
            "avatarUrl": "/avatars/db217415c56730872b9a807f3afb4e5b.svg",
            "fullname": "Jiawei Chen",
            "name": "jiawei-ucas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.00477",
            "authors": [
                {
                    "_id": "689300478da45ffb0a2b252a",
                    "user": {
                        "_id": "6751c2fd472d4a1f5270bbdb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6751c2fd472d4a1f5270bbdb/tNW2jEJkNl9g_oIxV82KD.jpeg",
                        "isPro": false,
                        "fullname": "Yuzhuo Chen",
                        "user": "Suchenl",
                        "type": "user"
                    },
                    "name": "Yuzhuo Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:15:22.375Z",
                    "hidden": false
                },
                {
                    "_id": "689300478da45ffb0a2b252b",
                    "name": "Zehua Ma",
                    "hidden": false
                },
                {
                    "_id": "689300478da45ffb0a2b252c",
                    "name": "Jianhua Wang",
                    "hidden": false
                },
                {
                    "_id": "689300478da45ffb0a2b252d",
                    "name": "Kai Kang",
                    "hidden": false
                },
                {
                    "_id": "689300478da45ffb0a2b252e",
                    "name": "Shunyu Yao",
                    "hidden": false
                },
                {
                    "_id": "689300478da45ffb0a2b252f",
                    "name": "Weiming Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T09:51:54.000Z",
            "submittedOnDailyAt": "2025-08-06T05:43:40.504Z",
            "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "64e5f0c23e220d8f697d1ab0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5f0c23e220d8f697d1ab0/qD-Egzxs-ZvJT5GXeqE3d.jpeg",
                "isPro": false,
                "fullname": "Jinsong Li",
                "user": "Jinsong-Li",
                "type": "user"
            },
            "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.",
            "upvotes": 4,
            "discussionId": "689300478da45ffb0a2b2530",
            "githubRepo": "https://github.com/Suchenl/LAMIC",
            "ai_summary": "LAMIC, a Layout-Aware Multi-Image Composition framework, extends single-reference diffusion models to multi-reference scenarios using attention mechanisms, achieving state-of-the-art performance in controllable image synthesis without training.",
            "ai_keywords": [
                "LAMIC",
                "Layout-Aware Multi-Image Composition",
                "diffusion models",
                "Group Isolation Attention",
                "Region-Modulated Attention",
                "Inclusion Ratio",
                "Fill Ratio",
                "Background Similarity",
                "ID-S",
                "BG-S",
                "IN-R",
                "AVG",
                "DPG",
                "zero-shot generalization"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-08-01T05:51:54.000Z",
        "title": "LAMIC: Layout-Aware Multi-Image Composition via Scalability of\n  Multimodal Diffusion Transformer",
        "summary": "In controllable image synthesis, generating coherent and consistent images\nfrom multiple references with spatial layout awareness remains an open\nchallenge. We present LAMIC, a Layout-Aware Multi-Image Composition framework\nthat, for the first time, extends single-reference diffusion models to\nmulti-reference scenarios in a training-free manner. Built upon the MMDiT\nmodel, LAMIC introduces two plug-and-play attention mechanisms: 1) Group\nIsolation Attention (GIA) to enhance entity disentanglement; and 2)\nRegion-Modulated Attention (RMA) to enable layout-aware generation. To\ncomprehensively evaluate model capabilities, we further introduce three\nmetrics: 1) Inclusion Ratio (IN-R) and Fill Ratio (FI-R) for assessing layout\ncontrol; and 2) Background Similarity (BG-S) for measuring background\nconsistency. Extensive experiments show that LAMIC achieves state-of-the-art\nperformance across most major metrics: it consistently outperforms existing\nmulti-reference baselines in ID-S, BG-S, IN-R and AVG scores across all\nsettings, and achieves the best DPG in complex composition tasks. These results\ndemonstrate LAMIC's superior abilities in identity keeping, background\npreservation, layout control, and prompt-following, all achieved without any\ntraining or fine-tuning, showcasing strong zero-shot generalization ability. By\ninheriting the strengths of advanced single-reference models and enabling\nseamless extension to multi-image scenarios, LAMIC establishes a new\ntraining-free paradigm for controllable multi-image composition. As foundation\nmodels continue to evolve, LAMIC's performance is expected to scale\naccordingly. Our implementation is available at:\nhttps://github.com/Suchenl/LAMIC.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00477.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e5f0c23e220d8f697d1ab0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5f0c23e220d8f697d1ab0/qD-Egzxs-ZvJT5GXeqE3d.jpeg",
            "fullname": "Jinsong Li",
            "name": "Jinsong-Li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.03164",
            "authors": [
                {
                    "_id": "6892f90b8da45ffb0a2b2510",
                    "name": "Junyoung Lim",
                    "hidden": false
                },
                {
                    "_id": "6892f90b8da45ffb0a2b2511",
                    "user": {
                        "_id": "64bb081c01f1983a863654dc",
                        "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
                        "isPro": false,
                        "fullname": "Jaewoo Ahn",
                        "user": "ahnpersie",
                        "type": "user"
                    },
                    "name": "Jaewoo Ahn",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:15:56.746Z",
                    "hidden": false
                },
                {
                    "_id": "6892f90b8da45ffb0a2b2512",
                    "name": "Gunhee Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T07:09:07.000Z",
            "submittedOnDailyAt": "2025-08-06T07:15:28.837Z",
            "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning",
            "submittedOnDailyBy": {
                "_id": "64bb081c01f1983a863654dc",
                "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
                "isPro": false,
                "fullname": "Jaewoo Ahn",
                "user": "ahnpersie",
                "type": "user"
            },
            "summary": "Generating accurate, informative, and hallucination-free captions for charts\nremains challenging for vision language models, primarily due to the lack of\nlarge-scale, high-quality datasets of real-world charts. However, existing\nreal-world chart datasets suffer from the inclusion of extraneous information\nthat cannot be inferred from the chart and failure to sufficiently capture\nstructural elements and key insights. Therefore, we introduce ChartCap, a\nlarge-scale dataset of 565K real-world chart images paired with type-specific,\ndense captions that exclude extraneous information and highlight both\nstructural elements and key insights in detail. To build ChartCap, we design a\nfour-stage pipeline that generates captions using only the discernible data\nfrom the chart and employ a cycle consistency-based human verification, which\naccelerates quality control without sacrificing accuracy. Additionally, we\npropose a novel metric, the Visual Consistency Score, which evaluates caption\nquality by measuring the similarity between the chart regenerated from a\ncaption and the original chart, independent of reference captions. Extensive\nexperiments confirms that models fine-tuned on ChartCap consistently generate\nmore accurate and informative captions with reduced hallucinations, surpassing\nboth open-source and proprietary models and even human-annotated captions.",
            "upvotes": 3,
            "discussionId": "6892f90b8da45ffb0a2b2513",
            "projectPage": "https://junyoung-00.github.io/ChartCap/",
            "ai_summary": "ChartCap, a large-scale dataset with dense, type-specific captions for real-world charts, improves caption accuracy and reduces hallucinations in vision language models.",
            "ai_keywords": [
                "vision language models",
                "large-scale dataset",
                "real-world charts",
                "dense captions",
                "extraneous information",
                "structural elements",
                "key insights",
                "four-stage pipeline",
                "cycle consistency-based human verification",
                "Visual Consistency Score",
                "caption quality",
                "hallucinations"
            ]
        },
        "publishedAt": "2025-08-05T03:09:07.000Z",
        "title": "ChartCap: Mitigating Hallucination of Dense Chart Captioning",
        "summary": "Generating accurate, informative, and hallucination-free captions for charts\nremains challenging for vision language models, primarily due to the lack of\nlarge-scale, high-quality datasets of real-world charts. However, existing\nreal-world chart datasets suffer from the inclusion of extraneous information\nthat cannot be inferred from the chart and failure to sufficiently capture\nstructural elements and key insights. Therefore, we introduce ChartCap, a\nlarge-scale dataset of 565K real-world chart images paired with type-specific,\ndense captions that exclude extraneous information and highlight both\nstructural elements and key insights in detail. To build ChartCap, we design a\nfour-stage pipeline that generates captions using only the discernible data\nfrom the chart and employ a cycle consistency-based human verification, which\naccelerates quality control without sacrificing accuracy. Additionally, we\npropose a novel metric, the Visual Consistency Score, which evaluates caption\nquality by measuring the similarity between the chart regenerated from a\ncaption and the original chart, independent of reference captions. Extensive\nexperiments confirms that models fine-tuned on ChartCap consistently generate\nmore accurate and informative captions with reduced hallucinations, surpassing\nboth open-source and proprietary models and even human-annotated captions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03164.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64bb081c01f1983a863654dc",
            "avatarUrl": "/avatars/038894bc72b92ec3f4ecb096cc60b60a.svg",
            "fullname": "Jaewoo Ahn",
            "name": "ahnpersie",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.02629",
            "authors": [
                {
                    "_id": "68938603741a16f544fbcd9c",
                    "name": "Yibin Liu",
                    "hidden": false
                },
                {
                    "_id": "68938603741a16f544fbcd9d",
                    "name": "Zhixuan Liang",
                    "hidden": false
                },
                {
                    "_id": "68938603741a16f544fbcd9e",
                    "name": "Zanxin Chen",
                    "hidden": false
                },
                {
                    "_id": "68938603741a16f544fbcd9f",
                    "name": "Tianxing Chen",
                    "hidden": false
                },
                {
                    "_id": "68938603741a16f544fbcda0",
                    "name": "Mengkang Hu",
                    "hidden": false
                },
                {
                    "_id": "68938603741a16f544fbcda1",
                    "name": "Wanxi Dong",
                    "hidden": false
                },
                {
                    "_id": "68938603741a16f544fbcda2",
                    "name": "Congsheng Xu",
                    "hidden": false
                },
                {
                    "_id": "68938603741a16f544fbcda3",
                    "name": "Zhaoming Han",
                    "hidden": false
                },
                {
                    "_id": "68938603741a16f544fbcda4",
                    "name": "Yusen Qin",
                    "hidden": false
                },
                {
                    "_id": "68938603741a16f544fbcda5",
                    "name": "Yao Mu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T17:18:14.000Z",
            "submittedOnDailyAt": "2025-08-06T15:14:37.159Z",
            "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents",
            "submittedOnDailyBy": {
                "_id": "662a471e94baa018b00c0f5c",
                "avatarUrl": "/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg",
                "isPro": false,
                "fullname": "Zhixuan Liang",
                "user": "Liang-ZX",
                "type": "user"
            },
            "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.",
            "upvotes": 3,
            "discussionId": "68938604741a16f544fbcda6",
            "ai_summary": "HyCodePolicy integrates code synthesis, geometric grounding, perceptual monitoring, and iterative repair to enhance the robustness and efficiency of embodied agent policies.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "code policy generation",
                "embodied agents",
                "HyCodePolicy",
                "code synthesis",
                "geometric grounding",
                "perceptual monitoring",
                "iterative repair",
                "closed-loop programming cycle",
                "natural language instruction",
                "subgoals",
                "executable program",
                "object-centric geometric primitives",
                "vision-language model",
                "VLM",
                "execution failures",
                "failure reasons",
                "structured execution traces",
                "perceptual feedback",
                "self-correcting program synthesis",
                "sample efficiency",
                "robot manipulation policies",
                "multimodal reasoning",
                "autonomous decision-making pipelines"
            ]
        },
        "publishedAt": "2025-08-04T13:18:14.000Z",
        "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents",
        "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02629.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "662a471e94baa018b00c0f5c",
            "avatarUrl": "/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg",
            "fullname": "Zhixuan Liang",
            "name": "Liang-ZX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02630",
            "authors": [
                {
                    "_id": "68937517741a16f544fbcd63",
                    "user": {
                        "_id": "64753220a855203d8fee2090",
                        "avatarUrl": "/avatars/543717791e29cdfb1475443ff01b9186.svg",
                        "isPro": true,
                        "fullname": "Amine",
                        "user": "AmineAllo",
                        "type": "user"
                    },
                    "name": "Amine Allouah",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:13:07.961Z",
                    "hidden": false
                },
                {
                    "_id": "68937517741a16f544fbcd64",
                    "name": "Omar Besbes",
                    "hidden": false
                },
                {
                    "_id": "68937517741a16f544fbcd65",
                    "name": "Josué D Figueroa",
                    "hidden": false
                },
                {
                    "_id": "68937517741a16f544fbcd66",
                    "name": "Yash Kanoria",
                    "hidden": false
                },
                {
                    "_id": "68937517741a16f544fbcd67",
                    "name": "Akshit Kumar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T17:19:36.000Z",
            "submittedOnDailyAt": "2025-08-06T18:24:42.707Z",
            "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce",
            "submittedOnDailyBy": {
                "_id": "64753220a855203d8fee2090",
                "avatarUrl": "/avatars/543717791e29cdfb1475443ff01b9186.svg",
                "isPro": true,
                "fullname": "Amine",
                "user": "AmineAllo",
                "type": "user"
            },
            "summary": "Online marketplaces will be transformed by autonomous AI agents acting on\nbehalf of consumers. Rather than humans browsing and clicking,\nvision-language-model (VLM) agents can parse webpages, evaluate products, and\ntransact. This raises a fundamental question: what do AI agents buy, and why?\nWe develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent\nwith a fully programmable mock marketplace to study this question. We first\nconduct basic rationality checks in the context of simple tasks, and then, by\nrandomizing product positions, prices, ratings, reviews, sponsored tags, and\nplatform endorsements, we obtain causal estimates of how frontier VLMs actually\nshop. Models show strong but heterogeneous position effects: all favor the top\nrow, yet different models prefer different columns, undermining the assumption\nof a universal \"top\" rank. They penalize sponsored tags and reward\nendorsements. Sensitivities to price, ratings, and reviews are directionally\nhuman-like but vary sharply in magnitude across models. Motivated by scenarios\nwhere sellers use AI agents to optimize product listings, we show that a\nseller-side agent that makes minor tweaks to product descriptions, targeting AI\nbuyer preferences, can deliver substantial market-share gains if AI-mediated\nshopping dominates. We also find that modal product choices can differ across\nmodels and, in some cases, demand may concentrate on a few select products,\nraising competition questions. Together, our results illuminate how AI agents\nmay behave in e-commerce settings and surface concrete seller strategy,\nplatform design, and regulatory questions in an AI-mediated ecosystem.",
            "upvotes": 2,
            "discussionId": "68937517741a16f544fbcd68",
            "projectPage": "https://ace.mycustomai.io/",
            "githubRepo": "https://github.com/mycustomai/ACES",
            "ai_summary": "ACES, a sandbox environment, studies AI agents' shopping behavior in a mock marketplace, revealing position effects, sensitivity to sponsored tags, endorsements, prices, ratings, and reviews, and highlighting implications for seller strategies and platform design.",
            "ai_keywords": [
                "vision-language-model",
                "VLM",
                "ACES",
                "sandbox environment",
                "mock marketplace",
                "position effects",
                "sponsored tags",
                "endorsements",
                "price sensitivity",
                "rating sensitivity",
                "review sensitivity",
                "AI-mediated shopping",
                "market-share gains",
                "competition questions"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-08-04T13:19:36.000Z",
        "title": "What Is Your AI Agent Buying? Evaluation, Implications and Emerging\n  Questions for Agentic E-Commerce",
        "summary": "Online marketplaces will be transformed by autonomous AI agents acting on\nbehalf of consumers. Rather than humans browsing and clicking,\nvision-language-model (VLM) agents can parse webpages, evaluate products, and\ntransact. This raises a fundamental question: what do AI agents buy, and why?\nWe develop ACES, a sandbox environment that pairs a platform-agnostic VLM agent\nwith a fully programmable mock marketplace to study this question. We first\nconduct basic rationality checks in the context of simple tasks, and then, by\nrandomizing product positions, prices, ratings, reviews, sponsored tags, and\nplatform endorsements, we obtain causal estimates of how frontier VLMs actually\nshop. Models show strong but heterogeneous position effects: all favor the top\nrow, yet different models prefer different columns, undermining the assumption\nof a universal \"top\" rank. They penalize sponsored tags and reward\nendorsements. Sensitivities to price, ratings, and reviews are directionally\nhuman-like but vary sharply in magnitude across models. Motivated by scenarios\nwhere sellers use AI agents to optimize product listings, we show that a\nseller-side agent that makes minor tweaks to product descriptions, targeting AI\nbuyer preferences, can deliver substantial market-share gains if AI-mediated\nshopping dominates. We also find that modal product choices can differ across\nmodels and, in some cases, demand may concentrate on a few select products,\nraising competition questions. Together, our results illuminate how AI agents\nmay behave in e-commerce settings and surface concrete seller strategy,\nplatform design, and regulatory questions in an AI-mediated ecosystem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02630.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64753220a855203d8fee2090",
            "avatarUrl": "/avatars/543717791e29cdfb1475443ff01b9186.svg",
            "fullname": "Amine",
            "name": "AmineAllo",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.02079",
            "authors": [
                {
                    "_id": "6892c2a18da45ffb0a2b241e",
                    "name": "Amitava Das",
                    "hidden": false
                },
                {
                    "_id": "6892c2a18da45ffb0a2b241f",
                    "name": "Abhilekh Borah",
                    "hidden": false
                },
                {
                    "_id": "6892c2a18da45ffb0a2b2420",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "6892c2a18da45ffb0a2b2421",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:17:17.881Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T05:45:24.000Z",
            "submittedOnDailyAt": "2025-08-06T01:22:23.102Z",
            "title": "AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization",
            "submittedOnDailyBy": {
                "_id": "63a4754927f1f64ed7238dac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                "isPro": false,
                "fullname": "Aman Chadha",
                "user": "amanchadha",
                "type": "user"
            },
            "summary": "Low-rank adaptation (LoRA) has become a standard tool for efficiently\nfine-tuning large language models (LLMs). Yet, even minor LoRA updates can\ninduce alignment drift, weakening safety and behavioral constraints through\nentangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL),\na principled framework for preserving alignment during finetuning. AGL\nintroduces several key components: a primary task loss for supervision, Fisher\nInformation Matrix-based regularization to restrict updates in\nalignment-sensitive subspaces, and task-specific regularization to stabilize\nthe integration of new knowledge. We further introduce collision-aware\nregularization, blending Riemannian overlap -- which penalizes coordinate-wise\ninterference -- and geodesic separation -- which encourages disjoint update\ngeometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and\nunsafe prompts designed to quantify alignment drift and safety degradation.\nEmpirical evaluations show that AGL mitigates alignment drift by up to 50% on\nsafety-critical benchmarks without degrading downstream task performance.\nComprehensive ablation confirms that each component contributes distinctly to\npreserving latent safety behaviors. Finally, we derive and validate a scaling\nlaw for catastrophic forgetting, revealing that AGL flattens post-finetuning\nloss escalation while preserving adaptation dynamics. AGL is a structurally\ngrounded refinement of LoRA, ensuring alignment preservation with minimal\ntrade-offs. To encourage further exploration and development, we open-source\nour implementation.",
            "upvotes": 2,
            "discussionId": "6892c2a18da45ffb0a2b2422",
            "ai_summary": "AlignGuard-LoRA (AGL) is a framework that preserves alignment during fine-tuning of large language models by introducing regularization techniques and a diagnostic benchmark to mitigate alignment drift.",
            "ai_keywords": [
                "LoRA",
                "AlignGuard-LoRA",
                "primary task loss",
                "Fisher Information Matrix-based regularization",
                "task-specific regularization",
                "collision-aware regularization",
                "Riemannian overlap",
                "geodesic separation",
                "DriftCaps",
                "alignment drift",
                "catastrophic forgetting",
                "adaptation dynamics"
            ]
        },
        "publishedAt": "2025-08-04T01:45:24.000Z",
        "title": "AlignGuard-LoRA: Alignment-Preserving Fine-Tuning via Fisher-Guided\n  Decomposition and Riemannian-Geodesic Collision Regularization",
        "summary": "Low-rank adaptation (LoRA) has become a standard tool for efficiently\nfine-tuning large language models (LLMs). Yet, even minor LoRA updates can\ninduce alignment drift, weakening safety and behavioral constraints through\nentangled parameter changes. To address this, we propose AlignGuard-LoRA (AGL),\na principled framework for preserving alignment during finetuning. AGL\nintroduces several key components: a primary task loss for supervision, Fisher\nInformation Matrix-based regularization to restrict updates in\nalignment-sensitive subspaces, and task-specific regularization to stabilize\nthe integration of new knowledge. We further introduce collision-aware\nregularization, blending Riemannian overlap -- which penalizes coordinate-wise\ninterference -- and geodesic separation -- which encourages disjoint update\ngeometry. We curate DriftCaps, a targeted diagnostic benchmark of safe and\nunsafe prompts designed to quantify alignment drift and safety degradation.\nEmpirical evaluations show that AGL mitigates alignment drift by up to 50% on\nsafety-critical benchmarks without degrading downstream task performance.\nComprehensive ablation confirms that each component contributes distinctly to\npreserving latent safety behaviors. Finally, we derive and validate a scaling\nlaw for catastrophic forgetting, revealing that AGL flattens post-finetuning\nloss escalation while preserving adaptation dynamics. AGL is a structurally\ngrounded refinement of LoRA, ensuring alignment preservation with minimal\ntrade-offs. To encourage further exploration and development, we open-source\nour implementation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02079.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.23284",
            "authors": [
                {
                    "_id": "688c35758c434640078cc3db",
                    "name": "Dohwan Ko",
                    "hidden": false
                },
                {
                    "_id": "688c35758c434640078cc3dc",
                    "name": "Ji Soo Lee",
                    "hidden": false
                },
                {
                    "_id": "688c35758c434640078cc3dd",
                    "name": "Minhyuk Choi",
                    "hidden": false
                },
                {
                    "_id": "688c35758c434640078cc3de",
                    "name": "Zihang Meng",
                    "hidden": false
                },
                {
                    "_id": "688c35758c434640078cc3df",
                    "name": "Hyunwoo J. Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T06:57:28.000Z",
            "submittedOnDailyAt": "2025-08-06T21:25:46.407Z",
            "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language\n  Models for Text-Video Retrieval",
            "submittedOnDailyBy": {
                "_id": "62bab5a69f045fd1fb9f760e",
                "avatarUrl": "/avatars/3f12a908faec1135e0221444f0769496.svg",
                "isPro": false,
                "fullname": "Dohwan Ko",
                "user": "ikodoh",
                "type": "user"
            },
            "summary": "Text-Video Retrieval aims to find the most relevant text (or video) candidate\ngiven a video (or text) query from large-scale online databases. Recent work\nleverages multi-modal large language models (MLLMs) to improve retrieval,\nespecially for long or complex query-candidate pairs. However, we observe that\nthe naive application of MLLMs, i.e., retrieval based on candidate likelihood,\nintroduces candidate prior bias, favoring candidates with inherently higher\npriors over those more relevant to the query. To this end, we propose a novel\nretrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),\nwhich leverages both query and candidate likelihoods by training the model to\ngenerate text from a given video as well as video features from a given text.\nFurthermore, we introduce Candidate Prior Normalization (CPN), a simple yet\neffective training-free score calibration module designed to mitigate candidate\nprior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,\nour BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4\nR@1 on average, effectively alleviating candidate prior bias and emphasizing\nquery-candidate relevance. Our in-depth analysis across various multi-modal\ntasks beyond retrieval highlights the broad applicability of CPN which enhances\nvisual understanding by reducing reliance on textual priors. Code is available\nat https://github.com/mlvlab/BLiM.",
            "upvotes": 2,
            "discussionId": "688c35768c434640078cc3e0",
            "projectPage": "https://ikodoh.github.io/BLiM",
            "githubRepo": "https://github.com/mlvlab/BLiM",
            "ai_summary": "A novel retrieval framework using bidirectional likelihood estimation with multi-modal large language models and candidate prior normalization improves text-video retrieval by reducing candidate prior bias and enhancing query-candidate relevance.",
            "ai_keywords": [
                "multi-modal large language models",
                "MLLMs",
                "bidirectional likelihood estimation",
                "BLiM",
                "candidate prior bias",
                "candidate likelihood",
                "Candidate Prior Normalization",
                "CPN",
                "text-video retrieval",
                "R@1",
                "visual understanding"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-07-31T02:57:28.000Z",
        "title": "Bidirectional Likelihood Estimation with Multi-Modal Large Language\n  Models for Text-Video Retrieval",
        "summary": "Text-Video Retrieval aims to find the most relevant text (or video) candidate\ngiven a video (or text) query from large-scale online databases. Recent work\nleverages multi-modal large language models (MLLMs) to improve retrieval,\nespecially for long or complex query-candidate pairs. However, we observe that\nthe naive application of MLLMs, i.e., retrieval based on candidate likelihood,\nintroduces candidate prior bias, favoring candidates with inherently higher\npriors over those more relevant to the query. To this end, we propose a novel\nretrieval framework, Bidirectional Likelihood Estimation with MLLM (BLiM),\nwhich leverages both query and candidate likelihoods by training the model to\ngenerate text from a given video as well as video features from a given text.\nFurthermore, we introduce Candidate Prior Normalization (CPN), a simple yet\neffective training-free score calibration module designed to mitigate candidate\nprior bias in candidate likelihood. On four Text-Video Retrieval benchmarks,\nour BLiM equipped with CPN outperforms previous state-of-the-art models by 6.4\nR@1 on average, effectively alleviating candidate prior bias and emphasizing\nquery-candidate relevance. Our in-depth analysis across various multi-modal\ntasks beyond retrieval highlights the broad applicability of CPN which enhances\nvisual understanding by reducing reliance on textual priors. Code is available\nat https://github.com/mlvlab/BLiM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23284.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62bab5a69f045fd1fb9f760e",
            "avatarUrl": "/avatars/3f12a908faec1135e0221444f0769496.svg",
            "fullname": "Dohwan Ko",
            "name": "ikodoh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.02455",
            "authors": [
                {
                    "_id": "689210ad1e348c42dfd27f8d",
                    "user": {
                        "_id": "6554e2dcfcb10ad33d88dcea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6554e2dcfcb10ad33d88dcea/V4WnB358ciZ0Ry3APDF57.png",
                        "isPro": false,
                        "fullname": "Daniele Cipollone",
                        "user": "DanCip",
                        "type": "user"
                    },
                    "name": "Daniele Cipollone",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:19:47.710Z",
                    "hidden": false
                },
                {
                    "_id": "689210ad1e348c42dfd27f8e",
                    "name": "Egor Bogomolov",
                    "hidden": false
                },
                {
                    "_id": "689210ad1e348c42dfd27f8f",
                    "name": "Arie van Deursen",
                    "hidden": false
                },
                {
                    "_id": "689210ad1e348c42dfd27f90",
                    "name": "Maliheh Izadi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6554e2dcfcb10ad33d88dcea/2biCRyH6saZWbVcGs-F1y.jpeg"
            ],
            "publishedAt": "2025-08-04T14:20:39.000Z",
            "submittedOnDailyAt": "2025-08-06T17:59:09.647Z",
            "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs",
            "submittedOnDailyBy": {
                "_id": "6554e2dcfcb10ad33d88dcea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6554e2dcfcb10ad33d88dcea/V4WnB358ciZ0Ry3APDF57.png",
                "isPro": false,
                "fullname": "Daniele Cipollone",
                "user": "DanCip",
                "type": "user"
            },
            "summary": "Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance.",
            "upvotes": 1,
            "discussionId": "689210ae1e348c42dfd27f91",
            "ai_summary": "A new scoring approach using language models ranks static code completions in IDEs by organizing them into a prefix tree and performing a single greedy decoding pass.",
            "ai_keywords": [
                "token-level code completion",
                "Integrated Development Environments (IDEs)",
                "static analysis",
                "language models",
                "prefix tree",
                "greedy decoding",
                "beam search",
                "prompt engineering",
                "model adaptations"
            ]
        },
        "publishedAt": "2025-08-04T10:20:39.000Z",
        "title": "TreeRanker: Fast and Model-agnostic Ranking System for Code Suggestions\n  in IDEs",
        "summary": "Token-level code completion is one of the most critical features in modern\nIntegrated Development Environments (IDEs). It assists developers by suggesting\nrelevant identifiers and APIs during coding. While completions are typically\nderived from static analysis, their usefulness depends heavily on how they are\nranked, as correct predictions buried deep in the list are rarely seen by\nusers. Most current systems rely on hand-crafted heuristics or lightweight\nmachine learning models trained on user logs, which can be further improved to\ncapture context information and generalize across projects and coding styles.\nIn this work, we propose a new scoring approach to ranking static completions\nusing language models in a lightweight and model-agnostic way. Our method\norganizes all valid completions into a prefix tree and performs a single greedy\ndecoding pass to collect token-level scores across the tree. This enables a\nprecise token-aware ranking without needing beam search, prompt engineering, or\nmodel adaptations. The approach is fast, architecture-agnostic, and compatible\nwith already deployed models for code completion. These findings highlight a\npractical and effective pathway for integrating language models into already\nexisting tools within IDEs, and ultimately providing smarter and more\nresponsive developer assistance.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6554e2dcfcb10ad33d88dcea/2biCRyH6saZWbVcGs-F1y.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02455.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6554e2dcfcb10ad33d88dcea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6554e2dcfcb10ad33d88dcea/V4WnB358ciZ0Ry3APDF57.png",
            "fullname": "Daniele Cipollone",
            "name": "DanCip",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.02063",
            "authors": [
                {
                    "_id": "6892c3aa8da45ffb0a2b242c",
                    "name": "Amitava Das",
                    "hidden": false
                },
                {
                    "_id": "6892c3aa8da45ffb0a2b242d",
                    "name": "Vinija Jain",
                    "hidden": false
                },
                {
                    "_id": "6892c3aa8da45ffb0a2b242e",
                    "user": {
                        "_id": "63a4754927f1f64ed7238dac",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                        "isPro": false,
                        "fullname": "Aman Chadha",
                        "user": "amanchadha",
                        "type": "user"
                    },
                    "name": "Aman Chadha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:17:14.301Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T05:03:35.000Z",
            "submittedOnDailyAt": "2025-08-06T01:23:46.765Z",
            "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs",
            "submittedOnDailyBy": {
                "_id": "63a4754927f1f64ed7238dac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                "isPro": false,
                "fullname": "Aman Chadha",
                "user": "amanchadha",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) fine-tuned to align with human values often\nexhibit alignment drift, producing unsafe or policy-violating completions when\nexposed to adversarial prompts, decoding perturbations, or paraphrased\njailbreaks. While prior work has behaviorally characterized alignment failure,\nlittle is known about the training-time belief sources underlying these\nfailures. We introduce TraceAlign, a unified framework for tracing unsafe\ncompletions back to their root causes in the model's training corpus. Central\nto our approach is the Belief Conflict Index (BCI), which quantifies semantic\ninconsistency between generated spans and aligned policies, based on retrieved\ntraining documents using suffix-array matching. We propose three complementary\ninterventions: (i) TraceShield, an inference-time safety filter that refuses\ncompletions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a\ncontrastive fine-tuning objective penalizing high-BCI continuations during DPO,\nand (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam\nexpansions predicted to yield high-BCI spans. Together, these defenses reduce\nalignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB)\nwhile preserving utility on standard tasks, with delta less than 0.2 and\nimproved refusal quality. We further derive a theoretical upper bound on drift\nlikelihood via suffix-array span statistics, linking memorization frequency and\nlength to adversarial reactivation risk. TraceAlign thus provides the first\nscalable, traceable, and grounded toolkit for understanding and mitigating\nalignment failures at source. To encourage further exploration and development,\nwe open-source our implementation at:\nhttps://anonymous.4open.science/r/tracealign-2DA7",
            "upvotes": 1,
            "discussionId": "6892c3ab8da45ffb0a2b242f",
            "ai_summary": "TraceAlign is a framework that identifies and mitigates alignment drift in LLMs by tracing unsafe completions to their training sources and applying interventions to reduce drift while maintaining utility.",
            "ai_keywords": [
                "Large Language Models",
                "alignment drift",
                "adversarial prompts",
                "decoding perturbations",
                "paraphrased jailbreaks",
                "TraceAlign",
                "Belief Conflict Index",
                "suffix-array matching",
                "TraceShield",
                "Contrastive Belief Deconfliction Loss",
                "Prov-Decode",
                "Alignment Drift Benchmark",
                "memorization frequency",
                "adversarial reactivation risk"
            ]
        },
        "publishedAt": "2025-08-04T01:03:35.000Z",
        "title": "TRACEALIGN -- Tracing the Drift: Attributing Alignment Failures to\n  Training-Time Belief Sources in LLMs",
        "summary": "Large Language Models (LLMs) fine-tuned to align with human values often\nexhibit alignment drift, producing unsafe or policy-violating completions when\nexposed to adversarial prompts, decoding perturbations, or paraphrased\njailbreaks. While prior work has behaviorally characterized alignment failure,\nlittle is known about the training-time belief sources underlying these\nfailures. We introduce TraceAlign, a unified framework for tracing unsafe\ncompletions back to their root causes in the model's training corpus. Central\nto our approach is the Belief Conflict Index (BCI), which quantifies semantic\ninconsistency between generated spans and aligned policies, based on retrieved\ntraining documents using suffix-array matching. We propose three complementary\ninterventions: (i) TraceShield, an inference-time safety filter that refuses\ncompletions with high-BCI spans, (ii) Contrastive Belief Deconfliction Loss, a\ncontrastive fine-tuning objective penalizing high-BCI continuations during DPO,\nand (iii) Prov-Decode, a provenance-aware decoding strategy that vetoes beam\nexpansions predicted to yield high-BCI spans. Together, these defenses reduce\nalignment drift by up to 85% on our curated Alignment Drift Benchmark (ADB)\nwhile preserving utility on standard tasks, with delta less than 0.2 and\nimproved refusal quality. We further derive a theoretical upper bound on drift\nlikelihood via suffix-array span statistics, linking memorization frequency and\nlength to adversarial reactivation risk. TraceAlign thus provides the first\nscalable, traceable, and grounded toolkit for understanding and mitigating\nalignment failures at source. To encourage further exploration and development,\nwe open-source our implementation at:\nhttps://anonymous.4open.science/r/tracealign-2DA7",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02063.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03793",
            "authors": [
                {
                    "_id": "6893fe74741a16f544fbce29",
                    "name": "Yanting Wang",
                    "hidden": false
                },
                {
                    "_id": "6893fe74741a16f544fbce2a",
                    "name": "Runpeng Geng",
                    "hidden": false
                },
                {
                    "_id": "6893fe74741a16f544fbce2b",
                    "name": "Ying Chen",
                    "hidden": false
                },
                {
                    "_id": "6893fe74741a16f544fbce2c",
                    "name": "Jinyuan Jia",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T17:56:51.000Z",
            "submittedOnDailyAt": "2025-08-06T23:48:01.295Z",
            "title": "AttnTrace: Attention-based Context Traceback for Long-Context LLMs",
            "submittedOnDailyBy": {
                "_id": "656f7bf329b40892e83819c4",
                "avatarUrl": "/avatars/4dace5c2516fb89947b509596e617f13.svg",
                "isPro": true,
                "fullname": "yanting wang",
                "user": "SecureLLMSys",
                "type": "user"
            },
            "summary": "Long-context large language models (LLMs), such as Gemini-2.5-Pro and\nClaude-Sonnet-4, are increasingly used to empower advanced AI systems,\nincluding retrieval-augmented generation (RAG) pipelines and autonomous agents.\nIn these systems, an LLM receives an instruction along with a context--often\nconsisting of texts retrieved from a knowledge database or memory--and\ngenerates a response that is contextually grounded by following the\ninstruction. Recent studies have designed solutions to trace back to a subset\nof texts in the context that contributes most to the response generated by the\nLLM. These solutions have numerous real-world applications, including\nperforming post-attack forensic analysis and improving the interpretability and\ntrustworthiness of LLM outputs. While significant efforts have been made,\nstate-of-the-art solutions such as TracLLM often lead to a high computation\ncost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a\nsingle response-context pair. In this work, we propose AttnTrace, a new context\ntraceback method based on the attention weights produced by an LLM for a\nprompt. To effectively utilize attention weights, we introduce two techniques\ndesigned to enhance the effectiveness of AttnTrace, and we provide theoretical\ninsights for our design choice. We also perform a systematic evaluation for\nAttnTrace. The results demonstrate that AttnTrace is more accurate and\nefficient than existing state-of-the-art context traceback methods. We also\nshow that AttnTrace can improve state-of-the-art methods in detecting prompt\ninjection under long contexts through the attribution-before-detection\nparadigm. As a real-world application, we demonstrate that AttnTrace can\neffectively pinpoint injected instructions in a paper designed to manipulate\nLLM-generated reviews. The code is at\nhttps://github.com/Wang-Yanting/AttnTrace.",
            "upvotes": 0,
            "discussionId": "6893fe75741a16f544fbce2d",
            "projectPage": "https://huggingface.co/spaces/SecureLLMSys/AttnTrace",
            "githubRepo": "https://github.com/Wang-Yanting/AttnTrace",
            "ai_summary": "AttnTrace, a new context traceback method using attention weights, improves accuracy and efficiency in detecting prompt injection in long-context large language models.",
            "ai_keywords": [
                "long-context large language models",
                "retrieval-augmented generation",
                "autonomous agents",
                "context traceback",
                "TracLLM",
                "attention weights",
                "prompt injection",
                "attribution-before-detection"
            ]
        },
        "publishedAt": "2025-08-05T13:56:51.000Z",
        "title": "AttnTrace: Attention-based Context Traceback for Long-Context LLMs",
        "summary": "Long-context large language models (LLMs), such as Gemini-2.5-Pro and\nClaude-Sonnet-4, are increasingly used to empower advanced AI systems,\nincluding retrieval-augmented generation (RAG) pipelines and autonomous agents.\nIn these systems, an LLM receives an instruction along with a context--often\nconsisting of texts retrieved from a knowledge database or memory--and\ngenerates a response that is contextually grounded by following the\ninstruction. Recent studies have designed solutions to trace back to a subset\nof texts in the context that contributes most to the response generated by the\nLLM. These solutions have numerous real-world applications, including\nperforming post-attack forensic analysis and improving the interpretability and\ntrustworthiness of LLM outputs. While significant efforts have been made,\nstate-of-the-art solutions such as TracLLM often lead to a high computation\ncost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a\nsingle response-context pair. In this work, we propose AttnTrace, a new context\ntraceback method based on the attention weights produced by an LLM for a\nprompt. To effectively utilize attention weights, we introduce two techniques\ndesigned to enhance the effectiveness of AttnTrace, and we provide theoretical\ninsights for our design choice. We also perform a systematic evaluation for\nAttnTrace. The results demonstrate that AttnTrace is more accurate and\nefficient than existing state-of-the-art context traceback methods. We also\nshow that AttnTrace can improve state-of-the-art methods in detecting prompt\ninjection under long contexts through the attribution-before-detection\nparadigm. As a real-world application, we demonstrate that AttnTrace can\neffectively pinpoint injected instructions in a paper designed to manipulate\nLLM-generated reviews. The code is at\nhttps://github.com/Wang-Yanting/AttnTrace.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03793.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "656f7bf329b40892e83819c4",
            "avatarUrl": "/avatars/4dace5c2516fb89947b509596e617f13.svg",
            "fullname": "yanting wang",
            "name": "SecureLLMSys",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01126",
            "authors": [
                {
                    "_id": "68938b44741a16f544fbcdaf",
                    "user": {
                        "_id": "66f10fa5f9cf1bf7e6798d15",
                        "avatarUrl": "/avatars/ff416c6778091eaf13f6c233a5e42244.svg",
                        "isPro": false,
                        "fullname": "Chaitanya Patel",
                        "user": "chaitanya100100",
                        "type": "user"
                    },
                    "name": "Chaitanya Patel",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:12:20.645Z",
                    "hidden": false
                },
                {
                    "_id": "68938b44741a16f544fbcdb0",
                    "name": "Hiroki Nakamura",
                    "hidden": false
                },
                {
                    "_id": "68938b44741a16f544fbcdb1",
                    "name": "Yuta Kyuragi",
                    "hidden": false
                },
                {
                    "_id": "68938b44741a16f544fbcdb2",
                    "name": "Kazuki Kozuka",
                    "hidden": false
                },
                {
                    "_id": "68938b44741a16f544fbcdb3",
                    "name": "Juan Carlos Niebles",
                    "hidden": false
                },
                {
                    "_id": "68938b44741a16f544fbcdb4",
                    "name": "Ehsan Adeli",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T00:41:20.000Z",
            "submittedOnDailyAt": "2025-08-06T15:38:12.067Z",
            "title": "UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,\n  Forecasting, and Generation",
            "submittedOnDailyBy": {
                "_id": "66f10fa5f9cf1bf7e6798d15",
                "avatarUrl": "/avatars/ff416c6778091eaf13f6c233a5e42244.svg",
                "isPro": false,
                "fullname": "Chaitanya Patel",
                "user": "chaitanya100100",
                "type": "user"
            },
            "summary": "Egocentric human motion generation and forecasting with scene-context is\ncrucial for enhancing AR/VR experiences, improving human-robot interaction,\nadvancing assistive technologies, and enabling adaptive healthcare solutions by\naccurately predicting and simulating movement from a first-person perspective.\nHowever, existing methods primarily focus on third-person motion synthesis with\nstructured 3D scene contexts, limiting their effectiveness in real-world\negocentric settings where limited field of view, frequent occlusions, and\ndynamic cameras hinder scene perception. To bridge this gap, we introduce\nEgocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks\nthat utilize first-person images for scene-aware motion synthesis without\nrelying on explicit 3D scene. We propose UniEgoMotion, a unified conditional\nmotion diffusion model with a novel head-centric motion representation tailored\nfor egocentric devices. UniEgoMotion's simple yet effective design supports\negocentric motion reconstruction, forecasting, and generation from first-person\nvisual inputs in a unified framework. Unlike previous works that overlook scene\nsemantics, our model effectively extracts image-based scene context to infer\nplausible 3D motion. To facilitate training, we introduce EE4D-Motion, a\nlarge-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth\n3D motion annotations. UniEgoMotion achieves state-of-the-art performance in\negocentric motion reconstruction and is the first to generate motion from a\nsingle egocentric image. Extensive evaluations demonstrate the effectiveness of\nour unified framework, setting a new benchmark for egocentric motion modeling\nand unlocking new possibilities for egocentric applications.",
            "upvotes": 0,
            "discussionId": "68938b44741a16f544fbcdb5",
            "projectPage": "https://chaitanya100100.github.io/UniEgoMotion/",
            "githubRepo": "https://github.com/chaitanya100100/UniEgoMotion",
            "ai_summary": "A unified conditional motion diffusion model, UniEgoMotion, is introduced for egocentric motion generation and forecasting using first-person images, achieving state-of-the-art performance and generating motion from a single image.",
            "ai_keywords": [
                "egocentric human motion generation",
                "egocentric motion forecasting",
                "scene-context",
                "AR/VR experiences",
                "human-robot interaction",
                "assistive technologies",
                "adaptive healthcare solutions",
                "first-person perspective",
                "third-person motion synthesis",
                "structured 3D scene contexts",
                "field of view",
                "occlusions",
                "dynamic cameras",
                "scene-aware motion synthesis",
                "head-centric motion representation",
                "egocentric devices",
                "unified framework",
                "image-based scene context",
                "plausible 3D motion",
                "EE4D-Motion",
                "EgoExo4D",
                "pseudo-ground-truth 3D motion annotations",
                "egocentric motion reconstruction",
                "unified conditional motion diffusion model"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-08-01T20:41:20.000Z",
        "title": "UniEgoMotion: A Unified Model for Egocentric Motion Reconstruction,\n  Forecasting, and Generation",
        "summary": "Egocentric human motion generation and forecasting with scene-context is\ncrucial for enhancing AR/VR experiences, improving human-robot interaction,\nadvancing assistive technologies, and enabling adaptive healthcare solutions by\naccurately predicting and simulating movement from a first-person perspective.\nHowever, existing methods primarily focus on third-person motion synthesis with\nstructured 3D scene contexts, limiting their effectiveness in real-world\negocentric settings where limited field of view, frequent occlusions, and\ndynamic cameras hinder scene perception. To bridge this gap, we introduce\nEgocentric Motion Generation and Egocentric Motion Forecasting, two novel tasks\nthat utilize first-person images for scene-aware motion synthesis without\nrelying on explicit 3D scene. We propose UniEgoMotion, a unified conditional\nmotion diffusion model with a novel head-centric motion representation tailored\nfor egocentric devices. UniEgoMotion's simple yet effective design supports\negocentric motion reconstruction, forecasting, and generation from first-person\nvisual inputs in a unified framework. Unlike previous works that overlook scene\nsemantics, our model effectively extracts image-based scene context to infer\nplausible 3D motion. To facilitate training, we introduce EE4D-Motion, a\nlarge-scale dataset derived from EgoExo4D, augmented with pseudo-ground-truth\n3D motion annotations. UniEgoMotion achieves state-of-the-art performance in\negocentric motion reconstruction and is the first to generate motion from a\nsingle egocentric image. Extensive evaluations demonstrate the effectiveness of\nour unified framework, setting a new benchmark for egocentric motion modeling\nand unlocking new possibilities for egocentric applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01126.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66f10fa5f9cf1bf7e6798d15",
            "avatarUrl": "/avatars/ff416c6778091eaf13f6c233a5e42244.svg",
            "fullname": "Chaitanya Patel",
            "name": "chaitanya100100",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]