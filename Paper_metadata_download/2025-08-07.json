[
    {
        "paper": {
            "id": "2508.01191",
            "authors": [
                {
                    "_id": "689414b0741a16f544fbcec8",
                    "user": {
                        "_id": "65b2fae679954e21ac426aec",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b2fae679954e21ac426aec/LybSb_awygRTQinm1npUq.jpeg",
                        "isPro": false,
                        "fullname": "Chengshuai Zhao",
                        "user": "chengshuaizhao",
                        "type": "user"
                    },
                    "name": "Chengshuai Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:44.551Z",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcec9",
                    "name": "Zhen Tan",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbceca",
                    "user": {
                        "_id": "68942d7d73b80b8311a584fa",
                        "avatarUrl": "/avatars/80114f28c5166ba79e4418a738960268.svg",
                        "isPro": false,
                        "fullname": "PingchuanMa",
                        "user": "ympc08",
                        "type": "user"
                    },
                    "name": "Pingchuan Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:47.428Z",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcecb",
                    "user": {
                        "_id": "6474e1afb68461d5cf7c41cc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6474e1afb68461d5cf7c41cc/bcoiD_qPrjHUBlB259djg.png",
                        "isPro": false,
                        "fullname": "Dawei Li",
                        "user": "wjldw",
                        "type": "user"
                    },
                    "name": "Dawei Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:54.238Z",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcecc",
                    "user": {
                        "_id": "61f087a0a57920a251ec1a6f",
                        "avatarUrl": "/avatars/4402b7986152bb37e02f1305c6bcce2e.svg",
                        "isPro": false,
                        "fullname": "Bohan Jiang",
                        "user": "Bohan-Jiang",
                        "type": "user"
                    },
                    "name": "Bohan Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:50.992Z",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcecd",
                    "name": "Yancheng Wang",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcece",
                    "name": "Yingzhen Yang",
                    "hidden": false
                },
                {
                    "_id": "689414b0741a16f544fbcecf",
                    "name": "Huan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T04:37:28.000Z",
            "submittedOnDailyAt": "2025-08-07T01:29:05.289Z",
            "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
            "submittedOnDailyBy": {
                "_id": "65b2fae679954e21ac426aec",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b2fae679954e21ac426aec/LybSb_awygRTQinm1npUq.jpeg",
                "isPro": false,
                "fullname": "Chengshuai Zhao",
                "user": "chengshuaizhao",
                "type": "user"
            },
            "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.",
            "upvotes": 142,
            "discussionId": "689414b0741a16f544fbced0",
            "githubRepo": "https://github.com/ChengshuaiZhao0/DataAlchemy",
            "ai_summary": "CoT reasoning in LLMs is found to be limited by the distribution discrepancy between training and test data, suggesting it is not a robust form of reasoning.",
            "ai_keywords": [
                "Chain-of-Thought",
                "Large Language Model",
                "CoT reasoning",
                "inductive bias",
                "DataAlchemy",
                "distribution discrepancy",
                "reasoning paths",
                "generalizable reasoning"
            ],
            "githubStars": 24
        },
        "publishedAt": "2025-08-02T00:37:28.000Z",
        "title": "Is Chain-of-Thought Reasoning of LLMs a Mirage? A Data Distribution Lens",
        "summary": "Chain-of-Thought (CoT) prompting has been shown to improve Large Language\nModel (LLM) performance on various tasks. With this approach, LLMs appear to\nproduce human-like reasoning steps before providing answers (a.k.a., CoT\nreasoning), which often leads to the perception that they engage in deliberate\ninferential processes. However, some initial findings suggest that CoT\nreasoning may be more superficial than it appears, motivating us to explore\nfurther. In this paper, we study CoT reasoning via a data distribution lens and\ninvestigate if CoT reasoning reflects a structured inductive bias learned from\nin-distribution data, allowing the model to conditionally generate reasoning\npaths that approximate those seen during training. Thus, its effectiveness is\nfundamentally bounded by the degree of distribution discrepancy between the\ntraining data and the test queries. With this lens, we dissect CoT reasoning\nvia three dimensions: task, length, and format. To investigate each dimension,\nwe design DataAlchemy, an isolated and controlled environment to train LLMs\nfrom scratch and systematically probe them under various distribution\nconditions. Our results reveal that CoT reasoning is a brittle mirage that\nvanishes when it is pushed beyond training distributions. This work offers a\ndeeper understanding of why and when CoT reasoning fails, emphasizing the\nongoing challenge of achieving genuine and generalizable reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01191.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "65b2fae679954e21ac426aec",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b2fae679954e21ac426aec/LybSb_awygRTQinm1npUq.jpeg",
            "fullname": "Chengshuai Zhao",
            "name": "chengshuaizhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04026",
            "authors": [
                {
                    "_id": "68941e5d741a16f544fbceed",
                    "name": "Shunyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbceee",
                    "user": {
                        "_id": "6417d9ea8f689506e7148417",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
                        "isPro": false,
                        "fullname": "minghao",
                        "user": "Liam-Liu",
                        "type": "user"
                    },
                    "name": "Minghao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:35.426Z",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbceef",
                    "name": "Huichi Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef0",
                    "name": "Zhenyu Cui",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef1",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef2",
                    "name": "Yuhao Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef3",
                    "name": "Wendong Fan",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef4",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef5",
                    "name": "Jiajun Shi",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef6",
                    "user": {
                        "_id": "65b8909c89eb3dfbe8d26780",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65b8909c89eb3dfbe8d26780/aJExXsCN9oQOdd9bsQOEO.jpeg",
                        "isPro": false,
                        "fullname": "Weihao XUAN",
                        "user": "weihao1115",
                        "type": "user"
                    },
                    "name": "Weihao Xuan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:31.782Z",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef7",
                    "name": "Jiaxing Huang",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef8",
                    "name": "Shuang Luo",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcef9",
                    "name": "Fang Wu",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefa",
                    "name": "Heli Qi",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefb",
                    "name": "Qingcheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefc",
                    "name": "Ziqi Ren",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefd",
                    "name": "Jialiang Gao",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcefe",
                    "name": "Jindi Lv",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbceff",
                    "name": "Junjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf00",
                    "name": "Aosong Feng",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf01",
                    "name": "Heng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf02",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf03",
                    "name": "Zhenfei Yin",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf04",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf05",
                    "name": "Guohao Li",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf06",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf07",
                    "name": "Irene Li",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf08",
                    "name": "Lei Ma",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf09",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf0a",
                    "name": "Qunshu Lin",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf0b",
                    "name": "Mingli Song",
                    "hidden": false
                },
                {
                    "_id": "68941e5d741a16f544fbcf0c",
                    "name": "Dacheng Tao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T02:38:18.000Z",
            "submittedOnDailyAt": "2025-08-07T02:06:19.798Z",
            "title": "VeriGUI: Verifiable Long-Chain GUI Dataset",
            "submittedOnDailyBy": {
                "_id": "6713afea187a20dc579e121b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg",
                "isPro": false,
                "fullname": "Shunyu Liu",
                "user": "liushunyu",
                "type": "user"
            },
            "summary": "Recent studies have delved into constructing autonomous agents capable of\nperforming complex Graphical User Interface (GUI)-based computer tasks, with\nthe potential to revolutionize human-computer interaction. Despite encouraging\nresults, existing efforts mainly focus on short-term interactions and rely on\noutcome-only verification, thereby limiting their scalability in real-world GUI\napplications that demand long-horizon task decomposition and execution. In this\nwork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed\nto facilitate the development and evaluation of generalist GUI agents operating\nin realistic computer environments. Our dataset emphasizes two critical\ndimensions: (1) long-chain complexity, with tasks decomposed into a sequence of\ninterdependent subtasks spanning hundreds of steps, explicitly designed to\nallow any subtask to serve as a valid starting point; and (2) subtask-level\nverifiability, which enables diverse exploration strategies within each\nsubtask, while ensuring that each subtask-level goal remains verifiable and\nconsistent. The dataset consists of GUI task trajectories across both desktop\nand web, annotated by human experts. Extensive experiments on VeriGUI using\nvarious agents with different foundation models reveal significant performance\ngaps in handling long-horizon tasks, highlighting the need for more robust\nplanning and decision-making capabilities in GUI agents.",
            "upvotes": 117,
            "discussionId": "68941e5e741a16f544fbcf0d",
            "githubRepo": "https://github.com/VeriGUI-Team/VeriGUI",
            "ai_summary": "VeriGUI is a novel dataset for evaluating GUI agents in long-horizon tasks, emphasizing long-chain complexity and subtask-level verifiability.",
            "ai_keywords": [
                "Graphical User Interface (GUI)",
                "GUI agents",
                "long-chain complexity",
                "subtask-level verifiability",
                "GUI task trajectories",
                "desktop",
                "web",
                "human experts",
                "long-horizon tasks",
                "robust planning",
                "decision-making capabilities"
            ],
            "githubStars": 58
        },
        "publishedAt": "2025-08-05T22:38:18.000Z",
        "title": "VeriGUI: Verifiable Long-Chain GUI Dataset",
        "summary": "Recent studies have delved into constructing autonomous agents capable of\nperforming complex Graphical User Interface (GUI)-based computer tasks, with\nthe potential to revolutionize human-computer interaction. Despite encouraging\nresults, existing efforts mainly focus on short-term interactions and rely on\noutcome-only verification, thereby limiting their scalability in real-world GUI\napplications that demand long-horizon task decomposition and execution. In this\nwork, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed\nto facilitate the development and evaluation of generalist GUI agents operating\nin realistic computer environments. Our dataset emphasizes two critical\ndimensions: (1) long-chain complexity, with tasks decomposed into a sequence of\ninterdependent subtasks spanning hundreds of steps, explicitly designed to\nallow any subtask to serve as a valid starting point; and (2) subtask-level\nverifiability, which enables diverse exploration strategies within each\nsubtask, while ensuring that each subtask-level goal remains verifiable and\nconsistent. The dataset consists of GUI task trajectories across both desktop\nand web, annotated by human experts. Extensive experiments on VeriGUI using\nvarious agents with different foundation models reveal significant performance\ngaps in handling long-horizon tasks, highlighting the need for more robust\nplanning and decision-making capabilities in GUI agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04026.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6713afea187a20dc579e121b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6713afea187a20dc579e121b/ELxVQLVF9ifuT-TCWPK22.jpeg",
            "fullname": "Shunyu Liu",
            "name": "liushunyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.02694",
            "authors": [
                {
                    "_id": "689385f0741a16f544fbcd8c",
                    "name": "Ningning Wang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd8d",
                    "name": "Xavier Hu",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd8e",
                    "name": "Pai Liu",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd8f",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd90",
                    "name": "Yue Hou",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd91",
                    "name": "Heyuan Huang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd92",
                    "name": "Shengyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd93",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd94",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd95",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd96",
                    "name": "Changwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd97",
                    "name": "Jun Wang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd98",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "689385f0741a16f544fbcd99",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-24T17:56:51.000Z",
            "submittedOnDailyAt": "2025-08-07T03:39:00.617Z",
            "title": "Efficient Agents: Building Effective Agents While Reducing Cost",
            "submittedOnDailyBy": {
                "_id": "6892b4459f604ad07412f117",
                "avatarUrl": "/avatars/fb9e54e1689b4add084e32d7e5dd2f16.svg",
                "isPro": false,
                "fullname": "Xavier Hu",
                "user": "xavier-hu",
                "type": "user"
            },
            "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from 0.398 to 0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.",
            "upvotes": 51,
            "discussionId": "689385f0741a16f544fbcd9a",
            "githubRepo": "https://github.com/OPPO-PersonalAI/OAgents",
            "ai_summary": "A study on the efficiency-effectiveness trade-off in LLM-driven agent systems identifies optimal agent framework design to reduce costs while maintaining performance.",
            "ai_keywords": [
                "Large Language Model",
                "LLM",
                "agent systems",
                "efficiency-effectiveness trade-off",
                "agentic tasks",
                "agent framework",
                "GAIA benchmark",
                "LLM backbone",
                "test-time scaling strategies",
                "cost-of-pass",
                "Efficient Agents",
                "OWL"
            ],
            "githubStars": 125
        },
        "publishedAt": "2025-07-24T13:56:51.000Z",
        "title": "Efficient Agents: Building Effective Agents While Reducing Cost",
        "summary": "The remarkable capabilities of Large Language Model (LLM)-driven agents have\nenabled sophisticated systems to tackle complex, multi-step tasks, but their\nescalating costs threaten scalability and accessibility. This work presents the\nfirst systematic study of the efficiency-effectiveness trade-off in modern\nagent systems, addressing the critical need for cost-effective designs without\nsacrificing performance. We investigate three key questions: (1) How much\ncomplexity do agentic tasks inherently require? (2) When do additional modules\nyield diminishing returns? (3) How much efficiency can be gained through the\ndesign of efficient agent frameworks? Through an empirical analysis on the GAIA\nbenchmark, we evaluate the impact of LLM backbone selection, agent framework\ndesigns, and test-time scaling strategies. Using the cost-of-pass metric, we\nquantify the efficiency-performance trade-off across these dimensions. Our\nfindings inform the development of Efficient Agents , a novel agent framework\nthat has an optimal complexity to task requirements. Efficient Agents retains\n96.7% of the performance of OWL, one leading open-source agent framework, while\nreducing operational costs from 0.398 to 0.228, resulting in a 28.4%\nimprovement in cost-of-pass. Our work provides actionable insights for\ndesigning efficient, high-performing agent systems, advancing the accessibility\nand sustainability of AI-driven solutions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02694.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6892b4459f604ad07412f117",
            "avatarUrl": "/avatars/fb9e54e1689b4add084e32d7e5dd2f16.svg",
            "fullname": "Xavier Hu",
            "name": "xavier-hu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.04700",
            "authors": [
                {
                    "_id": "68941f54741a16f544fbcf0f",
                    "user": {
                        "_id": "63fda3fced9eead590ff6918",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Zeyi Sun",
                        "user": "Zery",
                        "type": "user"
                    },
                    "name": "Zeyi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:28.544Z",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf10",
                    "name": "Ziyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf11",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:25.677Z",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf12",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf13",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf14",
                    "name": "Tong Wu",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf15",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "68941f54741a16f544fbcf16",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T17:58:46.000Z",
            "submittedOnDailyAt": "2025-08-07T02:09:08.225Z",
            "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience",
            "submittedOnDailyBy": {
                "_id": "63fda3fced9eead590ff6918",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
                "isPro": false,
                "fullname": "Zeyi Sun",
                "user": "Zery",
                "type": "user"
            },
            "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
            "upvotes": 37,
            "discussionId": "68941f54741a16f544fbcf17",
            "projectPage": "https://github.com/SunzeY/SEAgent",
            "githubRepo": "https://github.com/SunzeY/SEAgent",
            "ai_summary": "SEAgent, an agentic self-evolving framework, enables computer-use agents to autonomously master novel software through experiential learning and a curriculum of tasks, achieving superior performance compared to existing methods.",
            "ai_keywords": [
                "vision-language models",
                "computer use agents",
                "SEAgent",
                "experiential learning",
                "World State Model",
                "Curriculum Generator",
                "adversarial imitation",
                "Group Relative Policy Optimization",
                "specialist-to-generalist training",
                "OS-World",
                "UI-TARS"
            ],
            "githubStars": 55
        },
        "publishedAt": "2025-08-06T13:58:46.000Z",
        "title": "SEAgent: Self-Evolving Computer Use Agent with Autonomous Learning from\n  Experience",
        "summary": "Repurposing large vision-language models (LVLMs) as computer use agents\n(CUAs) has led to substantial breakthroughs, primarily driven by human-labeled\ndata. However, these models often struggle with novel and specialized software,\nparticularly in scenarios lacking human annotations. To address this challenge,\nwe propose SEAgent, an agentic self-evolving framework enabling CUAs to\nautonomously evolve through interactions with unfamiliar software.\nSpecifically, SEAgent empowers computer-use agents to autonomously master novel\nsoftware environments via experiential learning, where agents explore new\nsoftware, learn through iterative trial-and-error, and progressively tackle\nauto-generated tasks organized from simple to complex. To achieve this goal, we\ndesign a World State Model for step-wise trajectory assessment, along with a\nCurriculum Generator that generates increasingly diverse and challenging tasks.\nThe agent's policy is updated through experiential learning, comprised of\nadversarial imitation of failure actions and Group Relative Policy Optimization\n(GRPO) on successful ones. Furthermore, we introduce a specialist-to-generalist\ntraining strategy that integrates individual experiential insights from\nspecialist agents, facilitating the development of a stronger generalist CUA\ncapable of continuous autonomous evolution. This unified agent ultimately\nachieves performance surpassing ensembles of individual specialist agents on\ntheir specialized software. We validate the effectiveness of SEAgent across\nfive novel software environments within OS-World. Our approach achieves a\nsignificant improvement of 23.2% in success rate, from 11.3% to 34.5%, over a\ncompetitive open-source CUA, i.e., UI-TARS.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04700.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fda3fced9eead590ff6918",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
            "fullname": "Zeyi Sun",
            "name": "Zery",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03501",
            "authors": [
                {
                    "_id": "68946454741a16f544fbd048",
                    "name": "Alexander Golubev",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd049",
                    "name": "Maria Trofimova",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd04a",
                    "name": "Sergei Polezhaev",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd04b",
                    "name": "Ibragim Badertdinov",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd04c",
                    "name": "Maksim Nekrashevich",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd04d",
                    "name": "Anton Shevtsov",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd04e",
                    "name": "Simon Karasik",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd04f",
                    "name": "Sergey Abramov",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd050",
                    "name": "Andrei Andriushchenko",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd051",
                    "name": "Filipp Fisin",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd052",
                    "name": "Sergei Skvortsov",
                    "hidden": false
                },
                {
                    "_id": "68946454741a16f544fbd053",
                    "name": "Boris Yangel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T14:30:47.000Z",
            "submittedOnDailyAt": "2025-08-07T09:19:34.591Z",
            "title": "Training Long-Context, Multi-Turn Software Engineering Agents with\n  Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "65e48cb3a4e46e644ec1277d",
                "avatarUrl": "/avatars/dd2bf04a6f81bf0a0892080af5d485b2.svg",
                "isPro": false,
                "fullname": "Simon Karasik",
                "user": "sbkarasik",
                "type": "user"
            },
            "summary": "Research on applications of Reinforcement Learning (RL) to Large Language\nModels (LLMs) has mostly been focused on single-turn problems, such as\nmathematical reasoning or single-shot code generation. While these problems can\nbe viewed as token-level multi-turn MDPs, this view corresponds to a degenerate\ncase of multi-turn interaction where the environment provides no feedback. This\ncontrasts with many real-world domains, such as software engineering (SWE),\nwhich require rich multi-turn interactions with a stateful environment that\nresponds to each action with a non-trivial observation.\n  To bridge this gap, we demonstrate the successful application of RL to this\ngeneral regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)\nalgorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world\nsoftware engineering tasks. Our approach increases the agent's success rate on\nthe SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to\n39%, without relying on any teacher models. On SWE-rebench, our agent matches\nor outperforms leading open-weight models such as DeepSeek-V3-0324 and\nQwen3-235B-A22B using an identical scaffolding, offering a viable path toward\nbuilding more capable autonomous agents for complex real-world problems based\non open models.",
            "upvotes": 30,
            "discussionId": "68946455741a16f544fbd054"
        },
        "publishedAt": "2025-08-05T10:30:47.000Z",
        "title": "Training Long-Context, Multi-Turn Software Engineering Agents with\n  Reinforcement Learning",
        "summary": "Research on applications of Reinforcement Learning (RL) to Large Language\nModels (LLMs) has mostly been focused on single-turn problems, such as\nmathematical reasoning or single-shot code generation. While these problems can\nbe viewed as token-level multi-turn MDPs, this view corresponds to a degenerate\ncase of multi-turn interaction where the environment provides no feedback. This\ncontrasts with many real-world domains, such as software engineering (SWE),\nwhich require rich multi-turn interactions with a stateful environment that\nresponds to each action with a non-trivial observation.\n  To bridge this gap, we demonstrate the successful application of RL to this\ngeneral regime. Using a modified Decoupled Advantage Policy Optimization (DAPO)\nalgorithm, we train an agent based on Qwen2.5-72B-Instruct to solve real-world\nsoftware engineering tasks. Our approach increases the agent's success rate on\nthe SWE-bench Verified benchmark from a 20% rejection fine-tuned baseline to\n39%, without relying on any teacher models. On SWE-rebench, our agent matches\nor outperforms leading open-weight models such as DeepSeek-V3-0324 and\nQwen3-235B-A22B using an identical scaffolding, offering a viable path toward\nbuilding more capable autonomous agents for complex real-world problems based\non open models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03501.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "65e48cb3a4e46e644ec1277d",
            "avatarUrl": "/avatars/dd2bf04a6f81bf0a0892080af5d485b2.svg",
            "fullname": "Simon Karasik",
            "name": "sbkarasik",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.04280",
            "authors": [
                {
                    "_id": "68947269741a16f544fbd068",
                    "user": {
                        "_id": "6348202122bc15d2636ccf87",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1673510811337-6348202122bc15d2636ccf87.jpeg",
                        "isPro": true,
                        "fullname": "Natyren",
                        "user": "GeorgeBredis",
                        "type": "user"
                    },
                    "name": "George Bredis",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:37:32.805Z",
                    "hidden": false
                },
                {
                    "_id": "68947269741a16f544fbd069",
                    "name": "Stanislav Dereka",
                    "hidden": false
                },
                {
                    "_id": "68947269741a16f544fbd06a",
                    "name": "Viacheslav Sinii",
                    "hidden": false
                },
                {
                    "_id": "68947269741a16f544fbd06b",
                    "name": "Ruslan Rakhimov",
                    "hidden": false
                },
                {
                    "_id": "68947269741a16f544fbd06c",
                    "user": {
                        "_id": "62a9c8edc19f92ae443ab37f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
                        "isPro": false,
                        "fullname": "Daniil Gavrilov",
                        "user": "kefirski",
                        "type": "user"
                    },
                    "name": "Daniil Gavrilov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:37:30.344Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T10:08:48.000Z",
            "submittedOnDailyAt": "2025-08-07T08:11:29.844Z",
            "title": "Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success",
            "submittedOnDailyBy": {
                "_id": "62a9c8edc19f92ae443ab37f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
                "isPro": false,
                "fullname": "Daniil Gavrilov",
                "user": "kefirski",
                "type": "user"
            },
            "summary": "Interactive multimodal agents must convert raw visual observations into\ncoherent sequences of language-conditioned actions -- a capability that current\nvision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)\nefforts could, in principle, endow VLMs with such skills, but they have seldom\ntested whether the learned behaviours generalize beyond their training\nsimulators, and they depend either on brittle hyperparameter tuning or on\ndense-reward environments with low state variability. We introduce\nVision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,\nhyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens\nwhile learning value only at the environment-step level: an arrangement, to our\nknowledge, not previously explored for large VLMs or LLMs. This simple\ndecoupling removes unstable weighting terms and yields faster, more reliable\nconvergence. Training a single VLM with VL-DAC in one inexpensive simulator at\na time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies\nthat generalize widely: +50\\% relative on BALROG (game-centric agentic\ncontrol), +5\\% relative on the hardest part of VSI-Bench (spatial planning),\nand +2\\% on VisualWebBench (web navigation), all without degrading general\nimage understanding accuracy. These results provide the first evidence that a\nsimple RL algorithm can train VLMs entirely in cheap synthetic worlds while\ndelivering measurable gains on real-image agentic, spatial-reasoning, and\nweb-navigation benchmarks.",
            "upvotes": 28,
            "discussionId": "6894726a741a16f544fbd06d",
            "githubRepo": "https://github.com/corl-team/VL-DAC",
            "ai_summary": "A lightweight, hyperparameter-free RL algorithm, VL-DAC, enables VLMs to learn generalized policies from inexpensive simulators, improving performance on real-world benchmarks without sacrificing image understanding accuracy.",
            "ai_keywords": [
                "Vision-Language Decoupled Actor-Critic (VL-DAC)",
                "PPO updates",
                "action tokens",
                "value learning",
                "MiniWorld",
                "Gym-Cards",
                "ALFWorld",
                "WebShop",
                "BALROG",
                "VSI-Bench",
                "VisualWebBench",
                "general image understanding accuracy"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-08-06T06:08:48.000Z",
        "title": "Enhancing Vision-Language Model Training with Reinforcement Learning in\n  Synthetic Worlds for Real-World Success",
        "summary": "Interactive multimodal agents must convert raw visual observations into\ncoherent sequences of language-conditioned actions -- a capability that current\nvision-language models (VLMs) still lack. Earlier reinforcement-learning (RL)\nefforts could, in principle, endow VLMs with such skills, but they have seldom\ntested whether the learned behaviours generalize beyond their training\nsimulators, and they depend either on brittle hyperparameter tuning or on\ndense-reward environments with low state variability. We introduce\nVision-Language Decoupled Actor-Critic (VL-DAC), a lightweight,\nhyperparameter-free RL algorithm. VL-DAC applies PPO updates to action tokens\nwhile learning value only at the environment-step level: an arrangement, to our\nknowledge, not previously explored for large VLMs or LLMs. This simple\ndecoupling removes unstable weighting terms and yields faster, more reliable\nconvergence. Training a single VLM with VL-DAC in one inexpensive simulator at\na time (MiniWorld, Gym-Cards, ALFWorld, or WebShop) already produces policies\nthat generalize widely: +50\\% relative on BALROG (game-centric agentic\ncontrol), +5\\% relative on the hardest part of VSI-Bench (spatial planning),\nand +2\\% on VisualWebBench (web navigation), all without degrading general\nimage understanding accuracy. These results provide the first evidence that a\nsimple RL algorithm can train VLMs entirely in cheap synthetic worlds while\ndelivering measurable gains on real-image agentic, spatial-reasoning, and\nweb-navigation benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04280.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62a9c8edc19f92ae443ab37f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669110208492-62a9c8edc19f92ae443ab37f.png",
            "fullname": "Daniil Gavrilov",
            "name": "kefirski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 13
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03680",
            "authors": [
                {
                    "_id": "68940ca8741a16f544fbce6b",
                    "name": "Xufang Luo",
                    "hidden": false
                },
                {
                    "_id": "68940ca8741a16f544fbce6c",
                    "user": {
                        "_id": "6466d323ac657f60661d2778",
                        "avatarUrl": "/avatars/62f70630cdf1c252b80b4d5eaa5a4150.svg",
                        "isPro": false,
                        "fullname": "Yuge Zhang",
                        "user": "ultmaster",
                        "type": "user"
                    },
                    "name": "Yuge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:39:35.704Z",
                    "hidden": false
                },
                {
                    "_id": "68940ca8741a16f544fbce6d",
                    "user": {
                        "_id": "6455f5cababbbbd3486d6ee3",
                        "avatarUrl": "/avatars/b6c8f65fd2bef8a00aa3269856ea238e.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan He",
                        "user": "hzy46",
                        "type": "user"
                    },
                    "name": "Zhiyuan He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:39:32.667Z",
                    "hidden": false
                },
                {
                    "_id": "68940ca8741a16f544fbce6e",
                    "name": "Zilong Wang",
                    "hidden": false
                },
                {
                    "_id": "68940ca8741a16f544fbce6f",
                    "user": {
                        "_id": "64c20f8264e3e59137d88742",
                        "avatarUrl": "/avatars/8d3f6f2cbce053969866388ce75c602f.svg",
                        "isPro": false,
                        "fullname": "Siyun",
                        "user": "SiyunZhao",
                        "type": "user"
                    },
                    "name": "Siyun Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:39:29.935Z",
                    "hidden": false
                },
                {
                    "_id": "68940ca8741a16f544fbce70",
                    "name": "Dongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "68940ca8741a16f544fbce71",
                    "name": "Luna K. Qiu",
                    "hidden": false
                },
                {
                    "_id": "68940ca8741a16f544fbce72",
                    "name": "Yuqing Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T17:50:13.000Z",
            "submittedOnDailyAt": "2025-08-07T02:30:53.807Z",
            "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "66a1f912345b3106f47ce860",
                "avatarUrl": "/avatars/40177299c64e16703e7bfe83de0810be.svg",
                "isPro": false,
                "fullname": "Xufang Luo",
                "user": "daixufang",
                "type": "user"
            },
            "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.",
            "upvotes": 21,
            "discussionId": "68940ca8741a16f544fbce73",
            "projectPage": "https://www.microsoft.com/en-us/research/project/agent-lightning/",
            "githubRepo": "https://github.com/microsoft/agent-lightning",
            "ai_summary": "Agent Lightning is a flexible RL framework for training LLMs in various agents, using a hierarchical RL algorithm and decoupling execution from training to handle complex interactions.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Large Language Models",
                "Markov decision process",
                "hierarchical RL algorithm",
                "credit assignment module",
                "Training-Agent Disaggregation architecture",
                "agent observability frameworks",
                "text-to-SQL",
                "retrieval-augmented generation",
                "math tool-use tasks"
            ],
            "githubStars": 173
        },
        "publishedAt": "2025-08-05T13:50:13.000Z",
        "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
        "summary": "We present Agent Lightning, a flexible and extensible framework that enables\nReinforcement Learning (RL)-based training of Large Language Models (LLMs) for\nany AI agent. Unlike existing methods that tightly couple RL training with\nagent or rely on sequence concatenation with masking, Agent Lightning achieves\ncomplete decoupling between agent execution and training, allowing seamless\nintegration with existing agents developed via diverse ways (e.g., using\nframeworks like LangChain, OpenAI Agents SDK, AutoGen, and building from\nscratch) with almost ZERO code modifications. By formulating agent execution as\nMarkov decision process, we define an unified data interface and propose a\nhierarchical RL algorithm, LightningRL, which contains a credit assignment\nmodule, allowing us to decompose trajectories generated by ANY agents into\ntraining transition. This enables RL to handle complex interaction logic, such\nas multi-agent scenarios and dynamic workflows. For the system design, we\nintroduce a Training-Agent Disaggregation architecture, and brings agent\nobservability frameworks into agent runtime, providing a standardized agent\nfinetuning interface. Experiments across text-to-SQL, retrieval-augmented\ngeneration, and math tool-use tasks demonstrate stable, continuous\nimprovements, showcasing the framework's potential for real-world agent\ntraining and deployment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03680.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a1f912345b3106f47ce860",
            "avatarUrl": "/avatars/40177299c64e16703e7bfe83de0810be.svg",
            "fullname": "Xufang Luo",
            "name": "daixufang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.03159",
            "authors": [
                {
                    "_id": "68942f2f741a16f544fbcf54",
                    "name": "Jueon Park",
                    "hidden": false
                },
                {
                    "_id": "68942f2f741a16f544fbcf55",
                    "user": {
                        "_id": "64e5c8e594aa0690321f6b29",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png",
                        "isPro": false,
                        "fullname": "Yein Park",
                        "user": "P-YI",
                        "type": "user"
                    },
                    "name": "Yein Park",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:10.025Z",
                    "hidden": false
                },
                {
                    "_id": "68942f2f741a16f544fbcf56",
                    "name": "Minju Song",
                    "hidden": false
                },
                {
                    "_id": "68942f2f741a16f544fbcf57",
                    "name": "Soyon Park",
                    "hidden": false
                },
                {
                    "_id": "68942f2f741a16f544fbcf58",
                    "name": "Donghyeon Lee",
                    "hidden": false
                },
                {
                    "_id": "68942f2f741a16f544fbcf59",
                    "name": "Seungheun Baek",
                    "hidden": false
                },
                {
                    "_id": "68942f2f741a16f544fbcf5a",
                    "name": "Jaewoo Kang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64e5c8e594aa0690321f6b29/_CtEK1w-EMh2t8mwZJWhe.jpeg"
            ],
            "publishedAt": "2025-08-05T07:04:44.000Z",
            "submittedOnDailyAt": "2025-08-07T03:18:00.323Z",
            "title": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction",
            "submittedOnDailyBy": {
                "_id": "64e5c8e594aa0690321f6b29",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png",
                "isPro": false,
                "fullname": "Yein Park",
                "user": "P-YI",
                "type": "user"
            },
            "summary": "Drug toxicity remains a major challenge in pharmaceutical development. Recent\nmachine learning models have improved in silico toxicity prediction, but their\nreliance on annotated data and lack of interpretability limit their\napplicability. This limits their ability to capture organ-specific toxicities\ndriven by complex biological mechanisms. Large language models (LLMs) offer a\npromising alternative through step-by-step reasoning and integration of textual\ndata, yet prior approaches lack biological context and transparent rationale.\nTo address this issue, we propose CoTox, a novel framework that integrates LLM\nwith chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox\ncombines chemical structure data, biological pathways, and gene ontology (GO)\nterms to generate interpretable toxicity predictions through step-by-step\nreasoning. Using GPT-4o, we show that CoTox outperforms both traditional\nmachine learning and deep learning model. We further examine its performance\nacross various LLMs to identify where CoTox is most effective. Additionally, we\nfind that representing chemical structures with IUPAC names, which are easier\nfor LLMs to understand than SMILES, enhances the model's reasoning ability and\nimproves predictive performance. To demonstrate its practical utility in drug\ndevelopment, we simulate the treatment of relevant cell types with drug and\nincorporated the resulting biological context into the CoTox framework. This\napproach allow CoTox to generate toxicity predictions aligned with\nphysiological responses, as shown in case study. This result highlights the\npotential of LLM-based frameworks to improve interpretability and support\nearly-stage drug safety assessment. The code and prompt used in this work are\navailable at https://github.com/dmis-lab/CoTox.",
            "upvotes": 18,
            "discussionId": "68942f2f741a16f544fbcf5b",
            "ai_summary": "CoTox, a framework integrating LLMs with chain-of-thought reasoning, enhances multi-toxicity prediction by incorporating chemical structure data, biological pathways, and gene ontology terms, improving interpretability and predictive performance in drug development.",
            "ai_keywords": [
                "large language models",
                "LLM",
                "chain-of-thought",
                "CoT reasoning",
                "multi-toxicity prediction",
                "chemical structure data",
                "biological pathways",
                "gene ontology",
                "GPT-4o",
                "IUPAC names",
                "SMILES",
                "toxicity predictions",
                "physiological responses"
            ]
        },
        "publishedAt": "2025-08-05T03:04:44.000Z",
        "title": "CoTox: Chain-of-Thought-Based Molecular Toxicity Reasoning and\n  Prediction",
        "summary": "Drug toxicity remains a major challenge in pharmaceutical development. Recent\nmachine learning models have improved in silico toxicity prediction, but their\nreliance on annotated data and lack of interpretability limit their\napplicability. This limits their ability to capture organ-specific toxicities\ndriven by complex biological mechanisms. Large language models (LLMs) offer a\npromising alternative through step-by-step reasoning and integration of textual\ndata, yet prior approaches lack biological context and transparent rationale.\nTo address this issue, we propose CoTox, a novel framework that integrates LLM\nwith chain-of-thought (CoT) reasoning for multi-toxicity prediction. CoTox\ncombines chemical structure data, biological pathways, and gene ontology (GO)\nterms to generate interpretable toxicity predictions through step-by-step\nreasoning. Using GPT-4o, we show that CoTox outperforms both traditional\nmachine learning and deep learning model. We further examine its performance\nacross various LLMs to identify where CoTox is most effective. Additionally, we\nfind that representing chemical structures with IUPAC names, which are easier\nfor LLMs to understand than SMILES, enhances the model's reasoning ability and\nimproves predictive performance. To demonstrate its practical utility in drug\ndevelopment, we simulate the treatment of relevant cell types with drug and\nincorporated the resulting biological context into the CoTox framework. This\napproach allow CoTox to generate toxicity predictions aligned with\nphysiological responses, as shown in case study. This result highlights the\npotential of LLM-based frameworks to improve interpretability and support\nearly-stage drug safety assessment. The code and prompt used in this work are\navailable at https://github.com/dmis-lab/CoTox.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64e5c8e594aa0690321f6b29/_CtEK1w-EMh2t8mwZJWhe.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03159.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e5c8e594aa0690321f6b29",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/IW5LRzmPcAM-dri8taMN7.png",
            "fullname": "Yein Park",
            "name": "P-YI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03905",
            "authors": [
                {
                    "_id": "6894156a741a16f544fbced2",
                    "name": "Haofei Yu",
                    "hidden": false
                },
                {
                    "_id": "6894156a741a16f544fbced3",
                    "name": "Zhengyang Qi",
                    "hidden": false
                },
                {
                    "_id": "6894156a741a16f544fbced4",
                    "name": "Yining Zhao",
                    "hidden": false
                },
                {
                    "_id": "6894156a741a16f544fbced5",
                    "name": "Kolby Nottingham",
                    "hidden": false
                },
                {
                    "_id": "6894156a741a16f544fbced6",
                    "name": "Keyang Xuan",
                    "hidden": false
                },
                {
                    "_id": "6894156a741a16f544fbced7",
                    "name": "Bodhisattwa Prasad Majumder",
                    "hidden": false
                },
                {
                    "_id": "6894156a741a16f544fbced8",
                    "name": "Hao Zhu",
                    "hidden": false
                },
                {
                    "_id": "6894156a741a16f544fbced9",
                    "name": "Paul Pu Liang",
                    "hidden": false
                },
                {
                    "_id": "6894156a741a16f544fbceda",
                    "name": "Jiaxuan You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T20:43:42.000Z",
            "submittedOnDailyAt": "2025-08-07T01:30:32.752Z",
            "title": "Sotopia-RL: Reward Design for Social Intelligence",
            "submittedOnDailyBy": {
                "_id": "636453547cf2c0b4f0a3ee1e",
                "avatarUrl": "/avatars/a453340b44d08eec2281ecbe5e993707.svg",
                "isPro": false,
                "fullname": "Haofei Yu",
                "user": "lwaekfjlk",
                "type": "user"
            },
            "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.",
            "upvotes": 17,
            "discussionId": "6894156b741a16f544fbcedb",
            "ai_summary": "Sotopia-RL, a novel reinforcement learning framework, enhances social intelligence in large language models by refining feedback into utterance-level, multi-dimensional rewards, improving performance in social tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "socially intelligent agents",
                "partial observability",
                "multi-dimensionality",
                "Markov decision process",
                "episode-level rewards",
                "utterance-level credit assignment",
                "multi-dimensional rewards",
                "reward hacking",
                "social goal completion scores",
                "Sotopia",
                "Sotopia-RL"
            ]
        },
        "publishedAt": "2025-08-05T16:43:42.000Z",
        "title": "Sotopia-RL: Reward Design for Social Intelligence",
        "summary": "Social intelligence has become a critical capability for large language\nmodels (LLMs), enabling them to engage effectively in real-world social tasks\nsuch as accommodation, persuasion, collaboration, and negotiation.\nReinforcement learning (RL) is a natural fit for training socially intelligent\nagents because it allows models to learn sophisticated strategies directly\nthrough social interactions. However, social interactions have two key\ncharacteristics that set barriers for RL training: (1) partial observability,\nwhere utterances have indirect and delayed effects that complicate credit\nassignment, and (2) multi-dimensionality, where behaviors such as\nrapport-building or knowledge-seeking contribute indirectly to goal\nachievement. These characteristics make Markov decision process (MDP)-based RL\nwith single-dimensional episode-level rewards inefficient and unstable. To\naddress these challenges, we propose Sotopia-RL, a novel framework that refines\ncoarse episode-level feedback into utterance-level, multi-dimensional rewards.\nUtterance-level credit assignment mitigates partial observability by\nattributing outcomes to individual utterances, while multi-dimensional rewards\ncapture the full richness of social interactions and reduce reward hacking.\nExperiments in Sotopia, an open-ended social learning environment, demonstrate\nthat Sotopia-RL achieves state-of-the-art social goal completion scores (7.17\non Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing\napproaches. Ablation studies confirm the necessity of both utterance-level\ncredit assignment and multi-dimensional reward design for RL training. Our\nimplementation is publicly available at:\nhttps://github.com/sotopia-lab/sotopia-rl.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03905.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636453547cf2c0b4f0a3ee1e",
            "avatarUrl": "/avatars/a453340b44d08eec2281ecbe5e993707.svg",
            "fullname": "Haofei Yu",
            "name": "lwaekfjlk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01858",
            "authors": [
                {
                    "_id": "689326568da45ffb0a2b2598",
                    "user": {
                        "_id": "67189ab6034e7dc93591fcfa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67189ab6034e7dc93591fcfa/jeLp-_pdEDzGDsSyKuAks.png",
                        "isPro": false,
                        "fullname": "Eohan G",
                        "user": "Gnonymous",
                        "type": "user"
                    },
                    "name": "Yuhan Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:14:35.040Z",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b2599",
                    "name": "Cong Guo",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b259a",
                    "name": "Aiwen Sun",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b259b",
                    "name": "Hongliang He",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b259c",
                    "name": "Xinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b259d",
                    "name": "Yue Lu",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b259e",
                    "name": "Yingji Zhang",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b259f",
                    "name": "Xuntao Guo",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b25a0",
                    "name": "Dong Zhang",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b25a1",
                    "name": "Jianzhuang Liu",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b25a2",
                    "name": "Jiang Duan",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b25a3",
                    "name": "Yijia Xiao",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b25a4",
                    "name": "Liangjian Wen",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b25a5",
                    "name": "Hai-Ming Xu",
                    "hidden": false
                },
                {
                    "_id": "689326568da45ffb0a2b25a6",
                    "name": "Yong Dai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-03T17:17:52.000Z",
            "submittedOnDailyAt": "2025-08-07T01:40:19.751Z",
            "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents",
            "submittedOnDailyBy": {
                "_id": "67189ab6034e7dc93591fcfa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67189ab6034e7dc93591fcfa/jeLp-_pdEDzGDsSyKuAks.png",
                "isPro": false,
                "fullname": "Eohan G",
                "user": "Gnonymous",
                "type": "user"
            },
            "summary": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner",
            "upvotes": 15,
            "discussionId": "689326568da45ffb0a2b25a7",
            "projectPage": "https://eohan.me/Web-CogReasoner",
            "githubRepo": "https://github.com/Gnonymous/Web-CogReasoner",
            "ai_summary": "A framework for web agents decomposes their capabilities into knowledge content learning and cognitive processes, using a structured dataset and a novel reasoning framework to enhance generalization and performance.",
            "ai_keywords": [
                "Web-CogKnowledge Framework",
                "Factual knowledge",
                "Conceptual knowledge",
                "Procedural knowledge",
                "Memorizing",
                "Understanding",
                "Exploring",
                "Web-CogDataset",
                "Chain-of-Thought (CoT) reasoning",
                "Web-CogReasoner",
                "Web-CogBench"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-08-03T13:17:52.000Z",
        "title": "Web-CogReasoner: Towards Knowledge-Induced Cognitive Reasoning for Web\n  Agents",
        "summary": "Multimodal large-scale models have significantly advanced the development of\nweb agents, enabling perception and interaction with digital environments akin\nto human cognition. In this paper, we argue that web agents must first acquire\nsufficient knowledge to effectively engage in cognitive reasoning. Therefore,\nwe decompose a web agent's capabilities into two essential stages: knowledge\ncontent learning and cognitive processes. To formalize this, we propose\nWeb-CogKnowledge Framework, categorizing knowledge as Factual, Conceptual, and\nProcedural. In this framework, knowledge content learning corresponds to the\nagent's processes of Memorizing and Understanding, which rely on the first two\nknowledge types, representing the \"what\" of learning. Conversely, cognitive\nprocesses correspond to Exploring, grounded in Procedural knowledge, defining\nthe \"how\" of reasoning and action. To facilitate knowledge acquisition, we\nconstruct the Web-CogDataset, a structured resource curated from 14 real-world\nwebsites, designed to systematically instill core knowledge necessary for web\nagent. This dataset serves as the agent's conceptual grounding-the \"nouns\" upon\nwhich comprehension is built-as well as the basis for learning how to reason\nand act. Building on this foundation, we operationalize these processes through\na novel knowledge-driven Chain-of-Thought (CoT) reasoning framework, developing\nand training our proposed agent, the Web-CogReasoner. Extensive experimentation\nreveals its significant superiority over existing models, especially in\ngeneralizing to unseen tasks where structured knowledge is decisive. To enable\nrigorous evaluation, we introduce the Web-CogBench, a comprehensive evaluation\nsuite designed to assess and compare agent performance across the delineated\nknowledge domains and cognitive capabilities. Our code and data is open sourced\nat https://github.com/Gnonymous/Web-CogReasoner",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01858.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67189ab6034e7dc93591fcfa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67189ab6034e7dc93591fcfa/jeLp-_pdEDzGDsSyKuAks.png",
            "fullname": "Eohan G",
            "name": "Gnonymous",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03789",
            "authors": [
                {
                    "_id": "6894142c741a16f544fbcec2",
                    "name": "Yuhang Ma",
                    "hidden": false
                },
                {
                    "_id": "6894142c741a16f544fbcec3",
                    "name": "Xiaoshi Wu",
                    "hidden": false
                },
                {
                    "_id": "6894142c741a16f544fbcec4",
                    "name": "Keqiang Sun",
                    "hidden": false
                },
                {
                    "_id": "6894142c741a16f544fbcec5",
                    "name": "Hongsheng Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T17:17:13.000Z",
            "submittedOnDailyAt": "2025-08-07T01:20:19.742Z",
            "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
            "submittedOnDailyBy": {
                "_id": "645a46df5e6871b4b2d39bb8",
                "avatarUrl": "/avatars/5687fdca5bec77677377b57a06d64b9e.svg",
                "isPro": false,
                "fullname": "Yunhaoshui",
                "user": "xilanhua12138",
                "type": "user"
            },
            "summary": "Evaluating text-to-image generation models requires alignment with human\nperception, yet existing human-centric metrics are constrained by limited data\ncoverage, suboptimal feature extraction, and inefficient loss functions. To\naddress these challenges, we introduce Human Preference Score v3 (HPSv3). (1)\nWe release HPDv3, the first wide-spectrum human preference dataset integrating\n1.08M text-image pairs and 1.17M annotated pairwise comparisons from\nstate-of-the-art generative models and low to high-quality real-world images.\n(2) We introduce a VLM-based preference model trained using an\nuncertainty-aware ranking loss for fine-grained ranking. Besides, we propose\nChain-of-Human-Preference (CoHP), an iterative image refinement method that\nenhances quality without extra data, using HPSv3 to select the best image at\neach step. Extensive experiments demonstrate that HPSv3 serves as a robust\nmetric for wide-spectrum image evaluation, and CoHP offers an efficient and\nhuman-aligned approach to improve image generation quality. The code and\ndataset are available at the HPSv3 Homepage.",
            "upvotes": 12,
            "discussionId": "6894142d741a16f544fbcec6",
            "projectPage": "https://mizzenai.github.io/HPSv3.project/",
            "githubRepo": "https://github.com/MizzenAI/HPSv3",
            "ai_summary": "HPSv3, a human preference score using a wide-spectrum dataset and uncertainty-aware ranking loss, enhances text-to-image generation quality through iterative refinement.",
            "ai_keywords": [
                "human preference score",
                "HPDv3",
                "text-image pairs",
                "pairwise comparisons",
                "VLM-based preference model",
                "uncertainty-aware ranking loss",
                "Chain-of-Human-Preference",
                "CoHP",
                "image refinement",
                "image generation quality"
            ],
            "githubStars": 72
        },
        "publishedAt": "2025-08-05T13:17:13.000Z",
        "title": "HPSv3: Towards Wide-Spectrum Human Preference Score",
        "summary": "Evaluating text-to-image generation models requires alignment with human\nperception, yet existing human-centric metrics are constrained by limited data\ncoverage, suboptimal feature extraction, and inefficient loss functions. To\naddress these challenges, we introduce Human Preference Score v3 (HPSv3). (1)\nWe release HPDv3, the first wide-spectrum human preference dataset integrating\n1.08M text-image pairs and 1.17M annotated pairwise comparisons from\nstate-of-the-art generative models and low to high-quality real-world images.\n(2) We introduce a VLM-based preference model trained using an\nuncertainty-aware ranking loss for fine-grained ranking. Besides, we propose\nChain-of-Human-Preference (CoHP), an iterative image refinement method that\nenhances quality without extra data, using HPSv3 to select the best image at\neach step. Extensive experiments demonstrate that HPSv3 serves as a robust\nmetric for wide-spectrum image evaluation, and CoHP offers an efficient and\nhuman-aligned approach to improve image generation quality. The code and\ndataset are available at the HPSv3 Homepage.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03789.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645a46df5e6871b4b2d39bb8",
            "avatarUrl": "/avatars/5687fdca5bec77677377b57a06d64b9e.svg",
            "fullname": "Yunhaoshui",
            "name": "xilanhua12138",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.03560",
            "authors": [
                {
                    "_id": "6892be3d8da45ffb0a2b240f",
                    "user": {
                        "_id": "6523ad818fc4884992051c8f",
                        "avatarUrl": "/avatars/fe19eb397e7c121f78b370b456e542cf.svg",
                        "isPro": false,
                        "fullname": "Yi Gui",
                        "user": "starmage520",
                        "type": "user"
                    },
                    "name": "Yi Gui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:17:22.322Z",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2410",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2411",
                    "name": "Zhongyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2412",
                    "name": "Guohao Wang",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2413",
                    "name": "Tianpeng Lv",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2414",
                    "name": "Gaoyang Jiang",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2415",
                    "name": "Yi Liu",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2416",
                    "name": "Dongping Chen",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2417",
                    "name": "Yao Wan",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2418",
                    "name": "Hongyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b2419",
                    "name": "Wenbin Jiang",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b241a",
                    "name": "Xuanhua Shi",
                    "hidden": false
                },
                {
                    "_id": "6892be3d8da45ffb0a2b241b",
                    "name": "Hai Jin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T15:28:48.000Z",
            "submittedOnDailyAt": "2025-08-07T03:02:24.780Z",
            "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought",
            "submittedOnDailyBy": {
                "_id": "6523ad818fc4884992051c8f",
                "avatarUrl": "/avatars/fe19eb397e7c121f78b370b456e542cf.svg",
                "isPro": false,
                "fullname": "Yi Gui",
                "user": "starmage520",
                "type": "user"
            },
            "summary": "Converting webpage designs into code (design-to-code) plays a vital role in\nUser Interface (UI) development for front-end developers, bridging the gap\nbetween visual design and functional implementation. While recent Multimodal\nLarge Language Models (MLLMs) have shown significant potential in\ndesign-to-code tasks, they often fail to accurately preserve the layout during\ncode generation. To this end, we draw inspiration from the Chain-of-Thought\n(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that\nenhances layout preservation in webpage design during code generation with\nLayout-as-Thought (LaT). Specifically, we first introduce a simple yet\nefficient algorithm to divide the webpage design into image blocks. Next, we\nprompt MLLMs using a CoTbased approach to generate code for each block.\nFinally, we apply two assembly strategies-absolute positioning and an\nMLLM-based method-followed by dynamic selection to determine the optimal\noutput. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs\n(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly\nintroduced, more challenging benchmark (CC-HARD) that features complex layouts.\nThe experimental results on automatic metrics demonstrate significant\nimprovements. Specifically, TreeBLEU scores increased by 66.67% and MAE\ndecreased by 38% when using DeepSeek-VL2, compared to direct prompting.\nMoreover, the human preference evaluation results indicate that annotators\nfavor the webpages generated by LaTCoder in over 60% of cases, providing strong\nevidence of the effectiveness of our method.",
            "upvotes": 12,
            "discussionId": "6892be3d8da45ffb0a2b241c",
            "ai_summary": "LaTCoder enhances layout preservation in design-to-code tasks by dividing webpage designs into blocks and using Chain-of-Thought reasoning with MLLMs, achieving significant improvements in metrics and human preference.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "Chain-of-Thought",
                "Layout-as-Thought",
                "image blocks",
                "absolute positioning",
                "TreeBLEU",
                "MAE"
            ]
        },
        "publishedAt": "2025-08-05T11:28:48.000Z",
        "title": "LaTCoder: Converting Webpage Design to Code with Layout-as-Thought",
        "summary": "Converting webpage designs into code (design-to-code) plays a vital role in\nUser Interface (UI) development for front-end developers, bridging the gap\nbetween visual design and functional implementation. While recent Multimodal\nLarge Language Models (MLLMs) have shown significant potential in\ndesign-to-code tasks, they often fail to accurately preserve the layout during\ncode generation. To this end, we draw inspiration from the Chain-of-Thought\n(CoT) reasoning in human cognition and propose LaTCoder, a novel approach that\nenhances layout preservation in webpage design during code generation with\nLayout-as-Thought (LaT). Specifically, we first introduce a simple yet\nefficient algorithm to divide the webpage design into image blocks. Next, we\nprompt MLLMs using a CoTbased approach to generate code for each block.\nFinally, we apply two assembly strategies-absolute positioning and an\nMLLM-based method-followed by dynamic selection to determine the optimal\noutput. We evaluate the effectiveness of LaTCoder using multiple backbone MLLMs\n(i.e., DeepSeek-VL2, Gemini, and GPT-4o) on both a public benchmark and a newly\nintroduced, more challenging benchmark (CC-HARD) that features complex layouts.\nThe experimental results on automatic metrics demonstrate significant\nimprovements. Specifically, TreeBLEU scores increased by 66.67% and MAE\ndecreased by 38% when using DeepSeek-VL2, compared to direct prompting.\nMoreover, the human preference evaluation results indicate that annotators\nfavor the webpages generated by LaTCoder in over 60% of cases, providing strong\nevidence of the effectiveness of our method.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03560.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6523ad818fc4884992051c8f",
            "avatarUrl": "/avatars/fe19eb397e7c121f78b370b456e542cf.svg",
            "fullname": "Yi Gui",
            "name": "starmage520",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2507.23785",
            "authors": [
                {
                    "_id": "6892cf048da45ffb0a2b24b6",
                    "user": {
                        "_id": "6237f25f16004228e6c74e01",
                        "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
                        "isPro": false,
                        "fullname": "Bowen Zhang",
                        "user": "BwZhang",
                        "type": "user"
                    },
                    "name": "Bowen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:16:22.835Z",
                    "hidden": false
                },
                {
                    "_id": "6892cf048da45ffb0a2b24b7",
                    "name": "Sicheng Xu",
                    "hidden": false
                },
                {
                    "_id": "6892cf048da45ffb0a2b24b8",
                    "name": "Chuxin Wang",
                    "hidden": false
                },
                {
                    "_id": "6892cf048da45ffb0a2b24b9",
                    "name": "Jiaolong Yang",
                    "hidden": false
                },
                {
                    "_id": "6892cf048da45ffb0a2b24ba",
                    "name": "Feng Zhao",
                    "hidden": false
                },
                {
                    "_id": "6892cf048da45ffb0a2b24bb",
                    "name": "Dong Chen",
                    "hidden": false
                },
                {
                    "_id": "6892cf048da45ffb0a2b24bc",
                    "name": "Baining Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T17:59:51.000Z",
            "submittedOnDailyAt": "2025-08-07T01:26:55.033Z",
            "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
            "submittedOnDailyBy": {
                "_id": "6237f25f16004228e6c74e01",
                "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
                "isPro": false,
                "fullname": "Bowen Zhang",
                "user": "BwZhang",
                "type": "user"
            },
            "summary": "In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.",
            "upvotes": 12,
            "discussionId": "6892cf058da45ffb0a2b24bd",
            "projectPage": "https://gvfdiffusion.github.io/",
            "githubRepo": "https://github.com/ForeverFancy/gvfdiffusion",
            "ai_summary": "A novel framework uses a Direct 4DMesh-to-GS Variation Field VAE and Gaussian Variation Field diffusion model to generate high-quality dynamic 3D content from single video inputs, demonstrating superior quality and generalization.",
            "ai_keywords": [
                "Direct 4D diffusion modeling",
                "Gaussian Splats (GS)",
                "Direct 4DMesh-to-GS Variation Field VAE",
                "Gaussian Variation Field diffusion model",
                "Diffusion Transformer",
                "Objaverse dataset"
            ],
            "githubStars": 56
        },
        "publishedAt": "2025-07-31T13:59:51.000Z",
        "title": "Gaussian Variation Field Diffusion for High-fidelity Video-to-4D\n  Synthesis",
        "summary": "In this paper, we present a novel framework for video-to-4D generation that\ncreates high-quality dynamic 3D content from single video inputs. Direct 4D\ndiffusion modeling is extremely challenging due to costly data construction and\nthe high-dimensional nature of jointly representing 3D shape, appearance, and\nmotion. We address these challenges by introducing a Direct 4DMesh-to-GS\nVariation Field VAE that directly encodes canonical Gaussian Splats (GS) and\ntheir temporal variations from 3D animation data without per-instance fitting,\nand compresses high-dimensional animations into a compact latent space.\nBuilding upon this efficient representation, we train a Gaussian Variation\nField diffusion model with temporal-aware Diffusion Transformer conditioned on\ninput videos and canonical GS. Trained on carefully-curated animatable 3D\nobjects from the Objaverse dataset, our model demonstrates superior generation\nquality compared to existing methods. It also exhibits remarkable\ngeneralization to in-the-wild video inputs despite being trained exclusively on\nsynthetic data, paving the way for generating high-quality animated 3D content.\nProject page: https://gvfdiffusion.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23785.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6237f25f16004228e6c74e01",
            "avatarUrl": "/avatars/cc63ce464a25702c8155610d2a708595.svg",
            "fullname": "Bowen Zhang",
            "name": "BwZhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.02215",
            "authors": [
                {
                    "_id": "6894106d741a16f544fbcea6",
                    "user": {
                        "_id": "6541159afcbd1aa00682ec66",
                        "avatarUrl": "/avatars/94fd9ce0ea534e5d84736c1a1eb949e7.svg",
                        "isPro": false,
                        "fullname": "zhangyik21",
                        "user": "zhangyik21",
                        "type": "user"
                    },
                    "name": "Yike Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:39:04.154Z",
                    "hidden": false
                },
                {
                    "_id": "6894106d741a16f544fbcea7",
                    "user": {
                        "_id": "6455f5cababbbbd3486d6ee3",
                        "avatarUrl": "/avatars/b6c8f65fd2bef8a00aa3269856ea238e.svg",
                        "isPro": false,
                        "fullname": "Zhiyuan He",
                        "user": "hzy46",
                        "type": "user"
                    },
                    "name": "Zhiyuan He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:57.434Z",
                    "hidden": false
                },
                {
                    "_id": "6894106d741a16f544fbcea8",
                    "name": "Huiqiang Jiang",
                    "hidden": false
                },
                {
                    "_id": "6894106d741a16f544fbcea9",
                    "name": "Chengruidong Zhang",
                    "hidden": false
                },
                {
                    "_id": "6894106d741a16f544fbceaa",
                    "name": "Yuqing Yang",
                    "hidden": false
                },
                {
                    "_id": "6894106d741a16f544fbceab",
                    "name": "Jianyong Wang",
                    "hidden": false
                },
                {
                    "_id": "6894106d741a16f544fbceac",
                    "name": "Lili Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T09:08:43.000Z",
            "submittedOnDailyAt": "2025-08-07T01:18:44.229Z",
            "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
            "submittedOnDailyBy": {
                "_id": "6541159afcbd1aa00682ec66",
                "avatarUrl": "/avatars/94fd9ce0ea534e5d84736c1a1eb949e7.svg",
                "isPro": false,
                "fullname": "zhangyik21",
                "user": "zhangyik21",
                "type": "user"
            },
            "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
            "upvotes": 9,
            "discussionId": "6894106d741a16f544fbcead",
            "projectPage": "https://aka.ms/LeanK",
            "ai_summary": "LeanK, a learning-based method, prunes unimportant key cache channels in large language models to reduce memory usage and accelerate decoding without sacrificing accuracy.",
            "ai_keywords": [
                "large language models",
                "key-value cache",
                "channel sparsity",
                "two-stage training",
                "channel-wise static mask",
                "GPU memory",
                "decoding speedup",
                "attention computation",
                "long-context inference",
                "importance distribution"
            ]
        },
        "publishedAt": "2025-08-04T05:08:43.000Z",
        "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
        "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02215.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6541159afcbd1aa00682ec66",
            "avatarUrl": "/avatars/94fd9ce0ea534e5d84736c1a1eb949e7.svg",
            "fullname": "zhangyik21",
            "name": "zhangyik21",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.02807",
            "authors": [
                {
                    "_id": "68944b9e741a16f544fbcfec",
                    "name": "Tongchun Zuo",
                    "hidden": false
                },
                {
                    "_id": "68944b9e741a16f544fbcfed",
                    "name": "Zaiyu Huang",
                    "hidden": false
                },
                {
                    "_id": "68944b9e741a16f544fbcfee",
                    "name": "Shuliang Ning",
                    "hidden": false
                },
                {
                    "_id": "68944b9e741a16f544fbcfef",
                    "name": "Ente Lin",
                    "hidden": false
                },
                {
                    "_id": "68944b9e741a16f544fbcff0",
                    "name": "Chao Liang",
                    "hidden": false
                },
                {
                    "_id": "68944b9e741a16f544fbcff1",
                    "name": "Zerong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68944b9e741a16f544fbcff2",
                    "name": "Jianwen Jiang",
                    "hidden": false
                },
                {
                    "_id": "68944b9e741a16f544fbcff3",
                    "name": "Yuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68944b9e741a16f544fbcff4",
                    "name": "Mingyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "68944b9e741a16f544fbcff5",
                    "name": "Xin Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T18:27:55.000Z",
            "submittedOnDailyAt": "2025-08-07T05:16:54.059Z",
            "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework",
            "submittedOnDailyBy": {
                "_id": "6358d973f53b06a81271d8d6",
                "avatarUrl": "/avatars/06b7e205a1b08d2419afc2f6273028d1.svg",
                "isPro": false,
                "fullname": "Ning",
                "user": "Shuliang",
                "type": "user"
            },
            "summary": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. In the second stage, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/",
            "upvotes": 8,
            "discussionId": "68944b9e741a16f544fbcff6",
            "ai_summary": "DreamVVT, a two-stage framework using Diffusion Transformers and LoRA adapters, enhances video virtual try-on by leveraging unpaired human-centric data and pretrained models to preserve garment details and temporal consistency.",
            "ai_keywords": [
                "Diffusion Transformers",
                "DiTs",
                "multi-frame try-on model",
                "vision-language model",
                "VLM",
                "skeleton maps",
                "fine-grained motion",
                "appearance descriptions",
                "pretrained video generation model",
                "LoRA adapters",
                "long-term temporal coherence"
            ]
        },
        "publishedAt": "2025-08-04T14:27:55.000Z",
        "title": "DreamVVT: Mastering Realistic Video Virtual Try-On in the Wild via a\n  Stage-Wise Diffusion Transformer Framework",
        "summary": "Video virtual try-on (VVT) technology has garnered considerable academic\ninterest owing to its promising applications in e-commerce advertising and\nentertainment. However, most existing end-to-end methods rely heavily on scarce\npaired garment-centric datasets and fail to effectively leverage priors of\nadvanced visual models and test-time inputs, making it challenging to\naccurately preserve fine-grained garment details and maintain temporal\nconsistency in unconstrained scenarios. To address these challenges, we propose\nDreamVVT, a carefully designed two-stage framework built upon Diffusion\nTransformers (DiTs), which is inherently capable of leveraging diverse unpaired\nhuman-centric data to enhance adaptability in real-world scenarios. To further\nleverage prior knowledge from pretrained models and test-time inputs, in the\nfirst stage, we sample representative frames from the input video and utilize a\nmulti-frame try-on model integrated with a vision-language model (VLM), to\nsynthesize high-fidelity and semantically consistent keyframe try-on images.\nThese images serve as complementary appearance guidance for subsequent video\ngeneration. In the second stage, skeleton maps together with\nfine-grained motion and appearance descriptions are extracted from the input\ncontent, and these along with the keyframe try-on images are then fed into a\npretrained video generation model enhanced with LoRA adapters. This ensures\nlong-term temporal coherence for unseen regions and enables highly plausible\ndynamic motions. Extensive quantitative and qualitative experiments demonstrate\nthat DreamVVT surpasses existing methods in preserving detailed garment content\nand temporal stability in real-world scenarios. Our project page\nhttps://virtu-lab.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02807.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6358d973f53b06a81271d8d6",
            "avatarUrl": "/avatars/06b7e205a1b08d2419afc2f6273028d1.svg",
            "fullname": "Ning",
            "name": "Shuliang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.04664",
            "authors": [
                {
                    "_id": "689402a1741a16f544fbce59",
                    "user": {
                        "_id": "6576fe2b42ab083faea19841",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg",
                        "isPro": false,
                        "fullname": "Mo Li",
                        "user": "Mor-Li",
                        "type": "user"
                    },
                    "name": "Mo Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:41:38.538Z",
                    "hidden": false
                },
                {
                    "_id": "689402a1741a16f544fbce5a",
                    "name": "L. H. Xu",
                    "hidden": false
                },
                {
                    "_id": "689402a1741a16f544fbce5b",
                    "name": "Qitai Tan",
                    "hidden": false
                },
                {
                    "_id": "689402a1741a16f544fbce5c",
                    "name": "Ting Cao",
                    "hidden": false
                },
                {
                    "_id": "689402a1741a16f544fbce5d",
                    "name": "Yunxin Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6576fe2b42ab083faea19841/Dl7QjjxqaNg8sBGAYF0RV.png"
            ],
            "publishedAt": "2025-08-06T17:32:58.000Z",
            "submittedOnDailyAt": "2025-08-07T01:37:06.645Z",
            "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management",
            "submittedOnDailyBy": {
                "_id": "6576fe2b42ab083faea19841",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg",
                "isPro": false,
                "fullname": "Mo Li",
                "user": "Mor-Li",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.",
            "upvotes": 6,
            "discussionId": "689402a1741a16f544fbce5e",
            "ai_summary": "Sculptor, a framework for Active Context Management, enhances LLM performance on long contexts by enabling proactive attention and memory control, reducing proactive interference and improving reasoning reliability.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "proactive interference",
                "Active Context Management (ACM)",
                "context fragmentation",
                "summary",
                "hide",
                "restore",
                "intelligent search",
                "PI-LLM",
                "NeedleBench Multi-Needle Reasoning"
            ]
        },
        "publishedAt": "2025-08-06T13:32:58.000Z",
        "title": "Sculptor: Empowering LLMs with Cognitive Agency via Active Context\n  Management",
        "summary": "Large Language Models (LLMs) suffer from significant performance degradation\nwhen processing long contexts due to proactive interference, where irrelevant\ninformation in earlier parts of the context disrupts reasoning and memory\nrecall. While most research focuses on external memory systems to augment LLMs'\ncapabilities, we propose a complementary approach: empowering LLMs with Active\nContext Management (ACM) tools to actively sculpt their internal working\nmemory. We introduce Sculptor, a framework that equips LLMs with three\ncategories of tools: (1) context fragmentation, (2) summary, hide, and restore,\nand (3) intelligent search. Our approach enables LLMs to proactively manage\ntheir attention and working memory, analogous to how humans selectively focus\non relevant information while filtering out distractions. Experimental\nevaluation on information-sparse benchmarks-PI-LLM (proactive interference) and\nNeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly\nimproves performance even without specific training, leveraging LLMs' inherent\ntool calling generalization capabilities. By enabling Active Context\nManagement, Sculptor not only mitigates proactive interference but also\nprovides a cognitive foundation for more reliable reasoning across diverse\nlong-context tasks-highlighting that explicit context-control strategies,\nrather than merely larger token windows, are key to robustness at scale.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6576fe2b42ab083faea19841/Dl7QjjxqaNg8sBGAYF0RV.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04664.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6576fe2b42ab083faea19841",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/c91ZKOR2E0gL8iIVkvEUa.jpeg",
            "fullname": "Mo Li",
            "name": "Mor-Li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04586",
            "authors": [
                {
                    "_id": "6893fe53741a16f544fbce21",
                    "user": {
                        "_id": "641ac2207c21ab946bf036e8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641ac2207c21ab946bf036e8/r6c9gpOrul0eC59d9e2Mo.png",
                        "isPro": true,
                        "fullname": "Nuo Chen",
                        "user": "nuojohnchen",
                        "type": "user"
                    },
                    "name": "Nuo Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:41:41.756Z",
                    "hidden": false
                },
                {
                    "_id": "6893fe53741a16f544fbce22",
                    "name": "Moming Duan",
                    "hidden": false
                },
                {
                    "_id": "6893fe53741a16f544fbce23",
                    "name": "Andre Huikai Lin",
                    "hidden": false
                },
                {
                    "_id": "6893fe53741a16f544fbce24",
                    "name": "Qian Wang",
                    "hidden": false
                },
                {
                    "_id": "6893fe53741a16f544fbce25",
                    "name": "Jiaying Wu",
                    "hidden": false
                },
                {
                    "_id": "6893fe53741a16f544fbce26",
                    "name": "Bingsheng He",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/641ac2207c21ab946bf036e8/yyBhKvXaRlW-2Z24Q0vJ1.gif"
            ],
            "publishedAt": "2025-08-06T16:08:27.000Z",
            "submittedOnDailyAt": "2025-08-07T01:14:52.604Z",
            "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference",
            "submittedOnDailyBy": {
                "_id": "641ac2207c21ab946bf036e8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641ac2207c21ab946bf036e8/r6c9gpOrul0eC59d9e2Mo.png",
                "isPro": true,
                "fullname": "Nuo Chen",
                "user": "nuojohnchen",
                "type": "user"
            },
            "summary": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research.",
            "upvotes": 6,
            "discussionId": "6893fe53741a16f544fbce27",
            "ai_summary": "The paper diagnoses structural issues in AI conferences, including publication rates, carbon footprint, negative community sentiment, and logistical challenges, and proposes a Community-Federated Conference model to address these issues.",
            "ai_keywords": [
                ""
            ]
        },
        "publishedAt": "2025-08-06T12:08:27.000Z",
        "title": "Position: The Current AI Conference Model is Unsustainable! Diagnosing\n  the Crisis of Centralized AI Conference",
        "summary": "Artificial Intelligence (AI) conferences are essential for advancing\nresearch, sharing knowledge, and fostering academic community. However, their\nrapid expansion has rendered the centralized conference model increasingly\nunsustainable. This paper offers a data-driven diagnosis of a structural crisis\nthat threatens the foundational goals of scientific dissemination, equity, and\ncommunity well-being. We identify four key areas of strain: (1) scientifically,\nwith per-author publication rates more than doubling over the past decade to\nover 4.5 papers annually; (2) environmentally, with the carbon footprint of a\nsingle conference exceeding the daily emissions of its host city; (3)\npsychologically, with 71% of online community discourse reflecting negative\nsentiment and 35% referencing mental health concerns; and (4) logistically,\nwith attendance at top conferences such as NeurIPS 2024 beginning to outpace\nvenue capacity. These pressures point to a system that is misaligned with its\ncore mission. In response, we propose the Community-Federated Conference (CFC)\nmodel, which separates peer review, presentation, and networking into globally\ncoordinated but locally organized components, offering a more sustainable,\ninclusive, and resilient path forward for AI research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/641ac2207c21ab946bf036e8/yyBhKvXaRlW-2Z24Q0vJ1.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04586.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641ac2207c21ab946bf036e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641ac2207c21ab946bf036e8/r6c9gpOrul0eC59d9e2Mo.png",
            "fullname": "Nuo Chen",
            "name": "nuojohnchen",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.01928",
            "authors": [
                {
                    "_id": "6891b03ef01a094725f8360e",
                    "user": {
                        "_id": "662798e43e2a8eec1b53ed1a",
                        "avatarUrl": "/avatars/28871a0725cf49347457fa8535ce5fdb.svg",
                        "isPro": false,
                        "fullname": "Yaroslav Prytula",
                        "user": "YaroslavPrytula",
                        "type": "user"
                    },
                    "name": "Yaroslav Prytula",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:20:46.531Z",
                    "hidden": false
                },
                {
                    "_id": "6891b03ef01a094725f8360f",
                    "name": "Illia Tsiporenko",
                    "hidden": false
                },
                {
                    "_id": "6891b03ef01a094725f83610",
                    "name": "Ali Zeynalli",
                    "hidden": false
                },
                {
                    "_id": "6891b03ef01a094725f83611",
                    "name": "Dmytro Fishman",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/662798e43e2a8eec1b53ed1a/JeBv0n1NrlhQ1VkLoSB0-.png",
                "https://cdn-uploads.huggingface.co/production/uploads/662798e43e2a8eec1b53ed1a/yVPuHhBivIUONcQKn-tux.png"
            ],
            "publishedAt": "2025-08-03T21:36:20.000Z",
            "submittedOnDailyAt": "2025-08-07T07:06:11.978Z",
            "title": "IAUNet: Instance-Aware U-Net",
            "submittedOnDailyBy": {
                "_id": "662798e43e2a8eec1b53ed1a",
                "avatarUrl": "/avatars/28871a0725cf49347457fa8535ce5fdb.svg",
                "isPro": false,
                "fullname": "Yaroslav Prytula",
                "user": "YaroslavPrytula",
                "type": "user"
            },
            "summary": "Instance segmentation is critical in biomedical imaging to accurately\ndistinguish individual objects like cells, which often overlap and vary in\nsize. Recent query-based methods, where object queries guide segmentation, have\nshown strong performance. While U-Net has been a go-to architecture in medical\nimage segmentation, its potential in query-based approaches remains largely\nunexplored. In this work, we present IAUNet, a novel query-based U-Net\narchitecture. The core design features a full U-Net architecture, enhanced by a\nnovel lightweight convolutional Pixel decoder, making the model more efficient\nand reducing the number of parameters. Additionally, we propose a Transformer\ndecoder that refines object-specific features across multiple scales. Finally,\nwe introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource\nwith detailed annotations of overlapping cell cytoplasm in brightfield images,\nsetting a new benchmark for biomedical instance segmentation. Experiments on\nmultiple public datasets and our own show that IAUNet outperforms most\nstate-of-the-art fully convolutional, transformer-based, and query-based models\nand cell segmentation-specific models, setting a strong baseline for cell\ninstance segmentation tasks. Code is available at\nhttps://github.com/SlavkoPrytula/IAUNet",
            "upvotes": 5,
            "discussionId": "6891b03ef01a094725f83612",
            "projectPage": "https://slavkoprytula.github.io/IAUNet/",
            "githubRepo": "https://github.com/SlavkoPrytula/IAUNet",
            "ai_summary": "IAUNet, a query-based U-Net architecture with a lightweight convolutional Pixel decoder and Transformer decoder, outperforms state-of-the-art models in biomedical instance segmentation.",
            "ai_keywords": [
                "U-Net",
                "Pixel decoder",
                "Transformer decoder",
                "query-based methods",
                "biomedical instance segmentation",
                "Revvity Full Cell Segmentation Dataset"
            ],
            "githubStars": 19
        },
        "publishedAt": "2025-08-03T17:36:20.000Z",
        "title": "IAUNet: Instance-Aware U-Net",
        "summary": "Instance segmentation is critical in biomedical imaging to accurately\ndistinguish individual objects like cells, which often overlap and vary in\nsize. Recent query-based methods, where object queries guide segmentation, have\nshown strong performance. While U-Net has been a go-to architecture in medical\nimage segmentation, its potential in query-based approaches remains largely\nunexplored. In this work, we present IAUNet, a novel query-based U-Net\narchitecture. The core design features a full U-Net architecture, enhanced by a\nnovel lightweight convolutional Pixel decoder, making the model more efficient\nand reducing the number of parameters. Additionally, we propose a Transformer\ndecoder that refines object-specific features across multiple scales. Finally,\nwe introduce the 2025 Revvity Full Cell Segmentation Dataset, a unique resource\nwith detailed annotations of overlapping cell cytoplasm in brightfield images,\nsetting a new benchmark for biomedical instance segmentation. Experiments on\nmultiple public datasets and our own show that IAUNet outperforms most\nstate-of-the-art fully convolutional, transformer-based, and query-based models\nand cell segmentation-specific models, setting a strong baseline for cell\ninstance segmentation tasks. Code is available at\nhttps://github.com/SlavkoPrytula/IAUNet",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/662798e43e2a8eec1b53ed1a/JeBv0n1NrlhQ1VkLoSB0-.png",
            "https://cdn-uploads.huggingface.co/production/uploads/662798e43e2a8eec1b53ed1a/yVPuHhBivIUONcQKn-tux.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01928.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "662798e43e2a8eec1b53ed1a",
            "avatarUrl": "/avatars/28871a0725cf49347457fa8535ce5fdb.svg",
            "fullname": "Yaroslav Prytula",
            "name": "YaroslavPrytula",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04295",
            "authors": [
                {
                    "_id": "68941a73741a16f544fbcee3",
                    "name": "Chaofan Wang",
                    "hidden": false
                },
                {
                    "_id": "68941a73741a16f544fbcee4",
                    "name": "Tingrui Yu",
                    "hidden": false
                },
                {
                    "_id": "68941a73741a16f544fbcee5",
                    "name": "Jie Wang",
                    "hidden": false
                },
                {
                    "_id": "68941a73741a16f544fbcee6",
                    "name": "Dong Chen",
                    "hidden": false
                },
                {
                    "_id": "68941a73741a16f544fbcee7",
                    "name": "Wenrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68941a73741a16f544fbcee8",
                    "user": {
                        "_id": "645b0c3ec35da9c7afd95421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                        "isPro": false,
                        "fullname": "Yuling",
                        "user": "YerbaPage",
                        "type": "user"
                    },
                    "name": "Yuling Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:38.437Z",
                    "hidden": false
                },
                {
                    "_id": "68941a73741a16f544fbcee9",
                    "name": "Xiaodong Gu",
                    "hidden": false
                },
                {
                    "_id": "68941a73741a16f544fbceea",
                    "name": "Beijun Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T10:31:23.000Z",
            "submittedOnDailyAt": "2025-08-07T02:57:51.348Z",
            "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation",
            "submittedOnDailyBy": {
                "_id": "645b0c3ec35da9c7afd95421",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                "isPro": false,
                "fullname": "Yuling",
                "user": "YerbaPage",
                "type": "user"
            },
            "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.",
            "upvotes": 4,
            "discussionId": "68941a74741a16f544fbceeb",
            "ai_summary": "EvoC2Rust is an automated framework that translates entire C projects to Rust using a skeleton-guided approach, combining rule-based and LLM-based methods to improve syntax, semantics, and safety.",
            "ai_keywords": [
                "LLM",
                "feature-mapping-enhanced LLM",
                "static analysis",
                "evolutionary stages",
                "functional modules",
                "type-checked function stubs",
                "syntax accuracy",
                "semantic accuracy",
                "code safety",
                "compilation",
                "test pass rates"
            ]
        },
        "publishedAt": "2025-08-06T06:31:23.000Z",
        "title": "EVOC2RUST: A Skeleton-guided Framework for Project-Level C-to-Rust\n  Translation",
        "summary": "Rust's compile-time safety guarantees make it ideal for safety-critical\nsystems, creating demand for translating legacy C codebases to Rust. While\nvarious approaches have emerged for this task, they face inherent trade-offs:\nrule-based solutions face challenges in meeting code safety and idiomaticity\nrequirements, while LLM-based solutions often fail to generate semantically\nequivalent Rust code, due to the heavy dependencies of modules across the\nentire codebase. Recent studies have revealed that both solutions are limited\nto small-scale programs. In this paper, we propose EvoC2Rust, an automated\nframework for converting entire C projects to equivalent Rust ones. EvoC2Rust\nemploys a skeleton-guided translation strategy for project-level translation.\nThe pipeline consists of three evolutionary stages: 1) it first decomposes the\nC project into functional modules, employs a feature-mapping-enhanced LLM to\ntransform definitions and macros and generates type-checked function stubs,\nwhich form a compilable Rust skeleton; 2) it then incrementally translates the\nfunction, replacing the corresponding stub placeholder; 3) finally, it repairs\ncompilation errors by integrating LLM and static analysis. Through evolutionary\naugmentation, EvoC2Rust combines the advantages of both rule-based and\nLLM-based solutions. Our evaluation on open-source benchmarks and six\nindustrial projects demonstrates EvoC2Rust's superior performance in\nproject-level C-to-Rust translation. On average, it achieves 17.24% and 14.32%\nimprovements in syntax and semantic accuracy over the LLM-based approaches,\nalong with a 96.79% higher code safety rate than the rule-based tools. At the\nmodule level, EvoC2Rust reaches 92.25% compilation and 89.53% test pass rates\non industrial projects, even for complex codebases and long functions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04295.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "fullname": "Yuling",
            "name": "YerbaPage",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 276
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.00222",
            "authors": [
                {
                    "_id": "68944a32741a16f544fbcfdc",
                    "name": "Yihong Dong",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfdd",
                    "name": "Xue Jiang",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfde",
                    "name": "Yongding Tao",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfdf",
                    "name": "Huanyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe0",
                    "name": "Kechi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe1",
                    "name": "Lili Mou",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe2",
                    "name": "Rongyu Cao",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe3",
                    "name": "Yingwei Ma",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe4",
                    "name": "Jue Chen",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe5",
                    "name": "Binhua Li",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe6",
                    "name": "Zhi Jin",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe7",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe8",
                    "name": "Yongbin Li",
                    "hidden": false
                },
                {
                    "_id": "68944a32741a16f544fbcfe9",
                    "name": "Ge Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T23:55:29.000Z",
            "submittedOnDailyAt": "2025-08-07T05:10:17.088Z",
            "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization",
            "submittedOnDailyBy": {
                "_id": "62e0ef42edb0462c8d51818d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
                "isPro": false,
                "fullname": "Ting-En Lin",
                "user": "tnlin",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem.",
            "upvotes": 4,
            "discussionId": "68944a32741a16f544fbcfea",
            "ai_summary": "RL-PLUS, a hybrid-policy optimization approach, enhances LLM reasoning capabilities by integrating Multiple Importance Sampling and Exploration-Based Advantage Function, outperforming RLVR on various benchmarks and resolving capability boundary collapse.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Reward (RLVR)",
                "Large Language Models (LLMs)",
                "hybrid-policy optimization",
                "Multiple Importance Sampling",
                "Exploration-Based Advantage Function",
                "capability boundary collapse",
                "math reasoning benchmarks",
                "out-of-distribution reasoning tasks"
            ]
        },
        "publishedAt": "2025-07-31T19:55:29.000Z",
        "title": "RL-PLUS: Countering Capability Boundary Collapse of LLMs in\n  Reinforcement Learning with Hybrid-policy Optimization",
        "summary": "Reinforcement Learning with Verifiable Reward (RLVR) has significantly\nadvanced the complex reasoning abilities of Large Language Models (LLMs).\nHowever, it struggles to break through the inherent capability boundaries of\nthe base LLM, due to its essentially on-policy strategy coupled with LLM's\nimmense action space and sparse reward. Critically, RLVR can lead to the\ncapability boundary collapse, narrowing the LLM's problem-solving scope. To\naddress this problem, we propose RL-PLUS, a novel hybrid-policy optimization\napproach for LLMs that synergizes internal exploitation with external data to\nachieve stronger reasoning capabilities and surpass the boundaries of base\nmodels. RL-PLUS integrates two core components, i.e., Multiple Importance\nSampling to address distributional mismatch from external data, and\nExploration-Based Advantage Function to guide the model towards high-value,\nunexplored reasoning paths. We provide both theoretical analysis and extensive\nexperiments to demonstrate the superiority and generalizability of our\napproach. Compared with existing RLVR methods, RL-PLUS achieves 1)\nstate-of-the-art performance on six math reasoning benchmarks; 2) superior\nperformance on six out-of-distribution reasoning tasks; 3) consistent and\nsignificant gains across diverse model families, with average relative\nimprovements up to 69.2\\%. Moreover, the analysis of Pass@k curves indicates\nthat RL-PLUS effectively resolves the capability boundary collapse problem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00222.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e0ef42edb0462c8d51818d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e0ef42edb0462c8d51818d/3YM7DUynIWiiRFM6_enpg.jpeg",
            "fullname": "Ting-En Lin",
            "name": "tnlin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.21974",
            "authors": [
                {
                    "_id": "6889ded331e1218a089282ea",
                    "name": "Mohamed Sana",
                    "hidden": false
                },
                {
                    "_id": "6889ded331e1218a089282eb",
                    "user": {
                        "_id": "650ab653cc02352e1e43e43e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650ab653cc02352e1e43e43e/Zl_nv3yFWhJYeu3EYKwIU.jpeg",
                        "isPro": false,
                        "fullname": "Nicola Piovesan",
                        "user": "nicopi",
                        "type": "user"
                    },
                    "name": "Nicola Piovesan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:32:40.719Z",
                    "hidden": false
                },
                {
                    "_id": "6889ded331e1218a089282ec",
                    "name": "Antonio De Domenico",
                    "hidden": false
                },
                {
                    "_id": "6889ded331e1218a089282ed",
                    "name": "Yibin Kang",
                    "hidden": false
                },
                {
                    "_id": "6889ded331e1218a089282ee",
                    "name": "Haozhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "6889ded331e1218a089282ef",
                    "name": "Merouane Debbah",
                    "hidden": false
                },
                {
                    "_id": "6889ded331e1218a089282f0",
                    "name": "Fadhel Ayed",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-29T16:21:42.000Z",
            "submittedOnDailyAt": "2025-08-07T08:25:44.480Z",
            "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless\n  Networks",
            "submittedOnDailyBy": {
                "_id": "650ab653cc02352e1e43e43e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650ab653cc02352e1e43e43e/Zl_nv3yFWhJYeu3EYKwIU.jpeg",
                "isPro": false,
                "fullname": "Nicola Piovesan",
                "user": "nicopi",
                "type": "user"
            },
            "summary": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.",
            "upvotes": 4,
            "discussionId": "6889ded331e1218a089282f1",
            "ai_summary": "A lightweight framework using Large Language Models (LLMs) with TeleLogs dataset and a two-stage training methodology improves Root Cause Analysis (RCA) in mobile networks by enhancing interpretability and reasoning quality.",
            "ai_keywords": [
                "Large Language Models",
                "TeleLogs",
                "RCA",
                "domain-specific adaptation",
                "supervised fine-tuning",
                "reinforcement learning",
                "diagnostic explanations",
                "reasoning-enhanced LLMs"
            ]
        },
        "publishedAt": "2025-07-29T12:21:42.000Z",
        "title": "Reasoning Language Models for Root Cause Analysis in 5G Wireless\n  Networks",
        "summary": "Root Cause Analysis (RCA) in mobile networks remains a challenging task due\nto the need for interpretability, domain expertise, and causal reasoning. In\nthis work, we propose a lightweight framework that leverages Large Language\nModels (LLMs) for RCA. To do so, we introduce TeleLogs, a curated dataset of\nannotated troubleshooting problems designed to benchmark RCA capabilities. Our\nevaluation reveals that existing open-source reasoning LLMs struggle with these\nproblems, underscoring the need for domain-specific adaptation. To address this\nissue, we propose a two-stage training methodology that combines supervised\nfine-tuning with reinforcement learning to improve the accuracy and reasoning\nquality of LLMs. The proposed approach fine-tunes a series of RCA models to\nintegrate domain knowledge and generate structured, multi-step diagnostic\nexplanations, improving both interpretability and effectiveness. Extensive\nexperiments across multiple LLM sizes show significant performance gains over\nstate-of-the-art reasoning and non-reasoning models, including strong\ngeneralization to randomized test variants. These results demonstrate the\npromise of domain-adapted, reasoning-enhanced LLMs for practical and\nexplainable RCA in network operation and management.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.21974.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650ab653cc02352e1e43e43e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650ab653cc02352e1e43e43e/Zl_nv3yFWhJYeu3EYKwIU.jpeg",
            "fullname": "Nicola Piovesan",
            "name": "nicopi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04010",
            "authors": [
                {
                    "_id": "6894957b741a16f544fbd0b9",
                    "name": "Yurun Chen",
                    "hidden": false
                },
                {
                    "_id": "6894957b741a16f544fbd0ba",
                    "name": "Xavier Hu",
                    "hidden": false
                },
                {
                    "_id": "6894957b741a16f544fbd0bb",
                    "name": "Yuhan Liu",
                    "hidden": false
                },
                {
                    "_id": "6894957b741a16f544fbd0bc",
                    "name": "Keting Yin",
                    "hidden": false
                },
                {
                    "_id": "6894957b741a16f544fbd0bd",
                    "name": "Juncheng Li",
                    "hidden": false
                },
                {
                    "_id": "6894957b741a16f544fbd0be",
                    "name": "Zhuosheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6894957b741a16f544fbd0bf",
                    "name": "Shengyu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T01:49:32.000Z",
            "submittedOnDailyAt": "2025-08-07T13:38:17.766Z",
            "title": "HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive\n  Policy Enhancement and Dual-Objective Optimization",
            "submittedOnDailyBy": {
                "_id": "6892b4459f604ad07412f117",
                "avatarUrl": "/avatars/fb9e54e1689b4add084e32d7e5dd2f16.svg",
                "isPro": false,
                "fullname": "Xavier Hu",
                "user": "xavier-hu",
                "type": "user"
            },
            "summary": "Large language models enable agents to autonomously perform tasks in open web\nenvironments. However, as hidden threats within the web evolve, web agents face\nthe challenge of balancing task performance with emerging risks during\nlong-sequence operations. Although this challenge is critical, current research\nremains limited to single-objective optimization or single-turn scenarios,\nlacking the capability for collaborative optimization of both safety and\nutility in web environments. To address this gap, we propose HarmonyGuard, a\nmulti-agent collaborative framework that leverages policy enhancement and\nobjective optimization to jointly improve both utility and safety. HarmonyGuard\nfeatures a multi-agent architecture characterized by two fundamental\ncapabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent\nwithin HarmonyGuard, which automatically extracts and maintains structured\nsecurity policies from unstructured external documents, while continuously\nupdating policies in response to evolving threats. (2) Dual-Objective\nOptimization: Based on the dual objectives of safety and utility, the Utility\nAgent integrated within HarmonyGuard performs the Markovian real-time reasoning\nto evaluate the objectives and utilizes metacognitive capabilities for their\noptimization. Extensive evaluations on multiple benchmarks show that\nHarmonyGuard improves policy compliance by up to 38% and task completion by up\nto 20% over existing baselines, while achieving over 90% policy compliance\nacross all tasks. Our project is available here:\nhttps://github.com/YurunChen/HarmonyGuard.",
            "upvotes": 3,
            "discussionId": "6894957d741a16f544fbd0c0",
            "githubRepo": "https://github.com/YurunChen/HarmonyGuard",
            "ai_summary": "HarmonyGuard is a multi-agent framework that enhances policy compliance and task completion in web environments by adaptively updating security policies and optimizing dual objectives of safety and utility.",
            "ai_keywords": [
                "large language models",
                "web agents",
                "hidden threats",
                "multi-agent collaborative framework",
                "policy enhancement",
                "objective optimization",
                "Policy Agent",
                "Utility Agent",
                "Markovian real-time reasoning",
                "metacognitive capabilities",
                "policy compliance",
                "task completion"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-08-05T21:49:32.000Z",
        "title": "HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive\n  Policy Enhancement and Dual-Objective Optimization",
        "summary": "Large language models enable agents to autonomously perform tasks in open web\nenvironments. However, as hidden threats within the web evolve, web agents face\nthe challenge of balancing task performance with emerging risks during\nlong-sequence operations. Although this challenge is critical, current research\nremains limited to single-objective optimization or single-turn scenarios,\nlacking the capability for collaborative optimization of both safety and\nutility in web environments. To address this gap, we propose HarmonyGuard, a\nmulti-agent collaborative framework that leverages policy enhancement and\nobjective optimization to jointly improve both utility and safety. HarmonyGuard\nfeatures a multi-agent architecture characterized by two fundamental\ncapabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent\nwithin HarmonyGuard, which automatically extracts and maintains structured\nsecurity policies from unstructured external documents, while continuously\nupdating policies in response to evolving threats. (2) Dual-Objective\nOptimization: Based on the dual objectives of safety and utility, the Utility\nAgent integrated within HarmonyGuard performs the Markovian real-time reasoning\nto evaluate the objectives and utilizes metacognitive capabilities for their\noptimization. Extensive evaluations on multiple benchmarks show that\nHarmonyGuard improves policy compliance by up to 38% and task completion by up\nto 20% over existing baselines, while achieving over 90% policy compliance\nacross all tasks. Our project is available here:\nhttps://github.com/YurunChen/HarmonyGuard.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04010.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6892b4459f604ad07412f117",
            "avatarUrl": "/avatars/fb9e54e1689b4add084e32d7e5dd2f16.svg",
            "fullname": "Xavier Hu",
            "name": "xavier-hu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01197",
            "authors": [
                {
                    "_id": "6894158a741a16f544fbcedd",
                    "name": "Zhan Shi",
                    "hidden": false
                },
                {
                    "_id": "6894158a741a16f544fbcede",
                    "user": {
                        "_id": "66863d26e2b71e3d09189ae9",
                        "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
                        "isPro": false,
                        "fullname": "Song Wang",
                        "user": "songw-zju",
                        "type": "user"
                    },
                    "name": "Song Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:41.313Z",
                    "hidden": false
                },
                {
                    "_id": "6894158a741a16f544fbcedf",
                    "name": "Junbo Chen",
                    "hidden": false
                },
                {
                    "_id": "6894158a741a16f544fbcee0",
                    "name": "Jianke Zhu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T05:05:50.000Z",
            "submittedOnDailyAt": "2025-08-07T01:25:50.658Z",
            "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
            "submittedOnDailyBy": {
                "_id": "66863d26e2b71e3d09189ae9",
                "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
                "isPro": false,
                "fullname": "Song Wang",
                "user": "songw-zju",
                "type": "user"
            },
            "summary": "Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.",
            "upvotes": 3,
            "discussionId": "6894158a741a16f544fbcee1",
            "githubRepo": "https://github.com/RONINGOD/GroundingOcc",
            "ai_summary": "A benchmark and model for 3D occupancy grounding using natural language and voxel-level annotations improve object perception in autonomous driving.",
            "ai_keywords": [
                "visual grounding",
                "3D occupancy grounding",
                "nuScenes dataset",
                "voxel-level occupancy",
                "multimodal learning",
                "multimodal encoder",
                "occupancy head",
                "grounding head",
                "2D grounding module",
                "depth estimation module"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-08-02T01:05:50.000Z",
        "title": "A Coarse-to-Fine Approach to Multi-Modality 3D Occupancy Grounding",
        "summary": "Visual grounding aims to identify objects or regions in a scene based on\nnatural language descriptions, essential for spatially aware perception in\nautonomous driving. However, existing visual grounding tasks typically depend\non bounding boxes that often fail to capture fine-grained details. Not all\nvoxels within a bounding box are occupied, resulting in inaccurate object\nrepresentations. To address this, we introduce a benchmark for 3D occupancy\ngrounding in challenging outdoor scenes. Built on the nuScenes dataset, it\nintegrates natural language with voxel-level occupancy annotations, offering\nmore precise object perception compared to the traditional grounding task.\nMoreover, we propose GroundingOcc, an end-to-end model designed for 3D\noccupancy grounding through multi-modal learning. It combines visual, textual,\nand point cloud features to predict object location and occupancy information\nfrom coarse to fine. Specifically, GroundingOcc comprises a multimodal encoder\nfor feature extraction, an occupancy head for voxel-wise predictions, and a\ngrounding head to refine localization. Additionally, a 2D grounding module and\na depth estimation module enhance geometric understanding, thereby boosting\nmodel performance. Extensive experiments on the benchmark demonstrate that our\nmethod outperforms existing baselines on 3D occupancy grounding. The dataset is\navailable at https://github.com/RONINGOD/GroundingOcc.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01197.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66863d26e2b71e3d09189ae9",
            "avatarUrl": "/avatars/3c0e6f30e053f2e622ae75e1dc43edba.svg",
            "fullname": "Song Wang",
            "name": "songw-zju",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04632",
            "authors": [
                {
                    "_id": "689458cc741a16f544fbd02e",
                    "name": "Xu Guo",
                    "hidden": false
                },
                {
                    "_id": "689458cc741a16f544fbd02f",
                    "name": "Tianyi Liang",
                    "hidden": false
                },
                {
                    "_id": "689458cc741a16f544fbd030",
                    "name": "Tong Jian",
                    "hidden": false
                },
                {
                    "_id": "689458cc741a16f544fbd031",
                    "name": "Xiaogui Yang",
                    "hidden": false
                },
                {
                    "_id": "689458cc741a16f544fbd032",
                    "name": "Ling-I Wu",
                    "hidden": false
                },
                {
                    "_id": "689458cc741a16f544fbd033",
                    "name": "Chenhui Li",
                    "hidden": false
                },
                {
                    "_id": "689458cc741a16f544fbd034",
                    "name": "Zhihui Lu",
                    "hidden": false
                },
                {
                    "_id": "689458cc741a16f544fbd035",
                    "name": "Qipeng Guo",
                    "hidden": false
                },
                {
                    "_id": "689458cc741a16f544fbd036",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T17:00:54.000Z",
            "submittedOnDailyAt": "2025-08-07T09:19:32.034Z",
            "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards",
            "submittedOnDailyBy": {
                "_id": "62c14609ac1b639c2d87192c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
                "isPro": false,
                "fullname": "liangtianyi",
                "user": "tianyilt",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.",
            "upvotes": 2,
            "discussionId": "689458cd741a16f544fbd037",
            "ai_summary": "Instruction Following Decorator enhances RLVR by improving sample efficiency, intent alignment, and reducing reward hacking in large language models.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "large language models",
                "LLMs",
                "cooperative-adversarial data flywheel",
                "hybrid verifications",
                "IntentCheck",
                "trip wires",
                "reward hacking",
                "IFEval",
                "FollowBench"
            ]
        },
        "publishedAt": "2025-08-06T13:00:54.000Z",
        "title": "IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with\n  Verifiable Rewards",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction\nfollowing capabilities of large language models (LLMs), but suffers from\ntraining inefficiency due to inadequate difficulty assessment. Moreover, RLVR\nis prone to over-optimization, where LLMs exploit verification shortcuts\nwithout aligning to the actual intent of user instructions. We introduce\nInstruction Following Decorator (IFDecorator}, a framework that wraps RLVR\ntraining into a robust and sample-efficient pipeline. It consists of three\ncomponents: (1) a cooperative-adversarial data flywheel that co-evolves\ninstructions and hybrid verifications, generating progressively more\nchallenging instruction-verification pairs; (2) IntentCheck, a bypass module\nenforcing intent alignment; and (3) trip wires, a diagnostic mechanism that\ndetects reward hacking via trap instructions, which trigger and capture\nshortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves\n87.43% accuracy on IFEval, outperforming larger proprietary models such as\nGPT-4o. Additionally, we demonstrate substantial improvements on FollowBench\nwhile preserving general capabilities. Our trip wires show significant\nreductions in reward hacking rates. We will release models, code, and data for\nfuture research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04632.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c14609ac1b639c2d87192c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
            "fullname": "liangtianyi",
            "name": "tianyilt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.03983",
            "authors": [
                {
                    "_id": "68947602741a16f544fbd075",
                    "name": "Heinrich Dinkel",
                    "hidden": false
                },
                {
                    "_id": "68947602741a16f544fbd076",
                    "name": "Gang Li",
                    "hidden": false
                },
                {
                    "_id": "68947602741a16f544fbd077",
                    "name": "Jizhong Liu",
                    "hidden": false
                },
                {
                    "_id": "68947602741a16f544fbd078",
                    "name": "Jian Luan",
                    "hidden": false
                },
                {
                    "_id": "68947602741a16f544fbd079",
                    "name": "Yadong Niu",
                    "hidden": false
                },
                {
                    "_id": "68947602741a16f544fbd07a",
                    "name": "Xingwei Sun",
                    "hidden": false
                },
                {
                    "_id": "68947602741a16f544fbd07b",
                    "name": "Tianzi Wang",
                    "hidden": false
                },
                {
                    "_id": "68947602741a16f544fbd07c",
                    "name": "Qiyang Xiao",
                    "hidden": false
                },
                {
                    "_id": "68947602741a16f544fbd07d",
                    "name": "Junbo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68947602741a16f544fbd07e",
                    "name": "Jiahao Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T00:30:19.000Z",
            "submittedOnDailyAt": "2025-08-07T08:35:29.901Z",
            "title": "MiDashengLM: Efficient Audio Understanding with General Audio Captions",
            "submittedOnDailyBy": {
                "_id": "62ba6e4b0e2fb1dcddca938a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656385205895-62ba6e4b0e2fb1dcddca938a.jpeg",
                "isPro": false,
                "fullname": "Junbo Zhang",
                "user": "jimbozhang",
                "type": "user"
            },
            "summary": "Current approaches for large audio language models (LALMs) often rely on\nclosed data sources or proprietary models, limiting their generalization and\naccessibility. This paper introduces MiDashengLM, a novel open audio-language\nmodel designed for efficient and comprehensive audio understanding through the\nuse of general audio captions using our novel ACAVCaps training dataset.\nMiDashengLM exclusively relies on publicly available pretraining and supervised\nfine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At\nits core, MiDashengLM integrates Dasheng, an open-source audio encoder,\nspecifically engineered to process diverse auditory information effectively.\nUnlike previous works primarily focused on Automatic Speech Recognition (ASR)\nbased audio-text alignment, our strategy centers on general audio captions,\nfusing speech, sound and music information into one textual representation,\nenabling a holistic textual representation of complex audio scenes. Lastly,\nMiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT)\nand up to 20x higher throughput than comparable models. Checkpoints are\navailable online at https://huggingface.co/mispeech/midashenglm-7b and\nhttps://github.com/xiaomi-research/dasheng-lm.",
            "upvotes": 2,
            "discussionId": "68947602741a16f544fbd07f",
            "githubRepo": "https://github.com/xiaomi-research/dasheng-lm",
            "ai_summary": "MiDashengLM is an open audio-language model using general audio captions for efficient and comprehensive audio understanding, offering faster processing and higher throughput compared to existing models.",
            "ai_keywords": [
                "audio language models",
                "ACAVCaps",
                "Dasheng",
                "audio encoder",
                "Automatic Speech Recognition",
                "audio-text alignment",
                "general audio captions",
                "time-to-first-token",
                "throughput"
            ],
            "githubStars": 260
        },
        "publishedAt": "2025-08-05T20:30:19.000Z",
        "title": "MiDashengLM: Efficient Audio Understanding with General Audio Captions",
        "summary": "Current approaches for large audio language models (LALMs) often rely on\nclosed data sources or proprietary models, limiting their generalization and\naccessibility. This paper introduces MiDashengLM, a novel open audio-language\nmodel designed for efficient and comprehensive audio understanding through the\nuse of general audio captions using our novel ACAVCaps training dataset.\nMiDashengLM exclusively relies on publicly available pretraining and supervised\nfine-tuning (SFT) datasets, ensuring full transparency and reproducibility. At\nits core, MiDashengLM integrates Dasheng, an open-source audio encoder,\nspecifically engineered to process diverse auditory information effectively.\nUnlike previous works primarily focused on Automatic Speech Recognition (ASR)\nbased audio-text alignment, our strategy centers on general audio captions,\nfusing speech, sound and music information into one textual representation,\nenabling a holistic textual representation of complex audio scenes. Lastly,\nMiDashengLM provides an up to 4x speedup in terms of time-to-first-token (TTFT)\nand up to 20x higher throughput than comparable models. Checkpoints are\navailable online at https://huggingface.co/mispeech/midashenglm-7b and\nhttps://github.com/xiaomi-research/dasheng-lm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03983.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ba6e4b0e2fb1dcddca938a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656385205895-62ba6e4b0e2fb1dcddca938a.jpeg",
            "fullname": "Junbo Zhang",
            "name": "jimbozhang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.03178",
            "authors": [
                {
                    "_id": "6892ca068da45ffb0a2b2489",
                    "name": "Chenyang Wang",
                    "hidden": false
                },
                {
                    "_id": "6892ca068da45ffb0a2b248a",
                    "name": "Liang Wen",
                    "hidden": false
                },
                {
                    "_id": "6892ca068da45ffb0a2b248b",
                    "name": "Shousheng Jia",
                    "hidden": false
                },
                {
                    "_id": "6892ca068da45ffb0a2b248c",
                    "name": "Xiangzheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "6892ca068da45ffb0a2b248d",
                    "name": "Liang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T07:42:00.000Z",
            "submittedOnDailyAt": "2025-08-07T09:46:29.628Z",
            "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and\n  Self-Checking for Complex Instruction Following",
            "submittedOnDailyBy": {
                "_id": "64eeb81ad0ceda46832e0160",
                "avatarUrl": "/avatars/bbeb6a1582071dffcf3fca9780df5138.svg",
                "isPro": false,
                "fullname": "wenliang",
                "user": "wenliang1990",
                "type": "user"
            },
            "summary": "While advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifies lazy reasoning during the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involving preview and self-checking, essential for\nsatisfying strict instruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply a filtering process to obtain\nvalid prompts, resulting in three distinct prompt datasets categorized as hard,\neasy, and pass. Then, we employ rejection sampling on the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ an entropy-preserving supervised fine-tuning\n(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)\nreinforcement learning guided by rule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompass preview and self-checking.\nExtensive experiments conducted on instruction-following benchmarks demonstrate\nremarkable performance improvements across various model scales. Notably, our\nLight-IF-32B model surpasses both larger open-source models such as DeepSeek-R1\nand closed-source models like Doubao-1.6.",
            "upvotes": 2,
            "discussionId": "6892ca068da45ffb0a2b248e",
            "ai_summary": "A framework using entropy-preserving supervised fine-tuning and token-wise entropy-adaptive reinforcement learning improves instruction adherence in LLMs by fostering rigorous reasoning processes.",
            "ai_keywords": [
                "lazy reasoning",
                "rigorous reasoning",
                "preview",
                "self-checking",
                "instruction constraints",
                "filtering process",
                "prompt datasets",
                "rejection sampling",
                "entropy-preserving supervised fine-tuning",
                "Entropy-SFT",
                "token-wise entropy-adaptive",
                "TEA-RL",
                "reinforcement learning",
                "rule-based dense rewards",
                "instruction-following benchmarks",
                "Light-IF-32B",
                "DeepSeek-R1",
                "Doubao-1.6"
            ]
        },
        "publishedAt": "2025-08-05T03:42:00.000Z",
        "title": "Light-IF: Endowing LLMs with Generalizable Reasoning via Preview and\n  Self-Checking for Complex Instruction Following",
        "summary": "While advancements in the reasoning abilities of LLMs have significantly\nenhanced their performance in solving mathematical problems, coding tasks, and\ngeneral puzzles, their effectiveness in accurately adhering to instructions\nremains inconsistent, particularly with more complex directives. Our\ninvestigation identifies lazy reasoning during the thinking stage as the\nprimary factor contributing to poor instruction adherence. To mitigate this\nissue, we propose a comprehensive framework designed to enable rigorous\nreasoning processes involving preview and self-checking, essential for\nsatisfying strict instruction constraints. Specifically, we first generate\ninstructions with complex constraints and apply a filtering process to obtain\nvalid prompts, resulting in three distinct prompt datasets categorized as hard,\neasy, and pass. Then, we employ rejection sampling on the pass prompts to\ncurate a small yet high-quality dataset, enabling a cold-start initialization\nof the model and facilitating its adaptation to effective reasoning patterns.\nSubsequently, we employ an entropy-preserving supervised fine-tuning\n(Entropy-SFT) strategy coupled with token-wise entropy-adaptive (TEA-RL)\nreinforcement learning guided by rule-based dense rewards. This approach\nencourages the model to transform its reasoning mechanism, ultimately fostering\ngeneralizable reasoning abilities that encompass preview and self-checking.\nExtensive experiments conducted on instruction-following benchmarks demonstrate\nremarkable performance improvements across various model scales. Notably, our\nLight-IF-32B model surpasses both larger open-source models such as DeepSeek-R1\nand closed-source models like Doubao-1.6.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03178.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64eeb81ad0ceda46832e0160",
            "avatarUrl": "/avatars/bbeb6a1582071dffcf3fca9780df5138.svg",
            "fullname": "wenliang",
            "name": "wenliang1990",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01778",
            "authors": [
                {
                    "_id": "68943a7a741a16f544fbcf96",
                    "user": {
                        "_id": "6819f16fa41ec6a2ca4cb88d",
                        "avatarUrl": "/avatars/8ba11096e942aafd32025fe0655a2bae.svg",
                        "isPro": false,
                        "fullname": "SunZhigang",
                        "user": "SunZhigang7",
                        "type": "user"
                    },
                    "name": "Zhigang Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:37:52.546Z",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcf97",
                    "name": "Yiru Wang",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcf98",
                    "name": "Anqing Jiang",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcf99",
                    "name": "Shuo Wang",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcf9a",
                    "name": "Yu Gao",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcf9b",
                    "name": "Yuwen Heng",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcf9c",
                    "name": "Shouyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcf9d",
                    "name": "An He",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcf9e",
                    "name": "Hao Jiang",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcf9f",
                    "name": "Jinhao Chai",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcfa0",
                    "name": "Zichong Gu",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcfa1",
                    "name": "Wang Jijun",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcfa2",
                    "name": "Shichen Tang",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcfa3",
                    "name": "Lavdim Halilaj",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcfa4",
                    "name": "Juergen Luettin",
                    "hidden": false
                },
                {
                    "_id": "68943a7a741a16f544fbcfa5",
                    "name": "Hao Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-03T14:32:05.000Z",
            "submittedOnDailyAt": "2025-08-07T13:33:34.375Z",
            "title": "DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving\n  via Online HD Map Diffusion",
            "submittedOnDailyBy": {
                "_id": "6819f16fa41ec6a2ca4cb88d",
                "avatarUrl": "/avatars/8ba11096e942aafd32025fe0655a2bae.svg",
                "isPro": false,
                "fullname": "SunZhigang",
                "user": "SunZhigang7",
                "type": "user"
            },
            "summary": "Autonomous driving requires accurate scene understanding, including road\ngeometry, traffic agents, and their semantic relationships. In online HD map\ngeneration scenarios, raster-based representations are well-suited to vision\nmodels but lack geometric precision, while graph-based representations retain\nstructural detail but become unstable without precise maps. To harness the\ncomplementary strengths of both, we propose DiffSemanticFusion -- a fusion\nframework for multimodal trajectory prediction and planning. Our approach\nreasons over a semantic raster-fused BEV space, enhanced by a map diffusion\nmodule that improves both the stability and expressiveness of online HD map\nrepresentations. We validate our framework on two downstream tasks: trajectory\nprediction and planning-oriented end-to-end autonomous driving. Experiments on\nreal-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate\nimproved performance over several state-of-the-art methods. For the prediction\ntask on nuScenes, we integrate DiffSemanticFusion with the online HD map\ninformed QCNet, achieving a 5.1\\% performance improvement. For end-to-end\nautonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art\nresults, with a 15\\% performance gain in NavHard scenarios. In addition,\nextensive ablation and sensitivity studies show that our map diffusion module\ncan be seamlessly integrated into other vector-based approaches to enhance\nperformance. All artifacts are available at\nhttps://github.com/SunZhigang7/DiffSemanticFusion.",
            "upvotes": 2,
            "discussionId": "68943a7b741a16f544fbcfa6",
            "githubRepo": "https://github.com/SunZhigang7/DiffSemanticFusion",
            "ai_summary": "DiffSemanticFusion enhances autonomous driving by fusing semantic raster and graph-based representations using a map diffusion module, improving trajectory prediction and end-to-end driving performance.",
            "ai_keywords": [
                "DiffSemanticFusion",
                "semantic raster-fused BEV space",
                "map diffusion module",
                "trajectory prediction",
                "end-to-end autonomous driving",
                "nuScenes",
                "NAVSIM",
                "NavHard scenarios"
            ],
            "githubStars": 15
        },
        "publishedAt": "2025-08-03T10:32:05.000Z",
        "title": "DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving\n  via Online HD Map Diffusion",
        "summary": "Autonomous driving requires accurate scene understanding, including road\ngeometry, traffic agents, and their semantic relationships. In online HD map\ngeneration scenarios, raster-based representations are well-suited to vision\nmodels but lack geometric precision, while graph-based representations retain\nstructural detail but become unstable without precise maps. To harness the\ncomplementary strengths of both, we propose DiffSemanticFusion -- a fusion\nframework for multimodal trajectory prediction and planning. Our approach\nreasons over a semantic raster-fused BEV space, enhanced by a map diffusion\nmodule that improves both the stability and expressiveness of online HD map\nrepresentations. We validate our framework on two downstream tasks: trajectory\nprediction and planning-oriented end-to-end autonomous driving. Experiments on\nreal-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate\nimproved performance over several state-of-the-art methods. For the prediction\ntask on nuScenes, we integrate DiffSemanticFusion with the online HD map\ninformed QCNet, achieving a 5.1\\% performance improvement. For end-to-end\nautonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art\nresults, with a 15\\% performance gain in NavHard scenarios. In addition,\nextensive ablation and sensitivity studies show that our map diffusion module\ncan be seamlessly integrated into other vector-based approaches to enhance\nperformance. All artifacts are available at\nhttps://github.com/SunZhigang7/DiffSemanticFusion.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01778.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6819f16fa41ec6a2ca4cb88d",
            "avatarUrl": "/avatars/8ba11096e942aafd32025fe0655a2bae.svg",
            "fullname": "SunZhigang",
            "name": "SunZhigang7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.01630",
            "authors": [
                {
                    "_id": "68916f8df01a094725f8347f",
                    "user": {
                        "_id": "5fd5e18a90b6dc4633f6d292",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png",
                        "isPro": true,
                        "fullname": "Maziyar Panahi",
                        "user": "MaziyarPanahi",
                        "type": "user"
                    },
                    "name": "Maziyar Panahi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:22:17.859Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-03T07:33:28.000Z",
            "submittedOnDailyAt": "2025-08-07T03:31:30.498Z",
            "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets",
            "submittedOnDailyBy": {
                "_id": "5fd5e18a90b6dc4633f6d292",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png",
                "isPro": true,
                "fullname": "Maziyar Panahi",
                "user": "MaziyarPanahi",
                "type": "user"
            },
            "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.",
            "upvotes": 2,
            "discussionId": "68916f8df01a094725f83480",
            "projectPage": "https://huggingface.co/OpenMed",
            "ai_summary": "OpenMed NER, a suite of open-source transformer models using DAPT and LoRA, achieves state-of-the-art performance on diverse biomedical NER benchmarks with high efficiency and low computational cost.",
            "ai_keywords": [
                "transformer models",
                "lightweight domain-adaptive pre-training (DAPT)",
                "parameter-efficient Low-Rank Adaptation (LoRA)",
                "DeBERTa-v3",
                "PubMedBERT",
                "BioELECTRA",
                "micro-F1 scores",
                "BC5CDR-Disease",
                "gene",
                "clinical cell line corpora",
                "open-source checkpoints",
                "EU AI Act"
            ]
        },
        "publishedAt": "2025-08-03T03:33:28.000Z",
        "title": "OpenMed NER: Open-Source, Domain-Adapted State-of-the-Art Transformers\n  for Biomedical NER Across 12 Public Datasets",
        "summary": "Named-entity recognition (NER) is fundamental to extracting structured\ninformation from the >80% of healthcare data that resides in unstructured\nclinical notes and biomedical literature. Despite recent advances with large\nlanguage models, achieving state-of-the-art performance across diverse entity\ntypes while maintaining computational efficiency remains a significant\nchallenge. We introduce OpenMed NER, a suite of open-source, domain-adapted\ntransformer models that combine lightweight domain-adaptive pre-training (DAPT)\nwith parameter-efficient Low-Rank Adaptation (LoRA). Our approach performs\ncost-effective DAPT on a 350k-passage corpus compiled from ethically sourced,\npublicly available research repositories and de-identified clinical notes\n(PubMed, arXiv, and MIMIC-III) using DeBERTa-v3, PubMedBERT, and BioELECTRA\nbackbones. This is followed by task-specific fine-tuning with LoRA, which\nupdates less than 1.5% of model parameters. We evaluate our models on 12\nestablished biomedical NER benchmarks spanning chemicals, diseases, genes, and\nspecies. OpenMed NER achieves new state-of-the-art micro-F1 scores on 10 of\nthese 12 datasets, with substantial gains across diverse entity types. Our\nmodels advance the state-of-the-art on foundational disease and chemical\nbenchmarks (e.g., BC5CDR-Disease, +2.70 pp), while delivering even larger\nimprovements of over 5.3 and 9.7 percentage points on more specialized gene and\nclinical cell line corpora. This work demonstrates that strategically adapted\nopen-source models can surpass closed-source solutions. This performance is\nachieved with remarkable efficiency: training completes in under 12 hours on a\nsingle GPU with a low carbon footprint (< 1.2 kg CO2e), producing permissively\nlicensed, open-source checkpoints designed to help practitioners facilitate\ncompliance with emerging data protection and AI regulations, such as the EU AI\nAct.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01630.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "5fd5e18a90b6dc4633f6d292",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fd5e18a90b6dc4633f6d292/gZXHW5dd9R86AV9LMZ--y.png",
            "fullname": "Maziyar Panahi",
            "name": "MaziyarPanahi",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3553
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.00599",
            "authors": [
                {
                    "_id": "689188a5f01a094725f83540",
                    "user": {
                        "_id": "653f872dde20728b2714357a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sVFCwTSQyA1hNVbyIennB.jpeg",
                        "isPro": false,
                        "fullname": "Junzhe Lu",
                        "user": "Moon-bow",
                        "type": "user"
                    },
                    "name": "Junzhe Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:42:35.914Z",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f83541",
                    "name": "Jing Lin",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f83542",
                    "name": "Hongkun Dou",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f83543",
                    "name": "Ailing Zeng",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f83544",
                    "name": "Yue Deng",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f83545",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f83546",
                    "name": "Zhongang Cai",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f83547",
                    "name": "Lei Yang",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f83548",
                    "name": "Yulun Zhang",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f83549",
                    "name": "Haoqian Wang",
                    "hidden": false
                },
                {
                    "_id": "689188a5f01a094725f8354a",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T12:56:39.000Z",
            "submittedOnDailyAt": "2025-08-07T16:05:22.445Z",
            "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior",
            "submittedOnDailyBy": {
                "_id": "653f872dde20728b2714357a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sVFCwTSQyA1hNVbyIennB.jpeg",
                "isPro": false,
                "fullname": "Junzhe Lu",
                "user": "Moon-bow",
                "type": "user"
            },
            "summary": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling.",
            "upvotes": 2,
            "discussionId": "689188a5f01a094725f8354b",
            "projectPage": "https://dposer.github.io/",
            "githubRepo": "https://github.com/moonbow721/DPoser-X",
            "ai_summary": "DPoser-X, a diffusion-based model, addresses the complexity of 3D human poses using variational diffusion sampling and a novel truncated timestep scheduling method, outperforming existing models across various pose benchmarks.",
            "ai_keywords": [
                "diffusion-based prior model",
                "variational diffusion sampling",
                "truncated timestep scheduling",
                "masked training mechanism",
                "whole-body human pose modeling",
                "body",
                "hand",
                "face",
                "full-body pose modeling"
            ],
            "githubStars": 98
        },
        "publishedAt": "2025-08-01T08:56:39.000Z",
        "title": "DPoser-X: Diffusion Model as Robust 3D Whole-body Human Pose Prior",
        "summary": "We present DPoser-X, a diffusion-based prior model for 3D whole-body human\nposes. Building a versatile and robust full-body human pose prior remains\nchallenging due to the inherent complexity of articulated human poses and the\nscarcity of high-quality whole-body pose datasets. To address these\nlimitations, we introduce a Diffusion model as body Pose prior (DPoser) and\nextend it to DPoser-X for expressive whole-body human pose modeling. Our\napproach unifies various pose-centric tasks as inverse problems, solving them\nthrough variational diffusion sampling. To enhance performance on downstream\napplications, we introduce a novel truncated timestep scheduling method\nspecifically designed for pose data characteristics. We also propose a masked\ntraining mechanism that effectively combines whole-body and part-specific\ndatasets, enabling our model to capture interdependencies between body parts\nwhile avoiding overfitting to specific actions. Extensive experiments\ndemonstrate DPoser-X's robustness and versatility across multiple benchmarks\nfor body, hand, face, and full-body pose modeling. Our model consistently\noutperforms state-of-the-art alternatives, establishing a new benchmark for\nwhole-body human pose prior modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00599.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "653f872dde20728b2714357a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/sVFCwTSQyA1hNVbyIennB.jpeg",
            "fullname": "Junzhe Lu",
            "name": "Moon-bow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.00428",
            "authors": [
                {
                    "_id": "6893117e8da45ffb0a2b253f",
                    "name": "Nan Xiang",
                    "hidden": false
                },
                {
                    "_id": "6893117e8da45ffb0a2b2540",
                    "user": {
                        "_id": "62c14609ac1b639c2d87192c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
                        "isPro": false,
                        "fullname": "liangtianyi",
                        "user": "tianyilt",
                        "type": "user"
                    },
                    "name": "Tianyi Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:15:16.963Z",
                    "hidden": false
                },
                {
                    "_id": "6893117e8da45ffb0a2b2541",
                    "name": "Haiwen Huang",
                    "hidden": false
                },
                {
                    "_id": "6893117e8da45ffb0a2b2542",
                    "name": "Shiqi Jiang",
                    "hidden": false
                },
                {
                    "_id": "6893117e8da45ffb0a2b2543",
                    "name": "Hao Huang",
                    "hidden": false
                },
                {
                    "_id": "6893117e8da45ffb0a2b2544",
                    "name": "Yifei Huang",
                    "hidden": false
                },
                {
                    "_id": "6893117e8da45ffb0a2b2545",
                    "name": "Liangyu Chen",
                    "hidden": false
                },
                {
                    "_id": "6893117e8da45ffb0a2b2546",
                    "name": "Changbo Wang",
                    "hidden": false
                },
                {
                    "_id": "6893117e8da45ffb0a2b2547",
                    "name": "Chenhui Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-01T08:36:15.000Z",
            "submittedOnDailyAt": "2025-08-07T09:55:40.648Z",
            "title": "Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D\n  Generation",
            "submittedOnDailyBy": {
                "_id": "62c14609ac1b639c2d87192c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
                "isPro": false,
                "fullname": "liangtianyi",
                "user": "tianyilt",
                "type": "user"
            },
            "summary": "Text-to-3D (T23D) generation has transformed digital content creation, yet\nremains bottlenecked by blind trial-and-error prompting processes that yield\nunpredictable results. While visual prompt engineering has advanced in\ntext-to-image domains, its application to 3D generation presents unique\nchallenges requiring multi-view consistency evaluation and spatial\nunderstanding. We present Sel3DCraft, a visual prompt engineering system for\nT23D that transforms unstructured exploration into a guided visual process. Our\napproach introduces three key innovations: a dual-branch structure combining\nretrieval and generation for diverse candidate exploration; a multi-view hybrid\nscoring approach that leverages MLLMs with innovative high-level metrics to\nassess 3D models with human-expert consistency; and a prompt-driven visual\nanalytics suite that enables intuitive defect identification and refinement.\nExtensive testing and user studies demonstrate that Sel3DCraft surpasses other\nT23D systems in supporting creativity for designers.",
            "upvotes": 2,
            "discussionId": "6893117e8da45ffb0a2b2548",
            "ai_summary": "Sel3DCraft enhances text-to-3D generation through a dual-branch retrieval and generation system, multi-view hybrid scoring with MLLMs, and prompt-driven visual analytics, improving designer creativity.",
            "ai_keywords": [
                "dual-branch structure",
                "retrieval",
                "generation",
                "multi-view hybrid scoring",
                "MLLMs",
                "high-level metrics",
                "visual analytics",
                "prompt-driven"
            ]
        },
        "publishedAt": "2025-08-01T04:36:15.000Z",
        "title": "Sel3DCraft: Interactive Visual Prompts for User-Friendly Text-to-3D\n  Generation",
        "summary": "Text-to-3D (T23D) generation has transformed digital content creation, yet\nremains bottlenecked by blind trial-and-error prompting processes that yield\nunpredictable results. While visual prompt engineering has advanced in\ntext-to-image domains, its application to 3D generation presents unique\nchallenges requiring multi-view consistency evaluation and spatial\nunderstanding. We present Sel3DCraft, a visual prompt engineering system for\nT23D that transforms unstructured exploration into a guided visual process. Our\napproach introduces three key innovations: a dual-branch structure combining\nretrieval and generation for diverse candidate exploration; a multi-view hybrid\nscoring approach that leverages MLLMs with innovative high-level metrics to\nassess 3D models with human-expert consistency; and a prompt-driven visual\nanalytics suite that enables intuitive defect identification and refinement.\nExtensive testing and user studies demonstrate that Sel3DCraft surpasses other\nT23D systems in supporting creativity for designers.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00428.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c14609ac1b639c2d87192c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656833489364-noauth.png",
            "fullname": "liangtianyi",
            "name": "tianyilt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04440",
            "authors": [
                {
                    "_id": "6894a81a741a16f544fbd0e1",
                    "name": "Yutong Wu",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0e2",
                    "name": "Di Huang",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0e3",
                    "name": "Ruosi Wan",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0e4",
                    "name": "Yue Peng",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0e5",
                    "name": "Shijie Shang",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0e6",
                    "name": "Chenrui Cao",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0e7",
                    "name": "Lei Qi",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0e8",
                    "name": "Rui Zhang",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0e9",
                    "name": "Zidong Du",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0ea",
                    "name": "Jie Yan",
                    "hidden": false
                },
                {
                    "_id": "6894a81a741a16f544fbd0eb",
                    "name": "Xing Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T13:28:22.000Z",
            "submittedOnDailyAt": "2025-08-07T11:51:04.556Z",
            "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion",
            "submittedOnDailyBy": {
                "_id": "64e42be971139393aa3dd90a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e42be971139393aa3dd90a/N7CwgOiU6rx5FTIfLO1Yo.png",
                "isPro": false,
                "fullname": "Yutong Wu",
                "user": "wyt2000",
                "type": "user"
            },
            "summary": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.",
            "upvotes": 1,
            "discussionId": "6894a81a741a16f544fbd0ec",
            "ai_summary": "ThinkingF, a data synthesis and training pipeline, enhances autoformalization by improving formal knowledge and informal-to-formal reasoning, achieving state-of-the-art results in formalization tasks.",
            "ai_keywords": [
                "LLMs",
                "autoformalization",
                "formal-language domain knowledge",
                "natural language problem understanding",
                "informal-formal alignment",
                "data synthesis",
                "training pipeline",
                "SFT",
                "RLVR",
                "FormalMATH-Lite",
                "ProverBench",
                "StepFun-Formalizer"
            ]
        },
        "publishedAt": "2025-08-06T09:28:22.000Z",
        "title": "StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs\n  through Knowledge-Reasoning Fusion",
        "summary": "Autoformalization aims to translate natural-language mathematical statements\ninto a formal language. While LLMs have accelerated progress in this area,\nexisting methods still suffer from low accuracy. We identify two key abilities\nfor effective autoformalization: comprehensive mastery of formal-language\ndomain knowledge, and reasoning capability of natural language problem\nunderstanding and informal-formal alignment. Without the former, a model cannot\nidentify the correct formal objects; without the latter, it struggles to\ninterpret real-world contexts and map them precisely into formal expressions.\nTo address these gaps, we introduce ThinkingF, a data synthesis and training\npipeline that improves both abilities. First, we construct two datasets: one by\ndistilling and selecting large-scale examples rich in formal knowledge, and\nanother by generating informal-to-formal reasoning trajectories guided by\nexpert-designed templates. We then apply SFT and RLVR with these datasets to\nfurther fuse and refine the two abilities. The resulting 7B and 32B models\nexhibit both comprehensive formal knowledge and strong informal-to-formal\nreasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5%\non FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior\ngeneral-purpose and specialized models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04440.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64e42be971139393aa3dd90a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e42be971139393aa3dd90a/N7CwgOiU6rx5FTIfLO1Yo.png",
            "fullname": "Yutong Wu",
            "name": "wyt2000",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.03970",
            "authors": [
                {
                    "_id": "6895003b48b0ae5ca2710c75",
                    "name": "Alok Abhishek",
                    "hidden": false
                },
                {
                    "_id": "6895003b48b0ae5ca2710c76",
                    "name": "Lisa Erickson",
                    "hidden": false
                },
                {
                    "_id": "6895003b48b0ae5ca2710c77",
                    "name": "Tushar Bandopadhyay",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T23:15:31.000Z",
            "submittedOnDailyAt": "2025-08-07T18:07:20.563Z",
            "title": "Data and AI governance: Promoting equity, ethics, and fairness in large\n  language models",
            "submittedOnDailyBy": {
                "_id": "6478fc1512ae749b62ebbbd5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
                "isPro": false,
                "fullname": "Alok Abhishek",
                "user": "alokabhishek",
                "type": "user"
            },
            "summary": "In this paper, we cover approaches to systematically govern, assess and\nquantify bias across the complete life cycle of machine learning models, from\ninitial development and validation to ongoing production monitoring and\nguardrail implementation. Building upon our foundational work on the Bias\nEvaluation and Assessment Test Suite (BEATS) for Large Language Models, the\nauthors share prevalent bias and fairness related gaps in Large Language Models\n(LLMs) and discuss data and AI governance framework to address Bias, Ethics,\nFairness, and Factuality within LLMs. The data and AI governance approach\ndiscussed in this paper is suitable for practical, real-world applications,\nenabling rigorous benchmarking of LLMs prior to production deployment,\nfacilitating continuous real-time evaluation, and proactively governing LLM\ngenerated responses. By implementing the data and AI governance across the life\ncycle of AI development, organizations can significantly enhance the safety and\nresponsibility of their GenAI systems, effectively mitigating risks of\ndiscrimination and protecting against potential reputational or brand-related\nharm. Ultimately, through this article, we aim to contribute to advancement of\nthe creation and deployment of socially responsible and ethically aligned\ngenerative artificial intelligence powered applications.",
            "upvotes": 1,
            "discussionId": "6895003c48b0ae5ca2710c78",
            "ai_summary": "Approaches to govern, assess, and quantify bias in machine learning models, particularly large language models, are discussed, emphasizing data and AI governance frameworks for ethical deployment.",
            "ai_keywords": [
                "Large Language Models",
                "Bias Evaluation and Assessment Test Suite (BEATS)",
                "data and AI governance",
                "Bias",
                "Ethics",
                "Fairness",
                "Factuality",
                "GenAI systems"
            ]
        },
        "publishedAt": "2025-08-05T19:15:31.000Z",
        "title": "Data and AI governance: Promoting equity, ethics, and fairness in large\n  language models",
        "summary": "In this paper, we cover approaches to systematically govern, assess and\nquantify bias across the complete life cycle of machine learning models, from\ninitial development and validation to ongoing production monitoring and\nguardrail implementation. Building upon our foundational work on the Bias\nEvaluation and Assessment Test Suite (BEATS) for Large Language Models, the\nauthors share prevalent bias and fairness related gaps in Large Language Models\n(LLMs) and discuss data and AI governance framework to address Bias, Ethics,\nFairness, and Factuality within LLMs. The data and AI governance approach\ndiscussed in this paper is suitable for practical, real-world applications,\nenabling rigorous benchmarking of LLMs prior to production deployment,\nfacilitating continuous real-time evaluation, and proactively governing LLM\ngenerated responses. By implementing the data and AI governance across the life\ncycle of AI development, organizations can significantly enhance the safety and\nresponsibility of their GenAI systems, effectively mitigating risks of\ndiscrimination and protecting against potential reputational or brand-related\nharm. Ultimately, through this article, we aim to contribute to advancement of\nthe creation and deployment of socially responsible and ethically aligned\ngenerative artificial intelligence powered applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03970.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6478fc1512ae749b62ebbbd5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6478fc1512ae749b62ebbbd5/zw-wYy1vEWnG9-t6c9W_x.jpeg",
            "fullname": "Alok Abhishek",
            "name": "alokabhishek",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.03448",
            "authors": [
                {
                    "_id": "68943937741a16f544fbcf91",
                    "name": "Jan Melechovsky",
                    "hidden": false
                },
                {
                    "_id": "68943937741a16f544fbcf92",
                    "name": "Ambuj Mehrish",
                    "hidden": false
                },
                {
                    "_id": "68943937741a16f544fbcf93",
                    "user": {
                        "_id": "655431b2997379e9b0999d23",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
                        "isPro": false,
                        "fullname": "Dorien Herremans",
                        "user": "dorienh",
                        "type": "user"
                    },
                    "name": "Dorien Herremans",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:37:57.123Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T13:49:04.000Z",
            "submittedOnDailyAt": "2025-08-07T03:59:29.921Z",
            "title": "SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering",
            "submittedOnDailyBy": {
                "_id": "655431b2997379e9b0999d23",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
                "isPro": false,
                "fullname": "Dorien Herremans",
                "user": "dorienh",
                "type": "user"
            },
            "summary": "Music recordings often suffer from audio quality issues such as excessive\nreverberation, distortion, clipping, tonal imbalances, and a narrowed stereo\nimage, especially when created in non-professional settings without specialized\nequipment or expertise. These problems are typically corrected using separate\nspecialized tools and manual adjustments. In this paper, we introduce\nSonicMaster, the first unified generative model for music restoration and\nmastering that addresses a broad spectrum of audio artifacts with text-based\ncontrol. SonicMaster is conditioned on natural language instructions to apply\ntargeted enhancements, or can operate in an automatic mode for general\nrestoration. To train this model, we construct the SonicMaster dataset, a large\ndataset of paired degraded and high-quality tracks by simulating common\ndegradation types with nineteen degradation functions belonging to five\nenhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our\napproach leverages a flow-matching generative training paradigm to learn an\naudio transformation that maps degraded inputs to their cleaned, mastered\nversions guided by text prompts. Objective audio quality metrics demonstrate\nthat SonicMaster significantly improves sound quality across all artifact\ncategories. Furthermore, subjective listening tests confirm that listeners\nprefer SonicMaster's enhanced outputs over the original degraded audio,\nhighlighting the effectiveness of our unified approach.",
            "upvotes": 1,
            "discussionId": "68943938741a16f544fbcf94",
            "githubRepo": "https://github.com/AMAAI-Lab/SonicMaster",
            "ai_summary": "SonicMaster, a unified generative model, improves music audio quality by addressing various artifacts using text-based control and a flow-matching generative training paradigm.",
            "ai_keywords": [
                "generative model",
                "music restoration",
                "mastering",
                "text-based control",
                "flow-matching",
                "audio transformation",
                "equalization",
                "dynamics",
                "reverb",
                "amplitude",
                "stereo"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-08-05T09:49:04.000Z",
        "title": "SonicMaster: Towards Controllable All-in-One Music Restoration and\n  Mastering",
        "summary": "Music recordings often suffer from audio quality issues such as excessive\nreverberation, distortion, clipping, tonal imbalances, and a narrowed stereo\nimage, especially when created in non-professional settings without specialized\nequipment or expertise. These problems are typically corrected using separate\nspecialized tools and manual adjustments. In this paper, we introduce\nSonicMaster, the first unified generative model for music restoration and\nmastering that addresses a broad spectrum of audio artifacts with text-based\ncontrol. SonicMaster is conditioned on natural language instructions to apply\ntargeted enhancements, or can operate in an automatic mode for general\nrestoration. To train this model, we construct the SonicMaster dataset, a large\ndataset of paired degraded and high-quality tracks by simulating common\ndegradation types with nineteen degradation functions belonging to five\nenhancements groups: equalization, dynamics, reverb, amplitude, and stereo. Our\napproach leverages a flow-matching generative training paradigm to learn an\naudio transformation that maps degraded inputs to their cleaned, mastered\nversions guided by text prompts. Objective audio quality metrics demonstrate\nthat SonicMaster significantly improves sound quality across all artifact\ncategories. Furthermore, subjective listening tests confirm that listeners\nprefer SonicMaster's enhanced outputs over the original degraded audio,\nhighlighting the effectiveness of our unified approach.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03448.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655431b2997379e9b0999d23",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/655431b2997379e9b0999d23/OhHCPCXfOS61Z7CSewUCU.png",
            "fullname": "Dorien Herremans",
            "name": "dorienh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.01311",
            "authors": [
                {
                    "_id": "68942c64741a16f544fbcf39",
                    "name": "Haoquan Lu",
                    "hidden": false
                },
                {
                    "_id": "68942c64741a16f544fbcf3a",
                    "user": {
                        "_id": "67840ec0dd05c4028915b606",
                        "avatarUrl": "/avatars/44e36e4e5febc024b28e258520dce175.svg",
                        "isPro": false,
                        "fullname": "Hanzhe",
                        "user": "HanzheL",
                        "type": "user"
                    },
                    "name": "Hanzhe Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:38:19.601Z",
                    "hidden": false
                },
                {
                    "_id": "68942c64741a16f544fbcf3b",
                    "name": "Jie Zhang",
                    "hidden": false
                },
                {
                    "_id": "68942c64741a16f544fbcf3c",
                    "name": "Chenxi Hu",
                    "hidden": false
                },
                {
                    "_id": "68942c64741a16f544fbcf3d",
                    "name": "Jinbao Wang",
                    "hidden": false
                },
                {
                    "_id": "68942c64741a16f544fbcf3e",
                    "name": "Can Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T10:54:55.000Z",
            "submittedOnDailyAt": "2025-08-07T10:59:10.221Z",
            "title": "C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with\n  Learnable Advisor",
            "submittedOnDailyBy": {
                "_id": "67840ec0dd05c4028915b606",
                "avatarUrl": "/avatars/44e36e4e5febc024b28e258520dce175.svg",
                "isPro": false,
                "fullname": "Hanzhe",
                "user": "HanzheL",
                "type": "user"
            },
            "summary": "3D Anomaly Detection (AD) has shown great potential in detecting anomalies or\ndefects of high-precision industrial products. However, existing methods are\ntypically trained in a class-specific manner and also lack the capability of\nlearning from emerging classes. In this study, we proposed a continual learning\nframework named Continual 3D Anomaly Detection (C3D-AD), which can not only\nlearn generalized representations for multi-class point clouds but also handle\nnew classes emerging over time.Specifically, in the feature extraction module,\nto extract generalized local features from diverse product types of different\ntasks efficiently, Kernel Attention with random feature Layer (KAL) is\nintroduced, which normalizes the feature space. Then, to reconstruct data\ncorrectly and continually, an efficient Kernel Attention with learnable Advisor\n(KAA) mechanism is proposed, which learns the information from new categories\nwhile discarding redundant old information within both the encoder and decoder.\nFinally, to keep the representation consistency over tasks, a Reconstruction\nwith Parameter Perturbation (RPP) module is proposed by designing a\nrepresentation rehearsal loss function, which ensures that the model remembers\nprevious category information and returns category-adaptive\nrepresentation.Extensive experiments on three public datasets demonstrate the\neffectiveness of the proposed method, achieving an average performance of\n66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD,\nrespectively.",
            "upvotes": 1,
            "discussionId": "68942c65741a16f544fbcf3f",
            "githubRepo": "https://github.com/hzzzzzhappy/CL3AD?tab=readme-ov-file",
            "ai_summary": "A continual learning framework for 3D anomaly detection uses Kernel Attention mechanisms and parameter perturbation to handle multiple and emerging classes of point clouds.",
            "ai_keywords": [
                "continual learning",
                "3D anomaly detection",
                "C3D-AD",
                "Kernel Attention with random feature Layer",
                "KAL",
                "Kernel Attention with learnable Advisor",
                "KAA",
                "Reconstruction with Parameter Perturbation",
                "RPP",
                "representation rehearsal loss function",
                "AUROC"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-08-02T06:54:55.000Z",
        "title": "C3D-AD: Toward Continual 3D Anomaly Detection via Kernel Attention with\n  Learnable Advisor",
        "summary": "3D Anomaly Detection (AD) has shown great potential in detecting anomalies or\ndefects of high-precision industrial products. However, existing methods are\ntypically trained in a class-specific manner and also lack the capability of\nlearning from emerging classes. In this study, we proposed a continual learning\nframework named Continual 3D Anomaly Detection (C3D-AD), which can not only\nlearn generalized representations for multi-class point clouds but also handle\nnew classes emerging over time.Specifically, in the feature extraction module,\nto extract generalized local features from diverse product types of different\ntasks efficiently, Kernel Attention with random feature Layer (KAL) is\nintroduced, which normalizes the feature space. Then, to reconstruct data\ncorrectly and continually, an efficient Kernel Attention with learnable Advisor\n(KAA) mechanism is proposed, which learns the information from new categories\nwhile discarding redundant old information within both the encoder and decoder.\nFinally, to keep the representation consistency over tasks, a Reconstruction\nwith Parameter Perturbation (RPP) module is proposed by designing a\nrepresentation rehearsal loss function, which ensures that the model remembers\nprevious category information and returns category-adaptive\nrepresentation.Extensive experiments on three public datasets demonstrate the\neffectiveness of the proposed method, achieving an average performance of\n66.4%, 83.1%, and 63.4% AUROC on Real3D-AD, Anomaly-ShapeNet, and MulSen-AD,\nrespectively.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01311.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67840ec0dd05c4028915b606",
            "avatarUrl": "/avatars/44e36e4e5febc024b28e258520dce175.svg",
            "fullname": "Hanzhe",
            "name": "HanzheL",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.01226",
            "authors": [
                {
                    "_id": "68940cfb741a16f544fbce75",
                    "user": {
                        "_id": "65d6e56859173e7cc3ef8550",
                        "avatarUrl": "/avatars/b5a1003572171141ca0be3919dc9f824.svg",
                        "isPro": false,
                        "fullname": "chow",
                        "user": "enoche",
                        "type": "user"
                    },
                    "name": "Xin Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:39:26.955Z",
                    "hidden": false
                },
                {
                    "_id": "68940cfb741a16f544fbce76",
                    "name": "Yongjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68940cfb741a16f544fbce77",
                    "name": "Zhiqi Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T06:44:59.000Z",
            "submittedOnDailyAt": "2025-08-07T23:45:52.034Z",
            "title": "CM^3: Calibrating Multimodal Recommendation",
            "submittedOnDailyBy": {
                "_id": "65d6e56859173e7cc3ef8550",
                "avatarUrl": "/avatars/b5a1003572171141ca0be3919dc9f824.svg",
                "isPro": false,
                "fullname": "chow",
                "user": "enoche",
                "type": "user"
            },
            "summary": "Alignment and uniformity are fundamental principles within the domain of\ncontrastive learning. In recommender systems, prior work has established that\noptimizing the Bayesian Personalized Ranking (BPR) loss contributes to the\nobjectives of alignment and uniformity. Specifically, alignment aims to draw\ntogether the representations of interacting users and items, while uniformity\nmandates a uniform distribution of user and item embeddings across a unit\nhypersphere. This study revisits the alignment and uniformity properties within\nthe context of multimodal recommender systems, revealing a proclivity among\nextant models to prioritize uniformity to the detriment of alignment. Our\nhypothesis challenges the conventional assumption of equitable item treatment\nthrough a uniformity loss, proposing a more nuanced approach wherein items with\nsimilar multimodal attributes converge toward proximal representations within\nthe hyperspheric manifold. Specifically, we leverage the inherent similarity\nbetween items' multimodal data to calibrate their uniformity distribution,\nthereby inducing a more pronounced repulsive force between dissimilar entities\nwithin the embedding space. A theoretical analysis elucidates the relationship\nbetween this calibrated uniformity loss and the conventional uniformity\nfunction. Moreover, to enhance the fusion of multimodal features, we introduce\na Spherical B\\'ezier method designed to integrate an arbitrary number of\nmodalities while ensuring that the resulting fused features are constrained to\nthe same hyperspherical manifold. Empirical evaluations conducted on five\nreal-world datasets substantiate the superiority of our approach over competing\nbaselines. We also shown that the proposed methods can achieve up to a 5.4%\nincrease in NDCG@20 performance via the integration of MLLM-extracted features.\nSource code is available at: https://github.com/enoche/CM3.",
            "upvotes": 1,
            "discussionId": "68940cfb741a16f544fbce78",
            "ai_summary": "Revisiting alignment and uniformity in multimodal recommender systems, the study proposes a calibrated uniformity loss and Spherical Bézier method to improve feature fusion and performance.",
            "ai_keywords": [
                "contrastive learning",
                "Bayesian Personalized Ranking (BPR) loss",
                "alignment",
                "uniformity",
                "multimodal recommender systems",
                "hypersphere",
                "uniformity loss",
                "Spherical Bézier method",
                "NDCG@20",
                "MLLM-extracted features"
            ]
        },
        "publishedAt": "2025-08-02T02:44:59.000Z",
        "title": "CM^3: Calibrating Multimodal Recommendation",
        "summary": "Alignment and uniformity are fundamental principles within the domain of\ncontrastive learning. In recommender systems, prior work has established that\noptimizing the Bayesian Personalized Ranking (BPR) loss contributes to the\nobjectives of alignment and uniformity. Specifically, alignment aims to draw\ntogether the representations of interacting users and items, while uniformity\nmandates a uniform distribution of user and item embeddings across a unit\nhypersphere. This study revisits the alignment and uniformity properties within\nthe context of multimodal recommender systems, revealing a proclivity among\nextant models to prioritize uniformity to the detriment of alignment. Our\nhypothesis challenges the conventional assumption of equitable item treatment\nthrough a uniformity loss, proposing a more nuanced approach wherein items with\nsimilar multimodal attributes converge toward proximal representations within\nthe hyperspheric manifold. Specifically, we leverage the inherent similarity\nbetween items' multimodal data to calibrate their uniformity distribution,\nthereby inducing a more pronounced repulsive force between dissimilar entities\nwithin the embedding space. A theoretical analysis elucidates the relationship\nbetween this calibrated uniformity loss and the conventional uniformity\nfunction. Moreover, to enhance the fusion of multimodal features, we introduce\na Spherical B\\'ezier method designed to integrate an arbitrary number of\nmodalities while ensuring that the resulting fused features are constrained to\nthe same hyperspherical manifold. Empirical evaluations conducted on five\nreal-world datasets substantiate the superiority of our approach over competing\nbaselines. We also shown that the proposed methods can achieve up to a 5.4%\nincrease in NDCG@20 performance via the integration of MLLM-extracted features.\nSource code is available at: https://github.com/enoche/CM3.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01226.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d6e56859173e7cc3ef8550",
            "avatarUrl": "/avatars/b5a1003572171141ca0be3919dc9f824.svg",
            "fullname": "chow",
            "name": "enoche",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.00109",
            "authors": [
                {
                    "_id": "6894d8f248b0ae5ca2710c5d",
                    "name": "Mingda Chen",
                    "hidden": false
                },
                {
                    "_id": "6894d8f248b0ae5ca2710c5e",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "6894d8f248b0ae5ca2710c5f",
                    "name": "Xilun Chen",
                    "hidden": false
                },
                {
                    "_id": "6894d8f248b0ae5ca2710c60",
                    "name": "Adina Williams",
                    "hidden": false
                },
                {
                    "_id": "6894d8f248b0ae5ca2710c61",
                    "name": "Gargi Ghosh",
                    "hidden": false
                },
                {
                    "_id": "6894d8f248b0ae5ca2710c62",
                    "name": "Scott Yih",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T19:00:11.000Z",
            "submittedOnDailyAt": "2025-08-07T15:23:16.066Z",
            "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form\n  Factuality",
            "submittedOnDailyBy": {
                "_id": "66a79b27909a525bcbc0708f",
                "avatarUrl": "/avatars/91e297439833a55222bd5152bc438c53.svg",
                "isPro": false,
                "fullname": "Mingda Chen",
                "user": "mingdachenmeta",
                "type": "user"
            },
            "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.",
            "upvotes": 1,
            "discussionId": "6894d8f248b0ae5ca2710c63",
            "projectPage": "https://huggingface.co/datasets/facebook/FACTORY",
            "ai_summary": "FACTORY, a human-verified prompt set, evaluates the factuality of long-form responses from language models, revealing higher factual accuracy compared to existing datasets.",
            "ai_keywords": [
                "model-in-the-loop",
                "human evaluations",
                "state-of-the-art language models",
                "long-tailed facts"
            ]
        },
        "publishedAt": "2025-07-31T15:00:11.000Z",
        "title": "FACTORY: A Challenging Human-Verified Prompt Set for Long-Form\n  Factuality",
        "summary": "Long-form factuality evaluation assesses the ability of models to generate\naccurate, comprehensive responses to short prompts. Existing benchmarks often\nlack human verification, leading to potential quality issues. To address this\nlimitation, we introduce FACTORY, a large-scale, human-verified prompt set.\nDeveloped using a model-in-the-loop approach and refined by humans, FACTORY\nincludes challenging prompts that are fact-seeking, answerable, and\nunambiguous. We conduct human evaluations on 6 state-of-the-art language models\nusing FACTORY and existing datasets. Our results show that FACTORY is a\nchallenging benchmark: approximately 40% of the claims made in the responses of\nSOTA models are not factual, compared to only 10% for other datasets. Our\nanalysis identifies the strengths of FACTORY over prior benchmarks, emphasizing\nits reliability and the necessity for models to reason across long-tailed\nfacts.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.00109.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66a79b27909a525bcbc0708f",
            "avatarUrl": "/avatars/91e297439833a55222bd5152bc438c53.svg",
            "fullname": "Mingda Chen",
            "name": "mingdachenmeta",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.23313",
            "authors": [
                {
                    "_id": "68905b400a411b3b8d28d6f2",
                    "name": "Alfio Ferrara",
                    "hidden": false
                },
                {
                    "_id": "68905b400a411b3b8d28d6f3",
                    "user": {
                        "_id": "6458e5e8c16ecb4815de8aac",
                        "avatarUrl": "/avatars/c33c1a67960cdbbc8a970d751dff07af.svg",
                        "isPro": false,
                        "fullname": "Sergio Picascia",
                        "user": "sergiopicascia",
                        "type": "user"
                    },
                    "name": "Sergio Picascia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-06T19:25:00.992Z",
                    "hidden": false
                },
                {
                    "_id": "68905b400a411b3b8d28d6f4",
                    "name": "Elisabetta Rocchetti",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-31T07:47:01.000Z",
            "submittedOnDailyAt": "2025-08-07T02:58:12.465Z",
            "title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models",
            "submittedOnDailyBy": {
                "_id": "6458e5e8c16ecb4815de8aac",
                "avatarUrl": "/avatars/c33c1a67960cdbbc8a970d751dff07af.svg",
                "isPro": false,
                "fullname": "Sergio Picascia",
                "user": "sergiopicascia",
                "type": "user"
            },
            "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ngenerating artistic content by learning from billions of images, including\npopular artworks. However, the fundamental question of how these models\ninternally represent concepts, such as content and style in paintings, remains\nunexplored. Traditional computer vision assumes content and style are\northogonal, but diffusion models receive no explicit guidance about this\ndistinction during training. In this work, we investigate how transformer-based\ntext-to-image diffusion models encode content and style concepts when\ngenerating artworks. We leverage cross-attention heatmaps to attribute pixels\nin generated images to specific prompt tokens, enabling us to isolate image\nregions influenced by content-describing versus style-describing tokens. Our\nfindings reveal that diffusion models demonstrate varying degrees of\ncontent-style separation depending on the specific artistic prompt and style\nrequested. In many cases, content tokens primarily influence object-related\nregions while style tokens affect background and texture areas, suggesting an\nemergent understanding of the content-style distinction. These insights\ncontribute to our understanding of how large-scale generative models internally\nrepresent complex artistic concepts without explicit supervision. We share the\ncode and dataset, together with an exploratory tool for visualizing attention\nmaps at https://github.com/umilISLab/artistic-prompt-interpretation.",
            "upvotes": 1,
            "discussionId": "68905b400a411b3b8d28d6f5",
            "projectPage": "https://thecowofrembrandt.islab.di.unimi.it/",
            "githubRepo": "https://github.com/umilISLab/artistic-prompt-interpretation",
            "ai_summary": "Transformer-based text-to-image diffusion models show varying degrees of content-style separation in generated artworks, as revealed by cross-attention heatmaps.",
            "ai_keywords": [
                "text-to-image diffusion models",
                "transformer-based",
                "cross-attention heatmaps",
                "content-style separation",
                "artistic prompts",
                "attention maps"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-07-31T03:47:01.000Z",
        "title": "The Cow of Rembrandt - Analyzing Artistic Prompt Interpretation in\n  Text-to-Image Models",
        "summary": "Text-to-image diffusion models have demonstrated remarkable capabilities in\ngenerating artistic content by learning from billions of images, including\npopular artworks. However, the fundamental question of how these models\ninternally represent concepts, such as content and style in paintings, remains\nunexplored. Traditional computer vision assumes content and style are\northogonal, but diffusion models receive no explicit guidance about this\ndistinction during training. In this work, we investigate how transformer-based\ntext-to-image diffusion models encode content and style concepts when\ngenerating artworks. We leverage cross-attention heatmaps to attribute pixels\nin generated images to specific prompt tokens, enabling us to isolate image\nregions influenced by content-describing versus style-describing tokens. Our\nfindings reveal that diffusion models demonstrate varying degrees of\ncontent-style separation depending on the specific artistic prompt and style\nrequested. In many cases, content tokens primarily influence object-related\nregions while style tokens affect background and texture areas, suggesting an\nemergent understanding of the content-style distinction. These insights\ncontribute to our understanding of how large-scale generative models internally\nrepresent complex artistic concepts without explicit supervision. We share the\ncode and dataset, together with an exploratory tool for visualizing attention\nmaps at https://github.com/umilISLab/artistic-prompt-interpretation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.23313.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6458e5e8c16ecb4815de8aac",
            "avatarUrl": "/avatars/c33c1a67960cdbbc8a970d751dff07af.svg",
            "fullname": "Sergio Picascia",
            "name": "sergiopicascia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.02951",
            "authors": [
                {
                    "_id": "6895505748b0ae5ca2710cd3",
                    "name": "Mahtab Bigverdi",
                    "hidden": false
                },
                {
                    "_id": "6895505748b0ae5ca2710cd4",
                    "name": "Wisdom Ikezogwo",
                    "hidden": false
                },
                {
                    "_id": "6895505748b0ae5ca2710cd5",
                    "name": "Kevin Zhang",
                    "hidden": false
                },
                {
                    "_id": "6895505748b0ae5ca2710cd6",
                    "name": "Hyewon Jeong",
                    "hidden": false
                },
                {
                    "_id": "6895505748b0ae5ca2710cd7",
                    "name": "Mingyu Lu",
                    "hidden": false
                },
                {
                    "_id": "6895505748b0ae5ca2710cd8",
                    "name": "Sungjae Cho",
                    "hidden": false
                },
                {
                    "_id": "6895505748b0ae5ca2710cd9",
                    "name": "Linda Shapiro",
                    "hidden": false
                },
                {
                    "_id": "6895505748b0ae5ca2710cda",
                    "name": "Ranjay Krishna",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T23:19:18.000Z",
            "submittedOnDailyAt": "2025-08-07T23:54:54.902Z",
            "title": "MedBLINK: Probing Basic Perception in Multimodal Language Models for\n  Medicine",
            "submittedOnDailyBy": {
                "_id": "648380d1f9256e215de1ff88",
                "avatarUrl": "/avatars/a609c74911ed4f96843debdf2e7568ca.svg",
                "isPro": false,
                "fullname": "Mahtab Bigverdi",
                "user": "MahtabBg",
                "type": "user"
            },
            "summary": "Multimodal language models (MLMs) show promise for clinical decision support\nand diagnostic reasoning, raising the prospect of end-to-end automated medical\nimage interpretation. However, clinicians are highly selective in adopting AI\ntools; a model that makes errors on seemingly simple perception tasks such as\ndetermining image orientation or identifying whether a CT scan is\ncontrast-enhance are unlikely to be adopted for clinical tasks. We introduce\nMedblink, a benchmark designed to probe these models for such perceptual\nabilities. Medblink spans eight clinically meaningful tasks across multiple\nimaging modalities and anatomical regions, totaling 1,429 multiple-choice\nquestions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including\ngeneral purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,\nLLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the\nbest-performing model reaches only 65%. These results show that current MLMs\nfrequently fail at routine perceptual checks, suggesting the need to strengthen\ntheir visual grounding to support clinical adoption. Data is available on our\nproject page.",
            "upvotes": 0,
            "discussionId": "6895505848b0ae5ca2710cdb",
            "ai_summary": "Medblink benchmark evaluates the perceptual abilities of multimodal language models in clinical image interpretation, revealing significant gaps compared to human performance.",
            "ai_keywords": [
                "multimodal language models",
                "MLMS",
                "clinical decision support",
                "diagnostic reasoning",
                "end-to-end automated medical image interpretation",
                "Medblink",
                "perceptual abilities",
                "clinically meaningful tasks",
                "imaging modalities",
                "anatomical regions",
                "multiple-choice questions",
                "general purpose models",
                "domain specific models",
                "visual grounding"
            ]
        },
        "publishedAt": "2025-08-04T19:19:18.000Z",
        "title": "MedBLINK: Probing Basic Perception in Multimodal Language Models for\n  Medicine",
        "summary": "Multimodal language models (MLMs) show promise for clinical decision support\nand diagnostic reasoning, raising the prospect of end-to-end automated medical\nimage interpretation. However, clinicians are highly selective in adopting AI\ntools; a model that makes errors on seemingly simple perception tasks such as\ndetermining image orientation or identifying whether a CT scan is\ncontrast-enhance are unlikely to be adopted for clinical tasks. We introduce\nMedblink, a benchmark designed to probe these models for such perceptual\nabilities. Medblink spans eight clinically meaningful tasks across multiple\nimaging modalities and anatomical regions, totaling 1,429 multiple-choice\nquestions over 1,605 images. We evaluate 19 state-of-the-art MLMs, including\ngeneral purpose (GPT4o, Claude 3.5 Sonnet) and domain specific (Med Flamingo,\nLLaVA Med, RadFM) models. While human annotators achieve 96.4% accuracy, the\nbest-performing model reaches only 65%. These results show that current MLMs\nfrequently fail at routine perceptual checks, suggesting the need to strengthen\ntheir visual grounding to support clinical adoption. Data is available on our\nproject page.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02951.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648380d1f9256e215de1ff88",
            "avatarUrl": "/avatars/a609c74911ed4f96843debdf2e7568ca.svg",
            "fullname": "Mahtab Bigverdi",
            "name": "MahtabBg",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]