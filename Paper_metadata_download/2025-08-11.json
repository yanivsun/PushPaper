[
    {
        "paper": {
            "id": "2508.06471",
            "authors": [
                {
                    "_id": "68996c86f022d141f5d4354d",
                    "name": "GLM-4. 5 Team",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4354f",
                    "name": "Aohan Zeng",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43550",
                    "name": "Xin Lv",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43551",
                    "user": {
                        "_id": "6231576e92e83fd1179ac3f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1664543160657-6231576e92e83fd1179ac3f0.jpeg",
                        "isPro": false,
                        "fullname": "Qinkai Zheng",
                        "user": "Stanislas",
                        "type": "user"
                    },
                    "name": "Qinkai Zheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:45:31.330Z",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43552",
                    "name": "Zhenyu Hou",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43553",
                    "name": "Bin Chen",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43554",
                    "name": "Chengxing Xie",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43555",
                    "name": "Cunxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43556",
                    "name": "Da Yin",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43557",
                    "name": "Hao Zeng",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43558",
                    "name": "Jiajie Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43559",
                    "name": "Kedong Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4355a",
                    "name": "Lucen Zhong",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4355b",
                    "name": "Mingdao Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4355c",
                    "name": "Rui Lu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4355d",
                    "name": "Shulin Cao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4355e",
                    "name": "Xiaohan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4355f",
                    "name": "Xuancheng Huang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43560",
                    "name": "Yao Wei",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43561",
                    "user": {
                        "_id": "65acc5afe2a2c8635614de43",
                        "avatarUrl": "/avatars/c5fce792792cc0b52ed7475d72460c58.svg",
                        "isPro": false,
                        "fullname": "Yean Cheng",
                        "user": "LiquidAmmonia",
                        "type": "user"
                    },
                    "name": "Yean Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:45:35.718Z",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43562",
                    "name": "Yifan An",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43563",
                    "name": "Yilin Niu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43564",
                    "name": "Yuanhao Wen",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43565",
                    "name": "Yushi Bai",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43566",
                    "name": "Zhengxiao Du",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43567",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43568",
                    "name": "Zilin Zhu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43569",
                    "name": "Bohan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4356a",
                    "name": "Bosi Wen",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4356b",
                    "name": "Bowen Wu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4356c",
                    "name": "Bowen Xu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4356d",
                    "name": "Can Huang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4356e",
                    "name": "Casey Zhao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4356f",
                    "name": "Changpeng Cai",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43570",
                    "name": "Chao Yu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43571",
                    "name": "Chen Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43572",
                    "name": "Chendi Ge",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43573",
                    "name": "Chenghua Huang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43574",
                    "name": "Chenhui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43575",
                    "name": "Chenxi Xu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43576",
                    "name": "Chenzheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43577",
                    "name": "Chuang Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43578",
                    "name": "Congfeng Yin",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43579",
                    "name": "Daoyan Lin",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4357a",
                    "name": "Dayong Yang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4357b",
                    "name": "Dazhi Jiang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4357c",
                    "name": "Ding Ai",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4357d",
                    "name": "Erle Zhu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4357e",
                    "name": "Fei Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4357f",
                    "name": "Gengzheng Pan",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43580",
                    "name": "Guo Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43581",
                    "name": "Hailong Sun",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43582",
                    "name": "Haitao Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43583",
                    "name": "Haiyang Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43584",
                    "name": "Haiyi Hu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43585",
                    "name": "Hanyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43586",
                    "name": "Hao Peng",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43587",
                    "name": "Hao Tai",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43588",
                    "name": "Haoke Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43589",
                    "name": "Haoran Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4358a",
                    "name": "Haoyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4358b",
                    "name": "He Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4358c",
                    "name": "He Zhao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4358d",
                    "name": "Hongwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4358e",
                    "name": "Hongxi Yan",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4358f",
                    "name": "Huan Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43590",
                    "name": "Huilong Chen",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43591",
                    "name": "Ji Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43592",
                    "name": "Jiajing Zhao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43593",
                    "name": "Jiamin Ren",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43594",
                    "name": "Jian Jiao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43595",
                    "name": "Jiani Zhao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43596",
                    "name": "Jianyang Yan",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43597",
                    "name": "Jiaqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43598",
                    "name": "Jiayi Gui",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d43599",
                    "name": "Jiayue Zhao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4359a",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4359b",
                    "name": "Jijie Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4359c",
                    "name": "Jing Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4359d",
                    "name": "Jing Lu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4359e",
                    "name": "Jingsen Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d4359f",
                    "name": "Jingwei Yuan",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a0",
                    "name": "Jingxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a1",
                    "name": "Jingzhao Du",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a2",
                    "name": "Jinhua Du",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a3",
                    "name": "Jinxin Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a4",
                    "name": "Junkai Zhi",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a5",
                    "name": "Junli Gao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a6",
                    "name": "Ke Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a7",
                    "name": "Lekang Yang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a8",
                    "name": "Liang Xu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435a9",
                    "name": "Lin Fan",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435aa",
                    "name": "Lindong Wu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ab",
                    "name": "Lintao Ding",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ac",
                    "name": "Lu Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ad",
                    "name": "Man Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ae",
                    "name": "Minghao Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435af",
                    "name": "Minghuan Xu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b0",
                    "name": "Mingming Zhao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b1",
                    "name": "Mingshu Zhai",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b2",
                    "name": "Pengfan Du",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b3",
                    "name": "Qian Dong",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b4",
                    "name": "Shangde Lei",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b5",
                    "name": "Shangqing Tu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b6",
                    "name": "Shangtong Yang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b7",
                    "name": "Shaoyou Lu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b8",
                    "name": "Shijie Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435b9",
                    "name": "Shuang Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ba",
                    "name": "Shuang-Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435bb",
                    "name": "Shuxun Yang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435bc",
                    "name": "Sibo Yi",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435bd",
                    "name": "Tianshu Yu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435be",
                    "name": "Wei Tian",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435bf",
                    "name": "Weihan Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c0",
                    "name": "Wenbo Yu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c1",
                    "name": "Weng Lam Tam",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c2",
                    "name": "Wenjie Liang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c3",
                    "name": "Wentao Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c4",
                    "name": "Xiao Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c5",
                    "name": "Xiaohan Jia",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c6",
                    "name": "Xiaotao Gu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c7",
                    "name": "Xiaoying Ling",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c8",
                    "name": "Xin Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435c9",
                    "name": "Xing Fan",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ca",
                    "name": "Xingru Pan",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435cb",
                    "name": "Xinyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435cc",
                    "name": "Xinze Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435cd",
                    "name": "Xiuqing Fu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ce",
                    "name": "Xunkai Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435cf",
                    "name": "Yabo Xu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d0",
                    "name": "Yandong Wu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d1",
                    "name": "Yida Lu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d2",
                    "name": "Yidong Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d3",
                    "name": "Yilin Zhou",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d4",
                    "name": "Yiming Pan",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d5",
                    "name": "Ying Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d6",
                    "name": "Yingli Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d7",
                    "name": "Yingru Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d8",
                    "name": "Yinpei Su",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435d9",
                    "name": "Yipeng Geng",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435da",
                    "name": "Yitong Zhu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435db",
                    "name": "Yongkun Yang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435dc",
                    "name": "Yuhang Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435dd",
                    "name": "Yuhao Wu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435de",
                    "name": "Yujiang Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435df",
                    "name": "Yunan Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e0",
                    "name": "Yunqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e1",
                    "name": "Yuntao Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e2",
                    "user": {
                        "_id": "643507d1ce04fdb57e9d7e05",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643507d1ce04fdb57e9d7e05/QuCFAO3v7G3LrVGusqFj1.png",
                        "isPro": false,
                        "fullname": "zR",
                        "user": "ZAHNGYUXUAN",
                        "type": "user"
                    },
                    "name": "Yuxuan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:45:21.258Z",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e3",
                    "name": "Zezhen Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e4",
                    "name": "Zhen Yang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e5",
                    "name": "Zhengda Zhou",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e6",
                    "name": "Zhongpei Qiao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e7",
                    "name": "Zhuoer Feng",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e8",
                    "name": "Zhuorui Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435e9",
                    "name": "Zichen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ea",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435eb",
                    "name": "Zijun Yao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ec",
                    "name": "Zikang Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ed",
                    "name": "Ziqiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ee",
                    "name": "Ziwei Chai",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435ef",
                    "user": {
                        "_id": "68806066345a5d85e2494aba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/AkrQMIp4kQm3Zn1giMa5I.png",
                        "isPro": false,
                        "fullname": "Zixuan Li",
                        "user": "zixuanlimit",
                        "type": "user"
                    },
                    "name": "Zixuan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:45:26.163Z",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435f0",
                    "name": "Zuodong Zhao",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435f1",
                    "name": "Wenguang Chen",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435f2",
                    "name": "Jidong Zhai",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435f3",
                    "name": "Bin Xu",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435f4",
                    "name": "Minlie Huang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435f5",
                    "name": "Hongning Wang",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435f6",
                    "name": "Juanzi Li",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435f7",
                    "name": "Yuxiao Dong",
                    "hidden": false
                },
                {
                    "_id": "68996c86f022d141f5d435f8",
                    "name": "Jie Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-08T17:21:06.000Z",
            "submittedOnDailyAt": "2025-08-11T02:39:27.170Z",
            "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
            "submittedOnDailyBy": {
                "_id": "62d22496c58f969c152bcefd",
                "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
                "isPro": false,
                "fullname": "Tiezhen WANG",
                "user": "xianbao",
                "type": "user"
            },
            "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.",
            "upvotes": 77,
            "discussionId": "68996c87f022d141f5d435f9",
            "githubRepo": "https://github.com/zai-org/GLM-4.5",
            "ai_summary": "GLM-4.5, a Mixture-of-Experts large language model with 355B parameters, achieves strong performance across agentic, reasoning, and coding tasks using multi-stage training and reinforcement learning.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "hybrid reasoning method",
                "multi-stage training",
                "expert model iteration",
                "reinforcement learning",
                "TAU-Bench",
                "AIME 24",
                "SWE-bench Verified",
                "agentic benchmarks"
            ],
            "githubStars": 1816
        },
        "publishedAt": "2025-08-08T13:21:06.000Z",
        "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
        "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language\nmodel with 355B total parameters and 32B activated parameters, featuring a\nhybrid reasoning method that supports both thinking and direct response modes.\nThrough multi-stage training on 23T tokens and comprehensive post-training with\nexpert model iteration and reinforcement learning, GLM-4.5 achieves strong\nperformance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on\nTAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer\nparameters than several competitors, GLM-4.5 ranks 3rd overall among all\nevaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B\nparameters) and a compact version, GLM-4.5-Air (106B parameters), to advance\nresearch in reasoning and agentic AI systems. Code, models, and more\ninformation are available at https://github.com/zai-org/GLM-4.5.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06471.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62d22496c58f969c152bcefd",
            "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
            "fullname": "Tiezhen WANG",
            "name": "xianbao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 127
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.04825",
            "authors": [
                {
                    "_id": "6895558948b0ae5ca2710cf7",
                    "user": {
                        "_id": "660eb276a0de79aa07e754df",
                        "avatarUrl": "/avatars/48ee2d484e70714516589a8b13137036.svg",
                        "isPro": false,
                        "fullname": "Seungyong Lee",
                        "user": "RyanL22",
                        "type": "user"
                    },
                    "name": "Seungyong Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-08T16:20:16.112Z",
                    "hidden": false
                },
                {
                    "_id": "6895558948b0ae5ca2710cf8",
                    "user": {
                        "_id": "635035fedc10894a3f66ea79",
                        "avatarUrl": "/avatars/bd9c373705d29ad8f14987a2864e5956.svg",
                        "isPro": false,
                        "fullname": "Jeong-gi Kwak",
                        "user": "jgkwak",
                        "type": "user"
                    },
                    "name": "Jeong-gi Kwak",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-08T16:20:12.557Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/660eb276a0de79aa07e754df/kVH3Bi0LdvAD7H_HuIGRm.png"
            ],
            "publishedAt": "2025-08-06T19:10:58.000Z",
            "submittedOnDailyAt": "2025-08-11T00:37:43.016Z",
            "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off",
            "submittedOnDailyBy": {
                "_id": "660eb276a0de79aa07e754df",
                "avatarUrl": "/avatars/48ee2d484e70714516589a8b13137036.svg",
                "isPro": false,
                "fullname": "Seungyong Lee",
                "user": "RyanL22",
                "type": "user"
            },
            "summary": "Virtual try-on aims to synthesize a realistic image of a person wearing a\ntarget garment, but accurately modeling garment-body correspondence remains a\npersistent challenge, especially under pose and appearance variation. In this\npaper, we propose Voost - a unified and scalable framework that jointly learns\nvirtual try-on and try-off with a single diffusion transformer. By modeling\nboth tasks jointly, Voost enables each garment-person pair to supervise both\ndirections and supports flexible conditioning over generation direction and\ngarment category, enhancing garment-body relational reasoning without\ntask-specific networks, auxiliary losses, or additional labels. In addition, we\nintroduce two inference-time techniques: attention temperature scaling for\nrobustness to resolution or mask variation, and self-corrective sampling that\nleverages bidirectional consistency between tasks. Extensive experiments\ndemonstrate that Voost achieves state-of-the-art results on both try-on and\ntry-off benchmarks, consistently outperforming strong baselines in alignment\naccuracy, visual fidelity, and generalization.",
            "upvotes": 35,
            "discussionId": "6895558948b0ae5ca2710cf9",
            "projectPage": "https://nxnai.github.io/Voost/",
            "githubRepo": "https://github.com/nxnai/Voost",
            "ai_summary": "Voost, a unified diffusion transformer framework, jointly learns virtual try-on and try-off, enhancing garment-body correspondence and achieving state-of-the-art results across benchmarks.",
            "ai_keywords": [
                "diffusion transformer",
                "garment-body correspondence",
                "virtual try-on",
                "virtual try-off",
                "attention temperature scaling",
                "self-corrective sampling",
                "bidirectional consistency"
            ],
            "githubStars": 58
        },
        "publishedAt": "2025-08-06T15:10:58.000Z",
        "title": "Voost: A Unified and Scalable Diffusion Transformer for Bidirectional\n  Virtual Try-On and Try-Off",
        "summary": "Virtual try-on aims to synthesize a realistic image of a person wearing a\ntarget garment, but accurately modeling garment-body correspondence remains a\npersistent challenge, especially under pose and appearance variation. In this\npaper, we propose Voost - a unified and scalable framework that jointly learns\nvirtual try-on and try-off with a single diffusion transformer. By modeling\nboth tasks jointly, Voost enables each garment-person pair to supervise both\ndirections and supports flexible conditioning over generation direction and\ngarment category, enhancing garment-body relational reasoning without\ntask-specific networks, auxiliary losses, or additional labels. In addition, we\nintroduce two inference-time techniques: attention temperature scaling for\nrobustness to resolution or mask variation, and self-corrective sampling that\nleverages bidirectional consistency between tasks. Extensive experiments\ndemonstrate that Voost achieves state-of-the-art results on both try-on and\ntry-off benchmarks, consistently outperforming strong baselines in alignment\naccuracy, visual fidelity, and generalization.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/660eb276a0de79aa07e754df/kVH3Bi0LdvAD7H_HuIGRm.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04825.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "660eb276a0de79aa07e754df",
            "avatarUrl": "/avatars/48ee2d484e70714516589a8b13137036.svg",
            "fullname": "Seungyong Lee",
            "name": "RyanL22",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.05731",
            "authors": [
                {
                    "_id": "689945f3f022d141f5d434a5",
                    "user": {
                        "_id": "62722849517c0ca41f7cd13d",
                        "avatarUrl": "/avatars/bb1f8f2f2665944930cb5a7ce19c47d4.svg",
                        "isPro": false,
                        "fullname": "Yuhang Liu",
                        "user": "SiriusL",
                        "type": "user"
                    },
                    "name": "Yuhang Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:52:51.989Z",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434a6",
                    "name": "Zeyu Liu",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434a7",
                    "name": "Shuanghe Zhu",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434a8",
                    "name": "Pengxiang Li",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434a9",
                    "name": "Congkai Xie",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434aa",
                    "name": "Jiasheng Wang",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434ab",
                    "name": "Xueyu Hu",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434ac",
                    "user": {
                        "_id": "650dde4ce14eeb01d42b37a1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650dde4ce14eeb01d42b37a1/n5Yv24uofZ2XJjXdYCrKd.png",
                        "isPro": false,
                        "fullname": "Xiaotian Han",
                        "user": "xiaotianhan",
                        "type": "user"
                    },
                    "name": "Xiaotian Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:48:38.769Z",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434ad",
                    "name": "Jianbo Yuan",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434ae",
                    "name": "Xinyao Wang",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434af",
                    "name": "Shengyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434b0",
                    "name": "Hongxia Yang",
                    "hidden": false
                },
                {
                    "_id": "689945f3f022d141f5d434b1",
                    "name": "Fei Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-07T17:49:56.000Z",
            "submittedOnDailyAt": "2025-08-11T00:34:02.328Z",
            "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy\n  Optimization",
            "submittedOnDailyBy": {
                "_id": "62722849517c0ca41f7cd13d",
                "avatarUrl": "/avatars/bb1f8f2f2665944930cb5a7ce19c47d4.svg",
                "isPro": false,
                "fullname": "Yuhang Liu",
                "user": "SiriusL",
                "type": "user"
            },
            "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.",
            "upvotes": 17,
            "discussionId": "689945f3f022d141f5d434b2",
            "githubRepo": "https://github.com/InfiXAI/InfiGUI-G1",
            "ai_summary": "Adaptive Exploration Policy Optimization (AEPO) enhances semantic alignment in Multimodal Large Language Models (MLLMs) for GUI interaction, improving performance on benchmarks by up to 9.0% compared to RLVR.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "GUIs",
                "Reinforcement Learning with Verifiable Rewards",
                "Adaptive Exploration Policy Optimization",
                "multi-answer generation",
                "Adaptive Exploration Reward",
                "efficiency eta=U/C",
                "InfiGUI-G1-3B",
                "InfiGUI-G1-7B"
            ],
            "githubStars": 9
        },
        "publishedAt": "2025-08-07T13:49:56.000Z",
        "title": "InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy\n  Optimization",
        "summary": "The emergence of Multimodal Large Language Models (MLLMs) has propelled the\ndevelopment of autonomous agents that operate on Graphical User Interfaces\n(GUIs) using pure visual input. A fundamental challenge is robustly grounding\nnatural language instructions. This requires a precise spatial alignment, which\naccurately locates the coordinates of each element, and, more critically, a\ncorrect semantic alignment, which matches the instructions to the functionally\nappropriate UI element. Although Reinforcement Learning with Verifiable Rewards\n(RLVR) has proven to be effective at improving spatial alignment for these\nMLLMs, we find that inefficient exploration bottlenecks semantic alignment,\nwhich prevent models from learning difficult semantic associations. To address\nthis exploration problem, we present Adaptive Exploration Policy Optimization\n(AEPO), a new policy optimization framework. AEPO employs a multi-answer\ngeneration strategy to enforce broader exploration, which is then guided by a\ntheoretically grounded Adaptive Exploration Reward (AER) function derived from\nfirst principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B\nand InfiGUI-G1-7B, establish new state-of-the-art results across multiple\nchallenging GUI grounding benchmarks, achieving significant relative\nimprovements of up to 9.0% against the naive RLVR baseline on benchmarks\ndesigned to test generalization and semantic understanding. Resources are\navailable at https://github.com/InfiXAI/InfiGUI-G1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05731.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62722849517c0ca41f7cd13d",
            "avatarUrl": "/avatars/bb1f8f2f2665944930cb5a7ce19c47d4.svg",
            "fullname": "Yuhang Liu",
            "name": "SiriusL",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.06433",
            "authors": [
                {
                    "_id": "6899b66999477d568629a468",
                    "name": "Runnan Fang",
                    "hidden": false
                },
                {
                    "_id": "6899b66999477d568629a469",
                    "name": "Yuan Liang",
                    "hidden": false
                },
                {
                    "_id": "6899b66999477d568629a46a",
                    "name": "Xiaobin Wang",
                    "hidden": false
                },
                {
                    "_id": "6899b66999477d568629a46b",
                    "name": "Jialong Wu",
                    "hidden": false
                },
                {
                    "_id": "6899b66999477d568629a46c",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "6899b66999477d568629a46d",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "6899b66999477d568629a46e",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "6899b66999477d568629a46f",
                    "name": "Huajun Chen",
                    "hidden": false
                },
                {
                    "_id": "6899b66999477d568629a470",
                    "name": "Ningyu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-08T16:20:56.000Z",
            "submittedOnDailyAt": "2025-08-11T07:53:12.957Z",
            "title": "Memp: Exploring Agent Procedural Memory",
            "submittedOnDailyBy": {
                "_id": "620b3bbb0668e435407c8d0a",
                "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
                "isPro": true,
                "fullname": "Ningyu Zhang",
                "user": "Ningyu",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.",
            "upvotes": 16,
            "discussionId": "6899b66999477d568629a471",
            "ai_summary": "Agents equipped with a learnable, updatable procedural memory system, Memp, achieve improved performance and efficiency across tasks by distilling past experiences into detailed instructions and higher-level abstractions.",
            "ai_keywords": [
                "Large Language Models",
                "procedural memory",
                "Memp",
                "agent trajectories",
                "fine-grained instructions",
                "script-like abstractions",
                "Build",
                "Retrieval",
                "Update",
                "dynamic regimen",
                "TravelPlanner",
                "ALFWorld",
                "performance gains"
            ]
        },
        "publishedAt": "2025-08-08T12:20:56.000Z",
        "title": "Memp: Exploring Agent Procedural Memory",
        "summary": "Large Language Models (LLMs) based agents excel at diverse tasks, yet they\nsuffer from brittle procedural memory that is manually engineered or entangled\nin static parameters. In this work, we investigate strategies to endow agents\nwith a learnable, updatable, and lifelong procedural memory. We propose Memp\nthat distills past agent trajectories into both fine-grained, step-by-step\ninstructions and higher-level, script-like abstractions, and explore the impact\nof different strategies for Build, Retrieval, and Update of procedural memory.\nCoupled with a dynamic regimen that continuously updates, corrects, and\ndeprecates its contents, this repository evolves in lockstep with new\nexperience. Empirical evaluation on TravelPlanner and ALFWorld shows that as\nthe memory repository is refined, agents achieve steadily higher success rates\nand greater efficiency on analogous tasks. Moreover, procedural memory built\nfrom a stronger model retains its value: migrating the procedural memory to a\nweaker model yields substantial performance gains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06433.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620b3bbb0668e435407c8d0a",
            "avatarUrl": "/avatars/e0fccbb2577d76088e09f054c35cffbc.svg",
            "fullname": "Ningyu Zhang",
            "name": "Ningyu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 28
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.05988",
            "authors": [
                {
                    "_id": "68994f80f022d141f5d434cf",
                    "name": "Wenhao Zeng",
                    "hidden": false
                },
                {
                    "_id": "68994f80f022d141f5d434d0",
                    "name": "Yaoning Wang",
                    "hidden": false
                },
                {
                    "_id": "68994f80f022d141f5d434d1",
                    "name": "Chao Hu",
                    "hidden": false
                },
                {
                    "_id": "68994f80f022d141f5d434d2",
                    "user": {
                        "_id": "645b0c3ec35da9c7afd95421",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                        "isPro": false,
                        "fullname": "Yuling",
                        "user": "YerbaPage",
                        "type": "user"
                    },
                    "name": "Yuling Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:45:44.939Z",
                    "hidden": false
                },
                {
                    "_id": "68994f80f022d141f5d434d3",
                    "name": "Chengcheng Wan",
                    "hidden": false
                },
                {
                    "_id": "68994f80f022d141f5d434d4",
                    "name": "Hongyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68994f80f022d141f5d434d5",
                    "name": "Xiaodong Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-08T03:46:21.000Z",
            "submittedOnDailyAt": "2025-08-11T00:37:13.589Z",
            "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token\n  Surprisal",
            "submittedOnDailyBy": {
                "_id": "645b0c3ec35da9c7afd95421",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
                "isPro": false,
                "fullname": "Yuling",
                "user": "YerbaPage",
                "type": "user"
            },
            "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in code reasoning by scaling up the length of Chain-of-Thought\n(CoT). However, excessively long reasoning traces introduce substantial\nchallenges in terms of training cost, inference latency, and deployment\nfeasibility. While various CoT compression approaches have emerged to address\nthis challenge, they face inherent trade-offs: token-level methods often\ndisrupt syntactic and logical coherence, while step-level methods based on\nperplexity fail to reliably capture the logically critical reasoning steps. In\nthis paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel\ncoarse-to-fine framework for CoT compression. ASAP first performs anchor-guided\npruning to preserve the core reasoning structure, which efficiently reduces the\nsearch space for subsequent processing. It then enables a logic-aware pruning\nby selecting logically essential reasoning steps based on a novel first-token\nsurprisal metric. Finally, ASAP teaches models to autonomously generate and\nleverage these concise CoTs at inference time, enabling efficient reasoning in\ncoding tasks. Experiments show that ASAP achieves state-of-the-art accuracy\nacross multiple code generation benchmarks while substantially reducing\ntraining and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,\nour approach reduces token generation by 23.5% and inference latency by 43.5%\ncompared to the strongest baseline, while achieving a competitive accuracy of\n36.19% in Pass@1. Our results highlight a promising direction for building\npowerful and efficient LRMs.",
            "upvotes": 12,
            "discussionId": "68994f80f022d141f5d434d6",
            "githubRepo": "https://github.com/Zengwh02/ASAP",
            "ai_summary": "ASAP, a novel coarse-to-fine framework, compresses Chain-of-Thought in code reasoning by preserving core structure and essential steps, reducing costs and improving efficiency.",
            "ai_keywords": [
                "Large Reasoning Models",
                "Chain-of-Thought",
                "CoT compression",
                "anchor-guided pruning",
                "logic-aware pruning",
                "first-token surprisal metric",
                "LiveCodeBench",
                "Pass@1"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-08-07T23:46:21.000Z",
        "title": "Pruning the Unsurprising: Efficient Code Reasoning via First-Token\n  Surprisal",
        "summary": "Recently, Large Reasoning Models (LRMs) have demonstrated remarkable\ncapabilities in code reasoning by scaling up the length of Chain-of-Thought\n(CoT). However, excessively long reasoning traces introduce substantial\nchallenges in terms of training cost, inference latency, and deployment\nfeasibility. While various CoT compression approaches have emerged to address\nthis challenge, they face inherent trade-offs: token-level methods often\ndisrupt syntactic and logical coherence, while step-level methods based on\nperplexity fail to reliably capture the logically critical reasoning steps. In\nthis paper, we propose ASAP (Anchor-guided, Surprisal-based Pruning), a novel\ncoarse-to-fine framework for CoT compression. ASAP first performs anchor-guided\npruning to preserve the core reasoning structure, which efficiently reduces the\nsearch space for subsequent processing. It then enables a logic-aware pruning\nby selecting logically essential reasoning steps based on a novel first-token\nsurprisal metric. Finally, ASAP teaches models to autonomously generate and\nleverage these concise CoTs at inference time, enabling efficient reasoning in\ncoding tasks. Experiments show that ASAP achieves state-of-the-art accuracy\nacross multiple code generation benchmarks while substantially reducing\ntraining and inference costs. On the challenging LiveCodeBench v4_v5 benchmark,\nour approach reduces token generation by 23.5% and inference latency by 43.5%\ncompared to the strongest baseline, while achieving a competitive accuracy of\n36.19% in Pass@1. Our results highlight a promising direction for building\npowerful and efficient LRMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05988.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "645b0c3ec35da9c7afd95421",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/645b0c3ec35da9c7afd95421/vYBrCDagHsXAo6J2p-uG0.jpeg",
            "fullname": "Yuling",
            "name": "YerbaPage",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 276
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03616",
            "authors": [
                {
                    "_id": "68965081f022d141f5d43161",
                    "user": {
                        "_id": "646bd4fde96a751c52544801",
                        "avatarUrl": "/avatars/e8daa804dab4df52ad6c1b55bb4e3d82.svg",
                        "isPro": false,
                        "fullname": "Jorge Gallego Feliciano",
                        "user": "JorgeeGF",
                        "type": "user"
                    },
                    "name": "Jorge Gallego-Feliciano",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:50:29.339Z",
                    "hidden": false
                },
                {
                    "_id": "68965081f022d141f5d43162",
                    "user": {
                        "_id": "64512c65938967fd069cf7d1",
                        "avatarUrl": "/avatars/c36eb7fb0c407971b9220549680e4a7a.svg",
                        "isPro": false,
                        "fullname": "Steven Aaron McClendon",
                        "user": "aaronmac",
                        "type": "user"
                    },
                    "name": "S. Aaron McClendon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:50:32.983Z",
                    "hidden": false
                },
                {
                    "_id": "68965081f022d141f5d43163",
                    "name": "Juan Morinelli",
                    "hidden": false
                },
                {
                    "_id": "68965081f022d141f5d43164",
                    "name": "Stavros Zervoudakis",
                    "hidden": false
                },
                {
                    "_id": "68965081f022d141f5d43165",
                    "name": "Antonios Saravanos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T16:29:51.000Z",
            "submittedOnDailyAt": "2025-08-11T20:08:39.397Z",
            "title": "Hidden Dynamics of Massive Activations in Transformer Training",
            "submittedOnDailyBy": {
                "_id": "646bd4fde96a751c52544801",
                "avatarUrl": "/avatars/e8daa804dab4df52ad6c1b55bb4e3d82.svg",
                "isPro": false,
                "fullname": "Jorge Gallego Feliciano",
                "user": "JorgeeGF",
                "type": "user"
            },
            "summary": "Massive activations are scalar values in transformer hidden states that\nachieve values orders of magnitude larger than typical activations and have\nbeen shown to be critical for model functionality. While prior work has\ncharacterized these phenomena in fully trained models, the temporal dynamics of\ntheir emergence during training remain poorly understood. We present the first\ncomprehensive analysis of massive activation development throughout transformer\ntraining, using the Pythia model family as our testbed. Through systematic\nanalysis of various model sizes across multiple training checkpoints, we\ndemonstrate that massive activation emergence follows predictable mathematical\npatterns that can be accurately modeled using an exponentially-modulated\nlogarithmic function with five key parameters. We develop a machine learning\nframework to predict these mathematical parameters from architectural\nspecifications alone, achieving high accuracy for steady-state behavior and\nmoderate accuracy for emergence timing and magnitude. These findings enable\narchitects to predict and potentially control key aspects of massive activation\nemergence through design choices, with significant implications for model\nstability, training cycle length, interpretability, and optimization. Our\nfindings demonstrate that the emergence of massive activations is governed by\nmodel design and can be anticipated, and potentially controlled, before\ntraining begins.",
            "upvotes": 9,
            "discussionId": "68965081f022d141f5d43166",
            "ai_summary": "The emergence of massive activations in transformer models follows predictable patterns that can be modeled and predicted using architectural specifications, impacting model stability, training duration, and optimization.",
            "ai_keywords": [
                "massive activations",
                "transformer hidden states",
                "Pythia model family",
                "exponentially-modulated logarithmic function",
                "machine learning framework",
                "model design",
                "model stability",
                "training cycle length",
                "interpretability",
                "optimization"
            ]
        },
        "publishedAt": "2025-08-05T12:29:51.000Z",
        "title": "Hidden Dynamics of Massive Activations in Transformer Training",
        "summary": "Massive activations are scalar values in transformer hidden states that\nachieve values orders of magnitude larger than typical activations and have\nbeen shown to be critical for model functionality. While prior work has\ncharacterized these phenomena in fully trained models, the temporal dynamics of\ntheir emergence during training remain poorly understood. We present the first\ncomprehensive analysis of massive activation development throughout transformer\ntraining, using the Pythia model family as our testbed. Through systematic\nanalysis of various model sizes across multiple training checkpoints, we\ndemonstrate that massive activation emergence follows predictable mathematical\npatterns that can be accurately modeled using an exponentially-modulated\nlogarithmic function with five key parameters. We develop a machine learning\nframework to predict these mathematical parameters from architectural\nspecifications alone, achieving high accuracy for steady-state behavior and\nmoderate accuracy for emergence timing and magnitude. These findings enable\narchitects to predict and potentially control key aspects of massive activation\nemergence through design choices, with significant implications for model\nstability, training cycle length, interpretability, and optimization. Our\nfindings demonstrate that the emergence of massive activations is governed by\nmodel design and can be anticipated, and potentially controlled, before\ntraining begins.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03616.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "646bd4fde96a751c52544801",
            "avatarUrl": "/avatars/e8daa804dab4df52ad6c1b55bb4e3d82.svg",
            "fullname": "Jorge Gallego Feliciano",
            "name": "JorgeeGF",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.02831",
            "authors": [
                {
                    "_id": "689473b7741a16f544fbd06f",
                    "user": {
                        "_id": "653064c7800ac851b9b6ac29",
                        "avatarUrl": "/avatars/f8167661d1c80e8a1bcf409c618d63a0.svg",
                        "isPro": false,
                        "fullname": "Mikołaj Zieliński",
                        "user": "MikolajZ",
                        "type": "user"
                    },
                    "name": "Mikołaj Zieliński",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:37:26.983Z",
                    "hidden": false
                },
                {
                    "_id": "689473b7741a16f544fbd070",
                    "name": "Krzysztof Byrski",
                    "hidden": false
                },
                {
                    "_id": "689473b7741a16f544fbd071",
                    "name": "Tomasz Szczepanik",
                    "hidden": false
                },
                {
                    "_id": "689473b7741a16f544fbd072",
                    "name": "Przemysław Spurek",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-04T18:59:23.000Z",
            "submittedOnDailyAt": "2025-08-11T05:05:38.669Z",
            "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
            "submittedOnDailyBy": {
                "_id": "653064c7800ac851b9b6ac29",
                "avatarUrl": "/avatars/f8167661d1c80e8a1bcf409c618d63a0.svg",
                "isPro": false,
                "fullname": "Mikołaj Zieliński",
                "user": "MikolajZ",
                "type": "user"
            },
            "summary": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently\ntransformed 3D scene representation and rendering. NeRF achieves high-fidelity\nnovel view synthesis by learning volumetric representations through neural\nnetworks, but its implicit encoding makes editing and physical interaction\nchallenging. In contrast, GS represents scenes as explicit collections of\nGaussian primitives, enabling real-time rendering, faster training, and more\nintuitive manipulation. This explicit structure has made GS particularly\nwell-suited for interactive editing and integration with physics-based\nsimulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural\nRadiance Fields Interactive Editing), a hybrid model that combines the\nphotorealistic rendering quality of NeRF with the editable and structured\nrepresentation of GS. Instead of using spherical harmonics for appearance\nmodeling, we assign each Gaussian a trainable feature embedding. These\nembeddings are used to condition a NeRF network based on the k nearest\nGaussians to each query point. To make this conditioning efficient, we\nintroduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest\nGaussian search based on a modified ray-tracing pipeline. We also integrate a\nmulti-resolution hash grid to initialize and update Gaussian features.\nTogether, these components enable real-time, locality-aware editing: as\nGaussian primitives are repositioned or modified, their interpolated influence\nis immediately reflected in the rendered output. By combining the strengths of\nimplicit and explicit representations, GENIE supports intuitive scene\nmanipulation, dynamic interaction, and compatibility with physical simulation,\nbridging the gap between geometry-based editing and neural rendering. The code\ncan be found under (https://github.com/MikolajZielinski/genie)",
            "upvotes": 6,
            "discussionId": "689473b7741a16f544fbd073",
            "projectPage": "https://mikolajzielinski.github.io/genie.github.io/",
            "githubRepo": "https://github.com/MikolajZielinski/genie",
            "ai_summary": "GENIE combines NeRF's photorealistic rendering with Gaussian Splatting's editable and structured representation, enabling real-time, locality-aware editing and integration with physics-based simulation.",
            "ai_keywords": [
                "Neural Radiance Fields",
                "Gaussian Splatting",
                "volumetric representations",
                "neural networks",
                "implicit encoding",
                "explicit collections",
                "Gaussian primitives",
                "real-time rendering",
                "faster training",
                "intuitive manipulation",
                "interactive editing",
                "physics-based simulation",
                "Gaussian Encoding for Neural Radiance Fields Interactive Editing",
                "feature embedding",
                "NeRF network",
                "Ray-Traced Gaussian Proximity Search",
                "RT-GPS",
                "multi-resolution hash grid",
                "locality-aware editing",
                "scene manipulation",
                "dynamic interaction"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-08-04T14:59:23.000Z",
        "title": "GENIE: Gaussian Encoding for Neural Radiance Fields Interactive Editing",
        "summary": "Neural Radiance Fields (NeRF) and Gaussian Splatting (GS) have recently\ntransformed 3D scene representation and rendering. NeRF achieves high-fidelity\nnovel view synthesis by learning volumetric representations through neural\nnetworks, but its implicit encoding makes editing and physical interaction\nchallenging. In contrast, GS represents scenes as explicit collections of\nGaussian primitives, enabling real-time rendering, faster training, and more\nintuitive manipulation. This explicit structure has made GS particularly\nwell-suited for interactive editing and integration with physics-based\nsimulation. In this paper, we introduce GENIE (Gaussian Encoding for Neural\nRadiance Fields Interactive Editing), a hybrid model that combines the\nphotorealistic rendering quality of NeRF with the editable and structured\nrepresentation of GS. Instead of using spherical harmonics for appearance\nmodeling, we assign each Gaussian a trainable feature embedding. These\nembeddings are used to condition a NeRF network based on the k nearest\nGaussians to each query point. To make this conditioning efficient, we\nintroduce Ray-Traced Gaussian Proximity Search (RT-GPS), a fast nearest\nGaussian search based on a modified ray-tracing pipeline. We also integrate a\nmulti-resolution hash grid to initialize and update Gaussian features.\nTogether, these components enable real-time, locality-aware editing: as\nGaussian primitives are repositioned or modified, their interpolated influence\nis immediately reflected in the rendered output. By combining the strengths of\nimplicit and explicit representations, GENIE supports intuitive scene\nmanipulation, dynamic interaction, and compatibility with physical simulation,\nbridging the gap between geometry-based editing and neural rendering. The code\ncan be found under (https://github.com/MikolajZielinski/genie)",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02831.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653064c7800ac851b9b6ac29",
            "avatarUrl": "/avatars/f8167661d1c80e8a1bcf409c618d63a0.svg",
            "fullname": "Mikołaj Zieliński",
            "name": "MikolajZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.05547",
            "authors": [
                {
                    "_id": "689979a7f022d141f5d4360e",
                    "name": "Hao Dong",
                    "hidden": false
                },
                {
                    "_id": "689979a7f022d141f5d4360f",
                    "name": "Lijun Sheng",
                    "hidden": false
                },
                {
                    "_id": "689979a7f022d141f5d43610",
                    "name": "Jian Liang",
                    "hidden": false
                },
                {
                    "_id": "689979a7f022d141f5d43611",
                    "name": "Ran He",
                    "hidden": false
                },
                {
                    "_id": "689979a7f022d141f5d43612",
                    "name": "Eleni Chatzi",
                    "hidden": false
                },
                {
                    "_id": "689979a7f022d141f5d43613",
                    "name": "Olga Fink",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-07T16:27:37.000Z",
            "submittedOnDailyAt": "2025-08-11T03:35:44.570Z",
            "title": "Adapting Vision-Language Models Without Labels: A Comprehensive Survey",
            "submittedOnDailyBy": {
                "_id": "6649fb62a460da1da20f66d0",
                "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
                "isPro": false,
                "fullname": "Hao Dong",
                "user": "hdong51",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) have demonstrated remarkable generalization\ncapabilities across a wide range of tasks. However, their performance often\nremains suboptimal when directly applied to specific downstream scenarios\nwithout task-specific adaptation. To enhance their utility while preserving\ndata efficiency, recent research has increasingly focused on unsupervised\nadaptation methods that do not rely on labeled data. Despite the growing\ninterest in this area, there remains a lack of a unified, task-oriented survey\ndedicated to unsupervised VLM adaptation. To bridge this gap, we present a\ncomprehensive and structured overview of the field. We propose a taxonomy based\non the availability and nature of unlabeled visual data, categorizing existing\napproaches into four key paradigms: Data-Free Transfer (no data), Unsupervised\nDomain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),\nand Online Test-Time Adaptation (streaming data). Within this framework, we\nanalyze core methodologies and adaptation strategies associated with each\nparadigm, aiming to establish a systematic understanding of the field.\nAdditionally, we review representative benchmarks across diverse applications\nand highlight open challenges and promising directions for future research. An\nactively maintained repository of relevant literature is available at\nhttps://github.com/tim-learn/Awesome-LabelFree-VLMs.",
            "upvotes": 5,
            "discussionId": "689979a8f022d141f5d43614",
            "projectPage": "https://github.com/tim-learn/Awesome-LabelFree-VLMs",
            "githubRepo": "https://github.com/tim-learn/Awesome-LabelFree-VLMs",
            "ai_summary": "A comprehensive survey of unsupervised adaptation methods for Vision-Language Models (VLMs) categorizes approaches based on the availability of unlabeled visual data and discusses methodologies, benchmarks, and future research directions.",
            "ai_keywords": [
                "Vision-Language Models",
                "unsupervised adaptation",
                "Data-Free Transfer",
                "Unsupervised Domain Transfer",
                "Episodic Test-Time Adaptation",
                "Online Test-Time Adaptation"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-08-07T12:27:37.000Z",
        "title": "Adapting Vision-Language Models Without Labels: A Comprehensive Survey",
        "summary": "Vision-Language Models (VLMs) have demonstrated remarkable generalization\ncapabilities across a wide range of tasks. However, their performance often\nremains suboptimal when directly applied to specific downstream scenarios\nwithout task-specific adaptation. To enhance their utility while preserving\ndata efficiency, recent research has increasingly focused on unsupervised\nadaptation methods that do not rely on labeled data. Despite the growing\ninterest in this area, there remains a lack of a unified, task-oriented survey\ndedicated to unsupervised VLM adaptation. To bridge this gap, we present a\ncomprehensive and structured overview of the field. We propose a taxonomy based\non the availability and nature of unlabeled visual data, categorizing existing\napproaches into four key paradigms: Data-Free Transfer (no data), Unsupervised\nDomain Transfer (abundant data), Episodic Test-Time Adaptation (batch data),\nand Online Test-Time Adaptation (streaming data). Within this framework, we\nanalyze core methodologies and adaptation strategies associated with each\nparadigm, aiming to establish a systematic understanding of the field.\nAdditionally, we review representative benchmarks across diverse applications\nand highlight open challenges and promising directions for future research. An\nactively maintained repository of relevant literature is available at\nhttps://github.com/tim-learn/Awesome-LabelFree-VLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05547.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6649fb62a460da1da20f66d0",
            "avatarUrl": "/avatars/6afa26922ba8abe8c9603e6f7222531b.svg",
            "fullname": "Hao Dong",
            "name": "hdong51",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.05502",
            "authors": [
                {
                    "_id": "68994b7bf022d141f5d434be",
                    "name": "Yufei Gao",
                    "hidden": false
                },
                {
                    "_id": "68994b7bf022d141f5d434bf",
                    "name": "Jiaying Fei",
                    "hidden": false
                },
                {
                    "_id": "68994b7bf022d141f5d434c0",
                    "name": "Nuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68994b7bf022d141f5d434c1",
                    "name": "Ruirui Chen",
                    "hidden": false
                },
                {
                    "_id": "68994b7bf022d141f5d434c2",
                    "name": "Guohang Yan",
                    "hidden": false
                },
                {
                    "_id": "68994b7bf022d141f5d434c3",
                    "name": "Yunshi Lan",
                    "hidden": false
                },
                {
                    "_id": "68994b7bf022d141f5d434c4",
                    "name": "Botian Shi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-07T15:36:24.000Z",
            "submittedOnDailyAt": "2025-08-11T00:17:30.049Z",
            "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for\n  Low-Resource Language MLLMs",
            "submittedOnDailyBy": {
                "_id": "655469586bc4180700cf7a34",
                "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
                "isPro": false,
                "fullname": "Kejia Zhang",
                "user": "KejiaRobust",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable performance in\nhigh-resource languages. However, their effectiveness diminishes significantly\nin the contexts of low-resource languages. Current multilingual enhancement\nmethods are often limited to text modality or rely solely on machine\ntranslation. While such approaches help models acquire basic linguistic\ncapabilities and produce \"thin descriptions\", they neglect the importance of\nmultimodal informativeness and cultural groundedness, both of which are crucial\nfor serving low-resource language users effectively. To bridge this gap, in\nthis study, we identify two significant objectives for a truly effective MLLM\nin low-resource language settings, namely 1) linguistic capability and 2)\ncultural groundedness, placing special emphasis on cultural awareness. To\nachieve these dual objectives, we propose a dual-source strategy that guides\nthe collection of data tailored to each goal, sourcing native web alt-text for\nculture and MLLM-generated captions for linguistics. As a concrete\nimplementation, we introduce MELLA, a multimodal, multilingual dataset.\nExperiment results show that after fine-tuning on MELLA, there is a general\nperformance improvement for the eight languages on various MLLM backbones, with\nmodels producing \"thick descriptions\". We verify that the performance gains are\nfrom both cultural knowledge enhancement and linguistic capability enhancement.\nOur dataset can be found at https://opendatalab.com/applyMultilingualCorpus.",
            "upvotes": 4,
            "discussionId": "68994b7bf022d141f5d434c5",
            "ai_summary": "MELLA, a multimodal, multilingual dataset, enhances MLLMs in low-resource languages by improving linguistic capability and cultural groundedness through native web alt-text and MLLM-generated captions.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "low-resource languages",
                "multilingual enhancement",
                "native web alt-text",
                "MLLM-generated captions",
                "cultural awareness",
                "thick descriptions"
            ]
        },
        "publishedAt": "2025-08-07T11:36:24.000Z",
        "title": "MELLA: Bridging Linguistic Capability and Cultural Groundedness for\n  Low-Resource Language MLLMs",
        "summary": "Multimodal Large Language Models (MLLMs) have shown remarkable performance in\nhigh-resource languages. However, their effectiveness diminishes significantly\nin the contexts of low-resource languages. Current multilingual enhancement\nmethods are often limited to text modality or rely solely on machine\ntranslation. While such approaches help models acquire basic linguistic\ncapabilities and produce \"thin descriptions\", they neglect the importance of\nmultimodal informativeness and cultural groundedness, both of which are crucial\nfor serving low-resource language users effectively. To bridge this gap, in\nthis study, we identify two significant objectives for a truly effective MLLM\nin low-resource language settings, namely 1) linguistic capability and 2)\ncultural groundedness, placing special emphasis on cultural awareness. To\nachieve these dual objectives, we propose a dual-source strategy that guides\nthe collection of data tailored to each goal, sourcing native web alt-text for\nculture and MLLM-generated captions for linguistics. As a concrete\nimplementation, we introduce MELLA, a multimodal, multilingual dataset.\nExperiment results show that after fine-tuning on MELLA, there is a general\nperformance improvement for the eight languages on various MLLM backbones, with\nmodels producing \"thick descriptions\". We verify that the performance gains are\nfrom both cultural knowledge enhancement and linguistic capability enhancement.\nOur dataset can be found at https://opendatalab.com/applyMultilingualCorpus.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05502.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "655469586bc4180700cf7a34",
            "avatarUrl": "/avatars/252392d0c45783d8f149feac7a6215ec.svg",
            "fullname": "Kejia Zhang",
            "name": "KejiaRobust",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.01242",
            "authors": [
                {
                    "_id": "68997c1bf022d141f5d43616",
                    "name": "Shuangkang Fang",
                    "hidden": false
                },
                {
                    "_id": "68997c1bf022d141f5d43617",
                    "name": "I-Chao Shen",
                    "hidden": false
                },
                {
                    "_id": "68997c1bf022d141f5d43618",
                    "name": "Yufeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68997c1bf022d141f5d43619",
                    "name": "Yi-Hsuan Tsai",
                    "hidden": false
                },
                {
                    "_id": "68997c1bf022d141f5d4361a",
                    "name": "Yi Yang",
                    "hidden": false
                },
                {
                    "_id": "68997c1bf022d141f5d4361b",
                    "name": "Shuchang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68997c1bf022d141f5d4361c",
                    "name": "Wenrui Ding",
                    "hidden": false
                },
                {
                    "_id": "68997c1bf022d141f5d4361d",
                    "name": "Takeo Igarashi",
                    "hidden": false
                },
                {
                    "_id": "68997c1bf022d141f5d4361e",
                    "name": "Ming-Hsuan Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-02T07:37:37.000Z",
            "submittedOnDailyAt": "2025-08-11T03:44:55.489Z",
            "title": "MeshLLM: Empowering Large Language Models to Progressively Understand\n  and Generate 3D Mesh",
            "submittedOnDailyBy": {
                "_id": "66ee3663518ccb4e62be37c4",
                "avatarUrl": "/avatars/8d423c9bf63af13969300711a9efa870.svg",
                "isPro": false,
                "fullname": "Shuangkang Fang",
                "user": "fsk515",
                "type": "user"
            },
            "summary": "We present MeshLLM, a novel framework that leverages large language models\n(LLMs) to understand and generate text-serialized 3D meshes. Our approach\naddresses key limitations in existing methods, including the limited dataset\nscale when catering to LLMs' token length and the loss of 3D structural\ninformation during mesh serialization. We introduce a Primitive-Mesh\ndecomposition strategy, which divides 3D meshes into structurally meaningful\nsubunits. This enables the creation of a large-scale dataset with 1500k+\nsamples, almost 50 times larger than previous methods, which aligns better with\nthe LLM scaling law principles. Furthermore, we propose inferring face\nconnectivity from vertices and local mesh assembly training strategies,\nsignificantly enhancing the LLMs' ability to capture mesh topology and spatial\nstructures. Experiments show that MeshLLM outperforms the state-of-the-art\nLLaMA-Mesh in both mesh generation quality and shape understanding,\nhighlighting its great potential in processing text-serialized 3D meshes.",
            "upvotes": 3,
            "discussionId": "68997c2bf022d141f5d4361f",
            "ai_summary": "MeshLLM uses large language models to generate and understand text-serialized 3D meshes by decomposing them into meaningful subunits and training with local mesh assembly strategies.",
            "ai_keywords": [
                "MeshLLM",
                "large language models",
                "LLMs",
                "3D meshes",
                "Primitive-Mesh decomposition",
                "face connectivity",
                "local mesh assembly",
                "mesh topology",
                "spatial structures",
                "LLaMA-Mesh"
            ]
        },
        "publishedAt": "2025-08-02T03:37:37.000Z",
        "title": "MeshLLM: Empowering Large Language Models to Progressively Understand\n  and Generate 3D Mesh",
        "summary": "We present MeshLLM, a novel framework that leverages large language models\n(LLMs) to understand and generate text-serialized 3D meshes. Our approach\naddresses key limitations in existing methods, including the limited dataset\nscale when catering to LLMs' token length and the loss of 3D structural\ninformation during mesh serialization. We introduce a Primitive-Mesh\ndecomposition strategy, which divides 3D meshes into structurally meaningful\nsubunits. This enables the creation of a large-scale dataset with 1500k+\nsamples, almost 50 times larger than previous methods, which aligns better with\nthe LLM scaling law principles. Furthermore, we propose inferring face\nconnectivity from vertices and local mesh assembly training strategies,\nsignificantly enhancing the LLMs' ability to capture mesh topology and spatial\nstructures. Experiments show that MeshLLM outperforms the state-of-the-art\nLLaMA-Mesh in both mesh generation quality and shape understanding,\nhighlighting its great potential in processing text-serialized 3D meshes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01242.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66ee3663518ccb4e62be37c4",
            "avatarUrl": "/avatars/8d423c9bf63af13969300711a9efa870.svg",
            "fullname": "Shuangkang Fang",
            "name": "fsk515",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.04482",
            "authors": [
                {
                    "_id": "6899fdca72289c9658cf4776",
                    "name": "Xueyu Hu",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4777",
                    "name": "Tao Xiong",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4778",
                    "name": "Biao Yi",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4779",
                    "name": "Zishu Wei",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf477a",
                    "name": "Ruixuan Xiao",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf477b",
                    "name": "Yurun Chen",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf477c",
                    "name": "Jiasheng Ye",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf477d",
                    "name": "Meiling Tao",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf477e",
                    "name": "Xiangxin Zhou",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf477f",
                    "name": "Ziyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4780",
                    "name": "Yuhuai Li",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4781",
                    "name": "Shengze Xu",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4782",
                    "name": "Shenzhi Wang",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4783",
                    "name": "Xinchen Xu",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4784",
                    "name": "Shuofei Qiao",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4785",
                    "name": "Zhaokai Wang",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4786",
                    "name": "Kun Kuang",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4787",
                    "name": "Tieyong Zeng",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4788",
                    "name": "Liang Wang",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4789",
                    "name": "Jiwei Li",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf478a",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf478b",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf478c",
                    "name": "Guoyin Wang",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf478d",
                    "name": "Keting Yin",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf478e",
                    "name": "Zhou Zhao",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf478f",
                    "name": "Hongxia Yang",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4790",
                    "name": "Fan Wu",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4791",
                    "name": "Shengyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "6899fdca72289c9658cf4792",
                    "name": "Fei Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T14:33:45.000Z",
            "submittedOnDailyAt": "2025-08-11T12:58:15.195Z",
            "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use",
            "submittedOnDailyBy": {
                "_id": "65897684f8b453e1f57cdb26",
                "avatarUrl": "/avatars/80096d6c808805e1a84a68fb6194a7d4.svg",
                "isPro": false,
                "fullname": "huxueyu",
                "user": "huxueyu",
                "type": "user"
            },
            "summary": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.",
            "upvotes": 2,
            "discussionId": "6899fdca72289c9658cf4793",
            "projectPage": "https://os-agent-survey.github.io/",
            "githubRepo": "https://github.com/OS-Agent-Survey/OS-Agent-Survey",
            "githubStars": 326
        },
        "publishedAt": "2025-08-06T10:33:45.000Z",
        "title": "OS Agents: A Survey on MLLM-based Agents for General Computing Devices\n  Use",
        "summary": "The dream to create AI assistants as capable and versatile as the fictional\nJ.A.R.V.I.S from Iron Man has long captivated imaginations. With the evolution\nof (multi-modal) large language models ((M)LLMs), this dream is closer to\nreality, as (M)LLM-based Agents using computing devices (e.g., computers and\nmobile phones) by operating within the environments and interfaces (e.g.,\nGraphical User Interface (GUI)) provided by operating systems (OS) to automate\ntasks have significantly advanced. This paper presents a comprehensive survey\nof these advanced agents, designated as OS Agents. We begin by elucidating the\nfundamentals of OS Agents, exploring their key components including the\nenvironment, observation space, and action space, and outlining essential\ncapabilities such as understanding, planning, and grounding. We then examine\nmethodologies for constructing OS Agents, focusing on domain-specific\nfoundation models and agent frameworks. A detailed review of evaluation\nprotocols and benchmarks highlights how OS Agents are assessed across diverse\ntasks. Finally, we discuss current challenges and identify promising directions\nfor future research, including safety and privacy, personalization and\nself-evolution. This survey aims to consolidate the state of OS Agents\nresearch, providing insights to guide both academic inquiry and industrial\ndevelopment. An open-source GitHub repository is maintained as a dynamic\nresource to foster further innovation in this field. We present a 9-page\nversion of our work, accepted by ACL 2025, to provide a concise overview to the\ndomain.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04482.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65897684f8b453e1f57cdb26",
            "avatarUrl": "/avatars/80096d6c808805e1a84a68fb6194a7d4.svg",
            "fullname": "huxueyu",
            "name": "huxueyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2507.22025",
            "authors": [
                {
                    "_id": "6895adf548b0ae5ca2710e71",
                    "user": {
                        "_id": "669f53549a21428ccda89fab",
                        "avatarUrl": "/avatars/957c8251615ab4552f2e286ef7445c58.svg",
                        "isPro": false,
                        "fullname": "LianShuQuan",
                        "user": "LianShuQuan",
                        "type": "user"
                    },
                    "name": "Shuquan Lian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-08T16:19:02.176Z",
                    "hidden": false
                },
                {
                    "_id": "6895adf548b0ae5ca2710e72",
                    "name": "Yuhang Wu",
                    "hidden": false
                },
                {
                    "_id": "6895adf548b0ae5ca2710e73",
                    "name": "Jia Ma",
                    "hidden": false
                },
                {
                    "_id": "6895adf548b0ae5ca2710e74",
                    "name": "Zihan Song",
                    "hidden": false
                },
                {
                    "_id": "6895adf548b0ae5ca2710e75",
                    "name": "Bingqi Chen",
                    "hidden": false
                },
                {
                    "_id": "6895adf548b0ae5ca2710e76",
                    "name": "Xiawu Zheng",
                    "hidden": false
                },
                {
                    "_id": "6895adf548b0ae5ca2710e77",
                    "name": "Hui Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-07-29T17:22:07.000Z",
            "submittedOnDailyAt": "2025-08-11T03:39:00.218Z",
            "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding",
            "submittedOnDailyBy": {
                "_id": "669f53549a21428ccda89fab",
                "avatarUrl": "/avatars/957c8251615ab4552f2e286ef7445c58.svg",
                "isPro": false,
                "fullname": "LianShuQuan",
                "user": "LianShuQuan",
                "type": "user"
            },
            "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.",
            "upvotes": 2,
            "discussionId": "6895adf648b0ae5ca2710e78",
            "ai_summary": "UI-AGILE enhances GUI agents through improved training with a Continuous Reward function, Simple Thinking reward, and Cropping-based Resampling, and inference with Decomposed Grounding with Selection, achieving state-of-the-art performance on GUI benchmarks.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "GUI agents",
                "Supervised Fine-Tuning",
                "Continuous Reward function",
                "Simple Thinking reward",
                "Cropping-based Resampling",
                "Decomposed Grounding with Selection",
                "ScreenSpot-Pro",
                "ScreenSpot-v2"
            ]
        },
        "publishedAt": "2025-07-29T13:22:07.000Z",
        "title": "UI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and\n  Precise Inference-Time Grounding",
        "summary": "The emergence of Multimodal Large Language Models (MLLMs) has driven\nsignificant advances in Graphical User Interface (GUI) agent capabilities.\nNevertheless, existing GUI agent training and inference techniques still suffer\nfrom a dilemma for reasoning designs, ineffective reward, and visual noise. To\naddress these issues, we introduce UI-AGILE, a comprehensive framework\nenhancing GUI agents at both the training and inference stages. For training,\nwe propose a suite of improvements to the Supervised Fine-Tuning (SFT) process:\n1) a Continuous Reward function to incentivize high-precision grounding; 2) a\n\"Simple Thinking\" reward to balance planning with speed and grounding accuracy;\nand 3) a Cropping-based Resampling strategy to mitigate the sparse reward\nproblem and improve learning on complex tasks. For inference, we present\nDecomposed Grounding with Selection, a novel method that dramatically improves\ngrounding accuracy on high-resolution displays by breaking the image into\nsmaller, manageable parts. Experiments show that UI-AGILE achieves the\nstate-of-the-art performance on two benchmarks ScreenSpot-Pro and\nScreenSpot-v2. For instance, using both our proposed training and inference\nenhancement methods brings 23% grounding accuracy improvement over the best\nbaseline on ScreenSpot-Pro.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2507.22025.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "669f53549a21428ccda89fab",
            "avatarUrl": "/avatars/957c8251615ab4552f2e286ef7445c58.svg",
            "fullname": "LianShuQuan",
            "name": "LianShuQuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.06494",
            "authors": [
                {
                    "_id": "68995f65f022d141f5d4353b",
                    "user": {
                        "_id": "646ff038799a974be31bb344",
                        "avatarUrl": "/avatars/d9dc17246fba8360e709235f55445ef5.svg",
                        "isPro": false,
                        "fullname": "Yehonathan Litman",
                        "user": "thebluser",
                        "type": "user"
                    },
                    "name": "Yehonathan Litman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:45:39.834Z",
                    "hidden": false
                },
                {
                    "_id": "68995f65f022d141f5d4353c",
                    "name": "Fernando De la Torre",
                    "hidden": false
                },
                {
                    "_id": "68995f65f022d141f5d4353d",
                    "name": "Shubham Tulsiani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-08T17:59:52.000Z",
            "submittedOnDailyAt": "2025-08-11T08:25:34.277Z",
            "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion",
            "submittedOnDailyBy": {
                "_id": "646ff038799a974be31bb344",
                "avatarUrl": "/avatars/d9dc17246fba8360e709235f55445ef5.svg",
                "isPro": false,
                "fullname": "Yehonathan Litman",
                "user": "thebluser",
                "type": "user"
            },
            "summary": "Recent approaches for 3D relighting have shown promise in integrating 2D\nimage relighting generative priors to alter the appearance of a 3D\nrepresentation while preserving the underlying structure. Nevertheless,\ngenerative priors used for 2D relighting that directly relight from an input\nimage do not take advantage of intrinsic properties of the subject that can be\ninferred or cannot consider multi-view data at scale, leading to subpar\nrelighting. In this paper, we propose Lightswitch, a novel finetuned\nmaterial-relighting diffusion framework that efficiently relights an arbitrary\nnumber of input images to a target lighting condition while incorporating cues\nfrom inferred intrinsic properties. By using multi-view and material\ninformation cues together with a scalable denoising scheme, our method\nconsistently and efficiently relights dense multi-view data of objects with\ndiverse material compositions. We show that our 2D relighting prediction\nquality exceeds previous state-of-the-art relighting priors that directly\nrelight from images. We further demonstrate that LightSwitch matches or\noutperforms state-of-the-art diffusion inverse rendering methods in relighting\nsynthetic and real objects in as little as 2 minutes.",
            "upvotes": 1,
            "discussionId": "68995f65f022d141f5d4353e",
            "ai_summary": "Lightswitch, a material-relighting diffusion framework, enhances 3D relighting by integrating multi-view and material cues, achieving superior quality and efficiency compared to existing methods.",
            "ai_keywords": [
                "material-relighting diffusion framework",
                "denoising scheme",
                "multi-view data",
                "intrinsic properties",
                "2D relighting prediction",
                "diffusion inverse rendering"
            ]
        },
        "publishedAt": "2025-08-08T13:59:52.000Z",
        "title": "LightSwitch: Multi-view Relighting with Material-guided Diffusion",
        "summary": "Recent approaches for 3D relighting have shown promise in integrating 2D\nimage relighting generative priors to alter the appearance of a 3D\nrepresentation while preserving the underlying structure. Nevertheless,\ngenerative priors used for 2D relighting that directly relight from an input\nimage do not take advantage of intrinsic properties of the subject that can be\ninferred or cannot consider multi-view data at scale, leading to subpar\nrelighting. In this paper, we propose Lightswitch, a novel finetuned\nmaterial-relighting diffusion framework that efficiently relights an arbitrary\nnumber of input images to a target lighting condition while incorporating cues\nfrom inferred intrinsic properties. By using multi-view and material\ninformation cues together with a scalable denoising scheme, our method\nconsistently and efficiently relights dense multi-view data of objects with\ndiverse material compositions. We show that our 2D relighting prediction\nquality exceeds previous state-of-the-art relighting priors that directly\nrelight from images. We further demonstrate that LightSwitch matches or\noutperforms state-of-the-art diffusion inverse rendering methods in relighting\nsynthetic and real objects in as little as 2 minutes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06494.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "646ff038799a974be31bb344",
            "avatarUrl": "/avatars/d9dc17246fba8360e709235f55445ef5.svg",
            "fullname": "Yehonathan Litman",
            "name": "thebluser",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.02095",
            "authors": [
                {
                    "_id": "689a8e78fab6fdd2e52ac3f6",
                    "name": "Shijie Zhou",
                    "hidden": false
                },
                {
                    "_id": "689a8e78fab6fdd2e52ac3f7",
                    "name": "Alexander Vilesov",
                    "hidden": false
                },
                {
                    "_id": "689a8e78fab6fdd2e52ac3f8",
                    "name": "Xuehai He",
                    "hidden": false
                },
                {
                    "_id": "689a8e78fab6fdd2e52ac3f9",
                    "name": "Ziyu Wan",
                    "hidden": false
                },
                {
                    "_id": "689a8e78fab6fdd2e52ac3fa",
                    "name": "Shuwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "689a8e78fab6fdd2e52ac3fb",
                    "name": "Aditya Nagachandra",
                    "hidden": false
                },
                {
                    "_id": "689a8e78fab6fdd2e52ac3fc",
                    "name": "Di Chang",
                    "hidden": false
                },
                {
                    "_id": "689a8e78fab6fdd2e52ac3fd",
                    "name": "Dongdong Chen",
                    "hidden": false
                },
                {
                    "_id": "689a8e78fab6fdd2e52ac3fe",
                    "name": "Xin Eric Wang",
                    "hidden": false
                },
                {
                    "_id": "689a8e78fab6fdd2e52ac3ff",
                    "name": "Achuta Kadambi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/642a276516d4d8293c9a47e8/ubEMGC_dccNyMPQd3A4hp.png"
            ],
            "publishedAt": "2025-08-04T06:06:06.000Z",
            "submittedOnDailyAt": "2025-08-11T23:20:38.295Z",
            "title": "VLM4D: Towards Spatiotemporal Awareness in Vision Language Models",
            "submittedOnDailyBy": {
                "_id": "642a276516d4d8293c9a47e8",
                "avatarUrl": "/avatars/80e6db8bc2544f3486b11b57858a8692.svg",
                "isPro": false,
                "fullname": "Shijie Zhou",
                "user": "shijiezhou",
                "type": "user"
            },
            "summary": "Vision language models (VLMs) have shown remarkable capabilities in\nintegrating linguistic and visual reasoning but remain fundamentally limited in\nunderstanding dynamic spatiotemporal interactions. Humans effortlessly track\nand reason about object movements, rotations, and perspective shifts-abilities\nessential for robust dynamic real-world understanding yet notably lacking in\ncurrent VLMs. In this paper, we introduce VLM4D, the first benchmark\nspecifically designed to evaluate the spatiotemporal reasoning capabilities of\nVLMs. Our benchmark comprises diverse real-world and synthetic videos\naccompanied by carefully curated question-answer pairs emphasizing\ntranslational and rotational motions, perspective awareness, and motion\ncontinuity. Through comprehensive evaluations of state-of-the-art open and\nclosed-source VLMs, we identify significant performance gaps compared to human\nbaselines, highlighting fundamental deficiencies in existing models. Extensive\nanalysis reveals that VLMs struggle particularly with integrating multiple\nvisual cues and maintaining temporal coherence. We further explore promising\ndirections, such as leveraging 4D feature field reconstruction and targeted\nspatiotemporal supervised fine-tuning, demonstrating their effectiveness in\nenhancing spatiotemporal comprehension. Our work aims to encourage deeper\nexploration into improving VLMs' spatial and temporal grounding, paving the way\ntowards more capable and reliable visual intelligence for dynamic environments.",
            "upvotes": 1,
            "discussionId": "689a8e79fab6fdd2e52ac400",
            "ai_summary": "A benchmark evaluates VLMs' spatiotemporal reasoning, identifying gaps and suggesting improvements like 4D feature field reconstruction and fine-tuning.",
            "ai_keywords": [
                "VLMs",
                "spatiotemporal reasoning",
                "VLM4D",
                "real-world videos",
                "synthetic videos",
                "question-answer pairs",
                "translational motions",
                "rotational motions",
                "perspective awareness",
                "motion continuity",
                "state-of-the-art VLMs",
                "human baselines",
                "visual cues",
                "temporal coherence",
                "4D feature field reconstruction",
                "spatiotemporal supervised fine-tuning"
            ]
        },
        "publishedAt": "2025-08-04T02:06:06.000Z",
        "title": "VLM4D: Towards Spatiotemporal Awareness in Vision Language Models",
        "summary": "Vision language models (VLMs) have shown remarkable capabilities in\nintegrating linguistic and visual reasoning but remain fundamentally limited in\nunderstanding dynamic spatiotemporal interactions. Humans effortlessly track\nand reason about object movements, rotations, and perspective shifts-abilities\nessential for robust dynamic real-world understanding yet notably lacking in\ncurrent VLMs. In this paper, we introduce VLM4D, the first benchmark\nspecifically designed to evaluate the spatiotemporal reasoning capabilities of\nVLMs. Our benchmark comprises diverse real-world and synthetic videos\naccompanied by carefully curated question-answer pairs emphasizing\ntranslational and rotational motions, perspective awareness, and motion\ncontinuity. Through comprehensive evaluations of state-of-the-art open and\nclosed-source VLMs, we identify significant performance gaps compared to human\nbaselines, highlighting fundamental deficiencies in existing models. Extensive\nanalysis reveals that VLMs struggle particularly with integrating multiple\nvisual cues and maintaining temporal coherence. We further explore promising\ndirections, such as leveraging 4D feature field reconstruction and targeted\nspatiotemporal supervised fine-tuning, demonstrating their effectiveness in\nenhancing spatiotemporal comprehension. Our work aims to encourage deeper\nexploration into improving VLMs' spatial and temporal grounding, paving the way\ntowards more capable and reliable visual intelligence for dynamic environments.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642a276516d4d8293c9a47e8/ubEMGC_dccNyMPQd3A4hp.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.02095.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642a276516d4d8293c9a47e8",
            "avatarUrl": "/avatars/80e6db8bc2544f3486b11b57858a8692.svg",
            "fullname": "Shijie Zhou",
            "name": "shijiezhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    }
]