[
    {
        "paper": {
            "id": "2508.05748",
            "authors": [
                {
                    "_id": "689c0152fab6fdd2e52ac85d",
                    "user": {
                        "_id": "682b22ebac526172e1b4ed1b",
                        "avatarUrl": "/avatars/a9e486bf72d27013e6c1903b64a7754c.svg",
                        "isPro": false,
                        "fullname": "Geng Xinyu",
                        "user": "Ornamentt",
                        "type": "user"
                    },
                    "name": "Xinyu Geng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:48.684Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac85e",
                    "user": {
                        "_id": "643e9ee6f6bb3c31a26e7bc4",
                        "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                        "isPro": false,
                        "fullname": "Peng Xia",
                        "user": "richardxp888",
                        "type": "user"
                    },
                    "name": "Peng Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:38.450Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac85f",
                    "user": {
                        "_id": "646abd0f8dfd6ff79b6cfbb9",
                        "avatarUrl": "/avatars/3de3b7cbeda95c2b4f460f87f8e9a1f7.svg",
                        "isPro": false,
                        "fullname": "ZhenZhang",
                        "user": "zhzhen23",
                        "type": "user"
                    },
                    "name": "Zhen Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:20:47.118Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac860",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac861",
                    "name": "Qiuchen Wang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac862",
                    "name": "Ruixue Ding",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac863",
                    "name": "Chenxi Wang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac864",
                    "user": {
                        "_id": "644a4fbc2166258fccc664bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                        "isPro": false,
                        "fullname": "Jialong Wu",
                        "user": "callanwu",
                        "type": "user"
                    },
                    "name": "Jialong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:43.643Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac865",
                    "user": {
                        "_id": "66e4019518a1920fb7ca19d7",
                        "avatarUrl": "/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg",
                        "isPro": false,
                        "fullname": "Yida Zhao",
                        "user": "zhaoyd",
                        "type": "user"
                    },
                    "name": "Yida Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:34.963Z",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac866",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac867",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac868",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac869",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "689c0152fab6fdd2e52ac86a",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/1DotdrNI5gc_sKLMxtlnq.png",
                "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/VqGc9-ADKeMvExKGjvaWU.png"
            ],
            "publishedAt": "2025-08-07T18:03:50.000Z",
            "submittedOnDailyAt": "2025-08-13T02:05:16.133Z",
            "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
            "submittedOnDailyBy": {
                "_id": "643e9ee6f6bb3c31a26e7bc4",
                "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
                "isPro": false,
                "fullname": "Peng Xia",
                "user": "richardxp888",
                "type": "user"
            },
            "summary": "Web agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.",
            "upvotes": 84,
            "discussionId": "689c0152fab6fdd2e52ac86b",
            "githubRepo": "https://github.com/Alibaba-NLP/WebAgent//",
            "ai_summary": "WebWatcher, a multimodal agent with enhanced visual-language reasoning, outperforms existing agents in complex visual and textual information retrieval tasks using synthetic trajectories and reinforcement learning.",
            "ai_keywords": [
                "multimodal",
                "visual-language reasoning",
                "high-quality synthetic multimodal trajectories",
                "reinforcement learning",
                "BrowseComp-VL",
                "VQA benchmarks"
            ],
            "githubStars": 5999
        },
        "publishedAt": "2025-08-07T14:03:50.000Z",
        "title": "WebWatcher: Breaking New Frontier of Vision-Language Deep Research Agent",
        "summary": "Web agents such as Deep Research have demonstrated superhuman cognitive\nabilities, capable of solving highly challenging information-seeking problems.\nHowever, most research remains primarily text-centric, overlooking visual\ninformation in the real world. This makes multimodal Deep Research highly\nchallenging, as such agents require much stronger reasoning abilities in\nperception, logic, knowledge, and the use of more sophisticated tools compared\nto text-based agents. To address this limitation, we introduce WebWatcher, a\nmulti-modal Agent for Deep Research equipped with enhanced visual-language\nreasoning capabilities. It leverages high-quality synthetic multimodal\ntrajectories for efficient cold start training, utilizes various tools for deep\nreasoning, and further enhances generalization through reinforcement learning.\nTo better evaluate the capabilities of multimodal agents, we propose\nBrowseComp-VL, a benchmark with BrowseComp-style that requires complex\ninformation retrieval involving both visual and textual information.\nExperimental results show that WebWatcher significantly outperforms proprietary\nbaseline, RAG workflow and open-source agents in four challenging VQA\nbenchmarks, which paves the way for solving complex multimodal\ninformation-seeking tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/1DotdrNI5gc_sKLMxtlnq.png",
            "https://cdn-uploads.huggingface.co/production/uploads/643e9ee6f6bb3c31a26e7bc4/VqGc9-ADKeMvExKGjvaWU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05748.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "643e9ee6f6bb3c31a26e7bc4",
            "avatarUrl": "/avatars/acfaa7d6a23dada24c86b954c3be116a.svg",
            "fullname": "Peng Xia",
            "name": "richardxp888",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.08086",
            "authors": [
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e0",
                    "name": "Zhongqi Yang",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e1",
                    "name": "Wenhang Ge",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e2",
                    "name": "Yuqi Li",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e3",
                    "name": "Jiaqi Chen",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e4",
                    "name": "Haoyuan Li",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e5",
                    "user": {
                        "_id": "67a9664af5f1253c64259c50",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Jk9W1C8704pqxNOsqZ0d9.png",
                        "isPro": false,
                        "fullname": "an",
                        "user": "dearamy",
                        "type": "user"
                    },
                    "name": "Mengyin An",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:19:49.471Z",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e6",
                    "user": {
                        "_id": "67a9b36a2fbf63093c19d3de",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67a9b36a2fbf63093c19d3de/LbB2XH0ezcJ_tdu4hU0Of.png",
                        "isPro": false,
                        "fullname": "ËÄÅk",
                        "user": "kangfei",
                        "type": "user"
                    },
                    "name": "Fei Kang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:19:46.490Z",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e7",
                    "name": "Hua Xue",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e8",
                    "name": "Baixin Xu",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4e9",
                    "name": "Yuyang Yin",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4ea",
                    "name": "Eric Li",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4eb",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4ec",
                    "name": "Yikai Wang",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4ed",
                    "name": "Hao-Xiang Guo",
                    "hidden": false
                },
                {
                    "_id": "689ac0a0fab6fdd2e52ac4ee",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64758ee7d815855e4efa206b/gD6oavgX7cDu24qQLbXir.png"
            ],
            "publishedAt": "2025-08-11T15:29:57.000Z",
            "submittedOnDailyAt": "2025-08-13T00:45:23.058Z",
            "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
            "submittedOnDailyBy": {
                "_id": "64758ee7d815855e4efa206b",
                "avatarUrl": "/avatars/105ada08a9a982fc7b723bdc678f7e72.svg",
                "isPro": false,
                "fullname": "wenhang ge",
                "user": "spongy",
                "type": "user"
            },
            "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.",
            "upvotes": 51,
            "discussionId": "689ac0a0fab6fdd2e52ac4ef",
            "projectPage": "https://matrix-3d.github.io/",
            "githubRepo": "https://github.com/SkyworkAI/Matrix-3D/tree/main",
            "ai_summary": "Matrix-3D generates wide-coverage 3D worlds from single images or text using panoramic video diffusion and reconstruction models.",
            "ai_keywords": [
                "panoramic representation",
                "wide-coverage",
                "omnidirectional",
                "explorable 3D world generation",
                "conditional video generation",
                "panoramic 3D reconstruction",
                "trajectory-guided",
                "panoramic video diffusion model",
                "scene mesh renders",
                "feed-forward large panorama reconstruction model",
                "optimization-based pipeline",
                "Matrix-Pano dataset",
                "panoramic video generation",
                "3D world generation"
            ],
            "githubStars": 256
        },
        "publishedAt": "2025-08-11T11:29:57.000Z",
        "title": "Matrix-3D: Omnidirectional Explorable 3D World Generation",
        "summary": "Explorable 3D world generation from a single image or text prompt forms a\ncornerstone of spatial intelligence. Recent works utilize video model to\nachieve wide-scope and generalizable 3D world generation. However, existing\napproaches often suffer from a limited scope in the generated scenes. In this\nwork, we propose Matrix-3D, a framework that utilize panoramic representation\nfor wide-coverage omnidirectional explorable 3D world generation that combines\nconditional video generation and panoramic 3D reconstruction. We first train a\ntrajectory-guided panoramic video diffusion model that employs scene mesh\nrenders as condition, to enable high-quality and geometrically consistent scene\nvideo generation. To lift the panorama scene video to 3D world, we propose two\nseparate methods: (1) a feed-forward large panorama reconstruction model for\nrapid 3D scene reconstruction and (2) an optimization-based pipeline for\naccurate and detailed 3D scene reconstruction. To facilitate effective\ntraining, we also introduce the Matrix-Pano dataset, the first large-scale\nsynthetic collection comprising 116K high-quality static panoramic video\nsequences with depth and trajectory annotations. Extensive experiments\ndemonstrate that our proposed framework achieves state-of-the-art performance\nin panoramic video generation and 3D world generation. See more in\nhttps://matrix-3d.github.io.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64758ee7d815855e4efa206b/gD6oavgX7cDu24qQLbXir.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08086.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64758ee7d815855e4efa206b",
            "avatarUrl": "/avatars/105ada08a9a982fc7b723bdc678f7e72.svg",
            "fullname": "wenhang ge",
            "name": "spongy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.07976",
            "authors": [
                {
                    "_id": "689bf8c2fab6fdd2e52ac825",
                    "name": "Jiaxuan Gao",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac826",
                    "name": "Wei Fu",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac827",
                    "name": "Minyang Xie",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac828",
                    "name": "Shusheng Xu",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac829",
                    "name": "Chuyi He",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac82a",
                    "name": "Zhiyu Mei",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac82b",
                    "name": "Banghua Zhu",
                    "hidden": false
                },
                {
                    "_id": "689bf8c2fab6fdd2e52ac82c",
                    "name": "Yi Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T13:36:57.000Z",
            "submittedOnDailyAt": "2025-08-13T01:34:49.634Z",
            "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
            "submittedOnDailyBy": {
                "_id": "63159678915d0b80682fe9f9",
                "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
                "isPro": false,
                "fullname": "Shusheng Xu",
                "user": "xssstory",
                "type": "user"
            },
            "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
            "upvotes": 34,
            "discussionId": "689bf8c2fab6fdd2e52ac82d",
            "githubRepo": "https://github.com/inclusionAI/ASearcher",
            "ai_summary": "ASearcher is an open-source project that uses scalable asynchronous RL training to enhance search agents, achieving high performance on QA tasks with long-horizon search capabilities.",
            "ai_keywords": [
                "LLM-based agents",
                "search tools",
                "Search Intelligence",
                "open-source agents",
                "online RL methods",
                "fully asynchronous RL training",
                "prompt-based LLM agent",
                "QA dataset",
                "xBench",
                "GAIA",
                "Avg@4",
                "tool calls",
                "output tokens",
                "ASearcher-Web-QwQ"
            ],
            "githubStars": 111
        },
        "publishedAt": "2025-08-11T09:36:57.000Z",
        "title": "Beyond Ten Turns: Unlocking Long-Horizon Agentic Search with Large-Scale\n  Asynchronous RL",
        "summary": "Recent advancements in LLM-based agents have demonstrated remarkable\ncapabilities in handling complex, knowledge-intensive tasks by integrating\nexternal tools. Among diverse choices of tools, search tools play a pivotal\nrole in accessing vast external knowledge. However, open-source agents still\nfall short of achieving expert-level Search Intelligence, the ability to\nresolve ambiguous queries, generate precise searches, analyze results, and\nconduct thorough exploration. Existing approaches fall short in scalability,\nefficiency, and data quality. For example, small turn limits in existing online\nRL methods, e.g. <=10, restrict complex strategy learning. This paper\nintroduces ASearcher, an open-source project for large-scale RL training of\nsearch agents. Our key contributions include: (1) Scalable fully asynchronous\nRL training that enables long-horizon search while maintaining high training\nefficiency. (2) A prompt-based LLM agent that autonomously synthesizes\nhigh-quality and challenging QAs, creating a large-scale QA dataset. Through RL\ntraining, our prompt-based QwQ-32B agent achieves substantial improvements,\nwith 46.7% and 20.8% Avg@4 gains on xBench and GAIA, respectively. Notably, our\nagent exhibits extreme long-horizon search, with tool calls exceeding 40 turns\nand output tokens exceeding 150k during training time. With a simple agent\ndesign and no external LLMs, ASearcher-Web-QwQ achieves Avg@4 scores of 42.1 on\nxBench and 52.8 on GAIA, surpassing existing open-source 32B agents. We\nopen-source our models, training data, and codes in\nhttps://github.com/inclusionAI/ASearcher.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07976.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63159678915d0b80682fe9f9",
            "avatarUrl": "/avatars/a19e44ffe681099a155b8f8ecb53f0ce.svg",
            "fullname": "Shusheng Xu",
            "name": "xssstory",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.07409",
            "authors": [
                {
                    "_id": "689c1928fab6fdd2e52ac8a0",
                    "user": {
                        "_id": "663b5f08389689b517109b77",
                        "avatarUrl": "/avatars/b8d66083b9e15c7f7924b4bbc8ca3861.svg",
                        "isPro": false,
                        "fullname": "Gaojunyao",
                        "user": "Gaojunyao",
                        "type": "user"
                    },
                    "name": "Junyao Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:20.003Z",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a1",
                    "user": {
                        "_id": "65464049e70ffa3c07f22e92",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EAYwKThe-BGeza-CmzIdx.jpeg",
                        "isPro": false,
                        "fullname": " Li Jiaxing",
                        "user": "LiJiaxing",
                        "type": "user"
                    },
                    "name": "Jiaxing Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:23.461Z",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a2",
                    "name": "Wenran Liu",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a3",
                    "name": "Yanhong Zeng",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a4",
                    "name": "Fei Shen",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a5",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a6",
                    "name": "Yanan Sun",
                    "hidden": false
                },
                {
                    "_id": "689c1928fab6fdd2e52ac8a7",
                    "name": "Cairong Zhao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/663b5f08389689b517109b77/unLyeVoJYZogIaDHKufWQ.gif"
            ],
            "publishedAt": "2025-08-10T16:15:04.000Z",
            "submittedOnDailyAt": "2025-08-13T03:19:56.895Z",
            "title": "CharacterShot: Controllable and Consistent 4D Character Animation",
            "submittedOnDailyBy": {
                "_id": "663b5f08389689b517109b77",
                "avatarUrl": "/avatars/b8d66083b9e15c7f7924b4bbc8ca3861.svg",
                "isPro": false,
                "fullname": "Gaojunyao",
                "user": "Gaojunyao",
                "type": "user"
            },
            "summary": "In this paper, we propose CharacterShot, a controllable and\nconsistent 4D character animation framework that enables any individual\ndesigner to create dynamic 3D characters (i.e., 4D character animation) from a\nsingle reference character image and a 2D pose sequence. We begin by\npretraining a powerful 2D character animation model based on a cutting-edge\nDiT-based image-to-video model, which allows for any 2D pose sequnce as\ncontrollable signal. We then lift the animation model from 2D to 3D through\nintroducing dual-attention module together with camera prior to generate\nmulti-view videos with spatial-temporal and spatial-view consistency. Finally,\nwe employ a novel neighbor-constrained 4D gaussian splatting optimization on\nthese multi-view videos, resulting in continuous and stable 4D character\nrepresentations. Moreover, to improve character-centric performance, we\nconstruct a large-scale dataset Character4D, containing 13,115 unique\ncharacters with diverse appearances and motions, rendered from multiple\nviewpoints. Extensive experiments on our newly constructed benchmark,\nCharacterBench, demonstrate that our approach outperforms current\nstate-of-the-art methods. Code, models, and datasets will be publicly available\nat https://github.com/Jeoyal/CharacterShot.",
            "upvotes": 28,
            "discussionId": "689c1928fab6fdd2e52ac8a8",
            "githubRepo": "https://github.com/Jeoyal/CharacterShot",
            "ai_summary": "CharacterShot is a 4D character animation framework that uses a DiT-based model and dual-attention module to generate consistent 3D animations from a single image and 2D pose sequence.",
            "ai_keywords": [
                "DiT-based image-to-video model",
                "dual-attention module",
                "camera prior",
                "neighbor-constrained 4D gaussian splatting",
                "Character4D dataset",
                "CharacterBench benchmark"
            ],
            "githubStars": 22
        },
        "publishedAt": "2025-08-10T12:15:04.000Z",
        "title": "CharacterShot: Controllable and Consistent 4D Character Animation",
        "summary": "In this paper, we propose CharacterShot, a controllable and\nconsistent 4D character animation framework that enables any individual\ndesigner to create dynamic 3D characters (i.e., 4D character animation) from a\nsingle reference character image and a 2D pose sequence. We begin by\npretraining a powerful 2D character animation model based on a cutting-edge\nDiT-based image-to-video model, which allows for any 2D pose sequnce as\ncontrollable signal. We then lift the animation model from 2D to 3D through\nintroducing dual-attention module together with camera prior to generate\nmulti-view videos with spatial-temporal and spatial-view consistency. Finally,\nwe employ a novel neighbor-constrained 4D gaussian splatting optimization on\nthese multi-view videos, resulting in continuous and stable 4D character\nrepresentations. Moreover, to improve character-centric performance, we\nconstruct a large-scale dataset Character4D, containing 13,115 unique\ncharacters with diverse appearances and motions, rendered from multiple\nviewpoints. Extensive experiments on our newly constructed benchmark,\nCharacterBench, demonstrate that our approach outperforms current\nstate-of-the-art methods. Code, models, and datasets will be publicly available\nat https://github.com/Jeoyal/CharacterShot.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/663b5f08389689b517109b77/unLyeVoJYZogIaDHKufWQ.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07409.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "663b5f08389689b517109b77",
            "avatarUrl": "/avatars/b8d66083b9e15c7f7924b4bbc8ca3861.svg",
            "fullname": "Gaojunyao",
            "name": "Gaojunyao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09138",
            "authors": [
                {
                    "_id": "689befdbfab6fdd2e52ac80c",
                    "name": "Wen Wang",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac80d",
                    "name": "Bozhen Fang",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac80e",
                    "name": "Chenchen Jing",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac80f",
                    "user": {
                        "_id": "5e1058e9fcf41d740b69966d",
                        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                        "isPro": false,
                        "fullname": "Yongliang Shen",
                        "user": "tricktreat",
                        "type": "user"
                    },
                    "name": "Yongliang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:13:02.765Z",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac810",
                    "name": "Yangyi Shen",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac811",
                    "user": {
                        "_id": "64981bea09cea550852652af",
                        "avatarUrl": "/avatars/df528e9008972c8e5ae4d278e617476c.svg",
                        "isPro": false,
                        "fullname": "Qiuyu Wang",
                        "user": "qiuyuu",
                        "type": "user"
                    },
                    "name": "Qiuyu Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:13:07.409Z",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac812",
                    "name": "Hao Ouyang",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac813",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "689befdbfab6fdd2e52ac814",
                    "name": "Chunhua Shen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63f089456309c84d5f47f951/znHPdJqj7uNrvTS4acpqM.png"
            ],
            "publishedAt": "2025-08-12T17:59:57.000Z",
            "submittedOnDailyAt": "2025-08-13T00:24:54.833Z",
            "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "63f089456309c84d5f47f951",
                "avatarUrl": "/avatars/04b926a7f2ad091ee00fef0c59903492.svg",
                "isPro": false,
                "fullname": "Wen Wang",
                "user": "wwen1997",
                "type": "user"
            },
            "summary": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.",
            "upvotes": 26,
            "discussionId": "689befdbfab6fdd2e52ac815",
            "projectPage": "https://aim-uofa.github.io/dLLM-MidTruth/",
            "githubRepo": "https://github.com/aim-uofa/dLLM-MidTruth",
            "ai_summary": "Two methods, Temporal Self-Consistency Voting and Temporal Consistency Reinforcement, improve diffusion large language models by leveraging temporal consistency in intermediate predictions.",
            "ai_keywords": [
                "diffusion large language models",
                "denoising",
                "temporal oscillation",
                "Temporal Self-Consistency Voting",
                "Temporal Consistency Reinforcement",
                "Temporal Semantic Entropy",
                "semantic stability",
                "Countdown dataset",
                "GSM8K",
                "MATH500",
                "SVAMP"
            ],
            "githubStars": 30
        },
        "publishedAt": "2025-08-12T13:59:57.000Z",
        "title": "Time Is a Feature: Exploiting Temporal Dynamics in Diffusion Language\n  Models",
        "summary": "Diffusion large language models (dLLMs) generate text through iterative\ndenoising, yet current decoding strategies discard rich intermediate\npredictions in favor of the final output. Our work here reveals a critical\nphenomenon, temporal oscillation, where correct answers often emerge in the\nmiddle process, but are overwritten in later denoising steps. To address this\nissue, we introduce two complementary methods that exploit temporal\nconsistency: 1) Temporal Self-Consistency Voting, a training-free, test-time\ndecoding strategy that aggregates predictions across denoising steps to select\nthe most consistent output; and 2) a post-training method termed Temporal\nConsistency Reinforcement, which uses Temporal Semantic Entropy (TSE), a\nmeasure of semantic stability across intermediate predictions, as a reward\nsignal to encourage stable generations. Empirical results across multiple\nbenchmarks demonstrate the effectiveness of our approach. Using the negative\nTSE reward alone, we observe a remarkable average improvement of 24.7% on the\nCountdown dataset over an existing dLLM. Combined with the accuracy reward, we\nachieve absolute gains of 2.0% on GSM8K, 4.3% on MATH500, 6.6% on SVAMP, and\n25.3% on Countdown, respectively. Our findings underscore the untapped\npotential of temporal dynamics in dLLMs and offer two simple yet effective\ntools to harness them.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63f089456309c84d5f47f951/znHPdJqj7uNrvTS4acpqM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09138.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f089456309c84d5f47f951",
            "avatarUrl": "/avatars/04b926a7f2ad091ee00fef0c59903492.svg",
            "fullname": "Wen Wang",
            "name": "wwen1997",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 24
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08088",
            "authors": [
                {
                    "_id": "689ac80efab6fdd2e52ac510",
                    "user": {
                        "_id": "62e52483a944e2a56cd2c6ca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
                        "isPro": false,
                        "fullname": "Jiejun Tan",
                        "user": "zstanjj",
                        "type": "user"
                    },
                    "name": "Jiejun Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:19:26.764Z",
                    "hidden": false
                },
                {
                    "_id": "689ac80efab6fdd2e52ac511",
                    "name": "Zhicheng Dou",
                    "hidden": false
                },
                {
                    "_id": "689ac80efab6fdd2e52ac512",
                    "name": "Yan Yu",
                    "hidden": false
                },
                {
                    "_id": "689ac80efab6fdd2e52ac513",
                    "name": "Jiehan Cheng",
                    "hidden": false
                },
                {
                    "_id": "689ac80efab6fdd2e52ac514",
                    "name": "Qiang Ju",
                    "hidden": false
                },
                {
                    "_id": "689ac80efab6fdd2e52ac515",
                    "name": "Jian Xie",
                    "hidden": false
                },
                {
                    "_id": "689ac80efab6fdd2e52ac516",
                    "name": "Ji-Rong Wen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T15:31:47.000Z",
            "submittedOnDailyAt": "2025-08-13T01:08:11.771Z",
            "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating\n  Local and Web Searches",
            "submittedOnDailyBy": {
                "_id": "62e52483a944e2a56cd2c6ca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
                "isPro": false,
                "fullname": "Jiejun Tan",
                "user": "zstanjj",
                "type": "user"
            },
            "summary": "Recently, large reasoning models have demonstrated strong mathematical and\ncoding abilities, and deep search leverages their reasoning capabilities in\nchallenging information retrieval tasks. Existing deep search works are\ngenerally limited to a single knowledge source, either local or the Web.\nHowever, enterprises often require private deep search systems that can\nleverage search tools over both local and the Web corpus. Simply training an\nagent equipped with multiple search tools using flat reinforcement learning\n(RL) is a straightforward idea, but it has problems such as low training data\nefficiency and poor mastery of complex tools. To address the above issue, we\npropose a hierarchical agentic deep search framework, HierSearch, trained with\nhierarchical RL. At the low level, a local deep search agent and a Web deep\nsearch agent are trained to retrieve evidence from their corresponding domains.\nAt the high level, a planner agent coordinates low-level agents and provides\nthe final answer. Moreover, to prevent direct answer copying and error\npropagation, we design a knowledge refiner that filters out hallucinations and\nirrelevant evidence returned by low-level agents. Experiments show that\nHierSearch achieves better performance compared to flat RL, and outperforms\nvarious deep search and multi-source retrieval-augmented generation baselines\nin six benchmarks across general, finance, and medical domains.",
            "upvotes": 24,
            "discussionId": "689ac80efab6fdd2e52ac517",
            "githubRepo": "https://github.com/plageon/HierSearch",
            "ai_summary": "HierSearch, a hierarchical agentic deep search framework using hierarchical RL, improves performance in multi-source retrieval tasks by coordinating local and Web search agents and refining knowledge.",
            "ai_keywords": [
                "reinforcement learning",
                "hierarchical RL",
                "local deep search agent",
                "Web deep search agent",
                "planner agent",
                "knowledge refiner",
                "hallucinations",
                "irrelevant evidence",
                "multi-source retrieval-augmented generation",
                "benchmarks"
            ],
            "githubStars": 23
        },
        "publishedAt": "2025-08-11T11:31:47.000Z",
        "title": "HierSearch: A Hierarchical Enterprise Deep Search Framework Integrating\n  Local and Web Searches",
        "summary": "Recently, large reasoning models have demonstrated strong mathematical and\ncoding abilities, and deep search leverages their reasoning capabilities in\nchallenging information retrieval tasks. Existing deep search works are\ngenerally limited to a single knowledge source, either local or the Web.\nHowever, enterprises often require private deep search systems that can\nleverage search tools over both local and the Web corpus. Simply training an\nagent equipped with multiple search tools using flat reinforcement learning\n(RL) is a straightforward idea, but it has problems such as low training data\nefficiency and poor mastery of complex tools. To address the above issue, we\npropose a hierarchical agentic deep search framework, HierSearch, trained with\nhierarchical RL. At the low level, a local deep search agent and a Web deep\nsearch agent are trained to retrieve evidence from their corresponding domains.\nAt the high level, a planner agent coordinates low-level agents and provides\nthe final answer. Moreover, to prevent direct answer copying and error\npropagation, we design a knowledge refiner that filters out hallucinations and\nirrelevant evidence returned by low-level agents. Experiments show that\nHierSearch achieves better performance compared to flat RL, and outperforms\nvarious deep search and multi-source retrieval-augmented generation baselines\nin six benchmarks across general, finance, and medical domains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08088.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62e52483a944e2a56cd2c6ca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62e52483a944e2a56cd2c6ca/pG44O-1qD00q5CEJMMyFQ.jpeg",
            "fullname": "Jiejun Tan",
            "name": "zstanjj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09125",
            "authors": [
                {
                    "_id": "689ca7d9b083e610d741e88b",
                    "name": "Mian Zhang",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e88c",
                    "name": "Shujian Liu",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e88d",
                    "name": "Sixun Dong",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e88e",
                    "name": "Ming Yin",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e88f",
                    "name": "Yebowen Hu",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e890",
                    "name": "Xun Wang",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e891",
                    "name": "Steven Ma",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e892",
                    "name": "Song Wang",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e893",
                    "name": "Sathish Reddy Indurthi",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e894",
                    "name": "Haoyun Deng",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e895",
                    "name": "Zhiyu Zoey Chen",
                    "hidden": false
                },
                {
                    "_id": "689ca7d9b083e610d741e896",
                    "name": "Kaiqiang Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T17:54:27.000Z",
            "submittedOnDailyAt": "2025-08-13T14:05:42.533Z",
            "title": "Complex Logical Instruction Generation",
            "submittedOnDailyBy": {
                "_id": "650857fef3060ea840ffbbfe",
                "avatarUrl": "/avatars/f9c87a75c9e5cf829b7d60102a3b7096.svg",
                "isPro": false,
                "fullname": "Mian Zhang",
                "user": "billmianz",
                "type": "user"
            },
            "summary": "Instruction following has catalyzed the recent era of Large Language Models\n(LLMs) and is the foundational skill underpinning more advanced capabilities\nsuch as reasoning and agentic behaviors. As tasks grow more challenging, the\nlogic structures embedded in natural language instructions becomes increasingly\nintricate. However, how well LLMs perform on such logic-rich instructions\nremains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a\nscalable, automated framework for generating verifiable instructions from code\nfunctions, which can naturally express rich logic such as conditionals,\nnesting, recursion, and function calls. We further curate a collection of\ncomplex code functions and use LogicIFGen to construct LogicIFEval, a benchmark\ncomprising 426 verifiable logic-rich instructions. Our experiments demonstrate\nthat current state-of-the-art LLMs still struggle to correctly follow the\ninstructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the\ninstructions, revealing significant deficiencies in the instruction-following\nability. Code and Benchmark: https://github.com/mianzhang/LogicIF",
            "upvotes": 20,
            "discussionId": "689ca7d9b083e610d741e897",
            "githubRepo": "https://github.com/mianzhang/LogicIF",
            "ai_summary": "LogicIFGen and LogicIFEval assess the instruction-following capabilities of LLMs on complex, logic-rich instructions, revealing significant performance gaps.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "LogicIFGen",
                "LogicIFEval",
                "verifiable instructions",
                "code functions",
                "conditionals",
                "nesting",
                "recursion",
                "function calls",
                "benchmark"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-08-12T13:54:27.000Z",
        "title": "Complex Logical Instruction Generation",
        "summary": "Instruction following has catalyzed the recent era of Large Language Models\n(LLMs) and is the foundational skill underpinning more advanced capabilities\nsuch as reasoning and agentic behaviors. As tasks grow more challenging, the\nlogic structures embedded in natural language instructions becomes increasingly\nintricate. However, how well LLMs perform on such logic-rich instructions\nremains under-explored. We propose LogicIFGen and LogicIFEval. LogicIFGen is a\nscalable, automated framework for generating verifiable instructions from code\nfunctions, which can naturally express rich logic such as conditionals,\nnesting, recursion, and function calls. We further curate a collection of\ncomplex code functions and use LogicIFGen to construct LogicIFEval, a benchmark\ncomprising 426 verifiable logic-rich instructions. Our experiments demonstrate\nthat current state-of-the-art LLMs still struggle to correctly follow the\ninstructions in LogicIFEval. Most LLMs can only follow fewer than 60% of the\ninstructions, revealing significant deficiencies in the instruction-following\nability. Code and Benchmark: https://github.com/mianzhang/LogicIF",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09125.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650857fef3060ea840ffbbfe",
            "avatarUrl": "/avatars/f9c87a75c9e5cf829b7d60102a3b7096.svg",
            "fullname": "Mian Zhang",
            "name": "billmianz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09062",
            "authors": [
                {
                    "_id": "689bf9adfab6fdd2e52ac82f",
                    "user": {
                        "_id": "679da97f2000f0eecaaa8e9c",
                        "avatarUrl": "/avatars/646128f0629143f05d6496cc93719184.svg",
                        "isPro": true,
                        "fullname": "Xiang Zhang",
                        "user": "zx1239856",
                        "type": "user"
                    },
                    "name": "Xiang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:54.982Z",
                    "hidden": false
                },
                {
                    "_id": "689bf9adfab6fdd2e52ac830",
                    "name": "Yawar Siddiqui",
                    "hidden": false
                },
                {
                    "_id": "689bf9adfab6fdd2e52ac831",
                    "name": "Armen Avetisyan",
                    "hidden": false
                },
                {
                    "_id": "689bf9adfab6fdd2e52ac832",
                    "name": "Chris Xie",
                    "hidden": false
                },
                {
                    "_id": "689bf9adfab6fdd2e52ac833",
                    "name": "Jakob Engel",
                    "hidden": false
                },
                {
                    "_id": "689bf9adfab6fdd2e52ac834",
                    "name": "Henry Howard-Jenkins",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65d4991111930466954340ae/6d5aOWmFbEs3X6Sg-T5iq.qt"
            ],
            "publishedAt": "2025-08-12T16:25:46.000Z",
            "submittedOnDailyAt": "2025-08-13T03:34:12.555Z",
            "title": "VertexRegen: Mesh Generation with Continuous Level of Detail",
            "submittedOnDailyBy": {
                "_id": "65d4991111930466954340ae",
                "avatarUrl": "/avatars/ffc07be80323aa4b7dadf8849b70ffde.svg",
                "isPro": false,
                "fullname": "Henry Howard-Jenkins",
                "user": "henryhj",
                "type": "user"
            },
            "summary": "We introduce VertexRegen, a novel mesh generation framework that enables\ngeneration at a continuous level of detail. Existing autoregressive methods\ngenerate meshes in a partial-to-complete manner and thus intermediate steps of\ngeneration represent incomplete structures. VertexRegen takes inspiration from\nprogressive meshes and reformulates the process as the reversal of edge\ncollapse, i.e. vertex split, learned through a generative model. Experimental\nresults demonstrate that VertexRegen produces meshes of comparable quality to\nstate-of-the-art methods while uniquely offering anytime generation with the\nflexibility to halt at any step to yield valid meshes with varying levels of\ndetail.",
            "upvotes": 17,
            "discussionId": "689bf9aefab6fdd2e52ac835",
            "projectPage": "https://vertexregen.github.io/",
            "ai_summary": "VertexRegen generates meshes with continuous detail by reversing edge collapse through a generative model, offering anytime generation and flexibility in detail levels.",
            "ai_keywords": [
                "mesh generation",
                "autoregressive methods",
                "progressive meshes",
                "edge collapse",
                "vertex split",
                "generative model",
                "anytime generation"
            ]
        },
        "publishedAt": "2025-08-12T12:25:46.000Z",
        "title": "VertexRegen: Mesh Generation with Continuous Level of Detail",
        "summary": "We introduce VertexRegen, a novel mesh generation framework that enables\ngeneration at a continuous level of detail. Existing autoregressive methods\ngenerate meshes in a partial-to-complete manner and thus intermediate steps of\ngeneration represent incomplete structures. VertexRegen takes inspiration from\nprogressive meshes and reformulates the process as the reversal of edge\ncollapse, i.e. vertex split, learned through a generative model. Experimental\nresults demonstrate that VertexRegen produces meshes of comparable quality to\nstate-of-the-art methods while uniquely offering anytime generation with the\nflexibility to halt at any step to yield valid meshes with varying levels of\ndetail.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65d4991111930466954340ae/6d5aOWmFbEs3X6Sg-T5iq.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09062.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d4991111930466954340ae",
            "avatarUrl": "/avatars/ffc07be80323aa4b7dadf8849b70ffde.svg",
            "fullname": "Henry Howard-Jenkins",
            "name": "henryhj",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.05615",
            "authors": [
                {
                    "_id": "6895643848b0ae5ca2710d5a",
                    "user": {
                        "_id": "67ef9bdeeaf849151c740adf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/-n7Ivd3RJnKZvFjF-z0ji.jpeg",
                        "isPro": false,
                        "fullname": "Yong Du",
                        "user": "DIONG1024",
                        "type": "user"
                    },
                    "name": "Yong Du",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:27:47.987Z",
                    "hidden": false
                },
                {
                    "_id": "6895643848b0ae5ca2710d5b",
                    "user": {
                        "_id": "64098738342c26884c792c93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                        "isPro": false,
                        "fullname": "Yuchen Yan",
                        "user": "yanyc",
                        "type": "user"
                    },
                    "name": "Yuchen Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:49:57.884Z",
                    "hidden": false
                },
                {
                    "_id": "6895643848b0ae5ca2710d5c",
                    "name": "Fei Tang",
                    "hidden": false
                },
                {
                    "_id": "6895643848b0ae5ca2710d5d",
                    "name": "Zhengxi Lu",
                    "hidden": false
                },
                {
                    "_id": "6895643848b0ae5ca2710d5e",
                    "name": "Chang Zong",
                    "hidden": false
                },
                {
                    "_id": "6895643848b0ae5ca2710d5f",
                    "name": "Weiming Lu",
                    "hidden": false
                },
                {
                    "_id": "6895643848b0ae5ca2710d60",
                    "name": "Shengpei Jiang",
                    "hidden": false
                },
                {
                    "_id": "6895643848b0ae5ca2710d61",
                    "user": {
                        "_id": "5e1058e9fcf41d740b69966d",
                        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                        "isPro": false,
                        "fullname": "Yongliang Shen",
                        "user": "tricktreat",
                        "type": "user"
                    },
                    "name": "Yongliang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:50:01.160Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-07T17:54:27.000Z",
            "submittedOnDailyAt": "2025-08-13T00:38:26.010Z",
            "title": "Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency",
            "submittedOnDailyBy": {
                "_id": "64098738342c26884c792c93",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                "isPro": false,
                "fullname": "Yuchen Yan",
                "user": "yanyc",
                "type": "user"
            },
            "summary": "Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents.",
            "upvotes": 17,
            "discussionId": "6895643948b0ae5ca2710d62",
            "projectPage": "https://zju-real.github.io/gui-rcpo/",
            "githubRepo": "https://github.com/zju-real/gui-rcpo",
            "ai_summary": "GUI-RC and GUI-RCPO enhance GUI grounding accuracy by leveraging spatial consistency and reinforcement learning without additional training data.",
            "ai_keywords": [
                "GUI grounding",
                "spatial overlap patterns",
                "spatial voting grids",
                "consensus regions",
                "test-time scaling",
                "test-time reinforcement learning",
                "ScreenSpot benchmarks",
                "Qwen2.5-VL-3B-Instruct",
                "ScreenSpot-v2"
            ],
            "githubStars": 19
        },
        "publishedAt": "2025-08-07T13:54:27.000Z",
        "title": "Test-Time Reinforcement Learning for GUI Grounding via Region\n  Consistency",
        "summary": "Graphical User Interface (GUI) grounding, the task of mapping natural\nlanguage instructions to precise screen coordinates, is fundamental to\nautonomous GUI agents. While existing methods achieve strong performance\nthrough extensive supervised training or reinforcement learning with labeled\nrewards, they remain constrained by the cost and availability of pixel-level\nannotations. We observe that when models generate multiple predictions for the\nsame GUI element, the spatial overlap patterns reveal implicit confidence\nsignals that can guide more accurate localization. Leveraging this insight, we\npropose GUI-RC (Region Consistency), a test-time scaling method that constructs\nspatial voting grids from multiple sampled predictions to identify consensus\nregions where models show highest agreement. Without any training, GUI-RC\nimproves accuracy by 2-3% across various architectures on ScreenSpot\nbenchmarks. We further introduce GUI-RCPO (Region Consistency Policy\nOptimization), which transforms these consistency patterns into rewards for\ntest-time reinforcement learning. By computing how well each prediction aligns\nwith the collective consensus, GUI-RCPO enables models to iteratively refine\ntheir outputs on unlabeled data during inference. Extensive experiments\ndemonstrate the generality of our approach: GUI-RC boosts\nQwen2.5-VL-3B-Instruct from 80.11% to 83.57% on ScreenSpot-v2, while GUI-RCPO\nfurther improves it to 85.14% through self-supervised optimization. Our\napproach reveals the untapped potential of test-time scaling and test-time\nreinforcement learning for GUI grounding, offering a promising path toward more\nrobust and data-efficient GUI agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05615.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "fullname": "Yuchen Yan",
            "name": "yanyc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.05399",
            "authors": [
                {
                    "_id": "689bea5afab6fdd2e52ac7d1",
                    "name": "Wonjun Kang",
                    "hidden": false
                },
                {
                    "_id": "689bea5afab6fdd2e52ac7d2",
                    "name": "Byeongkeun Ahn",
                    "hidden": false
                },
                {
                    "_id": "689bea5afab6fdd2e52ac7d3",
                    "name": "Minjae Lee",
                    "hidden": false
                },
                {
                    "_id": "689bea5afab6fdd2e52ac7d4",
                    "name": "Kevin Galim",
                    "hidden": false
                },
                {
                    "_id": "689bea5afab6fdd2e52ac7d5",
                    "user": {
                        "_id": "64ae35dc00781825350e880b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae35dc00781825350e880b/VuON41yUDzNDACbvWAVhz.jpeg",
                        "isPro": false,
                        "fullname": "Seunghyuk Oh",
                        "user": "JakeOh",
                        "type": "user"
                    },
                    "name": "Seunghyuk Oh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:13:14.929Z",
                    "hidden": false
                },
                {
                    "_id": "689bea5afab6fdd2e52ac7d6",
                    "name": "Hyung Il Koo",
                    "hidden": false
                },
                {
                    "_id": "689bea5afab6fdd2e52ac7d7",
                    "name": "Nam Ik Cho",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-07T13:51:17.000Z",
            "submittedOnDailyAt": "2025-08-13T00:40:20.574Z",
            "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative\n  Transformers in Text-to-Image Generation",
            "submittedOnDailyBy": {
                "_id": "63e1d5247fbb6ae4d4f4cc8e",
                "avatarUrl": "/avatars/8a8f700adf9e8000641c2c2f6bd56080.svg",
                "isPro": false,
                "fullname": "Wonjun Kang",
                "user": "wjkang",
                "type": "user"
            },
            "summary": "Text-to-image (T2I) generation has been actively studied using Diffusion\nModels and Autoregressive Models. Recently, Masked Generative Transformers have\ngained attention as an alternative to Autoregressive Models to overcome the\ninherent limitations of causal attention and autoregressive decoding through\nbidirectional attention and parallel decoding, enabling efficient and\nhigh-quality image generation. However, compositional T2I generation remains\nchallenging, as even state-of-the-art Diffusion Models often fail to accurately\nbind attributes and achieve proper text-image alignment. While Diffusion Models\nhave been extensively studied for this issue, Masked Generative Transformers\nexhibit similar limitations but have not been explored in this context. To\naddress this, we propose Unmasking with Contrastive Attention Guidance\n(UNCAGE), a novel training-free method that improves compositional fidelity by\nleveraging attention maps to prioritize the unmasking of tokens that clearly\nrepresent individual objects. UNCAGE consistently improves performance in both\nquantitative and qualitative evaluations across multiple benchmarks and\nmetrics, with negligible inference overhead. Our code is available at\nhttps://github.com/furiosa-ai/uncage.",
            "upvotes": 13,
            "discussionId": "689bea5bfab6fdd2e52ac7d8",
            "githubRepo": "https://github.com/furiosa-ai/uncage",
            "ai_summary": "UNCAGE, a training-free method using contrastive attention guidance, enhances compositional fidelity in text-to-image generation by prioritizing the unmasking of object-representing tokens.",
            "ai_keywords": [
                "Diffusion Models",
                "Autoregressive Models",
                "Masked Generative Transformers",
                "bidirectional attention",
                "parallel decoding",
                "compositional T2I generation",
                "attention maps",
                "text-image alignment",
                "UNCAGE"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-08-07T09:51:17.000Z",
        "title": "UNCAGE: Contrastive Attention Guidance for Masked Generative\n  Transformers in Text-to-Image Generation",
        "summary": "Text-to-image (T2I) generation has been actively studied using Diffusion\nModels and Autoregressive Models. Recently, Masked Generative Transformers have\ngained attention as an alternative to Autoregressive Models to overcome the\ninherent limitations of causal attention and autoregressive decoding through\nbidirectional attention and parallel decoding, enabling efficient and\nhigh-quality image generation. However, compositional T2I generation remains\nchallenging, as even state-of-the-art Diffusion Models often fail to accurately\nbind attributes and achieve proper text-image alignment. While Diffusion Models\nhave been extensively studied for this issue, Masked Generative Transformers\nexhibit similar limitations but have not been explored in this context. To\naddress this, we propose Unmasking with Contrastive Attention Guidance\n(UNCAGE), a novel training-free method that improves compositional fidelity by\nleveraging attention maps to prioritize the unmasking of tokens that clearly\nrepresent individual objects. UNCAGE consistently improves performance in both\nquantitative and qualitative evaluations across multiple benchmarks and\nmetrics, with negligible inference overhead. Our code is available at\nhttps://github.com/furiosa-ai/uncage.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05399.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63e1d5247fbb6ae4d4f4cc8e",
            "avatarUrl": "/avatars/8a8f700adf9e8000641c2c2f6bd56080.svg",
            "fullname": "Wonjun Kang",
            "name": "wjkang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08665",
            "authors": [
                {
                    "_id": "689c18cffab6fdd2e52ac89b",
                    "user": {
                        "_id": "683d78e0ed8d78bf6d3e653f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cVHZSl7JlvcJG13oZbpJ8.png",
                        "isPro": false,
                        "fullname": "Ritvik Rastogi",
                        "user": "RitvikPW",
                        "type": "user"
                    },
                    "name": "Ritvik Rastogi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:28.385Z",
                    "hidden": false
                },
                {
                    "_id": "689c18cffab6fdd2e52ac89c",
                    "name": "Sachin Dharashivkar",
                    "hidden": false
                },
                {
                    "_id": "689c18cffab6fdd2e52ac89d",
                    "name": "Sandeep Varma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T06:20:07.000Z",
            "submittedOnDailyAt": "2025-08-13T06:05:27.587Z",
            "title": "Aryabhata: An exam-focused language model for JEE Math",
            "submittedOnDailyBy": {
                "_id": "683d78e0ed8d78bf6d3e653f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cVHZSl7JlvcJG13oZbpJ8.png",
                "isPro": false,
                "fullname": "Ritvik Rastogi",
                "user": "RitvikPW",
                "type": "user"
            },
            "summary": "We present Aryabhata 1.0, a compact 7B parameter math reasoning\nmodel optimized for the Indian academic exam, the Joint Entrance Examination\n(JEE). Despite rapid progress in large language models (LLMs), current models\noften remain unsuitable for educational use. Aryabhata 1.0 is built by merging\nstrong open-weight reasoning models, followed by supervised fine-tuning (SFT)\nwith curriculum learning on verified chain-of-thought (CoT) traces curated\nthrough best-of-n rejection sampling. To further boost performance, we apply\nreinforcement learning with verifiable rewards (RLVR) using A2C objective with\ngroup-relative advantage estimation alongwith novel exploration strategies such\nas Adaptive Group Resizing and Temperature Scaling.\nEvaluated on both in-distribution (JEE Main 2025) and out-of-distribution\n(MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and\nefficiency, while offering pedagogically useful step-by-step reasoning. We\nrelease Aryabhata as a foundation model to advance exam-centric, open-source\nsmall language models. This marks our first open release for community feedback\n(https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0{Aryabhata 1.0\non Hugging Face}); PW is actively training future models to further improve\nlearning outcomes for students.",
            "upvotes": 12,
            "discussionId": "689c18d0fab6fdd2e52ac89e",
            "ai_summary": "Aryabhata 1.0, a compact math reasoning model, outperforms existing models on educational exams and benchmarks by using supervised fine-tuning, reinforcement learning with verifiable rewards, and novel exploration strategies.",
            "ai_keywords": [
                "open-weight reasoning models",
                "supervised fine-tuning",
                "curriculum learning",
                "chain-of-thought traces",
                "reinforcement learning with verifiable rewards",
                "A2C objective",
                "group-relative advantage estimation",
                "Adaptive Group Resizing",
                "Temperature Scaling",
                "in-distribution",
                "out-of-distribution",
                "pedagogically useful step-by-step reasoning"
            ]
        },
        "publishedAt": "2025-08-12T02:20:07.000Z",
        "title": "Aryabhata: An exam-focused language model for JEE Math",
        "summary": "We present Aryabhata 1.0, a compact 7B parameter math reasoning\nmodel optimized for the Indian academic exam, the Joint Entrance Examination\n(JEE). Despite rapid progress in large language models (LLMs), current models\noften remain unsuitable for educational use. Aryabhata 1.0 is built by merging\nstrong open-weight reasoning models, followed by supervised fine-tuning (SFT)\nwith curriculum learning on verified chain-of-thought (CoT) traces curated\nthrough best-of-n rejection sampling. To further boost performance, we apply\nreinforcement learning with verifiable rewards (RLVR) using A2C objective with\ngroup-relative advantage estimation alongwith novel exploration strategies such\nas Adaptive Group Resizing and Temperature Scaling.\nEvaluated on both in-distribution (JEE Main 2025) and out-of-distribution\n(MATH, GSM8K) benchmarks, Aryabhata outperforms existing models in accuracy and\nefficiency, while offering pedagogically useful step-by-step reasoning. We\nrelease Aryabhata as a foundation model to advance exam-centric, open-source\nsmall language models. This marks our first open release for community feedback\n(https://huggingface.co/PhysicsWallahAI/Aryabhata-1.0{Aryabhata 1.0\non Hugging Face}); PW is actively training future models to further improve\nlearning outcomes for students.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08665.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "683d78e0ed8d78bf6d3e653f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/cVHZSl7JlvcJG13oZbpJ8.png",
            "fullname": "Ritvik Rastogi",
            "name": "RitvikPW",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.08940",
            "authors": [
                {
                    "_id": "689c3f0cfab6fdd2e52ac93a",
                    "name": "Hasan Abed Al Kader Hammoud",
                    "hidden": false
                },
                {
                    "_id": "689c3f0cfab6fdd2e52ac93b",
                    "name": "Kumail Alhamoud",
                    "hidden": false
                },
                {
                    "_id": "689c3f0cfab6fdd2e52ac93c",
                    "name": "Abed Hammoud",
                    "hidden": false
                },
                {
                    "_id": "689c3f0cfab6fdd2e52ac93d",
                    "name": "Elie Bou-Zeid",
                    "hidden": false
                },
                {
                    "_id": "689c3f0cfab6fdd2e52ac93e",
                    "name": "Marzyeh Ghassemi",
                    "hidden": false
                },
                {
                    "_id": "689c3f0cfab6fdd2e52ac93f",
                    "name": "Bernard Ghanem",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/4b_TnX67UO9R8vBKO9NkT.png"
            ],
            "publishedAt": "2025-08-12T13:48:03.000Z",
            "submittedOnDailyAt": "2025-08-13T06:01:08.584Z",
            "title": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning",
            "submittedOnDailyBy": {
                "_id": "642b51385bf2355d02a23d15",
                "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
                "isPro": true,
                "fullname": "Hasan Abed Al Kader Hammoud",
                "user": "hammh0a",
                "type": "user"
            },
            "summary": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo.",
            "upvotes": 11,
            "discussionId": "689c3f0cfab6fdd2e52ac940",
            "ai_summary": "A curriculum learning strategy using Group Relative Policy Optimization (GRPO) enhances the reasoning abilities of large language models by progressively tightening token budgets, improving accuracy and token efficiency.",
            "ai_keywords": [
                "Group Relative Policy Optimization (GRPO)",
                "curriculum learning",
                "token budgets",
                "verifier feedback",
                "structural tags",
                "GSM8K",
                "MATH500",
                "SVAMP",
                "College Math",
                "GSM+"
            ]
        },
        "publishedAt": "2025-08-12T09:48:03.000Z",
        "title": "Train Long, Think Short: Curriculum Learning for Efficient Reasoning",
        "summary": "Recent work on enhancing the reasoning abilities of large language models\n(LLMs) has introduced explicit length control as a means of constraining\ncomputational cost while preserving accuracy. However, existing approaches rely\non fixed-length training budgets, which do not take advantage of the natural\nprogression from exploration to compression during learning. In this work, we\npropose a curriculum learning strategy for length-controlled reasoning using\nGroup Relative Policy Optimization (GRPO). Our method starts with generous\ntoken budgets and gradually tightens them over training, encouraging models to\nfirst discover effective solution strategies and then distill them into more\nconcise reasoning traces. We augment GRPO with a reward function that balances\nthree signals: task correctness (via verifier feedback), length efficiency, and\nformatting adherence (via structural tags). Experiments on GSM8K, MATH500,\nSVAMP, College Math, and GSM+ demonstrate that curriculum-based training\nconsistently outperforms fixed-budget baselines at the same final budget,\nachieving higher accuracy and significantly improved token efficiency. We\nfurther ablate the impact of reward weighting and decay schedule design,\nshowing that progressive constraint serves as a powerful inductive bias for\ntraining efficient reasoning models. Our code and checkpoints are released at:\nhttps://github.com/hammoudhasan/curriculum_grpo.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/642b51385bf2355d02a23d15/4b_TnX67UO9R8vBKO9NkT.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08940.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642b51385bf2355d02a23d15",
            "avatarUrl": "/avatars/87985347643b2647555f2453fa4d94fb.svg",
            "fullname": "Hasan Abed Al Kader Hammoud",
            "name": "hammh0a",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08896",
            "authors": [
                {
                    "_id": "689c50c1fab6fdd2e52ac96d",
                    "name": "Haoyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac96e",
                    "name": "Linghao Zhuang",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac96f",
                    "name": "Xingyue Zhao",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac970",
                    "name": "Cheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac971",
                    "name": "Haoran Xu",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac972",
                    "name": "Yuming Jiang",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac973",
                    "name": "Jun Cen",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac974",
                    "name": "Kexiang Wang",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac975",
                    "name": "Jiayan Guo",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac976",
                    "name": "Siteng Huang",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac977",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac978",
                    "name": "Deli Zhao",
                    "hidden": false
                },
                {
                    "_id": "689c50c1fab6fdd2e52ac979",
                    "name": "Hua Zou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T12:36:01.000Z",
            "submittedOnDailyAt": "2025-08-13T07:18:12.559Z",
            "title": "Towards Affordance-Aware Robotic Dexterous Grasping with Human-like\n  Priors",
            "submittedOnDailyBy": {
                "_id": "65fd82762bf2cd20ddaa193f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
                "isPro": false,
                "fullname": "Siteng Huang",
                "user": "huangsiteng",
                "type": "user"
            },
            "summary": "A dexterous hand capable of generalizable grasping objects is fundamental for\nthe development of general-purpose embodied AI. However, previous methods focus\nnarrowly on low-level grasp stability metrics, neglecting affordance-aware\npositioning and human-like poses which are crucial for downstream manipulation.\nTo address these limitations, we propose AffordDex, a novel framework with\ntwo-stage training that learns a universal grasping policy with an inherent\nunderstanding of both motion priors and object affordances. In the first stage,\na trajectory imitator is pre-trained on a large corpus of human hand motions to\ninstill a strong prior for natural movement. In the second stage, a residual\nmodule is trained to adapt these general human-like motions to specific object\ninstances. This refinement is critically guided by two components: our Negative\nAffordance-aware Segmentation (NAA) module, which identifies functionally\ninappropriate contact regions, and a privileged teacher-student distillation\nprocess that ensures the final vision-based policy is highly successful.\nExtensive experiments demonstrate that AffordDex not only achieves universal\ndexterous grasping but also remains remarkably human-like in posture and\nfunctionally appropriate in contact location. As a result, AffordDex\nsignificantly outperforms state-of-the-art baselines across seen objects,\nunseen instances, and even entirely novel categories.",
            "upvotes": 9,
            "discussionId": "689c50c1fab6fdd2e52ac97a",
            "ai_summary": "AffordDex is a two-stage framework that learns a universal grasping policy with motion priors and object affordances, outperforming state-of-the-art methods in dexterous grasping and human-like postures.",
            "ai_keywords": [
                "trajectory imitator",
                "residual module",
                "Negative Affordance-aware Segmentation (NAA)",
                "privileged teacher-student distillation",
                "vision-based policy",
                "universal grasping",
                "human-like postures",
                "object affordances",
                "motion priors"
            ]
        },
        "publishedAt": "2025-08-12T08:36:01.000Z",
        "title": "Towards Affordance-Aware Robotic Dexterous Grasping with Human-like\n  Priors",
        "summary": "A dexterous hand capable of generalizable grasping objects is fundamental for\nthe development of general-purpose embodied AI. However, previous methods focus\nnarrowly on low-level grasp stability metrics, neglecting affordance-aware\npositioning and human-like poses which are crucial for downstream manipulation.\nTo address these limitations, we propose AffordDex, a novel framework with\ntwo-stage training that learns a universal grasping policy with an inherent\nunderstanding of both motion priors and object affordances. In the first stage,\na trajectory imitator is pre-trained on a large corpus of human hand motions to\ninstill a strong prior for natural movement. In the second stage, a residual\nmodule is trained to adapt these general human-like motions to specific object\ninstances. This refinement is critically guided by two components: our Negative\nAffordance-aware Segmentation (NAA) module, which identifies functionally\ninappropriate contact regions, and a privileged teacher-student distillation\nprocess that ensures the final vision-based policy is highly successful.\nExtensive experiments demonstrate that AffordDex not only achieves universal\ndexterous grasping but also remains remarkably human-like in posture and\nfunctionally appropriate in contact location. As a result, AffordDex\nsignificantly outperforms state-of-the-art baselines across seen objects,\nunseen instances, and even entirely novel categories.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08896.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65fd82762bf2cd20ddaa193f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/yBYbWp_mT7UusYdkqtAvw.png",
            "fullname": "Siteng Huang",
            "name": "huangsiteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08244",
            "authors": [
                {
                    "_id": "689bfef7fab6fdd2e52ac837",
                    "name": "Jingwen He",
                    "hidden": false
                },
                {
                    "_id": "689bfef7fab6fdd2e52ac838",
                    "name": "Hongbo Liu",
                    "hidden": false
                },
                {
                    "_id": "689bfef7fab6fdd2e52ac839",
                    "name": "Jiajun Li",
                    "hidden": false
                },
                {
                    "_id": "689bfef7fab6fdd2e52ac83a",
                    "name": "Ziqi Huang",
                    "hidden": false
                },
                {
                    "_id": "689bfef7fab6fdd2e52ac83b",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "689bfef7fab6fdd2e52ac83c",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "689bfef7fab6fdd2e52ac83d",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T17:56:59.000Z",
            "submittedOnDailyAt": "2025-08-13T03:01:55.702Z",
            "title": "Cut2Next: Generating Next Shot via In-Context Tuning",
            "submittedOnDailyBy": {
                "_id": "66a49f9b2b460286b05646b8",
                "avatarUrl": "/avatars/5123cf7c3d00b5bc47a86011afd6abe8.svg",
                "isPro": false,
                "fullname": "jwhe",
                "user": "jwhejwhe",
                "type": "user"
            },
            "summary": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots.",
            "upvotes": 9,
            "discussionId": "689bfef7fab6fdd2e52ac83e",
            "ai_summary": "Cut2Next, a framework using a Diffusion Transformer with in-context tuning and hierarchical prompting, generates high-quality, cinematically coherent shots that adhere to professional editing patterns.",
            "ai_keywords": [
                "Diffusion Transformer",
                "DiT",
                "in-context tuning",
                "Hierarchical Multi-Prompting",
                "Relational Prompts",
                "Individual Prompts",
                "Context-Aware Condition Injection",
                "CACI",
                "Hierarchical Attention Mask",
                "HAM",
                "RawCuts",
                "CuratedCuts",
                "CutBench"
            ]
        },
        "publishedAt": "2025-08-11T13:56:59.000Z",
        "title": "Cut2Next: Generating Next Shot via In-Context Tuning",
        "summary": "Effective multi-shot generation demands purposeful, film-like transitions and\nstrict cinematic continuity. Current methods, however, often prioritize basic\nvisual consistency, neglecting crucial editing patterns (e.g., shot/reverse\nshot, cutaways) that drive narrative flow for compelling storytelling. This\nyields outputs that may be visually coherent but lack narrative sophistication\nand true cinematic integrity. To bridge this, we introduce Next Shot Generation\n(NSG): synthesizing a subsequent, high-quality shot that critically conforms to\nprofessional editing patterns while upholding rigorous cinematic continuity.\nOur framework, Cut2Next, leverages a Diffusion Transformer (DiT). It employs\nin-context tuning guided by a novel Hierarchical Multi-Prompting strategy. This\nstrategy uses Relational Prompts to define overall context and inter-shot\nediting styles. Individual Prompts then specify per-shot content and\ncinematographic attributes. Together, these guide Cut2Next to generate\ncinematically appropriate next shots. Architectural innovations, Context-Aware\nCondition Injection (CACI) and Hierarchical Attention Mask (HAM), further\nintegrate these diverse signals without introducing new parameters. We\nconstruct RawCuts (large-scale) and CuratedCuts (refined) datasets, both with\nhierarchical prompts, and introduce CutBench for evaluation. Experiments show\nCut2Next excels in visual consistency and text fidelity. Crucially, user\nstudies reveal a strong preference for Cut2Next, particularly for its adherence\nto intended editing patterns and overall cinematic continuity, validating its\nability to generate high-quality, narratively expressive, and cinematically\ncoherent subsequent shots.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08244.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66a49f9b2b460286b05646b8",
            "avatarUrl": "/avatars/5123cf7c3d00b5bc47a86011afd6abe8.svg",
            "fullname": "jwhe",
            "name": "jwhejwhe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.07485",
            "authors": [
                {
                    "_id": "689b421bfab6fdd2e52ac6d8",
                    "user": {
                        "_id": "689b418f0b10f771fb5c780a",
                        "avatarUrl": "/avatars/553a9d6866b7a3d94d9cb09e764ec32b.svg",
                        "isPro": false,
                        "fullname": "Alex Duffy",
                        "user": "Alex-GSL",
                        "type": "user"
                    },
                    "name": "Alexander Duffy",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:14:22.861Z",
                    "hidden": false
                },
                {
                    "_id": "689b421bfab6fdd2e52ac6d9",
                    "name": "Samuel J Paech",
                    "hidden": false
                },
                {
                    "_id": "689b421bfab6fdd2e52ac6da",
                    "name": "Ishana Shastri",
                    "hidden": false
                },
                {
                    "_id": "689b421bfab6fdd2e52ac6db",
                    "name": "Elizabeth Karpinski",
                    "hidden": false
                },
                {
                    "_id": "689b421bfab6fdd2e52ac6dc",
                    "name": "Baptiste Alloui-Cros",
                    "hidden": false
                },
                {
                    "_id": "689b421bfab6fdd2e52ac6dd",
                    "user": {
                        "_id": "646535720e6c7618f6138daa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646535720e6c7618f6138daa/w1mFcDWodCtfwa6wB_EJZ.jpeg",
                        "isPro": false,
                        "fullname": "Tyler Marques",
                        "user": "tmarques",
                        "type": "user"
                    },
                    "name": "Tyler Marques",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:14:19.902Z",
                    "hidden": false
                },
                {
                    "_id": "689b421bfab6fdd2e52ac6de",
                    "name": "Matthew Lyle Olson",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/689b418f0b10f771fb5c780a/wxR0TTbznEYPgj6elnIVt.png"
            ],
            "publishedAt": "2025-08-10T21:07:08.000Z",
            "submittedOnDailyAt": "2025-08-13T10:50:54.493Z",
            "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language\n  Model on Full-Press Diplomacy",
            "submittedOnDailyBy": {
                "_id": "689b418f0b10f771fb5c780a",
                "avatarUrl": "/avatars/553a9d6866b7a3d94d9cb09e764ec32b.svg",
                "isPro": false,
                "fullname": "Alex Duffy",
                "user": "Alex-GSL",
                "type": "user"
            },
            "summary": "We present the first evaluation harness that enables any out-of-the-box,\nlocal, Large Language Models (LLMs) to play full-press Diplomacy without\nfine-tuning or specialized training. Previous work required frontier LLMs, or\nfine-tuning, due to the high complexity and information density of Diplomacy's\ngame state. Combined with the high variance of matches, these factors made\nDiplomacy prohibitive for study. In this work, we used data-driven iteration to\noptimize a textual game state representation such that a 24B model can reliably\ncomplete matches without any fine tuning. We develop tooling to facilitate\nhypothesis testing and statistical analysis, and we present case studies on\npersuasion, aggressive playstyles, and performance across a range of models. We\nconduct a variety of experiments across many popular LLMs, finding the larger\nmodels perform the best, but the smaller models still play adequately. We also\nintroduce Critical State Analysis: an experimental protocol for rapidly\niterating and analyzing key moments in a game at depth. Our harness\ndemocratizes the evaluation of strategic reasoning in LLMs by eliminating the\nneed for fine-tuning, and it provides insights into how these capabilities\nemerge naturally from widely used LLMs. Our code is available in the supplement\nand will be open sourced.",
            "upvotes": 9,
            "discussionId": "689b421bfab6fdd2e52ac6df",
            "ai_summary": "An evaluation harness allows large language models to play Diplomacy without fine-tuning, providing insights into their strategic reasoning capabilities.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "Diplomacy",
                "textual game state representation",
                "hypothesis testing",
                "statistical analysis",
                "Critical State Analysis",
                "strategic reasoning"
            ]
        },
        "publishedAt": "2025-08-10T17:07:08.000Z",
        "title": "Democratizing Diplomacy: A Harness for Evaluating Any Large Language\n  Model on Full-Press Diplomacy",
        "summary": "We present the first evaluation harness that enables any out-of-the-box,\nlocal, Large Language Models (LLMs) to play full-press Diplomacy without\nfine-tuning or specialized training. Previous work required frontier LLMs, or\nfine-tuning, due to the high complexity and information density of Diplomacy's\ngame state. Combined with the high variance of matches, these factors made\nDiplomacy prohibitive for study. In this work, we used data-driven iteration to\noptimize a textual game state representation such that a 24B model can reliably\ncomplete matches without any fine tuning. We develop tooling to facilitate\nhypothesis testing and statistical analysis, and we present case studies on\npersuasion, aggressive playstyles, and performance across a range of models. We\nconduct a variety of experiments across many popular LLMs, finding the larger\nmodels perform the best, but the smaller models still play adequately. We also\nintroduce Critical State Analysis: an experimental protocol for rapidly\niterating and analyzing key moments in a game at depth. Our harness\ndemocratizes the evaluation of strategic reasoning in LLMs by eliminating the\nneed for fine-tuning, and it provides insights into how these capabilities\nemerge naturally from widely used LLMs. Our code is available in the supplement\nand will be open sourced.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/689b418f0b10f771fb5c780a/wxR0TTbznEYPgj6elnIVt.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07485.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "689b418f0b10f771fb5c780a",
            "avatarUrl": "/avatars/553a9d6866b7a3d94d9cb09e764ec32b.svg",
            "fullname": "Alex Duffy",
            "name": "Alex-GSL",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.06964",
            "authors": [
                {
                    "_id": "689b413cfab6fdd2e52ac6d0",
                    "user": {
                        "_id": "689b16601133f93a6f4afdca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/MWlw4vtHV6ptOpTT6-U57.png",
                        "isPro": false,
                        "fullname": "Qiwei Tian",
                        "user": "michaeltqw108",
                        "type": "user"
                    },
                    "name": "Qiwei Tian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:14:26.314Z",
                    "hidden": false
                },
                {
                    "_id": "689b413cfab6fdd2e52ac6d1",
                    "name": "Chenhao Lin",
                    "hidden": false
                },
                {
                    "_id": "689b413cfab6fdd2e52ac6d2",
                    "name": "Zhengyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "689b413cfab6fdd2e52ac6d3",
                    "name": "Qian Li",
                    "hidden": false
                },
                {
                    "_id": "689b413cfab6fdd2e52ac6d4",
                    "name": "Shuai Liu",
                    "hidden": false
                },
                {
                    "_id": "689b413cfab6fdd2e52ac6d5",
                    "name": "Chao Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-09T12:20:13.000Z",
            "submittedOnDailyAt": "2025-08-13T06:28:33.122Z",
            "title": "Adversarial Video Promotion Against Text-to-Video Retrieval",
            "submittedOnDailyBy": {
                "_id": "689b16601133f93a6f4afdca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/MWlw4vtHV6ptOpTT6-U57.png",
                "isPro": false,
                "fullname": "Qiwei Tian",
                "user": "michaeltqw108",
                "type": "user"
            },
            "summary": "Thanks to the development of cross-modal models, text-to-video retrieval\n(T2VR) is advancing rapidly, but its robustness remains largely unexamined.\nExisting attacks against T2VR are designed to push videos away from queries,\ni.e., suppressing the ranks of videos, while the attacks that pull videos\ntowards selected queries, i.e., promoting the ranks of videos, remain largely\nunexplored. These attacks can be more impactful as attackers may gain more\nviews/clicks for financial benefits and widespread (mis)information. To this\nend, we pioneer the first attack against T2VR to promote videos adversarially,\ndubbed the Video Promotion attack (ViPro). We further propose Modal Refinement\n(MoRe) to capture the finer-grained, intricate interaction between visual and\ntextual modalities to enhance black-box transferability. Comprehensive\nexperiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing\ndatasets with over 10k videos, evaluated under 3 scenarios. All experiments are\nconducted in a multi-target setting to reflect realistic scenarios where\nattackers seek to promote the video regarding multiple queries simultaneously.\nWe also evaluated our attacks for defences and imperceptibility. Overall, ViPro\nsurpasses other baselines by over 30/10/4% for white/grey/black-box settings\non average. Our work highlights an overlooked vulnerability, provides a\nqualitative analysis on the upper/lower bound of our attacks, and offers\ninsights into potential counterplays. Code will be publicly available at\nhttps://github.com/michaeltian108/ViPro.",
            "upvotes": 8,
            "discussionId": "689b413dfab6fdd2e52ac6d6",
            "githubRepo": "https://github.com/michaeltian108/ViPro",
            "ai_summary": "The Video Promotion attack (ViPro) enhances the robustness of text-to-video retrieval (T2VR) by promoting videos towards selected queries, demonstrating significant improvements over existing baselines in various attack scenarios.",
            "ai_keywords": [
                "cross-modal models",
                "text-to-video retrieval",
                "T2VR",
                "Video Promotion attack",
                "ViPro",
                "Modal Refinement",
                "MoRe",
                "black-box transferability",
                "multi-target setting",
                "white-box",
                "grey-box",
                "black-box",
                "defences",
                "imperceptibility"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-08-09T08:20:13.000Z",
        "title": "Adversarial Video Promotion Against Text-to-Video Retrieval",
        "summary": "Thanks to the development of cross-modal models, text-to-video retrieval\n(T2VR) is advancing rapidly, but its robustness remains largely unexamined.\nExisting attacks against T2VR are designed to push videos away from queries,\ni.e., suppressing the ranks of videos, while the attacks that pull videos\ntowards selected queries, i.e., promoting the ranks of videos, remain largely\nunexplored. These attacks can be more impactful as attackers may gain more\nviews/clicks for financial benefits and widespread (mis)information. To this\nend, we pioneer the first attack against T2VR to promote videos adversarially,\ndubbed the Video Promotion attack (ViPro). We further propose Modal Refinement\n(MoRe) to capture the finer-grained, intricate interaction between visual and\ntextual modalities to enhance black-box transferability. Comprehensive\nexperiments cover 2 existing baselines, 3 leading T2VR models, 3 prevailing\ndatasets with over 10k videos, evaluated under 3 scenarios. All experiments are\nconducted in a multi-target setting to reflect realistic scenarios where\nattackers seek to promote the video regarding multiple queries simultaneously.\nWe also evaluated our attacks for defences and imperceptibility. Overall, ViPro\nsurpasses other baselines by over 30/10/4% for white/grey/black-box settings\non average. Our work highlights an overlooked vulnerability, provides a\nqualitative analysis on the upper/lower bound of our attacks, and offers\ninsights into potential counterplays. Code will be publicly available at\nhttps://github.com/michaeltian108/ViPro.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06964.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "689b16601133f93a6f4afdca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/MWlw4vtHV6ptOpTT6-U57.png",
            "fullname": "Qiwei Tian",
            "name": "michaeltqw108",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.03936",
            "authors": [
                {
                    "_id": "6893fdd6741a16f544fbce13",
                    "user": {
                        "_id": "64416825bd0c972652986a9e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64416825bd0c972652986a9e/1JgP2oCeEwyj_EC_D2gBn.png",
                        "isPro": true,
                        "fullname": "Alex Xu",
                        "user": "Alex-xu",
                        "type": "user"
                    },
                    "name": "Xiangzhe Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-07T10:41:51.786Z",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce14",
                    "name": "Guangyu Shen",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce15",
                    "name": "Zian Su",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce16",
                    "name": "Siyuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce17",
                    "name": "Hanxi Guo",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce18",
                    "name": "Lu Yan",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce19",
                    "name": "Xuan Chen",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce1a",
                    "name": "Jiasheng Jiang",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce1b",
                    "name": "Xiaolong Jin",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce1c",
                    "name": "Chengpeng Wang",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce1d",
                    "name": "Zhuo Zhang",
                    "hidden": false
                },
                {
                    "_id": "6893fdd6741a16f544fbce1e",
                    "name": "Xiangyu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T21:57:52.000Z",
            "submittedOnDailyAt": "2025-08-13T19:19:54.194Z",
            "title": "ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software\n  Assistants",
            "submittedOnDailyBy": {
                "_id": "64416825bd0c972652986a9e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64416825bd0c972652986a9e/1JgP2oCeEwyj_EC_D2gBn.png",
                "isPro": true,
                "fullname": "Alex Xu",
                "user": "Alex-xu",
                "type": "user"
            },
            "summary": "AI coding assistants like GitHub Copilot are rapidly transforming software\ndevelopment, but their safety remains deeply uncertain-especially in\nhigh-stakes domains like cybersecurity. Current red-teaming tools often rely on\nfixed benchmarks or unrealistic prompts, missing many real-world\nvulnerabilities. We present ASTRA, an automated agent system designed to\nsystematically uncover safety flaws in AI-driven code generation and security\nguidance systems. ASTRA works in three stages: (1) it builds structured\ndomain-specific knowledge graphs that model complex software tasks and known\nweaknesses; (2) it performs online vulnerability exploration of each target\nmodel by adaptively probing both its input space, i.e., the spatial\nexploration, and its reasoning processes, i.e., the temporal exploration,\nguided by the knowledge graphs; and (3) it generates high-quality\nviolation-inducing cases to improve model alignment. Unlike prior methods,\nASTRA focuses on realistic inputs-requests that developers might actually\nask-and uses both offline abstraction guided domain modeling and online domain\nknowledge graph adaptation to surface corner-case vulnerabilities. Across two\nmajor evaluation domains, ASTRA finds 11-66% more issues than existing\ntechniques and produces test cases that lead to 17% more effective alignment\ntraining, showing its practical value for building safer AI systems.",
            "upvotes": 8,
            "discussionId": "6893fdd6741a16f544fbce1f",
            "projectPage": "https://purcl.github.io/astra-web/",
            "githubRepo": "https://github.com/PurCL/ASTRA",
            "ai_summary": "ASTRA, an automated agent system, uncovers safety flaws in AI-driven code generation and security guidance by building knowledge graphs, exploring vulnerabilities, and generating violation-inducing cases, outperforming existing methods in real-world scenarios.",
            "ai_keywords": [
                "knowledge graphs",
                "vulnerability exploration",
                "input space",
                "reasoning processes",
                "violation-inducing cases",
                "domain modeling",
                "domain knowledge graph adaptation"
            ],
            "githubStars": 27
        },
        "publishedAt": "2025-08-05T17:57:52.000Z",
        "title": "ASTRA: Autonomous Spatial-Temporal Red-teaming for AI Software\n  Assistants",
        "summary": "AI coding assistants like GitHub Copilot are rapidly transforming software\ndevelopment, but their safety remains deeply uncertain-especially in\nhigh-stakes domains like cybersecurity. Current red-teaming tools often rely on\nfixed benchmarks or unrealistic prompts, missing many real-world\nvulnerabilities. We present ASTRA, an automated agent system designed to\nsystematically uncover safety flaws in AI-driven code generation and security\nguidance systems. ASTRA works in three stages: (1) it builds structured\ndomain-specific knowledge graphs that model complex software tasks and known\nweaknesses; (2) it performs online vulnerability exploration of each target\nmodel by adaptively probing both its input space, i.e., the spatial\nexploration, and its reasoning processes, i.e., the temporal exploration,\nguided by the knowledge graphs; and (3) it generates high-quality\nviolation-inducing cases to improve model alignment. Unlike prior methods,\nASTRA focuses on realistic inputs-requests that developers might actually\nask-and uses both offline abstraction guided domain modeling and online domain\nknowledge graph adaptation to surface corner-case vulnerabilities. Across two\nmajor evaluation domains, ASTRA finds 11-66% more issues than existing\ntechniques and produces test cases that lead to 17% more effective alignment\ntraining, showing its practical value for building safer AI systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.03936.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64416825bd0c972652986a9e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64416825bd0c972652986a9e/1JgP2oCeEwyj_EC_D2gBn.png",
            "fullname": "Alex Xu",
            "name": "Alex-xu",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.08938",
            "authors": [
                {
                    "_id": "689c5447fab6fdd2e52ac97d",
                    "name": "Alexander Polok",
                    "hidden": false
                },
                {
                    "_id": "689c5447fab6fdd2e52ac97e",
                    "name": "Santosh Kesiraju",
                    "hidden": false
                },
                {
                    "_id": "689c5447fab6fdd2e52ac97f",
                    "name": "Karel Bene≈°",
                    "hidden": false
                },
                {
                    "_id": "689c5447fab6fdd2e52ac980",
                    "name": "Bolaji Yusuf",
                    "hidden": false
                },
                {
                    "_id": "689c5447fab6fdd2e52ac981",
                    "name": "Luk√°≈° Burget",
                    "hidden": false
                },
                {
                    "_id": "689c5447fab6fdd2e52ac982",
                    "name": "Jan ƒåernock√Ω",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T13:44:50.000Z",
            "submittedOnDailyAt": "2025-08-13T07:32:27.390Z",
            "title": "DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech\n  Recognition",
            "submittedOnDailyBy": {
                "_id": "638d2b765e14c2f38677987b",
                "avatarUrl": "/avatars/622c183e897a99e33717f4a92305fbd3.svg",
                "isPro": false,
                "fullname": "Alexander Polok",
                "user": "Lakoc",
                "type": "user"
            },
            "summary": "This paper presents a simple yet effective regularization for the internal\nlanguage model induced by the decoder in encoder-decoder ASR models, thereby\nimproving robustness and generalization in both in- and out-of-domain settings.\nThe proposed method, Decoder-Centric Regularization in Encoder-Decoder\n(DeCRED), adds auxiliary classifiers to the decoder, enabling next token\nprediction via intermediate logits. Empirically, DeCRED reduces the mean\ninternal LM BPE perplexity by 36.6% relative to 11 test sets. Furthermore, this\ntranslates into actual WER improvements over the baseline in 5 of 7 in-domain\nand 3 of 4 out-of-domain test sets, reducing macro WER from 6.4% to 6.3% and\n18.2% to 16.2%, respectively. On TEDLIUM3, DeCRED achieves 7.0% WER, surpassing\nthe baseline and encoder-centric InterCTC regularization by 0.6% and 0.5%,\nrespectively. Finally, we compare DeCRED with OWSM v3.1 and Whisper-medium,\nshowing competitive WERs despite training on much less data with fewer\nparameters.",
            "upvotes": 7,
            "discussionId": "689c5447fab6fdd2e52ac985",
            "ai_summary": "Decoder-Centric Regularization in Encoder-Decoder (DeCRED) improves ASR robustness and generalization by adding auxiliary classifiers to the decoder, reducing internal language model perplexity and WER.",
            "ai_keywords": [
                "encoder-decoder ASR models",
                "Decoder-Centric Regularization in Encoder-Decoder",
                "DeCRED",
                "auxiliary classifiers",
                "next token prediction",
                "internal LM BPE perplexity",
                "WER",
                "in-domain",
                "out-of-domain",
                "TEDLIUM3",
                "InterCTC regularization",
                "OWSM v3.1",
                "Whisper-medium"
            ]
        },
        "publishedAt": "2025-08-12T09:44:50.000Z",
        "title": "DeCRED: Decoder-Centric Regularization for Encoder-Decoder Based Speech\n  Recognition",
        "summary": "This paper presents a simple yet effective regularization for the internal\nlanguage model induced by the decoder in encoder-decoder ASR models, thereby\nimproving robustness and generalization in both in- and out-of-domain settings.\nThe proposed method, Decoder-Centric Regularization in Encoder-Decoder\n(DeCRED), adds auxiliary classifiers to the decoder, enabling next token\nprediction via intermediate logits. Empirically, DeCRED reduces the mean\ninternal LM BPE perplexity by 36.6% relative to 11 test sets. Furthermore, this\ntranslates into actual WER improvements over the baseline in 5 of 7 in-domain\nand 3 of 4 out-of-domain test sets, reducing macro WER from 6.4% to 6.3% and\n18.2% to 16.2%, respectively. On TEDLIUM3, DeCRED achieves 7.0% WER, surpassing\nthe baseline and encoder-centric InterCTC regularization by 0.6% and 0.5%,\nrespectively. Finally, we compare DeCRED with OWSM v3.1 and Whisper-medium,\nshowing competitive WERs despite training on much less data with fewer\nparameters.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08938.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638d2b765e14c2f38677987b",
            "avatarUrl": "/avatars/622c183e897a99e33717f4a92305fbd3.svg",
            "fullname": "Alexander Polok",
            "name": "Lakoc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08791",
            "authors": [
                {
                    "_id": "689c08b8fab6fdd2e52ac871",
                    "user": {
                        "_id": "66384be673c2c55f2ded89fa",
                        "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
                        "isPro": false,
                        "fullname": "Junjie Ye",
                        "user": "Junjie-Ye",
                        "type": "user"
                    },
                    "name": "Junjie Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:12:31.844Z",
                    "hidden": false
                },
                {
                    "_id": "689c08b8fab6fdd2e52ac872",
                    "name": "Changhao Jiang",
                    "hidden": false
                },
                {
                    "_id": "689c08b8fab6fdd2e52ac873",
                    "name": "Zhengyin Du",
                    "hidden": false
                },
                {
                    "_id": "689c08b8fab6fdd2e52ac874",
                    "name": "Yufei Xu",
                    "hidden": false
                },
                {
                    "_id": "689c08b8fab6fdd2e52ac875",
                    "name": "Xuesong Yao",
                    "hidden": false
                },
                {
                    "_id": "689c08b8fab6fdd2e52ac876",
                    "name": "Zhiheng Xi",
                    "hidden": false
                },
                {
                    "_id": "689c08b8fab6fdd2e52ac877",
                    "name": "Xiaoran Fan",
                    "hidden": false
                },
                {
                    "_id": "689c08b8fab6fdd2e52ac878",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "689c08b8fab6fdd2e52ac879",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "689c08b8fab6fdd2e52ac87a",
                    "name": "Jiecao Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T09:45:19.000Z",
            "submittedOnDailyAt": "2025-08-13T02:35:32.989Z",
            "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments",
            "submittedOnDailyBy": {
                "_id": "66384be673c2c55f2ded89fa",
                "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
                "isPro": false,
                "fullname": "Junjie Ye",
                "user": "Junjie-Ye",
                "type": "user"
            },
            "summary": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models.",
            "upvotes": 7,
            "discussionId": "689c08b8fab6fdd2e52ac87b",
            "githubRepo": "https://github.com/bytedance/FTRL",
            "ai_summary": "An automated pipeline for constructing training environments and a verifiable reward mechanism enhance large language models' tool-use performance without compromising general capabilities.",
            "ai_keywords": [
                "reinforcement learning",
                "scenario decomposition",
                "document generation",
                "function integration",
                "complexity scaling",
                "localized deployment",
                "verifiable reward mechanism",
                "trajectory data",
                "standard RL algorithms",
                "context understanding",
                "reasoning",
                "MLP parameters"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-08-12T05:45:19.000Z",
        "title": "Feedback-Driven Tool-Use Improvements in Large Language Models via\n  Automated Build Environments",
        "summary": "Effective tool use is essential for large language models (LLMs) to interact\nmeaningfully with their environment. However, progress is limited by the lack\nof efficient reinforcement learning (RL) frameworks specifically designed for\ntool use, due to challenges in constructing stable training environments and\ndesigning verifiable reward mechanisms. To address this, we propose an\nautomated environment construction pipeline, incorporating scenario\ndecomposition, document generation, function integration, complexity scaling,\nand localized deployment. This enables the creation of high-quality training\nenvironments that provide detailed and measurable feedback without relying on\nexternal tools. Additionally, we introduce a verifiable reward mechanism that\nevaluates both the precision of tool use and the completeness of task\nexecution. When combined with trajectory data collected from the constructed\nenvironments, this mechanism integrates seamlessly with standard RL algorithms\nto facilitate feedback-driven model training. Experiments on LLMs of varying\nscales demonstrate that our approach significantly enhances the models'\ntool-use performance without degrading their general capabilities, regardless\nof inference modes or training algorithms. Our analysis suggests that these\ngains result from improved context understanding and reasoning, driven by\nupdates to the lower-layer MLP parameters in models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08791.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66384be673c2c55f2ded89fa",
            "avatarUrl": "/avatars/1d8721074f0f51fab405f81474f2035f.svg",
            "fullname": "Junjie Ye",
            "name": "Junjie-Ye",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09123",
            "authors": [
                {
                    "_id": "689bed2efab6fdd2e52ac7da",
                    "user": {
                        "_id": "67b327cdd4665a0448eef7d5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67b327cdd4665a0448eef7d5/_B5Z9MCa_qiFrDj1axKlz.png",
                        "isPro": true,
                        "fullname": "Xinyuan Wang",
                        "user": "xywang626",
                        "type": "user"
                    },
                    "name": "Xinyuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:13:11.475Z",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7db",
                    "name": "Bowen Wang",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7dc",
                    "name": "Dunjie Lu",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7dd",
                    "name": "Junlin Yang",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7de",
                    "name": "Tianbao Xie",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7df",
                    "name": "Junli Wang",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e0",
                    "name": "Jiaqi Deng",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e1",
                    "name": "Xiaole Guo",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e2",
                    "name": "Yiheng Xu",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e3",
                    "name": "Chen Henry Wu",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e4",
                    "name": "Zhennan Shen",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e5",
                    "name": "Zhuokai Li",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e6",
                    "name": "Ryan Li",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e7",
                    "name": "Xiaochuan Li",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e8",
                    "name": "Junda Chen",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7e9",
                    "name": "Boyuan Zheng",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7ea",
                    "name": "Peihang Li",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7eb",
                    "name": "Fangyu Lei",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7ec",
                    "name": "Ruisheng Cao",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7ed",
                    "name": "Yeqiao Fu",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7ee",
                    "name": "Dongchan Shin",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7ef",
                    "name": "Martin Shin",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f0",
                    "name": "Jiarui Hu",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f1",
                    "name": "Yuyan Wang",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f2",
                    "name": "Jixuan Chen",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f3",
                    "name": "Yuxiao Ye",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f4",
                    "name": "Danyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f5",
                    "name": "Dikang Du",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f6",
                    "name": "Hao Hu",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f7",
                    "name": "Huarong Chen",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f8",
                    "name": "Zaida Zhou",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7f9",
                    "name": "Yipu Wang",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7fa",
                    "name": "Heng Wang",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7fb",
                    "name": "Diyi Yang",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7fc",
                    "name": "Victor Zhong",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7fd",
                    "name": "Flood Sung",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7fe",
                    "name": "Y. Charles",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac7ff",
                    "name": "Zhilin Yang",
                    "hidden": false
                },
                {
                    "_id": "689bed2efab6fdd2e52ac800",
                    "name": "Tao Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T17:52:32.000Z",
            "submittedOnDailyAt": "2025-08-13T05:53:45.913Z",
            "title": "OpenCUA: Open Foundations for Computer-Use Agents",
            "submittedOnDailyBy": {
                "_id": "669ca7e678115e16bdfc9bfc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669ca7e678115e16bdfc9bfc/pku8NvQKqfNQACRqm1YrW.jpeg",
                "isPro": true,
                "fullname": "Lu Dunjie",
                "user": "ludunjie",
                "type": "user"
            },
            "summary": "Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.",
            "upvotes": 5,
            "discussionId": "689bed2efab6fdd2e52ac801",
            "projectPage": "https://opencua.xlang.ai/",
            "githubRepo": "https://github.com/xlang-ai/OpenCUA",
            "ai_summary": "OpenCUA is an open-source framework for vision-language models as computer-use agents, featuring an annotation infrastructure, a large-scale dataset, and a scalable pipeline that achieves state-of-the-art performance.",
            "ai_keywords": [
                "vision-language models",
                "computer-use agents",
                "annotation infrastructure",
                "AgentNet",
                "state-action pairs",
                "Chain-of-Thought reasoning",
                "OSWorld-Verified",
                "OpenAI CUA",
                "GPT-4o"
            ],
            "githubStars": 34
        },
        "publishedAt": "2025-08-12T13:52:32.000Z",
        "title": "OpenCUA: Open Foundations for Computer-Use Agents",
        "summary": "Vision-language models have demonstrated impressive capabilities as\ncomputer-use agents (CUAs) capable of automating diverse computer tasks. As\ntheir commercial potential grows, critical details of the most capable CUA\nsystems remain closed. As these agents will increasingly mediate digital\ninteractions and execute consequential decisions on our behalf, the research\ncommunity needs access to open CUA frameworks to study their capabilities,\nlimitations, and risks. To bridge this gap, we propose OpenCUA, a comprehensive\nopen-source framework for scaling CUA data and foundation models. Our framework\nconsists of: (1) an annotation infrastructure that seamlessly captures human\ncomputer-use demonstrations; (2) AgentNet, the first large-scale computer-use\ntask dataset spanning 3 operating systems and 200+ applications and websites;\n(3) a scalable pipeline that transforms demonstrations into state-action pairs\nwith reflective long Chain-of-Thought reasoning that sustain robust performance\ngains as data scales. Our end-to-end agent models demonstrate strong\nperformance across CUA benchmarks. In particular, OpenCUA-32B achieves an\naverage success rate of 34.8% on OSWorld-Verified, establishing a new\nstate-of-the-art (SOTA) among open-source models and surpassing OpenAI CUA\n(GPT-4o). Further analysis confirms that our approach generalizes well across\ndomains and benefits significantly from increased test-time computation. We\nrelease our annotation tool, datasets, code, and models to build open\nfoundations for further CUA research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09123.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "669ca7e678115e16bdfc9bfc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/669ca7e678115e16bdfc9bfc/pku8NvQKqfNQACRqm1YrW.jpeg",
            "fullname": "Lu Dunjie",
            "name": "ludunjie",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09101",
            "authors": [
                {
                    "_id": "689c00c7fab6fdd2e52ac84b",
                    "name": "Jason Chou",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac84c",
                    "name": "Ao Liu",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac84d",
                    "name": "Yuchi Deng",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac84e",
                    "name": "Zhiying Zeng",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac84f",
                    "name": "Tao Zhang",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac850",
                    "name": "Haotian Zhu",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac851",
                    "name": "Jianwei Cai",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac852",
                    "name": "Yue Mao",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac853",
                    "name": "Chenchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac854",
                    "name": "Lingyun Tan",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac855",
                    "name": "Ziyan Xu",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac856",
                    "name": "Bohui Zhai",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac857",
                    "name": "Hengyi Liu",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac858",
                    "name": "Speed Zhu",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac859",
                    "name": "Wiggin Zhou",
                    "hidden": false
                },
                {
                    "_id": "689c00c7fab6fdd2e52ac85a",
                    "name": "Fengzong Lian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T17:29:20.000Z",
            "submittedOnDailyAt": "2025-08-13T01:38:17.109Z",
            "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators",
            "submittedOnDailyBy": {
                "_id": "64b74b906ab5d14ca7f289cd",
                "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
                "isPro": false,
                "fullname": "xxzcc",
                "user": "xxzcc",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.",
            "upvotes": 5,
            "discussionId": "689c00c7fab6fdd2e52ac85b",
            "ai_summary": "AutoCodeGen creates a large-scale, multilingual code generation benchmark, AutoCodeBench, to evaluate LLMs on diverse and complex tasks without manual annotations.",
            "ai_keywords": [
                "Large Language Models",
                "code generation",
                "benchmarks",
                "manual annotations",
                "multilingual",
                "test cases",
                "multilingual sandbox",
                "reverse-order problem generation",
                "filtering steps",
                "AutoCodeBench",
                "few-shot code generation",
                "AutoCodeBench-Lite",
                "AutoCodeBench-Complete"
            ]
        },
        "publishedAt": "2025-08-12T13:29:20.000Z",
        "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark\n  Generators",
        "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious domains, with code generation emerging as a key area of focus. While\nnumerous benchmarks have been proposed to evaluate their code generation\nabilities, these benchmarks face several critical limitations. First, they\noften rely on manual annotations, which are time-consuming and difficult to\nscale across different programming languages and problem complexities. Second,\nmost existing benchmarks focus primarily on Python, while the few multilingual\nbenchmarks suffer from limited difficulty and uneven language distribution. To\naddress these challenges, we propose AutoCodeGen, an automated method for\ngenerating high-difficulty multilingual code generation datasets without manual\nannotations. AutoCodeGen ensures the correctness and completeness of test cases\nby generating test inputs with LLMs and obtaining test outputs through a\nmultilingual sandbox, while achieving high data quality through reverse-order\nproblem generation and multiple filtering steps. Using this novel method, we\nintroduce AutoCodeBench, a large-scale code generation benchmark comprising\n3,920 problems evenly distributed across 20 programming languages. It is\nspecifically designed to evaluate LLMs on challenging, diverse, and practical\nmultilingual tasks. We evaluate over 30 leading open-source and proprietary\nLLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The\nresults show that even the most advanced LLMs struggle with the complexity,\ndiversity, and multilingual nature of these tasks. Besides, we introduce\nAutoCodeBench-Complete, specifically designed for base models to assess their\nfew-shot code generation capabilities. We hope the AutoCodeBench series will\nserve as a valuable resource and inspire the community to focus on more\nchallenging and practical multilingual code generation scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09101.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "64b74b906ab5d14ca7f289cd",
            "avatarUrl": "/avatars/b131b7c4ce5216708ca4a678f35ead0a.svg",
            "fullname": "xxzcc",
            "name": "xxzcc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08113",
            "authors": [
                {
                    "_id": "689d2f69b083e610d741e980",
                    "name": "Yinpei Dai",
                    "hidden": false
                },
                {
                    "_id": "689d2f69b083e610d741e981",
                    "name": "Jayjun Lee",
                    "hidden": false
                },
                {
                    "_id": "689d2f69b083e610d741e982",
                    "name": "Yichi Zhang",
                    "hidden": false
                },
                {
                    "_id": "689d2f69b083e610d741e983",
                    "name": "Ziqiao Ma",
                    "hidden": false
                },
                {
                    "_id": "689d2f69b083e610d741e984",
                    "name": "Jed Yang",
                    "hidden": false
                },
                {
                    "_id": "689d2f69b083e610d741e985",
                    "name": "Amir Zadeh",
                    "hidden": false
                },
                {
                    "_id": "689d2f69b083e610d741e986",
                    "name": "Chuan Li",
                    "hidden": false
                },
                {
                    "_id": "689d2f69b083e610d741e987",
                    "name": "Nima Fazeli",
                    "hidden": false
                },
                {
                    "_id": "689d2f69b083e610d741e988",
                    "name": "Joyce Chai",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/648b889575d21f74acf9cbb1/EOgHoyYWZ042WM_AGpu-m.mp4"
            ],
            "publishedAt": "2025-08-11T15:53:23.000Z",
            "submittedOnDailyAt": "2025-08-13T23:17:46.788Z",
            "title": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of\n  Visuomotor Policies",
            "submittedOnDailyBy": {
                "_id": "648b889575d21f74acf9cbb1",
                "avatarUrl": "/avatars/0ffee0809f9e2670821721c005d8dde4.svg",
                "isPro": false,
                "fullname": "Dai",
                "user": "Yinpei",
                "type": "user"
            },
            "summary": "In this paper, we propose AimBot, a lightweight visual augmentation technique\nthat provides explicit spatial cues to improve visuomotor policy learning in\nrobotic manipulation. AimBot overlays shooting lines and scope reticles onto\nmulti-view RGB images, offering auxiliary visual guidance that encodes the\nend-effector's state. The overlays are computed from depth images, camera\nextrinsics, and the current end-effector pose, explicitly conveying spatial\nrelationships between the gripper and objects in the scene. AimBot incurs\nminimal computational overhead (less than 1 ms) and requires no changes to\nmodel architectures, as it simply replaces original RGB images with augmented\ncounterparts. Despite its simplicity, our results show that AimBot consistently\nimproves the performance of various visuomotor policies in both simulation and\nreal-world settings, highlighting the benefits of spatially grounded visual\nfeedback.",
            "upvotes": 4,
            "discussionId": "689d2f69b083e610d741e989",
            "projectPage": "https://aimbot-reticle.github.io/",
            "githubRepo": "https://github.com/aimbot-reticle/openpi0-aimbot",
            "ai_summary": "AimBot, a lightweight visual augmentation technique, improves visuomotor policy learning in robotic manipulation by overlaying spatial cues onto RGB images, enhancing performance in both simulation and real-world settings.",
            "ai_keywords": [
                "visuomotor policy learning",
                "robotic manipulation",
                "visual augmentation",
                "shooting lines",
                "scope reticles",
                "depth images",
                "camera extrinsics",
                "end-effector pose",
                "spatial relationships",
                "gripper",
                "augmented images"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-08-11T11:53:23.000Z",
        "title": "AimBot: A Simple Auxiliary Visual Cue to Enhance Spatial Awareness of\n  Visuomotor Policies",
        "summary": "In this paper, we propose AimBot, a lightweight visual augmentation technique\nthat provides explicit spatial cues to improve visuomotor policy learning in\nrobotic manipulation. AimBot overlays shooting lines and scope reticles onto\nmulti-view RGB images, offering auxiliary visual guidance that encodes the\nend-effector's state. The overlays are computed from depth images, camera\nextrinsics, and the current end-effector pose, explicitly conveying spatial\nrelationships between the gripper and objects in the scene. AimBot incurs\nminimal computational overhead (less than 1 ms) and requires no changes to\nmodel architectures, as it simply replaces original RGB images with augmented\ncounterparts. Despite its simplicity, our results show that AimBot consistently\nimproves the performance of various visuomotor policies in both simulation and\nreal-world settings, highlighting the benefits of spatially grounded visual\nfeedback.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/648b889575d21f74acf9cbb1/EOgHoyYWZ042WM_AGpu-m.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08113.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "648b889575d21f74acf9cbb1",
            "avatarUrl": "/avatars/0ffee0809f9e2670821721c005d8dde4.svg",
            "fullname": "Dai",
            "name": "Yinpei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08680",
            "authors": [
                {
                    "_id": "689c7f7bfab6fdd2e52ac9ce",
                    "name": "Armel Zebaze",
                    "hidden": false
                },
                {
                    "_id": "689c7f7bfab6fdd2e52ac9cf",
                    "name": "Beno√Æt Sagot",
                    "hidden": false
                },
                {
                    "_id": "689c7f7bfab6fdd2e52ac9d0",
                    "name": "Rachel Bawden",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T06:58:02.000Z",
            "submittedOnDailyAt": "2025-08-13T11:10:33.485Z",
            "title": "TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine\n  Translation",
            "submittedOnDailyBy": {
                "_id": "63952b31ee411d4c6fba891c",
                "avatarUrl": "/avatars/e901c3aaa16d3a06dff09896ce8a67a2.svg",
                "isPro": false,
                "fullname": "Armel Randy Zebaze",
                "user": "ArmelRandy",
                "type": "user"
            },
            "summary": "LLMs have been shown to perform well in machine translation (MT) with the use\nof in-context learning (ICL), rivaling supervised models when translating into\nhigh-resource languages (HRLs). However, they lag behind when translating into\nlow-resource language (LRLs). Example selection via similarity search and\nsupervised fine-tuning help. However the improvements they give are limited by\nthe size, quality and diversity of existing parallel datasets. A common\ntechnique in low-resource MT is synthetic parallel data creation, the most\nfrequent of which is backtranslation, whereby existing target-side texts are\nautomatically translated into the source language. However, this assumes the\nexistence of good quality and relevant target-side texts, which are not readily\navailable for many LRLs. In this paper, we present TopXGen, an\nLLM-based approach for the generation of high quality and topic-diverse data in\nmultiple LRLs, which can then be backtranslated to produce useful and diverse\nparallel texts for ICL and fine-tuning. Our intuition is that while LLMs\nstruggle to translate into LRLs, their ability to translate well into HRLs and\ntheir multilinguality enable them to generate good quality, natural-sounding\ntarget-side texts, which can be translated well into a high-resource source\nlanguage. We show that TopXGen boosts LLM translation performance\nduring fine-tuning and in-context learning. Code and outputs are available at\nhttps://github.com/ArmelRandy/topxgen.",
            "upvotes": 3,
            "discussionId": "689c7f7bfab6fdd2e52ac9d1",
            "githubRepo": "https://github.com/ArmelRandy/topxgen",
            "ai_summary": "TopXGen uses LLMs to generate high-quality, topic-diverse target-side texts for LRLs, which can be backtranslated to improve translation performance in ICL and fine-tuning.",
            "ai_keywords": [
                "LLMs",
                "machine translation",
                "in-context learning",
                "low-resource languages",
                "synthetic parallel data",
                "backtranslation",
                "multilinguality"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-08-12T02:58:02.000Z",
        "title": "TopXGen: Topic-Diverse Parallel Data Generation for Low-Resource Machine\n  Translation",
        "summary": "LLMs have been shown to perform well in machine translation (MT) with the use\nof in-context learning (ICL), rivaling supervised models when translating into\nhigh-resource languages (HRLs). However, they lag behind when translating into\nlow-resource language (LRLs). Example selection via similarity search and\nsupervised fine-tuning help. However the improvements they give are limited by\nthe size, quality and diversity of existing parallel datasets. A common\ntechnique in low-resource MT is synthetic parallel data creation, the most\nfrequent of which is backtranslation, whereby existing target-side texts are\nautomatically translated into the source language. However, this assumes the\nexistence of good quality and relevant target-side texts, which are not readily\navailable for many LRLs. In this paper, we present TopXGen, an\nLLM-based approach for the generation of high quality and topic-diverse data in\nmultiple LRLs, which can then be backtranslated to produce useful and diverse\nparallel texts for ICL and fine-tuning. Our intuition is that while LLMs\nstruggle to translate into LRLs, their ability to translate well into HRLs and\ntheir multilinguality enable them to generate good quality, natural-sounding\ntarget-side texts, which can be translated well into a high-resource source\nlanguage. We show that TopXGen boosts LLM translation performance\nduring fine-tuning and in-context learning. Code and outputs are available at\nhttps://github.com/ArmelRandy/topxgen.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08680.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63952b31ee411d4c6fba891c",
            "avatarUrl": "/avatars/e901c3aaa16d3a06dff09896ce8a67a2.svg",
            "fullname": "Armel Randy Zebaze",
            "name": "ArmelRandy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08248",
            "authors": [
                {
                    "_id": "689ab940fab6fdd2e52ac4ab",
                    "user": {
                        "_id": "66da6972eae491c64243e8f3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SgX1j3QGKMk_YFTnM9Wr_.png",
                        "isPro": false,
                        "fullname": "Shuyuan Tu",
                        "user": "FrancisRing",
                        "type": "user"
                    },
                    "name": "Shuyuan Tu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-13T19:02:50.161Z",
                    "hidden": false
                },
                {
                    "_id": "689ab940fab6fdd2e52ac4ac",
                    "name": "Yueming Pan",
                    "hidden": false
                },
                {
                    "_id": "689ab940fab6fdd2e52ac4ad",
                    "name": "Yinming Huang",
                    "hidden": false
                },
                {
                    "_id": "689ab940fab6fdd2e52ac4ae",
                    "name": "Xintong Han",
                    "hidden": false
                },
                {
                    "_id": "689ab940fab6fdd2e52ac4af",
                    "name": "Zhen Xing",
                    "hidden": false
                },
                {
                    "_id": "689ab940fab6fdd2e52ac4b0",
                    "name": "Qi Dai",
                    "hidden": false
                },
                {
                    "_id": "689ab940fab6fdd2e52ac4b1",
                    "name": "Chong Luo",
                    "hidden": false
                },
                {
                    "_id": "689ab940fab6fdd2e52ac4b2",
                    "name": "Zuxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "689ab940fab6fdd2e52ac4b3",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66da6972eae491c64243e8f3/ciOYZHqYpsL4ekNsMRPOC.jpeg"
            ],
            "publishedAt": "2025-08-11T17:58:24.000Z",
            "submittedOnDailyAt": "2025-08-13T23:24:15.134Z",
            "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation",
            "submittedOnDailyBy": {
                "_id": "66da6972eae491c64243e8f3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SgX1j3QGKMk_YFTnM9Wr_.png",
                "isPro": false,
                "fullname": "Shuyuan Tu",
                "user": "FrancisRing",
                "type": "user"
            },
            "summary": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.",
            "upvotes": 3,
            "discussionId": "689ab940fab6fdd2e52ac4b4",
            "projectPage": "https://francis-rings.github.io/StableAvatar/",
            "githubRepo": "https://github.com/Francis-Rings/StableAvatar",
            "ai_summary": "StableAvatar, an end-to-end video diffusion transformer, synthesizes infinite-length high-quality audio-driven avatar videos with natural synchronization and identity consistency using a Time-step-aware Audio Adapter and Audio Native Guidance Mechanism.",
            "ai_keywords": [
                "diffusion models",
                "audio-driven avatar video generation",
                "end-to-end video diffusion transformer",
                "infinite-length video generation",
                "reference image",
                "audio embeddings",
                "cross-attention",
                "diffusion backbones",
                "latent distribution error accumulation",
                "Time-step-aware Audio Adapter",
                "Audio Native Guidance Mechanism",
                "Dynamic Weighted Sliding-window Strategy"
            ],
            "githubStars": 147
        },
        "publishedAt": "2025-08-11T13:58:24.000Z",
        "title": "StableAvatar: Infinite-Length Audio-Driven Avatar Video Generation",
        "summary": "Current diffusion models for audio-driven avatar video generation struggle to\nsynthesize long videos with natural audio synchronization and identity\nconsistency. This paper presents StableAvatar, the first end-to-end video\ndiffusion transformer that synthesizes infinite-length high-quality videos\nwithout post-processing. Conditioned on a reference image and audio,\nStableAvatar integrates tailored training and inference modules to enable\ninfinite-length video generation. We observe that the main reason preventing\nexisting models from generating long videos lies in their audio modeling. They\ntypically rely on third-party off-the-shelf extractors to obtain audio\nembeddings, which are then directly injected into the diffusion model via\ncross-attention. Since current diffusion backbones lack any audio-related\npriors, this approach causes severe latent distribution error accumulation\nacross video clips, leading the latent distribution of subsequent segments to\ndrift away from the optimal distribution gradually. To address this,\nStableAvatar introduces a novel Time-step-aware Audio Adapter that prevents\nerror accumulation via time-step-aware modulation. During inference, we propose\na novel Audio Native Guidance Mechanism to further enhance the audio\nsynchronization by leveraging the diffusion's own evolving joint audio-latent\nprediction as a dynamic guidance signal. To enhance the smoothness of the\ninfinite-length videos, we introduce a Dynamic Weighted Sliding-window Strategy\nthat fuses latent over time. Experiments on benchmarks show the effectiveness\nof StableAvatar both qualitatively and quantitatively.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66da6972eae491c64243e8f3/ciOYZHqYpsL4ekNsMRPOC.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08248.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66da6972eae491c64243e8f3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SgX1j3QGKMk_YFTnM9Wr_.png",
            "fullname": "Shuyuan Tu",
            "name": "FrancisRing",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09050",
            "authors": [
                {
                    "_id": "689c2f33fab6fdd2e52ac8dd",
                    "name": "Germ√°n D√≠az Agreda",
                    "hidden": false
                },
                {
                    "_id": "689c2f33fab6fdd2e52ac8de",
                    "name": "Carlos Andres Duran Paredes",
                    "hidden": false
                },
                {
                    "_id": "689c2f33fab6fdd2e52ac8df",
                    "name": "Mateo Buenaventura Samboni",
                    "hidden": false
                },
                {
                    "_id": "689c2f33fab6fdd2e52ac8e0",
                    "name": "Jhon Alejandro Andrade",
                    "hidden": false
                },
                {
                    "_id": "689c2f33fab6fdd2e52ac8e1",
                    "user": {
                        "_id": "628ddf04986ae70e823298f7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
                        "isPro": false,
                        "fullname": "Sebasti√°n Andres Cajas Ord√≥√±ez",
                        "user": "sebasmos",
                        "type": "user"
                    },
                    "name": "Sebasti√°n Andr√©s Cajas Ordo√±ez",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:10:31.803Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T16:10:05.000Z",
            "submittedOnDailyAt": "2025-08-13T04:54:47.461Z",
            "title": "Bridging Theory and Practice in Quantum Game Theory: Optimized\n  Implementation of the Battle of the Sexes with Error Mitigation on NISQ\n  Hardware",
            "submittedOnDailyBy": {
                "_id": "628ddf04986ae70e823298f7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
                "isPro": false,
                "fullname": "Sebasti√°n Andres Cajas Ord√≥√±ez",
                "user": "sebasmos",
                "type": "user"
            },
            "summary": "Implementing quantum game theory on real hardware is challenging due to\nnoise, decoherence, and limited qubit connectivity, yet such demonstrations are\nessential to validate theoretical predictions. We present one of the first full\nexperimental realizations of the Battle of the Sexes game under the\nEisert-Wilkens-Lewenstein (EWL) framework on IBM Quantum's ibm sherbrooke\nsuperconducting processor. Four quantum strategies (I, H, R(pi/4), R(pi))\nwere evaluated across 31 entanglement values gamma in [0, pi] using 2048\nshots per configuration, enabling a direct comparison between analytical\npredictions and hardware execution. To mitigate noise and variability, we\nintroduce a Guided Circuit Mapping (GCM) method that dynamically selects qubit\npairs and optimizes routing based on real-time topology and calibration data.\nThe analytical model forecasts up to 108% payoff improvement over the\nclassical equilibrium, and despite hardware-induced deviations, experimental\nresults with GCM preserve the expected payoff trends within 3.5%-12%\nrelative error. These findings show that quantum advantages in strategic\ncoordination can persist under realistic NISQ conditions, providing a pathway\ntoward practical applications of quantum game theory in multi-agent, economic,\nand distributed decision-making systems.",
            "upvotes": 2,
            "discussionId": "689c2f34fab6fdd2e52ac8e2",
            "ai_summary": "Quantum game theory demonstrated on IBM Quantum hardware using the Eisert-Wilkens-Lewenstein framework shows persistent quantum advantages in strategic coordination despite noise and decoherence.",
            "ai_keywords": [
                "quantum game theory",
                "Eisert-Wilkens-Lewenstein framework",
                "IBM Quantum",
                "superconducting processor",
                "quantum strategies",
                "entanglement",
                "Guided Circuit Mapping",
                "NISQ conditions",
                "multi-agent systems",
                "economic systems",
                "distributed decision-making systems"
            ]
        },
        "publishedAt": "2025-08-12T12:10:05.000Z",
        "title": "Bridging Theory and Practice in Quantum Game Theory: Optimized\n  Implementation of the Battle of the Sexes with Error Mitigation on NISQ\n  Hardware",
        "summary": "Implementing quantum game theory on real hardware is challenging due to\nnoise, decoherence, and limited qubit connectivity, yet such demonstrations are\nessential to validate theoretical predictions. We present one of the first full\nexperimental realizations of the Battle of the Sexes game under the\nEisert-Wilkens-Lewenstein (EWL) framework on IBM Quantum's ibm sherbrooke\nsuperconducting processor. Four quantum strategies (I, H, R(pi/4), R(pi))\nwere evaluated across 31 entanglement values gamma in [0, pi] using 2048\nshots per configuration, enabling a direct comparison between analytical\npredictions and hardware execution. To mitigate noise and variability, we\nintroduce a Guided Circuit Mapping (GCM) method that dynamically selects qubit\npairs and optimizes routing based on real-time topology and calibration data.\nThe analytical model forecasts up to 108% payoff improvement over the\nclassical equilibrium, and despite hardware-induced deviations, experimental\nresults with GCM preserve the expected payoff trends within 3.5%-12%\nrelative error. These findings show that quantum advantages in strategic\ncoordination can persist under realistic NISQ conditions, providing a pathway\ntoward practical applications of quantum game theory in multi-agent, economic,\nand distributed decision-making systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09050.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "628ddf04986ae70e823298f7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/628ddf04986ae70e823298f7/P6GyCswDo3dDMd59DEkWC.png",
            "fullname": "Sebasti√°n Andres Cajas Ord√≥√±ez",
            "name": "sebasmos",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.05813",
            "authors": [
                {
                    "_id": "689b6dc9fab6fdd2e52ac74c",
                    "name": "Raphael Du Sablon",
                    "hidden": false
                },
                {
                    "_id": "689b6dc9fab6fdd2e52ac74d",
                    "user": {
                        "_id": "664f7d4829503f958d04180a",
                        "avatarUrl": "/avatars/c4ba9007b1e952f1e1fc906ef0acc3ba.svg",
                        "isPro": false,
                        "fullname": "David Hart",
                        "user": "incrl",
                        "type": "user"
                    },
                    "name": "David Hart",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:13:56.654Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-07T19:35:01.000Z",
            "submittedOnDailyAt": "2025-08-13T12:27:50.663Z",
            "title": "Optimization-Free Style Transfer for 3D Gaussian Splats",
            "submittedOnDailyBy": {
                "_id": "664f7d4829503f958d04180a",
                "avatarUrl": "/avatars/c4ba9007b1e952f1e1fc906ef0acc3ba.svg",
                "isPro": false,
                "fullname": "David Hart",
                "user": "incrl",
                "type": "user"
            },
            "summary": "The task of style transfer for 3D Gaussian splats has been explored in many\nprevious works, but these require reconstructing or fine-tuning the splat while\nincorporating style information or optimizing a feature extraction network on\nthe splat representation. We propose a reconstruction- and optimization-free\napproach to stylizing 3D Gaussian splats. This is done by generating a graph\nstructure across the implicit surface of the splat representation. A\nfeed-forward, surface-based stylization method is then used and interpolated\nback to the individual splats in the scene. This allows for any style image and\n3D Gaussian splat to be used without any additional training or optimization.\nThis also allows for fast stylization of splats, achieving speeds under 2\nminutes even on consumer-grade hardware. We demonstrate the quality results\nthis approach achieves and compare to other 3D Gaussian splat style transfer\nmethods. Code is publicly available at\nhttps://github.com/davidmhart/FastSplatStyler.",
            "upvotes": 2,
            "discussionId": "689b6dc9fab6fdd2e52ac74e",
            "ai_summary": "A novel method for style transfer of 3D Gaussian splats involves generating a graph structure and using a feed-forward, surface-based stylization technique without reconstruction or optimization.",
            "ai_keywords": [
                "Gaussian splats",
                "style transfer",
                "graph structure",
                "feed-forward",
                "surface-based stylization"
            ]
        },
        "publishedAt": "2025-08-07T15:35:01.000Z",
        "title": "Optimization-Free Style Transfer for 3D Gaussian Splats",
        "summary": "The task of style transfer for 3D Gaussian splats has been explored in many\nprevious works, but these require reconstructing or fine-tuning the splat while\nincorporating style information or optimizing a feature extraction network on\nthe splat representation. We propose a reconstruction- and optimization-free\napproach to stylizing 3D Gaussian splats. This is done by generating a graph\nstructure across the implicit surface of the splat representation. A\nfeed-forward, surface-based stylization method is then used and interpolated\nback to the individual splats in the scene. This allows for any style image and\n3D Gaussian splat to be used without any additional training or optimization.\nThis also allows for fast stylization of splats, achieving speeds under 2\nminutes even on consumer-grade hardware. We demonstrate the quality results\nthis approach achieves and compare to other 3D Gaussian splat style transfer\nmethods. Code is publicly available at\nhttps://github.com/davidmhart/FastSplatStyler.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05813.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "664f7d4829503f958d04180a",
            "avatarUrl": "/avatars/c4ba9007b1e952f1e1fc906ef0acc3ba.svg",
            "fullname": "David Hart",
            "name": "incrl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.08855",
            "authors": [
                {
                    "_id": "689c224ffab6fdd2e52ac8aa",
                    "user": {
                        "_id": "637390430938c0754238276c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637390430938c0754238276c/O7IIJdZsS9yjznh780XwA.jpeg",
                        "isPro": false,
                        "fullname": "Sekh Mainul Islam",
                        "user": "sekhcopenlu",
                        "type": "user"
                    },
                    "name": "Sekh Mainul Islam",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:10:36.205Z",
                    "hidden": false
                },
                {
                    "_id": "689c224ffab6fdd2e52ac8ab",
                    "name": "Nadav Borenstein",
                    "hidden": false
                },
                {
                    "_id": "689c224ffab6fdd2e52ac8ac",
                    "name": "Siddhesh Milind Pawar",
                    "hidden": false
                },
                {
                    "_id": "689c224ffab6fdd2e52ac8ad",
                    "name": "Haeun Yu",
                    "hidden": false
                },
                {
                    "_id": "689c224ffab6fdd2e52ac8ae",
                    "name": "Arnav Arora",
                    "hidden": false
                },
                {
                    "_id": "689c224ffab6fdd2e52ac8af",
                    "name": "Isabelle Augenstein",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T11:23:44.000Z",
            "submittedOnDailyAt": "2025-08-13T08:11:32.706Z",
            "title": "BiasGym: Fantastic Biases and How to Find (and Remove) Them",
            "submittedOnDailyBy": {
                "_id": "637390430938c0754238276c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637390430938c0754238276c/O7IIJdZsS9yjznh780XwA.jpeg",
                "isPro": false,
                "fullname": "Sekh Mainul Islam",
                "user": "sekhcopenlu",
                "type": "user"
            },
            "summary": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during training. We demonstrate the effectiveness\nof BiasGym in reducing real-world stereotypes (e.g., people from a country\nbeing `reckless drivers') and in probing fictional associations (e.g., people\nfrom a country having `blue skin'), showing its utility for both safety\ninterventions and interpretability research.",
            "upvotes": 1,
            "discussionId": "689c224ffab6fdd2e52ac8b0",
            "ai_summary": "BiasGym is a framework for injecting, analyzing, and mitigating biases in Large Language Models through token-based fine-tuning and signal analysis.",
            "ai_keywords": [
                "Large Language Models",
                "BiasGym",
                "BiasInject",
                "BiasScope",
                "token-based fine-tuning",
                "conceptual associations",
                "bias elicitation",
                "mechanistic analysis",
                "targeted debiasing",
                "real-world stereotypes",
                "fictional associations"
            ]
        },
        "publishedAt": "2025-08-12T07:23:44.000Z",
        "title": "BiasGym: Fantastic Biases and How to Find (and Remove) Them",
        "summary": "Understanding biases and stereotypes encoded in the weights of Large Language\nModels (LLMs) is crucial for developing effective mitigation strategies. Biased\nbehaviour is often subtle and non-trivial to isolate, even when deliberately\nelicited, making systematic analysis and debiasing particularly challenging. To\naddress this, we introduce BiasGym, a simple, cost-effective, and generalizable\nframework for reliably injecting, analyzing, and mitigating conceptual\nassociations within LLMs. BiasGym consists of two components: BiasInject, which\ninjects specific biases into the model via token-based fine-tuning while\nkeeping the model frozen, and BiasScope, which leverages these injected signals\nto identify and steer the components responsible for biased behavior. Our\nmethod enables consistent bias elicitation for mechanistic analysis, supports\ntargeted debiasing without degrading performance on downstream tasks, and\ngeneralizes to biases unseen during training. We demonstrate the effectiveness\nof BiasGym in reducing real-world stereotypes (e.g., people from a country\nbeing `reckless drivers') and in probing fictional associations (e.g., people\nfrom a country having `blue skin'), showing its utility for both safety\ninterventions and interpretability research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08855.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637390430938c0754238276c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637390430938c0754238276c/O7IIJdZsS9yjznh780XwA.jpeg",
            "fullname": "Sekh Mainul Islam",
            "name": "sekhcopenlu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.08180",
            "authors": [
                {
                    "_id": "689ade29fab6fdd2e52ac565",
                    "user": {
                        "_id": "67bed13027ea9754039d63c3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JHlZXuwcY0y_estgKPkPT.jpeg",
                        "isPro": false,
                        "fullname": "Luca Zedda",
                        "user": "Snarcy",
                        "type": "user"
                    },
                    "name": "Luca Zedda",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:18:59.626Z",
                    "hidden": false
                },
                {
                    "_id": "689ade29fab6fdd2e52ac566",
                    "name": "Andrea Loddo",
                    "hidden": false
                },
                {
                    "_id": "689ade29fab6fdd2e52ac567",
                    "name": "Cecilia Di Ruberto",
                    "hidden": false
                },
                {
                    "_id": "689ade29fab6fdd2e52ac568",
                    "name": "Carsten Marr",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T16:59:31.000Z",
            "submittedOnDailyAt": "2025-08-13T17:02:36.806Z",
            "title": "RedDino: A foundation model for red blood cell analysis",
            "submittedOnDailyBy": {
                "_id": "67bed13027ea9754039d63c3",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JHlZXuwcY0y_estgKPkPT.jpeg",
                "isPro": false,
                "fullname": "Luca Zedda",
                "user": "Snarcy",
                "type": "user"
            },
            "summary": "Red blood cells (RBCs) are essential to human health, and their precise\nmorphological analysis is important for diagnosing hematological disorders.\nDespite the promise of foundation models in medical diagnostics, comprehensive\nAI solutions for RBC analysis remain scarce. We present RedDino, a\nself-supervised foundation model designed for RBC image analysis. RedDino uses\nan RBC-specific adaptation of the DINOv2 self-supervised learning framework and\nis trained on a curated dataset of 1.25 million RBC images from diverse\nacquisition modalities and sources. Extensive evaluations show that RedDino\noutperforms existing state-of-the-art models on RBC shape classification.\nThrough assessments including linear probing and nearest neighbor\nclassification, we confirm its strong feature representations and\ngeneralization ability. Our main contributions are: (1) a foundation model\ntailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations\nfor RBC modeling, and (3) a detailed evaluation of generalization performance.\nRedDino addresses key challenges in computational hematology by capturing\nnuanced morphological features, advancing the development of reliable\ndiagnostic tools. The source code and pretrained models for RedDino are\navailable at https://github.com/Snarci/RedDino, and the pretrained models can\nbe downloaded from our Hugging Face collection at\nhttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc",
            "upvotes": 1,
            "discussionId": "689ade2afab6fdd2e52ac569",
            "githubRepo": "https://github.com/Snarci/RedDino",
            "ai_summary": "RedDino, a self-supervised foundation model using DINOv2, excels in RBC shape classification and generalization, addressing challenges in computational hematology.",
            "ai_keywords": [
                "self-supervised foundation model",
                "DINOv2",
                "RBC image analysis",
                "RBC shape classification",
                "linear probing",
                "nearest neighbor classification",
                "feature representations",
                "generalization ability",
                "computational hematology"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-08-11T12:59:31.000Z",
        "title": "RedDino: A foundation model for red blood cell analysis",
        "summary": "Red blood cells (RBCs) are essential to human health, and their precise\nmorphological analysis is important for diagnosing hematological disorders.\nDespite the promise of foundation models in medical diagnostics, comprehensive\nAI solutions for RBC analysis remain scarce. We present RedDino, a\nself-supervised foundation model designed for RBC image analysis. RedDino uses\nan RBC-specific adaptation of the DINOv2 self-supervised learning framework and\nis trained on a curated dataset of 1.25 million RBC images from diverse\nacquisition modalities and sources. Extensive evaluations show that RedDino\noutperforms existing state-of-the-art models on RBC shape classification.\nThrough assessments including linear probing and nearest neighbor\nclassification, we confirm its strong feature representations and\ngeneralization ability. Our main contributions are: (1) a foundation model\ntailored for RBC analysis, (2) ablation studies exploring DINOv2 configurations\nfor RBC modeling, and (3) a detailed evaluation of generalization performance.\nRedDino addresses key challenges in computational hematology by capturing\nnuanced morphological features, advancing the development of reliable\ndiagnostic tools. The source code and pretrained models for RedDino are\navailable at https://github.com/Snarci/RedDino, and the pretrained models can\nbe downloaded from our Hugging Face collection at\nhttps://huggingface.co/collections/Snarcy/reddino-689a13e29241d2e5690202fc",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08180.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67bed13027ea9754039d63c3",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/JHlZXuwcY0y_estgKPkPT.jpeg",
            "fullname": "Luca Zedda",
            "name": "Snarcy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.06813",
            "authors": [
                {
                    "_id": "689b62c6fab6fdd2e52ac72f",
                    "name": "Brendan R. Hogan",
                    "hidden": false
                },
                {
                    "_id": "689b62c6fab6fdd2e52ac730",
                    "name": "Will Brown",
                    "hidden": false
                },
                {
                    "_id": "689b62c6fab6fdd2e52ac731",
                    "name": "Adel Boyarsky",
                    "hidden": false
                },
                {
                    "_id": "689b62c6fab6fdd2e52ac732",
                    "name": "Anderson Schneider",
                    "hidden": false
                },
                {
                    "_id": "689b62c6fab6fdd2e52ac733",
                    "name": "Yuriy Nevmyvaka",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-09T04:22:07.000Z",
            "submittedOnDailyAt": "2025-08-13T13:21:45.370Z",
            "title": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language",
            "submittedOnDailyBy": {
                "_id": "608aabf24955d2bfc3cd99c6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/T762Ut0Y-w0sZB2ynvfbJ.jpeg",
                "isPro": true,
                "fullname": "Aritra Roy Gosthipaty",
                "user": "ariG23498",
                "type": "user"
            },
            "summary": "Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals.",
            "upvotes": 1,
            "discussionId": "689b62c6fab6fdd2e52ac734",
            "ai_summary": "A comprehensive approach is presented for adapting large language models to the Q programming language, achieving superior performance compared to existing models on a newly created benchmark dataset.",
            "ai_keywords": [
                "large language models",
                "Q programming language",
                "Leetcode style evaluation dataset",
                "pretraining",
                "supervised fine tuning",
                "reinforcement learning",
                "Qwen-2.5 series",
                "pass@1 accuracy",
                "Claude Opus-4",
                "GPT-4.1",
                "dataset construction",
                "model pretraining",
                "supervised fine-tuning",
                "reinforcement learning"
            ]
        },
        "publishedAt": "2025-08-09T00:22:07.000Z",
        "title": "Technical Report: Full-Stack Fine-Tuning for the Q Programming Language",
        "summary": "Even though large language models are becoming increasingly capable, it is\nstill unreasonable to expect them to excel at tasks that are under-represented\non the Internet. Leveraging LLMs for specialized applications, particularly in\nniche programming languages and private domains, remains challenging and\nlargely unsolved. In this work, we address this gap by presenting a\ncomprehensive, open-source approach for adapting LLMs to the Q programming\nlanguage, a popular tool in quantitative finance that is much less present on\nthe Internet compared to Python, C, Java, and other ``mainstream\" languages and\nis therefore not a strong suit of general-purpose AI models. We introduce a new\nLeetcode style evaluation dataset for Q, benchmark major frontier models on the\ndataset, then do pretraining, supervised fine tuning, and reinforcement\nlearning to train a suite of reasoning and non-reasoning models based on the\nQwen-2.5 series, spanning five parameter sizes (1.5B, 3B, 7B, 14B, 32B). Our\nbest model achieves a pass@1 accuracy of 59 percent on our Q benchmark,\nsurpassing the best-performing frontier model, Claude Opus-4 by 29.5 percent.\nAdditionally, all models, even our 1.5B model, outperform GPT-4.1 on this task.\nIn addition to releasing models, code, and data, we provide a detailed\nblueprint for dataset construction, model pretraining, supervised fine-tuning,\nand reinforcement learning. Our methodology is broadly applicable, and we\ndiscuss how these techniques can be extended to other tasks, including those\nwhere evaluation may rely on soft or subjective signals.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06813.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "608aabf24955d2bfc3cd99c6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/608aabf24955d2bfc3cd99c6/T762Ut0Y-w0sZB2ynvfbJ.jpeg",
            "fullname": "Aritra Roy Gosthipaty",
            "name": "ariG23498",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 298
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.06485",
            "authors": [
                {
                    "_id": "689b09b3fab6fdd2e52ac657",
                    "user": {
                        "_id": "689b06627c9c78105c6f7165",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jL7lwPMorVVWRHKgJX_7h.png",
                        "isPro": false,
                        "fullname": "Sofiane Bouaziz",
                        "user": "sofianebouaziz",
                        "type": "user"
                    },
                    "name": "Sofiane Bouaziz",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:15:29.192Z",
                    "hidden": false
                },
                {
                    "_id": "689b09b3fab6fdd2e52ac658",
                    "name": "Adel Hafiane",
                    "hidden": false
                },
                {
                    "_id": "689b09b3fab6fdd2e52ac659",
                    "name": "Raphael Canals",
                    "hidden": false
                },
                {
                    "_id": "689b09b3fab6fdd2e52ac65a",
                    "name": "Rachid Nedjai",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/689b06627c9c78105c6f7165/JS5Or4ix2cT_jKM45-r9O.gif"
            ],
            "publishedAt": "2025-08-08T17:49:46.000Z",
            "submittedOnDailyAt": "2025-08-13T07:03:32.091Z",
            "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface\n  Temperature Estimation via Spatio-Temporal Fusion",
            "submittedOnDailyBy": {
                "_id": "689b06627c9c78105c6f7165",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jL7lwPMorVVWRHKgJX_7h.png",
                "isPro": false,
                "fullname": "Sofiane Bouaziz",
                "user": "sofianebouaziz",
                "type": "user"
            },
            "summary": "Urbanization, climate change, and agricultural stress are increasing the\ndemand for precise and timely environmental monitoring. Land Surface\nTemperature (LST) is a key variable in this context and is retrieved from\nremote sensing satellites. However, these systems face a trade-off between\nspatial and temporal resolution. While spatio-temporal fusion methods offer\npromising solutions, few have addressed the estimation of daily LST at 10 m\nresolution. In this study, we present WGAST, a Weakly-Supervised Generative\nNetwork for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra\nMODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning\nframework designed for this task. It adopts a conditional generative\nadversarial architecture, with a generator composed of four stages: feature\nextraction, fusion, LST reconstruction, and noise suppression. The first stage\nemploys a set of encoders to extract multi-level latent representations from\nthe inputs, which are then fused in the second stage using cosine similarity,\nnormalization, and temporal attention mechanisms. The third stage decodes the\nfused features into high-resolution LST, followed by a Gaussian filter to\nsuppress high-frequency noise. Training follows a weakly supervised strategy\nbased on physical averaging principles and reinforced by a PatchGAN\ndiscriminator. Experiments demonstrate that WGAST outperforms existing methods\nin both quantitative and qualitative evaluations. Compared to the\nbest-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves\nSSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and\neffectively captures fine-scale thermal patterns, as validated against 33\nground-based sensors. The code is available at\nhttps://github.com/Sofianebouaziz1/WGAST.git.",
            "upvotes": 1,
            "discussionId": "689b09b4fab6fdd2e52ac65b",
            "githubRepo": "https://github.com/Sofianebouaziz1/WGAST",
            "ai_summary": "WGAST, a weakly-supervised generative network, enhances daily 10 m land surface temperature estimation using spatio-temporal fusion of Terra MODIS, Landsat 8, and Sentinel-2 data.",
            "ai_keywords": [
                "conditional generative adversarial architecture",
                "feature extraction",
                "fusion",
                "LST reconstruction",
                "noise suppression",
                "cosine similarity",
                "normalization",
                "temporal attention mechanisms",
                "Gaussian filter",
                "weakly supervised strategy",
                "PatchGAN discriminator",
                "RMSE",
                "SSIM"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-08-08T13:49:46.000Z",
        "title": "WGAST: Weakly-Supervised Generative Network for Daily 10 m Land Surface\n  Temperature Estimation via Spatio-Temporal Fusion",
        "summary": "Urbanization, climate change, and agricultural stress are increasing the\ndemand for precise and timely environmental monitoring. Land Surface\nTemperature (LST) is a key variable in this context and is retrieved from\nremote sensing satellites. However, these systems face a trade-off between\nspatial and temporal resolution. While spatio-temporal fusion methods offer\npromising solutions, few have addressed the estimation of daily LST at 10 m\nresolution. In this study, we present WGAST, a Weakly-Supervised Generative\nNetwork for Daily 10 m LST Estimation via Spatio-Temporal Fusion of Terra\nMODIS, Landsat 8, and Sentinel-2. WGAST is the first end-to-end deep learning\nframework designed for this task. It adopts a conditional generative\nadversarial architecture, with a generator composed of four stages: feature\nextraction, fusion, LST reconstruction, and noise suppression. The first stage\nemploys a set of encoders to extract multi-level latent representations from\nthe inputs, which are then fused in the second stage using cosine similarity,\nnormalization, and temporal attention mechanisms. The third stage decodes the\nfused features into high-resolution LST, followed by a Gaussian filter to\nsuppress high-frequency noise. Training follows a weakly supervised strategy\nbased on physical averaging principles and reinforced by a PatchGAN\ndiscriminator. Experiments demonstrate that WGAST outperforms existing methods\nin both quantitative and qualitative evaluations. Compared to the\nbest-performing baseline, on average, WGAST reduces RMSE by 17.18% and improves\nSSIM by 11.00%. Furthermore, WGAST is robust to cloud-induced LST and\neffectively captures fine-scale thermal patterns, as validated against 33\nground-based sensors. The code is available at\nhttps://github.com/Sofianebouaziz1/WGAST.git.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/689b06627c9c78105c6f7165/JS5Or4ix2cT_jKM45-r9O.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06485.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "689b06627c9c78105c6f7165",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jL7lwPMorVVWRHKgJX_7h.png",
            "fullname": "Sofiane Bouaziz",
            "name": "sofianebouaziz",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04676",
            "authors": [
                {
                    "_id": "689c4728fab6fdd2e52ac951",
                    "name": "Yunan Zhang",
                    "hidden": false
                },
                {
                    "_id": "689c4728fab6fdd2e52ac952",
                    "name": "Shuoran Jiang",
                    "hidden": false
                },
                {
                    "_id": "689c4728fab6fdd2e52ac953",
                    "name": "Mengchen Zhao",
                    "hidden": false
                },
                {
                    "_id": "689c4728fab6fdd2e52ac954",
                    "name": "Yuefeng Li",
                    "hidden": false
                },
                {
                    "_id": "689c4728fab6fdd2e52ac955",
                    "name": "Yang Fan",
                    "hidden": false
                },
                {
                    "_id": "689c4728fab6fdd2e52ac956",
                    "name": "Xiangping Wu",
                    "hidden": false
                },
                {
                    "_id": "689c4728fab6fdd2e52ac957",
                    "name": "Qingcai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T17:42:22.000Z",
            "submittedOnDailyAt": "2025-08-13T07:01:48.818Z",
            "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via\n  General Samples Replay",
            "submittedOnDailyBy": {
                "_id": "641d3e9a353524fe41f203e8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641d3e9a353524fe41f203e8/hxdQCUktxW-wp57c4VjeH.png",
                "isPro": false,
                "fullname": "YonasZhang",
                "user": "Qznan",
                "type": "user"
            },
            "summary": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.",
            "upvotes": 1,
            "discussionId": "689c4728fab6fdd2e52ac958",
            "githubRepo": "https://github.com/Qznan/GeRe",
            "ai_summary": "General Sample Replay (GeRe) framework with threshold-based margin (TM) loss addresses catastrophic forgetting in large language models by maintaining activation state consistency during replay learning.",
            "ai_keywords": [
                "continual learning",
                "large language models",
                "catastrophic forgetting",
                "General Sample Replay",
                "GeRe",
                "neural states",
                "enhanced activation states constrained optimization",
                "threshold-based margin",
                "TM loss",
                "replay learning",
                "vanilla label fitting",
                "logit imitation",
                "feature imitation"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-08-06T13:42:22.000Z",
        "title": "GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via\n  General Samples Replay",
        "summary": "The continual learning capability of large language models (LLMs) is crucial\nfor advancing artificial general intelligence. However, continual fine-tuning\nLLMs across various domains often suffers from catastrophic forgetting,\ncharacterized by: 1) significant forgetting of their general capabilities, and\n2) sharp performance declines in previously learned tasks. To simultaneously\naddress both issues in a simple yet stable manner, we propose General Sample\nReplay (GeRe), a framework that use usual pretraining texts for efficient\nanti-forgetting. Beyond revisiting the most prevalent replay-based practices\nunder GeRe, we further leverage neural states to introduce a enhanced\nactivation states constrained optimization method using threshold-based margin\n(TM) loss, which maintains activation state consistency during replay learning.\nWe are the first to validate that a small, fixed set of pre-collected general\nreplay samples is sufficient to resolve both concerns--retaining general\ncapabilities while promoting overall performance across sequential tasks.\nIndeed, the former can inherently facilitate the latter. Through controlled\nexperiments, we systematically compare TM with different replay strategies\nunder the GeRe framework, including vanilla label fitting, logit imitation via\nKL divergence and feature imitation via L1/L2 losses. Results demonstrate that\nTM consistently improves performance and exhibits better robustness. Our work\npaves the way for efficient replay of LLMs for the future. Our code and data\nare available at https://github.com/Qznan/GeRe.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04676.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "641d3e9a353524fe41f203e8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/641d3e9a353524fe41f203e8/hxdQCUktxW-wp57c4VjeH.png",
            "fullname": "YonasZhang",
            "name": "Qznan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08292",
            "authors": [
                {
                    "_id": "689d1406b083e610d741e954",
                    "name": "Aryan Gulati",
                    "hidden": false
                },
                {
                    "_id": "689d1406b083e610d741e955",
                    "name": "Brando Miranda",
                    "hidden": false
                },
                {
                    "_id": "689d1406b083e610d741e956",
                    "name": "Eric Chen",
                    "hidden": false
                },
                {
                    "_id": "689d1406b083e610d741e957",
                    "name": "Emily Xia",
                    "hidden": false
                },
                {
                    "_id": "689d1406b083e610d741e958",
                    "name": "Kai Fronsdal",
                    "hidden": false
                },
                {
                    "_id": "689d1406b083e610d741e959",
                    "name": "Bruno Dumont",
                    "hidden": false
                },
                {
                    "_id": "689d1406b083e610d741e95a",
                    "name": "Elyas Obbad",
                    "hidden": false
                },
                {
                    "_id": "689d1406b083e610d741e95b",
                    "name": "Sanmi Koyejo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-05T17:57:50.000Z",
            "submittedOnDailyAt": "2025-08-13T21:12:48.431Z",
            "title": "Putnam-AXIOM: A Functional and Static Benchmark",
            "submittedOnDailyBy": {
                "_id": "60ec6453046c81764c7e06f0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ec6453046c81764c7e06f0/gnKlFE5bkpvbDQNb2jrnV.png",
                "isPro": false,
                "fullname": "Brando Miranda",
                "user": "brando",
                "type": "user"
            },
            "summary": "Current mathematical reasoning benchmarks for large language models (LLMs)\nare approaching saturation, with some achieving > 90% accuracy, and are\nincreasingly compromised by training-set contamination. We introduce\nPutnam-AXIOM, a benchmark of 522 university-level competition problems drawn\nfrom the prestigious William Lowell Putnam Mathematical Competition, and\nPutnam-AXIOM Variation, an unseen companion set of 100 functional variants\ngenerated by programmatically perturbing variables and constants. The variation\nprotocol produces an unlimited stream of equally difficult, unseen instances --\nyielding a contamination-resilient test bed. On the Original set, OpenAI's\no1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy\ndrops by 19.6% (46.8% relative decrease) on the paired Variations. The\nremaining eighteen models show the same downward trend, ten of them with\nnon-overlapping 95% confidence intervals. These gaps suggest memorization and\nhighlight the necessity of dynamic benchmarks. We complement \"boxed\" accuracy\nwith Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores\nreasoning traces and automates natural language proof evaluations. Putnam-AXIOM\ntherefore provides a rigorous, contamination-resilient evaluation framework for\nassessing advanced mathematical reasoning of LLMs. Data and evaluation code are\npublicly available at https://github.com/brando90/putnam-axiom.",
            "upvotes": 1,
            "discussionId": "689d1406b083e610d741e95c",
            "ai_summary": "Putnam-AXIOM and its variation set provide a contamination-resilient benchmark for evaluating advanced mathematical reasoning in large language models, revealing issues with memorization.",
            "ai_keywords": [
                "Putnam-AXIOM",
                "Putnam-AXIOM Variation",
                "Teacher-Forced Accuracy",
                "TFA",
                "mathematical reasoning",
                "large language models",
                "LLMs",
                "training-set contamination",
                "memorization",
                "dynamic benchmarks"
            ]
        },
        "publishedAt": "2025-08-05T13:57:50.000Z",
        "title": "Putnam-AXIOM: A Functional and Static Benchmark",
        "summary": "Current mathematical reasoning benchmarks for large language models (LLMs)\nare approaching saturation, with some achieving > 90% accuracy, and are\nincreasingly compromised by training-set contamination. We introduce\nPutnam-AXIOM, a benchmark of 522 university-level competition problems drawn\nfrom the prestigious William Lowell Putnam Mathematical Competition, and\nPutnam-AXIOM Variation, an unseen companion set of 100 functional variants\ngenerated by programmatically perturbing variables and constants. The variation\nprotocol produces an unlimited stream of equally difficult, unseen instances --\nyielding a contamination-resilient test bed. On the Original set, OpenAI's\no1-preview -- the strongest evaluated model -- scores 41.9%, but its accuracy\ndrops by 19.6% (46.8% relative decrease) on the paired Variations. The\nremaining eighteen models show the same downward trend, ten of them with\nnon-overlapping 95% confidence intervals. These gaps suggest memorization and\nhighlight the necessity of dynamic benchmarks. We complement \"boxed\" accuracy\nwith Teacher-Forced Accuracy (TFA), a lightweight metric that directly scores\nreasoning traces and automates natural language proof evaluations. Putnam-AXIOM\ntherefore provides a rigorous, contamination-resilient evaluation framework for\nassessing advanced mathematical reasoning of LLMs. Data and evaluation code are\npublicly available at https://github.com/brando90/putnam-axiom.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08292.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60ec6453046c81764c7e06f0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ec6453046c81764c7e06f0/gnKlFE5bkpvbDQNb2jrnV.png",
            "fullname": "Brando Miranda",
            "name": "brando",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.08974",
            "authors": [
                {
                    "_id": "689cead8b083e610d741e92e",
                    "name": "Elman Ghazaei",
                    "hidden": false
                },
                {
                    "_id": "689cead8b083e610d741e92f",
                    "name": "Erchan Aptoula",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T14:37:53.000Z",
            "submittedOnDailyAt": "2025-08-13T18:14:08.127Z",
            "title": "Text-conditioned State Space Model For Domain-generalized Change\n  Detection Visual Question Answering",
            "submittedOnDailyBy": {
                "_id": "67ff950b9e4824de182acf83",
                "avatarUrl": "/avatars/d8bae10afdd7457a08ec9554f79c7429.svg",
                "isPro": false,
                "fullname": "Elman Ghazaei",
                "user": "ElmanGhazaei",
                "type": "user"
            },
            "summary": "The Earth's surface is constantly changing, and detecting these changes\nprovides valuable insights that benefit various aspects of human society. While\ntraditional change detection methods have been employed to detect changes from\nbi-temporal images, these approaches typically require expert knowledge for\naccurate interpretation. To enable broader and more flexible access to change\ninformation by non-expert users, the task of Change Detection Visual Question\nAnswering (CDVQA) has been introduced. However, existing CDVQA methods have\nbeen developed under the assumption that training and testing datasets share\nsimilar distributions. This assumption does not hold in real-world\napplications, where domain shifts often occur. In this paper, the CDVQA task is\nrevisited with a focus on addressing domain shift. To this end, a new\nmulti-modal and multi-domain dataset, BrightVQA, is introduced to facilitate\ndomain generalization research in CDVQA. Furthermore, a novel state space\nmodel, termed Text-Conditioned State Space Model (TCSSM), is proposed. The\nTCSSM framework is designed to leverage both bi-temporal imagery and\ngeo-disaster-related textual information in an unified manner to extract\ndomain-invariant features across domains. Input-dependent parameters existing\nin TCSSM are dynamically predicted by using both bi-temporal images and\ngeo-disaster-related description, thereby facilitating the alignment between\nbi-temporal visual data and the associated textual descriptions. Extensive\nexperiments are conducted to evaluate the proposed method against\nstate-of-the-art models, and superior performance is consistently demonstrated.\nThe code and dataset will be made publicly available upon acceptance at\nhttps://github.com/Elman295/TCSSM.",
            "upvotes": 0,
            "discussionId": "689cead8b083e610d741e930",
            "ai_summary": "A novel state space model, TCSSM, is introduced to address domain shift in Change Detection Visual Question Answering (CDVQA) by leveraging bi-temporal imagery and textual information.",
            "ai_keywords": [
                "Change Detection Visual Question Answering",
                "CDVQA",
                "domain shift",
                "multi-modal",
                "multi-domain",
                "BrightVQA",
                "Text-Conditioned State Space Model",
                "TCSSM",
                "domain-invariant features",
                "bi-temporal imagery",
                "geo-disaster-related textual information"
            ]
        },
        "publishedAt": "2025-08-12T10:37:53.000Z",
        "title": "Text-conditioned State Space Model For Domain-generalized Change\n  Detection Visual Question Answering",
        "summary": "The Earth's surface is constantly changing, and detecting these changes\nprovides valuable insights that benefit various aspects of human society. While\ntraditional change detection methods have been employed to detect changes from\nbi-temporal images, these approaches typically require expert knowledge for\naccurate interpretation. To enable broader and more flexible access to change\ninformation by non-expert users, the task of Change Detection Visual Question\nAnswering (CDVQA) has been introduced. However, existing CDVQA methods have\nbeen developed under the assumption that training and testing datasets share\nsimilar distributions. This assumption does not hold in real-world\napplications, where domain shifts often occur. In this paper, the CDVQA task is\nrevisited with a focus on addressing domain shift. To this end, a new\nmulti-modal and multi-domain dataset, BrightVQA, is introduced to facilitate\ndomain generalization research in CDVQA. Furthermore, a novel state space\nmodel, termed Text-Conditioned State Space Model (TCSSM), is proposed. The\nTCSSM framework is designed to leverage both bi-temporal imagery and\ngeo-disaster-related textual information in an unified manner to extract\ndomain-invariant features across domains. Input-dependent parameters existing\nin TCSSM are dynamically predicted by using both bi-temporal images and\ngeo-disaster-related description, thereby facilitating the alignment between\nbi-temporal visual data and the associated textual descriptions. Extensive\nexperiments are conducted to evaluate the proposed method against\nstate-of-the-art models, and superior performance is consistently demonstrated.\nThe code and dataset will be made publicly available upon acceptance at\nhttps://github.com/Elman295/TCSSM.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08974.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ff950b9e4824de182acf83",
            "avatarUrl": "/avatars/d8bae10afdd7457a08ec9554f79c7429.svg",
            "fullname": "Elman Ghazaei",
            "name": "ElmanGhazaei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.05769",
            "authors": [
                {
                    "_id": "689b6da9fab6fdd2e52ac747",
                    "name": "Seyed Hadi Seyed",
                    "hidden": false
                },
                {
                    "_id": "689b6da9fab6fdd2e52ac748",
                    "name": "Ayberk Cansever",
                    "hidden": false
                },
                {
                    "_id": "689b6da9fab6fdd2e52ac749",
                    "user": {
                        "_id": "664f7d4829503f958d04180a",
                        "avatarUrl": "/avatars/c4ba9007b1e952f1e1fc906ef0acc3ba.svg",
                        "isPro": false,
                        "fullname": "David Hart",
                        "user": "incrl",
                        "type": "user"
                    },
                    "name": "David Hart",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:14:01.847Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-07T18:35:44.000Z",
            "submittedOnDailyAt": "2025-08-13T12:24:47.241Z",
            "title": "Improving Masked Style Transfer using Blended Partial Convolution",
            "submittedOnDailyBy": {
                "_id": "664f7d4829503f958d04180a",
                "avatarUrl": "/avatars/c4ba9007b1e952f1e1fc906ef0acc3ba.svg",
                "isPro": false,
                "fullname": "David Hart",
                "user": "incrl",
                "type": "user"
            },
            "summary": "Artistic style transfer has long been possible with the advancements of\nconvolution- and transformer-based neural networks. Most algorithms apply the\nartistic style transfer to the whole image, but individual users may only need\nto apply a style transfer to a specific region in the image. The standard\npractice is to simply mask the image after the stylization. This work shows\nthat this approach tends to improperly capture the style features in the region\nof interest. We propose a partial-convolution-based style transfer network that\naccurately applies the style features exclusively to the region of interest.\nAdditionally, we present network-internal blending techniques that account for\nimperfections in the region selection. We show that this visually and\nquantitatively improves stylization using examples from the SA-1B dataset. Code\nis publicly available at https://github.com/davidmhart/StyleTransferMasked.",
            "upvotes": 0,
            "discussionId": "689b6daafab6fdd2e52ac74a",
            "ai_summary": "A partial-convolution-based style transfer network improves stylization of specific image regions by accurately applying style features and using network-internal blending techniques.",
            "ai_keywords": [
                "convolution-based neural networks",
                "transformer-based neural networks",
                "partial-convolution-based style transfer network",
                "network-internal blending techniques",
                "SA-1B dataset"
            ]
        },
        "publishedAt": "2025-08-07T14:35:44.000Z",
        "title": "Improving Masked Style Transfer using Blended Partial Convolution",
        "summary": "Artistic style transfer has long been possible with the advancements of\nconvolution- and transformer-based neural networks. Most algorithms apply the\nartistic style transfer to the whole image, but individual users may only need\nto apply a style transfer to a specific region in the image. The standard\npractice is to simply mask the image after the stylization. This work shows\nthat this approach tends to improperly capture the style features in the region\nof interest. We propose a partial-convolution-based style transfer network that\naccurately applies the style features exclusively to the region of interest.\nAdditionally, we present network-internal blending techniques that account for\nimperfections in the region selection. We show that this visually and\nquantitatively improves stylization using examples from the SA-1B dataset. Code\nis publicly available at https://github.com/davidmhart/StyleTransferMasked.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05769.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "664f7d4829503f958d04180a",
            "avatarUrl": "/avatars/c4ba9007b1e952f1e1fc906ef0acc3ba.svg",
            "fullname": "David Hart",
            "name": "incrl",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04195",
            "authors": [
                {
                    "_id": "689c33fbfab6fdd2e52ac8ec",
                    "name": "Huan Liao",
                    "hidden": false
                },
                {
                    "_id": "689c33fbfab6fdd2e52ac8ed",
                    "name": "Qinke Ni",
                    "hidden": false
                },
                {
                    "_id": "689c33fbfab6fdd2e52ac8ee",
                    "name": "Yuancheng Wang",
                    "hidden": false
                },
                {
                    "_id": "689c33fbfab6fdd2e52ac8ef",
                    "name": "Yiheng Lu",
                    "hidden": false
                },
                {
                    "_id": "689c33fbfab6fdd2e52ac8f0",
                    "name": "Haoyue Zhan",
                    "hidden": false
                },
                {
                    "_id": "689c33fbfab6fdd2e52ac8f1",
                    "name": "Pengyuan Xie",
                    "hidden": false
                },
                {
                    "_id": "689c33fbfab6fdd2e52ac8f2",
                    "name": "Qiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "689c33fbfab6fdd2e52ac8f3",
                    "name": "Zhizheng Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T08:25:26.000Z",
            "submittedOnDailyAt": "2025-08-13T05:14:31.171Z",
            "title": "NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech\n  Modeling with Paralinguistic Vocalizations",
            "submittedOnDailyBy": {
                "_id": "63072d60cd148dbc5e49f4dd",
                "avatarUrl": "/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg",
                "isPro": false,
                "fullname": "Yuancheng Wang",
                "user": "Hecheng0625",
                "type": "user"
            },
            "summary": "Paralinguistic vocalizations-including non-verbal sounds like laughter and\nbreathing, as well as lexicalized interjections such as \"uhm\" and \"oh\"-are\nintegral to natural spoken communication. Despite their importance in conveying\naffect, intent, and interactional cues, such cues remain largely overlooked in\nconventional automatic speech recognition (ASR) and text-to-speech (TTS)\nsystems. We present NVSpeech, an integrated and scalable pipeline that bridges\nthe recognition and synthesis of paralinguistic vocalizations, encompassing\ndataset construction, ASR modeling, and controllable TTS. (1) We introduce a\nmanually annotated dataset of 48,430 human-spoken utterances with 18 word-level\nparalinguistic categories. (2) We develop the paralinguistic-aware ASR model,\nwhich treats paralinguistic cues as inline decodable tokens (e.g., \"You're so\nfunny [Laughter]\"), enabling joint lexical and non-verbal transcription. This\nmodel is then used to automatically annotate a large corpus, the first\nlarge-scale Chinese dataset of 174,179 utterances (573 hours) with word-level\nalignment and paralingustic cues. (3) We finetune zero-shot TTS models on both\nhuman- and auto-labeled data to enable explicit control over paralinguistic\nvocalizations, allowing context-aware insertion at arbitrary token positions\nfor human-like speech synthesis. By unifying the recognition and generation of\nparalinguistic vocalizations, NVSpeech offers the first open, large-scale,\nword-level annotated pipeline for expressive speech modeling in Mandarin,\nintegrating recognition and synthesis in a scalable and controllable manner.\nDataset and audio demos are available at https://nvspeech170k.github.io/.",
            "upvotes": 0,
            "discussionId": "689c33fcfab6fdd2e52ac8f4",
            "ai_summary": "NVSpeech is a pipeline that integrates the recognition and synthesis of paralinguistic vocalizations in Mandarin, using a large annotated dataset and models that treat these cues as decodable tokens.",
            "ai_keywords": [
                "paralinguistic vocalizations",
                "ASR modeling",
                "TTS",
                "paralinguistic-aware ASR",
                "zero-shot TTS",
                "word-level paralinguistic categories",
                "paralinguistic cues",
                "lexical and non-verbal transcription",
                "human-like speech synthesis",
                "expressive speech modeling"
            ]
        },
        "publishedAt": "2025-08-06T04:25:26.000Z",
        "title": "NVSpeech: An Integrated and Scalable Pipeline for Human-Like Speech\n  Modeling with Paralinguistic Vocalizations",
        "summary": "Paralinguistic vocalizations-including non-verbal sounds like laughter and\nbreathing, as well as lexicalized interjections such as \"uhm\" and \"oh\"-are\nintegral to natural spoken communication. Despite their importance in conveying\naffect, intent, and interactional cues, such cues remain largely overlooked in\nconventional automatic speech recognition (ASR) and text-to-speech (TTS)\nsystems. We present NVSpeech, an integrated and scalable pipeline that bridges\nthe recognition and synthesis of paralinguistic vocalizations, encompassing\ndataset construction, ASR modeling, and controllable TTS. (1) We introduce a\nmanually annotated dataset of 48,430 human-spoken utterances with 18 word-level\nparalinguistic categories. (2) We develop the paralinguistic-aware ASR model,\nwhich treats paralinguistic cues as inline decodable tokens (e.g., \"You're so\nfunny [Laughter]\"), enabling joint lexical and non-verbal transcription. This\nmodel is then used to automatically annotate a large corpus, the first\nlarge-scale Chinese dataset of 174,179 utterances (573 hours) with word-level\nalignment and paralingustic cues. (3) We finetune zero-shot TTS models on both\nhuman- and auto-labeled data to enable explicit control over paralinguistic\nvocalizations, allowing context-aware insertion at arbitrary token positions\nfor human-like speech synthesis. By unifying the recognition and generation of\nparalinguistic vocalizations, NVSpeech offers the first open, large-scale,\nword-level annotated pipeline for expressive speech modeling in Mandarin,\nintegrating recognition and synthesis in a scalable and controllable manner.\nDataset and audio demos are available at https://nvspeech170k.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04195.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63072d60cd148dbc5e49f4dd",
            "avatarUrl": "/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg",
            "fullname": "Yuancheng Wang",
            "name": "Hecheng0625",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    }
]