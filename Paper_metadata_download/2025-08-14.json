[
    {
        "paper": {
            "id": "2508.09983",
            "authors": [
                {
                    "_id": "689d7a76b083e610d741ea88",
                    "user": {
                        "_id": "67c46a82a8ec9d71bf3df5bd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Wl5TdJCHm6bTsMeACA0x3.png",
                        "isPro": false,
                        "fullname": "David D",
                        "user": "daviddink",
                        "type": "user"
                    },
                    "name": "David Dinkevich",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:16.494Z",
                    "hidden": false
                },
                {
                    "_id": "689d7a76b083e610d741ea89",
                    "name": "Matan Levy",
                    "hidden": false
                },
                {
                    "_id": "689d7a76b083e610d741ea8a",
                    "user": {
                        "_id": "62f6ab4fc3372328414c8689",
                        "avatarUrl": "/avatars/4f728a5b70c9fe4a64e80e2b643ca620.svg",
                        "isPro": false,
                        "fullname": "Omri Avrahami",
                        "user": "omriav",
                        "type": "user"
                    },
                    "name": "Omri Avrahami",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:18.821Z",
                    "hidden": false
                },
                {
                    "_id": "689d7a76b083e610d741ea8b",
                    "name": "Dvir Samuel",
                    "hidden": false
                },
                {
                    "_id": "689d7a76b083e610d741ea8c",
                    "name": "Dani Lischinski",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62f6ab4fc3372328414c8689/PmghfKC-KB0_hrcZIsBwo.webp"
            ],
            "publishedAt": "2025-08-13T17:56:26.000Z",
            "submittedOnDailyAt": "2025-08-14T04:28:00.139Z",
            "title": "Story2Board: A Training-Free Approach for Expressive Storyboard\n  Generation",
            "submittedOnDailyBy": {
                "_id": "62f6ab4fc3372328414c8689",
                "avatarUrl": "/avatars/4f728a5b70c9fe4a64e80e2b643ca620.svg",
                "isPro": false,
                "fullname": "Omri Avrahami",
                "user": "omriav",
                "type": "user"
            },
            "summary": "We present Story2Board, a training-free framework for expressive storyboard\ngeneration from natural language. Existing methods narrowly focus on subject\nidentity, overlooking key aspects of visual storytelling such as spatial\ncomposition, background evolution, and narrative pacing. To address this, we\nintroduce a lightweight consistency framework composed of two components:\nLatent Panel Anchoring, which preserves a shared character reference across\npanels, and Reciprocal Attention Value Mixing, which softly blends visual\nfeatures between token pairs with strong reciprocal attention. Together, these\nmechanisms enhance coherence without architectural changes or fine-tuning,\nenabling state-of-the-art diffusion models to generate visually diverse yet\nconsistent storyboards. To structure generation, we use an off-the-shelf\nlanguage model to convert free-form stories into grounded panel-level prompts.\nTo evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain\nnarratives designed to assess layout diversity and background-grounded\nstorytelling, in addition to consistency. We also introduce a new Scene\nDiversity metric that quantifies spatial and pose variation across storyboards.\nOur qualitative and quantitative results, as well as a user study, show that\nStory2Board produces more dynamic, coherent, and narratively engaging\nstoryboards than existing baselines.",
            "upvotes": 41,
            "discussionId": "689d7a76b083e610d741ea8d",
            "projectPage": "https://daviddinkevich.github.io/Story2Board/",
            "githubRepo": "https://github.com/daviddinkevich/Story2Board",
            "ai_summary": "Story2Board generates expressive storyboards from natural language using a consistency framework that enhances coherence and diversity without fine-tuning.",
            "ai_keywords": [
                "Latent Panel Anchoring",
                "Reciprocal Attention Value Mixing",
                "diffusion models",
                "Rich Storyboard Benchmark",
                "Scene Diversity metric"
            ],
            "githubStars": 15
        },
        "publishedAt": "2025-08-13T13:56:26.000Z",
        "title": "Story2Board: A Training-Free Approach for Expressive Storyboard\n  Generation",
        "summary": "We present Story2Board, a training-free framework for expressive storyboard\ngeneration from natural language. Existing methods narrowly focus on subject\nidentity, overlooking key aspects of visual storytelling such as spatial\ncomposition, background evolution, and narrative pacing. To address this, we\nintroduce a lightweight consistency framework composed of two components:\nLatent Panel Anchoring, which preserves a shared character reference across\npanels, and Reciprocal Attention Value Mixing, which softly blends visual\nfeatures between token pairs with strong reciprocal attention. Together, these\nmechanisms enhance coherence without architectural changes or fine-tuning,\nenabling state-of-the-art diffusion models to generate visually diverse yet\nconsistent storyboards. To structure generation, we use an off-the-shelf\nlanguage model to convert free-form stories into grounded panel-level prompts.\nTo evaluate, we propose the Rich Storyboard Benchmark, a suite of open-domain\nnarratives designed to assess layout diversity and background-grounded\nstorytelling, in addition to consistency. We also introduce a new Scene\nDiversity metric that quantifies spatial and pose variation across storyboards.\nOur qualitative and quantitative results, as well as a user study, show that\nStory2Board produces more dynamic, coherent, and narratively engaging\nstoryboards than existing baselines.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62f6ab4fc3372328414c8689/PmghfKC-KB0_hrcZIsBwo.webp"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09983.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f6ab4fc3372328414c8689",
            "avatarUrl": "/avatars/4f728a5b70c9fe4a64e80e2b643ca620.svg",
            "fullname": "Omri Avrahami",
            "name": "omriav",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.08401",
            "authors": [
                {
                    "_id": "689d51fdb083e610d741e9ef",
                    "user": {
                        "_id": "6438e55cb2ea24b52ebc45ec",
                        "avatarUrl": "/avatars/e13c7398f77e7e0bd5eed03102aa5c36.svg",
                        "isPro": false,
                        "fullname": "Jiatong LI",
                        "user": "phenixace",
                        "type": "user"
                    },
                    "name": "Jiatong Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:39.481Z",
                    "hidden": false
                },
                {
                    "_id": "689d51fdb083e610d741e9f0",
                    "user": {
                        "_id": "661b9d96c153e4a0a25adc3e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
                        "isPro": false,
                        "fullname": "Weida Wang",
                        "user": "weidawang",
                        "type": "user"
                    },
                    "name": "Weida Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:37.341Z",
                    "hidden": false
                },
                {
                    "_id": "689d51fdb083e610d741e9f1",
                    "name": "Qinggang Zhang",
                    "hidden": false
                },
                {
                    "_id": "689d51fdb083e610d741e9f2",
                    "user": {
                        "_id": "656ae4088fb1ddf0d5ec9ac5",
                        "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
                        "isPro": false,
                        "fullname": "Junxian Li",
                        "user": "Duke-de-Artois",
                        "type": "user"
                    },
                    "name": "Junxian Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:33.168Z",
                    "hidden": false
                },
                {
                    "_id": "689d51fdb083e610d741e9f3",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "di-zhang-fdu",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:35.471Z",
                    "hidden": false
                },
                {
                    "_id": "689d51fdb083e610d741e9f4",
                    "name": "Changmeng Zheng",
                    "hidden": false
                },
                {
                    "_id": "689d51fdb083e610d741e9f5",
                    "name": "Shufei Zhang",
                    "hidden": false
                },
                {
                    "_id": "689d51fdb083e610d741e9f6",
                    "name": "Xiaoyong Wei",
                    "hidden": false
                },
                {
                    "_id": "689d51fdb083e610d741e9f7",
                    "name": "Qing Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T18:50:05.000Z",
            "submittedOnDailyAt": "2025-08-14T01:40:24.947Z",
            "title": "Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery",
            "submittedOnDailyBy": {
                "_id": "661b9d96c153e4a0a25adc3e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
                "isPro": false,
                "fullname": "Weida Wang",
                "user": "weidawang",
                "type": "user"
            },
            "summary": "Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT)\nreasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning\ncapabilities, achieving impressive performance in commonsense reasoning and\nmathematical inference. Despite their effectiveness, Long-CoT reasoning models\nare often criticized for their limited ability and low efficiency in\nknowledge-intensive domains such as molecule discovery. Success in this field\nrequires a precise understanding of domain knowledge, including molecular\nstructures and chemical principles, which is challenging due to the inherent\ncomplexity of molecular data and the scarcity of high-quality expert\nannotations. To bridge this gap, we introduce Mol-R1, a novel framework\ndesigned to improve explainability and reasoning performance of R1-like\nExplicit Long-CoT reasoning LLMs in text-based molecule generation. Our\napproach begins with a high-quality reasoning dataset curated through Prior\nRegulation via In-context Distillation (PRID), a dedicated distillation\nstrategy to effectively generate paired reasoning traces guided by prior\nregulations. Building upon this, we introduce MoIA, Molecular Iterative\nAdaptation, a sophisticated training strategy that iteratively combines\nSupervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO),\ntailored to boost the reasoning performance of R1-like reasoning models for\nmolecule discovery. Finally, we examine the performance of Mol-R1 in the\ntext-based molecule reasoning generation task, showing superior performance\nagainst existing baselines.",
            "upvotes": 29,
            "discussionId": "689d51feb083e610d741e9f8",
            "ai_summary": "Mol-R1 framework enhances molecule discovery by improving reasoning performance and explainability through PRID and MoIA strategies.",
            "ai_keywords": [
                "Large language models",
                "Explicit Long Chain-of-Thought",
                "DeepSeek-R1",
                "QWQ",
                "commonsense reasoning",
                "mathematical inference",
                "knowledge-intensive domains",
                "molecule discovery",
                "molecular structures",
                "chemical principles",
                "high-quality reasoning dataset",
                "Prior Regulation via In-context Distillation",
                "PRID",
                "MoIA",
                "Molecular Iterative Adaptation",
                "Supervised Fine-tuning",
                "SFT",
                "Reinforced Policy Optimization",
                "RPO",
                "text-based molecule generation"
            ]
        },
        "publishedAt": "2025-08-11T14:50:05.000Z",
        "title": "Mol-R1: Towards Explicit Long-CoT Reasoning in Molecule Discovery",
        "summary": "Large language models (LLMs), especially Explicit Long Chain-of-Thought (CoT)\nreasoning models like DeepSeek-R1 and QWQ, have demonstrated powerful reasoning\ncapabilities, achieving impressive performance in commonsense reasoning and\nmathematical inference. Despite their effectiveness, Long-CoT reasoning models\nare often criticized for their limited ability and low efficiency in\nknowledge-intensive domains such as molecule discovery. Success in this field\nrequires a precise understanding of domain knowledge, including molecular\nstructures and chemical principles, which is challenging due to the inherent\ncomplexity of molecular data and the scarcity of high-quality expert\nannotations. To bridge this gap, we introduce Mol-R1, a novel framework\ndesigned to improve explainability and reasoning performance of R1-like\nExplicit Long-CoT reasoning LLMs in text-based molecule generation. Our\napproach begins with a high-quality reasoning dataset curated through Prior\nRegulation via In-context Distillation (PRID), a dedicated distillation\nstrategy to effectively generate paired reasoning traces guided by prior\nregulations. Building upon this, we introduce MoIA, Molecular Iterative\nAdaptation, a sophisticated training strategy that iteratively combines\nSupervised Fine-tuning (SFT) with Reinforced Policy Optimization (RPO),\ntailored to boost the reasoning performance of R1-like reasoning models for\nmolecule discovery. Finally, we examine the performance of Mol-R1 in the\ntext-based molecule reasoning generation task, showing superior performance\nagainst existing baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08401.png",
        "numComments": 8,
        "submittedBy": {
            "_id": "661b9d96c153e4a0a25adc3e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/661b9d96c153e4a0a25adc3e/VRt7kCQ0KdJp-lhPLOajO.jpeg",
            "fullname": "Weida Wang",
            "name": "weidawang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 0
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.07901",
            "authors": [
                {
                    "_id": "689aaeabfab6fdd2e52ac47c",
                    "user": {
                        "_id": "6899c1c4c8d32e4cdea87215",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8plwumYYJa1TNkZvF1xoj.jpeg",
                        "isPro": false,
                        "fullname": "Bowen Xue",
                        "user": "BowenXue",
                        "type": "user"
                    },
                    "name": "Bowen Xue",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:39:40.376Z",
                    "hidden": false
                },
                {
                    "_id": "689aaeabfab6fdd2e52ac47d",
                    "name": "Qixin Yan",
                    "hidden": false
                },
                {
                    "_id": "689aaeabfab6fdd2e52ac47e",
                    "name": "Wenjing Wang",
                    "hidden": false
                },
                {
                    "_id": "689aaeabfab6fdd2e52ac47f",
                    "name": "Hao Liu",
                    "hidden": false
                },
                {
                    "_id": "689aaeabfab6fdd2e52ac480",
                    "name": "Chen Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T12:17:38.000Z",
            "submittedOnDailyAt": "2025-08-14T06:10:24.567Z",
            "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6683a05e74fb1736a4b7c934",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6683a05e74fb1736a4b7c934/eiz6qlqIUjAWGy5zfg8Cs.jpeg",
                "isPro": false,
                "fullname": "QRQ",
                "user": "RichardQRQ",
                "type": "user"
            },
            "summary": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just sim1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.",
            "upvotes": 27,
            "discussionId": "689aaeabfab6fdd2e52ac481",
            "projectPage": "https://stand-in-video.github.io/",
            "githubRepo": "https://github.com/WeChatCV/Stand-In",
            "ai_summary": "A lightweight framework for identity preservation in video generation using conditional image branches and restricted self-attentions outperforms full-parameter methods with minimal additional parameters.",
            "ai_keywords": [
                "conditional image branch",
                "pre-trained video generation model",
                "restricted self-attentions",
                "conditional position mapping",
                "subject-driven video generation",
                "pose-referenced video generation",
                "stylization",
                "face swapping"
            ],
            "githubStars": 181
        },
        "publishedAt": "2025-08-11T08:17:38.000Z",
        "title": "Stand-In: A Lightweight and Plug-and-Play Identity Control for Video\n  Generation",
        "summary": "Generating high-fidelity human videos that match user-specified identities is\nimportant yet challenging in the field of generative AI. Existing methods often\nrely on an excessive number of training parameters and lack compatibility with\nother AIGC tools. In this paper, we propose Stand-In, a lightweight and\nplug-and-play framework for identity preservation in video generation.\nSpecifically, we introduce a conditional image branch into the pre-trained\nvideo generation model. Identity control is achieved through restricted\nself-attentions with conditional position mapping, and can be learned quickly\nwith only 2000 pairs. Despite incorporating and training just sim1\\%\nadditional parameters, our framework achieves excellent results in video\nquality and identity preservation, outperforming other full-parameter training\nmethods. Moreover, our framework can be seamlessly integrated for other tasks,\nsuch as subject-driven video generation, pose-referenced video generation,\nstylization, and face swapping.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07901.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6683a05e74fb1736a4b7c934",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6683a05e74fb1736a4b7c934/eiz6qlqIUjAWGy5zfg8Cs.jpeg",
            "fullname": "QRQ",
            "name": "RichardQRQ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09889",
            "authors": [
                {
                    "_id": "689d4a16b083e610d741e9c1",
                    "name": "Zhitian Xie",
                    "hidden": false
                },
                {
                    "_id": "689d4a16b083e610d741e9c2",
                    "name": "Qintong Wu",
                    "hidden": false
                },
                {
                    "_id": "689d4a16b083e610d741e9c3",
                    "name": "Chengyue Yu",
                    "hidden": false
                },
                {
                    "_id": "689d4a16b083e610d741e9c4",
                    "name": "Chenyi Zhuang",
                    "hidden": false
                },
                {
                    "_id": "689d4a16b083e610d741e9c5",
                    "name": "Jinjie Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T15:46:25.000Z",
            "submittedOnDailyAt": "2025-08-14T01:13:03.061Z",
            "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving",
            "submittedOnDailyBy": {
                "_id": "64e847ab5ddcace745b8f5b1",
                "avatarUrl": "/avatars/89525e66ff2900f86f66a11043a298f9.svg",
                "isPro": true,
                "fullname": "chenyi zhuang",
                "user": "chengle",
                "type": "user"
            },
            "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems.",
            "upvotes": 22,
            "discussionId": "689d4a17b083e610d741e9c6",
            "githubRepo": "https://github.com/inclusionAI/AWorld",
            "ai_summary": "A dynamic Multi-Agent System (MAS) with Execution and Guard Agents improves the reliability and effectiveness of intelligent agents using external tools, outperforming single-agent systems on the GAIA leaderboard.",
            "ai_keywords": [
                "large language models",
                "Multi-Agent System",
                "Execution Agent",
                "Guard Agent",
                "dynamic supervision",
                "maneuvering mechanisms",
                "GAIA test dataset",
                "single-agent system",
                "tool-augmented systems",
                "GAIA leaderboard"
            ],
            "githubStars": 561
        },
        "publishedAt": "2025-08-13T11:46:25.000Z",
        "title": "AWorld: Dynamic Multi-Agent System with Stable Maneuvering for Robust\n  GAIA Problem Solving",
        "summary": "The rapid advancement of large language models (LLMs) has empowered\nintelligent agents to leverage diverse external tools for solving complex\nreal-world problems. However, as agents increasingly depend on multiple tools,\nthey encounter new challenges: extended contexts from disparate sources and\nnoisy or irrelevant tool outputs can undermine system reliability and accuracy.\nThese challenges underscore the necessity for enhanced stability in agent-based\nsystems. To address this, we introduce dynamic supervision and maneuvering\nmechanisms, constructing a robust and dynamic Multi-Agent System (MAS)\narchitecture within the AWorld framework. In our approach, the Execution Agent\ninvokes the Guard Agent at critical steps to verify and correct the reasoning\nprocess, effectively reducing errors arising from noise and bolstering\nproblem-solving robustness. Extensive experiments on the GAIA test dataset\nreveal that our dynamic maneuvering mechanism significantly improves both the\neffectiveness and stability of solutions, outperforming single-agent system\n(SAS) and standard tool-augmented systems. As a result, our dynamic MAS system\nachieved first place among open-source projects on the prestigious GAIA\nleaderboard. These findings highlight the practical value of collaborative\nagent roles in developing more reliable and trustworthy intelligent systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09889.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e847ab5ddcace745b8f5b1",
            "avatarUrl": "/avatars/89525e66ff2900f86f66a11043a298f9.svg",
            "fullname": "chenyi zhuang",
            "name": "chengle",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09192",
            "authors": [
                {
                    "_id": "689d3293b083e610d741e992",
                    "name": "Xu Wang",
                    "hidden": false
                },
                {
                    "_id": "689d3293b083e610d741e993",
                    "name": "Chenkai Xu",
                    "hidden": false
                },
                {
                    "_id": "689d3293b083e610d741e994",
                    "user": {
                        "_id": "66d05337e62d6bbf50186c2f",
                        "avatarUrl": "/avatars/f5be15e754f0fbbb37d2cc5ea417f729.svg",
                        "isPro": false,
                        "fullname": "Yijie Jin",
                        "user": "DrewJin0827",
                        "type": "user"
                    },
                    "name": "Yijie Jin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:50.550Z",
                    "hidden": false
                },
                {
                    "_id": "689d3293b083e610d741e995",
                    "name": "Jiachun Jin",
                    "hidden": false
                },
                {
                    "_id": "689d3293b083e610d741e996",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "689d3293b083e610d741e997",
                    "name": "Zhijie Deng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-08T04:51:37.000Z",
            "submittedOnDailyAt": "2025-08-14T02:37:01.874Z",
            "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
            "submittedOnDailyBy": {
                "_id": "65708920806dee337da0eef5",
                "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
                "isPro": false,
                "fullname": "xuchenkai",
                "user": "UnhurriedDawn",
                "type": "user"
            },
            "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than 2.5times inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than 50times while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
            "upvotes": 22,
            "discussionId": "689d3294b083e610d741e998",
            "projectPage": "https://zhijie-group.github.io/Discrete-Diffusion-Forcing/",
            "githubRepo": "https://github.com/zhijie-group/Discrete-Diffusion-Forcing",
            "ai_summary": "Discrete diffusion forcing enhances diffusion large language models with block-wise autoregressive generation and inter-block parallel decoding, significantly improving inference speed while maintaining quality.",
            "ai_keywords": [
                "diffusion large language models",
                "autoregressive LLMs",
                "discrete diffusion forcing",
                "block-wise autoregressive generation",
                "KV cache",
                "inter-block parallel decoding",
                "asymmetric distillation",
                "pipelined parallel decoding",
                "GSM8K",
                "LLaMA3",
                "Qwen2.5",
                "LLaDA",
                "Dream"
            ],
            "githubStars": 39
        },
        "publishedAt": "2025-08-08T00:51:37.000Z",
        "title": "Diffusion LLMs Can Do Faster-Than-AR Inference via Discrete Diffusion\n  Forcing",
        "summary": "Diffusion Large Language Models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs for text generation, with the potential\nto decode multiple tokens in a single iteration. However, none of the existing\nopen-source dLLMs have achieved superior inference speed over AR LLMs of\nsimilar size. This paper breaks this barrier based on a simple and effective\nstrategy named discrete diffusion forcing (D2F). D2F equips dLLMs with two key\ncapabilities: (1) block-wise autoregressive generation to enable KV cache\nutilization; (2) prediction of following tokens without requiring completion of\nprior blocks for inter-block parallel decoding. In this way, the vanilla dLLMs\nare refurbished into an AR-diffusion hybrid paradigm for efficient inference.\nD2F can be implemented with an asymmetric distillation process based on\npre-trained dLLMs. We further propose a pipelined parallel decoding algorithm,\nwhich enables a trade-off between efficiency and efficacy. Empirically, D2F\ndLLMs achieve more than 2.5times inference speed than LLaMA3 and\nQwen2.5 on GSM8K. Compared to vanilla dLLMs like LLaDA and Dream, the\nacceleration can be more than 50times while maintaining comparable\noutput quality. The code is available at\nhttps://github.com/zhijie-group/Discrete-Diffusion-Forcing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09192.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "65708920806dee337da0eef5",
            "avatarUrl": "/avatars/945e328dedc8e1e3111f48c344ad5b03.svg",
            "fullname": "xuchenkai",
            "name": "UnhurriedDawn",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09736",
            "authors": [
                {
                    "_id": "689d8af7b083e610d741eaaf",
                    "name": "Lin Long",
                    "hidden": false
                },
                {
                    "_id": "689d8af7b083e610d741eab0",
                    "name": "Yichen He",
                    "hidden": false
                },
                {
                    "_id": "689d8af7b083e610d741eab1",
                    "name": "Wentao Ye",
                    "hidden": false
                },
                {
                    "_id": "689d8af7b083e610d741eab2",
                    "name": "Yiyuan Pan",
                    "hidden": false
                },
                {
                    "_id": "689d8af7b083e610d741eab3",
                    "name": "Yuan Lin",
                    "hidden": false
                },
                {
                    "_id": "689d8af7b083e610d741eab4",
                    "name": "Hang Li",
                    "hidden": false
                },
                {
                    "_id": "689d8af7b083e610d741eab5",
                    "name": "Junbo Zhao",
                    "hidden": false
                },
                {
                    "_id": "689d8af7b083e610d741eab6",
                    "name": "Wei Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T12:03:03.000Z",
            "submittedOnDailyAt": "2025-08-14T05:38:40.513Z",
            "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory",
            "submittedOnDailyBy": {
                "_id": "60ea81771cc8dc259c58e905",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ea81771cc8dc259c58e905/kmGlaNvdS4EEHc_5qongT.jpeg",
                "isPro": false,
                "fullname": "yichen he",
                "user": "hyc2026",
                "type": "user"
            },
            "summary": "We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent",
            "upvotes": 21,
            "discussionId": "689d8af7b083e610d741eab7",
            "projectPage": "https://m3-agent.github.io/",
            "githubRepo": "https://github.com/ByteDance-Seed/m3-agent",
            "ai_summary": "M3-Agent, a multimodal agent with long-term memory, performs multi-turn reasoning and outperforms baselines on a new long-video question answering benchmark.",
            "ai_keywords": [
                "multimodal agent",
                "long-term memory",
                "episodic memory",
                "semantic memory",
                "real-time visual inputs",
                "real-time auditory inputs",
                "multi-turn reasoning",
                "iterative reasoning",
                "M3-Bench",
                "long-video question answering benchmark",
                "reinforcement learning",
                "Gemini-1.5-pro",
                "GPT-4o",
                "human understanding",
                "general knowledge extraction",
                "cross-modal reasoning"
            ],
            "githubStars": 64
        },
        "publishedAt": "2025-08-13T08:03:03.000Z",
        "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with\n  Long-Term Memory",
        "summary": "We introduce M3-Agent, a novel multimodal agent framework equipped with\nlong-term memory. Like humans, M3-Agent can process real-time visual and\nauditory inputs to build and update its long-term memory. Beyond episodic\nmemory, it also develops semantic memory, enabling it to accumulate world\nknowledge over time. Its memory is organized in an entity-centric, multimodal\nformat, allowing deeper and more consistent understanding of the environment.\nGiven an instruction, M3-Agent autonomously performs multi-turn, iterative\nreasoning and retrieves relevant information from memory to accomplish the\ntask. To evaluate memory effectiveness and memory-based reasoning in multimodal\nagents, we develop M3-Bench, a new long-video question answering benchmark.\nM3-Bench comprises 100 newly recorded real-world videos captured from a robot's\nperspective (M3-Bench-robot) and 929 web-sourced videos across diverse\nscenarios (M3-Bench-web). We annotate question-answer pairs designed to test\nkey capabilities essential for agent applications, such as human understanding,\ngeneral knowledge extraction, and cross-modal reasoning. Experimental results\nshow that M3-Agent, trained via reinforcement learning, outperforms the\nstrongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o,\nachieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web\nand VideoMME-long, respectively. Our work advances the multimodal agents toward\nmore human-like long-term memory and provides insights into their practical\ndesign. Model, code and data are available at\nhttps://github.com/bytedance-seed/m3-agent",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09736.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "60ea81771cc8dc259c58e905",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60ea81771cc8dc259c58e905/kmGlaNvdS4EEHc_5qongT.jpeg",
            "fullname": "yichen he",
            "name": "hyc2026",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09987",
            "authors": [
                {
                    "_id": "689d6033b083e610d741ea4c",
                    "name": "Junyan Ye",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea4d",
                    "user": {
                        "_id": "6349214f8146350b3a4c5cdf",
                        "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
                        "isPro": false,
                        "fullname": "Dongzhi Jiang",
                        "user": "CaraJ",
                        "type": "user"
                    },
                    "name": "Dongzhi Jiang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:21.218Z",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea4e",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea4f",
                    "name": "Leqi Zhu",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea50",
                    "name": "Zhenghao Hu",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea51",
                    "user": {
                        "_id": "6487e158f675b4a7867f45fa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6487e158f675b4a7867f45fa/J0sls6zZ682o-SH7iQs7B.jpeg",
                        "isPro": false,
                        "fullname": "Zilong Huang",
                        "user": "SereinH",
                        "type": "user"
                    },
                    "name": "Zilong Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:23.470Z",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea52",
                    "name": "Jun He",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea53",
                    "name": "Zhiyuan Yan",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea54",
                    "name": "Jinghua Yu",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea55",
                    "name": "Hongsheng Li",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea56",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "689d6033b083e610d741ea57",
                    "name": "Weijia Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T17:59:28.000Z",
            "submittedOnDailyAt": "2025-08-14T02:38:09.339Z",
            "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved\n  Image Generation",
            "submittedOnDailyBy": {
                "_id": "6349214f8146350b3a4c5cdf",
                "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
                "isPro": false,
                "fullname": "Dongzhi Jiang",
                "user": "CaraJ",
                "type": "user"
            },
            "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.",
            "upvotes": 16,
            "discussionId": "689d6033b083e610d741ea58",
            "projectPage": "https://yejy53.github.io/Echo-4o/",
            "githubRepo": "https://github.com/yejy53/Echo-4o",
            "ai_summary": "Echo-4o-Image, a synthetic dataset generated by GPT-4o, enhances image generation models by addressing rare scenarios and providing clean supervision, leading to improved performance and transferability.",
            "ai_keywords": [
                "GPT-4o",
                "synthetic dataset",
                "rare scenarios",
                "surreal fantasy",
                "multi-reference image generation",
                "clean supervision",
                "complex background noise",
                "text-to-image alignment",
                "unified multimodal generation",
                "Bagel",
                "Echo-4o",
                "GenEval++",
                "Imagine-Bench",
                "OmniGen2",
                "BLIP3-o"
            ],
            "githubStars": 40
        },
        "publishedAt": "2025-08-13T13:59:28.000Z",
        "title": "Echo-4o: Harnessing the Power of GPT-4o Synthetic Images for Improved\n  Image Generation",
        "summary": "Recently, GPT-4o has garnered significant attention for its strong\nperformance in image generation, yet open-source models still lag behind.\nSeveral studies have explored distilling image data from GPT-4o to enhance\nopen-source models, achieving notable progress. However, a key question\nremains: given that real-world image datasets already constitute a natural\nsource of high-quality data, why should we use GPT-4o-generated synthetic data?\nIn this work, we identify two key advantages of synthetic images. First, they\ncan complement rare scenarios in real-world datasets, such as surreal fantasy\nor multi-reference image generation, which frequently occur in user queries.\nSecond, they provide clean and controllable supervision. Real-world data often\ncontains complex background noise and inherent misalignment between text\ndescriptions and image content, whereas synthetic images offer pure backgrounds\nand long-tailed supervision signals, facilitating more accurate text-to-image\nalignment. Building on these insights, we introduce Echo-4o-Image, a 180K-scale\nsynthetic dataset generated by GPT-4o, harnessing the power of synthetic image\ndata to address blind spots in real-world coverage. Using this dataset, we\nfine-tune the unified multimodal generation baseline Bagel to obtain Echo-4o.\nIn addition, we propose two new evaluation benchmarks for a more accurate and\nchallenging assessment of image generation capabilities: GenEval++, which\nincreases instruction complexity to mitigate score saturation, and\nImagine-Bench, which focuses on evaluating both the understanding and\ngeneration of imaginative content. Echo-4o demonstrates strong performance\nacross standard benchmarks. Moreover, applying Echo-4o-Image to other\nfoundation models (e.g., OmniGen2, BLIP3-o) yields consistent performance gains\nacross multiple metrics, highlighting the datasets strong transferability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09987.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6349214f8146350b3a4c5cdf",
            "avatarUrl": "/avatars/cfd24caac9a87efb528d0f4c375932bc.svg",
            "fullname": "Dongzhi Jiang",
            "name": "CaraJ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.07750",
            "authors": [
                {
                    "_id": "689d9cfbb083e610d741eaf1",
                    "user": {
                        "_id": "64b509197da6a1dca8b20bb7",
                        "avatarUrl": "/avatars/0d3304b017b8ee735a9686123cec0b99.svg",
                        "isPro": false,
                        "fullname": "Haowen Wang",
                        "user": "whw199833",
                        "type": "user"
                    },
                    "name": "Haowen Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:04.723Z",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eaf2",
                    "name": "Yun Yue",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eaf3",
                    "user": {
                        "_id": "63d7bf58db69ae9b9beaeea3",
                        "avatarUrl": "/avatars/f1296f3c7af870535c1563e5f682da4f.svg",
                        "isPro": false,
                        "fullname": "Ye Zhiling",
                        "user": "yzlnew",
                        "type": "user"
                    },
                    "name": "Zhiling Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:07.534Z",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eaf4",
                    "name": "Shuowen Zhang",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eaf5",
                    "name": "Lei Fan",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eaf6",
                    "name": "Jiaxin Liang",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eaf7",
                    "name": "Jiadi Jiang",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eaf8",
                    "name": "Cheng Wei",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eaf9",
                    "name": "Jingyuan Deng",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eafa",
                    "name": "Xudong Han",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eafb",
                    "name": "Ji Li",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eafc",
                    "name": "Chunxiao Guo",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eafd",
                    "name": "Peng Wei",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eafe",
                    "name": "Jian Wang",
                    "hidden": false
                },
                {
                    "_id": "689d9cfbb083e610d741eaff",
                    "name": "Jinjie Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T08:28:47.000Z",
            "submittedOnDailyAt": "2025-08-14T07:00:08.478Z",
            "title": "Learning to Align, Aligning to Learn: A Unified Approach for\n  Self-Optimized Alignment",
            "submittedOnDailyBy": {
                "_id": "64b509197da6a1dca8b20bb7",
                "avatarUrl": "/avatars/0d3304b017b8ee735a9686123cec0b99.svg",
                "isPro": false,
                "fullname": "Haowen Wang",
                "user": "whw199833",
                "type": "user"
            },
            "summary": "Alignment methodologies have emerged as a critical pathway for enhancing\nlanguage model alignment capabilities. While SFT (supervised fine-tuning)\naccelerates convergence through direct token-level loss intervention, its\nefficacy is constrained by offline policy trajectory. In contrast,\nRL(reinforcement learning) facilitates exploratory policy optimization, but\nsuffers from low sample efficiency and stringent dependency on high-quality\nbase models. To address these dual challenges, we propose GRAO (Group Relative\nAlignment Optimization), a unified framework that synergizes the respective\nstrengths of SFT and RL through three key innovations: 1) A multi-sample\ngeneration strategy enabling comparative quality assessment via reward\nfeedback; 2) A novel Group Direct Alignment Loss formulation leveraging\nintra-group relative advantage weighting; 3) Reference-aware parameter updates\nguided by pairwise preference dynamics. Our theoretical analysis establishes\nGRAO's convergence guarantees and sample efficiency advantages over\nconventional approaches. Comprehensive evaluations across complex human\nalignment tasks demonstrate GRAO's superior performance, achieving\n57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and\nGRPO baselines respectively. This work provides both a theoretically grounded\nalignment framework and empirical evidence for efficient capability evolution\nin language models.",
            "upvotes": 14,
            "discussionId": "689d9cfcb083e610d741eb00",
            "ai_summary": "GRAO, a unified framework combining SFT and RL, enhances language model alignment through multi-sample generation, Group Direct Alignment Loss, and reference-aware parameter updates, demonstrating superior performance across human alignment tasks.",
            "ai_keywords": [
                "supervised fine-tuning",
                "reinforcement learning",
                "GRAO",
                "multi-sample generation",
                "Group Direct Alignment Loss",
                "reference-aware parameter updates",
                "convergence guarantees",
                "sample efficiency"
            ]
        },
        "publishedAt": "2025-08-11T04:28:47.000Z",
        "title": "Learning to Align, Aligning to Learn: A Unified Approach for\n  Self-Optimized Alignment",
        "summary": "Alignment methodologies have emerged as a critical pathway for enhancing\nlanguage model alignment capabilities. While SFT (supervised fine-tuning)\naccelerates convergence through direct token-level loss intervention, its\nefficacy is constrained by offline policy trajectory. In contrast,\nRL(reinforcement learning) facilitates exploratory policy optimization, but\nsuffers from low sample efficiency and stringent dependency on high-quality\nbase models. To address these dual challenges, we propose GRAO (Group Relative\nAlignment Optimization), a unified framework that synergizes the respective\nstrengths of SFT and RL through three key innovations: 1) A multi-sample\ngeneration strategy enabling comparative quality assessment via reward\nfeedback; 2) A novel Group Direct Alignment Loss formulation leveraging\nintra-group relative advantage weighting; 3) Reference-aware parameter updates\nguided by pairwise preference dynamics. Our theoretical analysis establishes\nGRAO's convergence guarantees and sample efficiency advantages over\nconventional approaches. Comprehensive evaluations across complex human\nalignment tasks demonstrate GRAO's superior performance, achieving\n57.70\\%,17.65\\% 7.95\\% and 5.18\\% relative improvements over SFT, DPO, PPO and\nGRPO baselines respectively. This work provides both a theoretically grounded\nalignment framework and empirical evidence for efficient capability evolution\nin language models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07750.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64b509197da6a1dca8b20bb7",
            "avatarUrl": "/avatars/0d3304b017b8ee735a9686123cec0b99.svg",
            "fullname": "Haowen Wang",
            "name": "whw199833",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.06009",
            "authors": [
                {
                    "_id": "68999432f022d141f5d4365d",
                    "user": {
                        "_id": "6894ed02b1bed8e651573585",
                        "avatarUrl": "/avatars/4532c56a77a87cf45f1c5040857aa6c3.svg",
                        "isPro": false,
                        "fullname": "Jun Feng",
                        "user": "junfeng0288",
                        "type": "user"
                    },
                    "name": "Jun Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:34:25.197Z",
                    "hidden": false
                },
                {
                    "_id": "68999432f022d141f5d4365e",
                    "name": "Zixin Wang",
                    "hidden": false
                },
                {
                    "_id": "68999432f022d141f5d4365f",
                    "name": "Zhentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68999432f022d141f5d43660",
                    "name": "Yue Guo",
                    "hidden": false
                },
                {
                    "_id": "68999432f022d141f5d43661",
                    "name": "Zhihan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68999432f022d141f5d43662",
                    "name": "Xiuyi Chen",
                    "hidden": false
                },
                {
                    "_id": "68999432f022d141f5d43663",
                    "name": "Zhenyang Li",
                    "hidden": false
                },
                {
                    "_id": "68999432f022d141f5d43664",
                    "name": "Dawei Yin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-08T04:39:16.000Z",
            "submittedOnDailyAt": "2025-08-14T01:33:48.978Z",
            "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math\n  Reasoning in Multimodal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6894ed02b1bed8e651573585",
                "avatarUrl": "/avatars/4532c56a77a87cf45f1c5040857aa6c3.svg",
                "isPro": false,
                "fullname": "Jun Feng",
                "user": "junfeng0288",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.",
            "upvotes": 11,
            "discussionId": "68999433f022d141f5d43665",
            "githubRepo": "https://github.com/junfeng0288/MathReal",
            "ai_summary": "MathReal, a dataset of real-world mathematical questions with images, evaluates the performance of multimodal large language models in authentic educational settings, highlighting challenges and providing insights for improvement.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "visual mathematical reasoning",
                "MathReal",
                "image quality degradation",
                "perspective variation",
                "irrelevant content interference",
                "multimodal mathematical reasoning",
                "experimental settings",
                "problem-solving abilities",
                "recognition",
                "comprehension",
                "reasoning capabilities"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-08-08T00:39:16.000Z",
        "title": "MathReal: We Keep It Real! A Real Scene Benchmark for Evaluating Math\n  Reasoning in Multimodal Large Language Models",
        "summary": "Multimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities in visual mathematical reasoning across various existing\nbenchmarks. However, these benchmarks are predominantly based on clean or\nprocessed multimodal inputs, without incorporating the images provided by\nreal-world Kindergarten through 12th grade (K-12) educational users. To address\nthis gap, we introduce MathReal, a meticulously curated dataset comprising\n2,000 mathematical questions with images captured by handheld mobile devices in\nauthentic scenarios. Each question is an image, containing the question text\nand visual element. We systematically classify the real images into three\nprimary categories: image quality degradation, perspective variation, and\nirrelevant content interference, which are further delineated into 14\nsubcategories. Additionally, MathReal spans five core knowledge and ability\ncategories, which encompass three question types and are divided into three\ndifficulty levels. To comprehensively evaluate the multimodal mathematical\nreasoning abilities of state-of-the-art MLLMs in real-world scenarios, we\ndesign six experimental settings that enable a systematic analysis of their\nperformance. Through extensive experimentation, we find that the\nproblem-solving abilities of existing MLLMs are significantly challenged in\nrealistic educational contexts. Based on this, we conduct a thorough analysis\nof their performance and error patterns, providing insights into their\nrecognition, comprehension, and reasoning capabilities, and outlining\ndirections for future improvements. Data and code:\nhttps://github.com/junfeng0288/MathReal.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06009.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6894ed02b1bed8e651573585",
            "avatarUrl": "/avatars/4532c56a77a87cf45f1c5040857aa6c3.svg",
            "fullname": "Jun Feng",
            "name": "junfeng0288",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.05613",
            "authors": [
                {
                    "_id": "6895646848b0ae5ca2710d70",
                    "name": "Haitao Hong",
                    "hidden": false
                },
                {
                    "_id": "6895646848b0ae5ca2710d71",
                    "user": {
                        "_id": "64098738342c26884c792c93",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                        "isPro": false,
                        "fullname": "Yuchen Yan",
                        "user": "yanyc",
                        "type": "user"
                    },
                    "name": "Yuchen Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:50:10.151Z",
                    "hidden": false
                },
                {
                    "_id": "6895646848b0ae5ca2710d72",
                    "name": "Xingyu Wu",
                    "hidden": false
                },
                {
                    "_id": "6895646848b0ae5ca2710d73",
                    "name": "Guiyang Hou",
                    "hidden": false
                },
                {
                    "_id": "6895646848b0ae5ca2710d74",
                    "name": "Wenqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "6895646848b0ae5ca2710d75",
                    "name": "Weiming Lu",
                    "hidden": false
                },
                {
                    "_id": "6895646848b0ae5ca2710d76",
                    "user": {
                        "_id": "5e1058e9fcf41d740b69966d",
                        "avatarUrl": "/avatars/ce74839ba871f2b54313a670a233ba82.svg",
                        "isPro": false,
                        "fullname": "Yongliang Shen",
                        "user": "tricktreat",
                        "type": "user"
                    },
                    "name": "Yongliang Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-11T06:50:12.645Z",
                    "hidden": false
                },
                {
                    "_id": "6895646848b0ae5ca2710d77",
                    "name": "Jun Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-07T17:53:56.000Z",
            "submittedOnDailyAt": "2025-08-14T01:18:12.379Z",
            "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models",
            "submittedOnDailyBy": {
                "_id": "64098738342c26884c792c93",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
                "isPro": false,
                "fullname": "Yuchen Yan",
                "user": "yanyc",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.",
            "upvotes": 10,
            "discussionId": "6895646948b0ae5ca2710d78",
            "projectPage": "https://zju-real.github.io/cooper/",
            "githubRepo": "https://github.com/ZJU-REAL/cooper",
            "ai_summary": "A reinforcement learning framework jointly optimizes policy and reward models to enhance robustness and mitigate reward hacking in large language models.",
            "ai_keywords": [
                "reinforcement learning",
                "policy model",
                "reward model",
                "model-based rewards",
                "rule-based rewards",
                "reward hacking",
                "hybrid annotation strategy",
                "reference-based reward modeling",
                "VerifyRM",
                "VerifyBench",
                "Qwen2.5-1.5B-Instruct"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-08-07T13:53:56.000Z",
        "title": "Cooper: Co-Optimizing Policy and Reward Models in Reinforcement Learning\n  for Large Language Models",
        "summary": "Large language models (LLMs) have demonstrated remarkable performance in\nreasoning tasks, where reinforcement learning (RL) serves as a key algorithm\nfor enhancing their reasoning capabilities. Currently, there are two mainstream\nreward paradigms: model-based rewards and rule-based rewards. However, both\napproaches suffer from limitations: rule-based rewards lack robustness, while\nmodel-based rewards are vulnerable to reward hacking. To address these issues,\nwe propose Cooper(Co-optimizing Policy Model and Reward Model), a RL framework\nthat jointly optimizes both the policy model and the reward model. Cooper\nleverages the high precision of rule-based rewards when identifying correct\nresponses, and dynamically constructs and selects positive-negative sample\npairs for continued training the reward model. This design enhances robustness\nand mitigates the risk of reward hacking. To further support Cooper, we\nintroduce a hybrid annotation strategy that efficiently and accurately\ngenerates training data for the reward model. We also propose a reference-based\nreward modeling paradigm, where the reward model takes a reference answer as\ninput. Based on this design, we train a reward model named VerifyRM, which\nachieves higher accuracy on VerifyBench compared to other models of the same\nsize. We conduct reinforcement learning using both VerifyRM and Cooper. Our\nexperiments show that Cooper not only alleviates reward hacking but also\nimproves end-to-end RL performance, for instance, achieving a 0.54% gain in\naverage accuracy on Qwen2.5-1.5B-Instruct. Our findings demonstrate that\ndynamically updating reward model is an effective way to combat reward hacking,\nproviding a reference for better integrating reward models into RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.05613.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64098738342c26884c792c93",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64098738342c26884c792c93/SxBUd-wLrl-PjQsrVYJte.jpeg",
            "fullname": "Yuchen Yan",
            "name": "yanyc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09968",
            "authors": [
                {
                    "_id": "689d5ebbb083e610d741ea45",
                    "name": "Luca Eyring",
                    "hidden": false
                },
                {
                    "_id": "689d5ebbb083e610d741ea46",
                    "user": {
                        "_id": "6254599b6e36fe62e141c8f9",
                        "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
                        "isPro": false,
                        "fullname": "Shyamgopal Karthik",
                        "user": "shyamgopal",
                        "type": "user"
                    },
                    "name": "Shyamgopal Karthik",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:25.592Z",
                    "hidden": false
                },
                {
                    "_id": "689d5ebbb083e610d741ea47",
                    "name": "Alexey Dosovitskiy",
                    "hidden": false
                },
                {
                    "_id": "689d5ebbb083e610d741ea48",
                    "name": "Nataniel Ruiz",
                    "hidden": false
                },
                {
                    "_id": "689d5ebbb083e610d741ea49",
                    "name": "Zeynep Akata",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T17:33:37.000Z",
            "submittedOnDailyAt": "2025-08-14T02:29:12.254Z",
            "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "6254599b6e36fe62e141c8f9",
                "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
                "isPro": false,
                "fullname": "Shyamgopal Karthik",
                "user": "shyamgopal",
                "type": "user"
            },
            "summary": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in\nLarge Language Models (LLMs) (e.g. reasoning models) and in generative vision\nmodels, allowing models to allocate additional computation during inference to\neffectively tackle increasingly complex problems. Despite the improvements of\nthis approach, an important limitation emerges: the substantial increase in\ncomputation time makes the process slow and impractical for many applications.\nGiven the success of this paradigm and its growing usage, we seek to preserve\nits benefits while eschewing the inference overhead. In this work we propose\none solution to the critical problem of integrating test-time scaling knowledge\ninto a model during post-training. Specifically, we replace reward guided\ntest-time noise optimization in diffusion models with a Noise Hypernetwork that\nmodulates initial input noise. We propose a theoretically grounded framework\nfor learning this reward-tilted distribution for distilled generators, through\na tractable noise-space objective that maintains fidelity to the base model\nwhile optimizing for desired characteristics. We show that our approach\nrecovers a substantial portion of the quality gains from explicit test-time\noptimization at a fraction of the computational cost. Code is available at\nhttps://github.com/ExplainableML/HyperNoise",
            "upvotes": 6,
            "discussionId": "689d5ebcb083e610d741ea4a",
            "ai_summary": "A Noise Hypernetwork is introduced to integrate test-time scaling knowledge into diffusion models, reducing computational cost while maintaining quality.",
            "ai_keywords": [
                "test-time scaling",
                "Large Language Models",
                "generative vision models",
                "diffusion models",
                "reward guided test-time noise optimization",
                "Noise Hypernetwork",
                "noise-space objective",
                "distilled generators"
            ]
        },
        "publishedAt": "2025-08-13T13:33:37.000Z",
        "title": "Noise Hypernetworks: Amortizing Test-Time Compute in Diffusion Models",
        "summary": "The new paradigm of test-time scaling has yielded remarkable breakthroughs in\nLarge Language Models (LLMs) (e.g. reasoning models) and in generative vision\nmodels, allowing models to allocate additional computation during inference to\neffectively tackle increasingly complex problems. Despite the improvements of\nthis approach, an important limitation emerges: the substantial increase in\ncomputation time makes the process slow and impractical for many applications.\nGiven the success of this paradigm and its growing usage, we seek to preserve\nits benefits while eschewing the inference overhead. In this work we propose\none solution to the critical problem of integrating test-time scaling knowledge\ninto a model during post-training. Specifically, we replace reward guided\ntest-time noise optimization in diffusion models with a Noise Hypernetwork that\nmodulates initial input noise. We propose a theoretically grounded framework\nfor learning this reward-tilted distribution for distilled generators, through\na tractable noise-space objective that maintains fidelity to the base model\nwhile optimizing for desired characteristics. We show that our approach\nrecovers a substantial portion of the quality gains from explicit test-time\noptimization at a fraction of the computational cost. Code is available at\nhttps://github.com/ExplainableML/HyperNoise",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09968.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6254599b6e36fe62e141c8f9",
            "avatarUrl": "/avatars/08d6b68b92c7bfd0a8be022ba9f2f289.svg",
            "fullname": "Shyamgopal Karthik",
            "name": "shyamgopal",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09456",
            "authors": [
                {
                    "_id": "689d555cb083e610d741e9fa",
                    "user": {
                        "_id": "656ae4088fb1ddf0d5ec9ac5",
                        "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
                        "isPro": false,
                        "fullname": "Junxian Li",
                        "user": "Duke-de-Artois",
                        "type": "user"
                    },
                    "name": "Junxian Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:30.188Z",
                    "hidden": false
                },
                {
                    "_id": "689d555cb083e610d741e9fb",
                    "name": "Beining Xu",
                    "hidden": false
                },
                {
                    "_id": "689d555cb083e610d741e9fc",
                    "user": {
                        "_id": "64bce15bafd1e46c5504ad38",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64bce15bafd1e46c5504ad38/vkEjiu-mIagKlrXzDH75o.png",
                        "isPro": false,
                        "fullname": "Di Zhang",
                        "user": "di-zhang-fdu",
                        "type": "user"
                    },
                    "name": "Di Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:37:27.650Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T03:22:19.000Z",
            "submittedOnDailyAt": "2025-08-14T01:51:36.770Z",
            "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding",
            "submittedOnDailyBy": {
                "_id": "656ae4088fb1ddf0d5ec9ac5",
                "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
                "isPro": false,
                "fullname": "Junxian Li",
                "user": "Duke-de-Artois",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.",
            "upvotes": 6,
            "discussionId": "689d555db083e610d741e9fd",
            "ai_summary": "A novel input-aware backdoor attack method, IAG, manipulates vision-language models to ground specific objects in images regardless of user queries, using a text-conditional U-Net and reconstruction loss to ensure stealthiness.",
            "ai_keywords": [
                "vision-language models",
                "visual grounding",
                "backdoor attacks",
                "input-aware backdoor attack",
                "adaptive trigger generator",
                "text-conditional U-Net",
                "reconstruction loss",
                "ASR@0.5",
                "InternVL-2.5-8B",
                "Ferret-7B",
                "LlaVA-1.5-7B",
                "ablation study",
                "potential defense"
            ]
        },
        "publishedAt": "2025-08-12T23:22:19.000Z",
        "title": "IAG: Input-aware Backdoor Attack on VLMs for Visual Grounding",
        "summary": "Vision-language models (VLMs) have shown significant advancements in tasks\nsuch as visual grounding, where they localize specific objects in images based\non natural language queries and images. However, security issues in visual\ngrounding tasks for VLMs remain underexplored, especially in the context of\nbackdoor attacks. In this paper, we introduce a novel input-aware backdoor\nattack method, IAG, designed to manipulate the grounding behavior of VLMs. This\nattack forces the model to ground a specific target object in the input image,\nregardless of the user's query. We propose an adaptive trigger generator that\nembeds the semantic information of the attack target's description into the\noriginal image using a text-conditional U-Net, thereby overcoming the\nopen-vocabulary attack challenge. To ensure the attack's stealthiness, we\nutilize a reconstruction loss to minimize visual discrepancies between poisoned\nand clean images. Additionally, we introduce a unified method for generating\nattack data. IAG is evaluated theoretically and empirically, demonstrating its\nfeasibility and effectiveness. Notably, our ASR@0.5 on InternVL-2.5-8B reaches\nover 65\\% on various testing sets. IAG also shows promising potential on\nmanipulating Ferret-7B and LlaVA-1.5-7B with very little accuracy decrease on\nclean samples. Extensive specific experiments, such as ablation study and\npotential defense, also indicate the robustness and transferability of our\nattack.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09456.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656ae4088fb1ddf0d5ec9ac5",
            "avatarUrl": "/avatars/e38468d2c0274f3c0f5732f30a2e3436.svg",
            "fullname": "Junxian Li",
            "name": "Duke-de-Artois",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09945",
            "authors": [
                {
                    "_id": "689da0ffb083e610d741eb02",
                    "name": "Lingjie Jiang",
                    "hidden": false
                },
                {
                    "_id": "689da0ffb083e610d741eb03",
                    "name": "Shaohan Huang",
                    "hidden": false
                },
                {
                    "_id": "689da0ffb083e610d741eb04",
                    "name": "Xun Wu",
                    "hidden": false
                },
                {
                    "_id": "689da0ffb083e610d741eb05",
                    "name": "Yixia Li",
                    "hidden": false
                },
                {
                    "_id": "689da0ffb083e610d741eb06",
                    "name": "Dongdong Zhang",
                    "hidden": false
                },
                {
                    "_id": "689da0ffb083e610d741eb07",
                    "name": "Furu Wei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T17:00:44.000Z",
            "submittedOnDailyAt": "2025-08-14T07:11:18.634Z",
            "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models",
            "submittedOnDailyBy": {
                "_id": "66ab80e9bfb7d73a56bc293c",
                "avatarUrl": "/avatars/9644266304c832c74ef572b5eb2d9468.svg",
                "isPro": false,
                "fullname": "Jack",
                "user": "lingjie23",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.",
            "upvotes": 4,
            "discussionId": "689da100b083e610d741eb08",
            "githubRepo": "https://github.com/JackLingjie/VisCodex",
            "ai_summary": "VisCodex integrates vision and coding models to enhance multimodal code generation, achieving top performance using a novel dataset and benchmark.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "VisCodex",
                "task vector-based model merging",
                "coding LLM",
                "vision-language backbone",
                "Multimodal Coding Dataset",
                "MCD",
                "InfiBench-V",
                "visually-rich programming questions"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-08-13T13:00:44.000Z",
        "title": "VisCodex: Unified Multimodal Code Generation via Merging Vision and\n  Coding Models",
        "summary": "Multimodal large language models (MLLMs) have significantly advanced the\nintegration of visual and textual understanding. However, their ability to\ngenerate code from multimodal inputs remains limited. In this work, we\nintroduce VisCodex, a unified framework that seamlessly merges vision and\ncoding language models to empower MLLMs with strong multimodal code generation\nabilities. Leveraging a task vector-based model merging technique, we integrate\na state-of-the-art coding LLM into a strong vision-language backbone, while\npreserving both visual comprehension and advanced coding skills. To support\ntraining and evaluation, we introduce the Multimodal Coding Dataset (MCD), a\nlarge-scale and diverse collection of 598k samples, including high-quality HTML\ncode, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic\nproblems. Furthermore, we propose InfiBench-V, a novel and challenging\nbenchmark specifically designed to assess models on visually-rich, real-world\nprogramming questions that demand a nuanced understanding of both textual and\nvisual contexts. Extensive experiments show that VisCodex achieves\nstate-of-the-art performance among open-source MLLMs and approaches proprietary\nmodels like GPT-4o, highlighting the effectiveness of our model merging\nstrategy and new datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09945.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66ab80e9bfb7d73a56bc293c",
            "avatarUrl": "/avatars/9644266304c832c74ef572b5eb2d9468.svg",
            "fullname": "Jack",
            "name": "lingjie23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09752",
            "authors": [
                {
                    "_id": "689e56b7a4caabb4320e5ca4",
                    "name": "Jan Maanicki",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5ca5",
                    "name": "Kamil Ciebiera",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5ca6",
                    "name": "Mateusz Boru",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5ca7",
                    "name": "Maciej Piro",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5ca8",
                    "name": "Jan Ludziejewski",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5ca9",
                    "name": "Maciej Stefaniak",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5caa",
                    "name": "Micha Krutul",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5cab",
                    "name": "Sebastian Jaszczur",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5cac",
                    "name": "Marek Cygan",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5cad",
                    "name": "Kamil Adamczewski",
                    "hidden": false
                },
                {
                    "_id": "689e56b7a4caabb4320e5cae",
                    "name": "Jakub Krajewski",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/651e96991b97c9f33d26bde6/foS1ExYU3lmcIC8OpH2md.png"
            ],
            "publishedAt": "2025-08-13T12:31:27.000Z",
            "submittedOnDailyAt": "2025-08-14T20:06:22.414Z",
            "title": "-Parametrization for Mixture of Experts",
            "submittedOnDailyBy": {
                "_id": "651e96991b97c9f33d26bde6",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
                "isPro": false,
                "fullname": "Elie Bakouch",
                "user": "eliebak",
                "type": "user"
            },
            "summary": "Recent years have seen a growing interest and adoption of LLMs, with\nmuTransfer becoming a key technique for tuning hyperparameters in\nlarge-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a\nleading architecture in extremely large models. However, the intersection of\nthese two advancements has remained unexplored. In this work, we derive a\nmu-Parameterization (muP) for MoE, providing theoretical guarantees for\nfeature learning across model widths in both the router and experts. We\nempirically validate our parameterization and further investigate how scaling\nthe number of experts and granularity affects the optimal learning rate.",
            "upvotes": 3,
            "discussionId": "689e56b8a4caabb4320e5caf",
            "ai_summary": "A new parameterization for Mixture-of-Experts models provides theoretical guarantees for feature learning and investigates the impact of scaling experts and granularity on learning rates.",
            "ai_keywords": [
                "Mixture-of-Experts",
                "MoE",
                "$\\mu$Transfer",
                "$\\mu$-Parameterization",
                "$\\mu$P",
                "router",
                "experts",
                "feature learning",
                "learning rate"
            ]
        },
        "publishedAt": "2025-08-13T08:31:27.000Z",
        "title": "-Parametrization for Mixture of Experts",
        "summary": "Recent years have seen a growing interest and adoption of LLMs, with\nmuTransfer becoming a key technique for tuning hyperparameters in\nlarge-scale training. Meanwhile, Mixture-of-Experts (MoE) has emerged as a\nleading architecture in extremely large models. However, the intersection of\nthese two advancements has remained unexplored. In this work, we derive a\nmu-Parameterization (muP) for MoE, providing theoretical guarantees for\nfeature learning across model widths in both the router and experts. We\nempirically validate our parameterization and further investigate how scaling\nthe number of experts and granularity affects the optimal learning rate.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/651e96991b97c9f33d26bde6/foS1ExYU3lmcIC8OpH2md.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09752.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651e96991b97c9f33d26bde6",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/651e96991b97c9f33d26bde6/-Bqs6qrmz0yCfwtB2e-6q.jpeg",
            "fullname": "Elie Bakouch",
            "name": "eliebak",
            "type": "user",
            "isPro": false,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 262
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09726",
            "authors": [
                {
                    "_id": "689d9af7b083e610d741eadd",
                    "name": "Vaishnavi Shrivastava",
                    "hidden": false
                },
                {
                    "_id": "689d9af7b083e610d741eade",
                    "name": "Ahmed Awadallah",
                    "hidden": false
                },
                {
                    "_id": "689d9af7b083e610d741eadf",
                    "name": "Vidhisha Balachandran",
                    "hidden": false
                },
                {
                    "_id": "689d9af7b083e610d741eae0",
                    "name": "Shivam Garg",
                    "hidden": false
                },
                {
                    "_id": "689d9af7b083e610d741eae1",
                    "name": "Harkirat Behl",
                    "hidden": false
                },
                {
                    "_id": "689d9af7b083e610d741eae2",
                    "name": "Dimitris Papailiopoulos",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T11:43:49.000Z",
            "submittedOnDailyAt": "2025-08-14T19:55:18.638Z",
            "title": "Sample More to Think Less: Group Filtered Policy Optimization for\n  Concise Reasoning",
            "submittedOnDailyBy": {
                "_id": "63b75a016fc56e43c3c15980",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672960383634-noauth.jpeg",
                "isPro": false,
                "fullname": "Vaishnavi Shrivastava",
                "user": "vshrivas",
                "type": "user"
            },
            "summary": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning.",
            "upvotes": 3,
            "discussionId": "689d9af8b083e610d741eae3",
            "ai_summary": "GFPO reduces length inflation in large language models by sampling larger groups and filtering responses based on length and token efficiency, maintaining accuracy and improving computational efficiency.",
            "ai_keywords": [
                "reinforcement learning",
                "verifiable rewards",
                "length explosion",
                "token efficiency",
                "Group Filtered Policy Optimization",
                "GFPO",
                "GRPO",
                "Phi-4-reasoning model",
                "AIME",
                "GPQA",
                "Omni-MATH",
                "LiveCodeBench",
                "Adaptive Difficulty GFPO"
            ]
        },
        "publishedAt": "2025-08-13T07:43:49.000Z",
        "title": "Sample More to Think Less: Group Filtered Policy Optimization for\n  Concise Reasoning",
        "summary": "Large language models trained with reinforcement learning with verifiable\nrewards tend to trade accuracy for length--inflating response lengths to\nachieve gains in accuracy. While longer answers may be warranted for harder\nproblems, many tokens are merely \"filler\": repetitive, verbose text that makes\nno real progress. We introduce GFPO (Group Filtered Policy Optimization), which\ncurbs this length explosion by sampling larger groups per problem during\ntraining and filtering responses to train on based on two key metrics: (1)\nresponse length and (2) token efficiency: reward per token ratio. By sampling\nmore at training time, we teach models to think less at inference time. On the\nPhi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across\nchallenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH,\nLiveCodeBench) while maintaining accuracy. Optimizing for reward per token\nfurther increases reductions in length inflation to 71-85%. We also propose\nAdaptive Difficulty GFPO, which dynamically allocates more training resources\nto harder problems based on real-time difficulty estimates, improving the\nbalance between computational efficiency and accuracy especially on difficult\nquestions. GFPO demonstrates that increased training-time compute directly\ntranslates to reduced test-time compute--a simple yet effective trade-off for\nefficient reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09726.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63b75a016fc56e43c3c15980",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1672960383634-noauth.jpeg",
            "fullname": "Vaishnavi Shrivastava",
            "name": "vshrivas",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09667",
            "authors": [
                {
                    "_id": "689dd697b083e610d741eb4d",
                    "name": "Xingyilang Yin",
                    "hidden": false
                },
                {
                    "_id": "689dd697b083e610d741eb4e",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "689dd697b083e610d741eb4f",
                    "name": "Jiahao Chang",
                    "hidden": false
                },
                {
                    "_id": "689dd697b083e610d741eb50",
                    "name": "Ying Feng",
                    "hidden": false
                },
                {
                    "_id": "689dd697b083e610d741eb51",
                    "name": "Qingnan Fan",
                    "hidden": false
                },
                {
                    "_id": "689dd697b083e610d741eb52",
                    "name": "Xi Yang",
                    "hidden": false
                },
                {
                    "_id": "689dd697b083e610d741eb53",
                    "name": "Chi-Man Pun",
                    "hidden": false
                },
                {
                    "_id": "689dd697b083e610d741eb54",
                    "name": "Huaqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "689dd697b083e610d741eb55",
                    "name": "Xiaodong Cun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T09:56:28.000Z",
            "submittedOnDailyAt": "2025-08-14T10:59:47.257Z",
            "title": "GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video\n  Diffusion Priors",
            "submittedOnDailyBy": {
                "_id": "63184c517ca1b876d99b7e0e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
                "isPro": false,
                "fullname": "Xiaodong Cun",
                "user": "vinthony",
                "type": "user"
            },
            "summary": "Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views\nis an ill-posed problem due to insufficient information, often resulting in\nnoticeable artifacts. While recent approaches have sought to leverage\ngenerative priors to complete information for under-constrained regions, they\nstruggle to generate content that remains consistent with input observations.\nTo address this challenge, we propose GSFixer, a novel framework designed to\nimprove the quality of 3DGS representations reconstructed from sparse inputs.\nThe core of our approach is the reference-guided video restoration model, built\nupon a DiT-based video diffusion model trained on paired artifact 3DGS renders\nand clean frames with additional reference-based conditions. Considering the\ninput sparse views as references, our model integrates both 2D semantic\nfeatures and 3D geometric features of reference views extracted from the visual\ngeometry foundation model, enhancing the semantic coherence and 3D consistency\nwhen fixing artifact novel views. Furthermore, considering the lack of suitable\nbenchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which\ncontains artifact frames rendered using low-quality 3DGS. Extensive experiments\ndemonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS\nartifact restoration and sparse-view 3D reconstruction. Project page:\nhttps://github.com/GVCLab/GSFixer.",
            "upvotes": 3,
            "discussionId": "689dd697b083e610d741eb56",
            "ai_summary": "GSFixer enhances 3D Gaussian Splatting reconstructions from sparse views using a DiT-based video diffusion model with reference-guided conditions, improving artifact restoration and 3D consistency.",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "DiT-based video diffusion model",
                "reference-guided video restoration",
                "2D semantic features",
                "3D geometric features",
                "visual geometry foundation model",
                "DL3DV-Res",
                "artifact restoration",
                "sparse-view 3D reconstruction"
            ]
        },
        "publishedAt": "2025-08-13T05:56:28.000Z",
        "title": "GSFixer: Improving 3D Gaussian Splatting with Reference-Guided Video\n  Diffusion Priors",
        "summary": "Reconstructing 3D scenes using 3D Gaussian Splatting (3DGS) from sparse views\nis an ill-posed problem due to insufficient information, often resulting in\nnoticeable artifacts. While recent approaches have sought to leverage\ngenerative priors to complete information for under-constrained regions, they\nstruggle to generate content that remains consistent with input observations.\nTo address this challenge, we propose GSFixer, a novel framework designed to\nimprove the quality of 3DGS representations reconstructed from sparse inputs.\nThe core of our approach is the reference-guided video restoration model, built\nupon a DiT-based video diffusion model trained on paired artifact 3DGS renders\nand clean frames with additional reference-based conditions. Considering the\ninput sparse views as references, our model integrates both 2D semantic\nfeatures and 3D geometric features of reference views extracted from the visual\ngeometry foundation model, enhancing the semantic coherence and 3D consistency\nwhen fixing artifact novel views. Furthermore, considering the lack of suitable\nbenchmarks for 3DGS artifact restoration evaluation, we present DL3DV-Res which\ncontains artifact frames rendered using low-quality 3DGS. Extensive experiments\ndemonstrate our GSFixer outperforms current state-of-the-art methods in 3DGS\nartifact restoration and sparse-view 3D reconstruction. Project page:\nhttps://github.com/GVCLab/GSFixer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09667.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63184c517ca1b876d99b7e0e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63184c517ca1b876d99b7e0e/b-qDExoeJuDXK0cJBZKnz.jpeg",
            "fullname": "Xiaodong Cun",
            "name": "vinthony",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 325
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.06937",
            "authors": [
                {
                    "_id": "689ca649fab6fdd2e52aca4d",
                    "user": {
                        "_id": "6523c037974423bd3e0fac90",
                        "avatarUrl": "/avatars/f1bc3dc2fd7afd59cf53f0f2391ed16b.svg",
                        "isPro": true,
                        "fullname": "Wy Xie",
                        "user": "vaynexie",
                        "type": "user"
                    },
                    "name": "Weiyan Xie",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:38:13.947Z",
                    "hidden": false
                },
                {
                    "_id": "689ca649fab6fdd2e52aca4e",
                    "name": "Han Gao",
                    "hidden": false
                },
                {
                    "_id": "689ca649fab6fdd2e52aca4f",
                    "name": "Didan Deng",
                    "hidden": false
                },
                {
                    "_id": "689ca649fab6fdd2e52aca50",
                    "name": "Kaican Li",
                    "hidden": false
                },
                {
                    "_id": "689ca649fab6fdd2e52aca51",
                    "name": "April Hua Liu",
                    "hidden": false
                },
                {
                    "_id": "689ca649fab6fdd2e52aca52",
                    "name": "Yongxiang Huang",
                    "hidden": false
                },
                {
                    "_id": "689ca649fab6fdd2e52aca53",
                    "name": "Nevin L. Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-09T11:06:58.000Z",
            "submittedOnDailyAt": "2025-08-14T12:17:39.792Z",
            "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
            "submittedOnDailyBy": {
                "_id": "6523c037974423bd3e0fac90",
                "avatarUrl": "/avatars/f1bc3dc2fd7afd59cf53f0f2391ed16b.svg",
                "isPro": true,
                "fullname": "Wy Xie",
                "user": "vaynexie",
                "type": "user"
            },
            "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
            "upvotes": 3,
            "discussionId": "689ca649fab6fdd2e52aca54",
            "projectPage": "https://vaynexie.github.io/CannyEdit/",
            "githubRepo": "https://github.com/vaynexie/CannyEdit",
            "ai_summary": "CannyEdit is a training-free framework that enhances text-to-image editing by balancing text adherence, context fidelity, and seamless integration through Selective Canny Control and Dual-Prompt Guidance.",
            "ai_keywords": [
                "Selective Canny Control",
                "Canny ControlNet",
                "inversion-phase ControlNet",
                "Dual-Prompt Guidance",
                "text-to-image",
                "regional image editing",
                "generative priors",
                "foundation models",
                "text adherence",
                "context fidelity",
                "editing seamlessness",
                "user studies",
                "AIGC experts"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-08-09T07:06:58.000Z",
        "title": "CannyEdit: Selective Canny Control and Dual-Prompt Guidance for\n  Training-Free Image Editing",
        "summary": "Recent advances in text-to-image (T2I) models have enabled training-free\nregional image editing by leveraging the generative priors of foundation\nmodels. However, existing methods struggle to balance text adherence in edited\nregions, context fidelity in unedited areas, and seamless integration of edits.\nWe introduce CannyEdit, a novel training-free framework that addresses these\nchallenges through two key innovations: (1) Selective Canny Control, which\nmasks the structural guidance of Canny ControlNet in user-specified editable\nregions while strictly preserving details of the source images in unedited\nareas via inversion-phase ControlNet information retention. This enables\nprecise, text-driven edits without compromising contextual integrity. (2)\nDual-Prompt Guidance, which combines local prompts for object-specific edits\nwith a global target prompt to maintain coherent scene interactions. On\nreal-world image editing tasks (addition, replacement, removal), CannyEdit\noutperforms prior methods like KV-Edit, achieving a 2.93 to 10.49 percent\nimprovement in the balance of text adherence and context fidelity. In terms of\nediting seamlessness, user studies reveal only 49.2 percent of general users\nand 42.0 percent of AIGC experts identified CannyEdit's results as AI-edited\nwhen paired with real images without edits, versus 76.08 to 89.09 percent for\ncompetitor methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06937.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6523c037974423bd3e0fac90",
            "avatarUrl": "/avatars/f1bc3dc2fd7afd59cf53f0f2391ed16b.svg",
            "fullname": "Wy Xie",
            "name": "vaynexie",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09776",
            "authors": [
                {
                    "_id": "689d93dab083e610d741ead0",
                    "name": "Mahdi Dhaini",
                    "hidden": false
                },
                {
                    "_id": "689d93dab083e610d741ead1",
                    "name": "Juraj Vladika",
                    "hidden": false
                },
                {
                    "_id": "689d93dab083e610d741ead2",
                    "name": "Ege Erdogan",
                    "hidden": false
                },
                {
                    "_id": "689d93dab083e610d741ead3",
                    "name": "Zineb Attaoui",
                    "hidden": false
                },
                {
                    "_id": "689d93dab083e610d741ead4",
                    "name": "Gjergji Kasneci",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T12:59:08.000Z",
            "submittedOnDailyAt": "2025-08-14T06:15:45.702Z",
            "title": "Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study",
            "submittedOnDailyBy": {
                "_id": "64ccf54986d8dc0caa7d1f64",
                "avatarUrl": "/avatars/ba47e70e4bc07eb2ba6b648192a15243.svg",
                "isPro": false,
                "fullname": "Mahdi Dhaini",
                "user": "mdhaini",
                "type": "user"
            },
            "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.",
            "upvotes": 2,
            "discussionId": "689d93dab083e610d741ead5",
            "githubRepo": "https://github.com/dmah10/helpful-natural-language-explanations",
            "ai_summary": "Automated generation of textual explanations using large language models improves model performance in natural language inference tasks, offering a scalable alternative to human annotation.",
            "ai_keywords": [
                "Explainable Natural Language Processing",
                "textual explanations",
                "large language models",
                "Natural Language Generation",
                "pre-trained language models",
                "natural language inference",
                "benchmark datasets"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-08-13T08:59:08.000Z",
        "title": "Can LLM-Generated Textual Explanations Enhance Model Classification\n  Performance? An Empirical Study",
        "summary": "In the rapidly evolving field of Explainable Natural Language Processing\n(NLP), textual explanations, i.e., human-like rationales, are pivotal for\nexplaining model predictions and enriching datasets with interpretable labels.\nTraditional approaches rely on human annotation, which is costly,\nlabor-intensive, and impedes scalability. In this work, we present an automated\nframework that leverages multiple state-of-the-art large language models (LLMs)\nto generate high-quality textual explanations. We rigorously assess the quality\nof these LLM-generated explanations using a comprehensive suite of Natural\nLanguage Generation (NLG) metrics. Furthermore, we investigate the downstream\nimpact of these explanations on the performance of pre-trained language models\n(PLMs) and LLMs across natural language inference tasks on two diverse\nbenchmark datasets. Our experiments demonstrate that automated explanations\nexhibit highly competitive effectiveness compared to human-annotated\nexplanations in improving model performance. Our findings underscore a\npromising avenue for scalable, automated LLM-based textual explanation\ngeneration for extending NLP datasets and enhancing model performance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09776.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ccf54986d8dc0caa7d1f64",
            "avatarUrl": "/avatars/ba47e70e4bc07eb2ba6b648192a15243.svg",
            "fullname": "Mahdi Dhaini",
            "name": "mdhaini",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.07237",
            "authors": [
                {
                    "_id": "689b4d2dfab6fdd2e52ac6ec",
                    "name": "Bo Wang",
                    "hidden": false
                },
                {
                    "_id": "689b4d2dfab6fdd2e52ac6ed",
                    "name": "Mengyuan Xu",
                    "hidden": false
                },
                {
                    "_id": "689b4d2dfab6fdd2e52ac6ee",
                    "name": "Yue Yan",
                    "hidden": false
                },
                {
                    "_id": "689b4d2dfab6fdd2e52ac6ef",
                    "user": {
                        "_id": "689b477fc6276c8d353c1cbd",
                        "avatarUrl": "/avatars/056b2b4496797989be2470ed146c1d09.svg",
                        "isPro": false,
                        "fullname": "yang",
                        "user": "Yuqunyang",
                        "type": "user"
                    },
                    "name": "Yuqun Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:39:08.493Z",
                    "hidden": false
                },
                {
                    "_id": "689b4d2dfab6fdd2e52ac6f0",
                    "name": "Kechen Shu",
                    "hidden": false
                },
                {
                    "_id": "689b4d2dfab6fdd2e52ac6f1",
                    "name": "Wei Ping",
                    "hidden": false
                },
                {
                    "_id": "689b4d2dfab6fdd2e52ac6f2",
                    "name": "Xu Tang",
                    "hidden": false
                },
                {
                    "_id": "689b4d2dfab6fdd2e52ac6f3",
                    "name": "Wei Jiang",
                    "hidden": false
                },
                {
                    "_id": "689b4d2dfab6fdd2e52ac6f4",
                    "name": "Zheng You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-10T08:33:03.000Z",
            "submittedOnDailyAt": "2025-08-14T13:57:59.705Z",
            "title": "ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and\n  Individual Variations for Fine-Grained Segmentation",
            "submittedOnDailyBy": {
                "_id": "689b477fc6276c8d353c1cbd",
                "avatarUrl": "/avatars/056b2b4496797989be2470ed146c1d09.svg",
                "isPro": false,
                "fullname": "yang",
                "user": "Yuqunyang",
                "type": "user"
            },
            "summary": "Precise lesion resection depends on accurately identifying fine-grained\nanatomical structures. While many coarse-grained segmentation (CGS) methods\nhave been successful in large-scale segmentation (e.g., organs), they fall\nshort in clinical scenarios requiring fine-grained segmentation (FGS), which\nremains challenging due to frequent individual variations in small-scale\nanatomical structures. Although recent Mamba-based models have advanced medical\nimage segmentation, they often rely on fixed manually-defined scanning orders,\nwhich limit their adaptability to individual variations in FGS. To address\nthis, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It\nintroduces adaptive scan scores to dynamically guide the scanning order,\ngenerated by combining group-level commonalities and individual-level\nvariations. Experiments on two public datasets (ACDC and Synapse) and a newly\nproposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that\nASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and\ndataset are available at https://github.com/YqunYang/ASM-UNet.",
            "upvotes": 2,
            "discussionId": "689b4d2dfab6fdd2e52ac6f5",
            "githubRepo": "https://github.com/YqunYang/ASM-UNet",
            "ai_summary": "ASM-UNet, a Mamba-based architecture with adaptive scan scores, enhances fine-grained segmentation by dynamically adjusting scanning orders to accommodate individual anatomical variations.",
            "ai_keywords": [
                "Mamba-based models",
                "adaptive scan scores",
                "fine-grained segmentation",
                "coarse-grained segmentation",
                "ACDC",
                "Synapse",
                "BTMS"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-08-10T04:33:03.000Z",
        "title": "ASM-UNet: Adaptive Scan Mamba Integrating Group Commonalities and\n  Individual Variations for Fine-Grained Segmentation",
        "summary": "Precise lesion resection depends on accurately identifying fine-grained\nanatomical structures. While many coarse-grained segmentation (CGS) methods\nhave been successful in large-scale segmentation (e.g., organs), they fall\nshort in clinical scenarios requiring fine-grained segmentation (FGS), which\nremains challenging due to frequent individual variations in small-scale\nanatomical structures. Although recent Mamba-based models have advanced medical\nimage segmentation, they often rely on fixed manually-defined scanning orders,\nwhich limit their adaptability to individual variations in FGS. To address\nthis, we propose ASM-UNet, a novel Mamba-based architecture for FGS. It\nintroduces adaptive scan scores to dynamically guide the scanning order,\ngenerated by combining group-level commonalities and individual-level\nvariations. Experiments on two public datasets (ACDC and Synapse) and a newly\nproposed challenging biliary tract FGS dataset, namely BTMS, demonstrate that\nASM-UNet achieves superior performance in both CGS and FGS tasks. Our code and\ndataset are available at https://github.com/YqunYang/ASM-UNet.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07237.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "689b477fc6276c8d353c1cbd",
            "avatarUrl": "/avatars/056b2b4496797989be2470ed146c1d09.svg",
            "fullname": "yang",
            "name": "Yuqunyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.01522",
            "authors": [
                {
                    "_id": "689ddcb9444103934017ef5c",
                    "user": {
                        "_id": "6895e27da51af9aacdf40bfc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/o6LhpvE5XuHQJBZPvZDLW.jpeg",
                        "isPro": false,
                        "fullname": "Jack Zeng",
                        "user": "jackzeng-robotics",
                        "type": "user"
                    },
                    "name": "Jack Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:38:16.141Z",
                    "hidden": false
                },
                {
                    "_id": "689ddcb9444103934017ef5d",
                    "name": "Andreu Matoses Gimenez",
                    "hidden": false
                },
                {
                    "_id": "689ddcb9444103934017ef5e",
                    "name": "Eugene Vinitsky",
                    "hidden": false
                },
                {
                    "_id": "689ddcb9444103934017ef5f",
                    "name": "Javier Alonso-Mora",
                    "hidden": false
                },
                {
                    "_id": "689ddcb9444103934017ef60",
                    "name": "Sihao Sun",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6895e27da51af9aacdf40bfc/HyfS9aGW7pmMMNw5ApH2-.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6895e27da51af9aacdf40bfc/cmHVXcQGwmh7NFObuFTk6.mp4"
            ],
            "publishedAt": "2025-08-02T23:52:33.000Z",
            "submittedOnDailyAt": "2025-08-14T12:12:07.990Z",
            "title": "Decentralized Aerial Manipulation of a Cable-Suspended Load using\n  Multi-Agent Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6895e27da51af9aacdf40bfc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/o6LhpvE5XuHQJBZPvZDLW.jpeg",
                "isPro": false,
                "fullname": "Jack Zeng",
                "user": "jackzeng-robotics",
                "type": "user"
            },
            "summary": "This paper presents the first decentralized method to enable real-world 6-DoF\nmanipulation of a cable-suspended load using a team of Micro-Aerial Vehicles\n(MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train\nan outer-loop control policy for each MAV. Unlike state-of-the-art controllers\nthat utilize a centralized scheme, our policy does not require global states,\ninter-MAV communications, nor neighboring MAV information. Instead, agents\ncommunicate implicitly through load pose observations alone, which enables high\nscalability and flexibility. It also significantly reduces computing costs\nduring inference time, enabling onboard deployment of the policy. In addition,\nwe introduce a new action space design for the MAVs using linear acceleration\nand body rates. This choice, combined with a robust low-level controller,\nenables reliable sim-to-real transfer despite significant uncertainties caused\nby cable tension during dynamic 3D motion. We validate our method in various\nreal-world experiments, including full-pose control under load model\nuncertainties, showing setpoint tracking performance comparable to the\nstate-of-the-art centralized method. We also demonstrate cooperation amongst\nagents with heterogeneous control policies, and robustness to the complete\nin-flight loss of one MAV. Videos of experiments:\nhttps://autonomousrobots.nl/paper_websites/aerial-manipulation-marl",
            "upvotes": 2,
            "discussionId": "689ddcba444103934017ef61",
            "projectPage": "https://autonomousrobots.nl/paper_websites/aerial-manipulation-marl",
            "githubRepo": "https://github.com/Aerial-Manipulation-Lab/MARL_cooperative_aerial_manipulation_ext",
            "ai_summary": "A decentralized multi-agent reinforcement learning method enables real-world 6-DoF manipulation of cable-suspended loads using MAVs, achieving performance comparable to centralized methods with improved scalability and robustness.",
            "ai_keywords": [
                "multi-agent reinforcement learning",
                "MARL",
                "outer-loop control policy",
                "decentralized control",
                "load pose observations",
                "linear acceleration",
                "body rates",
                "low-level controller",
                "sim-to-real transfer",
                "setpoint tracking",
                "heterogeneous control policies"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-08-02T19:52:33.000Z",
        "title": "Decentralized Aerial Manipulation of a Cable-Suspended Load using\n  Multi-Agent Reinforcement Learning",
        "summary": "This paper presents the first decentralized method to enable real-world 6-DoF\nmanipulation of a cable-suspended load using a team of Micro-Aerial Vehicles\n(MAVs). Our method leverages multi-agent reinforcement learning (MARL) to train\nan outer-loop control policy for each MAV. Unlike state-of-the-art controllers\nthat utilize a centralized scheme, our policy does not require global states,\ninter-MAV communications, nor neighboring MAV information. Instead, agents\ncommunicate implicitly through load pose observations alone, which enables high\nscalability and flexibility. It also significantly reduces computing costs\nduring inference time, enabling onboard deployment of the policy. In addition,\nwe introduce a new action space design for the MAVs using linear acceleration\nand body rates. This choice, combined with a robust low-level controller,\nenables reliable sim-to-real transfer despite significant uncertainties caused\nby cable tension during dynamic 3D motion. We validate our method in various\nreal-world experiments, including full-pose control under load model\nuncertainties, showing setpoint tracking performance comparable to the\nstate-of-the-art centralized method. We also demonstrate cooperation amongst\nagents with heterogeneous control policies, and robustness to the complete\nin-flight loss of one MAV. Videos of experiments:\nhttps://autonomousrobots.nl/paper_websites/aerial-manipulation-marl",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6895e27da51af9aacdf40bfc/HyfS9aGW7pmMMNw5ApH2-.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6895e27da51af9aacdf40bfc/cmHVXcQGwmh7NFObuFTk6.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.01522.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6895e27da51af9aacdf40bfc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/o6LhpvE5XuHQJBZPvZDLW.jpeg",
            "fullname": "Jack Zeng",
            "name": "jackzeng-robotics",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.06944",
            "authors": [
                {
                    "_id": "689b17fffab6fdd2e52ac681",
                    "name": "Lixuan He",
                    "hidden": false
                },
                {
                    "_id": "689b17fffab6fdd2e52ac682",
                    "user": {
                        "_id": "6465d3bd63e7e09dd02e95c3",
                        "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                        "isPro": false,
                        "fullname": "Jie Feng",
                        "user": "JJ-TMT",
                        "type": "user"
                    },
                    "name": "Jie Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:39:12.608Z",
                    "hidden": false
                },
                {
                    "_id": "689b17fffab6fdd2e52ac683",
                    "name": "Yong Li",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/VkaQHA1J_Ujq7jP0JT0IY.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/zISYu45fORE3Gy3mgL7Th.jpeg"
            ],
            "publishedAt": "2025-08-09T11:40:54.000Z",
            "submittedOnDailyAt": "2025-08-14T07:56:26.262Z",
            "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal\n  Imitation-Exploration Balance",
            "submittedOnDailyBy": {
                "_id": "6465d3bd63e7e09dd02e95c3",
                "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
                "isPro": false,
                "fullname": "Jie Feng",
                "user": "JJ-TMT",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks\nthrough a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by\nReinforcement Learning (RL), a process fraught with catastrophic forgetting and\nsuboptimal trade-offs between imitation and exploration. Recent single-stage\nmethods attempt to unify SFT and RL using heuristics, but lack a principled\nmechanism for dynamically balancing the two paradigms. In this paper, we\nreframe this challenge through the theoretical lens of implicit\nrewards, viewing SFT and RL not as distinct methods but as complementary\nreward signals. We introduce Adaptive Meta Fine-Tuning (AMFT), a novel\nsingle-stage algorithm that learns the optimal balance between SFT's implicit,\npath-level reward and RL's explicit, outcome-based reward. The core of AMFT is\na meta-gradient adaptive weight controller that treats the SFT-RL\nbalance as a learnable parameter, dynamically optimizing it to maximize\nlong-term task performance. This forward-looking approach, regularized by\npolicy entropy for stability, autonomously discovers an effective training\ncurriculum. We conduct a comprehensive evaluation on challenging benchmarks\nspanning mathematical reasoning, abstract visual reasoning (General Points),\nand vision-language navigation (V-IRL). AMFT consistently establishes a new\nstate-of-the-art and demonstrats superior generalization on out-of-distribution\n(OOD) tasks. Ablation studies and training dynamic analysis confirm that the\nmeta-learning controller is crucial for AMFT's stability, sample efficiency,\nand performance, offering a more principled and effective paradigm for LLM\nalignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.",
            "upvotes": 1,
            "discussionId": "689b1800fab6fdd2e52ac684",
            "githubRepo": "https://github.com/TSYJ-He/AMFT",
            "ai_summary": "Adaptive Meta Fine-Tuning (AMFT) dynamically balances Supervised Fine-Tuning and Reinforcement Learning using implicit rewards to improve LLM performance and generalization.",
            "ai_keywords": [
                "Supervised Fine-Tuning",
                "Reinforcement Learning",
                "implicit rewards",
                "Adaptive Meta Fine-Tuning",
                "meta-gradient adaptive weight controller",
                "policy entropy",
                "mathematical reasoning",
                "abstract visual reasoning",
                "vision-language navigation",
                "out-of-distribution tasks"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-08-09T07:40:54.000Z",
        "title": "AMFT: Aligning LLM Reasoners by Meta-Learning the Optimal\n  Imitation-Exploration Balance",
        "summary": "Large Language Models (LLMs) are typically fine-tuned for reasoning tasks\nthrough a two-stage pipeline of Supervised Fine-Tuning (SFT) followed by\nReinforcement Learning (RL), a process fraught with catastrophic forgetting and\nsuboptimal trade-offs between imitation and exploration. Recent single-stage\nmethods attempt to unify SFT and RL using heuristics, but lack a principled\nmechanism for dynamically balancing the two paradigms. In this paper, we\nreframe this challenge through the theoretical lens of implicit\nrewards, viewing SFT and RL not as distinct methods but as complementary\nreward signals. We introduce Adaptive Meta Fine-Tuning (AMFT), a novel\nsingle-stage algorithm that learns the optimal balance between SFT's implicit,\npath-level reward and RL's explicit, outcome-based reward. The core of AMFT is\na meta-gradient adaptive weight controller that treats the SFT-RL\nbalance as a learnable parameter, dynamically optimizing it to maximize\nlong-term task performance. This forward-looking approach, regularized by\npolicy entropy for stability, autonomously discovers an effective training\ncurriculum. We conduct a comprehensive evaluation on challenging benchmarks\nspanning mathematical reasoning, abstract visual reasoning (General Points),\nand vision-language navigation (V-IRL). AMFT consistently establishes a new\nstate-of-the-art and demonstrats superior generalization on out-of-distribution\n(OOD) tasks. Ablation studies and training dynamic analysis confirm that the\nmeta-learning controller is crucial for AMFT's stability, sample efficiency,\nand performance, offering a more principled and effective paradigm for LLM\nalignment.Our codes are open-sourced via https://github.com/hlxtsyj/AMFT.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/VkaQHA1J_Ujq7jP0JT0IY.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/6465d3bd63e7e09dd02e95c3/zISYu45fORE3Gy3mgL7Th.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06944.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6465d3bd63e7e09dd02e95c3",
            "avatarUrl": "/avatars/b2798bd5f8368f956bf7fab79d9432f0.svg",
            "fullname": "Jie Feng",
            "name": "JJ-TMT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09603",
            "authors": [
                {
                    "_id": "689e84dba4caabb4320e5cbe",
                    "name": "Skyler Hallinan",
                    "hidden": false
                },
                {
                    "_id": "689e84dba4caabb4320e5cbf",
                    "name": "Jaehun Jung",
                    "hidden": false
                },
                {
                    "_id": "689e84dba4caabb4320e5cc0",
                    "name": "Melanie Sclar",
                    "hidden": false
                },
                {
                    "_id": "689e84dba4caabb4320e5cc1",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "689e84dba4caabb4320e5cc2",
                    "name": "Abhilasha Ravichander",
                    "hidden": false
                },
                {
                    "_id": "689e84dba4caabb4320e5cc3",
                    "name": "Sahana Ramnath",
                    "hidden": false
                },
                {
                    "_id": "689e84dba4caabb4320e5cc4",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "689e84dba4caabb4320e5cc5",
                    "name": "Sai Praneeth Karimireddy",
                    "hidden": false
                },
                {
                    "_id": "689e84dba4caabb4320e5cc6",
                    "name": "Niloofar Mireshghallah",
                    "hidden": false
                },
                {
                    "_id": "689e84dba4caabb4320e5cc7",
                    "name": "Xiang Ren",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T08:35:16.000Z",
            "submittedOnDailyAt": "2025-08-14T23:22:59.419Z",
            "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram\n  Coverage",
            "submittedOnDailyBy": {
                "_id": "637d38fcb8e573d75bedc033",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d38fcb8e573d75bedc033/MbzfbMBGN2ajH7oZtitcB.png",
                "isPro": false,
                "fullname": "Skyler Hallinan",
                "user": "hallisky",
                "type": "user"
            },
            "summary": "Membership inference attacks serves as useful tool for fair use of language\nmodels, such as detecting potential copyright infringement and auditing data\nleakage. However, many current state-of-the-art attacks require access to\nmodels' hidden states or probability distribution, which prevents investigation\ninto more widely-used, API-access only models like GPT-4. In this work, we\nintroduce N-Gram Coverage Attack, a membership inference attack that relies\nsolely on text outputs from the target model, enabling attacks on completely\nblack-box models. We leverage the observation that models are more likely to\nmemorize and subsequently generate text patterns that were commonly observed in\ntheir training data. Specifically, to make a prediction on a candidate member,\nN-Gram Coverage Attack first obtains multiple model generations conditioned on\na prefix of the candidate. It then uses n-gram overlap metrics to compute and\naggregate the similarities of these outputs with the ground truth suffix; high\nsimilarities indicate likely membership. We first demonstrate on a diverse set\nof existing benchmarks that N-Gram Coverage Attack outperforms other black-box\nmethods while also impressively achieving comparable or even better performance\nto state-of-the-art white-box attacks - despite having access to only text\noutputs. Interestingly, we find that the success rate of our method scales with\nthe attack compute budget - as we increase the number of sequences generated\nfrom the target model conditioned on the prefix, attack performance tends to\nimprove. Having verified the accuracy of our method, we use it to investigate\npreviously unstudied closed OpenAI models on multiple domains. We find that\nmore recent models, such as GPT-4o, exhibit increased robustness to membership\ninference, suggesting an evolving trend toward improved privacy protections.",
            "upvotes": 0,
            "discussionId": "689e84dba4caabb4320e5cc8",
            "ai_summary": "N-Gram Coverage Attack is a black-box membership inference method that uses text outputs to detect data membership in language models, outperforming other black-box methods and achieving comparable results to white-box attacks.",
            "ai_keywords": [
                "membership inference attacks",
                "N-Gram Coverage Attack",
                "black-box models",
                "n-gram overlap metrics",
                "text outputs",
                "language models",
                "GPT-4",
                "privacy protections"
            ]
        },
        "publishedAt": "2025-08-13T04:35:16.000Z",
        "title": "The Surprising Effectiveness of Membership Inference with Simple N-Gram\n  Coverage",
        "summary": "Membership inference attacks serves as useful tool for fair use of language\nmodels, such as detecting potential copyright infringement and auditing data\nleakage. However, many current state-of-the-art attacks require access to\nmodels' hidden states or probability distribution, which prevents investigation\ninto more widely-used, API-access only models like GPT-4. In this work, we\nintroduce N-Gram Coverage Attack, a membership inference attack that relies\nsolely on text outputs from the target model, enabling attacks on completely\nblack-box models. We leverage the observation that models are more likely to\nmemorize and subsequently generate text patterns that were commonly observed in\ntheir training data. Specifically, to make a prediction on a candidate member,\nN-Gram Coverage Attack first obtains multiple model generations conditioned on\na prefix of the candidate. It then uses n-gram overlap metrics to compute and\naggregate the similarities of these outputs with the ground truth suffix; high\nsimilarities indicate likely membership. We first demonstrate on a diverse set\nof existing benchmarks that N-Gram Coverage Attack outperforms other black-box\nmethods while also impressively achieving comparable or even better performance\nto state-of-the-art white-box attacks - despite having access to only text\noutputs. Interestingly, we find that the success rate of our method scales with\nthe attack compute budget - as we increase the number of sequences generated\nfrom the target model conditioned on the prefix, attack performance tends to\nimprove. Having verified the accuracy of our method, we use it to investigate\npreviously unstudied closed OpenAI models on multiple domains. We find that\nmore recent models, such as GPT-4o, exhibit increased robustness to membership\ninference, suggesting an evolving trend toward improved privacy protections.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09603.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637d38fcb8e573d75bedc033",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/637d38fcb8e573d75bedc033/MbzfbMBGN2ajH7oZtitcB.png",
            "fullname": "Skyler Hallinan",
            "name": "hallisky",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.07321",
            "authors": [
                {
                    "_id": "689e506ea4caabb4320e5c9e",
                    "name": "Shubhra Ghosh",
                    "hidden": false
                },
                {
                    "_id": "689e506ea4caabb4320e5c9f",
                    "name": "Abhilekh Borah",
                    "hidden": false
                },
                {
                    "_id": "689e506ea4caabb4320e5ca0",
                    "name": "Aditya Kumar Guru",
                    "hidden": false
                },
                {
                    "_id": "689e506ea4caabb4320e5ca1",
                    "name": "Kripabandhu Ghosh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-10T12:27:52.000Z",
            "submittedOnDailyAt": "2025-08-14T19:41:47.410Z",
            "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated\n  Factual Question Answering",
            "submittedOnDailyBy": {
                "_id": "65425237ea69bcb6203c8d76",
                "avatarUrl": "/avatars/42953b27288faac8eb1397f194cecc66.svg",
                "isPro": false,
                "fullname": "Abhilekh Borah",
                "user": "abhilekhborah",
                "type": "user"
            },
            "summary": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available.",
            "upvotes": 0,
            "discussionId": "689e506fa4caabb4320e5ca2",
            "ai_summary": "ObfusQA, a novel framework with multi-tiered obfuscation levels, evaluates the robustness and adaptability of Large Language Models (LLMs) by examining their performance on obfuscated questions.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "ObfusQAte",
                "ObfusQA",
                "Named-Entity Indirection",
                "Distractor Indirection",
                "Contextual Overload",
                "hallucinated responses"
            ]
        },
        "publishedAt": "2025-08-10T08:27:52.000Z",
        "title": "ObfusQAte: A Proposed Framework to Evaluate LLM Robustness on Obfuscated\n  Factual Question Answering",
        "summary": "The rapid proliferation of Large Language Models (LLMs) has significantly\ncontributed to the development of equitable AI systems capable of factual\nquestion-answering (QA). However, no known study tests the LLMs' robustness\nwhen presented with obfuscated versions of questions. To systematically\nevaluate these limitations, we propose a novel technique, ObfusQAte and,\nleveraging the same, introduce ObfusQA, a comprehensive, first of its kind,\nframework with multi-tiered obfuscation levels designed to examine LLM\ncapabilities across three distinct dimensions: (i) Named-Entity Indirection,\n(ii) Distractor Indirection, and (iii) Contextual Overload. By capturing these\nfine-grained distinctions in language, ObfusQA provides a comprehensive\nbenchmark for evaluating LLM robustness and adaptability. Our study observes\nthat LLMs exhibit a tendency to fail or generate hallucinated responses when\nconfronted with these increasingly nuanced variations. To foster research in\nthis direction, we make ObfusQAte publicly available.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07321.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65425237ea69bcb6203c8d76",
            "avatarUrl": "/avatars/42953b27288faac8eb1397f194cecc66.svg",
            "fullname": "Abhilekh Borah",
            "name": "abhilekhborah",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]