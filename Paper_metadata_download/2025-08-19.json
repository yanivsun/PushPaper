[
    {
        "paper": {
            "id": "2508.11737",
            "authors": [
                {
                    "_id": "68a3e2c0b65388761d07448f",
                    "user": {
                        "_id": "658a8a837959448ef5500ce5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
                        "isPro": false,
                        "fullname": "Shiyin Lu",
                        "user": "runninglsy",
                        "type": "user"
                    },
                    "name": "Shiyin Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:40.445Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074490",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074491",
                    "user": {
                        "_id": "637aebed7ce76c3b834cea37",
                        "avatarUrl": "/avatars/78d6dd02d900e4a4b4fd89776b01f4fe.svg",
                        "isPro": false,
                        "fullname": "RainingXY",
                        "user": "xxyyy123",
                        "type": "user"
                    },
                    "name": "Yu Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:49.351Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074492",
                    "name": "Yuwei Hu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074493",
                    "user": {
                        "_id": "66ab4c8a1703f12f49583c6d",
                        "avatarUrl": "/avatars/59c77d4556edc049bb410e180813d5e3.svg",
                        "isPro": false,
                        "fullname": "zss",
                        "user": "Suikong",
                        "type": "user"
                    },
                    "name": "Shanshan Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:43.080Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074494",
                    "name": "Yanqing Ma",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074495",
                    "user": {
                        "_id": "6621ae16e774284ec1714f41",
                        "avatarUrl": "/avatars/0c91af9b001e810dc6b9748fadc9f734.svg",
                        "isPro": false,
                        "fullname": "Zhichao wei",
                        "user": "wzcjojo",
                        "type": "user"
                    },
                    "name": "Zhichao Wei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:07:22.136Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074496",
                    "user": {
                        "_id": "66842ad1eef2f5be9e113fb9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/SaEuI2ddyT6jzz_i84Jzp.png",
                        "isPro": false,
                        "fullname": "Yinglun Li",
                        "user": "Geralt-Lee",
                        "type": "user"
                    },
                    "name": "Yinglun Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:07:28.303Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074497",
                    "user": {
                        "_id": "65bef1410a16cbc80b905cbe",
                        "avatarUrl": "/avatars/6482efa09de4a089d4bcef83ab689421.svg",
                        "isPro": false,
                        "fullname": "Lunhao Duan",
                        "user": "Tumen04",
                        "type": "user"
                    },
                    "name": "Lunhao Duan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:07:34.397Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074498",
                    "name": "Jianshan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d074499",
                    "name": "Yuxuan Han",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449a",
                    "name": "Haijun Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449b",
                    "user": {
                        "_id": "6443cc8a9c1bd83bd19c90bf",
                        "avatarUrl": "/avatars/2da4f7930e77737da66082ef36e1e9b0.svg",
                        "isPro": false,
                        "fullname": "WANYING CHEN",
                        "user": "WANYING",
                        "type": "user"
                    },
                    "name": "Wanying Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:07:56.418Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449c",
                    "user": {
                        "_id": "64eda513fdab28c1058e68a8",
                        "avatarUrl": "/avatars/4149c680062eb1e855ed014439155d6e.svg",
                        "isPro": false,
                        "fullname": "Tjunke",
                        "user": "junketang",
                        "type": "user"
                    },
                    "name": "Junke Tang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-19T14:08:04.775Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449d",
                    "name": "Chengkun Hou",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449e",
                    "name": "Zhixing Du",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d07449f",
                    "name": "Tianli Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a0",
                    "name": "Wenjie Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a1",
                    "name": "Huping Ding",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a2",
                    "name": "Jiahe Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a3",
                    "name": "Wen Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a4",
                    "name": "Gui Hu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a5",
                    "name": "Yiliang Gu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a6",
                    "name": "Siran Yang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a7",
                    "name": "Jiamang Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a8",
                    "name": "Hailong Sun",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744a9",
                    "name": "Yibo Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744aa",
                    "name": "Hui Sun",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744ab",
                    "name": "Jinlong Huang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744ac",
                    "name": "Yuping He",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744ad",
                    "name": "Shengze Shi",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744ae",
                    "name": "Weihong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744af",
                    "name": "Guodong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b0",
                    "name": "Junpeng Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b1",
                    "name": "Sensen Gao",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b2",
                    "name": "Yi-Feng Wu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b3",
                    "name": "Sijia Chen",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b4",
                    "name": "Yuhui Chen",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b5",
                    "user": {
                        "_id": "63e1fde9419922d5a6d41a73",
                        "avatarUrl": "/avatars/8964cbbc64578f839a5a2773dcd565dd.svg",
                        "isPro": false,
                        "fullname": "Qing-Guo Chen",
                        "user": "cqgwin",
                        "type": "user"
                    },
                    "name": "Qing-Guo Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:46.841Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b6",
                    "name": "Zhao Xu",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b7",
                    "name": "Weihua Luo",
                    "hidden": false
                },
                {
                    "_id": "68a3e2c0b65388761d0744b8",
                    "name": "Kaifu Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-15T17:01:08.000Z",
            "submittedOnDailyAt": "2025-08-19T01:10:24.109Z",
            "title": "Ovis2.5 Technical Report",
            "submittedOnDailyBy": {
                "_id": "658a8a837959448ef5500ce5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
                "isPro": false,
                "fullname": "Shiyin Lu",
                "user": "runninglsy",
                "type": "user"
            },
            "summary": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution\nvisual perception and strong multimodal reasoning. Ovis2.5 integrates a\nnative-resolution vision transformer that processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyond linear chain-of-thought and perform reflection -- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensive\nfive-phase curriculum that progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement using DPO and GRPO. To scale these upgrades efficiently, we employ\nmultimodal data packing and hybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on-device scenarios. On the\nOpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a\nsubstantial improvement over its predecessor, Ovis2-8B, and achieving\nstate-of-the-art results among open-source MLLMs in the sub-40B parameter\nrange; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate\nscores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong\ncapabilities on grounding and video tasks, and achieves open-source SOTA at its\nscale for complex chart analysis.",
            "upvotes": 76,
            "discussionId": "68a3e2c0b65388761d0744b9",
            "githubRepo": "https://github.com/AIDC-AI/Ovis",
            "ai_summary": "Ovis2.5, a native-resolution vision transformer with multimodal reasoning, achieves state-of-the-art performance on various benchmarks through advanced training techniques and efficient scaling methods.",
            "ai_keywords": [
                "vision transformer",
                "native-resolution",
                "multimodal reasoning",
                "linear chain-of-thought",
                "reflection",
                "thinking mode",
                "five-phase curriculum",
                "DPO",
                "GRPO",
                "multimodal data packing",
                "hybrid parallelism",
                "OpenCompass",
                "MLLMs",
                "STEM benchmarks",
                "grounding",
                "video tasks",
                "complex chart analysis"
            ],
            "githubStars": 1150
        },
        "publishedAt": "2025-08-15T13:01:08.000Z",
        "title": "Ovis2.5 Technical Report",
        "summary": "We present Ovis2.5, a successor to Ovis2 designed for native-resolution\nvisual perception and strong multimodal reasoning. Ovis2.5 integrates a\nnative-resolution vision transformer that processes images at their native,\nvariable resolutions, avoiding the degradation from fixed-resolution tiling and\npreserving both fine detail and global layout -- crucial for visually dense\ncontent like complex charts. To strengthen reasoning, we train the model to\nmove beyond linear chain-of-thought and perform reflection -- including\nself-checking and revision. This advanced capability is exposed as an optional\n\"thinking mode\" at inference time, allowing users to trade latency for enhanced\naccuracy on difficult inputs. The model is trained via a comprehensive\nfive-phase curriculum that progressively builds its skills. The process begins\nwith foundational visual and multimodal pretraining, advances through\nlarge-scale instruction tuning, and culminates in alignment and reasoning\nenhancement using DPO and GRPO. To scale these upgrades efficiently, we employ\nmultimodal data packing and hybrid parallelism, yielding a significant\nend-to-end speedup. We release two open-source models: Ovis2.5-9B and\nOvis2.5-2B. The latter continues the \"small model, big performance\" philosophy\nof Ovis2, making it ideal for resource-constrained, on-device scenarios. On the\nOpenCompass multimodal leaderboard, Ovis2.5-9B averages 78.3, marking a\nsubstantial improvement over its predecessor, Ovis2-8B, and achieving\nstate-of-the-art results among open-source MLLMs in the sub-40B parameter\nrange; Ovis2.5-2B scores 73.9, establishing SOTA for its size. Beyond aggregate\nscores, Ovis2.5 achieves leading results on STEM benchmarks, exhibits strong\ncapabilities on grounding and video tasks, and achieves open-source SOTA at its\nscale for complex chart analysis.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11737.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "658a8a837959448ef5500ce5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/658a8a837959448ef5500ce5/4rb2RXrsgCDH80VnwZsEt.jpeg",
            "fullname": "Shiyin Lu",
            "name": "runninglsy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.10419",
            "authors": [
                {
                    "_id": "68a3e682b65388761d0744d6",
                    "name": "Juyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744d7",
                    "name": "Rongchen Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744d8",
                    "name": "Wei Wei",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744d9",
                    "name": "Yufeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744da",
                    "name": "Mo Yu",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744db",
                    "name": "Jie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744dc",
                    "name": "Jin Xu",
                    "hidden": false
                },
                {
                    "_id": "68a3e682b65388761d0744dd",
                    "user": {
                        "_id": "650f0fac11f3210cf7a8a849",
                        "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
                        "isPro": false,
                        "fullname": "Liyan Xu",
                        "user": "lxucs",
                        "type": "user"
                    },
                    "name": "Liyan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:40.740Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-14T07:52:09.000Z",
            "submittedOnDailyAt": "2025-08-19T01:21:34.597Z",
            "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning",
            "submittedOnDailyBy": {
                "_id": "650f0fac11f3210cf7a8a849",
                "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
                "isPro": false,
                "fullname": "Liyan Xu",
                "user": "lxucs",
                "type": "user"
            },
            "summary": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG",
            "upvotes": 46,
            "discussionId": "68a3e683b65388761d0744de",
            "githubRepo": "https://github.com/EternityJune25/ComoRAG",
            "ai_summary": "ComoRAG, an iterative retrieval-based approach, enhances long-context narrative comprehension by dynamically updating memory and generating probing queries, outperforming traditional RAG methods.",
            "ai_keywords": [
                "retrieval-based approaches",
                "RAG methods",
                "ComoRAG",
                "iterative reasoning cycles",
                "dynamic memory workspace",
                "probing queries",
                "global memory pool",
                "long-context narrative comprehension",
                "stateful reasoning"
            ],
            "githubStars": 60
        },
        "publishedAt": "2025-08-14T03:52:09.000Z",
        "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long\n  Narrative Reasoning",
        "summary": "Narrative comprehension on long stories and novels has been a challenging\ndomain attributed to their intricate plotlines and entangled, often evolving\nrelations among characters and entities. Given the LLM's diminished reasoning\nover extended context and high computational cost, retrieval-based approaches\nremain a pivotal role in practice. However, traditional RAG methods can fall\nshort due to their stateless, single-step retrieval process, which often\noverlooks the dynamic nature of capturing interconnected relations within\nlong-range context. In this work, we propose ComoRAG, holding the principle\nthat narrative reasoning is not a one-shot process, but a dynamic, evolving\ninterplay between new evidence acquisition and past knowledge consolidation,\nanalogous to human cognition when reasoning with memory-related signals in the\nbrain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes\niterative reasoning cycles while interacting with a dynamic memory workspace.\nIn each cycle, it generates probing queries to devise new exploratory paths,\nthen integrates the retrieved evidence of new aspects into a global memory\npool, thereby supporting the emergence of a coherent context for the query\nresolution. Across four challenging long-context narrative benchmarks (200K+\ntokens), ComoRAG outperforms strong RAG baselines with consistent relative\ngains up to 11% compared to the strongest baseline. Further analysis reveals\nthat ComoRAG is particularly advantageous for complex queries requiring global\ncomprehension, offering a principled, cognitively motivated paradigm for\nretrieval-based long context comprehension towards stateful reasoning. Our code\nis publicly released at https://github.com/EternityJune25/ComoRAG",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10419.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650f0fac11f3210cf7a8a849",
            "avatarUrl": "/avatars/687d56c3a6d4f5cdb34e424cdcff954d.svg",
            "fullname": "Liyan Xu",
            "name": "lxucs",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13154",
            "authors": [
                {
                    "_id": "68a3dfefb65388761d074471",
                    "name": "Zhaoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074472",
                    "name": "Tianqi Liu",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074473",
                    "name": "Long Zhuo",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074474",
                    "name": "Jiawei Ren",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074475",
                    "name": "Zeng Tao",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074476",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074477",
                    "name": "Fangzhou Hong",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074478",
                    "name": "Liang Pan",
                    "hidden": false
                },
                {
                    "_id": "68a3dfefb65388761d074479",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66d347eebb76fb26eedb256e/ll8ni6nZZorhKA7jCZgT_.mp4"
            ],
            "publishedAt": "2025-08-18T17:59:55.000Z",
            "submittedOnDailyAt": "2025-08-19T00:55:49.901Z",
            "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy",
            "submittedOnDailyBy": {
                "_id": "66d347eebb76fb26eedb256e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d347eebb76fb26eedb256e/iCPF7GkmZu--XCsWzoucl.jpeg",
                "isPro": false,
                "fullname": "tianqi liu",
                "user": "tqliu",
                "type": "user"
            },
            "summary": "We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.",
            "upvotes": 44,
            "discussionId": "68a3dfefb65388761d07447a",
            "projectPage": "https://4dnex.github.io/",
            "githubRepo": "https://github.com/3DTopia/4DNeX",
            "ai_summary": "4DNeX generates high-quality dynamic 3D scene representations from a single image using a fine-tuned pretrained video diffusion model, outperforming existing methods in efficiency and generalizability.",
            "ai_keywords": [
                "feed-forward framework",
                "4D scene representations",
                "video diffusion model",
                "4DNeX-10M",
                "6D video representation",
                "RGB",
                "XYZ sequences",
                "dynamic point clouds",
                "novel-view video synthesis",
                "generative 4D world models"
            ],
            "githubStars": 95
        },
        "publishedAt": "2025-08-18T13:59:55.000Z",
        "title": "4DNeX: Feed-Forward 4D Generative Modeling Made Easy",
        "summary": "We present 4DNeX, the first feed-forward framework for generating 4D (i.e.,\ndynamic 3D) scene representations from a single image. In contrast to existing\nmethods that rely on computationally intensive optimization or require\nmulti-frame video inputs, 4DNeX enables efficient, end-to-end image-to-4D\ngeneration by fine-tuning a pretrained video diffusion model. Specifically, 1)\nto alleviate the scarcity of 4D data, we construct 4DNeX-10M, a large-scale\ndataset with high-quality 4D annotations generated using advanced\nreconstruction approaches. 2) we introduce a unified 6D video representation\nthat jointly models RGB and XYZ sequences, facilitating structured learning of\nboth appearance and geometry. 3) we propose a set of simple yet effective\nadaptation strategies to repurpose pretrained video diffusion models for 4D\nmodeling. 4DNeX produces high-quality dynamic point clouds that enable\nnovel-view video synthesis. Extensive experiments demonstrate that 4DNeX\noutperforms existing 4D generation methods in efficiency and generalizability,\noffering a scalable solution for image-to-4D modeling and laying the foundation\nfor generative 4D world models that simulate dynamic scene evolution.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66d347eebb76fb26eedb256e/ll8ni6nZZorhKA7jCZgT_.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13154.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66d347eebb76fb26eedb256e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66d347eebb76fb26eedb256e/iCPF7GkmZu--XCsWzoucl.jpeg",
            "fullname": "tianqi liu",
            "name": "tqliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.12811",
            "authors": [
                {
                    "_id": "68a3e68db65388761d0744e0",
                    "user": {
                        "_id": "63463bc4547c70e4b7d3009f",
                        "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
                        "isPro": false,
                        "fullname": "Yikai Wang",
                        "user": "yikaiwang",
                        "type": "user"
                    },
                    "name": "Yikai Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:19.759Z",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e1",
                    "name": "Zhouxia Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e2",
                    "name": "Zhonghua Wu",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e3",
                    "name": "Qingyi Tao",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e4",
                    "name": "Kang Liao",
                    "hidden": false
                },
                {
                    "_id": "68a3e68db65388761d0744e5",
                    "name": "Chen Change Loy",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63463bc4547c70e4b7d3009f/Oc1rdp6mW2ISy-bEA9Uqy.mp4"
            ],
            "publishedAt": "2025-08-18T10:47:37.000Z",
            "submittedOnDailyAt": "2025-08-19T02:48:33.504Z",
            "title": "Next Visual Granularity Generation",
            "submittedOnDailyBy": {
                "_id": "63463bc4547c70e4b7d3009f",
                "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
                "isPro": false,
                "fullname": "Yikai Wang",
                "user": "yikaiwang",
                "type": "user"
            },
            "summary": "We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.",
            "upvotes": 37,
            "discussionId": "68a3e68db65388761d0744e6",
            "projectPage": "https://yikai-wang.github.io/nvg/",
            "githubRepo": "https://github.com/Yikai-Wang/nvg",
            "ai_summary": "A novel Next Visual Granularity (NVG) framework generates images by iteratively refining a sequence of visual granularities, outperforming existing methods in class-conditional image generation.",
            "ai_keywords": [
                "Next Visual Granularity (NVG)",
                "visual granularity sequence",
                "global layout",
                "fine details",
                "hierarchical",
                "layered representation",
                "class-conditional image generation",
                "ImageNet dataset",
                "FID scores"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-08-18T06:47:37.000Z",
        "title": "Next Visual Granularity Generation",
        "summary": "We propose a novel approach to image generation by decomposing an image into\na structured sequence, where each element in the sequence shares the same\nspatial resolution but differs in the number of unique tokens used, capturing\ndifferent level of visual granularity. Image generation is carried out through\nour newly introduced Next Visual Granularity (NVG) generation framework, which\ngenerates a visual granularity sequence beginning from an empty image and\nprogressively refines it, from global layout to fine details, in a structured\nmanner. This iterative process encodes a hierarchical, layered representation\nthat offers fine-grained control over the generation process across multiple\ngranularity levels. We train a series of NVG models for class-conditional image\ngeneration on the ImageNet dataset and observe clear scaling behavior. Compared\nto the VAR series, NVG consistently outperforms it in terms of FID scores (3.30\n-> 3.03, 2.57 ->2.44, 2.09 -> 2.06). We also conduct extensive analysis to\nshowcase the capability and potential of the NVG framework. Our code and models\nwill be released.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63463bc4547c70e4b7d3009f/Oc1rdp6mW2ISy-bEA9Uqy.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12811.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63463bc4547c70e4b7d3009f",
            "avatarUrl": "/avatars/6e5350fd998f0a7a4143d7504218164a.svg",
            "fullname": "Yikai Wang",
            "name": "yikaiwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09834",
            "authors": [
                {
                    "_id": "68a31244b65388761d074306",
                    "name": "Weigao Sun",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074307",
                    "name": "Jiaxi Hu",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074308",
                    "name": "Yucheng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074309",
                    "name": "Jusen Du",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430a",
                    "user": {
                        "_id": "66ea643899af9ac3463639b1",
                        "avatarUrl": "/avatars/252d470e761a57834dee3dbc60dfefed.svg",
                        "isPro": false,
                        "fullname": "Disen Lan",
                        "user": "landisen",
                        "type": "user"
                    },
                    "name": "Disen Lan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:05:04.185Z",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430b",
                    "name": "Kexin Wang",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430c",
                    "name": "Tong Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430d",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430e",
                    "name": "Yu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d07430f",
                    "name": "Xiaoyu Mo",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074310",
                    "name": "Daizong Liu",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074311",
                    "name": "Yuxuan Liang",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074312",
                    "name": "Wenliang Chen",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074313",
                    "name": "Guoqi Li",
                    "hidden": false
                },
                {
                    "_id": "68a31244b65388761d074314",
                    "name": "Yu Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T14:13:46.000Z",
            "submittedOnDailyAt": "2025-08-19T01:03:34.076Z",
            "title": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "6246bb33da617c00b48e4d92",
                "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
                "isPro": false,
                "fullname": "Weigao Sun",
                "user": "weigao266",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.",
            "upvotes": 33,
            "discussionId": "68a31244b65388761d074315",
            "projectPage": "https://github.com/weigao266/Awesome-Efficient-Arch",
            "githubRepo": "https://github.com/weigao266/Awesome-Efficient-Arch",
            "ai_summary": "This survey examines innovative architectures for large language models to enhance efficiency, covering linear and sparse sequence modeling, efficient attention mechanisms, sparse mixture-of-experts, hybrid models, and diffusion LLMs.",
            "ai_keywords": [
                "transformer models",
                "linear sequence modeling",
                "sparse sequence modeling",
                "efficient full attention",
                "sparse mixture-of-experts",
                "hybrid model architectures",
                "diffusion LLMs"
            ],
            "githubStars": 112
        },
        "publishedAt": "2025-08-13T10:13:46.000Z",
        "title": "Speed Always Wins: A Survey on Efficient Architectures for Large\n  Language Models",
        "summary": "Large Language Models (LLMs) have delivered impressive results in language\nunderstanding, generation, reasoning, and pushes the ability boundary of\nmultimodal models. Transformer models, as the foundation of modern LLMs, offer\na strong baseline with excellent scaling properties. However, the traditional\ntransformer architecture requires substantial computations and poses\nsignificant obstacles for large-scale training and practical deployment. In\nthis survey, we offer a systematic examination of innovative LLM architectures\nthat address the inherent limitations of transformers and boost the efficiency.\nStarting from language modeling, this survey covers the background and\ntechnical details of linear and sparse sequence modeling methods, efficient\nfull attention variants, sparse mixture-of-experts, hybrid model architectures\nincorporating the above techniques, and emerging diffusion LLMs. Additionally,\nwe discuss applications of these techniques to other modalities and consider\ntheir wider implications for developing scalable, resource-aware foundation\nmodels. By grouping recent studies into the above category, this survey\npresents a blueprint of modern efficient LLM architectures, and we hope this\ncould help motivate future research toward more efficient, versatile AI\nsystems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09834.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6246bb33da617c00b48e4d92",
            "avatarUrl": "/avatars/0304a9f6eb7f5dee4d933d03222f94e9.svg",
            "fullname": "Weigao Sun",
            "name": "weigao266",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.11383",
            "authors": [
                {
                    "_id": "68a2fa12a4caabb4320e6525",
                    "user": {
                        "_id": "654621f45cd5692b3a9d08cb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NJj1FJyYHNPRXb0QBXjv-.jpeg",
                        "isPro": false,
                        "fullname": "Mikhail Seleznyov",
                        "user": "myyycroft",
                        "type": "user"
                    },
                    "name": "Mikhail Seleznyov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:05:10.952Z",
                    "hidden": false
                },
                {
                    "_id": "68a2fa12a4caabb4320e6526",
                    "name": "Mikhail Chaichuk",
                    "hidden": false
                },
                {
                    "_id": "68a2fa12a4caabb4320e6527",
                    "name": "Gleb Ershov",
                    "hidden": false
                },
                {
                    "_id": "68a2fa12a4caabb4320e6528",
                    "user": {
                        "_id": "605473729d7c1d4d81b7e52b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1662046050710-605473729d7c1d4d81b7e52b.jpeg",
                        "isPro": false,
                        "fullname": "Alexander Panchenko",
                        "user": "apanc",
                        "type": "user"
                    },
                    "name": "Alexander Panchenko",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T11:21:58.330Z",
                    "hidden": false
                },
                {
                    "_id": "68a2fa12a4caabb4320e6529",
                    "name": "Elena Tutubalina",
                    "hidden": false
                },
                {
                    "_id": "68a2fa12a4caabb4320e652a",
                    "name": "Oleg Somov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-15T10:32:50.000Z",
            "submittedOnDailyAt": "2025-08-19T08:50:30.122Z",
            "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness\n  Methods for LLMs",
            "submittedOnDailyBy": {
                "_id": "654621f45cd5692b3a9d08cb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NJj1FJyYHNPRXb0QBXjv-.jpeg",
                "isPro": false,
                "fullname": "Mikhail Seleznyov",
                "user": "myyycroft",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.",
            "upvotes": 32,
            "discussionId": "68a2fa12a4caabb4320e652b",
            "githubRepo": "https://github.com/AIRI-Institute/when-punctuation-matters",
            "ai_summary": "A systematic evaluation of prompt robustness methods for Large Language Models across various models and tasks reveals insights into their effectiveness against format perturbations.",
            "ai_keywords": [
                "Large Language Models",
                "prompt robustness",
                "fine-tuned",
                "in-context learning",
                "distribution shifts",
                "GPT-4.1",
                "DeepSeek V3"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-08-15T06:32:50.000Z",
        "title": "When Punctuation Matters: A Large-Scale Comparison of Prompt Robustness\n  Methods for LLMs",
        "summary": "Large Language Models (LLMs) are highly sensitive to subtle, non-semantic\nvariations in prompt phrasing and formatting. In this work, we present the\nfirst systematic evaluation of 5 methods for improving prompt robustness within\na unified experimental framework. We benchmark these techniques on 8 models\nfrom Llama, Qwen and Gemma families across 52 tasks from Natural Instructions\ndataset. Our evaluation covers robustness methods from both fine-tuned and\nin-context learning paradigms, and tests their generalization against multiple\ntypes of distribution shifts. Finally, we extend our analysis to GPT-4.1 and\nDeepSeek V3 to assess frontier models' current robustness to format\nperturbations. Our findings offer actionable insights into the relative\neffectiveness of these robustness methods, enabling practitioners to make\ninformed decisions when aiming for stable and reliable LLM performance in\nreal-world applications. Code:\nhttps://github.com/AIRI-Institute/when-punctuation-matters.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11383.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654621f45cd5692b3a9d08cb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/NJj1FJyYHNPRXb0QBXjv-.jpeg",
            "fullname": "Mikhail Seleznyov",
            "name": "myyycroft",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13142",
            "authors": [
                {
                    "_id": "68a40b0db65388761d074558",
                    "user": {
                        "_id": "652d06833b5997ed71ce5c46",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
                        "isPro": false,
                        "fullname": "Zhongang Cai",
                        "user": "caizhongang",
                        "type": "user"
                    },
                    "name": "Zhongang Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:14.005Z",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074559",
                    "name": "Yubo Wang",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07455a",
                    "name": "Qingping Sun",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07455b",
                    "name": "Ruisi Wang",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07455c",
                    "name": "Chenyang Gu",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07455d",
                    "name": "Wanqi Yin",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07455e",
                    "name": "Zhiqian Lin",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07455f",
                    "name": "Zhitao Yang",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074560",
                    "name": "Chen Wei",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074561",
                    "name": "Xuanke Shi",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074562",
                    "name": "Kewang Deng",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074563",
                    "name": "Xiaoyang Han",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074564",
                    "name": "Zukai Chen",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074565",
                    "name": "Jiaqi Li",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074566",
                    "name": "Xiangyu Fan",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074567",
                    "name": "Hanming Deng",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074568",
                    "name": "Lewei Lu",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d074569",
                    "name": "Bo Li",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07456a",
                    "name": "Ziwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07456b",
                    "name": "Quan Wang",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07456c",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "68a40b0db65388761d07456d",
                    "user": {
                        "_id": "6626a471430a124253f197c8",
                        "avatarUrl": "/avatars/f5747fdbe495d1296fed9d16d8c95857.svg",
                        "isPro": false,
                        "fullname": "yl-1993",
                        "user": "yl-1993",
                        "type": "user"
                    },
                    "name": "Lei Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:11.955Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T17:55:17.000Z",
            "submittedOnDailyAt": "2025-08-19T03:58:00.897Z",
            "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
            "submittedOnDailyBy": {
                "_id": "652d06833b5997ed71ce5c46",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
                "isPro": false,
                "fullname": "Zhongang Cai",
                "user": "caizhongang",
                "type": "user"
            },
            "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
            "upvotes": 22,
            "discussionId": "68a40b0eb65388761d07456e",
            "ai_summary": "Recent multi-modal models, including GPT-5, show significant progress in spatial intelligence but still lag behind human performance across various benchmarks.",
            "ai_keywords": [
                "multi-modal models",
                "spatial understanding",
                "reasoning",
                "artificial general intelligence",
                "GPT-5",
                "spatial tasks",
                "benchmarks",
                "spatial intelligence",
                "qualitative evaluation"
            ]
        },
        "publishedAt": "2025-08-18T13:55:17.000Z",
        "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
        "summary": "Multi-modal models have achieved remarkable progress in recent years.\nNevertheless, they continue to exhibit notable limitations in spatial\nunderstanding and reasoning, which are fundamental capabilities to achieving\nartificial general intelligence. With the recent release of GPT-5, allegedly\nthe most powerful AI model to date, it is timely to examine where the leading\nmodels stand on the path toward spatial intelligence. First, we propose a\ncomprehensive taxonomy of spatial tasks that unifies existing benchmarks and\ndiscuss the challenges in ensuring fair evaluation. We then evaluate\nstate-of-the-art proprietary and open-source models on eight key benchmarks, at\na cost exceeding one billion total tokens. Our empirical study reveals that (1)\nGPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2)\nstill falls short of human performance across a broad spectrum of tasks.\nMoreover, we (3) identify the more challenging spatial intelligence problems\nfor multi-modal models, and (4) proprietary models do not exhibit a decisive\nadvantage when facing the most difficult problems. In addition, we conduct a\nqualitative evaluation across a diverse set of scenarios that are intuitive for\nhumans yet fail even the most advanced multi-modal models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13142.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652d06833b5997ed71ce5c46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/xZTXEcnEogEmBm_ledJQr.jpeg",
            "fullname": "Zhongang Cai",
            "name": "caizhongang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12782",
            "authors": [
                {
                    "_id": "68a42221b65388761d0745b9",
                    "name": "Petr Anokhin",
                    "hidden": false
                },
                {
                    "_id": "68a42221b65388761d0745ba",
                    "user": {
                        "_id": "657332fc3e0cb21bc7ae7772",
                        "avatarUrl": "/avatars/e7f50ed81ec0db6858cbd9f9782a6bfa.svg",
                        "isPro": false,
                        "fullname": "Roman Khalikov",
                        "user": "roxal",
                        "type": "user"
                    },
                    "name": "Roman Khalikov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:38.614Z",
                    "hidden": false
                },
                {
                    "_id": "68a42221b65388761d0745bb",
                    "name": "Stefan Rebrikov",
                    "hidden": false
                },
                {
                    "_id": "68a42221b65388761d0745bc",
                    "user": {
                        "_id": "67aadd77653ea1bf5ec27740",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67aadd77653ea1bf5ec27740/kITzDTYje-J7itQTz_gHy.png",
                        "isPro": false,
                        "fullname": "Viktor Volkov",
                        "user": "ShiftingBorder",
                        "type": "user"
                    },
                    "name": "Viktor Volkov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T15:58:46.930Z",
                    "hidden": false
                },
                {
                    "_id": "68a42221b65388761d0745bd",
                    "name": "Artyom Sorokin",
                    "hidden": false
                },
                {
                    "_id": "68a42221b65388761d0745be",
                    "name": "Vincent Bissonnette",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64f734b08e7fa6529fb693f1/5lYAeJYKFan23QhVoE9LA.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64f734b08e7fa6529fb693f1/l-4F3jzHLXRdJxQNq2NGm.png"
            ],
            "publishedAt": "2025-08-18T09:59:02.000Z",
            "submittedOnDailyAt": "2025-08-19T06:22:16.909Z",
            "title": "HeroBench: A Benchmark for Long-Horizon Planning and Structured\n  Reasoning in Virtual Worlds",
            "submittedOnDailyBy": {
                "_id": "64f734b08e7fa6529fb693f1",
                "avatarUrl": "/avatars/a50c7e1d22174f06fbaf99e8caca7c37.svg",
                "isPro": false,
                "fullname": "Petr Anokhin",
                "user": "petranokhin",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have shown remarkable capabilities in isolated\nstep-by-step reasoning tasks such as mathematics and programming, but their\nproficiency in long-horizon planning, where solutions require extended,\nstructured sequences of interdependent actions, remains underexplored. Existing\nbenchmarks typically assess LLMs through abstract or low-dimensional\nalgorithmic tasks, failing to capture the complexity of realistic planning\nenvironments. We introduce HeroBench, a novel benchmark designed specifically\nto evaluate long-horizon planning and structured reasoning within complex\nRPG-inspired virtual worlds. HeroBench provides a rigorously constructed\ndataset of tasks covering a wide range of difficulties, a simulated environment\nto execute and validate agent plans, and detailed analytical tools for\nevaluating model performance. Tasks challenge models to formulate strategic\nplans, efficiently gather resources, master necessary skills, craft equipment,\nand defeat adversaries, reflecting practical scenarios' layered dependencies\nand constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning\nboth open-source and proprietary models, including the GPT-5 family, reveals\nsubstantial performance disparities rarely observed in conventional reasoning\nbenchmarks. Detailed error analysis further uncovers specific weaknesses in\ncurrent models' abilities to generate robust high-level plans and reliably\nexecute structured actions. HeroBench thus not only significantly advances the\nevaluation of LLM reasoning but also provides a flexible, scalable foundation\nfor future research into advanced, autonomous planning in virtual environments.",
            "upvotes": 21,
            "discussionId": "68a42222b65388761d0745bf",
            "githubRepo": "https://github.com/stefanrer/HeroBench",
            "ai_summary": "HeroBench evaluates long-horizon planning and structured reasoning in complex virtual worlds, revealing performance disparities in state-of-the-art LLMs.",
            "ai_keywords": [
                "large language models",
                "long-horizon planning",
                "structured reasoning",
                "HeroBench",
                "benchmark",
                "RPG-inspired virtual worlds",
                "strategic plans",
                "resource gathering",
                "skill mastery",
                "equipment crafting",
                "adversary defeat",
                "error analysis",
                "autonomous planning"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-08-18T05:59:02.000Z",
        "title": "HeroBench: A Benchmark for Long-Horizon Planning and Structured\n  Reasoning in Virtual Worlds",
        "summary": "Large language models (LLMs) have shown remarkable capabilities in isolated\nstep-by-step reasoning tasks such as mathematics and programming, but their\nproficiency in long-horizon planning, where solutions require extended,\nstructured sequences of interdependent actions, remains underexplored. Existing\nbenchmarks typically assess LLMs through abstract or low-dimensional\nalgorithmic tasks, failing to capture the complexity of realistic planning\nenvironments. We introduce HeroBench, a novel benchmark designed specifically\nto evaluate long-horizon planning and structured reasoning within complex\nRPG-inspired virtual worlds. HeroBench provides a rigorously constructed\ndataset of tasks covering a wide range of difficulties, a simulated environment\nto execute and validate agent plans, and detailed analytical tools for\nevaluating model performance. Tasks challenge models to formulate strategic\nplans, efficiently gather resources, master necessary skills, craft equipment,\nand defeat adversaries, reflecting practical scenarios' layered dependencies\nand constraints. Our extensive evaluation of 25 state-of-the-art LLMs, spanning\nboth open-source and proprietary models, including the GPT-5 family, reveals\nsubstantial performance disparities rarely observed in conventional reasoning\nbenchmarks. Detailed error analysis further uncovers specific weaknesses in\ncurrent models' abilities to generate robust high-level plans and reliably\nexecute structured actions. HeroBench thus not only significantly advances the\nevaluation of LLM reasoning but also provides a flexible, scalable foundation\nfor future research into advanced, autonomous planning in virtual environments.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64f734b08e7fa6529fb693f1/5lYAeJYKFan23QhVoE9LA.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64f734b08e7fa6529fb693f1/l-4F3jzHLXRdJxQNq2NGm.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12782.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64f734b08e7fa6529fb693f1",
            "avatarUrl": "/avatars/a50c7e1d22174f06fbaf99e8caca7c37.svg",
            "fullname": "Petr Anokhin",
            "name": "petranokhin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.13009",
            "authors": [
                {
                    "_id": "68a3e934b65388761d0744e8",
                    "name": "Xianglong He",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744e9",
                    "name": "Chunli Peng",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744ea",
                    "name": "Zexiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744eb",
                    "name": "Boyang Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744ec",
                    "name": "Yifan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744ed",
                    "name": "Qi Cui",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744ee",
                    "name": "Fei Kang",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744ef",
                    "name": "Biao Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f0",
                    "name": "Mengyin An",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f1",
                    "name": "Yangyang Ren",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f2",
                    "name": "Baixin Xu",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f3",
                    "name": "Hao-Xiang Guo",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f4",
                    "name": "Kaixiong Gong",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f5",
                    "name": "Cyrus Wu",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f6",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f7",
                    "name": "Xuchen Song",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f8",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744f9",
                    "name": "Eric Li",
                    "hidden": false
                },
                {
                    "_id": "68a3e934b65388761d0744fa",
                    "name": "Yahui Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T15:28:53.000Z",
            "submittedOnDailyAt": "2025-08-19T01:32:45.902Z",
            "title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive\n  World Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in interactive video generations have demonstrated diffusion\nmodel's potential as world models by capturing complex physical dynamics and\ninteractive behaviors. However, existing interactive world models depend on\nbidirectional attention and lengthy inference steps, severely limiting\nreal-time performance. Consequently, they are hard to simulate real-world\ndynamics, where outcomes must update instantaneously based on historical\ncontext and current actions. To address this, we present Matrix-Game 2.0, an\ninteractive world model generates long videos on-the-fly via few-step\nauto-regressive diffusion. Our framework consists of three key components: (1)\nA scalable data production pipeline for Unreal Engine and GTA5 environments to\neffectively produce massive amounts (about 1200 hours) of video data with\ndiverse interaction annotations; (2) An action injection module that enables\nframe-level mouse and keyboard inputs as interactive conditions; (3) A few-step\ndistillation based on the casual architecture for real-time and streaming video\ngeneration. Matrix Game 2.0 can generate high-quality minute-level videos\nacross diverse scenes at an ultra-fast speed of 25 FPS. We open-source our\nmodel weights and codebase to advance research in interactive world modeling.",
            "upvotes": 15,
            "discussionId": "68a3e934b65388761d0744fb",
            "projectPage": "https://matrix-game-v2.github.io/",
            "githubRepo": "https://github.com/SkyworkAI/Matrix-Game/tree/main/Matrix-Game-2",
            "ai_summary": "Matrix-Game 2.0 generates real-time interactive videos using few-step auto-regressive diffusion, addressing the limitations of lengthy inference in existing models.",
            "ai_keywords": [
                "diffusion model",
                "world models",
                "bidirectional attention",
                "interactive world models",
                "few-step auto-regressive diffusion",
                "scalable data production pipeline",
                "Unreal Engine",
                "GTA5",
                "action injection module",
                "frame-level mouse and keyboard inputs",
                "few-step distillation",
                "casual architecture",
                "real-time video generation",
                "streaming video generation"
            ],
            "githubStars": 1434
        },
        "publishedAt": "2025-08-18T11:28:53.000Z",
        "title": "Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive\n  World Model",
        "summary": "Recent advances in interactive video generations have demonstrated diffusion\nmodel's potential as world models by capturing complex physical dynamics and\ninteractive behaviors. However, existing interactive world models depend on\nbidirectional attention and lengthy inference steps, severely limiting\nreal-time performance. Consequently, they are hard to simulate real-world\ndynamics, where outcomes must update instantaneously based on historical\ncontext and current actions. To address this, we present Matrix-Game 2.0, an\ninteractive world model generates long videos on-the-fly via few-step\nauto-regressive diffusion. Our framework consists of three key components: (1)\nA scalable data production pipeline for Unreal Engine and GTA5 environments to\neffectively produce massive amounts (about 1200 hours) of video data with\ndiverse interaction annotations; (2) An action injection module that enables\nframe-level mouse and keyboard inputs as interactive conditions; (3) A few-step\ndistillation based on the casual architecture for real-time and streaming video\ngeneration. Matrix Game 2.0 can generate high-quality minute-level videos\nacross diverse scenes at an ultra-fast speed of 25 FPS. We open-source our\nmodel weights and codebase to advance research in interactive world modeling.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13009.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.11598",
            "authors": [
                {
                    "_id": "68a2f5eda4caabb4320e651f",
                    "user": {
                        "_id": "605a5f796ce6cabbb3474b55",
                        "avatarUrl": "/avatars/971866aa240126e480bbe01eff64c009.svg",
                        "isPro": false,
                        "fullname": "Greta Tuckute",
                        "user": "gretatuckute",
                        "type": "user"
                    },
                    "name": "Greta Tuckute",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T13:23:51.680Z",
                    "hidden": false
                },
                {
                    "_id": "68a2f5eda4caabb4320e6520",
                    "user": {
                        "_id": "646d3cae4220471ca0c6cb13",
                        "avatarUrl": "/avatars/30e47db03a23d49d946cc5248b28959a.svg",
                        "isPro": false,
                        "fullname": "Klemen Kotar",
                        "user": "klemenk",
                        "type": "user"
                    },
                    "name": "Klemen Kotar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:28:40.445Z",
                    "hidden": false
                },
                {
                    "_id": "68a2f5eda4caabb4320e6521",
                    "name": "Evelina Fedorenko",
                    "hidden": false
                },
                {
                    "_id": "68a2f5eda4caabb4320e6522",
                    "name": "Daniel L. K. Yamins",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/0PS299xjgGwi5ALMpgH4E.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/M4ep8C1xZlZs8wfu-nERr.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/Dalxatim74-G-5MraCwe_.mp4"
            ],
            "publishedAt": "2025-08-15T17:06:04.000Z",
            "submittedOnDailyAt": "2025-08-19T01:32:02.566Z",
            "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens",
            "submittedOnDailyBy": {
                "_id": "646d3cae4220471ca0c6cb13",
                "avatarUrl": "/avatars/30e47db03a23d49d946cc5248b28959a.svg",
                "isPro": false,
                "fullname": "Klemen Kotar",
                "user": "klemenk",
                "type": "user"
            },
            "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete cochlear tokens.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.",
            "upvotes": 12,
            "discussionId": "68a2f5eda4caabb4320e6523",
            "projectPage": "https://tukoresearch.github.io/auristream-speech/",
            "ai_summary": "AuriStream, a biologically inspired two-stage model, encodes speech using cochlear tokens and an autoregressive sequence model, achieving state-of-the-art performance on speech tasks and generating interpretable audio continuations.",
            "ai_keywords": [
                "cochlear tokens",
                "autoregressive sequence model",
                "human auditory processing hierarchy",
                "human cochlea",
                "lexical semantics",
                "spectrogram space"
            ]
        },
        "publishedAt": "2025-08-15T13:06:04.000Z",
        "title": "Representing Speech Through Autoregressive Prediction of Cochlear Tokens",
        "summary": "We introduce AuriStream, a biologically inspired model for encoding speech\nvia a two-stage framework inspired by the human auditory processing hierarchy.\nThe first stage transforms raw audio into a time-frequency representation based\non the human cochlea, from which we extract discrete cochlear tokens.\nThe second stage applies an autoregressive sequence model over the cochlear\ntokens. AuriStream learns meaningful phoneme and word representations, and\nstate-of-the-art lexical semantics. AuriStream shows competitive performance on\ndiverse downstream SUPERB speech tasks. Complementing AuriStream's strong\nrepresentational capabilities, it generates continuations of audio which can be\nvisualized in a spectrogram space and decoded back into audio, providing\ninsights into the model's predictions. In summary, we present a two-stage\nframework for speech representation learning to advance the development of more\nhuman-like models that efficiently handle a range of speech-based tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/0PS299xjgGwi5ALMpgH4E.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/M4ep8C1xZlZs8wfu-nERr.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/Dalxatim74-G-5MraCwe_.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11598.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646d3cae4220471ca0c6cb13",
            "avatarUrl": "/avatars/30e47db03a23d49d946cc5248b28959a.svg",
            "fullname": "Klemen Kotar",
            "name": "klemenk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12945",
            "authors": [
                {
                    "_id": "68a3e992b65388761d0744fd",
                    "name": "Jianshu Zeng",
                    "hidden": false
                },
                {
                    "_id": "68a3e992b65388761d0744fe",
                    "name": "Yuxuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68a3e992b65388761d0744ff",
                    "name": "Yutong Feng",
                    "hidden": false
                },
                {
                    "_id": "68a3e992b65388761d074500",
                    "name": "Chenxuan Miao",
                    "hidden": false
                },
                {
                    "_id": "68a3e992b65388761d074501",
                    "name": "Zixiang Gao",
                    "hidden": false
                },
                {
                    "_id": "68a3e992b65388761d074502",
                    "name": "Jiwang Qu",
                    "hidden": false
                },
                {
                    "_id": "68a3e992b65388761d074503",
                    "name": "Jianzhang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a3e992b65388761d074504",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "68a3e992b65388761d074505",
                    "name": "Kun Yuan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T14:21:22.000Z",
            "submittedOnDailyAt": "2025-08-19T01:34:17.041Z",
            "title": "Lumen: Consistent Video Relighting and Harmonious Background Replacement\n  with Video Generative Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Video relighting is a challenging yet valuable task, aiming to replace the\nbackground in videos while correspondingly adjusting the lighting in the\nforeground with harmonious blending. During translation, it is essential to\npreserve the original properties of the foreground, e.g., albedo, and propagate\nconsistent relighting among temporal frames. In this paper, we propose Lumen,\nan end-to-end video relighting framework developed on large-scale video\ngenerative models, receiving flexible textual description for instructing the\ncontrol of lighting and background. Considering the scarcity of high-qualified\npaired videos with the same foreground in various lighting conditions, we\nconstruct a large-scale dataset with a mixture of realistic and synthetic\nvideos. For the synthetic domain, benefiting from the abundant 3D assets in the\ncommunity, we leverage advanced 3D rendering engine to curate video pairs in\ndiverse environments. For the realistic domain, we adapt a HDR-based lighting\nsimulation to complement the lack of paired in-the-wild videos. Powered by the\naforementioned dataset, we design a joint training curriculum to effectively\nunleash the strengths of each domain, i.e., the physical consistency in\nsynthetic videos, and the generalized domain distribution in realistic videos.\nTo implement this, we inject a domain-aware adapter into the model to decouple\nthe learning of relighting and domain appearance distribution. We construct a\ncomprehensive benchmark to evaluate Lumen together with existing methods, from\nthe perspectives of foreground preservation and video consistency assessment.\nExperimental results demonstrate that Lumen effectively edit the input into\ncinematic relighted videos with consistent lighting and strict foreground\npreservation. Our project page: https://lumen-relight.github.io/",
            "upvotes": 10,
            "discussionId": "68a3e993b65388761d074506",
            "projectPage": "https://lumen-relight.github.io/",
            "ai_summary": "Lumen is an end-to-end video relighting framework that uses a large-scale dataset of realistic and synthetic videos to achieve consistent lighting and foreground preservation in edited videos.",
            "ai_keywords": [
                "video relighting",
                "albedo",
                "temporal frames",
                "end-to-end framework",
                "large-scale video generative models",
                "textual description",
                "3D rendering engine",
                "HDR-based lighting simulation",
                "domain-aware adapter",
                "foreground preservation",
                "video consistency assessment"
            ]
        },
        "publishedAt": "2025-08-18T10:21:22.000Z",
        "title": "Lumen: Consistent Video Relighting and Harmonious Background Replacement\n  with Video Generative Models",
        "summary": "Video relighting is a challenging yet valuable task, aiming to replace the\nbackground in videos while correspondingly adjusting the lighting in the\nforeground with harmonious blending. During translation, it is essential to\npreserve the original properties of the foreground, e.g., albedo, and propagate\nconsistent relighting among temporal frames. In this paper, we propose Lumen,\nan end-to-end video relighting framework developed on large-scale video\ngenerative models, receiving flexible textual description for instructing the\ncontrol of lighting and background. Considering the scarcity of high-qualified\npaired videos with the same foreground in various lighting conditions, we\nconstruct a large-scale dataset with a mixture of realistic and synthetic\nvideos. For the synthetic domain, benefiting from the abundant 3D assets in the\ncommunity, we leverage advanced 3D rendering engine to curate video pairs in\ndiverse environments. For the realistic domain, we adapt a HDR-based lighting\nsimulation to complement the lack of paired in-the-wild videos. Powered by the\naforementioned dataset, we design a joint training curriculum to effectively\nunleash the strengths of each domain, i.e., the physical consistency in\nsynthetic videos, and the generalized domain distribution in realistic videos.\nTo implement this, we inject a domain-aware adapter into the model to decouple\nthe learning of relighting and domain appearance distribution. We construct a\ncomprehensive benchmark to evaluate Lumen together with existing methods, from\nthe perspectives of foreground preservation and video consistency assessment.\nExperimental results demonstrate that Lumen effectively edit the input into\ncinematic relighted videos with consistent lighting and strict foreground\npreservation. Our project page: https://lumen-relight.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12945.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.11379",
            "authors": [
                {
                    "_id": "68a45c66b65388761d0746d0",
                    "user": {
                        "_id": "66d07487b86483cf13320564",
                        "avatarUrl": "/avatars/ad59d8edc680336e17a9a4f095a5d0da.svg",
                        "isPro": false,
                        "fullname": "Ramil Khafizov",
                        "user": "smileyenot983",
                        "type": "user"
                    },
                    "name": "Ramil Khafizov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T11:22:52.622Z",
                    "hidden": false
                },
                {
                    "_id": "68a45c66b65388761d0746d1",
                    "name": "Artem Komarichev",
                    "hidden": false
                },
                {
                    "_id": "68a45c66b65388761d0746d2",
                    "user": {
                        "_id": "6482ddba57d6e62bf4d8b837",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6482ddba57d6e62bf4d8b837/XmiNnZ7BB3vgrvEV8nsNI.png",
                        "isPro": false,
                        "fullname": "Ruslan Rakhimov",
                        "user": "rusrakhimov",
                        "type": "user"
                    },
                    "name": "Ruslan Rakhimov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T11:21:48.836Z",
                    "hidden": false
                },
                {
                    "_id": "68a45c66b65388761d0746d3",
                    "name": "Peter Wonka",
                    "hidden": false
                },
                {
                    "_id": "68a45c66b65388761d0746d4",
                    "name": "Evgeny Burnaev",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6482ddba57d6e62bf4d8b837/KgBcjEDe8m3RjRmkKZWHx.png"
            ],
            "publishedAt": "2025-08-15T10:25:58.000Z",
            "submittedOnDailyAt": "2025-08-19T09:48:59.765Z",
            "title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior\n  Integration",
            "submittedOnDailyBy": {
                "_id": "6482ddba57d6e62bf4d8b837",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6482ddba57d6e62bf4d8b837/XmiNnZ7BB3vgrvEV8nsNI.png",
                "isPro": false,
                "fullname": "Ruslan Rakhimov",
                "user": "rusrakhimov",
                "type": "user"
            },
            "summary": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.",
            "upvotes": 9,
            "discussionId": "68a45c66b65388761d0746d5",
            "ai_summary": "G-CUT3R enhances 3D scene reconstruction by integrating auxiliary data through dedicated encoders and zero convolution, improving performance across benchmarks.",
            "ai_keywords": [
                "CUT3R",
                "feed-forward approach",
                "guided 3D scene reconstruction",
                "auxiliary data",
                "depth",
                "camera calibrations",
                "camera positions",
                "dedicated encoder",
                "zero convolution",
                "RGB image tokens",
                "multi-view tasks"
            ]
        },
        "publishedAt": "2025-08-15T06:25:58.000Z",
        "title": "G-CUT3R: Guided 3D Reconstruction with Camera and Depth Prior\n  Integration",
        "summary": "We introduce G-CUT3R, a novel feed-forward approach for guided 3D scene\nreconstruction that enhances the CUT3R model by integrating prior information.\nUnlike existing feed-forward methods that rely solely on input images, our\nmethod leverages auxiliary data, such as depth, camera calibrations, or camera\npositions, commonly available in real-world scenarios. We propose a lightweight\nmodification to CUT3R, incorporating a dedicated encoder for each modality to\nextract features, which are fused with RGB image tokens via zero convolution.\nThis flexible design enables seamless integration of any combination of prior\ninformation during inference. Evaluated across multiple benchmarks, including\n3D reconstruction and other multi-view tasks, our approach demonstrates\nsignificant performance improvements, showing its ability to effectively\nutilize available priors while maintaining compatibility with varying input\nmodalities.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6482ddba57d6e62bf4d8b837/KgBcjEDe8m3RjRmkKZWHx.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11379.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6482ddba57d6e62bf4d8b837",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6482ddba57d6e62bf4d8b837/XmiNnZ7BB3vgrvEV8nsNI.png",
            "fullname": "Ruslan Rakhimov",
            "name": "rusrakhimov",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12880",
            "authors": [
                {
                    "_id": "68a3ea3db65388761d07450b",
                    "name": "Chubin Chen",
                    "hidden": false
                },
                {
                    "_id": "68a3ea3db65388761d07450c",
                    "name": "Jiashu Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a3ea3db65388761d07450d",
                    "name": "Xiaokun Feng",
                    "hidden": false
                },
                {
                    "_id": "68a3ea3db65388761d07450e",
                    "name": "Nisha Huang",
                    "hidden": false
                },
                {
                    "_id": "68a3ea3db65388761d07450f",
                    "name": "Meiqi Wu",
                    "hidden": false
                },
                {
                    "_id": "68a3ea3db65388761d074510",
                    "name": "Fangyuan Mao",
                    "hidden": false
                },
                {
                    "_id": "68a3ea3db65388761d074511",
                    "name": "Jiahong Wu",
                    "hidden": false
                },
                {
                    "_id": "68a3ea3db65388761d074512",
                    "name": "Xiangxiang Chu",
                    "hidden": false
                },
                {
                    "_id": "68a3ea3db65388761d074513",
                    "name": "Xiu Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T12:31:20.000Z",
            "submittedOnDailyAt": "2025-08-19T01:36:46.592Z",
            "title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of\n  Diffusion Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Classifier-free Guidance (CFG) is a widely used technique in modern diffusion\nmodels for enhancing sample quality and prompt adherence. However, through an\nempirical analysis on Gaussian mixture modeling with a closed-form solution, we\nobserve a discrepancy between the suboptimal results produced by CFG and the\nground truth. The model's excessive reliance on these suboptimal predictions\noften leads to semantic incoherence and low-quality outputs. To address this\nissue, we first empirically demonstrate that the model's suboptimal predictions\ncan be effectively refined using sub-networks of the model itself. Building on\nthis insight, we propose S^2-Guidance, a novel method that leverages stochastic\nblock-dropping during the forward process to construct stochastic sub-networks,\neffectively guiding the model away from potential low-quality predictions and\ntoward high-quality outputs. Extensive qualitative and quantitative experiments\non text-to-image and text-to-video generation tasks demonstrate that\nS^2-Guidance delivers superior performance, consistently surpassing CFG and\nother advanced guidance strategies. Our code will be released.",
            "upvotes": 8,
            "discussionId": "68a3ea3eb65388761d074514",
            "projectPage": "https://s2guidance.github.io/",
            "githubRepo": "https://github.com/AMAP-ML/S2-Guidance",
            "ai_summary": "S^2-Guidance, a novel method using stochastic block-dropping, improves sample quality and prompt adherence in diffusion models by refining suboptimal predictions, outperforming Classifier-free Guidance and other advanced strategies.",
            "ai_keywords": [
                "Classifier-free Guidance",
                "Gaussian mixture modeling",
                "suboptimal predictions",
                "sub-networks",
                "stochastic block-dropping",
                "S^2-Guidance",
                "text-to-image",
                "text-to-video generation"
            ],
            "githubStars": 22
        },
        "publishedAt": "2025-08-18T08:31:20.000Z",
        "title": "S^2-Guidance: Stochastic Self Guidance for Training-Free Enhancement of\n  Diffusion Models",
        "summary": "Classifier-free Guidance (CFG) is a widely used technique in modern diffusion\nmodels for enhancing sample quality and prompt adherence. However, through an\nempirical analysis on Gaussian mixture modeling with a closed-form solution, we\nobserve a discrepancy between the suboptimal results produced by CFG and the\nground truth. The model's excessive reliance on these suboptimal predictions\noften leads to semantic incoherence and low-quality outputs. To address this\nissue, we first empirically demonstrate that the model's suboptimal predictions\ncan be effectively refined using sub-networks of the model itself. Building on\nthis insight, we propose S^2-Guidance, a novel method that leverages stochastic\nblock-dropping during the forward process to construct stochastic sub-networks,\neffectively guiding the model away from potential low-quality predictions and\ntoward high-quality outputs. Extensive qualitative and quantitative experiments\non text-to-image and text-to-video generation tasks demonstrate that\nS^2-Guidance delivers superior performance, consistently surpassing CFG and\nother advanced guidance strategies. Our code will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12880.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.12466",
            "authors": [
                {
                    "_id": "68a3deb1b65388761d07446d",
                    "user": {
                        "_id": "652c6ee9b118f26df765e4bb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652c6ee9b118f26df765e4bb/SwKLzRs5iVm0ORSCH-WLH.jpeg",
                        "isPro": false,
                        "fullname": "Xuhui Zhan",
                        "user": "xuhuizhan5",
                        "type": "user"
                    },
                    "name": "Xuhui Zhan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:52.380Z",
                    "hidden": false
                },
                {
                    "_id": "68a3deb1b65388761d07446e",
                    "name": "Tyler Derr",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/652c6ee9b118f26df765e4bb/PVHGV5FdpWkY4RFJ0CeG1.png",
                "https://cdn-uploads.huggingface.co/production/uploads/652c6ee9b118f26df765e4bb/hFDLfJ2ksRL_mCDzGr70m.png"
            ],
            "publishedAt": "2025-08-17T18:36:04.000Z",
            "submittedOnDailyAt": "2025-08-19T00:49:35.874Z",
            "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision\n  Mapping",
            "submittedOnDailyBy": {
                "_id": "652c6ee9b118f26df765e4bb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652c6ee9b118f26df765e4bb/SwKLzRs5iVm0ORSCH-WLH.jpeg",
                "isPro": false,
                "fullname": "Xuhui Zhan",
                "user": "xuhuizhan5",
                "type": "user"
            },
            "summary": "Traditional multimodal learning approaches require expensive alignment\npre-training to bridge vision and language modalities, typically projecting\nvisual features into discrete text token spaces. We challenge both fundamental\nassumptions underlying this paradigm by proposing Inverse-LLaVA, a novel\napproach that eliminates alignment pre-training entirely while inverting the\nconventional mapping direction. Rather than projecting visual features to text\nspace, our method maps text embeddings into continuous visual representation\nspace and performs fusion within transformer intermediate layers. Through\nselective additive components in attention mechanisms, we enable dynamic\nintegration of visual and textual representations without requiring massive\nimage-text alignment datasets. Comprehensive experiments across nine multimodal\nbenchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves\nnotable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,\nVizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing\nexpected decreases in perception tasks requiring memorized visual-text\nassociations (celebrity recognition: -49.5%, OCR: -21.3%). These results\nprovide the first empirical evidence that alignment pre-training is not\nnecessary for effective multimodal learning, particularly for complex reasoning\ntasks. Our work establishes the feasibility of a new paradigm that reduces\ncomputational requirements by 45%, challenges conventional wisdom about\nmodality fusion, and opens new research directions for efficient multimodal\narchitectures that preserve modality-specific characteristics. Our project\nwebsite with code and additional resources is available at\nhttps://inverse-llava.github.io.",
            "upvotes": 8,
            "discussionId": "68a3decbb65388761d07446f",
            "projectPage": "https://inverse-llava.github.io",
            "githubRepo": "https://github.com/xuhuizhan5/Inverse-LLaVA",
            "ai_summary": "Inverse-LLaVA eliminates alignment pre-training by mapping text embeddings into continuous visual representation space, improving reasoning tasks while reducing computational requirements.",
            "ai_keywords": [
                "Inverse-LLaVA",
                "multimodal learning",
                "alignment pre-training",
                "visual features",
                "text embeddings",
                "continuous visual representation space",
                "transformer intermediate layers",
                "attention mechanisms",
                "MM-VET",
                "VizWiz",
                "ScienceQA",
                "cognitive reasoning",
                "celebrity recognition",
                "OCR"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-08-17T14:36:04.000Z",
        "title": "Inverse-LLaVA: Eliminating Alignment Pre-training Through Text-to-Vision\n  Mapping",
        "summary": "Traditional multimodal learning approaches require expensive alignment\npre-training to bridge vision and language modalities, typically projecting\nvisual features into discrete text token spaces. We challenge both fundamental\nassumptions underlying this paradigm by proposing Inverse-LLaVA, a novel\napproach that eliminates alignment pre-training entirely while inverting the\nconventional mapping direction. Rather than projecting visual features to text\nspace, our method maps text embeddings into continuous visual representation\nspace and performs fusion within transformer intermediate layers. Through\nselective additive components in attention mechanisms, we enable dynamic\nintegration of visual and textual representations without requiring massive\nimage-text alignment datasets. Comprehensive experiments across nine multimodal\nbenchmarks demonstrate nuanced performance trade-offs: Inverse-LLaVA achieves\nnotable improvements on reasoning-intensive and cognitive tasks (MM-VET: +0.2%,\nVizWiz: +1.8%, ScienceQA: +0.2%, cognitive reasoning: +27.2%), while showing\nexpected decreases in perception tasks requiring memorized visual-text\nassociations (celebrity recognition: -49.5%, OCR: -21.3%). These results\nprovide the first empirical evidence that alignment pre-training is not\nnecessary for effective multimodal learning, particularly for complex reasoning\ntasks. Our work establishes the feasibility of a new paradigm that reduces\ncomputational requirements by 45%, challenges conventional wisdom about\nmodality fusion, and opens new research directions for efficient multimodal\narchitectures that preserve modality-specific characteristics. Our project\nwebsite with code and additional resources is available at\nhttps://inverse-llava.github.io.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/652c6ee9b118f26df765e4bb/PVHGV5FdpWkY4RFJ0CeG1.png",
            "https://cdn-uploads.huggingface.co/production/uploads/652c6ee9b118f26df765e4bb/hFDLfJ2ksRL_mCDzGr70m.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12466.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652c6ee9b118f26df765e4bb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652c6ee9b118f26df765e4bb/SwKLzRs5iVm0ORSCH-WLH.jpeg",
            "fullname": "Xuhui Zhan",
            "name": "xuhuizhan5",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12790",
            "authors": [
                {
                    "_id": "68a42f43b65388761d0745c1",
                    "name": "Zenan Huang",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745c2",
                    "user": {
                        "_id": "673b5f24e863f1d28b402efc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/19gUgtPEY3-FtY0sNlI_-.png",
                        "isPro": false,
                        "fullname": "yihongzhuang",
                        "user": "utdawn",
                        "type": "user"
                    },
                    "name": "Yihong Zhuang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T10:48:36.226Z",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745c3",
                    "name": "Guoshan Lu",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745c4",
                    "name": "Zeyu Qin",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745c5",
                    "name": "Haokai Xu",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745c6",
                    "name": "Tianyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745c7",
                    "name": "Ru Peng",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745c8",
                    "name": "Jiaqi Hu",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745c9",
                    "name": "Zhanming Shen",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745ca",
                    "name": "Xiaomeng Hu",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745cb",
                    "name": "Xijun Gu",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745cc",
                    "name": "Peiyi Tu",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745cd",
                    "name": "Jiaxin Liu",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745ce",
                    "name": "Wenyu Chen",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745cf",
                    "name": "Yuzhuo Fu",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745d0",
                    "name": "Zhiting Fan",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745d1",
                    "name": "Yanmei Gu",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745d2",
                    "name": "Yuanyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745d3",
                    "name": "Zhengkai Yang",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745d4",
                    "name": "Jianguo Li",
                    "hidden": false
                },
                {
                    "_id": "68a42f43b65388761d0745d5",
                    "name": "Junbo Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T10:06:08.000Z",
            "submittedOnDailyAt": "2025-08-19T11:03:58.333Z",
            "title": "Reinforcement Learning with Rubric Anchors",
            "submittedOnDailyBy": {
                "_id": "649ee7ac1f44b2c0301df24e",
                "avatarUrl": "/avatars/00864ad358cd5e0416c03b9587dfc56e.svg",
                "isPro": false,
                "fullname": "Guoshan Lu",
                "user": "grason-lu",
                "type": "user"
            },
            "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases.",
            "upvotes": 5,
            "discussionId": "68a42f43b65388761d0745d6",
            "ai_summary": "RLVR is extended to open-ended tasks using rubric-based rewards, improving performance on benchmarks and providing stylistic control in LLMs.",
            "ai_keywords": [
                "Reinforcement Learning from Verifiable Rewards",
                "RLVR",
                "Large Language Models",
                "LLMs",
                "rubric-based rewards",
                "Qwen-30B-A3B",
                "DeepSeek-V3",
                "open-ended benchmarks",
                "stylistic control"
            ]
        },
        "publishedAt": "2025-08-18T06:06:08.000Z",
        "title": "Reinforcement Learning with Rubric Anchors",
        "summary": "Reinforcement Learning from Verifiable Rewards (RLVR) has emerged as a\npowerful paradigm for enhancing Large Language Models (LLMs), exemplified by\nthe success of OpenAI's o-series. In RLVR, rewards are derived from verifiable\nsignals-such as passing unit tests in code generation or matching correct\nanswers in mathematical reasoning. While effective, this requirement largely\nconfines RLVR to domains with automatically checkable outcomes. To overcome\nthis, we extend the RLVR paradigm to open-ended tasks by integrating\nrubric-based rewards, where carefully designed rubrics serve as structured,\nmodel-interpretable criteria for automatic scoring of subjective outputs. We\nconstruct, to our knowledge, the largest rubric reward system to date, with\nover 10,000 rubrics from humans, LLMs, or a hybrid human-LLM collaboration.\nImplementing rubric-based RL is challenging; we tackle these issues with a\nclear framework and present an open-sourced Qwen-30B-A3B model with notable\ngains: 1) With only 5K+ samples, our system improves by +5.2% on open-ended\nbenchmarks (especially humanities), outperforming a 671B DeepSeek-V3 model by\n+2.4%, while preserving general and reasoning abilities. 2) Our method provides\nfine-grained stylistic control, using rubrics as anchors to mitigate the\n\"AI-like\" tone and produce more human-like, expressive responses. We share key\nlessons in rubric construction, data selection, and training, and discuss\nlimitations and future releases.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12790.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649ee7ac1f44b2c0301df24e",
            "avatarUrl": "/avatars/00864ad358cd5e0416c03b9587dfc56e.svg",
            "fullname": "Guoshan Lu",
            "name": "grason-lu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.13104",
            "authors": [
                {
                    "_id": "68a40c90b65388761d074570",
                    "user": {
                        "_id": "6422dd402f38c0a50cfd5405",
                        "avatarUrl": "/avatars/d3943b7b34c8026fa26eee3837590260.svg",
                        "isPro": false,
                        "fullname": "Yuang Wang",
                        "user": "angshineee",
                        "type": "user"
                    },
                    "name": "Yuang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:28:42.262Z",
                    "hidden": false
                },
                {
                    "_id": "68a40c90b65388761d074571",
                    "user": {
                        "_id": "650d0c442a602ba349183fca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650d0c442a602ba349183fca/-UId-lPK64nRXjGxPJE9k.png",
                        "isPro": false,
                        "fullname": "Chao Wen",
                        "user": "walsvid",
                        "type": "user"
                    },
                    "name": "Chao Wen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:06.723Z",
                    "hidden": false
                },
                {
                    "_id": "68a40c90b65388761d074572",
                    "name": "Haoyu Guo",
                    "hidden": false
                },
                {
                    "_id": "68a40c90b65388761d074573",
                    "name": "Sida Peng",
                    "hidden": false
                },
                {
                    "_id": "68a40c90b65388761d074574",
                    "name": "Minghan Qin",
                    "hidden": false
                },
                {
                    "_id": "68a40c90b65388761d074575",
                    "name": "Hujun Bao",
                    "hidden": false
                },
                {
                    "_id": "68a40c90b65388761d074576",
                    "name": "Xiaowei Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a40c90b65388761d074577",
                    "name": "Ruizhen Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T17:12:28.000Z",
            "submittedOnDailyAt": "2025-08-19T04:03:42.709Z",
            "title": "Precise Action-to-Video Generation Through Visual Action Prompts",
            "submittedOnDailyBy": {
                "_id": "650d0c442a602ba349183fca",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650d0c442a602ba349183fca/-UId-lPK64nRXjGxPJE9k.png",
                "isPro": false,
                "fullname": "Chao Wen",
                "user": "walsvid",
                "type": "user"
            },
            "summary": "We present visual action prompts, a unified action representation for\naction-to-video generation of complex high-DoF interactions while maintaining\ntransferable visual dynamics across domains. Action-driven video generation\nfaces a precision-generality trade-off: existing methods using text, primitive\nactions, or coarse masks offer generality but lack precision, while\nagent-centric action signals provide precision at the cost of cross-domain\ntransferability. To balance action precision and dynamic transferability, we\npropose to \"render\" actions into precise visual prompts as domain-agnostic\nrepresentations that preserve both geometric precision and cross-domain\nadaptability for complex actions; specifically, we choose visual skeletons for\ntheir generality and accessibility. We propose robust pipelines to construct\nskeletons from two interaction-rich data sources - human-object interactions\n(HOI) and dexterous robotic manipulation - enabling cross-domain training of\naction-driven generative models. By integrating visual skeletons into\npretrained video generation models via lightweight fine-tuning, we enable\nprecise action control of complex interaction while preserving the learning of\ncross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the\neffectiveness of our proposed approach. Project page:\nhttps://zju3dv.github.io/VAP/.",
            "upvotes": 4,
            "discussionId": "68a40c90b65388761d074578",
            "ai_summary": "Visual action prompts, using visual skeletons, enable precise action control in video generation while maintaining cross-domain transferability.",
            "ai_keywords": [
                "visual action prompts",
                "action-to-video generation",
                "high-DoF interactions",
                "visual dynamics",
                "action-driven video generation",
                "precision-generality trade-off",
                "agent-centric action signals",
                "visual skeletons",
                "human-object interactions",
                "dexterous robotic manipulation",
                "pretrained video generation models",
                "lightweight fine-tuning",
                "EgoVid",
                "RT-1",
                "DROID"
            ]
        },
        "publishedAt": "2025-08-18T13:12:28.000Z",
        "title": "Precise Action-to-Video Generation Through Visual Action Prompts",
        "summary": "We present visual action prompts, a unified action representation for\naction-to-video generation of complex high-DoF interactions while maintaining\ntransferable visual dynamics across domains. Action-driven video generation\nfaces a precision-generality trade-off: existing methods using text, primitive\nactions, or coarse masks offer generality but lack precision, while\nagent-centric action signals provide precision at the cost of cross-domain\ntransferability. To balance action precision and dynamic transferability, we\npropose to \"render\" actions into precise visual prompts as domain-agnostic\nrepresentations that preserve both geometric precision and cross-domain\nadaptability for complex actions; specifically, we choose visual skeletons for\ntheir generality and accessibility. We propose robust pipelines to construct\nskeletons from two interaction-rich data sources - human-object interactions\n(HOI) and dexterous robotic manipulation - enabling cross-domain training of\naction-driven generative models. By integrating visual skeletons into\npretrained video generation models via lightweight fine-tuning, we enable\nprecise action control of complex interaction while preserving the learning of\ncross-domain dynamics. Experiments on EgoVid, RT-1 and DROID demonstrate the\neffectiveness of our proposed approach. Project page:\nhttps://zju3dv.github.io/VAP/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13104.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650d0c442a602ba349183fca",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/650d0c442a602ba349183fca/-UId-lPK64nRXjGxPJE9k.png",
            "fullname": "Chao Wen",
            "name": "walsvid",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12730",
            "authors": [
                {
                    "_id": "68a3f629b65388761d07453b",
                    "user": {
                        "_id": "6663459bd6bf635504ec7dfc",
                        "avatarUrl": "/avatars/8727370401076a42666f8d4a05cf463d.svg",
                        "isPro": false,
                        "fullname": "jaeunglee",
                        "user": "jaeunglee",
                        "type": "user"
                    },
                    "name": "Jaeung Lee",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:15.966Z",
                    "hidden": false
                },
                {
                    "_id": "68a3f629b65388761d07453c",
                    "name": "Suhyeon Yu",
                    "hidden": false
                },
                {
                    "_id": "68a3f629b65388761d07453d",
                    "name": "Yurim Jang",
                    "hidden": false
                },
                {
                    "_id": "68a3f629b65388761d07453e",
                    "name": "Simon S. Woo",
                    "hidden": false
                },
                {
                    "_id": "68a3f629b65388761d07453f",
                    "name": "Jaemin Jo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T08:53:53.000Z",
            "submittedOnDailyAt": "2025-08-19T12:55:54.774Z",
            "title": "Unlearning Comparator: A Visual Analytics System for Comparative\n  Evaluation of Machine Unlearning Methods",
            "submittedOnDailyBy": {
                "_id": "6663459bd6bf635504ec7dfc",
                "avatarUrl": "/avatars/8727370401076a42666f8d4a05cf463d.svg",
                "isPro": false,
                "fullname": "jaeunglee",
                "user": "jaeunglee",
                "type": "user"
            },
            "summary": "Machine Unlearning (MU) aims to remove target training data from a trained\nmodel so that the removed data no longer influences the model's behavior,\nfulfilling \"right to be forgotten\" obligations under data privacy laws. Yet, we\nobserve that researchers in this rapidly emerging field face challenges in\nanalyzing and understanding the behavior of different MU methods, especially in\nterms of three fundamental principles in MU: accuracy, efficiency, and privacy.\nConsequently, they often rely on aggregate metrics and ad-hoc evaluations,\nmaking it difficult to accurately assess the trade-offs between methods. To\nfill this gap, we introduce a visual analytics system, Unlearning Comparator,\ndesigned to facilitate the systematic evaluation of MU methods. Our system\nsupports two important tasks in the evaluation process: model comparison and\nattack simulation. First, it allows the user to compare the behaviors of two\nmodels, such as a model generated by a certain method and a retrained baseline,\nat class-, instance-, and layer-levels to better understand the changes made\nafter unlearning. Second, our system simulates membership inference attacks\n(MIAs) to evaluate the privacy of a method, where an attacker attempts to\ndetermine whether specific data samples were part of the original training set.\nWe evaluate our system through a case study visually analyzing prominent MU\nmethods and demonstrate that it helps the user not only understand model\nbehaviors but also gain insights that can inform the improvement of MU methods.",
            "upvotes": 3,
            "discussionId": "68a3f629b65388761d074540",
            "projectPage": "https://gnueaj.github.io/Machine-Unlearning-Comparator/",
            "githubRepo": "https://github.com/gnueaj/Machine-Unlearning-Comparator",
            "ai_summary": "A visual analytics system, Unlearning Comparator, facilitates the evaluation of Machine Unlearning methods by comparing model behaviors and simulating membership inference attacks to assess privacy.",
            "ai_keywords": [
                "Machine Unlearning",
                "MU",
                "right to be forgotten",
                "data privacy laws",
                "accuracy",
                "efficiency",
                "privacy",
                "visual analytics system",
                "model comparison",
                "attack simulation",
                "membership inference attacks",
                "MIAs"
            ],
            "githubStars": 56
        },
        "publishedAt": "2025-08-18T04:53:53.000Z",
        "title": "Unlearning Comparator: A Visual Analytics System for Comparative\n  Evaluation of Machine Unlearning Methods",
        "summary": "Machine Unlearning (MU) aims to remove target training data from a trained\nmodel so that the removed data no longer influences the model's behavior,\nfulfilling \"right to be forgotten\" obligations under data privacy laws. Yet, we\nobserve that researchers in this rapidly emerging field face challenges in\nanalyzing and understanding the behavior of different MU methods, especially in\nterms of three fundamental principles in MU: accuracy, efficiency, and privacy.\nConsequently, they often rely on aggregate metrics and ad-hoc evaluations,\nmaking it difficult to accurately assess the trade-offs between methods. To\nfill this gap, we introduce a visual analytics system, Unlearning Comparator,\ndesigned to facilitate the systematic evaluation of MU methods. Our system\nsupports two important tasks in the evaluation process: model comparison and\nattack simulation. First, it allows the user to compare the behaviors of two\nmodels, such as a model generated by a certain method and a retrained baseline,\nat class-, instance-, and layer-levels to better understand the changes made\nafter unlearning. Second, our system simulates membership inference attacks\n(MIAs) to evaluate the privacy of a method, where an attacker attempts to\ndetermine whether specific data samples were part of the original training set.\nWe evaluate our system through a case study visually analyzing prominent MU\nmethods and demonstrate that it helps the user not only understand model\nbehaviors but also gain insights that can inform the improvement of MU methods.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12730.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6663459bd6bf635504ec7dfc",
            "avatarUrl": "/avatars/8727370401076a42666f8d4a05cf463d.svg",
            "fullname": "jaeunglee",
            "name": "jaeunglee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.11252",
            "authors": [
                {
                    "_id": "68a3e308b65388761d0744bb",
                    "user": {
                        "_id": "5fcc4707563427b03e9af25b",
                        "avatarUrl": "/avatars/effcf3bcc9ef7163c454eeb4a90e1b09.svg",
                        "isPro": false,
                        "fullname": "YouchengHuang",
                        "user": "YouchengHuang",
                        "type": "user"
                    },
                    "name": "Youcheng Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T08:04:24.093Z",
                    "hidden": true
                },
                {
                    "_id": "68a3e308b65388761d0744bc",
                    "name": "Bowen Qin",
                    "hidden": false
                },
                {
                    "_id": "68a3e308b65388761d0744bd",
                    "name": "Chen Huang",
                    "hidden": false
                },
                {
                    "_id": "68a3e308b65388761d0744be",
                    "name": "Duanyu Feng",
                    "hidden": false
                },
                {
                    "_id": "68a3e308b65388761d0744bf",
                    "name": "Xi Yang",
                    "hidden": false
                },
                {
                    "_id": "68a3e308b65388761d0744c0",
                    "name": "Wenqiang Lei",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-15T06:42:00.000Z",
            "submittedOnDailyAt": "2025-08-19T06:48:16.475Z",
            "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning\n  Models to Ask for Information",
            "submittedOnDailyBy": {
                "_id": "5fcc4707563427b03e9af25b",
                "avatarUrl": "/avatars/effcf3bcc9ef7163c454eeb4a90e1b09.svg",
                "isPro": false,
                "fullname": "YouchengHuang",
                "user": "YouchengHuang",
                "type": "user"
            },
            "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.",
            "upvotes": 2,
            "discussionId": "68a3e316b65388761d0744c1",
            "ai_summary": "Systematic evaluation of Large Reasoning Models on incomplete problems reveals their inability to proactively seek information, highlighting issues like overthinking and hallucination, and the challenges of supervised fine-tuning for developing genuine intelligence.",
            "ai_keywords": [
                "Large Reasoning Models",
                "incomplete problems",
                "proactive information seeking",
                "overthinking",
                "hallucination",
                "supervised fine-tuning"
            ]
        },
        "publishedAt": "2025-08-15T02:42:00.000Z",
        "title": "Beyond Solving Math Quiz: Evaluating the Ability of Large Reasoning\n  Models to Ask for Information",
        "summary": "Large Reasoning Models (LRMs) have demonstrated remarkable problem-solving\nabilities in mathematics, as evaluated by existing benchmarks exclusively on\nwell-defined problems. However, such evaluation setup constitutes a critical\ngap, since a genuine intelligent agent should not only solve problems (as a\nmath quiz solver), but also be able~to ask for information when the problems\nlack sufficient information, enabling proactivity in responding users'\nrequests. To bridge such gap, we proposes a new dataset consisting of two types\nof incomplete problems with diverse contexts. Based on the dataset, our\nsystematical evaluation of LRMs reveals their inability in proactively asking\nfor information. In addition, we uncover the behaviors related to overthinking\nand hallucination of LRMs, and highlight the potential and challenges of\nsupervised fine-tuning in learning such ability. We hope to provide new\ninsights in developing LRMs with genuine intelligence, rather than just solving\nproblems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11252.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5fcc4707563427b03e9af25b",
            "avatarUrl": "/avatars/effcf3bcc9ef7163c454eeb4a90e1b09.svg",
            "fullname": "YouchengHuang",
            "name": "YouchengHuang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13968",
            "authors": [
                {
                    "_id": "68a51e046cf0bf898542ebf8",
                    "name": "Tianyi Niu",
                    "hidden": false
                },
                {
                    "_id": "68a51e046cf0bf898542ebf9",
                    "name": "Jaemin Cho",
                    "hidden": false
                },
                {
                    "_id": "68a51e046cf0bf898542ebfa",
                    "name": "Elias Stengel-Eskin",
                    "hidden": false
                },
                {
                    "_id": "68a51e046cf0bf898542ebfb",
                    "name": "Mohit Bansal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T15:58:25.000Z",
            "submittedOnDailyAt": "2025-08-19T23:30:12.342Z",
            "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying\n  Image Rotation",
            "submittedOnDailyBy": {
                "_id": "5ffe32d8942cf3533d364449",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
                "isPro": false,
                "fullname": "Jaemin Cho",
                "user": "j-min",
                "type": "user"
            },
            "summary": "We investigate to what extent Multimodal Large Language Models (MLLMs) can\naccurately identify the orientation of input images rotated 0{\\deg}, 90{\\deg},\n180{\\deg}, and 270{\\deg}. This task demands robust visual reasoning\ncapabilities to detect rotational cues and contextualize spatial relationships\nwithin images, regardless of their orientation. To evaluate MLLMs on these\nabilities, we introduce RotBench -- a 350-image manually-filtered benchmark\ncomprising lifestyle, portrait, and landscape images. Despite the relatively\nsimple nature of this task, we show that several state-of-the-art open and\nproprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably\nidentify rotation in input images. Providing models with auxiliary information\n-- including captions, depth maps, and more -- or using chain-of-thought\nprompting offers only small and inconsistent improvements. Our results indicate\nthat most models are able to reliably identify right-side-up (0{\\deg}) images,\nwhile certain models are able to identify upside-down (180{\\deg}) images. None\ncan reliably distinguish between 90{\\deg} and 270{\\deg}. Simultaneously showing\nthe image rotated in different orientations leads to moderate performance gains\nfor reasoning models, while a modified setup using voting improves the\nperformance of weaker models. We further show that fine-tuning does not improve\nmodels' ability to distinguish 90{\\deg} and 270{\\deg} rotations, despite\nsubstantially improving the identification of 180{\\deg} images. Together, these\nresults reveal a significant gap between MLLMs' spatial reasoning capabilities\nand human perception in identifying rotation.",
            "upvotes": 0,
            "discussionId": "68a51e046cf0bf898542ebfc",
            "githubRepo": "https://github.com/tianyiniu/RotBench",
            "ai_summary": "MLLMs struggle to accurately identify image rotations, particularly 90 and 270, despite improvements with auxiliary information and fine-tuning.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "RotBench",
                "visual reasoning",
                "captions",
                "depth maps",
                "chain-of-thought prompting",
                "fine-tuning"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-08-19T11:58:25.000Z",
        "title": "RotBench: Evaluating Multimodal Large Language Models on Identifying\n  Image Rotation",
        "summary": "We investigate to what extent Multimodal Large Language Models (MLLMs) can\naccurately identify the orientation of input images rotated 0{\\deg}, 90{\\deg},\n180{\\deg}, and 270{\\deg}. This task demands robust visual reasoning\ncapabilities to detect rotational cues and contextualize spatial relationships\nwithin images, regardless of their orientation. To evaluate MLLMs on these\nabilities, we introduce RotBench -- a 350-image manually-filtered benchmark\ncomprising lifestyle, portrait, and landscape images. Despite the relatively\nsimple nature of this task, we show that several state-of-the-art open and\nproprietary MLLMs, including GPT-5, o3, and Gemini-2.5-Pro, do not reliably\nidentify rotation in input images. Providing models with auxiliary information\n-- including captions, depth maps, and more -- or using chain-of-thought\nprompting offers only small and inconsistent improvements. Our results indicate\nthat most models are able to reliably identify right-side-up (0{\\deg}) images,\nwhile certain models are able to identify upside-down (180{\\deg}) images. None\ncan reliably distinguish between 90{\\deg} and 270{\\deg}. Simultaneously showing\nthe image rotated in different orientations leads to moderate performance gains\nfor reasoning models, while a modified setup using voting improves the\nperformance of weaker models. We further show that fine-tuning does not improve\nmodels' ability to distinguish 90{\\deg} and 270{\\deg} rotations, despite\nsubstantially improving the identification of 180{\\deg} images. Together, these\nresults reveal a significant gap between MLLMs' spatial reasoning capabilities\nand human perception in identifying rotation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13968.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "5ffe32d8942cf3533d364449",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654821969191-5ffe32d8942cf3533d364449.jpeg",
            "fullname": "Jaemin Cho",
            "name": "j-min",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    }
]