[
    {
        "paper": {
            "id": "2508.13167",
            "authors": [
                {
                    "_id": "68a535f16cf0bf898542ec6c",
                    "name": "Weizhen Li",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec6d",
                    "name": "Jianbo Lin",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec6e",
                    "name": "Zhuosong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec6f",
                    "name": "Jingyi Cao",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec70",
                    "name": "Xinpeng Liu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec71",
                    "name": "Jiayu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec72",
                    "name": "Zhenqiang Huang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec73",
                    "name": "Qianben Chen",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec74",
                    "name": "Weichen Sun",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec75",
                    "name": "Qiexiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec76",
                    "name": "Hongxuan Lu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec77",
                    "user": {
                        "_id": "64301abe450c0de9a1d3d18e",
                        "avatarUrl": "/avatars/01b284874dadc7d21d656c53dcb77e42.svg",
                        "isPro": false,
                        "fullname": "tianrui",
                        "user": "tianyue818",
                        "type": "user"
                    },
                    "name": "Tianrui Qin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:31.629Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec78",
                    "name": "Chenghao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec79",
                    "name": "Yi Yao",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7a",
                    "name": "Shuying Fan",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7b",
                    "user": {
                        "_id": "68022a47806e99452af4d05e",
                        "avatarUrl": "/avatars/4546956c787242b9950a05cd6ad26a21.svg",
                        "isPro": false,
                        "fullname": "wanwan",
                        "user": "wanwan1212",
                        "type": "user"
                    },
                    "name": "Xiaowan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:19.190Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7c",
                    "name": "Tiannan Wang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7d",
                    "name": "Pai Liu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7e",
                    "user": {
                        "_id": "6578265ddea7e2122d02f6ba",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6578265ddea7e2122d02f6ba/Bh6JjoVF5ceLSjV7Z7nTk.jpeg",
                        "isPro": false,
                        "fullname": "kang zhu",
                        "user": "kangz",
                        "type": "user"
                    },
                    "name": "King Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:21.203Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec7f",
                    "name": "He Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec80",
                    "user": {
                        "_id": "657c1f7e688f1a0f7ecfe264",
                        "avatarUrl": "/avatars/265afcb7b0eeddbcf66ec4cdd4920dd3.svg",
                        "isPro": false,
                        "fullname": "Dingfeng Shi",
                        "user": "hugteste",
                        "type": "user"
                    },
                    "name": "Dingfeng Shi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:24.492Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec81",
                    "name": "Piaohong Wang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec82",
                    "name": "Yeyi Guan",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec83",
                    "name": "Xiangru Tang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec84",
                    "user": {
                        "_id": "6417d9ea8f689506e7148417",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
                        "isPro": false,
                        "fullname": "minghao",
                        "user": "Liam-Liu",
                        "type": "user"
                    },
                    "name": "Minghao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:27.711Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec85",
                    "name": "Yuchen Eleanor Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec86",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec87",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec88",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:36.844Z",
                    "hidden": false
                },
                {
                    "_id": "68a535f16cf0bf898542ec89",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T17:01:02.000Z",
            "submittedOnDailyAt": "2025-08-20T01:12:39.713Z",
            "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent\n  Distillation and Agentic RL",
            "submittedOnDailyBy": {
                "_id": "628c8598ef14f971b698107f",
                "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
                "isPro": false,
                "fullname": "Zhou",
                "user": "Wangchunshu",
                "type": "user"
            },
            "summary": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.",
            "upvotes": 67,
            "discussionId": "68a535f16cf0bf898542ec8a",
            "projectPage": "https://chain-of-agents-afm.github.io/",
            "githubRepo": "https://github.com/OPPO-PersonalAI/Agent_Foundation_Models",
            "ai_summary": "Chain-of-Agents (CoA) paradigm enables end-to-end complex problem-solving in LLMs through dynamic agent activation, improving performance via multi-agent distillation and agentic reinforcement learning.",
            "ai_keywords": [
                "large language models",
                "multi-agent systems",
                "deep research",
                "vibe coding",
                "mathematical reasoning",
                "prompt/workflow engineering",
                "agent frameworks",
                "chain-of-agents",
                "tool agents",
                "role-playing agents",
                "multi-agent distillation",
                "agentic supervised fine-tuning",
                "agentic reinforcement learning",
                "Agent Foundation Models",
                "AFMs",
                "web agent",
                "code agent",
                "verifiable agentic tasks"
            ],
            "githubStars": 74
        },
        "publishedAt": "2025-08-06T13:01:02.000Z",
        "title": "Chain-of-Agents: End-to-End Agent Foundation Models via Multi-Agent\n  Distillation and Agentic RL",
        "summary": "Recent advances in large language models (LLMs) and multi-agent systems have\ndemonstrated remarkable capabilities in complex problem-solving tasks such as\ndeep research, vibe coding, and mathematical reasoning. However, most existing\nmulti-agent systems are built upon manual prompt/workflow engineering with\nsophisticated agent frameworks, making them computationally inefficient, less\ncapable, and can not benefit from data-centric learning. In this work, we\nintroduce Chain-of-Agents (CoA), a novel paradigm of LLM reasoning that enables\nnative end-to-end complex problem-solving in the same way as a multi-agent\nsystem (i.e., multi-turn problem solving with multiple tools and multiple\nagents) within one model. In chain-of-agents problem-solving, the model\ndynamically activates different tool agents and role-playing agents to simulate\nmulti-agent collaboration in an end-to-end fashion. To elicit end-to-end\nchain-of-agents problem-solving abilities in LLMs, we introduce a multi-agent\ndistillation framework to distill state-of-the-art multi-agent systems into\nchain-of-agents trajectories for agentic supervised fine-tuning. We then use\nagentic reinforcement learning on verifiable agentic tasks to further improve\nthe models' capabilities on chain-of-agents problem solving. We call the\nresulting models Agent Foundation Models (AFMs). Our empirical studies\ndemonstrate that AFM establishes new state-of-the-art performance across\ndiverse benchmarks in both web agent and code agent settings. We make the\nentire research, including the model weights, code for training and evaluation,\nand the training data, fully open-sourced, which offers a solid starting point\nfor future research on agent models and agentic RL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13167.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "628c8598ef14f971b698107f",
            "avatarUrl": "/avatars/3a4ad87e6b5f9e836a1160d869df1447.svg",
            "fullname": "Zhou",
            "name": "Wangchunshu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.14041",
            "authors": [
                {
                    "_id": "68a5270e6cf0bf898542ec12",
                    "user": {
                        "_id": "666afb91e936f6cbcfc8b50c",
                        "avatarUrl": "/avatars/a618c074c9e11e6b9444d0e366efbbdf.svg",
                        "isPro": false,
                        "fullname": "LIN, CHIN-YANG",
                        "user": "linjohnss",
                        "type": "user"
                    },
                    "name": "Chin-Yang Lin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:57.117Z",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec13",
                    "name": "Cheng Sun",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec14",
                    "name": "Fu-En Yang",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec15",
                    "user": {
                        "_id": "64ae22dd1aee69ece065cdcd",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ae22dd1aee69ece065cdcd/JG7QaHIrr4i2k4uwR4pZK.png",
                        "isPro": false,
                        "fullname": "Min-Hung Chen",
                        "user": "cmhungsteve",
                        "type": "user"
                    },
                    "name": "Min-Hung Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:58.995Z",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec16",
                    "name": "Yen-Yu Lin",
                    "hidden": false
                },
                {
                    "_id": "68a5270e6cf0bf898542ec17",
                    "name": "Yu-Lun Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/dCg9OGuyLHLqwelz7AgUB.mp4"
            ],
            "publishedAt": "2025-08-19T17:59:56.000Z",
            "submittedOnDailyAt": "2025-08-20T01:41:48.509Z",
            "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
            "submittedOnDailyBy": {
                "_id": "6459d5da3b6fafd9664807ab",
                "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
                "isPro": false,
                "fullname": "Yu-Lun Liu",
                "user": "yulunliu",
                "type": "user"
            },
            "summary": "LongSplat addresses critical challenges in novel view synthesis (NVS) from\ncasually captured long videos characterized by irregular camera motion, unknown\ncamera poses, and expansive scenes. Current methods often suffer from pose\ndrift, inaccurate geometry initialization, and severe memory limitations. To\naddress these issues, we introduce LongSplat, a robust unposed 3D Gaussian\nSplatting framework featuring: (1) Incremental Joint Optimization that\nconcurrently optimizes camera poses and 3D Gaussians to avoid local minima and\nensure global consistency; (2) a robust Pose Estimation Module leveraging\nlearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that\nconverts dense point clouds into anchors based on spatial density. Extensive\nexperiments on challenging benchmarks demonstrate that LongSplat achieves\nstate-of-the-art results, substantially improving rendering quality, pose\naccuracy, and computational efficiency compared to prior approaches. Project\npage: https://linjohnss.github.io/longsplat/",
            "upvotes": 40,
            "discussionId": "68a5270e6cf0bf898542ec18",
            "projectPage": "https://linjohnss.github.io/longsplat/",
            "githubRepo": "https://github.com/NVlabs/LongSplat",
            "ai_summary": "LongSplat improves novel view synthesis from long videos with irregular motion through joint optimization, robust pose estimation, and efficient anchor formation.",
            "ai_keywords": [
                "novel view synthesis",
                "3D Gaussian Splatting",
                "Incremental Joint Optimization",
                "camera poses",
                "3D Gaussians",
                "Pose Estimation Module",
                "3D priors",
                "Octree Anchor Formation",
                "dense point clouds",
                "anchors",
                "rendering quality",
                "pose accuracy",
                "computational efficiency"
            ],
            "githubStars": 102
        },
        "publishedAt": "2025-08-19T13:59:56.000Z",
        "title": "LongSplat: Robust Unposed 3D Gaussian Splatting for Casual Long Videos",
        "summary": "LongSplat addresses critical challenges in novel view synthesis (NVS) from\ncasually captured long videos characterized by irregular camera motion, unknown\ncamera poses, and expansive scenes. Current methods often suffer from pose\ndrift, inaccurate geometry initialization, and severe memory limitations. To\naddress these issues, we introduce LongSplat, a robust unposed 3D Gaussian\nSplatting framework featuring: (1) Incremental Joint Optimization that\nconcurrently optimizes camera poses and 3D Gaussians to avoid local minima and\nensure global consistency; (2) a robust Pose Estimation Module leveraging\nlearned 3D priors; and (3) an efficient Octree Anchor Formation mechanism that\nconverts dense point clouds into anchors based on spatial density. Extensive\nexperiments on challenging benchmarks demonstrate that LongSplat achieves\nstate-of-the-art results, substantially improving rendering quality, pose\naccuracy, and computational efficiency compared to prior approaches. Project\npage: https://linjohnss.github.io/longsplat/",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6459d5da3b6fafd9664807ab/dCg9OGuyLHLqwelz7AgUB.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14041.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6459d5da3b6fafd9664807ab",
            "avatarUrl": "/avatars/57430d1bbde3a2fe5586e5fbcafb0e74.svg",
            "fullname": "Yu-Lun Liu",
            "name": "yulunliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.13948",
            "authors": [
                {
                    "_id": "68a5366b6cf0bf898542ec8c",
                    "name": "Yuge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a5366b6cf0bf898542ec8d",
                    "name": "Nan Chen",
                    "hidden": false
                },
                {
                    "_id": "68a5366b6cf0bf898542ec8e",
                    "user": {
                        "_id": "62abdf657b037eafffc48808",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1655430982462-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Jiahang Xu",
                        "user": "Jiahang",
                        "type": "user"
                    },
                    "name": "Jiahang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:16.884Z",
                    "hidden": false
                },
                {
                    "_id": "68a5366b6cf0bf898542ec8f",
                    "name": "Yuqing Yang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6466d323ac657f60661d2778/rnVtFjaxiAUmMm2vyb7Uu.mp4"
            ],
            "publishedAt": "2025-08-19T15:37:29.000Z",
            "submittedOnDailyAt": "2025-08-20T01:28:00.780Z",
            "title": "Prompt Orchestration Markup Language",
            "submittedOnDailyBy": {
                "_id": "6466d323ac657f60661d2778",
                "avatarUrl": "/avatars/62f70630cdf1c252b80b4d5eaa5a4150.svg",
                "isPro": false,
                "fullname": "Yuge Zhang",
                "user": "ultmaster",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
            "upvotes": 24,
            "discussionId": "68a5366c6cf0bf898542ec90",
            "projectPage": "https://microsoft.github.io/poml/",
            "githubRepo": "https://github.com/microsoft/poml",
            "ai_summary": "POML addresses challenges in prompting Large Language Models by providing a structured, data-integrated, and format-sensitive markup language with templating and developer tools.",
            "ai_keywords": [
                "POML",
                "Prompt Orchestration Markup Language",
                "component-based markup",
                "specialized tags",
                "CSS-like styling system",
                "templating",
                "developer toolkit",
                "IDE support",
                "SDKs",
                "version control",
                "collaboration",
                "PomLink",
                "TableQA"
            ],
            "githubStars": 3559
        },
        "publishedAt": "2025-08-19T11:37:29.000Z",
        "title": "Prompt Orchestration Markup Language",
        "summary": "Large Language Models (LLMs) require sophisticated prompting, yet current\npractices face challenges in structure, data integration, format sensitivity,\nand tooling. Existing methods lack comprehensive solutions for organizing\ncomplex prompts involving diverse data types (documents, tables, images) or\nmanaging presentation variations systematically. To address these gaps, we\nintroduce POML (Prompt Orchestration Markup Language). POML employs\ncomponent-based markup for logical structure (roles, tasks, examples),\nspecialized tags for seamless data integration, and a CSS-like styling system\nto decouple content from presentation, reducing formatting sensitivity. It\nincludes templating for dynamic prompts and a comprehensive developer toolkit\n(IDE support, SDKs) to improve version control and collaboration. We validate\nPOML through two case studies demonstrating its impact on complex application\nintegration (PomLink) and accuracy performance (TableQA), as well as a user\nstudy assessing its effectiveness in real-world development scenarios.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6466d323ac657f60661d2778/rnVtFjaxiAUmMm2vyb7Uu.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13948.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6466d323ac657f60661d2778",
            "avatarUrl": "/avatars/62f70630cdf1c252b80b4d5eaa5a4150.svg",
            "fullname": "Yuge Zhang",
            "name": "ultmaster",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.06905",
            "authors": [
                {
                    "_id": "689d51b2b083e610d741e9e4",
                    "name": "Ruoxi Chen",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e5",
                    "name": "Dongping Chen",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e6",
                    "name": "Siyuan Wu",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e7",
                    "user": {
                        "_id": "67b589ab0914159f2e33f7d2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/qZRV_ePJ1zaRxZrg7Ea4K.jpeg",
                        "isPro": false,
                        "fullname": "Sinan Wang",
                        "user": "wsnHowest",
                        "type": "user"
                    },
                    "name": "Sinan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:53:24.787Z",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e8",
                    "name": "Shiyun Lang",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9e9",
                    "name": "Petr Sushko",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9ea",
                    "name": "Gaoyang Jiang",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9eb",
                    "name": "Yao Wan",
                    "hidden": false
                },
                {
                    "_id": "689d51b2b083e610d741e9ec",
                    "name": "Ranjay Krishna",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-09T09:36:21.000Z",
            "submittedOnDailyAt": "2025-08-20T03:20:45.314Z",
            "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
            "submittedOnDailyBy": {
                "_id": "643be8879f5d314db2d9ed23",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
                "isPro": false,
                "fullname": "Chen Dongping",
                "user": "shuaishuaicdp",
                "type": "user"
            },
            "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.",
            "upvotes": 14,
            "discussionId": "689d51b2b083e610d741e9ed",
            "ai_summary": "Experiments with multiple image-text models and agentic frameworks show that even state-of-the-art systems struggle with generating images from multiple visual references, highlighting the need for more flexible creative tools.",
            "ai_keywords": [
                "MultiRef-bench",
                "RefBlend",
                "MultiRef",
                "OmniGen",
                "ACE",
                "Show-o",
                "ChatDiT",
                "LLM + SD"
            ]
        },
        "publishedAt": "2025-08-09T05:36:21.000Z",
        "title": "MultiRef: Controllable Image Generation with Multiple Visual References",
        "summary": "Visual designers naturally draw inspiration from multiple visual references,\ncombining diverse elements and aesthetic principles to create artwork. However,\ncurrent image generative frameworks predominantly rely on single-source inputs\n-- either text prompts or individual reference images. In this paper, we focus\non the task of controllable image generation using multiple visual references.\nWe introduce MultiRef-bench, a rigorous evaluation framework comprising 990\nsynthetic and 1,000 real-world samples that require incorporating visual\ncontent from multiple reference images. The synthetic samples are synthetically\ngenerated through our data engine RefBlend, with 10 reference types and 33\nreference combinations. Based on RefBlend, we further construct a dataset\nMultiRef containing 38k high-quality images to facilitate further research. Our\nexperiments across three interleaved image-text models (i.e., OmniGen, ACE, and\nShow-o) and six agentic frameworks (e.g., ChatDiT and LLM + SD) reveal that\neven state-of-the-art systems struggle with multi-reference conditioning, with\nthe best model OmniGen achieving only 66.6% in synthetic samples and 79.0% in\nreal-world cases on average compared to the golden answer. These findings\nprovide valuable directions for developing more flexible and human-like\ncreative tools that can effectively integrate multiple sources of visual\ninspiration. The dataset is publicly available at: https://multiref.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.06905.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
            "fullname": "Chen Dongping",
            "name": "shuaishuaicdp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.09131",
            "authors": [
                {
                    "_id": "689ea8bea4caabb4320e5d9b",
                    "user": {
                        "_id": "6503ccaf13d750b4604649e4",
                        "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
                        "isPro": false,
                        "fullname": "Zixin Yin",
                        "user": "zachary-yin",
                        "type": "user"
                    },
                    "name": "Zixin Yin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-18T06:57:13.352Z",
                    "hidden": false
                },
                {
                    "_id": "689ea8bea4caabb4320e5d9c",
                    "name": "Xili Dai",
                    "hidden": false
                },
                {
                    "_id": "689ea8bea4caabb4320e5d9d",
                    "user": {
                        "_id": "63109a4d61cab0446e48c83b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63109a4d61cab0446e48c83b/JQlVkQp0ok586ND1GmB0w.png",
                        "isPro": false,
                        "fullname": "Ling-Hao Chen",
                        "user": "EvanTHU",
                        "type": "user"
                    },
                    "name": "Ling-Hao Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:53:12.927Z",
                    "hidden": false
                },
                {
                    "_id": "689ea8bea4caabb4320e5d9e",
                    "name": "Deyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "689ea8bea4caabb4320e5d9f",
                    "name": "Jianan Wang",
                    "hidden": false
                },
                {
                    "_id": "689ea8bea4caabb4320e5da0",
                    "user": {
                        "_id": "64ae9b88a22a179fc4d07992",
                        "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "dorni",
                        "type": "user"
                    },
                    "name": "Duomin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:53:16.419Z",
                    "hidden": false
                },
                {
                    "_id": "689ea8bea4caabb4320e5da1",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "689ea8bea4caabb4320e5da2",
                    "name": "Lionel M. Ni",
                    "hidden": false
                },
                {
                    "_id": "689ea8bea4caabb4320e5da3",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "689ea8bea4caabb4320e5da4",
                    "name": "Heung-Yeung Shum",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T17:57:04.000Z",
            "submittedOnDailyAt": "2025-08-20T01:14:07.228Z",
            "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion\n  Transformer",
            "submittedOnDailyBy": {
                "_id": "6503ccaf13d750b4604649e4",
                "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
                "isPro": false,
                "fullname": "Zixin Yin",
                "user": "zachary-yin",
                "type": "user"
            },
            "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.",
            "upvotes": 10,
            "discussionId": "689ea8bea4caabb4320e5da5",
            "ai_summary": "ColorCtrl, a training-free method using Multi-Modal Diffusion Transformers, achieves precise and consistent color editing in images and videos with word-level control and superior performance compared to existing approaches.",
            "ai_keywords": [
                "Multi-Modal Diffusion Transformers",
                "MM-DiT",
                "attention mechanisms",
                "attention maps",
                "value tokens",
                "color editing",
                "edit quality",
                "consistency",
                "temporal coherence",
                "editing stability",
                "instruction-based editing diffusion models"
            ]
        },
        "publishedAt": "2025-08-12T13:57:04.000Z",
        "title": "Training-Free Text-Guided Color Editing with Multi-Modal Diffusion\n  Transformer",
        "summary": "Text-guided color editing in images and videos is a fundamental yet unsolved\nproblem, requiring fine-grained manipulation of color attributes, including\nalbedo, light source color, and ambient lighting, while preserving physical\nconsistency in geometry, material properties, and light-matter interactions.\nExisting training-free methods offer broad applicability across editing tasks\nbut struggle with precise color control and often introduce visual\ninconsistency in both edited and non-edited regions. In this work, we present\nColorCtrl, a training-free color editing method that leverages the attention\nmechanisms of modern Multi-Modal Diffusion Transformers (MM-DiT). By\ndisentangling structure and color through targeted manipulation of attention\nmaps and value tokens, our method enables accurate and consistent color\nediting, along with word-level control of attribute intensity. Our method\nmodifies only the intended regions specified by the prompt, leaving unrelated\nareas untouched. Extensive experiments on both SD3 and FLUX.1-dev demonstrate\nthat ColorCtrl outperforms existing training-free approaches and achieves\nstate-of-the-art performances in both edit quality and consistency.\nFurthermore, our method surpasses strong commercial models such as FLUX.1\nKontext Max and GPT-4o Image Generation in terms of consistency. When extended\nto video models like CogVideoX, our approach exhibits greater advantages,\nparticularly in maintaining temporal coherence and editing stability. Finally,\nour method also generalizes to instruction-based editing diffusion models such\nas Step1X-Edit and FLUX.1 Kontext dev, further demonstrating its versatility.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09131.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6503ccaf13d750b4604649e4",
            "avatarUrl": "/avatars/96bebff9284d61f37c83e7da6a7e9bac.svg",
            "fullname": "Zixin Yin",
            "name": "zachary-yin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.08777",
            "authors": [
                {
                    "_id": "68a5825738160bfd39426a54",
                    "user": {
                        "_id": "65eed0a600f1a613dae4d9be",
                        "avatarUrl": "/avatars/a420d0e2c77a482eb5022deeccb2ebf5.svg",
                        "isPro": false,
                        "fullname": "Francesco Fabbri",
                        "user": "frafabbri",
                        "type": "user"
                    },
                    "name": "Francesco Fabbri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:50:51.265Z",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a55",
                    "name": "Gustavo Penha",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a56",
                    "name": "Edoardo D'Amico",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a57",
                    "name": "Alice Wang",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a58",
                    "user": {
                        "_id": "626193a918176506fab35c62",
                        "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
                        "isPro": false,
                        "fullname": "Marco De Nadai",
                        "user": "marcodena",
                        "type": "user"
                    },
                    "name": "Marco De Nadai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:50:59.013Z",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a59",
                    "name": "Jackie Doremus",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a5a",
                    "name": "Paul Gigioli",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a5b",
                    "name": "Andreas Damianou",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a5c",
                    "name": "Oskar Stal",
                    "hidden": false
                },
                {
                    "_id": "68a5825738160bfd39426a5d",
                    "name": "Mounia Lalmas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-12T09:23:35.000Z",
            "submittedOnDailyAt": "2025-08-20T06:38:30.390Z",
            "title": "Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge",
            "submittedOnDailyBy": {
                "_id": "626193a918176506fab35c62",
                "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
                "isPro": false,
                "fullname": "Marco De Nadai",
                "user": "marcodena",
                "type": "user"
            },
            "summary": "Evaluating personalized recommendations remains a central challenge,\nespecially in long-form audio domains like podcasts, where traditional offline\nmetrics suffer from exposure bias and online methods such as A/B testing are\ncostly and operationally constrained. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) as offline judges to\nassess the quality of podcast recommendations in a scalable and interpretable\nmanner. Our two-stage profile-aware approach first constructs natural-language\nuser profiles distilled from 90 days of listening history. These profiles\nsummarize both topical interests and behavioral patterns, serving as compact,\ninterpretable representations of user preferences. Rather than prompting the\nLLM with raw data, we use these profiles to provide high-level, semantically\nrich context-enabling the LLM to reason more effectively about alignment\nbetween a user's interests and recommended episodes. This reduces input\ncomplexity and improves interpretability. The LLM is then prompted to deliver\nfine-grained pointwise and pairwise judgments based on the profile-episode\nmatch. In a controlled study with 47 participants, our profile-aware judge\nmatched human judgments with high fidelity and outperformed or matched a\nvariant using raw listening histories. The framework enables efficient,\nprofile-aware evaluation for iterative testing and model selection in\nrecommender systems.",
            "upvotes": 10,
            "discussionId": "68a5826638160bfd39426a5e",
            "ai_summary": "A novel framework uses Large Language Models to evaluate podcast recommendations by constructing user profiles and providing context for the LLM to make judgments, improving efficiency and interpretability.",
            "ai_keywords": [
                "Large Language Models",
                "profile-aware",
                "natural-language user profiles",
                "semantic context",
                "pointwise judgments",
                "pairwise judgments",
                "recommender systems"
            ]
        },
        "publishedAt": "2025-08-12T05:23:35.000Z",
        "title": "Evaluating Podcast Recommendations with Profile-Aware LLM-as-a-Judge",
        "summary": "Evaluating personalized recommendations remains a central challenge,\nespecially in long-form audio domains like podcasts, where traditional offline\nmetrics suffer from exposure bias and online methods such as A/B testing are\ncostly and operationally constrained. In this paper, we propose a novel\nframework that leverages Large Language Models (LLMs) as offline judges to\nassess the quality of podcast recommendations in a scalable and interpretable\nmanner. Our two-stage profile-aware approach first constructs natural-language\nuser profiles distilled from 90 days of listening history. These profiles\nsummarize both topical interests and behavioral patterns, serving as compact,\ninterpretable representations of user preferences. Rather than prompting the\nLLM with raw data, we use these profiles to provide high-level, semantically\nrich context-enabling the LLM to reason more effectively about alignment\nbetween a user's interests and recommended episodes. This reduces input\ncomplexity and improves interpretability. The LLM is then prompted to deliver\nfine-grained pointwise and pairwise judgments based on the profile-episode\nmatch. In a controlled study with 47 participants, our profile-aware judge\nmatched human judgments with high fidelity and outperformed or matched a\nvariant using raw listening histories. The framework enables efficient,\nprofile-aware evaluation for iterative testing and model selection in\nrecommender systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08777.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626193a918176506fab35c62",
            "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
            "fullname": "Marco De Nadai",
            "name": "marcodena",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13998",
            "authors": [
                {
                    "_id": "68a58dd638160bfd39426a85",
                    "user": {
                        "_id": "6695eb6c0a6ba12a31c3a1be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/d1a6iQGvhxxSN0j-kFHkZ.png",
                        "isPro": false,
                        "fullname": "Yifu Yuan",
                        "user": "IffYuan",
                        "type": "user"
                    },
                    "name": "Yifu Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-20T19:40:29.884Z",
                    "hidden": false
                },
                {
                    "_id": "68a58dd638160bfd39426a86",
                    "name": "Haiqin Cui",
                    "hidden": false
                },
                {
                    "_id": "68a58dd638160bfd39426a87",
                    "name": "Yaoting Huang",
                    "hidden": false
                },
                {
                    "_id": "68a58dd638160bfd39426a88",
                    "name": "Yibin Chen",
                    "hidden": false
                },
                {
                    "_id": "68a58dd638160bfd39426a89",
                    "name": "Fei Ni",
                    "hidden": false
                },
                {
                    "_id": "68a58dd638160bfd39426a8a",
                    "name": "Zibin Dong",
                    "hidden": false
                },
                {
                    "_id": "68a58dd638160bfd39426a8b",
                    "name": "Pengyi Li",
                    "hidden": false
                },
                {
                    "_id": "68a58dd638160bfd39426a8c",
                    "name": "Yan Zheng",
                    "hidden": false
                },
                {
                    "_id": "68a58dd638160bfd39426a8d",
                    "name": "Jianye Hao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6695eb6c0a6ba12a31c3a1be/795caCYYfVfL4n_KSjwas.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/6695eb6c0a6ba12a31c3a1be/-YHwTAjNSVInRBFQgPZtl.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/6695eb6c0a6ba12a31c3a1be/GnGm7NT6GYGOnahqjsHPS.mp4"
            ],
            "publishedAt": "2025-08-19T16:50:01.000Z",
            "submittedOnDailyAt": "2025-08-20T07:30:21.817Z",
            "title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic\n  Manipulation",
            "submittedOnDailyBy": {
                "_id": "6695eb6c0a6ba12a31c3a1be",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/d1a6iQGvhxxSN0j-kFHkZ.png",
                "isPro": false,
                "fullname": "Yifu Yuan",
                "user": "IffYuan",
                "type": "user"
            },
            "summary": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which\nstems from data scarcity and embodiment heterogeneity. To address this, we\npioneer \"pointing\" as a unified, embodiment-agnostic intermediate\nrepresentation, defining four core embodied pointing abilities that bridge\nhigh-level vision-language comprehension with low-level action primitives. We\nintroduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed\nfor embodied reasoning and pointing. We use a wide range of embodied and\ngeneral visual reasoning datasets as sources to construct a large-scale\ndataset, Embodied-Points-200K, which supports key embodied pointing\ncapabilities. We then train Embodied-R1 using a two-stage Reinforced\nFine-tuning (RFT) curriculum with a specialized multi-task reward design.\nEmbodied-R1 achieves state-of-the-art performance on 11 embodied spatial and\npointing benchmarks. Critically, it demonstrates robust zero-shot\ngeneralization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%\nacross 8 real-world XArm tasks without any task-specific fine-tuning,\nrepresenting a 62% improvement over strong baselines. Furthermore, the model\nexhibits high robustness against diverse visual disturbances. Our work shows\nthat a pointing-centric representation, combined with an RFT training paradigm,\noffers an effective and generalizable pathway to closing the perception-action\ngap in robotics.",
            "upvotes": 9,
            "discussionId": "68a58dd638160bfd39426a8e",
            "projectPage": "https://embodied-r1.github.io/",
            "githubRepo": "https://github.com/pickxiguapi/Embodied-R1",
            "ai_summary": "A pointing-centric representation and reinforced fine-tuning approach improve embodied AI generalization across various tasks and visual disturbances.",
            "ai_keywords": [
                "Vision-Language Model",
                "embodied reasoning",
                "pointing",
                "Embodied-R1",
                "Embodied-Points-200K",
                "Reinforced Fine-tuning",
                "RFT",
                "embodied spatial benchmarks",
                "zero-shot generalization",
                "SIMPLEREnv",
                "XArm tasks",
                "robustness",
                "visual disturbances"
            ],
            "githubStars": 22
        },
        "publishedAt": "2025-08-19T12:50:01.000Z",
        "title": "Embodied-R1: Reinforced Embodied Reasoning for General Robotic\n  Manipulation",
        "summary": "Generalization in embodied AI is hindered by the \"seeing-to-doing gap,\" which\nstems from data scarcity and embodiment heterogeneity. To address this, we\npioneer \"pointing\" as a unified, embodiment-agnostic intermediate\nrepresentation, defining four core embodied pointing abilities that bridge\nhigh-level vision-language comprehension with low-level action primitives. We\nintroduce Embodied-R1, a 3B Vision-Language Model (VLM) specifically designed\nfor embodied reasoning and pointing. We use a wide range of embodied and\ngeneral visual reasoning datasets as sources to construct a large-scale\ndataset, Embodied-Points-200K, which supports key embodied pointing\ncapabilities. We then train Embodied-R1 using a two-stage Reinforced\nFine-tuning (RFT) curriculum with a specialized multi-task reward design.\nEmbodied-R1 achieves state-of-the-art performance on 11 embodied spatial and\npointing benchmarks. Critically, it demonstrates robust zero-shot\ngeneralization by achieving a 56.2% success rate in the SIMPLEREnv and 87.5%\nacross 8 real-world XArm tasks without any task-specific fine-tuning,\nrepresenting a 62% improvement over strong baselines. Furthermore, the model\nexhibits high robustness against diverse visual disturbances. Our work shows\nthat a pointing-centric representation, combined with an RFT training paradigm,\noffers an effective and generalizable pathway to closing the perception-action\ngap in robotics.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6695eb6c0a6ba12a31c3a1be/795caCYYfVfL4n_KSjwas.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/6695eb6c0a6ba12a31c3a1be/-YHwTAjNSVInRBFQgPZtl.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/6695eb6c0a6ba12a31c3a1be/GnGm7NT6GYGOnahqjsHPS.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13998.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6695eb6c0a6ba12a31c3a1be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/d1a6iQGvhxxSN0j-kFHkZ.png",
            "fullname": "Yifu Yuan",
            "name": "IffYuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12040",
            "authors": [
                {
                    "_id": "68a535566cf0bf898542ec5f",
                    "user": {
                        "_id": "67ed200efd37e21a47149075",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Sg7Sxp1LAFbRY906EJoiU.png",
                        "isPro": false,
                        "fullname": "Jinyi Han",
                        "user": "JinyiHan",
                        "type": "user"
                    },
                    "name": "Jinyi Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:39.018Z",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec60",
                    "name": "Tingyun Li",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec61",
                    "name": "Shisong Chen",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec62",
                    "name": "Jie Shi",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec63",
                    "name": "Xinyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec64",
                    "name": "Guanglei Yue",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec65",
                    "name": "Jiaqing Liang",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec66",
                    "name": "Xin Lin",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec67",
                    "name": "Liqian Wen",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec68",
                    "name": "Zulong Chen",
                    "hidden": false
                },
                {
                    "_id": "68a535566cf0bf898542ec69",
                    "name": "Yanghua Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-16T13:29:35.000Z",
            "submittedOnDailyAt": "2025-08-20T09:43:29.807Z",
            "title": "Mind the Generation Process: Fine-Grained Confidence Estimation During\n  LLM Generation",
            "submittedOnDailyBy": {
                "_id": "67ed200efd37e21a47149075",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Sg7Sxp1LAFbRY906EJoiU.png",
                "isPro": false,
                "fullname": "Jinyi Han",
                "user": "JinyiHan",
                "type": "user"
            },
            "summary": "While large language models (LLMs) have demonstrated remarkable performance\nacross diverse tasks, they fundamentally lack self-awareness and frequently\nexhibit overconfidence, assigning high confidence scores to incorrect\npredictions. Accurate confidence estimation is therefore critical for enhancing\nthe trustworthiness and reliability of LLM-generated outputs. However, existing\napproaches suffer from coarse-grained scoring mechanisms that fail to provide\nfine-grained, continuous confidence estimates throughout the generation\nprocess. To address these limitations, we introduce FineCE, a novel confidence\nestimation method that delivers accurate, fine-grained confidence scores during\ntext generation. Specifically, we first develop a comprehensive pipeline for\nconstructing training data that effectively captures the underlying\nprobabilistic distribution of LLM responses, and then train a model to predict\nconfidence scores for arbitrary text sequences in a supervised manner.\nFurthermore, we propose a Backward Confidence Integration (BCI) strategy that\nleverages information from the subsequent text to enhance confidence estimation\nfor the current sequence during inference. We also introduce three strategies\nfor identifying optimal positions to perform confidence estimation within the\ngeneration process. Extensive experiments on multiple benchmark datasets\ndemonstrate that FineCE consistently outperforms existing classical confidence\nestimation methods. Our code and all baselines used in the paper are available\non GitHub.",
            "upvotes": 9,
            "discussionId": "68a535566cf0bf898542ec6a",
            "ai_summary": "FineCE is a novel confidence estimation method for large language models that provides accurate, fine-grained confidence scores during text generation, outperforming existing methods.",
            "ai_keywords": [
                "large language models",
                "confidence estimation",
                "FineCE",
                "probabilistic distribution",
                "supervised learning",
                "Backward Confidence Integration",
                "BCI"
            ]
        },
        "publishedAt": "2025-08-16T09:29:35.000Z",
        "title": "Mind the Generation Process: Fine-Grained Confidence Estimation During\n  LLM Generation",
        "summary": "While large language models (LLMs) have demonstrated remarkable performance\nacross diverse tasks, they fundamentally lack self-awareness and frequently\nexhibit overconfidence, assigning high confidence scores to incorrect\npredictions. Accurate confidence estimation is therefore critical for enhancing\nthe trustworthiness and reliability of LLM-generated outputs. However, existing\napproaches suffer from coarse-grained scoring mechanisms that fail to provide\nfine-grained, continuous confidence estimates throughout the generation\nprocess. To address these limitations, we introduce FineCE, a novel confidence\nestimation method that delivers accurate, fine-grained confidence scores during\ntext generation. Specifically, we first develop a comprehensive pipeline for\nconstructing training data that effectively captures the underlying\nprobabilistic distribution of LLM responses, and then train a model to predict\nconfidence scores for arbitrary text sequences in a supervised manner.\nFurthermore, we propose a Backward Confidence Integration (BCI) strategy that\nleverages information from the subsequent text to enhance confidence estimation\nfor the current sequence during inference. We also introduce three strategies\nfor identifying optimal positions to perform confidence estimation within the\ngeneration process. Extensive experiments on multiple benchmark datasets\ndemonstrate that FineCE consistently outperforms existing classical confidence\nestimation methods. Our code and all baselines used in the paper are available\non GitHub.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12040.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ed200efd37e21a47149075",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Sg7Sxp1LAFbRY906EJoiU.png",
            "fullname": "Jinyi Han",
            "name": "JinyiHan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13632",
            "authors": [
                {
                    "_id": "68a533b66cf0bf898542ec55",
                    "user": {
                        "_id": "64a54e468cfaa458bd6844bf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a54e468cfaa458bd6844bf/5Gmf4tAr59GNl-2VaZDbu.png",
                        "isPro": false,
                        "fullname": "Yutong Feng",
                        "user": "fengyutong",
                        "type": "user"
                    },
                    "name": "Yutong Feng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:41.803Z",
                    "hidden": false
                },
                {
                    "_id": "68a533b66cf0bf898542ec56",
                    "name": "Linlin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a533b66cf0bf898542ec57",
                    "name": "Hengyuan Cao",
                    "hidden": false
                },
                {
                    "_id": "68a533b66cf0bf898542ec58",
                    "name": "Yiming Chen",
                    "hidden": false
                },
                {
                    "_id": "68a533b66cf0bf898542ec59",
                    "name": "Xiaoduan Feng",
                    "hidden": false
                },
                {
                    "_id": "68a533b66cf0bf898542ec5a",
                    "name": "Jian Cao",
                    "hidden": false
                },
                {
                    "_id": "68a533b66cf0bf898542ec5b",
                    "name": "Yuxiong Wu",
                    "hidden": false
                },
                {
                    "_id": "68a533b66cf0bf898542ec5c",
                    "name": "Bin Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/rhpuJxc1U-nbYvSCOLqJC.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/35kf2hrsNQcheYU8YiObo.jpeg"
            ],
            "publishedAt": "2025-08-19T08:47:31.000Z",
            "submittedOnDailyAt": "2025-08-20T01:08:25.013Z",
            "title": "OmniTry: Virtual Try-On Anything without Masks",
            "submittedOnDailyBy": {
                "_id": "64a54e468cfaa458bd6844bf",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a54e468cfaa458bd6844bf/5Gmf4tAr59GNl-2VaZDbu.png",
                "isPro": false,
                "fullname": "Yutong Feng",
                "user": "fengyutong",
                "type": "user"
            },
            "summary": "Virtual Try-ON (VTON) is a practical and widely-applied task, for which most\nof existing works focus on clothes. This paper presents OmniTry, a unified\nframework that extends VTON beyond garment to encompass any wearable objects,\ne.g., jewelries and accessories, with mask-free setting for more practical\napplication. When extending to various types of objects, data curation is\nchallenging for obtaining paired images, i.e., the object image and the\ncorresponding try-on result. To tackle this problem, we propose a two-staged\npipeline: For the first stage, we leverage large-scale unpaired images, i.e.,\nportraits with any wearable items, to train the model for mask-free\nlocalization. Specifically, we repurpose the inpainting model to automatically\ndraw objects in suitable positions given an empty mask. For the second stage,\nthe model is further fine-tuned with paired images to transfer the consistency\nof object appearance. We observed that the model after the first stage shows\nquick convergence even with few paired samples. OmniTry is evaluated on a\ncomprehensive benchmark consisting of 12 common classes of wearable objects,\nwith both in-shop and in-the-wild images. Experimental results suggest that\nOmniTry shows better performance on both object localization and\nID-preservation compared with existing methods. The code, model weights, and\nevaluation benchmark of OmniTry will be made publicly available at\nhttps://omnitry.github.io/.",
            "upvotes": 8,
            "discussionId": "68a533b66cf0bf898542ec5d",
            "projectPage": "https://omnitry.github.io/",
            "githubRepo": "https://github.com/Kunbyte-AI/OmniTry",
            "ai_summary": "OmniTry extends Virtual Try-ON to various wearable objects using a two-stage pipeline that combines unpaired and paired image training to improve object localization and appearance consistency.",
            "ai_keywords": [
                "Virtual Try-ON",
                "OmniTry",
                "mask-free localization",
                "inpainting model",
                "fine-tuning",
                "object localization",
                "ID-preservation"
            ],
            "githubStars": 40
        },
        "publishedAt": "2025-08-19T04:47:31.000Z",
        "title": "OmniTry: Virtual Try-On Anything without Masks",
        "summary": "Virtual Try-ON (VTON) is a practical and widely-applied task, for which most\nof existing works focus on clothes. This paper presents OmniTry, a unified\nframework that extends VTON beyond garment to encompass any wearable objects,\ne.g., jewelries and accessories, with mask-free setting for more practical\napplication. When extending to various types of objects, data curation is\nchallenging for obtaining paired images, i.e., the object image and the\ncorresponding try-on result. To tackle this problem, we propose a two-staged\npipeline: For the first stage, we leverage large-scale unpaired images, i.e.,\nportraits with any wearable items, to train the model for mask-free\nlocalization. Specifically, we repurpose the inpainting model to automatically\ndraw objects in suitable positions given an empty mask. For the second stage,\nthe model is further fine-tuned with paired images to transfer the consistency\nof object appearance. We observed that the model after the first stage shows\nquick convergence even with few paired samples. OmniTry is evaluated on a\ncomprehensive benchmark consisting of 12 common classes of wearable objects,\nwith both in-shop and in-the-wild images. Experimental results suggest that\nOmniTry shows better performance on both object localization and\nID-preservation compared with existing methods. The code, model weights, and\nevaluation benchmark of OmniTry will be made publicly available at\nhttps://omnitry.github.io/.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/rhpuJxc1U-nbYvSCOLqJC.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64a54e468cfaa458bd6844bf/35kf2hrsNQcheYU8YiObo.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13632.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a54e468cfaa458bd6844bf",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a54e468cfaa458bd6844bf/5Gmf4tAr59GNl-2VaZDbu.png",
            "fullname": "Yutong Feng",
            "name": "fengyutong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12903",
            "authors": [
                {
                    "_id": "68a5320b6cf0bf898542ec35",
                    "user": {
                        "_id": "67ed200efd37e21a47149075",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Sg7Sxp1LAFbRY906EJoiU.png",
                        "isPro": false,
                        "fullname": "Jinyi Han",
                        "user": "JinyiHan",
                        "type": "user"
                    },
                    "name": "Jinyi Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:44.028Z",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec36",
                    "name": "Xinyi Wang",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec37",
                    "name": "Haiquan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec38",
                    "name": "Tingyun li",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec39",
                    "name": "Zishang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec3a",
                    "name": "Sihang Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec3b",
                    "name": "Jiaqing Liang",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec3c",
                    "name": "Xin Lin",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec3d",
                    "name": "Weikang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec3e",
                    "name": "Zeye Sun",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec3f",
                    "name": "Fei Yu",
                    "hidden": false
                },
                {
                    "_id": "68a5320b6cf0bf898542ec40",
                    "name": "Yanghua Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T13:07:21.000Z",
            "submittedOnDailyAt": "2025-08-20T09:39:15.587Z",
            "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "67ed200efd37e21a47149075",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Sg7Sxp1LAFbRY906EJoiU.png",
                "isPro": false,
                "fullname": "Jinyi Han",
                "user": "JinyiHan",
                "type": "user"
            },
            "summary": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.",
            "upvotes": 8,
            "discussionId": "68a5320c6cf0bf898542ec41",
            "ai_summary": "ProActive Self-Refinement (PASR) dynamically enhances LLM outputs during generation, reducing token consumption and improving accuracy.",
            "ai_keywords": [
                "self-refinement",
                "large language models (LLMs)",
                "iterative refinement",
                "reactive process",
                "internal state",
                "evolving context",
                "ProActive Self-Refinement (PASR)",
                "Qwen3-8B"
            ]
        },
        "publishedAt": "2025-08-18T09:07:21.000Z",
        "title": "A Stitch in Time Saves Nine: Proactive Self-Refinement for Language\n  Models",
        "summary": "Recent advances in self-refinement have demonstrated significant potential\nfor improving the outputs of large language models (LLMs) through iterative\nrefinement. However, most existing self-refinement methods rely on a reactive\nprocess with a fixed number of iterations, making it difficult to determine the\noptimal timing and content of refinement based on the evolving generation\ncontext. Inspired by the way humans dynamically refine their thoughts during\nexecution, we propose ProActive Self-Refinement (PASR), a novel method that\nenables LLMs to refine their outputs during the generation process. Unlike\nmethods that regenerate entire responses, PASR proactively decides whether,\nwhen, and how to refine based on the model's internal state and evolving\ncontext. We conduct extensive experiments on a diverse set of 10 tasks to\nevaluate the effectiveness of PASR. Experimental results show that PASR\nsignificantly enhances problem-solving performance. In particular, on Qwen3-8B,\nPASR reduces average token consumption by 41.6 percent compared to standard\ngeneration, while also achieving an 8.2 percent improvement in accuracy. Our\ncode and all baselines used in the paper are available in the GitHub.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12903.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67ed200efd37e21a47149075",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Sg7Sxp1LAFbRY906EJoiU.png",
            "fullname": "Jinyi Han",
            "name": "JinyiHan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12669",
            "authors": [
                {
                    "_id": "68a53eea6cf0bf898542ecb6",
                    "name": "Bishanka Seal",
                    "hidden": false
                },
                {
                    "_id": "68a53eea6cf0bf898542ecb7",
                    "name": "Rahul Seetharaman",
                    "hidden": false
                },
                {
                    "_id": "68a53eea6cf0bf898542ecb8",
                    "name": "Aman Bansal",
                    "hidden": false
                },
                {
                    "_id": "68a53eea6cf0bf898542ecb9",
                    "user": {
                        "_id": "5f89da6c5d083370c711f37c",
                        "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
                        "isPro": false,
                        "fullname": "Abhilash Nandy",
                        "user": "abhi1nandy2",
                        "type": "user"
                    },
                    "name": "Abhilash Nandy",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-20T19:39:43.656Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T07:02:59.000Z",
            "submittedOnDailyAt": "2025-08-20T01:55:07.512Z",
            "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
            "submittedOnDailyBy": {
                "_id": "5f89da6c5d083370c711f37c",
                "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
                "isPro": false,
                "fullname": "Abhilash Nandy",
                "user": "abhi1nandy2",
                "type": "user"
            },
            "summary": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
            "upvotes": 8,
            "discussionId": "68a53eeb6cf0bf898542ecba",
            "ai_summary": "LLMs predict misery scores from text using various prompting strategies, with few-shot approaches outperforming zero-shot, and a gamified framework assessing their adaptability in emotional reasoning tasks.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "regression problem",
                "zero-shot",
                "few-shot",
                "BERT sentence embeddings",
                "Misery Game Show",
                "ordinal comparison",
                "binary classification",
                "scalar estimation",
                "feedback-driven reasoning"
            ]
        },
        "publishedAt": "2025-08-18T03:02:59.000Z",
        "title": "Leveraging Large Language Models for Predictive Analysis of Human Misery",
        "summary": "This study investigates the use of Large Language Models (LLMs) for\npredicting human-perceived misery scores from natural language descriptions of\nreal-world scenarios. The task is framed as a regression problem, where the\nmodel assigns a scalar value from 0 to 100 to each input statement. We evaluate\nmultiple prompting strategies, including zero-shot, fixed-context few-shot, and\nretrieval-based prompting using BERT sentence embeddings. Few-shot approaches\nconsistently outperform zero-shot baselines, underscoring the value of\ncontextual examples in affective prediction. To move beyond static evaluation,\nwe introduce the \"Misery Game Show\", a novel gamified framework inspired by a\ntelevision format. It tests LLMs through structured rounds involving ordinal\ncomparison, binary classification, scalar estimation, and feedback-driven\nreasoning. This setup enables us to assess not only predictive accuracy but\nalso the model's ability to adapt based on corrective feedback. The gamified\nevaluation highlights the broader potential of LLMs in dynamic emotional\nreasoning tasks beyond standard regression. Code and data link:\nhttps://github.com/abhi1nandy2/Misery_Data_Exps_GitHub",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12669.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f89da6c5d083370c711f37c",
            "avatarUrl": "/avatars/c2f17a4a636973817fd5da2ae6dbaac3.svg",
            "fullname": "Abhilash Nandy",
            "name": "abhi1nandy2",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.10830",
            "authors": [
                {
                    "_id": "689e9b67a4caabb4320e5d1d",
                    "user": {
                        "_id": "6387676c23da90491eb9fb16",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Kai Li",
                        "user": "JusperLee",
                        "type": "user"
                    },
                    "name": "Kai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:53:18.572Z",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d1e",
                    "name": "Guo Chen",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d1f",
                    "name": "Wendi Sang",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d20",
                    "name": "Yi Luo",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d21",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d22",
                    "name": "Shuai Wang",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d23",
                    "name": "Shulin He",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d24",
                    "name": "Zhong-Qiu Wang",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d25",
                    "name": "Andong Li",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d26",
                    "name": "Zhiyong Wu",
                    "hidden": false
                },
                {
                    "_id": "689e9b67a4caabb4320e5d27",
                    "name": "Xiaolin Hu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-14T16:54:34.000Z",
            "submittedOnDailyAt": "2025-08-20T06:05:33.367Z",
            "title": "Advances in Speech Separation: Techniques, Challenges, and Future Trends",
            "submittedOnDailyBy": {
                "_id": "6387676c23da90491eb9fb16",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
                "isPro": false,
                "fullname": "Kai Li",
                "user": "JusperLee",
                "type": "user"
            },
            "summary": "The field of speech separation, addressing the \"cocktail party problem\", has\nseen revolutionary advances with DNNs. Speech separation enhances clarity in\ncomplex acoustic environments and serves as crucial pre-processing for speech\nrecognition and speaker recognition. However, current literature focuses\nnarrowly on specific architectures or isolated approaches, creating fragmented\nunderstanding. This survey addresses this gap by providing systematic\nexamination of DNN-based speech separation techniques. Our work differentiates\nitself through: (I) Comprehensive perspective: We systematically investigate\nlearning paradigms, separation scenarios with known/unknown speakers,\ncomparative analysis of supervised/self-supervised/unsupervised frameworks, and\narchitectural components from encoders to estimation strategies. (II)\nTimeliness: Coverage of cutting-edge developments ensures access to current\ninnovations and benchmarks. (III) Unique insights: Beyond summarization, we\nevaluate technological trajectories, identify emerging patterns, and highlight\npromising directions including domain-robust frameworks, efficient\narchitectures, multimodal integration, and novel self-supervised paradigms.\n(IV) Fair evaluation: We provide quantitative evaluations on standard datasets,\nrevealing true capabilities and limitations of different methods. This\ncomprehensive survey serves as an accessible reference for experienced\nresearchers and newcomers navigating speech separation's complex landscape.",
            "upvotes": 8,
            "discussionId": "689e9b68a4caabb4320e5d28",
            "projectPage": "https://cslikai.cn/Speech-Separation-Paper-Tutorial",
            "githubRepo": "https://github.com/JusperLee/Speech-Separation-Paper-Tutorial",
            "ai_summary": "A survey of DNN-based speech separation techniques, covering learning paradigms, separation scenarios, and architectural components, with a focus on current advancements and promising future directions.",
            "ai_keywords": [
                "DNNs",
                "speech separation",
                "cocktail party problem",
                "learning paradigms",
                "separation scenarios",
                "supervised frameworks",
                "self-supervised frameworks",
                "unsupervised frameworks",
                "encoders",
                "estimation strategies",
                "domain-robust frameworks",
                "efficient architectures",
                "multimodal integration",
                "novel self-supervised paradigms"
            ],
            "githubStars": 803
        },
        "publishedAt": "2025-08-14T12:54:34.000Z",
        "title": "Advances in Speech Separation: Techniques, Challenges, and Future Trends",
        "summary": "The field of speech separation, addressing the \"cocktail party problem\", has\nseen revolutionary advances with DNNs. Speech separation enhances clarity in\ncomplex acoustic environments and serves as crucial pre-processing for speech\nrecognition and speaker recognition. However, current literature focuses\nnarrowly on specific architectures or isolated approaches, creating fragmented\nunderstanding. This survey addresses this gap by providing systematic\nexamination of DNN-based speech separation techniques. Our work differentiates\nitself through: (I) Comprehensive perspective: We systematically investigate\nlearning paradigms, separation scenarios with known/unknown speakers,\ncomparative analysis of supervised/self-supervised/unsupervised frameworks, and\narchitectural components from encoders to estimation strategies. (II)\nTimeliness: Coverage of cutting-edge developments ensures access to current\ninnovations and benchmarks. (III) Unique insights: Beyond summarization, we\nevaluate technological trajectories, identify emerging patterns, and highlight\npromising directions including domain-robust frameworks, efficient\narchitectures, multimodal integration, and novel self-supervised paradigms.\n(IV) Fair evaluation: We provide quantitative evaluations on standard datasets,\nrevealing true capabilities and limitations of different methods. This\ncomprehensive survey serves as an accessible reference for experienced\nresearchers and newcomers navigating speech separation's complex landscape.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10830.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6387676c23da90491eb9fb16",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669818175965-noauth.jpeg",
            "fullname": "Kai Li",
            "name": "JusperLee",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 25
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04324",
            "authors": [
                {
                    "_id": "68a56e446cf0bf898542ed36",
                    "user": {
                        "_id": "66974e7a57a5a55a1f4a0469",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66974e7a57a5a55a1f4a0469/76WAtxoxJO5rjZWxEmF2t.jpeg",
                        "isPro": false,
                        "fullname": "xxh",
                        "user": "shreddedpork",
                        "type": "user"
                    },
                    "name": "Xiaoxuan He",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:12:50.108Z",
                    "hidden": false
                },
                {
                    "_id": "68a56e446cf0bf898542ed37",
                    "name": "Siming Fu",
                    "hidden": false
                },
                {
                    "_id": "68a56e446cf0bf898542ed38",
                    "name": "Yuke Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a56e446cf0bf898542ed39",
                    "name": "Wanli Li",
                    "hidden": false
                },
                {
                    "_id": "68a56e446cf0bf898542ed3a",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68a56e446cf0bf898542ed3b",
                    "name": "Dacheng Yin",
                    "hidden": false
                },
                {
                    "_id": "68a56e446cf0bf898542ed3c",
                    "name": "Fengyun Rao",
                    "hidden": false
                },
                {
                    "_id": "68a56e446cf0bf898542ed3d",
                    "name": "Bo Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T11:10:39.000Z",
            "submittedOnDailyAt": "2025-08-20T05:16:03.933Z",
            "title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models",
            "submittedOnDailyBy": {
                "_id": "6485dd6d07a2c1915060f603",
                "avatarUrl": "/avatars/8594d647359a7d19ab29b8ec91d1444e.svg",
                "isPro": false,
                "fullname": "fu",
                "user": "simingfu",
                "type": "user"
            },
            "summary": "Recent flow matching models for text-to-image generation have achieved\nremarkable quality, yet their integration with reinforcement learning for human\npreference alignment remains suboptimal, hindering fine-grained reward-based\noptimization. We observe that the key impediment to effective GRPO training of\nflow models is the temporal uniformity assumption in existing approaches:\nsparse terminal rewards with uniform credit assignment fail to capture the\nvarying criticality of decisions across generation timesteps, resulting in\ninefficient exploration and suboptimal convergence. To remedy this shortcoming,\nwe introduce TempFlow-GRPO (Temporal Flow GRPO), a principled GRPO\nframework that captures and exploits the temporal structure inherent in\nflow-based generation. TempFlow-GRPO introduces two key innovations: (i) a\ntrajectory branching mechanism that provides process rewards by concentrating\nstochasticity at designated branching points, enabling precise credit\nassignment without requiring specialized intermediate reward models; and (ii) a\nnoise-aware weighting scheme that modulates policy optimization according to\nthe intrinsic exploration potential of each timestep, prioritizing learning\nduring high-impact early stages while ensuring stable refinement in later\nphases. These innovations endow the model with temporally-aware optimization\nthat respects the underlying generative dynamics, leading to state-of-the-art\nperformance in human preference alignment and standard text-to-image\nbenchmarks.",
            "upvotes": 6,
            "discussionId": "68a56e446cf0bf898542ed3e",
            "projectPage": "https://tempflowgrpo.github.io/",
            "githubRepo": "https://github.com/Shredded-Pork/TempFlow-GRPO",
            "ai_summary": "TempFlow-GRPO enhances text-to-image generation by addressing temporal credit assignment and noise-aware optimization in flow models, improving human preference alignment and benchmark performance.",
            "ai_keywords": [
                "flow matching models",
                "text-to-image generation",
                "reinforcement learning",
                "human preference alignment",
                "GRPO",
                "temporal uniformity assumption",
                "trajectory branching mechanism",
                "process rewards",
                "noise-aware weighting scheme",
                "temporally-aware optimization",
                "generative dynamics"
            ],
            "githubStars": 128
        },
        "publishedAt": "2025-08-06T07:10:39.000Z",
        "title": "TempFlow-GRPO: When Timing Matters for GRPO in Flow Models",
        "summary": "Recent flow matching models for text-to-image generation have achieved\nremarkable quality, yet their integration with reinforcement learning for human\npreference alignment remains suboptimal, hindering fine-grained reward-based\noptimization. We observe that the key impediment to effective GRPO training of\nflow models is the temporal uniformity assumption in existing approaches:\nsparse terminal rewards with uniform credit assignment fail to capture the\nvarying criticality of decisions across generation timesteps, resulting in\ninefficient exploration and suboptimal convergence. To remedy this shortcoming,\nwe introduce TempFlow-GRPO (Temporal Flow GRPO), a principled GRPO\nframework that captures and exploits the temporal structure inherent in\nflow-based generation. TempFlow-GRPO introduces two key innovations: (i) a\ntrajectory branching mechanism that provides process rewards by concentrating\nstochasticity at designated branching points, enabling precise credit\nassignment without requiring specialized intermediate reward models; and (ii) a\nnoise-aware weighting scheme that modulates policy optimization according to\nthe intrinsic exploration potential of each timestep, prioritizing learning\nduring high-impact early stages while ensuring stable refinement in later\nphases. These innovations endow the model with temporally-aware optimization\nthat respects the underlying generative dynamics, leading to state-of-the-art\nperformance in human preference alignment and standard text-to-image\nbenchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04324.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6485dd6d07a2c1915060f603",
            "avatarUrl": "/avatars/8594d647359a7d19ab29b8ec91d1444e.svg",
            "fullname": "fu",
            "name": "simingfu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.12845",
            "authors": [
                {
                    "_id": "68a5c37fc4b3ea17d2d2cfff",
                    "user": {
                        "_id": "6638a1f2ef9d012052d339a2",
                        "avatarUrl": "/avatars/e11767ad04c26fa84966017d5c43dbaa.svg",
                        "isPro": false,
                        "fullname": "Artem Pshenitsyn",
                        "user": "Square596",
                        "type": "user"
                    },
                    "name": "Artem Pshenitsyn",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T19:11:42.553Z",
                    "hidden": false
                },
                {
                    "_id": "68a5c37fc4b3ea17d2d2d000",
                    "name": "Aleksandr Panov",
                    "hidden": false
                },
                {
                    "_id": "68a5c37fc4b3ea17d2d2d001",
                    "name": "Alexey Skrynnik",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/652ced57756a15d750266362/XemB8hiI_zycg0ou5i4vQ.png"
            ],
            "publishedAt": "2025-08-18T11:32:26.000Z",
            "submittedOnDailyAt": "2025-08-20T11:21:15.331Z",
            "title": "CAMAR: Continuous Actions Multi-Agent Routing",
            "submittedOnDailyBy": {
                "_id": "652ced57756a15d750266362",
                "avatarUrl": "/avatars/5ef33e4935ccc6e930ceb2475c270bb1.svg",
                "isPro": false,
                "fullname": "Alexey Skrynnik",
                "user": "tviskaron",
                "type": "user"
            },
            "summary": "Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving\ncooperative and competitive decision-making problems. While many MARL\nbenchmarks have been proposed, few combine continuous state and action spaces\nwith challenging coordination and planning tasks. We introduce CAMAR, a new\nMARL benchmark designed explicitly for multi-agent pathfinding in environments\nwith continuous actions. CAMAR supports cooperative and competitive\ninteractions between agents and runs efficiently at up to 100,000 environment\nsteps per second. We also propose a three-tier evaluation protocol to better\ntrack algorithmic progress and enable deeper analysis of performance. In\naddition, CAMAR allows the integration of classical planning methods such as\nRRT and RRT* into MARL pipelines. We use them as standalone baselines and\ncombine RRT* with popular MARL algorithms to create hybrid approaches. We\nprovide a suite of test scenarios and benchmarking tools to ensure\nreproducibility and fair comparison. Experiments show that CAMAR presents a\nchallenging and realistic testbed for the MARL community.",
            "upvotes": 5,
            "discussionId": "68a5c37fc4b3ea17d2d2d002",
            "ai_summary": "CAMAR is a new MARL benchmark for continuous action pathfinding that supports efficient evaluation and hybrid approaches with classical planning methods.",
            "ai_keywords": [
                "multi-agent reinforcement learning",
                "MARL",
                "continuous state and action spaces",
                "multi-agent pathfinding",
                "three-tier evaluation protocol",
                "RRT",
                "RRT*",
                "hybrid approaches",
                "test scenarios",
                "benchmarking tools"
            ]
        },
        "publishedAt": "2025-08-18T07:32:26.000Z",
        "title": "CAMAR: Continuous Actions Multi-Agent Routing",
        "summary": "Multi-agent reinforcement learning (MARL) is a powerful paradigm for solving\ncooperative and competitive decision-making problems. While many MARL\nbenchmarks have been proposed, few combine continuous state and action spaces\nwith challenging coordination and planning tasks. We introduce CAMAR, a new\nMARL benchmark designed explicitly for multi-agent pathfinding in environments\nwith continuous actions. CAMAR supports cooperative and competitive\ninteractions between agents and runs efficiently at up to 100,000 environment\nsteps per second. We also propose a three-tier evaluation protocol to better\ntrack algorithmic progress and enable deeper analysis of performance. In\naddition, CAMAR allows the integration of classical planning methods such as\nRRT and RRT* into MARL pipelines. We use them as standalone baselines and\ncombine RRT* with popular MARL algorithms to create hybrid approaches. We\nprovide a suite of test scenarios and benchmarking tools to ensure\nreproducibility and fair comparison. Experiments show that CAMAR presents a\nchallenging and realistic testbed for the MARL community.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/652ced57756a15d750266362/XemB8hiI_zycg0ou5i4vQ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12845.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652ced57756a15d750266362",
            "avatarUrl": "/avatars/5ef33e4935ccc6e930ceb2475c270bb1.svg",
            "fullname": "Alexey Skrynnik",
            "name": "tviskaron",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.11548",
            "authors": [
                {
                    "_id": "68a28870a4caabb4320e63cc",
                    "name": "Zhenhua Xu",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63cd",
                    "name": "Xubin Yue",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63ce",
                    "user": {
                        "_id": "6746d09dfb7a1de1dca3cc21",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6746d09dfb7a1de1dca3cc21/uGNJVBm3g_O6N_R77yoMs.png",
                        "isPro": false,
                        "fullname": "Zhebo Wang",
                        "user": "BreynaldDva",
                        "type": "user"
                    },
                    "name": "Zhebo Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-18T06:53:58.643Z",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63cf",
                    "name": "Qichen Liu",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63d0",
                    "name": "Xixiang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63d1",
                    "name": "Jingxuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63d2",
                    "name": "Wenjun Zeng",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63d3",
                    "name": "Wengpeng Xing",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63d4",
                    "name": "Dezhang Kong",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63d5",
                    "name": "Changting Lin",
                    "hidden": false
                },
                {
                    "_id": "68a28870a4caabb4320e63d6",
                    "name": "Meng Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-15T15:50:20.000Z",
            "submittedOnDailyAt": "2025-08-20T06:00:06.343Z",
            "title": "Copyright Protection for Large Language Models: A Survey of Methods,\n  Challenges, and Trends",
            "submittedOnDailyBy": {
                "_id": "6746d09dfb7a1de1dca3cc21",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6746d09dfb7a1de1dca3cc21/uGNJVBm3g_O6N_R77yoMs.png",
                "isPro": false,
                "fullname": "Zhebo Wang",
                "user": "BreynaldDva",
                "type": "user"
            },
            "summary": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property.",
            "upvotes": 5,
            "discussionId": "68a28871a4caabb4320e63d7",
            "githubRepo": "https://github.com/Xuzhenhua55/awesome-llm-copyright-protection",
            "ai_summary": "A survey of LLM copyright protection technologies, focusing on model fingerprinting, clarifies its relationship with text watermarking and provides a comprehensive overview of fingerprinting techniques, evaluation metrics, and open challenges.",
            "ai_keywords": [
                "text watermarking",
                "model watermarking",
                "model fingerprinting",
                "fingerprint transfer",
                "fingerprint removal",
                "evaluation metrics",
                "effectiveness",
                "harmlessness",
                "robustness",
                "stealthiness",
                "reliability"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-08-15T11:50:20.000Z",
        "title": "Copyright Protection for Large Language Models: A Survey of Methods,\n  Challenges, and Trends",
        "summary": "Copyright protection for large language models is of critical importance,\ngiven their substantial development costs, proprietary value, and potential for\nmisuse. Existing surveys have predominantly focused on techniques for tracing\nLLM-generated content-namely, text watermarking-while a systematic exploration\nof methods for protecting the models themselves (i.e., model watermarking and\nmodel fingerprinting) remains absent. Moreover, the relationships and\ndistinctions among text watermarking, model watermarking, and model\nfingerprinting have not been comprehensively clarified. This work presents a\ncomprehensive survey of the current state of LLM copyright protection\ntechnologies, with a focus on model fingerprinting, covering the following\naspects: (1) clarifying the conceptual connection from text watermarking to\nmodel watermarking and fingerprinting, and adopting a unified terminology that\nincorporates model watermarking into the broader fingerprinting framework; (2)\nproviding an overview and comparison of diverse text watermarking techniques,\nhighlighting cases where such methods can function as model fingerprinting; (3)\nsystematically categorizing and comparing existing model fingerprinting\napproaches for LLM copyright protection; (4) presenting, for the first time,\ntechniques for fingerprint transfer and fingerprint removal; (5) summarizing\nevaluation metrics for model fingerprints, including effectiveness,\nharmlessness, robustness, stealthiness, and reliability; and (6) discussing\nopen challenges and future research directions. This survey aims to offer\nresearchers a thorough understanding of both text watermarking and model\nfingerprinting technologies in the era of LLMs, thereby fostering further\nadvances in protecting their intellectual property.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11548.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6746d09dfb7a1de1dca3cc21",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6746d09dfb7a1de1dca3cc21/uGNJVBm3g_O6N_R77yoMs.png",
            "fullname": "Zhebo Wang",
            "name": "BreynaldDva",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.09789",
            "authors": [
                {
                    "_id": "68a581fd38160bfd39426a4f",
                    "user": {
                        "_id": "626193a918176506fab35c62",
                        "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
                        "isPro": false,
                        "fullname": "Marco De Nadai",
                        "user": "marcodena",
                        "type": "user"
                    },
                    "name": "Marco De Nadai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:01.302Z",
                    "hidden": false
                },
                {
                    "_id": "68a581fd38160bfd39426a50",
                    "name": "Andreas Damianou",
                    "hidden": false
                },
                {
                    "_id": "68a581fd38160bfd39426a51",
                    "name": "Mounia Lalmas",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T13:19:31.000Z",
            "submittedOnDailyAt": "2025-08-20T06:37:15.440Z",
            "title": "Describe What You See with Multimodal Large Language Models to Enhance\n  Video Recommendations",
            "submittedOnDailyBy": {
                "_id": "626193a918176506fab35c62",
                "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
                "isPro": false,
                "fullname": "Marco De Nadai",
                "user": "marcodena",
                "type": "user"
            },
            "summary": "Existing video recommender systems rely primarily on user-defined metadata or\non low-level visual and acoustic signals extracted by specialised encoders.\nThese low-level features describe what appears on the screen but miss deeper\nsemantics such as intent, humour, and world knowledge that make clips resonate\nwith viewers. For example, is a 30-second clip simply a singer on a rooftop, or\nan ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such\ndistinctions are critical to personalised recommendations yet remain invisible\nto traditional encoding pipelines. In this paper, we introduce a simple,\nrecommendation system-agnostic zero-finetuning framework that injects\nhigh-level semantics into the recommendation pipeline by prompting an\noff-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip\ninto a rich natural-language description (e.g. \"a superhero parody with\nslapstick fights and orchestral stabs\"), bridging the gap between raw content\nand user intent. We use MLLM output with a state-of-the-art text encoder and\nfeed it into standard collaborative, content-based, and generative\nrecommenders. On the MicroLens-100K dataset, which emulates user interactions\nwith TikTok-style videos, our framework consistently surpasses conventional\nvideo, audio, and metadata features in five representative models. Our findings\nhighlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to\nbuild more intent-aware video recommenders.",
            "upvotes": 4,
            "discussionId": "68a581fe38160bfd39426a52",
            "ai_summary": "A zero-finetuning framework uses Multimodal Large Language Models to inject high-level semantics into video recommendations, improving intent-awareness over traditional methods.",
            "ai_keywords": [
                "Multimodal Large Language Model",
                "MLLM",
                "natural-language description",
                "text encoder",
                "collaborative recommenders",
                "content-based recommenders",
                "generative recommenders",
                "MicroLens-100K dataset",
                "TikTok-style videos",
                "intent-aware video recommenders"
            ]
        },
        "publishedAt": "2025-08-13T09:19:31.000Z",
        "title": "Describe What You See with Multimodal Large Language Models to Enhance\n  Video Recommendations",
        "summary": "Existing video recommender systems rely primarily on user-defined metadata or\non low-level visual and acoustic signals extracted by specialised encoders.\nThese low-level features describe what appears on the screen but miss deeper\nsemantics such as intent, humour, and world knowledge that make clips resonate\nwith viewers. For example, is a 30-second clip simply a singer on a rooftop, or\nan ironic parody filmed amid the fairy chimneys of Cappadocia, Turkey? Such\ndistinctions are critical to personalised recommendations yet remain invisible\nto traditional encoding pipelines. In this paper, we introduce a simple,\nrecommendation system-agnostic zero-finetuning framework that injects\nhigh-level semantics into the recommendation pipeline by prompting an\noff-the-shelf Multimodal Large Language Model (MLLM) to summarise each clip\ninto a rich natural-language description (e.g. \"a superhero parody with\nslapstick fights and orchestral stabs\"), bridging the gap between raw content\nand user intent. We use MLLM output with a state-of-the-art text encoder and\nfeed it into standard collaborative, content-based, and generative\nrecommenders. On the MicroLens-100K dataset, which emulates user interactions\nwith TikTok-style videos, our framework consistently surpasses conventional\nvideo, audio, and metadata features in five representative models. Our findings\nhighlight the promise of leveraging MLLMs as on-the-fly knowledge extractors to\nbuild more intent-aware video recommenders.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.09789.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "626193a918176506fab35c62",
            "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
            "fullname": "Marco De Nadai",
            "name": "marcodena",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13992",
            "authors": [
                {
                    "_id": "68a539856cf0bf898542ec92",
                    "name": "Sonal Kumar",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec93",
                    "name": "Šimon Sedláček",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec94",
                    "name": "Vaibhavi Lokegaonkar",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec95",
                    "name": "Fernando López",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec96",
                    "name": "Wenyi Yu",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec97",
                    "name": "Nishit Anand",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec98",
                    "name": "Hyeonggon Ryu",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec99",
                    "name": "Lichang Chen",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec9a",
                    "name": "Maxim Plička",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec9b",
                    "name": "Miroslav Hlaváček",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec9c",
                    "name": "William Fineas Ellingwood",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec9d",
                    "name": "Sathvik Udupa",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec9e",
                    "name": "Siyuan Hou",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ec9f",
                    "name": "Allison Ferner",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca0",
                    "name": "Sara Barahona",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca1",
                    "name": "Cecilia Bolaños",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca2",
                    "name": "Satish Rahi",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca3",
                    "name": "Laura Herrera-Alarcón",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca4",
                    "name": "Satvik Dixit",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca5",
                    "name": "Siddhi Patil",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca6",
                    "name": "Soham Deshmukh",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca7",
                    "name": "Lasha Koroshinadze",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca8",
                    "name": "Yao Liu",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542eca9",
                    "name": "Leibny Paola Garcia Perera",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecaa",
                    "name": "Eleni Zanou",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecab",
                    "name": "Themos Stafylakis",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecac",
                    "name": "Joon Son Chung",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecad",
                    "name": "David Harwath",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecae",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecaf",
                    "name": "Dinesh Manocha",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecb0",
                    "name": "Alicia Lozano-Diez",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecb1",
                    "name": "Santosh Kesiraju",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecb2",
                    "user": {
                        "_id": "62c9664eb34e600d7eaa4beb",
                        "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
                        "isPro": false,
                        "fullname": "Ghosh",
                        "user": "Sreyan88",
                        "type": "user"
                    },
                    "name": "Sreyan Ghosh",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:15.060Z",
                    "hidden": false
                },
                {
                    "_id": "68a539856cf0bf898542ecb3",
                    "name": "Ramani Duraiswami",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T16:33:49.000Z",
            "submittedOnDailyAt": "2025-08-20T01:34:00.837Z",
            "title": "MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic\n  Evaluation of Audio General Intelligence",
            "submittedOnDailyBy": {
                "_id": "62c9664eb34e600d7eaa4beb",
                "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
                "isPro": false,
                "fullname": "Ghosh",
                "user": "Sreyan88",
                "type": "user"
            },
            "summary": "Audio comprehension-including speech, non-speech sounds, and music-is\nessential for achieving human-level intelligence. Consequently, AI agents must\ndemonstrate holistic audio understanding to qualify as generally intelligent.\nHowever, evaluating auditory intelligence comprehensively remains challenging.\nTo address this gap, we introduce MMAU-Pro, the most comprehensive and\nrigorously curated benchmark for assessing audio intelligence in AI systems.\nMMAU-Pro contains 5,305 instances, where each instance has one or more audios\npaired with human expert-generated question-answer pairs, spanning speech,\nsound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro\nevaluates auditory intelligence across 49 unique skills and multiple complex\ndimensions, including long-form audio comprehension, spatial audio reasoning,\nmulti-audio understanding, among others. All questions are meticulously\ndesigned to require deliberate multi-hop reasoning, including both\nmultiple-choice and open-ended response formats. Importantly, audio data is\nsourced directly ``from the wild\" rather than from existing datasets with known\ndistributions. We evaluate 22 leading open-source and proprietary multimodal AI\nmodels, revealing significant limitations: even state-of-the-art models such as\nGemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy,\nrespectively, approaching random performance in multiple categories. Our\nextensive analysis highlights specific shortcomings and provides novel\ninsights, offering actionable perspectives for the community to enhance future\nAI systems' progression toward audio general intelligence. The benchmark and\ncode is available at https://sonalkum.github.io/mmau-pro.",
            "upvotes": 3,
            "discussionId": "68a539866cf0bf898542ecb4",
            "ai_summary": "MMAU-Pro is a comprehensive benchmark for evaluating audio intelligence in AI systems, assessing 49 unique skills across speech, sound, music, and their combinations, revealing significant limitations in current models.",
            "ai_keywords": [
                "MMAU-Pro",
                "long-form audio comprehension",
                "spatial audio reasoning",
                "multi-audio understanding",
                "multi-hop reasoning",
                "multimodal AI models",
                "Gemini 2.5 Flash",
                "Audio Flamingo 3"
            ]
        },
        "publishedAt": "2025-08-19T12:33:49.000Z",
        "title": "MMAU-Pro: A Challenging and Comprehensive Benchmark for Holistic\n  Evaluation of Audio General Intelligence",
        "summary": "Audio comprehension-including speech, non-speech sounds, and music-is\nessential for achieving human-level intelligence. Consequently, AI agents must\ndemonstrate holistic audio understanding to qualify as generally intelligent.\nHowever, evaluating auditory intelligence comprehensively remains challenging.\nTo address this gap, we introduce MMAU-Pro, the most comprehensive and\nrigorously curated benchmark for assessing audio intelligence in AI systems.\nMMAU-Pro contains 5,305 instances, where each instance has one or more audios\npaired with human expert-generated question-answer pairs, spanning speech,\nsound, music, and their combinations. Unlike existing benchmarks, MMAU-Pro\nevaluates auditory intelligence across 49 unique skills and multiple complex\ndimensions, including long-form audio comprehension, spatial audio reasoning,\nmulti-audio understanding, among others. All questions are meticulously\ndesigned to require deliberate multi-hop reasoning, including both\nmultiple-choice and open-ended response formats. Importantly, audio data is\nsourced directly ``from the wild\" rather than from existing datasets with known\ndistributions. We evaluate 22 leading open-source and proprietary multimodal AI\nmodels, revealing significant limitations: even state-of-the-art models such as\nGemini 2.5 Flash and Audio Flamingo 3 achieve only 59.2% and 51.7% accuracy,\nrespectively, approaching random performance in multiple categories. Our\nextensive analysis highlights specific shortcomings and provides novel\ninsights, offering actionable perspectives for the community to enhance future\nAI systems' progression toward audio general intelligence. The benchmark and\ncode is available at https://sonalkum.github.io/mmau-pro.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13992.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c9664eb34e600d7eaa4beb",
            "avatarUrl": "/avatars/ca23ecdec2d31c99ecce97d9b180ae0c.svg",
            "fullname": "Ghosh",
            "name": "Sreyan88",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13186",
            "authors": [
                {
                    "_id": "68a5b381c4b3ea17d2d2cfbc",
                    "name": "Shilong Li",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfbd",
                    "user": {
                        "_id": "6444e7765691ca69b0d95856",
                        "avatarUrl": "/avatars/4ae5001e7c7dea89c4deaf2b05436857.svg",
                        "isPro": false,
                        "fullname": "Xingyuan Bu",
                        "user": "sefira32",
                        "type": "user"
                    },
                    "name": "Xingyuan Bu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-20T19:40:08.260Z",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfbe",
                    "name": "Wenjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfbf",
                    "name": "Jiaheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc0",
                    "name": "Jun Dong",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc1",
                    "name": "Haoyang He",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc2",
                    "name": "Hao Lu",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc3",
                    "name": "Haozhe Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc4",
                    "name": "Chenchen Jing",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc5",
                    "name": "Zhen Li",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc6",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc7",
                    "name": "Jiayi Tian",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc8",
                    "name": "Chenchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfc9",
                    "name": "Tianhao Peng",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfca",
                    "name": "Yancheng He",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfcb",
                    "name": "Jihao Gu",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfcc",
                    "name": "Yuanxing Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfcd",
                    "name": "Jian Yang",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfce",
                    "name": "Ge Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfcf",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfd0",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfd1",
                    "name": "Zhaoxiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfd2",
                    "name": "Ruizhe Ding",
                    "hidden": false
                },
                {
                    "_id": "68a5b381c4b3ea17d2d2cfd3",
                    "name": "Shilei Wen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-14T13:46:47.000Z",
            "submittedOnDailyAt": "2025-08-20T10:09:38.599Z",
            "title": "MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents",
            "submittedOnDailyBy": {
                "_id": "6444e7765691ca69b0d95856",
                "avatarUrl": "/avatars/4ae5001e7c7dea89c4deaf2b05436857.svg",
                "isPro": false,
                "fullname": "Xingyuan Bu",
                "user": "sefira32",
                "type": "user"
            },
            "summary": "AI agents with advanced reasoning and tool use capabilities have demonstrated\nimpressive performance in web browsing for deep search. While existing\nbenchmarks such as BrowseComp evaluate these browsing abilities, they primarily\nfocus on textual information, overlooking the prevalence of multimodal content.\nTo bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising\n224 challenging, hand-crafted questions specifically designed to assess agents'\nmultimodal retrieval and reasoning capabilities. These questions often\nincorporate images in prompts, and crucial information encountered during the\nsearch and reasoning process may also be embedded within images or videos on\nwebpages. Consequently, methods relying solely on text prove insufficient for\nour benchmark. Additionally, we provide a verified checklist for each question,\nenabling fine-grained analysis of multimodal dependencies and reasoning paths.\nOur comprehensive evaluation of state-of-the-art models on MM-BrowseComp\nreveals that even top models like OpenAI o3 with tools achieve only 29.02\\%\naccuracy, highlighting the suboptimal multimodal capabilities and lack of\nnative multimodal reasoning in current models.",
            "upvotes": 3,
            "discussionId": "68a5b382c4b3ea17d2d2cfd4",
            "projectPage": "https://github.com/MMBrowseComp/MM-BrowseComp",
            "githubRepo": "https://github.com/MMBrowseComp/MM-BrowseComp",
            "ai_summary": "A new benchmark, MM-BrowseComp, evaluates AI agents' multimodal retrieval and reasoning capabilities, revealing limitations in current models' handling of images and videos in web browsing tasks.",
            "ai_keywords": [
                "multimodal content",
                "multimodal retrieval",
                "multimodal reasoning",
                "MM-BrowseComp",
                "OpenAI o3"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-08-14T09:46:47.000Z",
        "title": "MM-BrowseComp: A Comprehensive Benchmark for Multimodal Browsing Agents",
        "summary": "AI agents with advanced reasoning and tool use capabilities have demonstrated\nimpressive performance in web browsing for deep search. While existing\nbenchmarks such as BrowseComp evaluate these browsing abilities, they primarily\nfocus on textual information, overlooking the prevalence of multimodal content.\nTo bridge this gap, we introduce MM-BrowseComp, a novel benchmark comprising\n224 challenging, hand-crafted questions specifically designed to assess agents'\nmultimodal retrieval and reasoning capabilities. These questions often\nincorporate images in prompts, and crucial information encountered during the\nsearch and reasoning process may also be embedded within images or videos on\nwebpages. Consequently, methods relying solely on text prove insufficient for\nour benchmark. Additionally, we provide a verified checklist for each question,\nenabling fine-grained analysis of multimodal dependencies and reasoning paths.\nOur comprehensive evaluation of state-of-the-art models on MM-BrowseComp\nreveals that even top models like OpenAI o3 with tools achieve only 29.02\\%\naccuracy, highlighting the suboptimal multimodal capabilities and lack of\nnative multimodal reasoning in current models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13186.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6444e7765691ca69b0d95856",
            "avatarUrl": "/avatars/4ae5001e7c7dea89c4deaf2b05436857.svg",
            "fullname": "Xingyuan Bu",
            "name": "sefira32",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13139",
            "authors": [
                {
                    "_id": "68a553016cf0bf898542eccd",
                    "user": {
                        "_id": "63109a4d61cab0446e48c83b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63109a4d61cab0446e48c83b/JQlVkQp0ok586ND1GmB0w.png",
                        "isPro": false,
                        "fullname": "Ling-Hao Chen",
                        "user": "EvanTHU",
                        "type": "user"
                    },
                    "name": "Ling-Hao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-20T19:40:45.874Z",
                    "hidden": false
                },
                {
                    "_id": "68a553016cf0bf898542ecce",
                    "name": "Yuhong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a553016cf0bf898542eccf",
                    "name": "Zixin Yin",
                    "hidden": false
                },
                {
                    "_id": "68a553016cf0bf898542ecd0",
                    "name": "Zhiyang Dou",
                    "hidden": false
                },
                {
                    "_id": "68a553016cf0bf898542ecd1",
                    "name": "Xin Chen",
                    "hidden": false
                },
                {
                    "_id": "68a553016cf0bf898542ecd2",
                    "name": "Jingbo Wang",
                    "hidden": false
                },
                {
                    "_id": "68a553016cf0bf898542ecd3",
                    "name": "Taku Komura",
                    "hidden": false
                },
                {
                    "_id": "68a553016cf0bf898542ecd4",
                    "name": "Lei Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T17:50:31.000Z",
            "submittedOnDailyAt": "2025-08-20T03:34:25.932Z",
            "title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence",
            "submittedOnDailyBy": {
                "_id": "63109a4d61cab0446e48c83b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63109a4d61cab0446e48c83b/JQlVkQp0ok586ND1GmB0w.png",
                "isPro": false,
                "fullname": "Ling-Hao Chen",
                "user": "EvanTHU",
                "type": "user"
            },
            "summary": "This work studies the challenge of transfer animations between characters\nwhose skeletal topologies differ substantially. While many techniques have\nadvanced retargeting techniques in decades, transfer motions across diverse\ntopologies remains less-explored. The primary obstacle lies in the inherent\ntopological inconsistency between source and target skeletons, which restricts\nthe establishment of straightforward one-to-one bone correspondences. Besides,\nthe current lack of large-scale paired motion datasets spanning different\ntopological structures severely constrains the development of data-driven\napproaches. To address these limitations, we introduce Motion2Motion, a novel,\ntraining-free framework. Simply yet effectively, Motion2Motion works with only\none or a few example motions on the target skeleton, by accessing a sparse set\nof bone correspondences between the source and target skeletons. Through\ncomprehensive qualitative and quantitative evaluations, we demonstrate that\nMotion2Motion achieves efficient and reliable performance in both\nsimilar-skeleton and cross-species skeleton transfer scenarios. The practical\nutility of our approach is further evidenced by its successful integration in\ndownstream applications and user interfaces, highlighting its potential for\nindustrial applications. Code and data are available at\nhttps://lhchen.top/Motion2Motion.",
            "upvotes": 2,
            "discussionId": "68a553026cf0bf898542ecd5",
            "ai_summary": "Motion2Motion is a training-free framework that efficiently transfers animations between characters with different skeletal topologies using sparse bone correspondences.",
            "ai_keywords": [
                "Motion2Motion",
                "retargeting techniques",
                "skeletal topologies",
                "bone correspondences",
                "similar-skeleton",
                "cross-species skeleton transfer"
            ]
        },
        "publishedAt": "2025-08-18T13:50:31.000Z",
        "title": "Motion2Motion: Cross-topology Motion Transfer with Sparse Correspondence",
        "summary": "This work studies the challenge of transfer animations between characters\nwhose skeletal topologies differ substantially. While many techniques have\nadvanced retargeting techniques in decades, transfer motions across diverse\ntopologies remains less-explored. The primary obstacle lies in the inherent\ntopological inconsistency between source and target skeletons, which restricts\nthe establishment of straightforward one-to-one bone correspondences. Besides,\nthe current lack of large-scale paired motion datasets spanning different\ntopological structures severely constrains the development of data-driven\napproaches. To address these limitations, we introduce Motion2Motion, a novel,\ntraining-free framework. Simply yet effectively, Motion2Motion works with only\none or a few example motions on the target skeleton, by accessing a sparse set\nof bone correspondences between the source and target skeletons. Through\ncomprehensive qualitative and quantitative evaluations, we demonstrate that\nMotion2Motion achieves efficient and reliable performance in both\nsimilar-skeleton and cross-species skeleton transfer scenarios. The practical\nutility of our approach is further evidenced by its successful integration in\ndownstream applications and user interfaces, highlighting its potential for\nindustrial applications. Code and data are available at\nhttps://lhchen.top/Motion2Motion.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13139.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63109a4d61cab0446e48c83b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63109a4d61cab0446e48c83b/JQlVkQp0ok586ND1GmB0w.png",
            "fullname": "Ling-Hao Chen",
            "name": "EvanTHU",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12535",
            "authors": [
                {
                    "_id": "68a57f3c38160bfd39426a41",
                    "user": {
                        "_id": "64105805928400b416439f10",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
                        "isPro": false,
                        "fullname": "Seonglae Cho",
                        "user": "seonglae",
                        "type": "user"
                    },
                    "name": "Seonglae Cho",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:12:44.053Z",
                    "hidden": false
                },
                {
                    "_id": "68a57f3c38160bfd39426a42",
                    "name": "Zekun Wu",
                    "hidden": false
                },
                {
                    "_id": "68a57f3c38160bfd39426a43",
                    "name": "Adriano Koshiyama",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64105805928400b416439f10/SNakpXI_xxnLYZ3OW2U0Q.png",
                "https://cdn-uploads.huggingface.co/production/uploads/64105805928400b416439f10/nTchPmWt-2WldsRRL_JN-.png"
            ],
            "publishedAt": "2025-08-18T00:01:42.000Z",
            "submittedOnDailyAt": "2025-08-20T06:27:55.001Z",
            "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through\n  Correlation-based Sparse Autoencoder Feature Selection",
            "submittedOnDailyBy": {
                "_id": "64105805928400b416439f10",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
                "isPro": false,
                "fullname": "Seonglae Cho",
                "user": "seonglae",
                "type": "user"
            },
            "summary": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.",
            "upvotes": 2,
            "discussionId": "68a57f3c38160bfd39426a44",
            "ai_summary": "CorrSteer improves language model performance by selecting relevant features through correlation with SAE activations at inference time, enhancing tasks like QA, bias mitigation, and reasoning.",
            "ai_keywords": [
                "Sparse Autoencoders",
                "SAEs",
                "large language models",
                "LLMs",
                "contrastive datasets",
                "activation storage",
                "sample correctness",
                "inference-time activations",
                "steering coefficients",
                "MMLU",
                "HarmBench",
                "semantically meaningful patterns"
            ]
        },
        "publishedAt": "2025-08-17T20:01:42.000Z",
        "title": "CorrSteer: Steering Improves Task Performance and Safety in LLMs through\n  Correlation-based Sparse Autoencoder Feature Selection",
        "summary": "Sparse Autoencoders (SAEs) can extract interpretable features from large\nlanguage models (LLMs) without supervision. However, their effectiveness in\ndownstream steering tasks is limited by the requirement for contrastive\ndatasets or large activation storage. To address these limitations, we propose\nCorrSteer, which selects features by correlating sample correctness with SAE\nactivations from generated tokens at inference time. This approach uses only\ninference-time activations to extract more relevant features, thereby avoiding\nspurious correlations. It also obtains steering coefficients from average\nactivations, automating the entire pipeline. Our method shows improved task\nperformance on QA, bias mitigation, jailbreaking prevention, and reasoning\nbenchmarks on Gemma 2 2B and LLaMA 3.1 8B, notably achieving a +4.1%\nimprovement in MMLU performance and a +22.9% improvement in HarmBench with only\n4000 samples. Selected features demonstrate semantically meaningful patterns\naligned with each task's requirements, revealing the underlying capabilities\nthat drive performance. Our work establishes correlationbased selection as an\neffective and scalable approach for automated SAE steering across language\nmodel applications.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64105805928400b416439f10/SNakpXI_xxnLYZ3OW2U0Q.png",
            "https://cdn-uploads.huggingface.co/production/uploads/64105805928400b416439f10/nTchPmWt-2WldsRRL_JN-.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12535.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64105805928400b416439f10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64105805928400b416439f10/i0jFLo47RTDeNl26hiR9y.jpeg",
            "fullname": "Seonglae Cho",
            "name": "seonglae",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.11032",
            "authors": [
                {
                    "_id": "68a2e436a4caabb4320e64ee",
                    "user": {
                        "_id": "65bf56b90a0c57943f0540aa",
                        "avatarUrl": "/avatars/7f92f81cd9084f615dcf27943d377722.svg",
                        "isPro": false,
                        "fullname": "Yanwu",
                        "user": "podismine",
                        "type": "user"
                    },
                    "name": "Yanwu Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:53:11.002Z",
                    "hidden": false
                },
                {
                    "_id": "68a2e436a4caabb4320e64ef",
                    "user": {
                        "_id": "630ed6cfc9af0163b96a71aa",
                        "avatarUrl": "/avatars/bf984bb1c2f68336bcf042c6fc7e0d9d.svg",
                        "isPro": false,
                        "fullname": "Guinan-Su",
                        "user": "guinansu",
                        "type": "user"
                    },
                    "name": "Guinan Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:53:07.865Z",
                    "hidden": false
                },
                {
                    "_id": "68a2e436a4caabb4320e64f0",
                    "name": "Jiesi Hu",
                    "hidden": false
                },
                {
                    "_id": "68a2e436a4caabb4320e64f1",
                    "name": "Francesco Sammarco",
                    "hidden": false
                },
                {
                    "_id": "68a2e436a4caabb4320e64f2",
                    "name": "Jonas Geiping",
                    "hidden": false
                },
                {
                    "_id": "68a2e436a4caabb4320e64f3",
                    "name": "Thomas Wolfers",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-14T19:35:57.000Z",
            "submittedOnDailyAt": "2025-08-20T06:50:23.024Z",
            "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image\n  Segmentation",
            "submittedOnDailyBy": {
                "_id": "630ed6cfc9af0163b96a71aa",
                "avatarUrl": "/avatars/bf984bb1c2f68336bcf042c6fc7e0d9d.svg",
                "isPro": false,
                "fullname": "Guinan-Su",
                "user": "guinansu",
                "type": "user"
            },
            "summary": "Universal medical image segmentation models have emerged as a promising\nparadigm due to their strong generalizability across diverse tasks, showing\ngreat potential for a wide range of clinical applications. This potential has\nbeen partly driven by the success of general-purpose vision models such as the\nSegment Anything Model (SAM), which has inspired the development of various\nfine-tuned variants for medical segmentation tasks. However, fine-tuned\nvariants like MedSAM are trained on comparatively limited medical imaging data\nthat often suffers from heterogeneity, scarce annotations, and distributional\nshifts. These challenges limit their ability to generalize across a wide range\nof medical segmentation tasks. In this regard, we propose MedSAMix, a\ntraining-free model merging method that integrates the strengths of both\ngeneralist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical\nimage segmentation. In contrast to traditional model merging approaches that\nrely on manual configuration and often result in suboptimal outcomes, we\npropose a zero-order optimization method to automatically discover optimal\nlayer-wise merging solutions. Furthermore, for clinical applications, we\ndevelop two regimes to meet the demand of domain-specificity and\ngeneralizability in different scenarios by single-task optimization and\nmulti-objective optimization respectively. Extensive evaluations on 25 medical\nsegmentation tasks demonstrate that MedSAMix effectively mitigates model bias\nand consistently improves performance in both domain-specific accuracy and\ngeneralization, achieving improvements of 6.67% on specialized tasks and 4.37%\non multi-task evaluations.",
            "upvotes": 2,
            "discussionId": "68a2e436a4caabb4320e64f4",
            "githubRepo": "https://github.com/podismine/MedSAMix",
            "ai_summary": "MedSAMix, a training-free model merging method, combines generalist and specialist models to improve medical image segmentation performance across diverse tasks.",
            "ai_keywords": [
                "Segment Anything Model (SAM)",
                "MedSAM",
                "model merging",
                "zero-order optimization",
                "layer-wise merging",
                "domain-specificity",
                "generalizability",
                "single-task optimization",
                "multi-objective optimization",
                "medical segmentation tasks"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-08-14T15:35:57.000Z",
        "title": "MedSAMix: A Training-Free Model Merging Approach for Medical Image\n  Segmentation",
        "summary": "Universal medical image segmentation models have emerged as a promising\nparadigm due to their strong generalizability across diverse tasks, showing\ngreat potential for a wide range of clinical applications. This potential has\nbeen partly driven by the success of general-purpose vision models such as the\nSegment Anything Model (SAM), which has inspired the development of various\nfine-tuned variants for medical segmentation tasks. However, fine-tuned\nvariants like MedSAM are trained on comparatively limited medical imaging data\nthat often suffers from heterogeneity, scarce annotations, and distributional\nshifts. These challenges limit their ability to generalize across a wide range\nof medical segmentation tasks. In this regard, we propose MedSAMix, a\ntraining-free model merging method that integrates the strengths of both\ngeneralist models (e.g., SAM) and specialist models (e.g., MedSAM) for medical\nimage segmentation. In contrast to traditional model merging approaches that\nrely on manual configuration and often result in suboptimal outcomes, we\npropose a zero-order optimization method to automatically discover optimal\nlayer-wise merging solutions. Furthermore, for clinical applications, we\ndevelop two regimes to meet the demand of domain-specificity and\ngeneralizability in different scenarios by single-task optimization and\nmulti-objective optimization respectively. Extensive evaluations on 25 medical\nsegmentation tasks demonstrate that MedSAMix effectively mitigates model bias\nand consistently improves performance in both domain-specific accuracy and\ngeneralization, achieving improvements of 6.67% on specialized tasks and 4.37%\non multi-task evaluations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11032.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630ed6cfc9af0163b96a71aa",
            "avatarUrl": "/avatars/bf984bb1c2f68336bcf042c6fc7e0d9d.svg",
            "fullname": "Guinan-Su",
            "name": "guinansu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.10478",
            "authors": [
                {
                    "_id": "68a582c138160bfd39426a64",
                    "name": "Gustavo Penha",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a65",
                    "name": "Edoardo D'Amico",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a66",
                    "user": {
                        "_id": "626193a918176506fab35c62",
                        "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
                        "isPro": false,
                        "fullname": "Marco De Nadai",
                        "user": "marcodena",
                        "type": "user"
                    },
                    "name": "Marco De Nadai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:50:39.370Z",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a67",
                    "name": "Enrico Palumbo",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a68",
                    "name": "Alexandre Tamborrino",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a69",
                    "name": "Ali Vardasbi",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a6a",
                    "name": "Max Lefarov",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a6b",
                    "name": "Shawn Lin",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a6c",
                    "name": "Timothy Heath",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a6d",
                    "user": {
                        "_id": "65eed0a600f1a613dae4d9be",
                        "avatarUrl": "/avatars/a420d0e2c77a482eb5022deeccb2ebf5.svg",
                        "isPro": false,
                        "fullname": "Francesco Fabbri",
                        "user": "frafabbri",
                        "type": "user"
                    },
                    "name": "Francesco Fabbri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:50:37.227Z",
                    "hidden": false
                },
                {
                    "_id": "68a582c138160bfd39426a6e",
                    "name": "Hugues Bouchard",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-14T09:28:49.000Z",
            "submittedOnDailyAt": "2025-08-20T06:39:44.877Z",
            "title": "Semantic IDs for Joint Generative Search and Recommendation",
            "submittedOnDailyBy": {
                "_id": "626193a918176506fab35c62",
                "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
                "isPro": false,
                "fullname": "Marco De Nadai",
                "user": "marcodena",
                "type": "user"
            },
            "summary": "Generative models powered by Large Language Models (LLMs) are emerging as a\nunified solution for powering both recommendation and search tasks. A key\ndesign choice in these models is how to represent items, traditionally through\nunique identifiers (IDs) and more recently with Semantic IDs composed of\ndiscrete codes, obtained from embeddings. While task-specific embedding models\ncan improve performance for individual tasks, they may not generalize well in a\njoint setting. In this paper, we explore how to construct Semantic IDs that\nperform well both in search and recommendation when using a unified model. We\ncompare a range of strategies to construct Semantic IDs, looking into\ntask-specific and cross-tasks approaches, and also whether each task should\nhave its own semantic ID tokens in a joint search and recommendation generative\nmodel. Our results show that using a bi-encoder model fine-tuned on both search\nand recommendation tasks to obtain item embeddings, followed by the\nconstruction of a unified Semantic ID space provides an effective trade-off,\nenabling strong performance in both tasks. We hope these findings spark\nfollow-up work on generalisable, semantically grounded ID schemes and inform\nthe next wave of unified generative recommender architectures.",
            "upvotes": 2,
            "discussionId": "68a582c138160bfd39426a6f",
            "ai_summary": "A bi-encoder model fine-tuned on both search and recommendation tasks provides effective Semantic IDs for a unified generative recommender system.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Semantic IDs",
                "embeddings",
                "bi-encoder model",
                "task-specific embedding models",
                "semantic ID tokens",
                "unified Semantic ID space"
            ]
        },
        "publishedAt": "2025-08-14T05:28:49.000Z",
        "title": "Semantic IDs for Joint Generative Search and Recommendation",
        "summary": "Generative models powered by Large Language Models (LLMs) are emerging as a\nunified solution for powering both recommendation and search tasks. A key\ndesign choice in these models is how to represent items, traditionally through\nunique identifiers (IDs) and more recently with Semantic IDs composed of\ndiscrete codes, obtained from embeddings. While task-specific embedding models\ncan improve performance for individual tasks, they may not generalize well in a\njoint setting. In this paper, we explore how to construct Semantic IDs that\nperform well both in search and recommendation when using a unified model. We\ncompare a range of strategies to construct Semantic IDs, looking into\ntask-specific and cross-tasks approaches, and also whether each task should\nhave its own semantic ID tokens in a joint search and recommendation generative\nmodel. Our results show that using a bi-encoder model fine-tuned on both search\nand recommendation tasks to obtain item embeddings, followed by the\nconstruction of a unified Semantic ID space provides an effective trade-off,\nenabling strong performance in both tasks. We hope these findings spark\nfollow-up work on generalisable, semantically grounded ID schemes and inform\nthe next wave of unified generative recommender architectures.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10478.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "626193a918176506fab35c62",
            "avatarUrl": "/avatars/44a34405e4821fa9047cfa635e198f61.svg",
            "fullname": "Marco De Nadai",
            "name": "marcodena",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04326",
            "authors": [
                {
                    "_id": "68a43b91b65388761d074602",
                    "user": {
                        "_id": "67b1cf0b8a1b0f0b4880e56f",
                        "avatarUrl": "/avatars/bdaa4d9cd2280ceccc84c24a5ba1b5b4.svg",
                        "isPro": false,
                        "fullname": "Ke Li",
                        "user": "cocolinux",
                        "type": "user"
                    },
                    "name": "Ke Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-19T15:58:49.505Z",
                    "hidden": false
                },
                {
                    "_id": "68a43b91b65388761d074603",
                    "name": "Mana Masuda",
                    "hidden": false
                },
                {
                    "_id": "68a43b91b65388761d074604",
                    "name": "Susanne Schmidt",
                    "hidden": false
                },
                {
                    "_id": "68a43b91b65388761d074605",
                    "user": {
                        "_id": "6613fa91e8dcce192f969c95",
                        "avatarUrl": "/avatars/72951ae850bbb31611627dc23e778fff.svg",
                        "isPro": false,
                        "fullname": "Shohei Mori",
                        "user": "Mugichoko445",
                        "type": "user"
                    },
                    "name": "Shohei Mori",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:52:30.542Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67b1cf0b8a1b0f0b4880e56f/xFJaNueE2DK5UhlfTH-sw.jpeg"
            ],
            "publishedAt": "2025-08-06T11:14:06.000Z",
            "submittedOnDailyAt": "2025-08-20T06:15:15.882Z",
            "title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned\n  and Addressed for XR Research",
            "submittedOnDailyBy": {
                "_id": "67b1cf0b8a1b0f0b4880e56f",
                "avatarUrl": "/avatars/bdaa4d9cd2280ceccc84c24a5ba1b5b4.svg",
                "isPro": false,
                "fullname": "Ke Li",
                "user": "cocolinux",
                "type": "user"
            },
            "summary": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)\nand Neural Radiance Fields (NeRF), has revolutionized interactive\nphotorealistic view synthesis and presents enormous opportunities for XR\nresearch and applications. However, despite the exponential growth of RF\nresearch, RF-related contributions to the XR community remain sparse. To better\nunderstand this research gap, we performed a systematic survey of current RF\nliterature to analyze (i) how RF is envisioned for XR applications, (ii) how\nthey have already been implemented, and (iii) the remaining research gaps. We\ncollected 365 RF contributions related to XR from computer vision, computer\ngraphics, robotics, multimedia, human-computer interaction, and XR communities,\nseeking to answer the above research questions. Among the 365 papers, we\nperformed an analysis of 66 papers that already addressed a detailed aspect of\nRF research for XR. With this survey, we extended and positioned XR-specific RF\nresearch topics in the broader RF research field and provide a helpful resource\nfor the XR community to navigate within the rapid development of RF research.",
            "upvotes": 2,
            "discussionId": "68a43b9db65388761d074606",
            "projectPage": "https://mediated-reality.github.io/rf4xr/papers/li_tvcg25/",
            "githubRepo": "https://github.com/mediated-reality/awesome-rf4xr",
            "ai_summary": "A systematic survey of radiance fields in XR applications identifies implementation details, research gaps, and future directions within the broader RF research field.",
            "ai_keywords": [
                "radiance fields",
                "3D Gaussian Splatting",
                "Neural Radiance Fields",
                "XR",
                "computer vision",
                "computer graphics",
                "robotics",
                "multimedia",
                "human-computer interaction"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-08-06T07:14:06.000Z",
        "title": "Radiance Fields in XR: A Survey on How Radiance Fields are Envisioned\n  and Addressed for XR Research",
        "summary": "The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS)\nand Neural Radiance Fields (NeRF), has revolutionized interactive\nphotorealistic view synthesis and presents enormous opportunities for XR\nresearch and applications. However, despite the exponential growth of RF\nresearch, RF-related contributions to the XR community remain sparse. To better\nunderstand this research gap, we performed a systematic survey of current RF\nliterature to analyze (i) how RF is envisioned for XR applications, (ii) how\nthey have already been implemented, and (iii) the remaining research gaps. We\ncollected 365 RF contributions related to XR from computer vision, computer\ngraphics, robotics, multimedia, human-computer interaction, and XR communities,\nseeking to answer the above research questions. Among the 365 papers, we\nperformed an analysis of 66 papers that already addressed a detailed aspect of\nRF research for XR. With this survey, we extended and positioned XR-specific RF\nresearch topics in the broader RF research field and provide a helpful resource\nfor the XR community to navigate within the rapid development of RF research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67b1cf0b8a1b0f0b4880e56f/xFJaNueE2DK5UhlfTH-sw.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04326.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67b1cf0b8a1b0f0b4880e56f",
            "avatarUrl": "/avatars/bdaa4d9cd2280ceccc84c24a5ba1b5b4.svg",
            "fullname": "Ke Li",
            "name": "cocolinux",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13804",
            "authors": [
                {
                    "_id": "68a57e0438160bfd39426a3d",
                    "user": {
                        "_id": "646344a2efb4e8550484863e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646344a2efb4e8550484863e/aHZTPDVEmy_K0i7fuAzdr.jpeg",
                        "isPro": false,
                        "fullname": "Maciej Skorski",
                        "user": "maciejskorski",
                        "type": "user"
                    },
                    "name": "Maciej Skorski",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:12:46.084Z",
                    "hidden": false
                },
                {
                    "_id": "68a57e0438160bfd39426a3e",
                    "name": "Alina Landowska",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T13:05:48.000Z",
            "submittedOnDailyAt": "2025-08-20T06:51:13.476Z",
            "title": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding",
            "submittedOnDailyBy": {
                "_id": "646344a2efb4e8550484863e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646344a2efb4e8550484863e/aHZTPDVEmy_K0i7fuAzdr.jpeg",
                "isPro": false,
                "fullname": "Maciej Skorski",
                "user": "maciejskorski",
                "type": "user"
            },
            "summary": "How do large language models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,\nDeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on\n100K+ texts spanning social media, news, and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators, achieving\nmuch better-than-average balanced accuracy. Importantly, we find that AI\nproduces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities.",
            "upvotes": 1,
            "discussionId": "68a57e0438160bfd39426a3f",
            "projectPage": "https://maciejskorski.github.io/moral-foundations-llm-eval",
            "githubRepo": "https://github.com/maciejskorski/moral-foundations-llm-eval",
            "ai_summary": "A Bayesian evaluation framework assesses large language models' moral understanding by modeling human annotator disagreements, showing AI models perform well with fewer false negatives.",
            "ai_keywords": [
                "Bayesian evaluation",
                "aleatoric uncertainty",
                "epistemic uncertainty",
                "model domain sensitivity",
                "GPU-optimized Bayesian framework",
                "balanced accuracy",
                "moral detection capabilities"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-08-19T09:05:48.000Z",
        "title": "Beyond Human Judgment: A Bayesian Evaluation of LLMs' Moral Values\n  Understanding",
        "summary": "How do large language models understand moral dimensions compared to humans?\n  This first large-scale Bayesian evaluation of market-leading language models\nprovides the answer. In contrast to prior work using deterministic ground truth\n(majority or inclusion rules), we model annotator disagreements to capture both\naleatoric uncertainty (inherent human disagreement) and epistemic uncertainty\n(model domain sensitivity). We evaluate top language models (Claude Sonnet 4,\nDeepSeek-V3, Llama 4 Maverick) across 250K+ annotations from ~700 annotators on\n100K+ texts spanning social media, news, and forums.\n  Our GPU-optimized Bayesian framework processed 1M+ model queries, revealing\nthat AI models typically rank among the top 25\\% of human annotators, achieving\nmuch better-than-average balanced accuracy. Importantly, we find that AI\nproduces far fewer false negatives than humans, highlighting their more\nsensitive moral detection capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13804.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646344a2efb4e8550484863e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/646344a2efb4e8550484863e/aHZTPDVEmy_K0i7fuAzdr.jpeg",
            "fullname": "Maciej Skorski",
            "name": "maciejskorski",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13320",
            "authors": [
                {
                    "_id": "68a5dbdf9e4b49496aac684f",
                    "user": {
                        "_id": "6446ab9815a27291ef8b7313",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6446ab9815a27291ef8b7313/rw5xK-BV2jyct0G3ft13a.png",
                        "isPro": false,
                        "fullname": "Ashi Garg",
                        "user": "ash56",
                        "type": "user"
                    },
                    "name": "Ashi Garg",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T19:11:40.862Z",
                    "hidden": false
                },
                {
                    "_id": "68a5dbdf9e4b49496aac6850",
                    "name": "Zexin Cai",
                    "hidden": false
                },
                {
                    "_id": "68a5dbdf9e4b49496aac6851",
                    "name": "Henry Li Xinyuan",
                    "hidden": false
                },
                {
                    "_id": "68a5dbdf9e4b49496aac6852",
                    "name": "Leibny Paola García-Perera",
                    "hidden": false
                },
                {
                    "_id": "68a5dbdf9e4b49496aac6853",
                    "name": "Kevin Duh",
                    "hidden": false
                },
                {
                    "_id": "68a5dbdf9e4b49496aac6854",
                    "name": "Sanjeev Khudanpur",
                    "hidden": false
                },
                {
                    "_id": "68a5dbdf9e4b49496aac6855",
                    "name": "Matthew Wiesner",
                    "hidden": false
                },
                {
                    "_id": "68a5dbdf9e4b49496aac6856",
                    "name": "Nicholas Andrews",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T19:14:45.000Z",
            "submittedOnDailyAt": "2025-08-20T13:02:51.850Z",
            "title": "Rapidly Adapting to New Voice Spoofing: Few-Shot Detection of\n  Synthesized Speech Under Distribution Shifts",
            "submittedOnDailyBy": {
                "_id": "6446ab9815a27291ef8b7313",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6446ab9815a27291ef8b7313/rw5xK-BV2jyct0G3ft13a.png",
                "isPro": false,
                "fullname": "Ashi Garg",
                "user": "ash56",
                "type": "user"
            },
            "summary": "We address the challenge of detecting synthesized speech under distribution\nshifts -- arising from unseen synthesis methods, speakers, languages, or audio\nconditions -- relative to the training data. Few-shot learning methods are a\npromising way to tackle distribution shifts by rapidly adapting on the basis of\na few in-distribution samples. We propose a self-attentive prototypical network\nto enable more robust few-shot adaptation. To evaluate our approach, we\nsystematically compare the performance of traditional zero-shot detectors and\nthe proposed few-shot detectors, carefully controlling training conditions to\nintroduce distribution shifts at evaluation time. In conditions where\ndistribution shifts hamper the zero-shot performance, our proposed few-shot\nadaptation technique can quickly adapt using as few as 10 in-distribution\nsamples -- achieving upto 32% relative EER reduction on deepfakes in Japanese\nlanguage and 20% relative reduction on ASVspoof 2021 Deepfake dataset.",
            "upvotes": 1,
            "discussionId": "68a5dbdf9e4b49496aac6857",
            "ai_summary": "A self-attentive prototypical network improves few-shot adaptation for detecting synthesized speech under distribution shifts, achieving significant performance gains over zero-shot detectors.",
            "ai_keywords": [
                "self-attentive prototypical network",
                "few-shot learning",
                "distribution shifts",
                "zero-shot detectors",
                "deepfakes",
                "ASVspoof 2021",
                "EER reduction"
            ]
        },
        "publishedAt": "2025-08-18T15:14:45.000Z",
        "title": "Rapidly Adapting to New Voice Spoofing: Few-Shot Detection of\n  Synthesized Speech Under Distribution Shifts",
        "summary": "We address the challenge of detecting synthesized speech under distribution\nshifts -- arising from unseen synthesis methods, speakers, languages, or audio\nconditions -- relative to the training data. Few-shot learning methods are a\npromising way to tackle distribution shifts by rapidly adapting on the basis of\na few in-distribution samples. We propose a self-attentive prototypical network\nto enable more robust few-shot adaptation. To evaluate our approach, we\nsystematically compare the performance of traditional zero-shot detectors and\nthe proposed few-shot detectors, carefully controlling training conditions to\nintroduce distribution shifts at evaluation time. In conditions where\ndistribution shifts hamper the zero-shot performance, our proposed few-shot\nadaptation technique can quickly adapt using as few as 10 in-distribution\nsamples -- achieving upto 32% relative EER reduction on deepfakes in Japanese\nlanguage and 20% relative reduction on ASVspoof 2021 Deepfake dataset.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13320.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6446ab9815a27291ef8b7313",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6446ab9815a27291ef8b7313/rw5xK-BV2jyct0G3ft13a.png",
            "fullname": "Ashi Garg",
            "name": "ash56",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.11386",
            "authors": [
                {
                    "_id": "68a622599e4b49496aac68ab",
                    "user": {
                        "_id": "639080fed2cf01fdfe322a8e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671645871849-639080fed2cf01fdfe322a8e.jpeg",
                        "isPro": false,
                        "fullname": "Ryan Chan",
                        "user": "rchan26",
                        "type": "user"
                    },
                    "name": "Ryan Sze-Yin Chan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T19:39:13.251Z",
                    "hidden": false
                },
                {
                    "_id": "68a622599e4b49496aac68ac",
                    "name": "Federico Nanni",
                    "hidden": false
                },
                {
                    "_id": "68a622599e4b49496aac68ad",
                    "name": "Tomas Lazauskas",
                    "hidden": false
                },
                {
                    "_id": "68a622599e4b49496aac68ae",
                    "name": "Rosie Wood",
                    "hidden": false
                },
                {
                    "_id": "68a622599e4b49496aac68af",
                    "name": "Penelope Yong",
                    "hidden": false
                },
                {
                    "_id": "68a622599e4b49496aac68b0",
                    "name": "Lionel Tarassenko",
                    "hidden": false
                },
                {
                    "_id": "68a622599e4b49496aac68b1",
                    "name": "Mark Girolami",
                    "hidden": false
                },
                {
                    "_id": "68a622599e4b49496aac68b2",
                    "name": "James Geddes",
                    "hidden": false
                },
                {
                    "_id": "68a622599e4b49496aac68b3",
                    "name": "Andrew Duncan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-15T10:38:15.000Z",
            "submittedOnDailyAt": "2025-08-20T18:02:04.230Z",
            "title": "Retrieval-augmented reasoning with lean language models",
            "submittedOnDailyBy": {
                "_id": "639080fed2cf01fdfe322a8e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671645871849-639080fed2cf01fdfe322a8e.jpeg",
                "isPro": false,
                "fullname": "Ryan Chan",
                "user": "rchan26",
                "type": "user"
            },
            "summary": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.",
            "upvotes": 1,
            "discussionId": "68a622599e4b49496aac68b4",
            "ai_summary": "A retrieval-augmented conversational agent using a lightweight model achieves high accuracy and consistency in domain-specific queries through fine-tuning and document compression.",
            "ai_keywords": [
                "reasoning and retrieval augmented generation (RAG)",
                "test-time scaling",
                "small-scale reasoning models",
                "dense retriever",
                "Qwen2.5-Instruct",
                "synthetic query generation",
                "reasoning traces",
                "summarisation-based document compression",
                "synthetic data design",
                "reasoning-aware fine-tuning",
                "domain-specific fine-tuning"
            ]
        },
        "publishedAt": "2025-08-15T06:38:15.000Z",
        "title": "Retrieval-augmented reasoning with lean language models",
        "summary": "This technical report details a novel approach to combining reasoning and\nretrieval augmented generation (RAG) within a single, lean language model\narchitecture. While existing RAG systems typically rely on large-scale models\nand external APIs, our work addresses the increasing demand for performant and\nprivacy-preserving solutions deployable in resource-constrained or secure\nenvironments. Building on recent developments in test-time scaling and\nsmall-scale reasoning models, we develop a retrieval augmented conversational\nagent capable of interpreting complex, domain-specific queries using a\nlightweight backbone model. Our system integrates a dense retriever with\nfine-tuned Qwen2.5-Instruct models, using synthetic query generation and\nreasoning traces derived from frontier models (e.g., DeepSeek-R1) over a\ncurated corpus, in this case, the NHS A-to-Z condition pages. We explore the\nimpact of summarisation-based document compression, synthetic data design, and\nreasoning-aware fine-tuning on model performance. Evaluation against both\nnon-reasoning and general-purpose lean models demonstrates that our\ndomain-specific fine-tuning approach yields substantial gains in answer\naccuracy and consistency, approaching frontier-level performance while\nremaining feasible for local deployment. All implementation details and code\nare publicly released to support reproducibility and adaptation across domains.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11386.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "639080fed2cf01fdfe322a8e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1671645871849-639080fed2cf01fdfe322a8e.jpeg",
            "fullname": "Ryan Chan",
            "name": "rchan26",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.04038",
            "authors": [
                {
                    "_id": "6899e842996e0a645a52a801",
                    "user": {
                        "_id": "6535c28d9ded17e619e4aec2",
                        "avatarUrl": "/avatars/fc6f3238d2be5da77ec6acd218caee79.svg",
                        "isPro": false,
                        "fullname": "Zechen Li",
                        "user": "zechenli03",
                        "type": "user"
                    },
                    "name": "Zechen Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-13T07:22:02.583Z",
                    "hidden": false
                },
                {
                    "_id": "6899e842996e0a645a52a802",
                    "user": {
                        "_id": "67b5890b0d878eff1a36ce8d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Py6177rLRA6KQtafyAZ90.jpeg",
                        "isPro": false,
                        "fullname": "Breeze Chen",
                        "user": "Breezelled",
                        "type": "user"
                    },
                    "name": "Baiyu Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:53:31.770Z",
                    "hidden": false
                },
                {
                    "_id": "6899e842996e0a645a52a803",
                    "name": "Hao Xue",
                    "hidden": false
                },
                {
                    "_id": "6899e842996e0a645a52a804",
                    "name": "Flora D. Salim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-06T02:57:57.000Z",
            "submittedOnDailyAt": "2025-08-20T05:57:22.702Z",
            "title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval\n  Driven LLM Agents",
            "submittedOnDailyBy": {
                "_id": "67b5890b0d878eff1a36ce8d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Py6177rLRA6KQtafyAZ90.jpeg",
                "isPro": false,
                "fullname": "Breeze Chen",
                "user": "Breezelled",
                "type": "user"
            },
            "summary": "Motion sensor time-series are central to human activity recognition (HAR),\nwith applications in health, sports, and smart devices. However, existing\nmethods are trained for fixed activity sets and require costly retraining when\nnew behaviours or sensor setups appear. Recent attempts to use large language\nmodels (LLMs) for HAR, typically by converting signals into text or images,\nsuffer from limited accuracy and lack verifiable interpretability. We propose\nZARA, the first agent-based framework for zero-shot, explainable HAR directly\nfrom raw motion time-series. ZARA integrates an automatically derived pair-wise\nfeature knowledge base that captures discriminative statistics for every\nactivity pair, a multi-sensor retrieval module that surfaces relevant evidence,\nand a hierarchical agent pipeline that guides the LLM to iteratively select\nfeatures, draw on this evidence, and produce both activity predictions and\nnatural-language explanations. ZARA enables flexible and interpretable HAR\nwithout any fine-tuning or task-specific classifiers. Extensive experiments on\n8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering\nclear reasoning while exceeding the strongest baselines by 2.53x in macro F1.\nAblation studies further confirm the necessity of each module, marking ZARA as\na promising step toward trustworthy, plug-and-play motion time-series analysis.\nOur codes are available at https://github.com/zechenli03/ZARA.",
            "upvotes": 1,
            "discussionId": "6899e843996e0a645a52a805",
            "githubRepo": "https://github.com/zechenli03/ZARA",
            "ai_summary": "ZARA is an agent-based framework for zero-shot, explainable human activity recognition from raw motion time-series, achieving state-of-the-art performance without fine-tuning.",
            "ai_keywords": [
                "agent-based framework",
                "zero-shot",
                "explainable HAR",
                "raw motion time-series",
                "feature knowledge base",
                "multi-sensor retrieval module",
                "hierarchical agent pipeline",
                "LLM",
                "macro F1",
                "plug-and-play motion time-series analysis"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-08-05T22:57:57.000Z",
        "title": "ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval\n  Driven LLM Agents",
        "summary": "Motion sensor time-series are central to human activity recognition (HAR),\nwith applications in health, sports, and smart devices. However, existing\nmethods are trained for fixed activity sets and require costly retraining when\nnew behaviours or sensor setups appear. Recent attempts to use large language\nmodels (LLMs) for HAR, typically by converting signals into text or images,\nsuffer from limited accuracy and lack verifiable interpretability. We propose\nZARA, the first agent-based framework for zero-shot, explainable HAR directly\nfrom raw motion time-series. ZARA integrates an automatically derived pair-wise\nfeature knowledge base that captures discriminative statistics for every\nactivity pair, a multi-sensor retrieval module that surfaces relevant evidence,\nand a hierarchical agent pipeline that guides the LLM to iteratively select\nfeatures, draw on this evidence, and produce both activity predictions and\nnatural-language explanations. ZARA enables flexible and interpretable HAR\nwithout any fine-tuning or task-specific classifiers. Extensive experiments on\n8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering\nclear reasoning while exceeding the strongest baselines by 2.53x in macro F1.\nAblation studies further confirm the necessity of each module, marking ZARA as\na promising step toward trustworthy, plug-and-play motion time-series analysis.\nOur codes are available at https://github.com/zechenli03/ZARA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.04038.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67b5890b0d878eff1a36ce8d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Py6177rLRA6KQtafyAZ90.jpeg",
            "fullname": "Breeze Chen",
            "name": "Breezelled",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12800",
            "authors": [
                {
                    "_id": "68a55dc46cf0bf898542ecf6",
                    "user": {
                        "_id": "63ea134a43d976de6e4e8ea7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ea134a43d976de6e4e8ea7/OsyE7SYDW9foqsyWXtTiL.png",
                        "isPro": false,
                        "fullname": "deng yong",
                        "user": "dikw",
                        "type": "user"
                    },
                    "name": "Yong Deng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:11.768Z",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ecf7",
                    "name": "Guoqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ecf8",
                    "name": "Zhenzhe Ying",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ecf9",
                    "name": "Xiaofeng Wu",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ecfa",
                    "name": "Jinzhen Lin",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ecfb",
                    "name": "Wenwen Xiong",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ecfc",
                    "name": "Yuqin Dai",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ecfd",
                    "name": "Shuo Yang",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ecfe",
                    "name": "Zhanwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ecff",
                    "name": "Qiwen Wang",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ed00",
                    "name": "Yang Qin",
                    "hidden": false
                },
                {
                    "_id": "68a55dc46cf0bf898542ed01",
                    "name": "Changhua Meng",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63ea134a43d976de6e4e8ea7/T1TzZI-A3Hvn5RTqCz7lc.png"
            ],
            "publishedAt": "2025-08-18T10:23:10.000Z",
            "submittedOnDailyAt": "2025-08-20T11:31:29.751Z",
            "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward",
            "submittedOnDailyBy": {
                "_id": "63ea134a43d976de6e4e8ea7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ea134a43d976de6e4e8ea7/OsyE7SYDW9foqsyWXtTiL.png",
                "isPro": false,
                "fullname": "deng yong",
                "user": "dikw",
                "type": "user"
            },
            "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
            "upvotes": 0,
            "discussionId": "68a55dc46cf0bf898542ed02",
            "ai_summary": "Atom-Searcher, an RL framework integrating Atomic Thought and Reasoning Reward Models, enhances LLMs' multi-hop reasoning and strategic search capabilities, improving performance and interpretability.",
            "ai_keywords": [
                "Retrieval-Augmented Generation",
                "RAG",
                "agentic deep research",
                "reinforcement learning",
                "RL",
                "Atomic Thought",
                "Reasoning Reward Models",
                "RRMs",
                "Atomic Thought Rewards",
                "ATR",
                "curriculum-inspired reward schedule",
                "multi-hop reasoning",
                "strategic search",
                "interpretable reasoning"
            ]
        },
        "publishedAt": "2025-08-18T06:23:10.000Z",
        "title": "Atom-Searcher: Enhancing Agentic Deep Research via Fine-Grained Atomic\n  Thought Reward",
        "summary": "Large language models (LLMs) exhibit remarkable problem-solving abilities,\nbut struggle with complex tasks due to static internal knowledge.\nRetrieval-Augmented Generation (RAG) enhances access to external information,\nyet remains limited in multi-hop reasoning and strategic search due to rigid\nworkflows. Recent advancements in agentic deep research empower LLMs to\nautonomously reason, search, and synthesize information. However, current\napproaches relying on outcome-based reinforcement learning (RL) face critical\nissues such as conflicting gradients and reward sparsity, limiting performance\ngains and training efficiency. To address these, we first propose Atomic\nThought, a novel LLM thinking paradigm that decomposes reasoning into\nfine-grained functional units. These units are supervised by Reasoning Reward\nModels (RRMs), which provide Atomic Thought Rewards (ATR) for fine-grained\nguidance. Building on this, we propose Atom-Searcher, a novel RL framework for\nagentic deep research that integrates Atomic Thought and ATR. Atom-Searcher\nuses a curriculum-inspired reward schedule, prioritizing process-level ATR\nearly and transitioning to outcome rewards, accelerating convergence on\neffective reasoning paths. Experiments on seven benchmarks show consistent\nimprovements over the state-of-the-art. Key advantages include: (1)\nAtom-Searcher scales computation at test-time. (2) Atomic Thought provides\nsupervision anchors for RRMs, bridging deep research tasks and RRMs. (3)\nAtom-Searcher exhibits more interpretable, human-like reasoning patterns.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63ea134a43d976de6e4e8ea7/T1TzZI-A3Hvn5RTqCz7lc.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12800.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ea134a43d976de6e4e8ea7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63ea134a43d976de6e4e8ea7/OsyE7SYDW9foqsyWXtTiL.png",
            "fullname": "deng yong",
            "name": "dikw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    }
]