[
    {
        "paper": {
            "id": "2508.14460",
            "authors": [
                {
                    "_id": "68a69c4a9e4b49496aac699d",
                    "user": {
                        "_id": "61a9ccca3e8d72e791476614",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61a9ccca3e8d72e791476614/icULFDp3dPKgwUePllNly.png",
                        "isPro": false,
                        "fullname": "Shuaijie She",
                        "user": "kevinpro",
                        "type": "user"
                    },
                    "name": "Shuaijie She",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:54:56.836Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c4a9e4b49496aac699e",
                    "user": {
                        "_id": "6142f4041fbcc4d3c42020fa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/51c_M6rDZoJvnuC3eauqZ.png",
                        "isPro": false,
                        "fullname": "Yu Bao",
                        "user": "baoy",
                        "type": "user"
                    },
                    "name": "Yu Bao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T13:32:14.025Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c4a9e4b49496aac699f",
                    "user": {
                        "_id": "65e56e44e8b017ee13408589",
                        "avatarUrl": "/avatars/f9a8a7f7d2ff4474cad752a66452ce1e.svg",
                        "isPro": false,
                        "fullname": "Yu Lu",
                        "user": "YuLu0713",
                        "type": "user"
                    },
                    "name": "Yu Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T13:32:09.682Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c4a9e4b49496aac69a0",
                    "name": "Lu Xu",
                    "hidden": false
                },
                {
                    "_id": "68a69c4a9e4b49496aac69a1",
                    "name": "Tao Li",
                    "hidden": false
                },
                {
                    "_id": "68a69c4a9e4b49496aac69a2",
                    "user": {
                        "_id": "649d7d8968586ca9bf7f5fe6",
                        "avatarUrl": "/avatars/b444240770d4025dea41871cf38126dc.svg",
                        "isPro": false,
                        "fullname": "Wenhao Zhu",
                        "user": "Wenhao97",
                        "type": "user"
                    },
                    "name": "Wenhao Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T13:32:11.842Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c4a9e4b49496aac69a3",
                    "user": {
                        "_id": "687ba59e595ea10fe729316e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/mHa4lsl7glwop3S9lq4BP.png",
                        "isPro": false,
                        "fullname": "Shujian Huang",
                        "user": "ShujianHuang",
                        "type": "user"
                    },
                    "name": "Shujian Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:55:13.771Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c4a9e4b49496aac69a4",
                    "user": {
                        "_id": "632ab0407fb39c2b6350c10a",
                        "avatarUrl": "/avatars/fb02cad2a017654965486418bf370157.svg",
                        "isPro": false,
                        "fullname": "Cheng",
                        "user": "Shanbo",
                        "type": "user"
                    },
                    "name": "Shanbo Cheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:55:07.743Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c4a9e4b49496aac69a5",
                    "name": "Lu Lu",
                    "hidden": false
                },
                {
                    "_id": "68a69c4a9e4b49496aac69a6",
                    "name": "Yuxuan Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-20T06:31:18.000Z",
            "submittedOnDailyAt": "2025-08-21T02:41:01.130Z",
            "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference\n  Optimization",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present DuPO, a dual learning-based preference optimization framework that\ngenerates annotation-free feedback via a generalized duality. DuPO addresses\ntwo key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s\nreliance on costly labels and applicability restricted to verifiable tasks, and\ntraditional dual learning's restriction to strictly dual task pairs (e.g.,\ntranslation and back-translation). Specifically, DuPO decomposes a primal\ntask's input into known and unknown components, then constructs its dual task\nto reconstruct the unknown part using the primal output and known information\n(e.g., reversing math solutions to recover hidden variables), broadening\napplicability to non-invertible tasks. The quality of this reconstruction\nserves as a self-supervised reward to optimize the primal task, synergizing\nwith LLMs' ability to instantiate both tasks via a single model. Empirically,\nDuPO achieves substantial gains across diverse tasks: it enhances the average\ntranslation quality by 2.13 COMET over 756 directions, boosts the mathematical\nreasoning accuracy by an average of 6.4 points on three challenge benchmarks,\nand enhances performance by 9.3 points as an inference-time reranker (trading\ncomputation for accuracy). These results position DuPO as a scalable, general,\nand annotation-free paradigm for LLM optimization.",
            "upvotes": 54,
            "discussionId": "68a69c4b9e4b49496aac69a7",
            "ai_summary": "DuPO is a dual learning framework that generates annotation-free feedback using a generalized duality, enhancing performance across various tasks without relying on costly labels.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "dual learning",
                "primal task",
                "dual task",
                "self-supervised reward",
                "LLMs",
                "translation quality",
                "mathematical reasoning accuracy",
                "inference-time reranker"
            ]
        },
        "publishedAt": "2025-08-20T02:31:18.000Z",
        "title": "DuPO: Enabling Reliable LLM Self-Verification via Dual Preference\n  Optimization",
        "summary": "We present DuPO, a dual learning-based preference optimization framework that\ngenerates annotation-free feedback via a generalized duality. DuPO addresses\ntwo key limitations: Reinforcement Learning with Verifiable Rewards (RLVR)'s\nreliance on costly labels and applicability restricted to verifiable tasks, and\ntraditional dual learning's restriction to strictly dual task pairs (e.g.,\ntranslation and back-translation). Specifically, DuPO decomposes a primal\ntask's input into known and unknown components, then constructs its dual task\nto reconstruct the unknown part using the primal output and known information\n(e.g., reversing math solutions to recover hidden variables), broadening\napplicability to non-invertible tasks. The quality of this reconstruction\nserves as a self-supervised reward to optimize the primal task, synergizing\nwith LLMs' ability to instantiate both tasks via a single model. Empirically,\nDuPO achieves substantial gains across diverse tasks: it enhances the average\ntranslation quality by 2.13 COMET over 756 directions, boosts the mathematical\nreasoning accuracy by an average of 6.4 points on three challenge benchmarks,\nand enhances performance by 9.3 points as an inference-time reranker (trading\ncomputation for accuracy). These results position DuPO as a scalable, general,\nand annotation-free paradigm for LLM optimization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14460.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.13491",
            "authors": [
                {
                    "_id": "68a685519e4b49496aac6938",
                    "name": "Ziyan Kuang",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac6939",
                    "name": "Feiyu Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac693a",
                    "name": "Maowei Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac693b",
                    "user": {
                        "_id": "6880eca712035d59e50ba716",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/-6sPgohq0n_-oxFmyAvmr.png",
                        "isPro": false,
                        "fullname": "Yanzhao Lai",
                        "user": "2083L",
                        "type": "user"
                    },
                    "name": "Yanzhao Lai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:58:05.932Z",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac693c",
                    "name": "Zelin Wang",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac693d",
                    "name": "Zhitong Wang",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac693e",
                    "name": "Meikang Qiu",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac693f",
                    "user": {
                        "_id": "66f3c0f97a2d332b0d81d18f",
                        "avatarUrl": "/avatars/a8283846b8b104af0c2d3566455b2004.svg",
                        "isPro": false,
                        "fullname": "Jiajia Huang",
                        "user": "hugai101",
                        "type": "user"
                    },
                    "name": "Jiajia Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:57:35.753Z",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac6940",
                    "name": "Min Peng",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac6941",
                    "user": {
                        "_id": "6479f4317c18dca75e9a9324",
                        "avatarUrl": "/avatars/9aa709230b057f57ee4415c04a622c63.svg",
                        "isPro": false,
                        "fullname": "Xie",
                        "user": "QianqianXie1994",
                        "type": "user"
                    },
                    "name": "Qianqian Xie",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:57:19.006Z",
                    "hidden": false
                },
                {
                    "_id": "68a685519e4b49496aac6942",
                    "user": {
                        "_id": "66f6cb352c5d4ef3578a9c3f",
                        "avatarUrl": "/avatars/0a70c94072bc5e1d018cf12da0904ff0.svg",
                        "isPro": false,
                        "fullname": "Sophia Ananiadou",
                        "user": "Effoula",
                        "type": "user"
                    },
                    "name": "Sophia Ananiadou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:57:04.485Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T03:52:15.000Z",
            "submittedOnDailyAt": "2025-08-21T02:04:42.491Z",
            "title": "From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating\n  Financial Large Language Models",
            "submittedOnDailyBy": {
                "_id": "663adb42e14047f710dc1d29",
                "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg",
                "isPro": false,
                "fullname": "Mengxi Xiao",
                "user": "ElsaShaw",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have shown promise for financial applications,\nyet their suitability for this high-stakes domain remains largely unproven due\nto inadequacies in existing benchmarks. Existing benchmarks solely rely on\nscore-level evaluation, summarizing performance with a single score that\nobscures the nuanced understanding of what models truly know and their precise\nlimitations. They also rely on datasets that cover only a narrow subset of\nfinancial concepts, while overlooking other essentials for real-world\napplications. To address these gaps, we introduce FinCDM, the first cognitive\ndiagnosis evaluation framework tailored for financial LLMs, enabling the\nevaluation of LLMs at the knowledge-skill level, identifying what financial\nskills and knowledge they have or lack based on their response patterns across\nskill-tagged tasks, rather than a single aggregated number. We construct\nCPA-QKA, the first cognitively informed financial evaluation dataset derived\nfrom the Certified Public Accountant (CPA) examination, with comprehensive\ncoverage of real-world accounting and financial skills. It is rigorously\nannotated by domain experts, who author, validate, and annotate questions with\nhigh inter-annotator agreement and fine-grained knowledge labels. Our extensive\nexperiments on 30 proprietary, open-source, and domain-specific LLMs show that\nFinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax\nand regulatory reasoning overlooked by traditional benchmarks, and uncovers\nbehavioral clusters among models. FinCDM introduces a new paradigm for\nfinancial LLM evaluation by enabling interpretable, skill-aware diagnosis that\nsupports more trustworthy and targeted model development, and all datasets and\nevaluation scripts will be publicly released to support further research.",
            "upvotes": 53,
            "discussionId": "68a685529e4b49496aac6943",
            "ai_summary": "FinCDM, a cognitive diagnosis framework, evaluates financial LLMs at the knowledge-skill level using a comprehensive dataset, revealing hidden knowledge gaps and supporting more trustworthy model development.",
            "ai_keywords": [
                "Large Language Models",
                "FinCDM",
                "cognitive diagnosis evaluation framework",
                "CPA-QKA",
                "Certified Public Accountant",
                "accounting and financial skills",
                "knowledge-skill level",
                "knowledge gaps",
                "tax and regulatory reasoning",
                "behavioral clusters",
                "skill-aware diagnosis"
            ]
        },
        "publishedAt": "2025-08-18T23:52:15.000Z",
        "title": "From Scores to Skills: A Cognitive Diagnosis Framework for Evaluating\n  Financial Large Language Models",
        "summary": "Large Language Models (LLMs) have shown promise for financial applications,\nyet their suitability for this high-stakes domain remains largely unproven due\nto inadequacies in existing benchmarks. Existing benchmarks solely rely on\nscore-level evaluation, summarizing performance with a single score that\nobscures the nuanced understanding of what models truly know and their precise\nlimitations. They also rely on datasets that cover only a narrow subset of\nfinancial concepts, while overlooking other essentials for real-world\napplications. To address these gaps, we introduce FinCDM, the first cognitive\ndiagnosis evaluation framework tailored for financial LLMs, enabling the\nevaluation of LLMs at the knowledge-skill level, identifying what financial\nskills and knowledge they have or lack based on their response patterns across\nskill-tagged tasks, rather than a single aggregated number. We construct\nCPA-QKA, the first cognitively informed financial evaluation dataset derived\nfrom the Certified Public Accountant (CPA) examination, with comprehensive\ncoverage of real-world accounting and financial skills. It is rigorously\nannotated by domain experts, who author, validate, and annotate questions with\nhigh inter-annotator agreement and fine-grained knowledge labels. Our extensive\nexperiments on 30 proprietary, open-source, and domain-specific LLMs show that\nFinCDM reveals hidden knowledge gaps, identifies under-tested areas such as tax\nand regulatory reasoning overlooked by traditional benchmarks, and uncovers\nbehavioral clusters among models. FinCDM introduces a new paradigm for\nfinancial LLM evaluation by enabling interpretable, skill-aware diagnosis that\nsupports more trustworthy and targeted model development, and all datasets and\nevaluation scripts will be publicly released to support further research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13491.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "663adb42e14047f710dc1d29",
            "avatarUrl": "/avatars/7ca49d67a4a8b4cf0ee896e07646715f.svg",
            "fullname": "Mengxi Xiao",
            "name": "ElsaShaw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.11987",
            "authors": [
                {
                    "_id": "68a43789b65388761d0745dc",
                    "name": "Zhiyuan Zeng",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745dd",
                    "user": {
                        "_id": "6842e440d029f9bcf58077b4",
                        "avatarUrl": "/avatars/bbdc6c603c38849d762055ae10d41d03.svg",
                        "isPro": false,
                        "fullname": "jiashuo liu",
                        "user": "liujiashuo77",
                        "type": "user"
                    },
                    "name": "Jiashuo Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:54:09.753Z",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745de",
                    "name": "Siyuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745df",
                    "user": {
                        "_id": "68a6b767fddc160a09aa883d",
                        "avatarUrl": "/avatars/0d8e94727f0e2298174c724afccf8c01.svg",
                        "isPro": false,
                        "fullname": "Tianci He",
                        "user": "Tianci-He",
                        "type": "user"
                    },
                    "name": "Tianci He",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:53:54.777Z",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e0",
                    "name": "Yali Liao",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e1",
                    "name": "Jinpeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e2",
                    "name": "Zaiyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e3",
                    "name": "Yang Yang",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e4",
                    "user": {
                        "_id": "68a6d4bf99ba7d66206e2ce5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/f29nNljyQNHSkovNqXBDy.png",
                        "isPro": false,
                        "fullname": "Yin",
                        "user": "YinLingyue",
                        "type": "user"
                    },
                    "name": "Lingyue Yin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:54:47.013Z",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e5",
                    "name": "Mingren Yin",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e6",
                    "user": {
                        "_id": "6178b6a62be97faa212726ba",
                        "avatarUrl": "/avatars/07e216aeb6e45f6f9779238339ba8eec.svg",
                        "isPro": false,
                        "fullname": "ZHENWEI ZHU",
                        "user": "Nuori",
                        "type": "user"
                    },
                    "name": "Zhenwei Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:55:03.392Z",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e7",
                    "user": {
                        "_id": "64459357c525660aa20be337",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/1fryBXRuSvAgMIDkpWBpI.jpeg",
                        "isPro": false,
                        "fullname": "Tianle Cai",
                        "user": "tianlecai",
                        "type": "user"
                    },
                    "name": "Tianle Cai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:55:10.542Z",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e8",
                    "user": {
                        "_id": "64892d31cbda0d1cdb956897",
                        "avatarUrl": "/avatars/3cdafe03a8295124636347d15a099aaf.svg",
                        "isPro": false,
                        "fullname": "Zehui Chen",
                        "user": "lovesnowbest",
                        "type": "user"
                    },
                    "name": "Zehui Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:55:16.941Z",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745e9",
                    "user": {
                        "_id": "66df70fe5a0c5910d663160d",
                        "avatarUrl": "/avatars/980ca32bd0049ef5bbf002e7dc9f911c.svg",
                        "isPro": false,
                        "fullname": "jiecao.chen",
                        "user": "xmerge123",
                        "type": "user"
                    },
                    "name": "Jiecao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:55:23.724Z",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745ea",
                    "name": "Yantao Du",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745eb",
                    "name": "Xiang Gao",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745ec",
                    "name": "Jiacheng Guo",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745ed",
                    "name": "Liang Hu",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745ee",
                    "name": "Jianpeng Jiao",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745ef",
                    "user": {
                        "_id": "61a883b0d4b1c264b4b668d9",
                        "avatarUrl": "/avatars/87d569aaf821fda56e32849b728f021c.svg",
                        "isPro": false,
                        "fullname": "Xiangsheng Li",
                        "user": "lixsh6",
                        "type": "user"
                    },
                    "name": "Xiangsheng Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T09:55:53.652Z",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f0",
                    "name": "Jingkai Liu",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f1",
                    "name": "Shuang Ni",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f2",
                    "name": "Zhoufutu Wen",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f3",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:52:32.340Z",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f4",
                    "name": "Kaiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f5",
                    "name": "Xin Zhou",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f6",
                    "name": "Jose Blanchet",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f7",
                    "name": "Xipeng Qiu",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f8",
                    "name": "Mengdi Wang",
                    "hidden": false
                },
                {
                    "_id": "68a43789b65388761d0745f9",
                    "name": "Wenhao Huang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6842e440d029f9bcf58077b4/zDuiKey8NEonyLxc5XC_d.png"
            ],
            "publishedAt": "2025-08-16T08:54:08.000Z",
            "submittedOnDailyAt": "2025-08-21T01:39:14.470Z",
            "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
            "submittedOnDailyBy": {
                "_id": "6842e440d029f9bcf58077b4",
                "avatarUrl": "/avatars/bbdc6c603c38849d762055ae10d41d03.svg",
                "isPro": false,
                "fullname": "jiashuo liu",
                "user": "liujiashuo77",
                "type": "user"
            },
            "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\nFutureX, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.",
            "upvotes": 46,
            "discussionId": "68a4378ab65388761d0745fa",
            "projectPage": "https://futurex-ai.github.io/",
            "ai_summary": "FutureX is a dynamic, live benchmark for evaluating LLM agents in future prediction tasks, addressing challenges in real-time updates and data contamination.",
            "ai_keywords": [
                "LLM agents",
                "future prediction",
                "adaptive reasoning",
                "real-time updates",
                "data contamination",
                "automated pipeline",
                "question gathering",
                "answer collection",
                "failure modes",
                "performance pitfalls",
                "fake web pages",
                "temporal validity"
            ]
        },
        "publishedAt": "2025-08-16T04:54:08.000Z",
        "title": "FutureX: An Advanced Live Benchmark for LLM Agents in Future Prediction",
        "summary": "Future prediction is a complex task for LLM agents, requiring a high level of\nanalytical thinking, information gathering, contextual understanding, and\ndecision-making under uncertainty. Agents must not only gather and interpret\nvast amounts of dynamic information but also integrate diverse data sources,\nweigh uncertainties, and adapt predictions based on emerging trends, just as\nhuman experts do in fields like politics, economics, and finance. Despite its\nimportance, no large-scale benchmark exists for evaluating agents on future\nprediction, largely due to challenges in handling real-time updates and\nretrieving timely, accurate answers. To address this, we introduce\nFutureX, a dynamic and live evaluation benchmark specifically\ndesigned for LLM agents performing future prediction tasks. FutureX is the\nlargest and most diverse live benchmark for future prediction, supporting\nreal-time daily updates and eliminating data contamination through an automated\npipeline for question gathering and answer collection. We evaluate 25 LLM/agent\nmodels, including those with reasoning, search capabilities, and integration of\nexternal tools such as the open-source Deep Research Agent and closed-source\nDeep Research models. This comprehensive evaluation assesses agents' adaptive\nreasoning and performance in dynamic environments. Additionally, we provide\nin-depth analyses of agents' failure modes and performance pitfalls in\nfuture-oriented tasks, including the vulnerability to fake web pages and the\ntemporal validity. Our goal is to establish a dynamic, contamination-free\nevaluation standard that drives the development of LLM agents capable of\nperforming at the level of professional human analysts in complex reasoning and\npredictive thinking.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6842e440d029f9bcf58077b4/zDuiKey8NEonyLxc5XC_d.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11987.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6842e440d029f9bcf58077b4",
            "avatarUrl": "/avatars/bbdc6c603c38849d762055ae10d41d03.svg",
            "fullname": "jiashuo liu",
            "name": "liujiashuo77",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.14879",
            "authors": [
                {
                    "_id": "68a68c549e4b49496aac696e",
                    "name": "Bingquan Dai",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac696f",
                    "name": "Li Ray Luo",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6970",
                    "user": {
                        "_id": "673dbb45df154a60e3aed0f1",
                        "avatarUrl": "/avatars/4a6f8996193d115ba3e5eae43be4c80e.svg",
                        "isPro": false,
                        "fullname": "Qihong Tang",
                        "user": "tangqh",
                        "type": "user"
                    },
                    "name": "Qihong Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T09:15:56.638Z",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6971",
                    "name": "Jie Wang",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6972",
                    "name": "Xinyu Lian",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6973",
                    "name": "Hao Xu",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6974",
                    "user": {
                        "_id": "642002b51ccd411979d72b18",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642002b51ccd411979d72b18/JO9e0o8fAFNC-eYFbx_JW.png",
                        "isPro": false,
                        "fullname": "Minghan Qin",
                        "user": "Qmh",
                        "type": "user"
                    },
                    "name": "Minghan Qin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:31:54.784Z",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6975",
                    "name": "Xudong Xu",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6976",
                    "user": {
                        "_id": "678be86f2ef23c3cae684a1f",
                        "avatarUrl": "/avatars/1dc09d5a8dfbf777bf85077e1739b197.svg",
                        "isPro": false,
                        "fullname": "Bo Dai",
                        "user": "asrnline",
                        "type": "user"
                    },
                    "name": "Bo Dai",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:31:48.784Z",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6977",
                    "name": "Haoqian Wang",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6978",
                    "user": {
                        "_id": "63f2ec797ddf724fbcc75aee",
                        "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                        "isPro": false,
                        "fullname": "Zhaoyang Lyu",
                        "user": "ZhaoyangLyu",
                        "type": "user"
                    },
                    "name": "Zhaoyang Lyu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:31:20.817Z",
                    "hidden": false
                },
                {
                    "_id": "68a68c549e4b49496aac6979",
                    "user": {
                        "_id": "65783ee6ee33d547aecc3ffc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65783ee6ee33d547aecc3ffc/lWZX88c-0dCsN-yB9Jhlf.jpeg",
                        "isPro": false,
                        "fullname": "Jiangmiao Pang",
                        "user": "Jiangmiao",
                        "type": "user"
                    },
                    "name": "Jiangmiao Pang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:31:27.829Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/Ol5wGXJbiZPqL_OGDddxv.mp4"
            ],
            "publishedAt": "2025-08-20T17:50:15.000Z",
            "submittedOnDailyAt": "2025-08-21T01:50:57.946Z",
            "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
            "submittedOnDailyBy": {
                "_id": "63f2ec797ddf724fbcc75aee",
                "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
                "isPro": false,
                "fullname": "Zhaoyang Lyu",
                "user": "ZhaoyangLyu",
                "type": "user"
            },
            "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.",
            "upvotes": 28,
            "discussionId": "68a68c549e4b49496aac697a",
            "projectPage": "https://daibingquan.github.io/MeshCoder",
            "githubRepo": "https://github.com/ZhaoyangLyu/MeshCoder",
            "ai_summary": "MeshCoder reconstructs complex 3D objects from point clouds into editable Blender Python scripts, enhancing shape-to-code reconstruction and 3D shape understanding through a multimodal large language model.",
            "ai_keywords": [
                "MeshCoder",
                "Blender Python APIs",
                "multimodal large language model",
                "point clouds",
                "shape-to-code reconstruction",
                "3D shape understanding"
            ],
            "githubStars": 79
        },
        "publishedAt": "2025-08-20T13:50:15.000Z",
        "title": "MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds",
        "summary": "Reconstructing 3D objects into editable programs is pivotal for applications\nlike reverse engineering and shape editing. However, existing methods often\nrely on limited domain-specific languages (DSLs) and small-scale datasets,\nrestricting their ability to model complex geometries and structures. To\naddress these challenges, we introduce MeshCoder, a novel framework that\nreconstructs complex 3D objects from point clouds into editable Blender Python\nscripts. We develop a comprehensive set of expressive Blender Python APIs\ncapable of synthesizing intricate geometries. Leveraging these APIs, we\nconstruct a large-scale paired object-code dataset, where the code for each\nobject is decomposed into distinct semantic parts. Subsequently, we train a\nmultimodal large language model (LLM) that translates 3D point cloud into\nexecutable Blender Python scripts. Our approach not only achieves superior\nperformance in shape-to-code reconstruction tasks but also facilitates\nintuitive geometric and topological editing through convenient code\nmodifications. Furthermore, our code-based representation enhances the\nreasoning capabilities of LLMs in 3D shape understanding tasks. Together, these\ncontributions establish MeshCoder as a powerful and flexible solution for\nprogrammatic 3D shape reconstruction and understanding.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63f2ec797ddf724fbcc75aee/Ol5wGXJbiZPqL_OGDddxv.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14879.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63f2ec797ddf724fbcc75aee",
            "avatarUrl": "/avatars/e93432ad11da703d46fe5e594d69f8c0.svg",
            "fullname": "Zhaoyang Lyu",
            "name": "ZhaoyangLyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.14811",
            "authors": [
                {
                    "_id": "68a690589e4b49496aac698a",
                    "user": {
                        "_id": "646efd223dd912a539e0bd46",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
                        "isPro": false,
                        "fullname": "Canyu Zhao",
                        "user": "Canyu",
                        "type": "user"
                    },
                    "name": "Canyu Zhao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:29:26.094Z",
                    "hidden": false
                },
                {
                    "_id": "68a690589e4b49496aac698b",
                    "name": "Xiaoman Li",
                    "hidden": false
                },
                {
                    "_id": "68a690589e4b49496aac698c",
                    "name": "Tianjian Feng",
                    "hidden": false
                },
                {
                    "_id": "68a690589e4b49496aac698d",
                    "name": "Zhiyue Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a690589e4b49496aac698e",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "68a690589e4b49496aac698f",
                    "name": "Chunhua Shen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/646efd223dd912a539e0bd46/Rk_JTxNPyxfOwly7MC3Wc.mp4"
            ],
            "publishedAt": "2025-08-20T16:02:59.000Z",
            "submittedOnDailyAt": "2025-08-21T01:50:49.884Z",
            "title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From\n  Sparse Inputs without Per-Scene Optimization",
            "submittedOnDailyBy": {
                "_id": "646efd223dd912a539e0bd46",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
                "isPro": false,
                "fullname": "Canyu Zhao",
                "user": "Canyu",
                "type": "user"
            },
            "summary": "We introduce Tinker, a versatile framework for high-fidelity 3D editing that\noperates in both one-shot and few-shot regimes without any per-scene\nfinetuning. Unlike prior techniques that demand extensive per-scene\noptimization to ensure multi-view consistency or to produce dozens of\nconsistent edited input views, Tinker delivers robust, multi-view consistent\nedits from as few as one or two images. This capability stems from repurposing\npretrained diffusion models, which unlocks their latent 3D awareness. To drive\nresearch in this space, we curate the first large-scale multi-view editing\ndataset and data pipeline, spanning diverse scenes and styles. Building on this\ndataset, we develop our framework capable of generating multi-view consistent\nedited views without per-scene training, which consists of two novel\ncomponents: (1) Referring multi-view editor: Enables precise, reference-driven\nedits that remain coherent across all viewpoints. (2) Any-view-to-video\nsynthesizer: Leverages spatial-temporal priors from video diffusion to perform\nhigh-quality scene completion and novel-view generation even from sparse\ninputs. Through extensive experiments, Tinker significantly reduces the barrier\nto generalizable 3D content creation, achieving state-of-the-art performance on\nediting, novel-view synthesis, and rendering enhancement tasks. We believe that\nTinker represents a key step towards truly scalable, zero-shot 3D editing.\nProject webpage: https://aim-uofa.github.io/Tinker",
            "upvotes": 25,
            "discussionId": "68a690599e4b49496aac6990",
            "projectPage": "https://aim-uofa.github.io/Tinker/",
            "githubRepo": "https://github.com/aim-uofa/Tinker",
            "ai_summary": "Tinker is a framework for high-fidelity 3D editing using pretrained diffusion models, enabling multi-view consistency with minimal per-scene training.",
            "ai_keywords": [
                "diffusion models",
                "latent 3D awareness",
                "multi-view editing",
                "referring multi-view editor",
                "any-view-to-video synthesizer",
                "video diffusion",
                "scene completion",
                "novel-view generation",
                "zero-shot 3D editing"
            ],
            "githubStars": 45
        },
        "publishedAt": "2025-08-20T12:02:59.000Z",
        "title": "Tinker: Diffusion's Gift to 3D--Multi-View Consistent Editing From\n  Sparse Inputs without Per-Scene Optimization",
        "summary": "We introduce Tinker, a versatile framework for high-fidelity 3D editing that\noperates in both one-shot and few-shot regimes without any per-scene\nfinetuning. Unlike prior techniques that demand extensive per-scene\noptimization to ensure multi-view consistency or to produce dozens of\nconsistent edited input views, Tinker delivers robust, multi-view consistent\nedits from as few as one or two images. This capability stems from repurposing\npretrained diffusion models, which unlocks their latent 3D awareness. To drive\nresearch in this space, we curate the first large-scale multi-view editing\ndataset and data pipeline, spanning diverse scenes and styles. Building on this\ndataset, we develop our framework capable of generating multi-view consistent\nedited views without per-scene training, which consists of two novel\ncomponents: (1) Referring multi-view editor: Enables precise, reference-driven\nedits that remain coherent across all viewpoints. (2) Any-view-to-video\nsynthesizer: Leverages spatial-temporal priors from video diffusion to perform\nhigh-quality scene completion and novel-view generation even from sparse\ninputs. Through extensive experiments, Tinker significantly reduces the barrier\nto generalizable 3D content creation, achieving state-of-the-art performance on\nediting, novel-view synthesis, and rendering enhancement tasks. We believe that\nTinker represents a key step towards truly scalable, zero-shot 3D editing.\nProject webpage: https://aim-uofa.github.io/Tinker",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/646efd223dd912a539e0bd46/Rk_JTxNPyxfOwly7MC3Wc.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14811.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646efd223dd912a539e0bd46",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/EOFAv5xvOgJOzuDgh4nSb.png",
            "fullname": "Canyu Zhao",
            "name": "Canyu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 14
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.14111",
            "authors": [
                {
                    "_id": "68a6a2e59e4b49496aac6a93",
                    "user": {
                        "_id": "6539bc7756c9b35961021fa8",
                        "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg",
                        "isPro": false,
                        "fullname": "Jiaqi Wei",
                        "user": "VitaCoco",
                        "type": "user"
                    },
                    "name": "Jiaqi Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T09:15:38.233Z",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a94",
                    "name": "Yuejin Yang",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a95",
                    "name": "Xiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a96",
                    "name": "Yuhan Chen",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a97",
                    "name": "Xiang Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a98",
                    "user": {
                        "_id": "63f1e45f0a16587ea9640c67",
                        "avatarUrl": "/avatars/ed5e594560e32c8fef4c5508d5ee466e.svg",
                        "isPro": false,
                        "fullname": "Zhangyang Gao",
                        "user": "ZhangyangGao",
                        "type": "user"
                    },
                    "name": "Zhangyang Gao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:32:34.526Z",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a99",
                    "user": {
                        "_id": "6538b861613fe158bd581e35",
                        "avatarUrl": "/avatars/6817dbfe903675721fd227058b0a91ac.svg",
                        "isPro": false,
                        "fullname": "Dongzhan Zhou",
                        "user": "schrodingers-tiger",
                        "type": "user"
                    },
                    "name": "Dongzhan Zhou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:32:42.215Z",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a9a",
                    "user": {
                        "_id": "64a4f1cdac6c687e8e903e15",
                        "avatarUrl": "/avatars/fc1be5cc5f237fe0e0853a4562ce8878.svg",
                        "isPro": false,
                        "fullname": "GuangshuaiWang",
                        "user": "Wanggsh",
                        "type": "user"
                    },
                    "name": "Guangshuai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:32:48.394Z",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a9b",
                    "name": "Zhiqiang Gao",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a9c",
                    "name": "Juntai Cao",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a9d",
                    "user": {
                        "_id": "682dd02c4e8320c969c15a03",
                        "avatarUrl": "/avatars/4efd8343320b9623fcdec4e0024d345a.svg",
                        "isPro": false,
                        "fullname": "qiu",
                        "user": "zijieqiu",
                        "type": "user"
                    },
                    "name": "Zijie Qiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:33:11.907Z",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a9e",
                    "name": "Xuming He",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6a9f",
                    "name": "Qiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6aa0",
                    "user": {
                        "_id": "6466d463060756d2854ab3e1",
                        "avatarUrl": "/avatars/4401387180c16472a6823f78aaa86d54.svg",
                        "isPro": false,
                        "fullname": "Chenyu You",
                        "user": "Charlesyooo",
                        "type": "user"
                    },
                    "name": "Chenyu You",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:33:46.203Z",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6aa1",
                    "user": {
                        "_id": "6438c14f27a3c24cf4a5bdc9",
                        "avatarUrl": "/avatars/19997d1529f287bc20eaeb0cd5dc634f.svg",
                        "isPro": false,
                        "fullname": "Shuangjia Zheng",
                        "user": "AbHunter",
                        "type": "user"
                    },
                    "name": "Shuangjia Zheng",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:33:53.289Z",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6aa2",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6aa3",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6aa4",
                    "user": {
                        "_id": "68385a89ecf59de6ef0510ca",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6qZ0KijBOhFri0ue3T1pB.jpeg",
                        "isPro": false,
                        "fullname": "Nanqing Dong",
                        "user": "eveningdong",
                        "type": "user"
                    },
                    "name": "Nanqing Dong",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:34:08.444Z",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6aa5",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6aa6",
                    "name": "Siqi Sun",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6aa7",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68a6a2e59e4b49496aac6aa8",
                    "name": "Bowen Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-18T05:25:54.000Z",
            "submittedOnDailyAt": "2025-08-21T05:42:52.665Z",
            "title": "From AI for Science to Agentic Science: A Survey on Autonomous\n  Scientific Discovery",
            "submittedOnDailyBy": {
                "_id": "656553d89bf6665f10e3a92d",
                "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                "isPro": false,
                "fullname": "xiang wyatt zhang",
                "user": "Wyattz23",
                "type": "user"
            },
            "summary": "Artificial intelligence (AI) is reshaping scientific discovery, evolving from\nspecialized computational tools into autonomous research partners. We position\nAgentic Science as a pivotal stage within the broader AI for Science paradigm,\nwhere AI systems progress from partial assistance to full scientific agency.\nEnabled by large language models (LLMs), multimodal systems, and integrated\nresearch platforms, agentic AI shows capabilities in hypothesis generation,\nexperimental design, execution, analysis, and iterative refinement -- behaviors\nonce regarded as uniquely human. This survey provides a domain-oriented review\nof autonomous scientific discovery across life sciences, chemistry, materials\nscience, and physics. We unify three previously fragmented perspectives --\nprocess-oriented, autonomy-oriented, and mechanism-oriented -- through a\ncomprehensive framework that connects foundational capabilities, core\nprocesses, and domain-specific realizations. Building on this framework, we (i)\ntrace the evolution of AI for Science, (ii) identify five core capabilities\nunderpinning scientific agency, (iii) model discovery as a dynamic four-stage\nworkflow, (iv) review applications across the above domains, and (v) synthesize\nkey challenges and future opportunities. This work establishes a\ndomain-oriented synthesis of autonomous scientific discovery and positions\nAgentic Science as a structured paradigm for advancing AI-driven research.",
            "upvotes": 20,
            "discussionId": "68a6a2e59e4b49496aac6aa9",
            "ai_summary": "Agentic Science leverages large language models, multimodal systems, and integrated platforms to enable autonomous scientific discovery across various domains, encompassing hypothesis generation, experimental design, execution, analysis, and iterative refinement.",
            "ai_keywords": [
                "large language models",
                "multimodal systems",
                "integrated research platforms",
                "hypothesis generation",
                "experimental design",
                "scientific agency",
                "process-oriented",
                "autonomy-oriented",
                "mechanism-oriented",
                "domain-specific realizations",
                "dynamic four-stage workflow"
            ]
        },
        "publishedAt": "2025-08-18T01:25:54.000Z",
        "title": "From AI for Science to Agentic Science: A Survey on Autonomous\n  Scientific Discovery",
        "summary": "Artificial intelligence (AI) is reshaping scientific discovery, evolving from\nspecialized computational tools into autonomous research partners. We position\nAgentic Science as a pivotal stage within the broader AI for Science paradigm,\nwhere AI systems progress from partial assistance to full scientific agency.\nEnabled by large language models (LLMs), multimodal systems, and integrated\nresearch platforms, agentic AI shows capabilities in hypothesis generation,\nexperimental design, execution, analysis, and iterative refinement -- behaviors\nonce regarded as uniquely human. This survey provides a domain-oriented review\nof autonomous scientific discovery across life sciences, chemistry, materials\nscience, and physics. We unify three previously fragmented perspectives --\nprocess-oriented, autonomy-oriented, and mechanism-oriented -- through a\ncomprehensive framework that connects foundational capabilities, core\nprocesses, and domain-specific realizations. Building on this framework, we (i)\ntrace the evolution of AI for Science, (ii) identify five core capabilities\nunderpinning scientific agency, (iii) model discovery as a dynamic four-stage\nworkflow, (iv) review applications across the above domains, and (v) synthesize\nkey challenges and future opportunities. This work establishes a\ndomain-oriented synthesis of autonomous scientific discovery and positions\nAgentic Science as a structured paradigm for advancing AI-driven research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14111.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656553d89bf6665f10e3a92d",
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "fullname": "xiang wyatt zhang",
            "name": "Wyattz23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.14704",
            "authors": [
                {
                    "_id": "68a704f99e4b49496aac6ba4",
                    "user": {
                        "_id": "6090ff099a8bcaa437b234a4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6090ff099a8bcaa437b234a4/iUvw7JXT-ngI7rGk1x-io.jpeg",
                        "isPro": false,
                        "fullname": "Ziyang Luo",
                        "user": "Ziyang",
                        "type": "user"
                    },
                    "name": "Ziyang Luo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T13:29:21.151Z",
                    "hidden": false
                },
                {
                    "_id": "68a704f99e4b49496aac6ba5",
                    "name": "Zhiqi Shen",
                    "hidden": false
                },
                {
                    "_id": "68a704f99e4b49496aac6ba6",
                    "name": "Wenzhuo Yang",
                    "hidden": false
                },
                {
                    "_id": "68a704f99e4b49496aac6ba7",
                    "name": "Zirui Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a704f99e4b49496aac6ba8",
                    "name": "Prathyusha Jwalapuram",
                    "hidden": false
                },
                {
                    "_id": "68a704f99e4b49496aac6ba9",
                    "name": "Amrita Saha",
                    "hidden": false
                },
                {
                    "_id": "68a704f99e4b49496aac6baa",
                    "name": "Doyen Sahoo",
                    "hidden": false
                },
                {
                    "_id": "68a704f99e4b49496aac6bab",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "68a704f99e4b49496aac6bac",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68a704f99e4b49496aac6bad",
                    "name": "Junnan Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-20T13:28:58.000Z",
            "submittedOnDailyAt": "2025-08-21T10:23:07.461Z",
            "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model\n  Context Protocol Servers",
            "submittedOnDailyBy": {
                "_id": "6090ff099a8bcaa437b234a4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6090ff099a8bcaa437b234a4/iUvw7JXT-ngI7rGk1x-io.jpeg",
                "isPro": false,
                "fullname": "Ziyang Luo",
                "user": "Ziyang",
                "type": "user"
            },
            "summary": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem.",
            "upvotes": 17,
            "discussionId": "68a704f99e4b49496aac6bae",
            "projectPage": "https://mcp-universe.github.io/",
            "githubRepo": "https://github.com/SalesforceAIResearch/MCP-Universe",
            "ai_summary": "MCP-Universe is a comprehensive benchmark designed to evaluate large language models in realistic tasks through interaction with real-world MCP servers, addressing challenges like long-horizon reasoning and unfamiliar tool spaces.",
            "ai_keywords": [
                "Model Context Protocol",
                "MCP-Universe",
                "large language models",
                "LLMs",
                "execution-based evaluators",
                "format evaluators",
                "static evaluators",
                "dynamic evaluators",
                "long-context challenge",
                "unknown-tools challenge",
                "ReAct frameworks"
            ],
            "githubStars": 21
        },
        "publishedAt": "2025-08-20T09:28:58.000Z",
        "title": "MCP-Universe: Benchmarking Large Language Models with Real-World Model\n  Context Protocol Servers",
        "summary": "The Model Context Protocol has emerged as a transformative standard for\nconnecting large language models to external data sources and tools, rapidly\ngaining adoption across major AI providers and development platforms. However,\nexisting benchmarks are overly simplistic and fail to capture real application\nchallenges such as long-horizon reasoning and large, unfamiliar tool spaces. To\naddress this critical gap, we introduce MCP-Universe, the first comprehensive\nbenchmark specifically designed to evaluate LLMs in realistic and hard tasks\nthrough interaction with real-world MCP servers. Our benchmark encompasses 6\ncore domains spanning 11 different MCP servers: Location Navigation, Repository\nManagement, Financial Analysis, 3D Design, Browser Automation, and Web\nSearching. To ensure rigorous evaluation, we implement execution-based\nevaluators, including format evaluators for agent format compliance, static\nevaluators for time-invariant content matching, and dynamic evaluators that\nautomatically retrieve real-time ground truth for temporally sensitive tasks.\nThrough extensive evaluation of leading LLMs, we find that even SOTA models\nsuch as GPT-5 (43.72%), Grok-4 (33.33%) and Claude-4.0-Sonnet (29.44%) exhibit\nsignificant performance limitations. In addition, our benchmark poses a\nsignificant long-context challenge for LLM agents, as the number of input\ntokens increases rapidly with the number of interaction steps. Moreover, it\nintroduces an unknown-tools challenge, as LLM agents often lack familiarity\nwith the precise usage of the MCP servers. Notably, enterprise-level agents\nlike Cursor cannot achieve better performance than standard ReAct frameworks.\nBeyond evaluation, we open-source our extensible evaluation framework with UI\nsupport, enabling researchers and practitioners to seamlessly integrate new\nagents and MCP servers while fostering innovation in the rapidly evolving MCP\necosystem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14704.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6090ff099a8bcaa437b234a4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6090ff099a8bcaa437b234a4/iUvw7JXT-ngI7rGk1x-io.jpeg",
            "fullname": "Ziyang Luo",
            "name": "Ziyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 20
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.14896",
            "authors": [
                {
                    "_id": "68a69cf99e4b49496aac6a7f",
                    "user": {
                        "_id": "6841378dcb9938c7aed795be",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WNI8gQVHdWyt0OvmRINDp.png",
                        "isPro": false,
                        "fullname": "Haokun Lin",
                        "user": "Felix1023",
                        "type": "user"
                    },
                    "name": "Haokun Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:40:58.332Z",
                    "hidden": false
                },
                {
                    "_id": "68a69cf99e4b49496aac6a80",
                    "name": "Haobo Xu",
                    "hidden": false
                },
                {
                    "_id": "68a69cf99e4b49496aac6a81",
                    "user": {
                        "_id": "67b05af6626cd81034211436",
                        "avatarUrl": "/avatars/c2f13727955fbb2b8f1df2df3972e417.svg",
                        "isPro": false,
                        "fullname": "Yichen Wu",
                        "user": "chriswyc",
                        "type": "user"
                    },
                    "name": "Yichen Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:41:33.065Z",
                    "hidden": false
                },
                {
                    "_id": "68a69cf99e4b49496aac6a82",
                    "user": {
                        "_id": "647d9ab61a1fcad2fdbf2d3d",
                        "avatarUrl": "/avatars/48c8aeae8979d2c87df8bde922437d62.svg",
                        "isPro": true,
                        "fullname": "Ziyu Guo",
                        "user": "ZiyuG",
                        "type": "user"
                    },
                    "name": "Ziyu Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:41:38.881Z",
                    "hidden": false
                },
                {
                    "_id": "68a69cf99e4b49496aac6a83",
                    "name": "Renrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a69cf99e4b49496aac6a84",
                    "name": "Zhichao Lu",
                    "hidden": false
                },
                {
                    "_id": "68a69cf99e4b49496aac6a85",
                    "name": "Ying Wei",
                    "hidden": false
                },
                {
                    "_id": "68a69cf99e4b49496aac6a86",
                    "user": {
                        "_id": "64478bf230fa4ecb85ddf661",
                        "avatarUrl": "/avatars/16ffa30a8785b20c8fdb61021d385070.svg",
                        "isPro": false,
                        "fullname": "Qingfu Zhang",
                        "user": "cityug7353",
                        "type": "user"
                    },
                    "name": "Qingfu Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:42:17.485Z",
                    "hidden": false
                },
                {
                    "_id": "68a69cf99e4b49496aac6a87",
                    "name": "Zhenan Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-20T17:59:51.000Z",
            "submittedOnDailyAt": "2025-08-21T02:44:16.778Z",
            "title": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs",
            "submittedOnDailyBy": {
                "_id": "6841378dcb9938c7aed795be",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WNI8gQVHdWyt0OvmRINDp.png",
                "isPro": false,
                "fullname": "Haokun Lin",
                "user": "Felix1023",
                "type": "user"
            },
            "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.",
            "upvotes": 14,
            "discussionId": "68a69cf99e4b49496aac6a88",
            "ai_summary": "A systematic study on quantizing diffusion large language models identifies challenges and evaluates state-of-the-art methods across various configurations to improve deployment on edge devices.",
            "ai_keywords": [
                "diffusion large language models",
                "autoregressive LLMs",
                "full attention",
                "denoising-based decoding",
                "post-training quantization",
                "activation outliers",
                "low-bit quantization",
                "bit-width",
                "quantization method",
                "task category",
                "model type"
            ]
        },
        "publishedAt": "2025-08-20T13:59:51.000Z",
        "title": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs",
        "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. All codes and\nexperimental setups will be released to support the community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14896.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6841378dcb9938c7aed795be",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/WNI8gQVHdWyt0OvmRINDp.png",
            "fullname": "Haokun Lin",
            "name": "Felix1023",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.14444",
            "authors": [
                {
                    "_id": "68a69c709e4b49496aac69a9",
                    "name": "NVIDIA",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ab",
                    "name": "Aarti Basant",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ac",
                    "name": "Abhijit Khairnar",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ad",
                    "user": {
                        "_id": "633fa79c33ba83e00bdcca3d",
                        "avatarUrl": "/avatars/05eb40c5e25f4a29c1d5032f489323f2.svg",
                        "isPro": false,
                        "fullname": "Abhijit Paithankar",
                        "user": "apaithan",
                        "type": "user"
                    },
                    "name": "Abhijit Paithankar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:55:41.707Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ae",
                    "user": {
                        "_id": "63216617b754c42716f1e048",
                        "avatarUrl": "/avatars/29bf04d1b2d41f681df4d2141c352ca4.svg",
                        "isPro": false,
                        "fullname": "Abhinav Khattar",
                        "user": "aklife97",
                        "type": "user"
                    },
                    "name": "Abhinav Khattar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:55:47.279Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69af",
                    "user": {
                        "_id": "652d963065a4619fb5c9a4c1",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/652d963065a4619fb5c9a4c1/PTEcYzFnXOomPQpTPhnqc.png",
                        "isPro": false,
                        "fullname": "Adi Renduchintala",
                        "user": "adirendu",
                        "type": "user"
                    },
                    "name": "Adi Renduchintala",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:56:01.199Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b0",
                    "name": "Adithya Renduchintala",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b1",
                    "user": {
                        "_id": "620ae6b441aa65c2deea283c",
                        "avatarUrl": "/avatars/fb00a1a421ebe7582b61b960b19472ac.svg",
                        "isPro": false,
                        "fullname": "Aditya Malte",
                        "user": "aditya-malte",
                        "type": "user"
                    },
                    "name": "Aditya Malte",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:56:10.026Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b2",
                    "user": {
                        "_id": "66857bd849a4ed9de4c31936",
                        "avatarUrl": "/avatars/f6f016bf36fad5b29f30fbec6cde3e4d.svg",
                        "isPro": false,
                        "fullname": "Akhiad Bercovich",
                        "user": "abercovich",
                        "type": "user"
                    },
                    "name": "Akhiad Bercovich",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:56:15.612Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b3",
                    "user": {
                        "_id": "654d1e1b67555cb9e6b8cf3e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654d1e1b67555cb9e6b8cf3e/-_5_qZCDheM1qtrFIee5t.png",
                        "isPro": false,
                        "fullname": "Akshay Hazare",
                        "user": "ahazare",
                        "type": "user"
                    },
                    "name": "Akshay Hazare",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:56:20.859Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b4",
                    "name": "Alejandra Rico",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b5",
                    "user": {
                        "_id": "6394bfd02bb41bad0d3c6a79",
                        "avatarUrl": "/avatars/d5fa8536ec61dc1fa903dee03f1a32a2.svg",
                        "isPro": false,
                        "fullname": "Aleksander Ficek",
                        "user": "aleksficek",
                        "type": "user"
                    },
                    "name": "Aleksander Ficek",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:56:32.258Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b6",
                    "name": "Alex Kondratenko",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b7",
                    "name": "Alex Shaposhnikov",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b8",
                    "user": {
                        "_id": "654a4bd5b9cfada0bd39536e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/hRIrBfdbTTnnBp2eHrnBV.png",
                        "isPro": false,
                        "fullname": "Ali Taghibakhshi",
                        "user": "jrd971000",
                        "type": "user"
                    },
                    "name": "Ali Taghibakhshi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:57:08.279Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69b9",
                    "name": "Amelia Barton",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ba",
                    "user": {
                        "_id": "659ef64b419de2ea602cf271",
                        "avatarUrl": "/avatars/b5313218c009cf6d3fb8e3a7ac11091a.svg",
                        "isPro": false,
                        "fullname": "Ameya Sunil Mahabaleshwarkar",
                        "user": "ameyasunilm",
                        "type": "user"
                    },
                    "name": "Ameya Sunil Mahabaleshwarkar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:57:18.560Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69bb",
                    "user": {
                        "_id": "677e1df33d23e9cec92e3b72",
                        "avatarUrl": "/avatars/3653228d7b2a9af796340b0688d9719e.svg",
                        "isPro": false,
                        "fullname": "Amy Shen",
                        "user": "AmyS2025",
                        "type": "user"
                    },
                    "name": "Amy Shen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:57:25.233Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69bc",
                    "name": "Andrew Tao",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69bd",
                    "user": {
                        "_id": "68072520b0cb768cae28beb1",
                        "avatarUrl": "/avatars/7e1d8cd1c6809e9a8f8e1c70c58f1f6c.svg",
                        "isPro": false,
                        "fullname": "Ann Guan",
                        "user": "anguan",
                        "type": "user"
                    },
                    "name": "Ann Guan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:57:36.794Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69be",
                    "user": {
                        "_id": "6642f04c3d008bab5d9febcf",
                        "avatarUrl": "/avatars/ca5aa35651ad6e842f07518f0bb9830a.svg",
                        "isPro": false,
                        "fullname": "Anna Shors",
                        "user": "ashors",
                        "type": "user"
                    },
                    "name": "Anna Shors",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:57:43.532Z",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69bf",
                    "name": "Anubhav Mandarwal",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c0",
                    "name": "Arham Mehta",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c1",
                    "name": "Arun Venkatesan",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c2",
                    "name": "Ashton Sharabiani",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c3",
                    "name": "Ashwath Aithal",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c4",
                    "name": "Ashwin Poojary",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c5",
                    "name": "Ayush Dattagupta",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c6",
                    "name": "Balaram Buddharaju",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c7",
                    "name": "Banghua Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c8",
                    "name": "Barnaby Simkin",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69c9",
                    "name": "Bilal Kartal",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ca",
                    "name": "Bita Darvish Rouhani",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69cb",
                    "name": "Bobby Chen",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69cc",
                    "name": "Boris Ginsburg",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69cd",
                    "name": "Brandon Norick",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ce",
                    "name": "Brian Yu",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69cf",
                    "name": "Bryan Catanzaro",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d0",
                    "name": "Charles Wang",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d1",
                    "name": "Charlie Truong",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d2",
                    "name": "Chetan Mungekar",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d3",
                    "name": "Chintan Patel",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d4",
                    "name": "Chris Alexiuk",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d5",
                    "name": "Christian Munley",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d6",
                    "name": "Christopher Parisien",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d7",
                    "name": "Dan Su",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d8",
                    "name": "Daniel Afrimi",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69d9",
                    "name": "Daniel Korzekwa",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69da",
                    "name": "Daniel Rohrer",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69db",
                    "name": "Daria Gitman",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69dc",
                    "name": "David Mosallanezhad",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69dd",
                    "name": "Deepak Narayanan",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69de",
                    "name": "Dima Rekesh",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69df",
                    "name": "Dina Yared",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e0",
                    "name": "Dmytro Pykhtar",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e1",
                    "name": "Dong Ahn",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e2",
                    "name": "Duncan Riach",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e3",
                    "name": "Eileen Long",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e4",
                    "name": "Elliott Ning",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e5",
                    "name": "Eric Chung",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e6",
                    "name": "Erick Galinkin",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e7",
                    "name": "Evelina Bakhturina",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e8",
                    "name": "Gargi Prasad",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69e9",
                    "name": "Gerald Shen",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ea",
                    "name": "Haim Elisha",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69eb",
                    "name": "Harsh Sharma",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ec",
                    "name": "Hayley Ross",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ed",
                    "name": "Helen Ngo",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ee",
                    "name": "Herman Sahota",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ef",
                    "name": "Hexin Wang",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f0",
                    "name": "Hoo Chang Shin",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f1",
                    "name": "Hua Huang",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f2",
                    "name": "Iain Cunningham",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f3",
                    "name": "Igor Gitman",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f4",
                    "name": "Ivan Moshkov",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f5",
                    "name": "Jaehun Jung",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f6",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f7",
                    "name": "Jane Polak Scowcroft",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f8",
                    "name": "Jared Casper",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69f9",
                    "name": "Jimmy Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69fa",
                    "name": "Jinze Xue",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69fb",
                    "name": "Jocelyn Huang",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69fc",
                    "name": "Joey Conway",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69fd",
                    "name": "John Kamalu",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69fe",
                    "name": "Jonathan Cohen",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac69ff",
                    "name": "Joseph Jennings",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a00",
                    "name": "Julien Veron Vialard",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a01",
                    "name": "Junkeun Yi",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a02",
                    "name": "Jupinder Parmar",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a03",
                    "name": "Kari Briski",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a04",
                    "name": "Katherine Cheung",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a05",
                    "name": "Katherine Luna",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a06",
                    "name": "Keith Wyss",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a07",
                    "name": "Keshav Santhanam",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a08",
                    "name": "Kezhi Kong",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a09",
                    "name": "Krzysztof Pawelec",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a0a",
                    "name": "Kumar Anik",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a0b",
                    "name": "Kunlun Li",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a0c",
                    "name": "Kushan Ahmadian",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a0d",
                    "name": "Lawrence McAfee",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a0e",
                    "name": "Laya Sleiman",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a0f",
                    "name": "Leon Derczynski",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a10",
                    "name": "Luis Vega",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a11",
                    "name": "Maer Rodrigues de Melo",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a12",
                    "name": "Makesh Narsimhan Sreedhar",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a13",
                    "name": "Marcin Chochowski",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a14",
                    "name": "Mark Cai",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a15",
                    "name": "Markus Kliegl",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a16",
                    "name": "Marta Stepniewska-Dziubinska",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a17",
                    "name": "Matvei Novikov",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a18",
                    "name": "Mehrzad Samadi",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a19",
                    "name": "Meredith Price",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a1a",
                    "name": "Meriem Boubdir",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a1b",
                    "name": "Michael Boone",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a1c",
                    "name": "Michael Evans",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a1d",
                    "name": "Michal Bien",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a1e",
                    "name": "Michal Zawalski",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a1f",
                    "name": "Miguel Martinez",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a20",
                    "name": "Mike Chrzanowski",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a21",
                    "name": "Mohammad Shoeybi",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a22",
                    "name": "Mostofa Patwary",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a23",
                    "name": "Namit Dhameja",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a24",
                    "name": "Nave Assaf",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a25",
                    "name": "Negar Habibi",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a26",
                    "name": "Nidhi Bhatia",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a27",
                    "name": "Nikki Pope",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a28",
                    "name": "Nima Tajbakhsh",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a29",
                    "name": "Nirmal Kumar Juluru",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a2a",
                    "name": "Oleg Rybakov",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a2b",
                    "name": "Oleksii Hrinchuk",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a2c",
                    "name": "Oleksii Kuchaiev",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a2d",
                    "name": "Oluwatobi Olabiyi",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a2e",
                    "name": "Pablo Ribalta",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a2f",
                    "name": "Padmavathy Subramanian",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a30",
                    "name": "Parth Chadha",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a31",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a32",
                    "name": "Peter Dykas",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a33",
                    "name": "Peter Jin",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a34",
                    "name": "Piotr Bialecki",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a35",
                    "name": "Piotr Januszewski",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a36",
                    "name": "Pradeep Thalasta",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a37",
                    "name": "Prashant Gaikwad",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a38",
                    "name": "Prasoon Varshney",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a39",
                    "name": "Pritam Gundecha",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a3a",
                    "name": "Przemek Tredak",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a3b",
                    "name": "Rabeeh Karimi Mahabadi",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a3c",
                    "name": "Rajen Patel",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a3d",
                    "name": "Ran El-Yaniv",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a3e",
                    "name": "Ranjit Rajan",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a3f",
                    "name": "Ria Cheruvu",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a40",
                    "name": "Rima Shahbazyan",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a41",
                    "name": "Ritika Borkar",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a42",
                    "name": "Ritu Gala",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a43",
                    "name": "Roger Waleffe",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a44",
                    "name": "Ruoxi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a45",
                    "name": "Russell J. Hewett",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a46",
                    "name": "Ryan Prenger",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a47",
                    "name": "Sahil Jain",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a48",
                    "name": "Samuel Kriman",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a49",
                    "name": "Sanjeev Satheesh",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a4a",
                    "name": "Saori Kaji",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a4b",
                    "name": "Sarah Yurick",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a4c",
                    "name": "Saurav Muralidharan",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a4d",
                    "name": "Sean Narenthiran",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a4e",
                    "name": "Seonmyeong Bak",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a4f",
                    "name": "Sepehr Sameni",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a50",
                    "name": "Seungju Han",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a51",
                    "name": "Shanmugam Ramasamy",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a52",
                    "name": "Shaona Ghosh",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a53",
                    "name": "Sharath Turuvekere Sreenivas",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a54",
                    "name": "Shelby Thomas",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a55",
                    "name": "Shizhe Diao",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a56",
                    "name": "Shreya Gopal",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a57",
                    "name": "Shrimai Prabhumoye",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a58",
                    "name": "Shubham Toshniwal",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a59",
                    "name": "Shuoyang Ding",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a5a",
                    "name": "Siddharth Singh",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a5b",
                    "name": "Siddhartha Jain",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a5c",
                    "name": "Somshubra Majumdar",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a5d",
                    "name": "Stefania Alborghetti",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a5e",
                    "name": "Syeda Nahida Akter",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a5f",
                    "name": "Terry Kong",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a60",
                    "name": "Tim Moon",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a61",
                    "name": "Tomasz Hliwiak",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a62",
                    "name": "Tomer Asida",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a63",
                    "name": "Tony Wang",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a64",
                    "name": "Twinkle Vashishth",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a65",
                    "name": "Tyler Poon",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a66",
                    "name": "Udi Karpas",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a67",
                    "name": "Vahid Noroozi",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a68",
                    "name": "Venkat Srinivasan",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a69",
                    "name": "Vijay Korthikanti",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a6a",
                    "name": "Vikram Fugro",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a6b",
                    "name": "Vineeth Kalluru",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a6c",
                    "name": "Vitaly Kurin",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a6d",
                    "name": "Vitaly Lavrukhin",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a6e",
                    "name": "Wasi Uddin Ahmad",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a6f",
                    "name": "Wei Du",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a70",
                    "name": "Wonmin Byeon",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a71",
                    "name": "Ximing Lu",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a72",
                    "name": "Xin Dong",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a73",
                    "name": "Yashaswi Karnati",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a74",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a75",
                    "name": "Yian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a76",
                    "name": "Ying Lin",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a77",
                    "name": "Yonggan Fu",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a78",
                    "name": "Yoshi Suhara",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a79",
                    "name": "Zhen Dong",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a7a",
                    "name": "Zhiyu Li",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a7b",
                    "name": "Zhongbo Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a69c709e4b49496aac6a7c",
                    "name": "Zijia Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-20T06:00:57.000Z",
            "submittedOnDailyAt": "2025-08-21T02:41:40.962Z",
            "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid\n  Mamba-Transformer Reasoning Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of the self-attention layers in the common Transformer architecture\nare replaced with Mamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks\nwhile achieving up to 6x higher inference throughput in reasoning settings like\n8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,\nNemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with\nthe majority of our pre- and post-training datasets on Hugging Face.",
            "upvotes": 13,
            "discussionId": "68a69c719e4b49496aac6a7d",
            "ai_summary": "Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer model, enhances reasoning workload throughput and accuracy by replacing self-attention layers with Mamba-2 layers and using the Minitron strategy for compression.",
            "ai_keywords": [
                "Mamba-Transformer",
                "self-attention layers",
                "Mamba-2 layers",
                "Minitron strategy",
                "inference throughput",
                "reasoning benchmarks"
            ]
        },
        "publishedAt": "2025-08-20T02:00:57.000Z",
        "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid\n  Mamba-Transformer Reasoning Model",
        "summary": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model\ndesigned to increase throughput for reasoning workloads while achieving\nstate-of-the-art accuracy compared to similarly-sized models.\nNemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the\nmajority of the self-attention layers in the common Transformer architecture\nare replaced with Mamba-2 layers, to achieve improved inference speed when\ngenerating the long thinking traces needed for reasoning. We create\nNemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model\n(Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe.\nAfter aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to\ncompress and distill the model with the goal of enabling inference on up to\n128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision).\nCompared to existing similarly-sized models (e.g., Qwen3-8B), we show that\nNemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks\nwhile achieving up to 6x higher inference throughput in reasoning settings like\n8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2,\nNemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with\nthe majority of our pre- and post-training datasets on Hugging Face.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14444.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.14160",
            "authors": [
                {
                    "_id": "68a68f329e4b49496aac697f",
                    "user": {
                        "_id": "64731a68a7f23affe7736d3d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8wESKLFcS2ltPzL-wpG4Z.jpeg",
                        "isPro": false,
                        "fullname": "Ronghao Dang",
                        "user": "RH-Dang",
                        "type": "user"
                    },
                    "name": "Ronghao Dang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:42:56.047Z",
                    "hidden": false
                },
                {
                    "_id": "68a68f329e4b49496aac6980",
                    "user": {
                        "_id": "64a3fe3dde901eb01df12398",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
                        "isPro": false,
                        "fullname": "YuqianYuan",
                        "user": "CircleRadon",
                        "type": "user"
                    },
                    "name": "Yuqian Yuan",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:42:42.962Z",
                    "hidden": false
                },
                {
                    "_id": "68a68f329e4b49496aac6981",
                    "user": {
                        "_id": "67fcc97cede5c434e0cc37e3",
                        "avatarUrl": "/avatars/b07e0a4744c1045828a621146ee6d3c2.svg",
                        "isPro": false,
                        "fullname": "yunxuan mao",
                        "user": "maoyunxuan",
                        "type": "user"
                    },
                    "name": "Yunxuan Mao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:43:01.835Z",
                    "hidden": false
                },
                {
                    "_id": "68a68f329e4b49496aac6982",
                    "user": {
                        "_id": "66836aba7b50b433cd681182",
                        "avatarUrl": "/avatars/483a6fef6d27680d19f9ea1789122bdd.svg",
                        "isPro": false,
                        "fullname": "Kehan LI",
                        "user": "CausalLi",
                        "type": "user"
                    },
                    "name": "Kehan Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:43:08.690Z",
                    "hidden": false
                },
                {
                    "_id": "68a68f329e4b49496aac6983",
                    "user": {
                        "_id": "67a6082b2f32323bfb5e6641",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/NZvjm4Kapc7AgReexgRgi.png",
                        "isPro": false,
                        "fullname": "jiangpin",
                        "user": "jiangpinliu",
                        "type": "user"
                    },
                    "name": "Jiangpin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:43:13.996Z",
                    "hidden": false
                },
                {
                    "_id": "68a68f329e4b49496aac6984",
                    "name": "Zhikai Wang",
                    "hidden": false
                },
                {
                    "_id": "68a68f329e4b49496aac6985",
                    "name": "Xin Li",
                    "hidden": false
                },
                {
                    "_id": "68a68f329e4b49496aac6986",
                    "name": "Fan Wang",
                    "hidden": false
                },
                {
                    "_id": "68a68f329e4b49496aac6987",
                    "name": "Deli Zhao",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/za0ek08sv8tqTUwNe-5v2.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/lCUNSM9dYqB_AZSvP1cOz.png"
            ],
            "publishedAt": "2025-08-19T18:00:01.000Z",
            "submittedOnDailyAt": "2025-08-21T01:57:33.918Z",
            "title": "RynnEC: Bringing MLLMs into Embodied World",
            "submittedOnDailyBy": {
                "_id": "64a3fe3dde901eb01df12398",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
                "isPro": false,
                "fullname": "YuqianYuan",
                "user": "CircleRadon",
                "type": "user"
            },
            "summary": "We introduce RynnEC, a video multimodal large language model designed for\nembodied cognition. Built upon a general-purpose vision-language foundation\nmodel, RynnEC incorporates a region encoder and a mask decoder, enabling\nflexible region-level video interaction. Despite its compact architecture,\nRynnEC achieves state-of-the-art performance in object property understanding,\nobject segmentation, and spatial reasoning. Conceptually, it offers a\nregion-centric video paradigm for the brain of embodied agents, providing\nfine-grained perception of the physical world and enabling more precise\ninteractions. To mitigate the scarcity of annotated 3D datasets, we propose an\negocentric video based pipeline for generating embodied cognition data.\nFurthermore, we introduce RynnEC-Bench, a region-centered benchmark for\nevaluating embodied cognitive capabilities. We anticipate that RynnEC will\nadvance the development of general-purpose cognitive cores for embodied agents\nand facilitate generalization across diverse embodied tasks. The code, model\ncheckpoints, and benchmark are available at:\nhttps://github.com/alibaba-damo-academy/RynnEC",
            "upvotes": 9,
            "discussionId": "68a68f339e4b49496aac6988",
            "githubRepo": "https://github.com/alibaba-damo-academy/RynnEC",
            "ai_summary": "RynnEC, a video multimodal large language model with a region-centric approach, achieves state-of-the-art performance in object property understanding, segmentation, and spatial reasoning, using an egocentric video pipeline and a region-centered benchmark.",
            "ai_keywords": [
                "video multimodal large language model",
                "embodied cognition",
                "region encoder",
                "mask decoder",
                "region-level video interaction",
                "object property understanding",
                "object segmentation",
                "spatial reasoning",
                "egocentric video",
                "RynnEC-Bench",
                "embodied cognitive capabilities"
            ],
            "githubStars": 94
        },
        "publishedAt": "2025-08-19T14:00:01.000Z",
        "title": "RynnEC: Bringing MLLMs into Embodied World",
        "summary": "We introduce RynnEC, a video multimodal large language model designed for\nembodied cognition. Built upon a general-purpose vision-language foundation\nmodel, RynnEC incorporates a region encoder and a mask decoder, enabling\nflexible region-level video interaction. Despite its compact architecture,\nRynnEC achieves state-of-the-art performance in object property understanding,\nobject segmentation, and spatial reasoning. Conceptually, it offers a\nregion-centric video paradigm for the brain of embodied agents, providing\nfine-grained perception of the physical world and enabling more precise\ninteractions. To mitigate the scarcity of annotated 3D datasets, we propose an\negocentric video based pipeline for generating embodied cognition data.\nFurthermore, we introduce RynnEC-Bench, a region-centered benchmark for\nevaluating embodied cognitive capabilities. We anticipate that RynnEC will\nadvance the development of general-purpose cognitive cores for embodied agents\nand facilitate generalization across diverse embodied tasks. The code, model\ncheckpoints, and benchmark are available at:\nhttps://github.com/alibaba-damo-academy/RynnEC",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/za0ek08sv8tqTUwNe-5v2.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64a3fe3dde901eb01df12398/lCUNSM9dYqB_AZSvP1cOz.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14160.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a3fe3dde901eb01df12398",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a3fe3dde901eb01df12398/Js2bEx4rxKuEKVt5z9I2D.jpeg",
            "fullname": "YuqianYuan",
            "name": "CircleRadon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13421",
            "authors": [
                {
                    "_id": "68a689869e4b49496aac6945",
                    "user": {
                        "_id": "68a68709b31cf04906ee189a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xeWDDCGxcYjDWf_l7UxA9.png",
                        "isPro": false,
                        "fullname": "Gabrielle",
                        "user": "gwehr",
                        "type": "user"
                    },
                    "name": "Gabrielle Wehr",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T09:15:58.966Z",
                    "hidden": false
                },
                {
                    "_id": "68a689869e4b49496aac6946",
                    "name": "Reuben Rideaux",
                    "hidden": false
                },
                {
                    "_id": "68a689869e4b49496aac6947",
                    "name": "Amaya J. Fox",
                    "hidden": false
                },
                {
                    "_id": "68a689869e4b49496aac6948",
                    "name": "David R. Lightfoot",
                    "hidden": false
                },
                {
                    "_id": "68a689869e4b49496aac6949",
                    "name": "Jason Tangen",
                    "hidden": false
                },
                {
                    "_id": "68a689869e4b49496aac694a",
                    "name": "Jason B. Mattingley",
                    "hidden": false
                },
                {
                    "_id": "68a689869e4b49496aac694b",
                    "name": "Shane E. Ehrhardt",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T00:35:56.000Z",
            "submittedOnDailyAt": "2025-08-21T21:49:27.116Z",
            "title": "Virtuous Machines: Towards Artificial General Science",
            "submittedOnDailyBy": {
                "_id": "68a68709b31cf04906ee189a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xeWDDCGxcYjDWf_l7UxA9.png",
                "isPro": false,
                "fullname": "Gabrielle",
                "user": "gwehr",
                "type": "user"
            },
            "summary": "Artificial intelligence systems are transforming scientific discovery by\naccelerating specific research tasks, from protein structure prediction to\nmaterials design, yet remain confined to narrow domains requiring substantial\nhuman oversight. The exponential growth of scientific literature and increasing\ndomain specialisation constrain researchers' capacity to synthesise knowledge\nacross disciplines and develop unifying theories, motivating exploration of\nmore general-purpose AI systems for science. Here we show that a\ndomain-agnostic, agentic AI system can independently navigate the scientific\nworkflow - from hypothesis generation through data collection to manuscript\npreparation. The system autonomously designed and executed three psychological\nstudies on visual working memory, mental rotation, and imagery vividness,\nexecuted one new online data collection with 288 participants, developed\nanalysis pipelines through 8-hour+ continuous coding sessions, and produced\ncompleted manuscripts. The results demonstrate the capability of AI scientific\ndiscovery pipelines to conduct non-trivial research with theoretical reasoning\nand methodological rigour comparable to experienced researchers, though with\nlimitations in conceptual nuance and theoretical interpretation. This is a step\ntoward embodied AI that can test hypotheses through real-world experiments,\naccelerating discovery by autonomously exploring regions of scientific space\nthat human cognitive and resource constraints might otherwise leave unexplored.\nIt raises important questions about the nature of scientific understanding and\nthe attribution of scientific credit.",
            "upvotes": 5,
            "discussionId": "68a689869e4b49496aac694c",
            "ai_summary": "A domain-agnostic AI system can autonomously design, execute, and analyze psychological studies, producing manuscripts with methodological rigor comparable to human researchers.",
            "ai_keywords": [
                "domain-agnostic",
                "agentic AI",
                "scientific workflow",
                "hypothesis generation",
                "data collection",
                "manuscript preparation",
                "psychological studies",
                "visual working memory",
                "mental rotation",
                "imagery vividness",
                "analysis pipelines",
                "theoretical reasoning",
                "methodological rigour",
                "embodied AI",
                "real-world experiments",
                "scientific discovery"
            ]
        },
        "publishedAt": "2025-08-18T20:35:56.000Z",
        "title": "Virtuous Machines: Towards Artificial General Science",
        "summary": "Artificial intelligence systems are transforming scientific discovery by\naccelerating specific research tasks, from protein structure prediction to\nmaterials design, yet remain confined to narrow domains requiring substantial\nhuman oversight. The exponential growth of scientific literature and increasing\ndomain specialisation constrain researchers' capacity to synthesise knowledge\nacross disciplines and develop unifying theories, motivating exploration of\nmore general-purpose AI systems for science. Here we show that a\ndomain-agnostic, agentic AI system can independently navigate the scientific\nworkflow - from hypothesis generation through data collection to manuscript\npreparation. The system autonomously designed and executed three psychological\nstudies on visual working memory, mental rotation, and imagery vividness,\nexecuted one new online data collection with 288 participants, developed\nanalysis pipelines through 8-hour+ continuous coding sessions, and produced\ncompleted manuscripts. The results demonstrate the capability of AI scientific\ndiscovery pipelines to conduct non-trivial research with theoretical reasoning\nand methodological rigour comparable to experienced researchers, though with\nlimitations in conceptual nuance and theoretical interpretation. This is a step\ntoward embodied AI that can test hypotheses through real-world experiments,\naccelerating discovery by autonomously exploring regions of scientific space\nthat human cognitive and resource constraints might otherwise leave unexplored.\nIt raises important questions about the nature of scientific understanding and\nthe attribution of scientific credit.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13421.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "68a68709b31cf04906ee189a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/xeWDDCGxcYjDWf_l7UxA9.png",
            "fullname": "Gabrielle",
            "name": "gwehr",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.11408",
            "authors": [
                {
                    "_id": "68a52f576cf0bf898542ec27",
                    "user": {
                        "_id": "63f46a0fa096536aeab6ee75",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f46a0fa096536aeab6ee75/Bte71XHp05z_vXvt8POev.png",
                        "isPro": false,
                        "fullname": "garyzhang",
                        "user": "xiaoniqiu",
                        "type": "user"
                    },
                    "name": "Wenhao Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:51:54.803Z",
                    "hidden": false
                },
                {
                    "_id": "68a52f576cf0bf898542ec28",
                    "name": "Yuexiang Xie",
                    "hidden": false
                },
                {
                    "_id": "68a52f576cf0bf898542ec29",
                    "name": "Yuchang Sun",
                    "hidden": false
                },
                {
                    "_id": "68a52f576cf0bf898542ec2a",
                    "name": "Yanxi Chen",
                    "hidden": false
                },
                {
                    "_id": "68a52f576cf0bf898542ec2b",
                    "name": "Guoyin Wang",
                    "hidden": false
                },
                {
                    "_id": "68a52f576cf0bf898542ec2c",
                    "name": "Yaliang Li",
                    "hidden": false
                },
                {
                    "_id": "68a52f576cf0bf898542ec2d",
                    "name": "Bolin Ding",
                    "hidden": false
                },
                {
                    "_id": "68a52f576cf0bf898542ec2e",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-15T11:20:03.000Z",
            "submittedOnDailyAt": "2025-08-21T07:16:27.915Z",
            "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised\n  Fine-Tuning and Reinforcement Learning via Dynamic Weighting",
            "submittedOnDailyBy": {
                "_id": "63f46a0fa096536aeab6ee75",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f46a0fa096536aeab6ee75/Bte71XHp05z_vXvt8POev.png",
                "isPro": false,
                "fullname": "garyzhang",
                "user": "xiaoniqiu",
                "type": "user"
            },
            "summary": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research.",
            "upvotes": 4,
            "discussionId": "68a52f576cf0bf898542ec2f",
            "ai_summary": "CHORD integrates supervised fine-tuning and reinforcement learning by dynamically weighting off-policy and on-policy data to improve model stability and performance.",
            "ai_keywords": [
                "Supervised Fine-Tuning",
                "Reinforcement Learning",
                "Large Language Models",
                "off-policy",
                "on-policy",
                "Controllable Harmonization",
                "Dynamic Weighting",
                "dual-control mechanism",
                "global coefficient",
                "token-wise weighting function",
                "on-policy exploration",
                "off-policy data",
                "empirical evidence",
                "benchmarks"
            ]
        },
        "publishedAt": "2025-08-15T07:20:03.000Z",
        "title": "On-Policy RL Meets Off-Policy Experts: Harmonizing Supervised\n  Fine-Tuning and Reinforcement Learning via Dynamic Weighting",
        "summary": "Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) are two\nprominent post-training paradigms for refining the capabilities and aligning\nthe behavior of Large Language Models (LLMs). Existing approaches that\nintegrate SFT and RL often face the risk of disrupting established model\npatterns and inducing overfitting to expert data. To address this, we present a\nnovel investigation into the unified view of SFT and RL through an off-policy\nversus on-policy lens. We propose CHORD, a framework for the Controllable\nHarmonization of On- and Off-Policy Reinforcement Learning via Dynamic\nWeighting, which reframes SFT not as a separate stage but as a dynamically\nweighted auxiliary objective within the on-policy RL process. Based on an\nanalysis of off-policy expert data's influence at both holistic and granular\nlevels, we incorporate a dual-control mechanism in CHORD. Specifically, the\nframework first employs a global coefficient to holistically guide the\ntransition from off-policy imitation to on-policy exploration, and then applies\na token-wise weighting function that enables granular learning from expert\ntokens, which preserves on-policy exploration and mitigates disruption from\noff-policy data. We conduct extensive experiments on widely used benchmarks,\nproviding empirical evidence that CHORD achieves a stable and efficient\nlearning process. By effectively harmonizing off-policy expert data with\non-policy exploration, CHORD demonstrates significant improvements over\nbaselines. We release the implementation at\nhttps://github.com/modelscope/Trinity-RFT/tree/main/examples/mix_chord to\ninspire further research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.11408.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "63f46a0fa096536aeab6ee75",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63f46a0fa096536aeab6ee75/Bte71XHp05z_vXvt8POev.png",
            "fullname": "garyzhang",
            "name": "xiaoniqiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13680",
            "authors": [
                {
                    "_id": "68a5830038160bfd39426a76",
                    "user": {
                        "_id": "67a87266e113700e5ee1c3fc",
                        "avatarUrl": "/avatars/ea21f5d239244b810c892333663b5390.svg",
                        "isPro": false,
                        "fullname": "Tng Vy",
                        "user": "tuongvy2603",
                        "type": "user"
                    },
                    "name": "Vy Tuong Dang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T10:30:47.794Z",
                    "hidden": false
                },
                {
                    "_id": "68a5830038160bfd39426a77",
                    "user": {
                        "_id": "6631fd5961a4305e5610d403",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6631fd5961a4305e5610d403/P1Dtxzn-KIbYDDsiw60nr.jpeg",
                        "isPro": false,
                        "fullname": "An Vo",
                        "user": "anvo25",
                        "type": "user"
                    },
                    "name": "An Vo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T09:16:18.691Z",
                    "hidden": false
                },
                {
                    "_id": "68a5830038160bfd39426a78",
                    "name": "Quang Tau",
                    "hidden": false
                },
                {
                    "_id": "68a5830038160bfd39426a79",
                    "name": "Duc Dm",
                    "hidden": false
                },
                {
                    "_id": "68a5830038160bfd39426a7a",
                    "name": "Daeyoung Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T09:31:18.000Z",
            "submittedOnDailyAt": "2025-08-21T07:20:42.509Z",
            "title": "ViExam: Are Vision Language Models Better than Humans on Vietnamese\n  Multimodal Exam Questions?",
            "submittedOnDailyBy": {
                "_id": "6631fd5961a4305e5610d403",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6631fd5961a4305e5610d403/P1Dtxzn-KIbYDDsiw60nr.jpeg",
                "isPro": false,
                "fullname": "An Vo",
                "user": "anvo25",
                "type": "user"
            },
            "summary": "Vision language models (VLMs) demonstrate remarkable capabilities on English\nmultimodal tasks, but their performance on low-resource languages with\ngenuinely multimodal educational content remains largely unexplored. In this\nwork, we test how VLMs perform on Vietnamese educational assessments,\ninvestigating whether VLMs trained predominantly on English data can handle\nreal-world cross-lingual multimodal reasoning. Our work presents the first\ncomprehensive evaluation of VLM capabilities on multimodal Vietnamese exams\nthrough proposing ViExam, a benchmark containing 2,548 multimodal questions. We\nfind that state-of-the-art VLMs achieve only 57.74% while open-source models\nachieve 27.70% mean accuracy across 7 academic domains, including Mathematics,\nPhysics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs\nunderperform average human test-takers (66.54%), with only the thinking VLM o3\n(74.07%) exceeding human average performance, yet still falling substantially\nshort of human best performance (99.60%). Cross-lingual prompting with English\ninstructions while maintaining Vietnamese content fails to improve performance,\ndecreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop\ncollaboration can partially improve VLM performance by 5 percentage points.\nCode and data are available at: https://vi-exam.github.io.",
            "upvotes": 3,
            "discussionId": "68a5831738160bfd39426a7b",
            "projectPage": "https://vi-exam.github.io",
            "githubRepo": "https://github.com/vytuongdang/ViExam",
            "ai_summary": "VLMs perform poorly on Vietnamese educational assessments, with state-of-the-art models achieving lower accuracy than human test-takers, and cross-lingual prompting does not significantly improve performance.",
            "ai_keywords": [
                "vision language models",
                "VLMs",
                "Vietnamese educational assessments",
                "cross-lingual multimodal reasoning",
                "ViExam",
                "multimodal questions",
                "academic domains",
                "Mathematics",
                "Physics",
                "Chemistry",
                "Biology",
                "Geography",
                "Driving Test",
                "IQ Test",
                "human-in-the-loop collaboration"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-08-19T05:31:18.000Z",
        "title": "ViExam: Are Vision Language Models Better than Humans on Vietnamese\n  Multimodal Exam Questions?",
        "summary": "Vision language models (VLMs) demonstrate remarkable capabilities on English\nmultimodal tasks, but their performance on low-resource languages with\ngenuinely multimodal educational content remains largely unexplored. In this\nwork, we test how VLMs perform on Vietnamese educational assessments,\ninvestigating whether VLMs trained predominantly on English data can handle\nreal-world cross-lingual multimodal reasoning. Our work presents the first\ncomprehensive evaluation of VLM capabilities on multimodal Vietnamese exams\nthrough proposing ViExam, a benchmark containing 2,548 multimodal questions. We\nfind that state-of-the-art VLMs achieve only 57.74% while open-source models\nachieve 27.70% mean accuracy across 7 academic domains, including Mathematics,\nPhysics, Chemistry, Biology, Geography, Driving Test, and IQ Test. Most VLMs\nunderperform average human test-takers (66.54%), with only the thinking VLM o3\n(74.07%) exceeding human average performance, yet still falling substantially\nshort of human best performance (99.60%). Cross-lingual prompting with English\ninstructions while maintaining Vietnamese content fails to improve performance,\ndecreasing accuracy by 1 percentage point for SOTA VLMs. Human-in-the-loop\ncollaboration can partially improve VLM performance by 5 percentage points.\nCode and data are available at: https://vi-exam.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13680.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6631fd5961a4305e5610d403",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6631fd5961a4305e5610d403/P1Dtxzn-KIbYDDsiw60nr.jpeg",
            "fullname": "An Vo",
            "name": "anvo25",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.12594",
            "authors": [
                {
                    "_id": "68a4b57e6cf0bf898542eb76",
                    "user": {
                        "_id": "6729087b934501c4f242f768",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6729087b934501c4f242f768/u4KZEqXovyq3uZJDnK-aX.jpeg",
                        "isPro": false,
                        "fullname": "Vedant Puri",
                        "user": "vedantpuri",
                        "type": "user"
                    },
                    "name": "Vedant Puri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T08:52:19.021Z",
                    "hidden": false
                },
                {
                    "_id": "68a4b57e6cf0bf898542eb77",
                    "name": "Aditya Joglekar",
                    "hidden": false
                },
                {
                    "_id": "68a4b57e6cf0bf898542eb78",
                    "name": "Kevin Ferguson",
                    "hidden": false
                },
                {
                    "_id": "68a4b57e6cf0bf898542eb79",
                    "name": "Yu-hsuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68a4b57e6cf0bf898542eb7a",
                    "name": "Yongjie Jessica Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a4b57e6cf0bf898542eb7b",
                    "name": "Levent Burak Kara",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6729087b934501c4f242f768/_HyB9U8Sv7bMYyQRYTQ4i.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6729087b934501c4f242f768/7DM-ffLJgKNogobcoo1i9.png"
            ],
            "publishedAt": "2025-08-18T03:00:55.000Z",
            "submittedOnDailyAt": "2025-08-21T16:24:52.173Z",
            "title": "FLARE: Fast Low-rank Attention Routing Engine",
            "submittedOnDailyBy": {
                "_id": "6729087b934501c4f242f768",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6729087b934501c4f242f768/u4KZEqXovyq3uZJDnK-aX.jpeg",
                "isPro": false,
                "fullname": "Vedant Puri",
                "user": "vedantpuri",
                "type": "user"
            },
            "summary": "The quadratic complexity of self-attention limits its applicability and\nscalability on large unstructured meshes. We introduce Fast Low-rank Attention\nRouting Engine (FLARE), a linear complexity self-attention mechanism that\nroutes attention through fixed-length latent sequences. Each attention head\nperforms global communication among N tokens by projecting the input sequence\nonto a fixed length latent sequence of M ll N tokens using learnable query\ntokens. By routing attention through a bottleneck sequence, FLARE learns a\nlow-rank form of attention that can be applied at O(NM) cost. FLARE not only\nscales to unprecedented problem sizes, but also delivers superior accuracy\ncompared to state-of-the-art neural PDE surrogates across diverse benchmarks.\nWe also release a new additive manufacturing dataset to spur further research.\nOur code is available at https://github.com/vpuri3/FLARE.py.",
            "upvotes": 3,
            "discussionId": "68a4b57e6cf0bf898542eb7c",
            "ai_summary": "FLARE, a linear complexity self-attention mechanism, improves scalability and accuracy for large unstructured meshes and neural PDE surrogates.",
            "ai_keywords": [
                "self-attention",
                "Fast Low-rank Attention Routing Engine",
                "FLARE",
                "linear complexity",
                "fixed-length latent sequences",
                "attention heads",
                "global communication",
                "learnable query tokens",
                "low-rank form of attention",
                "neural PDE surrogates",
                "additive manufacturing dataset"
            ]
        },
        "publishedAt": "2025-08-17T23:00:55.000Z",
        "title": "FLARE: Fast Low-rank Attention Routing Engine",
        "summary": "The quadratic complexity of self-attention limits its applicability and\nscalability on large unstructured meshes. We introduce Fast Low-rank Attention\nRouting Engine (FLARE), a linear complexity self-attention mechanism that\nroutes attention through fixed-length latent sequences. Each attention head\nperforms global communication among N tokens by projecting the input sequence\nonto a fixed length latent sequence of M ll N tokens using learnable query\ntokens. By routing attention through a bottleneck sequence, FLARE learns a\nlow-rank form of attention that can be applied at O(NM) cost. FLARE not only\nscales to unprecedented problem sizes, but also delivers superior accuracy\ncompared to state-of-the-art neural PDE surrogates across diverse benchmarks.\nWe also release a new additive manufacturing dataset to spur further research.\nOur code is available at https://github.com/vpuri3/FLARE.py.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6729087b934501c4f242f768/_HyB9U8Sv7bMYyQRYTQ4i.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6729087b934501c4f242f768/7DM-ffLJgKNogobcoo1i9.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.12594.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6729087b934501c4f242f768",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6729087b934501c4f242f768/u4KZEqXovyq3uZJDnK-aX.jpeg",
            "fullname": "Vedant Puri",
            "name": "vedantpuri",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.15754",
            "authors": [
                {
                    "_id": "68a7c62a39413c456c05afbd",
                    "name": "Yufeng Zhao",
                    "hidden": false
                },
                {
                    "_id": "68a7c62a39413c456c05afbe",
                    "name": "Junnan Liu",
                    "hidden": false
                },
                {
                    "_id": "68a7c62a39413c456c05afbf",
                    "name": "Hongwei Liu",
                    "hidden": false
                },
                {
                    "_id": "68a7c62a39413c456c05afc0",
                    "name": "Dongsheng Zhu",
                    "hidden": false
                },
                {
                    "_id": "68a7c62a39413c456c05afc1",
                    "name": "Yuan Shen",
                    "hidden": false
                },
                {
                    "_id": "68a7c62a39413c456c05afc2",
                    "name": "Songyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68a7c62a39413c456c05afc3",
                    "name": "Kai Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-21T17:50:24.000Z",
            "submittedOnDailyAt": "2025-08-21T23:53:19.825Z",
            "title": "Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis",
            "submittedOnDailyBy": {
                "_id": "643d26979347842571bc9613",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
                "isPro": false,
                "fullname": "Junnan Liu",
                "user": "jnanliu",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have made significant strides in reasoning tasks\nthrough methods like chain-of-thought (CoT) reasoning. However, they often fall\nshort in tasks requiring precise computations. Tool-Integrated Reasoning (TIR)\nhas emerged as a solution by incorporating external tools into the reasoning\nprocess. Nevertheless, the generalization of TIR in improving the reasoning\nability of LLM is still unclear. Additionally, whether TIR has improved the\nmodel's reasoning behavior and helped the model think remains to be studied. We\nintroduce ReasonZoo, a comprehensive benchmark encompassing nine diverse\nreasoning categories, to evaluate the effectiveness of TIR across various\ndomains. Additionally, we propose two novel metrics, Performance-Aware Cost\n(PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning\nefficiency. Our empirical evaluation demonstrates that TIR-enabled models\nconsistently outperform their non-TIR counterparts in both mathematical and\nnon-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as\nevidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more\nstreamlined reasoning. These findings underscore the domain-general benefits of\nTIR and its potential to advance LLM capabilities in complex reasoning tasks.",
            "upvotes": 1,
            "discussionId": "68a7c62a39413c456c05afc4",
            "ai_summary": "Tool-Integrated Reasoning (TIR) improves the reasoning ability and efficiency of Large Language Models (LLMs) across diverse tasks, as demonstrated by ReasonZoo and novel evaluation metrics.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "chain-of-thought (CoT) reasoning",
                "Tool-Integrated Reasoning (TIR)",
                "ReasonZoo",
                "Performance-Aware Cost (PAC)",
                "Area Under the Performance-Cost Curve (AUC-PCC)"
            ]
        },
        "publishedAt": "2025-08-21T13:50:24.000Z",
        "title": "Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis",
        "summary": "Large Language Models (LLMs) have made significant strides in reasoning tasks\nthrough methods like chain-of-thought (CoT) reasoning. However, they often fall\nshort in tasks requiring precise computations. Tool-Integrated Reasoning (TIR)\nhas emerged as a solution by incorporating external tools into the reasoning\nprocess. Nevertheless, the generalization of TIR in improving the reasoning\nability of LLM is still unclear. Additionally, whether TIR has improved the\nmodel's reasoning behavior and helped the model think remains to be studied. We\nintroduce ReasonZoo, a comprehensive benchmark encompassing nine diverse\nreasoning categories, to evaluate the effectiveness of TIR across various\ndomains. Additionally, we propose two novel metrics, Performance-Aware Cost\n(PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning\nefficiency. Our empirical evaluation demonstrates that TIR-enabled models\nconsistently outperform their non-TIR counterparts in both mathematical and\nnon-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as\nevidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more\nstreamlined reasoning. These findings underscore the domain-general benefits of\nTIR and its potential to advance LLM capabilities in complex reasoning tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15754.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "643d26979347842571bc9613",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/3heFf7h3jbhhJWJ4JfGfh.jpeg",
            "fullname": "Junnan Liu",
            "name": "jnanliu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.14568",
            "authors": [
                {
                    "_id": "68a6ee339e4b49496aac6b65",
                    "user": {
                        "_id": "68a6ee2fae023c92c2721c78",
                        "avatarUrl": "/avatars/39e9ee844dd2a0009cce3f22dc2f9d56.svg",
                        "isPro": false,
                        "fullname": "Wout Legi",
                        "user": "woutLegiest",
                        "type": "user"
                    },
                    "name": "Wouter Legiest",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T10:30:45.863Z",
                    "hidden": false
                },
                {
                    "_id": "68a6ee339e4b49496aac6b66",
                    "name": "Jan-Pieter D'Anvers",
                    "hidden": false
                },
                {
                    "_id": "68a6ee339e4b49496aac6b67",
                    "name": "Bojan Spasic",
                    "hidden": false
                },
                {
                    "_id": "68a6ee339e4b49496aac6b68",
                    "name": "Nam-Luc Tran",
                    "hidden": false
                },
                {
                    "_id": "68a6ee339e4b49496aac6b69",
                    "name": "Ingrid Verbauwhede",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-20T09:40:06.000Z",
            "submittedOnDailyAt": "2025-08-21T10:32:37.009Z",
            "title": "Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single\n  Bootstrap per Cell",
            "submittedOnDailyBy": {
                "_id": "68a6ee2fae023c92c2721c78",
                "avatarUrl": "/avatars/39e9ee844dd2a0009cce3f22dc2f9d56.svg",
                "isPro": false,
                "fullname": "Wout Legi",
                "user": "woutLegiest",
                "type": "user"
            },
            "summary": "This paper presents a novel approach to calculating the Levenshtein (edit)\ndistance within the framework of Fully Homomorphic Encryption (FHE),\nspecifically targeting third-generation schemes like TFHE. Edit distance\ncomputations are essential in applications across finance and genomics, such as\nDNA sequence alignment. We introduce an optimised algorithm that significantly\nreduces the cost of edit distance calculations called Leuvenshtein. This\nalgorithm specifically reduces the number of programmable bootstraps (PBS)\nneeded per cell of the calculation, lowering it from approximately 94\noperations -- required by the conventional Wagner-Fisher algorithm -- to just\n1. Additionally, we propose an efficient method for performing equality checks\non characters, reducing ASCII character comparisons to only 2 PBS operations.\nFinally, we explore the potential for further performance improvements by\nutilising preprocessing when one of the input strings is unencrypted. Our\nLeuvenshtein achieves up to 278times faster performance compared to the best\navailable TFHE implementation and up to 39times faster than an optimised\nimplementation of the Wagner-Fisher algorithm. Moreover, when offline\npreprocessing is possible due to the presence of one unencrypted input on the\nserver side, an additional 3times speedup can be achieved.",
            "upvotes": 1,
            "discussionId": "68a6ee339e4b49496aac6b6a",
            "githubRepo": "https://github.com/KULeuven-COSIC/leuvenshtein",
            "ai_summary": "An optimized algorithm reduces the computational cost of edit distance calculations within Fully Homomorphic Encryption, achieving significant performance improvements over existing methods.",
            "ai_keywords": [
                "Fully Homomorphic Encryption",
                "TFHE",
                "Wagner-Fisher algorithm",
                "programmable bootstraps",
                "ASCII character comparisons"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-08-20T05:40:06.000Z",
        "title": "Leuvenshtein: Efficient FHE-based Edit Distance Computation with Single\n  Bootstrap per Cell",
        "summary": "This paper presents a novel approach to calculating the Levenshtein (edit)\ndistance within the framework of Fully Homomorphic Encryption (FHE),\nspecifically targeting third-generation schemes like TFHE. Edit distance\ncomputations are essential in applications across finance and genomics, such as\nDNA sequence alignment. We introduce an optimised algorithm that significantly\nreduces the cost of edit distance calculations called Leuvenshtein. This\nalgorithm specifically reduces the number of programmable bootstraps (PBS)\nneeded per cell of the calculation, lowering it from approximately 94\noperations -- required by the conventional Wagner-Fisher algorithm -- to just\n1. Additionally, we propose an efficient method for performing equality checks\non characters, reducing ASCII character comparisons to only 2 PBS operations.\nFinally, we explore the potential for further performance improvements by\nutilising preprocessing when one of the input strings is unencrypted. Our\nLeuvenshtein achieves up to 278times faster performance compared to the best\navailable TFHE implementation and up to 39times faster than an optimised\nimplementation of the Wagner-Fisher algorithm. Moreover, when offline\npreprocessing is possible due to the presence of one unencrypted input on the\nserver side, an additional 3times speedup can be achieved.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14568.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68a6ee2fae023c92c2721c78",
            "avatarUrl": "/avatars/39e9ee844dd2a0009cce3f22dc2f9d56.svg",
            "fullname": "Wout Legi",
            "name": "woutLegiest",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.14187",
            "authors": [
                {
                    "_id": "68a69d2b9e4b49496aac6a8a",
                    "user": {
                        "_id": "661e07e02a8496916011c08a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
                        "isPro": false,
                        "fullname": "Md Ashiqur Rahman",
                        "user": "ashiq24",
                        "type": "user"
                    },
                    "name": "Md Ashiqur Rahman",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T09:15:51.907Z",
                    "hidden": false
                },
                {
                    "_id": "68a69d2b9e4b49496aac6a8b",
                    "name": "Chiao-An Yang",
                    "hidden": false
                },
                {
                    "_id": "68a69d2b9e4b49496aac6a8c",
                    "name": "Michael N. Cheng",
                    "hidden": false
                },
                {
                    "_id": "68a69d2b9e4b49496aac6a8d",
                    "name": "Lim Jun Hao",
                    "hidden": false
                },
                {
                    "_id": "68a69d2b9e4b49496aac6a8e",
                    "name": "Jeremiah Jiang",
                    "hidden": false
                },
                {
                    "_id": "68a69d2b9e4b49496aac6a8f",
                    "name": "Teck-Yian Lim",
                    "hidden": false
                },
                {
                    "_id": "68a69d2b9e4b49496aac6a90",
                    "name": "Raymond A. Yeh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/_MhgNnRLFRElwm0SfWCZe.gif"
            ],
            "publishedAt": "2025-08-19T18:21:59.000Z",
            "submittedOnDailyAt": "2025-08-21T02:47:36.441Z",
            "title": "Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer",
            "submittedOnDailyBy": {
                "_id": "661e07e02a8496916011c08a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
                "isPro": false,
                "fullname": "Md Ashiqur Rahman",
                "user": "ashiq24",
                "type": "user"
            },
            "summary": "Scale variation is a fundamental challenge in computer vision. Objects of the\nsame class can have different sizes, and their perceived size is further\naffected by the distance from the camera. These variations are local to the\nobjects, i.e., different object sizes may change differently within the same\nimage. To effectively handle scale variations, we present a deep equilibrium\ncanonicalizer (DEC) to improve the local scale equivariance of a model. DEC can\nbe easily incorporated into existing network architectures and can be adapted\nto a pre-trained model. Notably, we show that on the competitive ImageNet\nbenchmark, DEC improves both model performance and local scale consistency\nacross four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our\ncode is available at https://github.com/ashiq24/local-scale-equivariance.",
            "upvotes": 1,
            "discussionId": "68a69d2b9e4b49496aac6a91",
            "projectPage": "https://ashiq24.github.io/local-scale-equivariance/",
            "githubRepo": "https://github.com/ashiq24/local-scale-equivariance",
            "ai_summary": "A deep equilibrium canonicalizer (DEC) enhances local scale equivariance in deep networks, improving performance and consistency on ImageNet.",
            "ai_keywords": [
                "deep equilibrium canonicalizer",
                "DEC",
                "local scale equivariance",
                "ImageNet",
                "ViT",
                "DeiT",
                "Swin",
                "BEiT"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-08-19T14:21:59.000Z",
        "title": "Local Scale Equivariance with Latent Deep Equilibrium Canonicalizer",
        "summary": "Scale variation is a fundamental challenge in computer vision. Objects of the\nsame class can have different sizes, and their perceived size is further\naffected by the distance from the camera. These variations are local to the\nobjects, i.e., different object sizes may change differently within the same\nimage. To effectively handle scale variations, we present a deep equilibrium\ncanonicalizer (DEC) to improve the local scale equivariance of a model. DEC can\nbe easily incorporated into existing network architectures and can be adapted\nto a pre-trained model. Notably, we show that on the competitive ImageNet\nbenchmark, DEC improves both model performance and local scale consistency\nacross four popular pre-trained deep-nets, e.g., ViT, DeiT, Swin, and BEiT. Our\ncode is available at https://github.com/ashiq24/local-scale-equivariance.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/661e07e02a8496916011c08a/_MhgNnRLFRElwm0SfWCZe.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14187.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "661e07e02a8496916011c08a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
            "fullname": "Md Ashiqur Rahman",
            "name": "ashiq24",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.10137",
            "authors": [
                {
                    "_id": "68a6b4809e4b49496aac6aca",
                    "user": {
                        "_id": "658a8208c4b2004663d82daf",
                        "avatarUrl": "/avatars/af6bcee06aec82602c2b931f79c008e7.svg",
                        "isPro": false,
                        "fullname": "Nghia Trung Ngo",
                        "user": "ntnghia1811",
                        "type": "user"
                    },
                    "name": "Nghia Trung Ngo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:59:34.771Z",
                    "hidden": false
                },
                {
                    "_id": "68a6b4809e4b49496aac6acb",
                    "user": {
                        "_id": "62c5947524171688a9feb992",
                        "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                        "isPro": false,
                        "fullname": "Franck Dernoncourt",
                        "user": "Franck-Dernoncourt",
                        "type": "user"
                    },
                    "name": "Franck Dernoncourt",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-21T09:15:35.546Z",
                    "hidden": false
                },
                {
                    "_id": "68a6b4809e4b49496aac6acc",
                    "user": {
                        "_id": "64804fad8c6a3b8f11f73912",
                        "avatarUrl": "/avatars/61e37a91d4bba35fda9bf52aadd87745.svg",
                        "isPro": false,
                        "fullname": "Thien Huu Nguyen",
                        "user": "anoperson",
                        "type": "user"
                    },
                    "name": "Thien Huu Nguyen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:59:41.189Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-13T18:59:02.000Z",
            "submittedOnDailyAt": "2025-08-21T04:24:27.752Z",
            "title": "mSCoRe: a Multilingual and Scalable Benchmark for Skill-based\n  Commonsense Reasoning",
            "submittedOnDailyBy": {
                "_id": "62c5947524171688a9feb992",
                "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
                "isPro": false,
                "fullname": "Franck Dernoncourt",
                "user": "Franck-Dernoncourt",
                "type": "user"
            },
            "summary": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have\nshown remarkable capabilities in complex reasoning tasks. However, the\nmechanism underlying their utilization of different human reasoning skills\nremains poorly investigated, especially for multilingual commonsense reasoning\nthat involves everyday knowledge across different languages and cultures. To\naddress this gap, we propose a Multilingual and Scalable Benchmark for\nSkill-based Commonsense Reasoning (mSCoRe).\nOur benchmark incorporates three key components that are designed to\nsystematically evaluate LLM's reasoning capabilities, including: (1) a novel\ntaxonomy of reasoning skills that enables fine-grained analysis of models'\nreasoning processes, (2) a robust data synthesis pipeline tailored specifically\nfor commonsense reasoning evaluation, and (3) a complexity scaling framework\nallowing task difficulty to scale dynamically alongside future improvements in\nLLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying\nsizes and training approaches demonstrate that mSCoRe remains\nsignificantly challenging for current models, particularly at higher complexity\nlevels. Our results reveal the limitations of such reasoning-reinforced models\nwhen confronted with nuanced multilingual general and cultural commonsense. We\nfurther provide detailed analysis on the models' reasoning processes,\nsuggesting future directions for improving multilingual commonsense reasoning\ncapabilities.",
            "upvotes": 1,
            "discussionId": "68a6b4889e4b49496aac6acd",
            "ai_summary": "A multilingual benchmark evaluates the reasoning skills of large language models across different languages and cultures, revealing their limitations in nuanced commonsense understanding.",
            "ai_keywords": [
                "Large Language Models",
                "multilingual commonsense reasoning",
                "reasoning skills",
                "data synthesis pipeline",
                "complexity scaling framework"
            ]
        },
        "publishedAt": "2025-08-13T14:59:02.000Z",
        "title": "mSCoRe: a Multilingual and Scalable Benchmark for Skill-based\n  Commonsense Reasoning",
        "summary": "Recent advancements in reasoning-reinforced Large Language Models (LLMs) have\nshown remarkable capabilities in complex reasoning tasks. However, the\nmechanism underlying their utilization of different human reasoning skills\nremains poorly investigated, especially for multilingual commonsense reasoning\nthat involves everyday knowledge across different languages and cultures. To\naddress this gap, we propose a Multilingual and Scalable Benchmark for\nSkill-based Commonsense Reasoning (mSCoRe).\nOur benchmark incorporates three key components that are designed to\nsystematically evaluate LLM's reasoning capabilities, including: (1) a novel\ntaxonomy of reasoning skills that enables fine-grained analysis of models'\nreasoning processes, (2) a robust data synthesis pipeline tailored specifically\nfor commonsense reasoning evaluation, and (3) a complexity scaling framework\nallowing task difficulty to scale dynamically alongside future improvements in\nLLM abilities. Extensive experiments on eights state-of-the-art LLMs of varying\nsizes and training approaches demonstrate that mSCoRe remains\nsignificantly challenging for current models, particularly at higher complexity\nlevels. Our results reveal the limitations of such reasoning-reinforced models\nwhen confronted with nuanced multilingual general and cultural commonsense. We\nfurther provide detailed analysis on the models' reasoning processes,\nsuggesting future directions for improving multilingual commonsense reasoning\ncapabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10137.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62c5947524171688a9feb992",
            "avatarUrl": "/avatars/5a151713b9eae8dc566f5957acee3475.svg",
            "fullname": "Franck Dernoncourt",
            "name": "Franck-Dernoncourt",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.13745",
            "authors": [
                {
                    "_id": "68a5a213c4b3ea17d2d2cf47",
                    "user": {
                        "_id": "68a59cc87de3c48922820701",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RXoYPK8yALphay3LFXvy-.png",
                        "isPro": false,
                        "fullname": "ShouxingMa",
                        "user": "MrShouxingMa",
                        "type": "user"
                    },
                    "name": "Shouxing Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-20T19:13:28.696Z",
                    "hidden": false
                },
                {
                    "_id": "68a5a213c4b3ea17d2d2cf48",
                    "name": "Yawen Zeng",
                    "hidden": false
                },
                {
                    "_id": "68a5a213c4b3ea17d2d2cf49",
                    "name": "Shiqing Wu",
                    "hidden": false
                },
                {
                    "_id": "68a5a213c4b3ea17d2d2cf4a",
                    "user": {
                        "_id": "682ad4f2d7bad2f7d4b528f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/wI2ZMU1EBmkiM-EjVlM2q.png",
                        "isPro": false,
                        "fullname": "dongxu",
                        "user": "guandongxu",
                        "type": "user"
                    },
                    "name": "Guandong Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-21T10:59:52.343Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T11:35:48.000Z",
            "submittedOnDailyAt": "2025-08-21T05:22:05.445Z",
            "title": "Refining Contrastive Learning and Homography Relations for Multi-Modal\n  Recommendation",
            "submittedOnDailyBy": {
                "_id": "68a59cc87de3c48922820701",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RXoYPK8yALphay3LFXvy-.png",
                "isPro": false,
                "fullname": "ShouxingMa",
                "user": "MrShouxingMa",
                "type": "user"
            },
            "summary": "Multi-modal recommender system focuses on utilizing rich modal information (\ni.e., images and textual descriptions) of items to improve recommendation\nperformance. The current methods have achieved remarkable success with the\npowerful structure modeling capability of graph neural networks. However, these\nmethods are often hindered by sparse data in real-world scenarios. Although\ncontrastive learning and homography ( i.e., homogeneous graphs) are employed to\naddress the data sparsity challenge, existing methods still suffer two main\nlimitations: 1) Simple multi-modal feature contrasts fail to produce effective\nrepresentations, causing noisy modal-shared features and loss of valuable\ninformation in modal-unique features; 2) The lack of exploration of the\nhomograph relations between user interests and item co-occurrence results in\nincomplete mining of user-item interplay.\n  To address the above limitations, we propose a novel framework for\nREfining multi-modAl contRastive learning\nand hoMography relations (REARM). Specifically, we complement\nmulti-modal contrastive learning by employing meta-network and orthogonal\nconstraint strategies, which filter out noise in modal-shared features and\nretain recommendation-relevant information in modal-unique features. To mine\nhomogeneous relationships effectively, we integrate a newly constructed user\ninterest graph and an item co-occurrence graph with the existing user\nco-occurrence and item semantic graphs for graph learning. The extensive\nexperiments on three real-world datasets demonstrate the superiority of REARM\nto various state-of-the-art baselines. Our visualization further shows an\nimprovement made by REARM in distinguishing between modal-shared and\nmodal-unique features. Code is available\nhttps://github.com/MrShouxingMa/REARM{here}.",
            "upvotes": 0,
            "discussionId": "68a5a213c4b3ea17d2d2cf4b",
            "ai_summary": "A novel framework, REARM, enhances multi-modal recommender systems by refining contrastive learning and homography relations, improving feature representation and user-item interaction mining.",
            "ai_keywords": [
                "graph neural networks",
                "contrastive learning",
                "homography",
                "meta-network",
                "orthogonal constraint",
                "user interest graph",
                "item co-occurrence graph",
                "graph learning"
            ]
        },
        "publishedAt": "2025-08-19T07:35:48.000Z",
        "title": "Refining Contrastive Learning and Homography Relations for Multi-Modal\n  Recommendation",
        "summary": "Multi-modal recommender system focuses on utilizing rich modal information (\ni.e., images and textual descriptions) of items to improve recommendation\nperformance. The current methods have achieved remarkable success with the\npowerful structure modeling capability of graph neural networks. However, these\nmethods are often hindered by sparse data in real-world scenarios. Although\ncontrastive learning and homography ( i.e., homogeneous graphs) are employed to\naddress the data sparsity challenge, existing methods still suffer two main\nlimitations: 1) Simple multi-modal feature contrasts fail to produce effective\nrepresentations, causing noisy modal-shared features and loss of valuable\ninformation in modal-unique features; 2) The lack of exploration of the\nhomograph relations between user interests and item co-occurrence results in\nincomplete mining of user-item interplay.\n  To address the above limitations, we propose a novel framework for\nREfining multi-modAl contRastive learning\nand hoMography relations (REARM). Specifically, we complement\nmulti-modal contrastive learning by employing meta-network and orthogonal\nconstraint strategies, which filter out noise in modal-shared features and\nretain recommendation-relevant information in modal-unique features. To mine\nhomogeneous relationships effectively, we integrate a newly constructed user\ninterest graph and an item co-occurrence graph with the existing user\nco-occurrence and item semantic graphs for graph learning. The extensive\nexperiments on three real-world datasets demonstrate the superiority of REARM\nto various state-of-the-art baselines. Our visualization further shows an\nimprovement made by REARM in distinguishing between modal-shared and\nmodal-unique features. Code is available\nhttps://github.com/MrShouxingMa/REARM{here}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13745.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68a59cc87de3c48922820701",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RXoYPK8yALphay3LFXvy-.png",
            "fullname": "ShouxingMa",
            "name": "MrShouxingMa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]