[
    {
        "paper": {
            "id": "2508.16153",
            "authors": [
                {
                    "_id": "68abe23f86b21a0e2e358af8",
                    "user": {
                        "_id": "645d7f107c7258d904e82749",
                        "avatarUrl": "/avatars/a4e9d47b281f18616c522c1a8b8ee7e5.svg",
                        "isPro": false,
                        "fullname": "HuichiZhou",
                        "user": "Zhouhc",
                        "type": "user"
                    },
                    "name": "Huichi Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:29:38.720Z",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358af9",
                    "user": {
                        "_id": "66996ea912210698d6fb453b",
                        "avatarUrl": "/avatars/d898f7967d4d0785e0c7a1e94b7a237c.svg",
                        "isPro": false,
                        "fullname": "Yihang Chen",
                        "user": "scyyc9",
                        "type": "user"
                    },
                    "name": "Yihang Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T12:37:49.106Z",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358afa",
                    "name": "Siyuan Guo",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358afb",
                    "name": "Xue Yan",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358afc",
                    "name": "Kin Hei Lee",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358afd",
                    "name": "Zihan Wang",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358afe",
                    "name": "Ka Yiu Lee",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358aff",
                    "name": "Guchun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358b00",
                    "name": "Kun Shao",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358b01",
                    "user": {
                        "_id": "64895683f534abe18eec264b",
                        "avatarUrl": "/avatars/73cc9e6db6db86793787750776b57c63.svg",
                        "isPro": false,
                        "fullname": "Linyi Yang",
                        "user": "linyiyang2023",
                        "type": "user"
                    },
                    "name": "Linyi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:18:49.407Z",
                    "hidden": false
                },
                {
                    "_id": "68abe23f86b21a0e2e358b02",
                    "name": "Jun Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T07:25:30.000Z",
            "submittedOnDailyAt": "2025-08-25T02:40:51.791Z",
            "title": "AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "In this paper, we introduce a novel learning paradigm for adaptive Large\nLanguage Model (LLM) agents that eliminates the need for fine-tuning the\nunderlying LLMs. Existing approaches are often either rigid, relying on static,\nhandcrafted reflection workflows, or computationally intensive, requiring\ngradient updates of LLM model parameters. In contrast, our method enables\nlow-cost continual adaptation via memory-based online reinforcement learning.\nWe formalise this as a Memory-augmented Markov Decision Process (M-MDP),\nequipped with a neural case-selection policy to guide action decisions. Past\nexperiences are stored in an episodic memory, either differentiable or\nnon-parametric. The policy is continually updated based on environmental\nfeedback through a memory rewriting mechanism, whereas policy improvement is\nachieved through efficient memory reading (retrieval). We instantiate our agent\nmodel in the deep research setting, namely AgentFly, which attains top-1 on\nGAIA validation (87.88% Pass@3) and 79.40% on the test set. It reaches\n66.6% F1 and 80.4% PM on the DeepResearcher dataset, outperforming the\nstate-of-the-art training-based method, while case-based memory adds 4.7% to\n9.6% absolute points on out-of-distribution tasks. Our approach offers a\nscalable and efficient pathway for developing generalist LLM agents capable of\ncontinuous, real-time learning without gradient updates, advancing machine\nlearning towards open-ended skill acquisition and deep research scenarios. The\ncode is available at https://github.com/Agent-on-the-Fly/AgentFly.",
            "upvotes": 55,
            "discussionId": "68abe23f86b21a0e2e358b03",
            "githubRepo": "https://github.com/Agent-on-the-Fly/AgentFly",
            "ai_summary": "A novel memory-augmented reinforcement learning paradigm enables adaptive LLM agents to continually learn without fine-tuning, using episodic memory and a neural case-selection policy.",
            "ai_keywords": [
                "Large Language Model (LLM)",
                "memory-based online reinforcement learning",
                "Memory-augmented Markov Decision Process (M-MDP)",
                "neural case-selection policy",
                "episodic memory",
                "differentiable memory",
                "non-parametric memory",
                "memory rewriting mechanism",
                "memory reading",
                "AgentFly",
                "GAIA validation",
                "DeepResearcher dataset",
                "open-ended skill acquisition",
                "deep research scenarios"
            ],
            "githubStars": 143
        },
        "publishedAt": "2025-08-22T03:25:30.000Z",
        "title": "AgentFly: Fine-tuning LLM Agents without Fine-tuning LLMs",
        "summary": "In this paper, we introduce a novel learning paradigm for adaptive Large\nLanguage Model (LLM) agents that eliminates the need for fine-tuning the\nunderlying LLMs. Existing approaches are often either rigid, relying on static,\nhandcrafted reflection workflows, or computationally intensive, requiring\ngradient updates of LLM model parameters. In contrast, our method enables\nlow-cost continual adaptation via memory-based online reinforcement learning.\nWe formalise this as a Memory-augmented Markov Decision Process (M-MDP),\nequipped with a neural case-selection policy to guide action decisions. Past\nexperiences are stored in an episodic memory, either differentiable or\nnon-parametric. The policy is continually updated based on environmental\nfeedback through a memory rewriting mechanism, whereas policy improvement is\nachieved through efficient memory reading (retrieval). We instantiate our agent\nmodel in the deep research setting, namely AgentFly, which attains top-1 on\nGAIA validation (87.88% Pass@3) and 79.40% on the test set. It reaches\n66.6% F1 and 80.4% PM on the DeepResearcher dataset, outperforming the\nstate-of-the-art training-based method, while case-based memory adds 4.7% to\n9.6% absolute points on out-of-distribution tasks. Our approach offers a\nscalable and efficient pathway for developing generalist LLM agents capable of\ncontinuous, real-time learning without gradient updates, advancing machine\nlearning towards open-ended skill acquisition and deep research scenarios. The\ncode is available at https://github.com/Agent-on-the-Fly/AgentFly.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16153.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.08240",
            "authors": [
                {
                    "_id": "68ac10cd86b21a0e2e358b8a",
                    "user": {
                        "_id": "664497116ed556326833b214",
                        "avatarUrl": "/avatars/729db0cd64c77030a5d8b2749fce084b.svg",
                        "isPro": false,
                        "fullname": "Kaijun Wang",
                        "user": "Ka12un",
                        "type": "user"
                    },
                    "name": "Kaijun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:18:39.940Z",
                    "hidden": false
                },
                {
                    "_id": "68ac10cd86b21a0e2e358b8b",
                    "name": "Liqin Lu",
                    "hidden": false
                },
                {
                    "_id": "68ac10cd86b21a0e2e358b8c",
                    "user": {
                        "_id": "652e25d2e647b0ee0a024f26",
                        "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
                        "isPro": false,
                        "fullname": "Mingyu Liu",
                        "user": "MingyuLiu",
                        "type": "user"
                    },
                    "name": "Mingyu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T12:37:47.093Z",
                    "hidden": false
                },
                {
                    "_id": "68ac10cd86b21a0e2e358b8d",
                    "name": "Jianuo Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ac10cd86b21a0e2e358b8e",
                    "name": "Zeju Li",
                    "hidden": false
                },
                {
                    "_id": "68ac10cd86b21a0e2e358b8f",
                    "name": "Bolin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ac10cd86b21a0e2e358b90",
                    "name": "Wancai Zheng",
                    "hidden": false
                },
                {
                    "_id": "68ac10cd86b21a0e2e358b91",
                    "name": "Xinyi Yu",
                    "hidden": false
                },
                {
                    "_id": "68ac10cd86b21a0e2e358b92",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "68ac10cd86b21a0e2e358b93",
                    "name": "Chunhua Shen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T17:54:31.000Z",
            "submittedOnDailyAt": "2025-08-25T06:03:40.605Z",
            "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for\n  Long-Horizon Tasks",
            "submittedOnDailyBy": {
                "_id": "652e25d2e647b0ee0a024f26",
                "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
                "isPro": false,
                "fullname": "Mingyu Liu",
                "user": "MingyuLiu",
                "type": "user"
            },
            "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/",
            "upvotes": 34,
            "discussionId": "68ac10cd86b21a0e2e358b94",
            "projectPage": "https://kaijwang.github.io/odyssey.github.io/",
            "ai_summary": "ODYSSEY is a unified mobile manipulation framework for quadruped robots that integrates high-level task planning with low-level whole-body control, addressing challenges in egocentric perception, generalization, and coordination in unstructured environments.",
            "ai_keywords": [
                "hierarchical planner",
                "vision-language model",
                "long-horizon instruction decomposition",
                "whole-body policy",
                "sim-to-real transfer",
                "benchmark for long-horizon mobile manipulation"
            ]
        },
        "publishedAt": "2025-08-11T13:54:31.000Z",
        "title": "ODYSSEY: Open-World Quadrupeds Exploration and Manipulation for\n  Long-Horizon Tasks",
        "summary": "Language-guided long-horizon mobile manipulation has long been a grand\nchallenge in embodied semantic reasoning, generalizable manipulation, and\nadaptive locomotion. Three fundamental limitations hinder progress: First,\nalthough large language models have improved spatial reasoning and task\nplanning through semantic priors, existing implementations remain confined to\ntabletop scenarios, failing to address the constrained perception and limited\nactuation ranges of mobile platforms. Second, current manipulation strategies\nexhibit insufficient generalization when confronted with the diverse object\nconfigurations encountered in open-world environments. Third, while crucial for\npractical deployment, the dual requirement of maintaining high platform\nmaneuverability alongside precise end-effector control in unstructured settings\nremains understudied.\n  In this work, we present ODYSSEY, a unified mobile manipulation framework for\nagile quadruped robots equipped with manipulators, which seamlessly integrates\nhigh-level task planning with low-level whole-body control. To address the\nchallenge of egocentric perception in language-conditioned tasks, we introduce\na hierarchical planner powered by a vision-language model, enabling\nlong-horizon instruction decomposition and precise action execution. At the\ncontrol level, our novel whole-body policy achieves robust coordination across\nchallenging terrains. We further present the first benchmark for long-horizon\nmobile manipulation, evaluating diverse indoor and outdoor scenarios. Through\nsuccessful sim-to-real transfer, we demonstrate the system's generalization and\nrobustness in real-world deployments, underscoring the practicality of legged\nmanipulators in unstructured environments. Our work advances the feasibility of\ngeneralized robotic assistants capable of complex, dynamic tasks. Our project\npage: https://kaijwang.github.io/odyssey.github.io/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.08240.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "652e25d2e647b0ee0a024f26",
            "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
            "fullname": "Mingyu Liu",
            "name": "MingyuLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.14029",
            "authors": [
                {
                    "_id": "68a93fc286b21a0e2e358862",
                    "user": {
                        "_id": "6560763e152b659e623865ae",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
                        "isPro": false,
                        "fullname": "Xiao Liang",
                        "user": "MasterVito",
                        "type": "user"
                    },
                    "name": "Xiao Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:20:26.427Z",
                    "hidden": false
                },
                {
                    "_id": "68a93fc286b21a0e2e358863",
                    "name": "Zhongzhi Li",
                    "hidden": false
                },
                {
                    "_id": "68a93fc286b21a0e2e358864",
                    "name": "Yeyun Gong",
                    "hidden": false
                },
                {
                    "_id": "68a93fc286b21a0e2e358865",
                    "name": "Yelong Shen",
                    "hidden": false
                },
                {
                    "_id": "68a93fc286b21a0e2e358866",
                    "name": "Ying Nian Wu",
                    "hidden": false
                },
                {
                    "_id": "68a93fc286b21a0e2e358867",
                    "name": "Zhijiang Guo",
                    "hidden": false
                },
                {
                    "_id": "68a93fc286b21a0e2e358868",
                    "name": "Weizhu Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6560763e152b659e623865ae/N7Jg4_JHEXLgA-ObNb-WO.png"
            ],
            "publishedAt": "2025-08-19T17:42:45.000Z",
            "submittedOnDailyAt": "2025-08-25T03:49:31.546Z",
            "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains\n  RLVR",
            "submittedOnDailyBy": {
                "_id": "6560763e152b659e623865ae",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
                "isPro": false,
                "fullname": "Xiao Liang",
                "user": "MasterVito",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na key paradigm for post-training Large Language Models (LLMs), particularly for\ncomplex reasoning tasks. However, vanilla RLVR training has been shown to\nimprove Pass@1 performance at the expense of policy entropy, leading to reduced\ngeneration diversity and limiting the Pass@k performance, which typically\nrepresents the upper bound of LLM reasoning capability. In this paper, we\nsystematically analyze the policy's generation diversity from the perspective\nof training problems and find that augmenting and updating training problems\nhelps mitigate entropy collapse during training. Based on these observations,\nwe propose an online Self-play with Variational problem Synthesis (SvS)\nstrategy for RLVR training, which uses the policy's correct solutions to\nsynthesize variational problems while ensuring their reference answers remain\nidentical to the originals. This self-improving strategy effectively maintains\npolicy entropy during training and substantially improves Pass@k compared with\nstandard RLVR, sustaining prolonged improvements and achieving absolute gains\nof 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and\nAIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model\nsizes from 3B to 32B consistently demonstrate the generalizability and\nrobustness of SvS.",
            "upvotes": 30,
            "discussionId": "68a93fc286b21a0e2e358869",
            "projectPage": "https://mastervito.github.io/SvS.github.io/",
            "githubRepo": "https://github.com/MasterVito/SvS",
            "ai_summary": "An online self-play strategy with variational problem synthesis for RLVR training maintains policy entropy and improves Pass@k performance on reasoning benchmarks.",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards (RLVR)",
                "Large Language Models (LLMs)",
                "policy entropy",
                "generation diversity",
                "Pass@1",
                "Pass@k",
                "self-play",
                "variational problem synthesis",
                "AIME24",
                "AIME25"
            ],
            "githubStars": 16
        },
        "publishedAt": "2025-08-19T13:42:45.000Z",
        "title": "Beyond Pass@1: Self-Play with Variational Problem Synthesis Sustains\n  RLVR",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as\na key paradigm for post-training Large Language Models (LLMs), particularly for\ncomplex reasoning tasks. However, vanilla RLVR training has been shown to\nimprove Pass@1 performance at the expense of policy entropy, leading to reduced\ngeneration diversity and limiting the Pass@k performance, which typically\nrepresents the upper bound of LLM reasoning capability. In this paper, we\nsystematically analyze the policy's generation diversity from the perspective\nof training problems and find that augmenting and updating training problems\nhelps mitigate entropy collapse during training. Based on these observations,\nwe propose an online Self-play with Variational problem Synthesis (SvS)\nstrategy for RLVR training, which uses the policy's correct solutions to\nsynthesize variational problems while ensuring their reference answers remain\nidentical to the originals. This self-improving strategy effectively maintains\npolicy entropy during training and substantially improves Pass@k compared with\nstandard RLVR, sustaining prolonged improvements and achieving absolute gains\nof 18.3% and 22.8% in Pass@32 performance on the competition-level AIME24 and\nAIME25 benchmarks. Experiments on 12 reasoning benchmarks across varying model\nsizes from 3B to 32B consistently demonstrate the generalizability and\nrobustness of SvS.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6560763e152b659e623865ae/N7Jg4_JHEXLgA-ObNb-WO.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14029.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6560763e152b659e623865ae",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6560763e152b659e623865ae/cTT2jGnPU_8XMrUTvqZ2h.jpeg",
            "fullname": "Xiao Liang",
            "name": "MasterVito",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13013",
            "authors": [
                {
                    "_id": "68ac07b886b21a0e2e358b54",
                    "user": {
                        "_id": "6474d585bb0e9dd77647fd1a",
                        "avatarUrl": "/avatars/95cd50f164cb8b3e8885235fd78bc7e7.svg",
                        "isPro": false,
                        "fullname": "Qogir",
                        "user": "JingqiaoXiu",
                        "type": "user"
                    },
                    "name": "Jingqiao Xiu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-25T20:35:06.193Z",
                    "hidden": true
                },
                {
                    "_id": "68ac07b886b21a0e2e358b55",
                    "name": "Fangzhou Hong",
                    "hidden": false
                },
                {
                    "_id": "68ac07b886b21a0e2e358b56",
                    "name": "Yicong Li",
                    "hidden": false
                },
                {
                    "_id": "68ac07b886b21a0e2e358b57",
                    "name": "Mengze Li",
                    "hidden": false
                },
                {
                    "_id": "68ac07b886b21a0e2e358b58",
                    "name": "Wentao Wang",
                    "hidden": false
                },
                {
                    "_id": "68ac07b886b21a0e2e358b59",
                    "name": "Sirui Han",
                    "hidden": false
                },
                {
                    "_id": "68ac07b886b21a0e2e358b5a",
                    "name": "Liang Pan",
                    "hidden": false
                },
                {
                    "_id": "68ac07b886b21a0e2e358b5b",
                    "user": {
                        "_id": "62ab1ac1d48b4d8b048a3473",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                        "isPro": false,
                        "fullname": "Ziwei Liu",
                        "user": "liuziwei7",
                        "type": "user"
                    },
                    "name": "Ziwei Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-25T20:35:00.219Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62ab1ac1d48b4d8b048a3473/zGGBREKU8wyFcyqfFSVp9.mp4"
            ],
            "publishedAt": "2025-08-18T15:33:09.000Z",
            "submittedOnDailyAt": "2025-08-25T05:30:21.977Z",
            "title": "EgoTwin: Dreaming Body and View in First Person",
            "submittedOnDailyBy": {
                "_id": "62ab1ac1d48b4d8b048a3473",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
                "isPro": false,
                "fullname": "Ziwei Liu",
                "user": "liuziwei7",
                "type": "user"
            },
            "summary": "While exocentric video synthesis has achieved great progress, egocentric\nvideo generation remains largely underexplored, which requires modeling\nfirst-person view content along with camera motion patterns induced by the\nwearer's body movements. To bridge this gap, we introduce a novel task of joint\negocentric video and human motion generation, characterized by two key\nchallenges: 1) Viewpoint Alignment: the camera trajectory in the generated\nvideo must accurately align with the head trajectory derived from human motion;\n2) Causal Interplay: the synthesized human motion must causally align with the\nobserved visual dynamics across adjacent video frames. To address these\nchallenges, we propose EgoTwin, a joint video-motion generation framework built\non the diffusion transformer architecture. Specifically, EgoTwin introduces a\nhead-centric motion representation that anchors the human motion to the head\njoint and incorporates a cybernetics-inspired interaction mechanism that\nexplicitly captures the causal interplay between video and motion within\nattention operations. For comprehensive evaluation, we curate a large-scale\nreal-world dataset of synchronized text-video-motion triplets and design novel\nmetrics to assess video-motion consistency. Extensive experiments demonstrate\nthe effectiveness of the EgoTwin framework.",
            "upvotes": 15,
            "discussionId": "68ac07b986b21a0e2e358b5c",
            "ai_summary": "EgoTwin, a diffusion transformer framework, addresses viewpoint alignment and causal interplay in joint egocentric video and human motion generation using a head-centric motion representation and cybernetics-inspired interaction mechanism.",
            "ai_keywords": [
                "exocentric video synthesis",
                "egocentric video generation",
                "camera trajectory",
                "head trajectory",
                "human motion",
                "viewpoint alignment",
                "causal interplay",
                "diffusion transformer",
                "head-centric motion representation",
                "cybernetics-inspired interaction mechanism"
            ]
        },
        "publishedAt": "2025-08-18T11:33:09.000Z",
        "title": "EgoTwin: Dreaming Body and View in First Person",
        "summary": "While exocentric video synthesis has achieved great progress, egocentric\nvideo generation remains largely underexplored, which requires modeling\nfirst-person view content along with camera motion patterns induced by the\nwearer's body movements. To bridge this gap, we introduce a novel task of joint\negocentric video and human motion generation, characterized by two key\nchallenges: 1) Viewpoint Alignment: the camera trajectory in the generated\nvideo must accurately align with the head trajectory derived from human motion;\n2) Causal Interplay: the synthesized human motion must causally align with the\nobserved visual dynamics across adjacent video frames. To address these\nchallenges, we propose EgoTwin, a joint video-motion generation framework built\non the diffusion transformer architecture. Specifically, EgoTwin introduces a\nhead-centric motion representation that anchors the human motion to the head\njoint and incorporates a cybernetics-inspired interaction mechanism that\nexplicitly captures the causal interplay between video and motion within\nattention operations. For comprehensive evaluation, we curate a large-scale\nreal-world dataset of synchronized text-video-motion triplets and design novel\nmetrics to assess video-motion consistency. Extensive experiments demonstrate\nthe effectiveness of the EgoTwin framework.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62ab1ac1d48b4d8b048a3473/zGGBREKU8wyFcyqfFSVp9.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13013.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62ab1ac1d48b4d8b048a3473",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656826685333-62ab1ac1d48b4d8b048a3473.png",
            "fullname": "Ziwei Liu",
            "name": "liuziwei7",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 47
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13650",
            "authors": [
                {
                    "_id": "68a6fc7d9e4b49496aac6b8c",
                    "user": {
                        "_id": "638dbf006b5c2ccc6240d6fc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638dbf006b5c2ccc6240d6fc/7jZlE4S35f15PLSN3_bVD.jpeg",
                        "isPro": false,
                        "fullname": "Tomer Ashuach",
                        "user": "Tomertech",
                        "type": "user"
                    },
                    "name": "Tomer Ashuach",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:27:35.955Z",
                    "hidden": false
                },
                {
                    "_id": "68a6fc7d9e4b49496aac6b8d",
                    "name": "Dana Arad",
                    "hidden": false
                },
                {
                    "_id": "68a6fc7d9e4b49496aac6b8e",
                    "name": "Aaron Mueller",
                    "hidden": false
                },
                {
                    "_id": "68a6fc7d9e4b49496aac6b8f",
                    "name": "Martin Tutek",
                    "hidden": false
                },
                {
                    "_id": "68a6fc7d9e4b49496aac6b90",
                    "name": "Yonatan Belinkov",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/3OA3H9GFY6pp8pUSR7xDw.png",
                "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/MMdJ5csxXnZv2oBebcFaH.png",
                "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/bDb3zdZ5G3MGxXlPMlDun.png",
                "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/BxcOXuE5LQALsW4grj77I.png",
                "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/U0RaUICyGlw-BdBHVO3TK.png"
            ],
            "publishedAt": "2025-08-19T09:01:22.000Z",
            "submittedOnDailyAt": "2025-08-25T04:21:32.618Z",
            "title": "CRISP: Persistent Concept Unlearning via Sparse Autoencoders",
            "submittedOnDailyBy": {
                "_id": "638dbf006b5c2ccc6240d6fc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638dbf006b5c2ccc6240d6fc/7jZlE4S35f15PLSN3_bVD.jpeg",
                "isPro": false,
                "fullname": "Tomer Ashuach",
                "user": "Tomertech",
                "type": "user"
            },
            "summary": "As large language models (LLMs) are increasingly deployed in real-world\napplications, the need to selectively remove unwanted knowledge while\npreserving model utility has become paramount. Recent work has explored sparse\nautoencoders (SAEs) to perform precise interventions on monosemantic features.\nHowever, most SAE-based methods operate at inference time, which does not\ncreate persistent changes in the model's parameters. Such interventions can be\nbypassed or reversed by malicious actors with parameter access. We introduce\nCRISP, a parameter-efficient method for persistent concept unlearning using\nSAEs. CRISP automatically identifies salient SAE features across multiple\nlayers and suppresses their activations. We experiment with two LLMs and show\nthat our method outperforms prior approaches on safety-critical unlearning\ntasks from the WMDP benchmark, successfully removing harmful knowledge while\npreserving general and in-domain capabilities. Feature-level analysis reveals\nthat CRISP achieves semantically coherent separation between target and benign\nconcepts, allowing precise suppression of the target features.",
            "upvotes": 11,
            "discussionId": "68a6fc7d9e4b49496aac6b91",
            "ai_summary": "CRISP is a parameter-efficient method using sparse autoencoders to permanently remove unwanted knowledge from large language models while preserving their utility.",
            "ai_keywords": [
                "sparse autoencoders",
                "parameter-efficient",
                "concept unlearning",
                "salient features",
                "feature-level analysis",
                "WMDP benchmark",
                "semantically coherent separation"
            ]
        },
        "publishedAt": "2025-08-19T05:01:22.000Z",
        "title": "CRISP: Persistent Concept Unlearning via Sparse Autoencoders",
        "summary": "As large language models (LLMs) are increasingly deployed in real-world\napplications, the need to selectively remove unwanted knowledge while\npreserving model utility has become paramount. Recent work has explored sparse\nautoencoders (SAEs) to perform precise interventions on monosemantic features.\nHowever, most SAE-based methods operate at inference time, which does not\ncreate persistent changes in the model's parameters. Such interventions can be\nbypassed or reversed by malicious actors with parameter access. We introduce\nCRISP, a parameter-efficient method for persistent concept unlearning using\nSAEs. CRISP automatically identifies salient SAE features across multiple\nlayers and suppresses their activations. We experiment with two LLMs and show\nthat our method outperforms prior approaches on safety-critical unlearning\ntasks from the WMDP benchmark, successfully removing harmful knowledge while\npreserving general and in-domain capabilities. Feature-level analysis reveals\nthat CRISP achieves semantically coherent separation between target and benign\nconcepts, allowing precise suppression of the target features.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/3OA3H9GFY6pp8pUSR7xDw.png",
            "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/MMdJ5csxXnZv2oBebcFaH.png",
            "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/bDb3zdZ5G3MGxXlPMlDun.png",
            "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/BxcOXuE5LQALsW4grj77I.png",
            "https://cdn-uploads.huggingface.co/production/uploads/638dbf006b5c2ccc6240d6fc/U0RaUICyGlw-BdBHVO3TK.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13650.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638dbf006b5c2ccc6240d6fc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638dbf006b5c2ccc6240d6fc/7jZlE4S35f15PLSN3_bVD.jpeg",
            "fullname": "Tomer Ashuach",
            "name": "Tomertech",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.07877",
            "authors": [
                {
                    "_id": "689c9309fab6fdd2e52ac9fe",
                    "user": {
                        "_id": "637b9711086af1cb122e99b3",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ZyMIY8L2AmE7tE07A7gS9.png",
                        "isPro": false,
                        "fullname": "WonJun Moon",
                        "user": "WJ0830",
                        "type": "user"
                    },
                    "name": "WonJun Moon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:27:59.549Z",
                    "hidden": false
                },
                {
                    "_id": "689c9309fab6fdd2e52ac9ff",
                    "user": {
                        "_id": "689b448ed7a19791dd3c7912",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8GZpAYFVtAB9u0YSuRKl2.png",
                        "isPro": false,
                        "fullname": "Hyun Seok Seong",
                        "user": "hynnsk",
                        "type": "user"
                    },
                    "name": "Hyun Seok Seong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-14T13:38:24.925Z",
                    "hidden": false
                },
                {
                    "_id": "689c9309fab6fdd2e52aca00",
                    "name": "Jae-Pil Heo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-11T11:49:37.000Z",
            "submittedOnDailyAt": "2025-08-25T04:46:33.566Z",
            "title": "Selective Contrastive Learning for Weakly Supervised Affordance\n  Grounding",
            "submittedOnDailyBy": {
                "_id": "689b448ed7a19791dd3c7912",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8GZpAYFVtAB9u0YSuRKl2.png",
                "isPro": false,
                "fullname": "Hyun Seok Seong",
                "user": "hynnsk",
                "type": "user"
            },
            "summary": "Facilitating an entity's interaction with objects requires accurately\nidentifying parts that afford specific actions. Weakly supervised affordance\ngrounding (WSAG) seeks to imitate human learning from third-person\ndemonstrations, where humans intuitively grasp functional parts without needing\npixel-level annotations. To achieve this, grounding is typically learned using\na shared classifier across images from different perspectives, along with\ndistillation strategies incorporating part discovery process. However, since\naffordance-relevant parts are not always easily distinguishable, models\nprimarily rely on classification, often focusing on common class-specific\npatterns that are unrelated to affordance. To address this limitation, we move\nbeyond isolated part-level learning by introducing selective prototypical and\npixel contrastive objectives that adaptively learn affordance-relevant cues at\nboth the part and object levels, depending on the granularity of the available\ninformation. Initially, we find the action-associated objects in both\negocentric (object-focused) and exocentric (third-person example) images by\nleveraging CLIP. Then, by cross-referencing the discovered objects of\ncomplementary views, we excavate the precise part-level affordance clues in\neach perspective. By consistently learning to distinguish affordance-relevant\nregions from affordance-irrelevant background context, our approach effectively\nshifts activation from irrelevant areas toward meaningful affordance cues.\nExperimental results demonstrate the effectiveness of our method. Codes are\navailable at github.com/hynnsk/SelectiveCL.",
            "upvotes": 10,
            "discussionId": "689c9309fab6fdd2e52aca01",
            "githubRepo": "https://github.com/hynnsk/SelectiveCL",
            "ai_summary": "The method uses selective prototypical and pixel contrastive objectives to learn affordance-relevant cues from third-person demonstrations, improving upon traditional weakly supervised affordance grounding by focusing on both part and object levels.",
            "ai_keywords": [
                "weakly supervised affordance grounding",
                "WSAG",
                "CLIP",
                "prototypical objectives",
                "pixel contrastive objectives",
                "affordance-relevant cues",
                "affordance-irrelevant background context",
                "egocentric images",
                "exocentric images"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-08-11T07:49:37.000Z",
        "title": "Selective Contrastive Learning for Weakly Supervised Affordance\n  Grounding",
        "summary": "Facilitating an entity's interaction with objects requires accurately\nidentifying parts that afford specific actions. Weakly supervised affordance\ngrounding (WSAG) seeks to imitate human learning from third-person\ndemonstrations, where humans intuitively grasp functional parts without needing\npixel-level annotations. To achieve this, grounding is typically learned using\na shared classifier across images from different perspectives, along with\ndistillation strategies incorporating part discovery process. However, since\naffordance-relevant parts are not always easily distinguishable, models\nprimarily rely on classification, often focusing on common class-specific\npatterns that are unrelated to affordance. To address this limitation, we move\nbeyond isolated part-level learning by introducing selective prototypical and\npixel contrastive objectives that adaptively learn affordance-relevant cues at\nboth the part and object levels, depending on the granularity of the available\ninformation. Initially, we find the action-associated objects in both\negocentric (object-focused) and exocentric (third-person example) images by\nleveraging CLIP. Then, by cross-referencing the discovered objects of\ncomplementary views, we excavate the precise part-level affordance clues in\neach perspective. By consistently learning to distinguish affordance-relevant\nregions from affordance-irrelevant background context, our approach effectively\nshifts activation from irrelevant areas toward meaningful affordance cues.\nExperimental results demonstrate the effectiveness of our method. Codes are\navailable at github.com/hynnsk/SelectiveCL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.07877.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "689b448ed7a19791dd3c7912",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/8GZpAYFVtAB9u0YSuRKl2.png",
            "fullname": "Hyun Seok Seong",
            "name": "hynnsk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.16402",
            "authors": [
                {
                    "_id": "68abd69686b21a0e2e358a9a",
                    "user": {
                        "_id": "68345ea8beb0d467e37cd421",
                        "avatarUrl": "/avatars/70ddbf00db1c517d61af3a3d283edf42.svg",
                        "isPro": false,
                        "fullname": "Zihan Wang",
                        "user": "zhwang01",
                        "type": "user"
                    },
                    "name": "Zihan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:18:53.636Z",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358a9b",
                    "name": "Jiaze Chen",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358a9c",
                    "name": "Zhicheng Liu",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358a9d",
                    "name": "Markus Mak",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358a9e",
                    "name": "Yidi Du",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358a9f",
                    "name": "Geonsik Moon",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa0",
                    "name": "Luoqi Xu",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa1",
                    "name": "Aaron Tua",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa2",
                    "name": "Kunshuo Peng",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa3",
                    "name": "Jiayi Lu",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa4",
                    "name": "Mingfei Xia",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa5",
                    "name": "Boqian Zou",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa6",
                    "name": "Chenyang Ran",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa7",
                    "name": "Guang Tian",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa8",
                    "name": "Shoutai Zhu",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aa9",
                    "name": "Yeheng Duan",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aaa",
                    "name": "Zhenghui Kang",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aab",
                    "name": "Zhenxing Lin",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aac",
                    "name": "Shangshu Li",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aad",
                    "name": "Qiang Luo",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aae",
                    "name": "Qingshen Long",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358aaf",
                    "name": "Zhiyong Chen",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358ab0",
                    "name": "Yihan Xiao",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358ab1",
                    "name": "Yurong Wu",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358ab2",
                    "name": "Daoguang Zan",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358ab3",
                    "name": "Yuyi Fu",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358ab4",
                    "name": "Mingxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68abd69686b21a0e2e358ab5",
                    "name": "Ming Ding",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T14:04:55.000Z",
            "submittedOnDailyAt": "2025-08-25T01:51:27.058Z",
            "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  Competitions",
            "submittedOnDailyBy": {
                "_id": "68345ea8beb0d467e37cd421",
                "avatarUrl": "/avatars/70ddbf00db1c517d61af3a3d283edf42.svg",
                "isPro": false,
                "fullname": "Zihan Wang",
                "user": "zhwang01",
                "type": "user"
            },
            "summary": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning.",
            "upvotes": 9,
            "discussionId": "68abd69686b21a0e2e358ab6",
            "ai_summary": "AetherCode is a new benchmark for evaluating Large Language Models in competitive programming, offering more challenging and expert-validated test cases than existing benchmarks.",
            "ai_keywords": [
                "Large Language Models",
                "competitive programming",
                "benchmark",
                "reasoning",
                "coding capabilities",
                "evaluation bias",
                "IOI",
                "ICPC",
                "test suites",
                "automated generation",
                "human curation",
                "code reasoning"
            ]
        },
        "publishedAt": "2025-08-22T10:04:55.000Z",
        "title": "AetherCode: Evaluating LLMs' Ability to Win In Premier Programming\n  Competitions",
        "summary": "Competitive programming has emerged as a critical benchmark for evaluating\nthe reasoning and coding capabilities of Large Language Models (LLMs). Despite\nimpressive progress on existing benchmarks, we argue that current evaluations\noverstate model proficiency, masking a substantial gap between LLMs and elite\nhuman programmers. This gap arises from two key limitations: insufficient\ndifficulty and scope of benchmark problems, and evaluation bias from\nlow-quality test cases. To address these shortcomings, we present AetherCode, a\nnew benchmark that draws problems from premier programming competitions such as\nIOI and ICPC, offering broader coverage and higher difficulty. AetherCode\nfurther incorporates comprehensive, expert-validated test suites built through\na hybrid of automated generation and human curation, ensuring rigorous and\nreliable assessment. By combining challenging problem design with robust\nevaluation, AetherCode provides a more faithful measure of LLM capabilities and\nsets a new standard for future research in code reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16402.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68345ea8beb0d467e37cd421",
            "avatarUrl": "/avatars/70ddbf00db1c517d61af3a3d283edf42.svg",
            "fullname": "Zihan Wang",
            "name": "zhwang01",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.15746",
            "authors": [
                {
                    "_id": "68abd6e286b21a0e2e358ac4",
                    "name": "Qiaoyu Zheng",
                    "hidden": false
                },
                {
                    "_id": "68abd6e286b21a0e2e358ac5",
                    "name": "Yuze Sun",
                    "hidden": false
                },
                {
                    "_id": "68abd6e286b21a0e2e358ac6",
                    "name": "Chaoyi Wu",
                    "hidden": false
                },
                {
                    "_id": "68abd6e286b21a0e2e358ac7",
                    "user": {
                        "_id": "64365addfae287005149dd24",
                        "avatarUrl": "/avatars/bd6d4512d66fd9fd7fd5476ea7a44b46.svg",
                        "isPro": false,
                        "fullname": "Weike Zhao",
                        "user": "Angelakeke",
                        "type": "user"
                    },
                    "name": "Weike Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:18:51.751Z",
                    "hidden": false
                },
                {
                    "_id": "68abd6e286b21a0e2e358ac8",
                    "name": "Pengcheng Qiu",
                    "hidden": false
                },
                {
                    "_id": "68abd6e286b21a0e2e358ac9",
                    "name": "Yongguo Yu",
                    "hidden": false
                },
                {
                    "_id": "68abd6e286b21a0e2e358aca",
                    "name": "Kun Sun",
                    "hidden": false
                },
                {
                    "_id": "68abd6e286b21a0e2e358acb",
                    "name": "Yanfeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68abd6e286b21a0e2e358acc",
                    "name": "Ya Zhang",
                    "hidden": false
                },
                {
                    "_id": "68abd6e286b21a0e2e358acd",
                    "name": "Weidi Xie",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-21T17:42:47.000Z",
            "submittedOnDailyAt": "2025-08-25T03:18:00.922Z",
            "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic\n  Reasoning",
            "submittedOnDailyBy": {
                "_id": "6436aaaa0c77d7c5036abdbd",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg",
                "isPro": false,
                "fullname": "Chaoyi Wu",
                "user": "chaoyi-wu",
                "type": "user"
            },
            "summary": "Accurate diagnosis with medical large language models is hindered by\nknowledge gaps and hallucinations. Retrieval and tool-augmented methods help,\nbut their impact is limited by weak use of external knowledge and poor\nfeedback-reasoning traceability. To address these challenges, We introduce\nDeep-DxSearch, an agentic RAG system trained end-to-end with reinforcement\nlearning (RL) that enables steer tracebale retrieval-augmented reasoning for\nmedical diagnosis. In Deep-DxSearch, we first construct a large-scale medical\nretrieval corpus comprising patient records and reliable medical knowledge\nsources to support retrieval-aware reasoning across diagnostic scenarios. More\ncrutially, we frame the LLM as the core agent and the retrieval corpus as its\nenvironment, using tailored rewards on format, retrieval, reasoning structure,\nand diagnostic accuracy, thereby evolving the agentic RAG policy from\nlarge-scale data through RL.\n  Experiments demonstrate that our end-to-end agentic RL training framework\nconsistently outperforms prompt-engineering and training-free RAG approaches\nacross multiple data centers. After training, Deep-DxSearch achieves\nsubstantial gains in diagnostic accuracy, surpassing strong diagnostic\nbaselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks\nfor both common and rare disease diagnosis under in-distribution and\nout-of-distribution settings. Moreover, ablation studies on reward design and\nretrieval corpus components confirm their critical roles, underscoring the\nuniqueness and effectiveness of our approach compared with traditional\nimplementations. Finally, case studies and interpretability analyses highlight\nimprovements in Deep-DxSearch's diagnostic policy, providing deeper insight\ninto its performance gains and supporting clinicians in delivering more\nreliable and precise preliminary diagnoses. See\nhttps://github.com/MAGIC-AI4Med/Deep-DxSearch.",
            "upvotes": 6,
            "discussionId": "68abd6e386b21a0e2e358ace",
            "projectPage": "https://qiaoyu-zheng.github.io/Deep-DxSearch",
            "githubRepo": "https://github.com/MAGIC-AI4Med/Deep-DxSearch",
            "ai_summary": "Deep-DxSearch, an agentic RAG system trained with reinforcement learning, enhances medical diagnosis accuracy by integrating a large-scale retrieval corpus and tailored rewards.",
            "ai_keywords": [
                "Deep-DxSearch",
                "RAG system",
                "reinforcement learning",
                "retrieval corpus",
                "retrieval-aware reasoning",
                "diagnostic accuracy",
                "prompt-engineering",
                "training-free RAG",
                "GPT-4o",
                "DeepSeek-R1",
                "in-distribution",
                "out-of-distribution",
                "reward design",
                "interpretability analyses"
            ],
            "githubStars": 24
        },
        "publishedAt": "2025-08-21T13:42:47.000Z",
        "title": "End-to-End Agentic RAG System Training for Traceable Diagnostic\n  Reasoning",
        "summary": "Accurate diagnosis with medical large language models is hindered by\nknowledge gaps and hallucinations. Retrieval and tool-augmented methods help,\nbut their impact is limited by weak use of external knowledge and poor\nfeedback-reasoning traceability. To address these challenges, We introduce\nDeep-DxSearch, an agentic RAG system trained end-to-end with reinforcement\nlearning (RL) that enables steer tracebale retrieval-augmented reasoning for\nmedical diagnosis. In Deep-DxSearch, we first construct a large-scale medical\nretrieval corpus comprising patient records and reliable medical knowledge\nsources to support retrieval-aware reasoning across diagnostic scenarios. More\ncrutially, we frame the LLM as the core agent and the retrieval corpus as its\nenvironment, using tailored rewards on format, retrieval, reasoning structure,\nand diagnostic accuracy, thereby evolving the agentic RAG policy from\nlarge-scale data through RL.\n  Experiments demonstrate that our end-to-end agentic RL training framework\nconsistently outperforms prompt-engineering and training-free RAG approaches\nacross multiple data centers. After training, Deep-DxSearch achieves\nsubstantial gains in diagnostic accuracy, surpassing strong diagnostic\nbaselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks\nfor both common and rare disease diagnosis under in-distribution and\nout-of-distribution settings. Moreover, ablation studies on reward design and\nretrieval corpus components confirm their critical roles, underscoring the\nuniqueness and effectiveness of our approach compared with traditional\nimplementations. Finally, case studies and interpretability analyses highlight\nimprovements in Deep-DxSearch's diagnostic policy, providing deeper insight\ninto its performance gains and supporting clinicians in delivering more\nreliable and precise preliminary diagnoses. See\nhttps://github.com/MAGIC-AI4Med/Deep-DxSearch.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15746.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6436aaaa0c77d7c5036abdbd",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6436aaaa0c77d7c5036abdbd/C9A276yEeAPkKLEqUcjl_.jpeg",
            "fullname": "Chaoyi Wu",
            "name": "chaoyi-wu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.16292",
            "authors": [
                {
                    "_id": "68abe15486b21a0e2e358ad7",
                    "user": {
                        "_id": "67aefef1bae0204268556790",
                        "avatarUrl": "/avatars/cc30835fca7855926360a00c012b88bc.svg",
                        "isPro": false,
                        "fullname": "Wen Han Hsieh",
                        "user": "wen-han",
                        "type": "user"
                    },
                    "name": "Wen-Han Hsieh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-25T20:33:50.102Z",
                    "hidden": false
                },
                {
                    "_id": "68abe15486b21a0e2e358ad8",
                    "name": "Elvis Hsieh",
                    "hidden": false
                },
                {
                    "_id": "68abe15486b21a0e2e358ad9",
                    "name": "Dantong Niu",
                    "hidden": false
                },
                {
                    "_id": "68abe15486b21a0e2e358ada",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "68abe15486b21a0e2e358adb",
                    "name": "Roei Herzig",
                    "hidden": false
                },
                {
                    "_id": "68abe15486b21a0e2e358adc",
                    "name": "David M. Chan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T10:54:33.000Z",
            "submittedOnDailyAt": "2025-08-25T02:36:58.231Z",
            "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recently, Vision-Language-Action (VLA) models have demonstrated strong\nperformance on a range of robotic tasks. These models rely on multimodal\ninputs, with language instructions playing a crucial role -- not only in\npredicting actions, but also in robustly interpreting user intent, even when\nthe requests are impossible to fulfill. In this work, we investigate how VLAs\ncan recognize, interpret, and respond to false-premise instructions: natural\nlanguage commands that reference objects or conditions absent from the\nenvironment. We propose Instruct-Verify-and-Act (IVA), a unified framework that\n(i) detects when an instruction cannot be executed due to a false premise, (ii)\nengages in language-based clarification or correction, and (iii) grounds\nplausible alternatives in perception and action. Towards this end, we construct\na large-scale instruction tuning setup with structured language prompts and\ntrain a VLA model capable of handling both accurate and erroneous requests. Our\napproach leverages a contextually augmented, semi-synthetic dataset containing\npaired positive and false-premise instructions, enabling robust detection and\nnatural language correction. Our experiments show that IVA improves false\npremise detection accuracy by 97.56% over baselines, while increasing\nsuccessful responses in false-premise scenarios by 50.78%.",
            "upvotes": 5,
            "discussionId": "68abe15586b21a0e2e358add",
            "ai_summary": "A unified framework, Instruct-Verify-and-Act (IVA), enhances Vision-Language-Action (VLA) models to detect and respond to false-premise instructions by leveraging contextually augmented datasets.",
            "ai_keywords": [
                "Vision-Language-Action models",
                "multimodal inputs",
                "language instructions",
                "false-premise instructions",
                "Instruct-Verify-and-Act",
                "contextually augmented",
                "semi-synthetic dataset"
            ]
        },
        "publishedAt": "2025-08-22T06:54:33.000Z",
        "title": "Do What? Teaching Vision-Language-Action Models to Reject the Impossible",
        "summary": "Recently, Vision-Language-Action (VLA) models have demonstrated strong\nperformance on a range of robotic tasks. These models rely on multimodal\ninputs, with language instructions playing a crucial role -- not only in\npredicting actions, but also in robustly interpreting user intent, even when\nthe requests are impossible to fulfill. In this work, we investigate how VLAs\ncan recognize, interpret, and respond to false-premise instructions: natural\nlanguage commands that reference objects or conditions absent from the\nenvironment. We propose Instruct-Verify-and-Act (IVA), a unified framework that\n(i) detects when an instruction cannot be executed due to a false premise, (ii)\nengages in language-based clarification or correction, and (iii) grounds\nplausible alternatives in perception and action. Towards this end, we construct\na large-scale instruction tuning setup with structured language prompts and\ntrain a VLA model capable of handling both accurate and erroneous requests. Our\napproach leverages a contextually augmented, semi-synthetic dataset containing\npaired positive and false-premise instructions, enabling robust detection and\nnatural language correction. Our experiments show that IVA improves false\npremise detection accuracy by 97.56% over baselines, while increasing\nsuccessful responses in false-premise scenarios by 50.78%.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16292.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.16279",
            "authors": [
                {
                    "_id": "68abe18a86b21a0e2e358adf",
                    "name": "Dawei Gao",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae0",
                    "name": "Zitao Li",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae1",
                    "name": "Yuexiang Xie",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae2",
                    "user": {
                        "_id": "62734fc860bc37aa9c326f50",
                        "avatarUrl": "/avatars/2238451f16559c7a6d77f3dcd6fcd1f3.svg",
                        "isPro": false,
                        "fullname": "weirui kuang",
                        "user": "rayrayray",
                        "type": "user"
                    },
                    "name": "Weirui Kuang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-25T20:34:06.130Z",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae3",
                    "name": "Liuyi Yao",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae4",
                    "name": "Bingchen Qian",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae5",
                    "name": "Zhijian Ma",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae6",
                    "name": "Yue Cui",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae7",
                    "name": "Haohao Luo",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae8",
                    "name": "Shen Li",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358ae9",
                    "name": "Lu Yi",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358aea",
                    "name": "Yi Yu",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358aeb",
                    "name": "Shiqi He",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358aec",
                    "name": "Zhiling Luo",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358aed",
                    "name": "Wenmeng Zhou",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358aee",
                    "name": "Zhicheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358aef",
                    "name": "Xuguang He",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358af0",
                    "name": "Ziqian Chen",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358af1",
                    "name": "Weikai Liao",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358af2",
                    "name": "Farruh Isakulovich Kushnazarov",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358af3",
                    "name": "Yaliang Li",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358af4",
                    "name": "Bolin Ding",
                    "hidden": false
                },
                {
                    "_id": "68abe18a86b21a0e2e358af5",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T10:35:56.000Z",
            "submittedOnDailyAt": "2025-08-25T02:37:54.746Z",
            "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.",
            "upvotes": 5,
            "discussionId": "68abe18b86b21a0e2e358af6",
            "githubRepo": "https://github.com/agentscope-ai/agentscope",
            "ai_summary": "AgentScope enhances agentic applications by providing flexible tool-based interactions, unified interfaces, and advanced infrastructure based on the ReAct paradigm, supporting efficient and safe development and deployment.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "AgentScope",
                "tool-based agent-environment interactions",
                "ReAct paradigm",
                "asynchronous design",
                "human-agent interactions",
                "agent-agent interactions",
                "built-in agents",
                "scalable evaluation module",
                "visual studio interface",
                "runtime sandbox"
            ],
            "githubStars": 7771
        },
        "publishedAt": "2025-08-22T06:35:56.000Z",
        "title": "AgentScope 1.0: A Developer-Centric Framework for Building Agentic\n  Applications",
        "summary": "Driven by rapid advancements of Large Language Models (LLMs), agents are\nempowered to combine intrinsic knowledge with dynamic tool use, greatly\nenhancing their capacity to address real-world tasks. In line with such an\nevolution, AgentScope introduces major improvements in a new version (1.0),\ntowards comprehensively supporting flexible and efficient tool-based\nagent-environment interactions for building agentic applications. Specifically,\nwe abstract foundational components essential for agentic applications and\nprovide unified interfaces and extensible modules, enabling developers to\neasily leverage the latest progress, such as new models and MCPs. Furthermore,\nwe ground agent behaviors in the ReAct paradigm and offer advanced agent-level\ninfrastructure based on a systematic asynchronous design, which enriches both\nhuman-agent and agent-agent interaction patterns while improving execution\nefficiency. Building on this foundation, we integrate several built-in agents\ntailored to specific practical scenarios. AgentScope also includes robust\nengineering support for developer-friendly experiences. We provide a scalable\nevaluation module with a visual studio interface, making the development of\nlong-trajectory agentic applications more manageable and easier to trace. In\naddition, AgentScope offers a runtime sandbox to ensure safe agent execution\nand facilitates rapid deployment in production environments. With these\nenhancements, AgentScope provides a practical foundation for building scalable,\nadaptive, and effective agentic applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16279.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.15881",
            "authors": [
                {
                    "_id": "68abc7c886b21a0e2e358a78",
                    "user": {
                        "_id": "6433c6ad7b8247480106c189",
                        "avatarUrl": "/avatars/42d84a7bbcfb9f5cc60bb460e6375f14.svg",
                        "isPro": false,
                        "fullname": "Aurora",
                        "user": "xiaojuan0920",
                        "type": "user"
                    },
                    "name": "Xiaojuan Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:19:43.131Z",
                    "hidden": false
                },
                {
                    "_id": "68abc7c886b21a0e2e358a79",
                    "name": "Fanxu Meng",
                    "hidden": false
                },
                {
                    "_id": "68abc7c886b21a0e2e358a7a",
                    "name": "Pingzhi Tang",
                    "hidden": false
                },
                {
                    "_id": "68abc7c886b21a0e2e358a7b",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68abc7c886b21a0e2e358a7c",
                    "name": "Di Yin",
                    "hidden": false
                },
                {
                    "_id": "68abc7c886b21a0e2e358a7d",
                    "name": "Xing Sun",
                    "hidden": false
                },
                {
                    "_id": "68abc7c886b21a0e2e358a7e",
                    "name": "Muhan Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-21T15:25:40.000Z",
            "submittedOnDailyAt": "2025-08-25T00:50:18.402Z",
            "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill \\& Decode Inference",
            "submittedOnDailyBy": {
                "_id": "643f55d4ec817b766686438a",
                "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
                "isPro": false,
                "fullname": "mengfanxu",
                "user": "fxmeng",
                "type": "user"
            },
            "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
            "upvotes": 5,
            "discussionId": "68abc7c886b21a0e2e358a7f",
            "ai_summary": "Tensor-Parallel Latent Attention (TPLA) enhances tensor parallelism efficiency by partitioning latent representations and input dimensions, preserving the benefits of compressed key-value caches while maintaining strong representational capacity.",
            "ai_keywords": [
                "Multi-Head Latent Attention (MLA)",
                "tensor parallelism (TP)",
                "latent representation",
                "attention heads",
                "Grouped Query Attention (GQA)",
                "Tensor-Parallel Latent Attention (TPLA)",
                "Grouped Latent Attention (GLA)",
                "prefilling",
                "tensor-parallel decoding",
                "Hadamard transform",
                "PCA",
                "FlashAttention-3",
                "commonsense",
                "LongBench benchmarks"
            ]
        },
        "publishedAt": "2025-08-21T11:25:40.000Z",
        "title": "TPLA: Tensor Parallel Latent Attention for Efficient Disaggregated\n  Prefill \\& Decode Inference",
        "summary": "Multi-Head Latent Attention (MLA), introduced in DeepSeek-V2, compresses\nkey-value states into a low-rank latent vector, caching only this vector to\nreduce memory. In tensor parallelism (TP), however, attention heads are\ncomputed across multiple devices, and each device must load the full cache,\neroding the advantage of MLA over Grouped Query Attention (GQA). We propose\nTensor-Parallel Latent Attention (TPLA): a scheme that partitions both the\nlatent representation and each head's input dimension across devices, performs\nattention independently per shard, and then combines results with an\nall-reduce. TPLA preserves the benefits of a compressed KV cache while\nunlocking TP efficiency. Unlike Grouped Latent Attention (GLA), every head in\nTPLA still leverages the full latent representation, maintaining stronger\nrepresentational capacity. TPLA is drop-in compatible with models pre-trained\nusing MLA: it supports MLA-style prefilling and enables efficient\ntensor-parallel decoding without retraining. Applying simple orthogonal\ntransforms -- e.g., the Hadamard transform or PCA -- before TP slicing further\nmitigates cross-shard interference, yielding minimal accuracy degradation. By\nreducing the per-device KV cache for DeepSeek-V3 and Kimi-K2, we achieve 1.79x\nand 1.93x speedups, respectively, at a 32K-token context length while\nmaintaining performance on commonsense and LongBench benchmarks. TPLA can be\nimplemented with FlashAttention-3, enabling practical end-to-end acceleration.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15881.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643f55d4ec817b766686438a",
            "avatarUrl": "/avatars/0feb460432c92ab9ada0d417a7a38f6a.svg",
            "fullname": "mengfanxu",
            "name": "fxmeng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 17
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.14037",
            "authors": [
                {
                    "_id": "68ac8ad486b21a0e2e358c90",
                    "name": "Lintao Xiang",
                    "hidden": false
                },
                {
                    "_id": "68ac8ad486b21a0e2e358c91",
                    "name": "Xinkai Chen",
                    "hidden": false
                },
                {
                    "_id": "68ac8ad486b21a0e2e358c92",
                    "name": "Jianhuang Lai",
                    "hidden": false
                },
                {
                    "_id": "68ac8ad486b21a0e2e358c93",
                    "user": {
                        "_id": "62e893da40bd989bb71b8f89",
                        "avatarUrl": "/avatars/34e755d1124303a498429a3c4d01367b.svg",
                        "isPro": false,
                        "fullname": "Guangcong Wang",
                        "user": "GuangcongWang",
                        "type": "user"
                    },
                    "name": "Guangcong Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-25T20:34:22.508Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62e893da40bd989bb71b8f89/DLUZg0INEEezq5JrTFw68.mp4"
            ],
            "publishedAt": "2025-08-19T17:59:26.000Z",
            "submittedOnDailyAt": "2025-08-25T14:46:26.384Z",
            "title": "Distilled-3DGS:Distilled 3D Gaussian Splatting",
            "submittedOnDailyBy": {
                "_id": "62e893da40bd989bb71b8f89",
                "avatarUrl": "/avatars/34e755d1124303a498429a3c4d01367b.svg",
                "isPro": false,
                "fullname": "Guangcong Wang",
                "user": "GuangcongWang",
                "type": "user"
            },
            "summary": "3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view\nsynthesis (NVS). However, it suffers from a significant drawback: achieving\nhigh-fidelity rendering typically necessitates a large number of 3D Gaussians,\nresulting in substantial memory consumption and storage requirements. To\naddress this challenge, we propose the first knowledge distillation framework\nfor 3DGS, featuring various teacher models, including vanilla 3DGS,\nnoise-augmented variants, and dropout-regularized versions. The outputs of\nthese teachers are aggregated to guide the optimization of a lightweight\nstudent model. To distill the hidden geometric structure, we propose a\nstructural similarity loss to boost the consistency of spatial geometric\ndistributions between the student and teacher model. Through comprehensive\nquantitative and qualitative evaluations across diverse datasets, the proposed\nDistilled-3DGS, a simple yet effective framework without bells and whistles,\nachieves promising rendering results in both rendering quality and storage\nefficiency compared to state-of-the-art methods. Project page:\nhttps://distilled3dgs.github.io . Code:\nhttps://github.com/lt-xiang/Distilled-3DGS .",
            "upvotes": 4,
            "discussionId": "68ac8ad586b21a0e2e358c94",
            "projectPage": "https://distilled3dgs.github.io/",
            "githubRepo": "https://github.com/lt-xiang/Distilled-3DGS",
            "ai_summary": "A knowledge distillation framework for 3D Gaussian Splatting improves rendering quality and storage efficiency by aggregating outputs from multiple teacher models to optimize a lightweight student model.",
            "ai_keywords": [
                "3D Gaussian Splatting",
                "novel view synthesis",
                "knowledge distillation",
                "structural similarity loss",
                "spatial geometric distributions"
            ],
            "githubStars": 43
        },
        "publishedAt": "2025-08-19T13:59:26.000Z",
        "title": "Distilled-3DGS:Distilled 3D Gaussian Splatting",
        "summary": "3D Gaussian Splatting (3DGS) has exhibited remarkable efficacy in novel view\nsynthesis (NVS). However, it suffers from a significant drawback: achieving\nhigh-fidelity rendering typically necessitates a large number of 3D Gaussians,\nresulting in substantial memory consumption and storage requirements. To\naddress this challenge, we propose the first knowledge distillation framework\nfor 3DGS, featuring various teacher models, including vanilla 3DGS,\nnoise-augmented variants, and dropout-regularized versions. The outputs of\nthese teachers are aggregated to guide the optimization of a lightweight\nstudent model. To distill the hidden geometric structure, we propose a\nstructural similarity loss to boost the consistency of spatial geometric\ndistributions between the student and teacher model. Through comprehensive\nquantitative and qualitative evaluations across diverse datasets, the proposed\nDistilled-3DGS, a simple yet effective framework without bells and whistles,\nachieves promising rendering results in both rendering quality and storage\nefficiency compared to state-of-the-art methods. Project page:\nhttps://distilled3dgs.github.io . Code:\nhttps://github.com/lt-xiang/Distilled-3DGS .",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62e893da40bd989bb71b8f89/DLUZg0INEEezq5JrTFw68.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14037.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62e893da40bd989bb71b8f89",
            "avatarUrl": "/avatars/34e755d1124303a498429a3c4d01367b.svg",
            "fullname": "Guangcong Wang",
            "name": "GuangcongWang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13797",
            "authors": [
                {
                    "_id": "68a5a31dc4b3ea17d2d2cf4d",
                    "user": {
                        "_id": "6424538b9f9e65b42389920e",
                        "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
                        "isPro": false,
                        "fullname": "Feng-Lin Liu",
                        "user": "Okrin",
                        "type": "user"
                    },
                    "name": "Feng-Lin Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-25T20:33:36.346Z",
                    "hidden": false
                },
                {
                    "_id": "68a5a31dc4b3ea17d2d2cf4e",
                    "name": "Shi-Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68a5a31dc4b3ea17d2d2cf4f",
                    "name": "Yan-Pei Cao",
                    "hidden": false
                },
                {
                    "_id": "68a5a31dc4b3ea17d2d2cf50",
                    "name": "Hongbo Fu",
                    "hidden": false
                },
                {
                    "_id": "68a5a31dc4b3ea17d2d2cf51",
                    "name": "Lin Gao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T12:57:31.000Z",
            "submittedOnDailyAt": "2025-08-25T11:56:03.827Z",
            "title": "Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing",
            "submittedOnDailyBy": {
                "_id": "6424538b9f9e65b42389920e",
                "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
                "isPro": false,
                "fullname": "Feng-Lin Liu",
                "user": "Okrin",
                "type": "user"
            },
            "summary": "Recent video editing methods achieve attractive results in style transfer or\nappearance modification. However, editing the structural content of 3D scenes\nin videos remains challenging, particularly when dealing with significant\nviewpoint changes, such as large camera rotations or zooms. Key challenges\ninclude generating novel view content that remains consistent with the original\nvideo, preserving unedited regions, and translating sparse 2D inputs into\nrealistic 3D video outputs. To address these issues, we propose Sketch3DVE, a\nsketch-based 3D-aware video editing method to enable detailed local\nmanipulation of videos with significant viewpoint changes. To solve the\nchallenge posed by sparse inputs, we employ image editing methods to generate\nedited results for the first frame, which are then propagated to the remaining\nframes of the video. We utilize sketching as an interaction tool for precise\ngeometry control, while other mask-based image editing methods are also\nsupported. To handle viewpoint changes, we perform a detailed analysis and\nmanipulation of the 3D information in the video. Specifically, we utilize a\ndense stereo method to estimate a point cloud and the camera parameters of the\ninput video. We then propose a point cloud editing approach that uses depth\nmaps to represent the 3D geometry of newly edited components, aligning them\neffectively with the original 3D scene. To seamlessly merge the newly edited\ncontent with the original video while preserving the features of unedited\nregions, we introduce a 3D-aware mask propagation strategy and employ a video\ndiffusion model to produce realistic edited videos. Extensive experiments\ndemonstrate the superiority of Sketch3DVE in video editing. Homepage and code:\nhttp://http://geometrylearning.com/Sketch3DVE/",
            "upvotes": 3,
            "discussionId": "68a5a31ec4b3ea17d2d2cf52",
            "githubRepo": "https://github.com/IGLICT/Sketch3DVE",
            "ai_summary": "Sketch3DVE is a sketch-based 3D-aware video editing method that uses dense stereo estimation, point cloud editing, and a video diffusion model to handle sparse inputs and viewpoint changes in video editing.",
            "ai_keywords": [
                "dense stereo method",
                "point cloud",
                "camera parameters",
                "point cloud editing",
                "depth maps",
                "3D-aware mask propagation",
                "video diffusion model"
            ],
            "githubStars": 17
        },
        "publishedAt": "2025-08-19T08:57:31.000Z",
        "title": "Sketch3DVE: Sketch-based 3D-Aware Scene Video Editing",
        "summary": "Recent video editing methods achieve attractive results in style transfer or\nappearance modification. However, editing the structural content of 3D scenes\nin videos remains challenging, particularly when dealing with significant\nviewpoint changes, such as large camera rotations or zooms. Key challenges\ninclude generating novel view content that remains consistent with the original\nvideo, preserving unedited regions, and translating sparse 2D inputs into\nrealistic 3D video outputs. To address these issues, we propose Sketch3DVE, a\nsketch-based 3D-aware video editing method to enable detailed local\nmanipulation of videos with significant viewpoint changes. To solve the\nchallenge posed by sparse inputs, we employ image editing methods to generate\nedited results for the first frame, which are then propagated to the remaining\nframes of the video. We utilize sketching as an interaction tool for precise\ngeometry control, while other mask-based image editing methods are also\nsupported. To handle viewpoint changes, we perform a detailed analysis and\nmanipulation of the 3D information in the video. Specifically, we utilize a\ndense stereo method to estimate a point cloud and the camera parameters of the\ninput video. We then propose a point cloud editing approach that uses depth\nmaps to represent the 3D geometry of newly edited components, aligning them\neffectively with the original 3D scene. To seamlessly merge the newly edited\ncontent with the original video while preserving the features of unedited\nregions, we introduce a 3D-aware mask propagation strategy and employ a video\ndiffusion model to produce realistic edited videos. Extensive experiments\ndemonstrate the superiority of Sketch3DVE in video editing. Homepage and code:\nhttp://http://geometrylearning.com/Sketch3DVE/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13797.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6424538b9f9e65b42389920e",
            "avatarUrl": "/avatars/9b912e2af9eebe9a481181f006765059.svg",
            "fullname": "Feng-Lin Liu",
            "name": "Okrin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.16359",
            "authors": [
                {
                    "_id": "68ac400686b21a0e2e358be8",
                    "user": {
                        "_id": "65e0a09f5cc7e39aba11eb21",
                        "avatarUrl": "/avatars/297775c57d820a40f762ace9456abcd4.svg",
                        "isPro": true,
                        "fullname": "Odin HG",
                        "user": "odinhg",
                        "type": "user"
                    },
                    "name": "Odin Hoff Gardaa",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T12:37:29.662Z",
                    "hidden": false
                },
                {
                    "_id": "68ac400686b21a0e2e358be9",
                    "name": "Nello Blaser",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T13:05:55.000Z",
            "submittedOnDailyAt": "2025-08-25T12:39:36.815Z",
            "title": "RotaTouille: Rotation Equivariant Deep Learning for Contours",
            "submittedOnDailyBy": {
                "_id": "65e0a09f5cc7e39aba11eb21",
                "avatarUrl": "/avatars/297775c57d820a40f762ace9456abcd4.svg",
                "isPro": true,
                "fullname": "Odin HG",
                "user": "odinhg",
                "type": "user"
            },
            "summary": "Contours or closed planar curves are common in many domains. For example,\nthey appear as object boundaries in computer vision, isolines in meteorology,\nand the orbits of rotating machinery. In many cases when learning from contour\ndata, planar rotations of the input will result in correspondingly rotated\noutputs. It is therefore desirable that deep learning models be rotationally\nequivariant. In addition, contours are typically represented as an ordered\nsequence of edge points, where the choice of starting point is arbitrary. It is\ntherefore also desirable for deep learning methods to be equivariant under\ncyclic shifts. We present RotaTouille, a deep learning framework for learning\nfrom contour data that achieves both rotation and cyclic shift equivariance\nthrough complex-valued circular convolution. We further introduce and\ncharacterize equivariant non-linearities, coarsening layers, and global pooling\nlayers to obtain invariant representations for downstream tasks. Finally, we\ndemonstrate the effectiveness of RotaTouille through experiments in shape\nclassification, reconstruction, and contour regression.",
            "upvotes": 1,
            "discussionId": "68ac400686b21a0e2e358bea",
            "githubRepo": "https://github.com/odinhg/rotation-equivariant-contour-learning",
            "ai_summary": "RotaTouille is a deep learning framework that achieves rotation and cyclic shift equivariance for contour data using complex-valued circular convolution, enabling effective performance in shape classification, reconstruction, and contour regression.",
            "ai_keywords": [
                "rotationally equivariant",
                "cyclic shift equivariance",
                "complex-valued circular convolution",
                "equivariant non-linearities",
                "coarsening layers",
                "global pooling layers",
                "invariant representations"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-08-22T09:05:55.000Z",
        "title": "RotaTouille: Rotation Equivariant Deep Learning for Contours",
        "summary": "Contours or closed planar curves are common in many domains. For example,\nthey appear as object boundaries in computer vision, isolines in meteorology,\nand the orbits of rotating machinery. In many cases when learning from contour\ndata, planar rotations of the input will result in correspondingly rotated\noutputs. It is therefore desirable that deep learning models be rotationally\nequivariant. In addition, contours are typically represented as an ordered\nsequence of edge points, where the choice of starting point is arbitrary. It is\ntherefore also desirable for deep learning methods to be equivariant under\ncyclic shifts. We present RotaTouille, a deep learning framework for learning\nfrom contour data that achieves both rotation and cyclic shift equivariance\nthrough complex-valued circular convolution. We further introduce and\ncharacterize equivariant non-linearities, coarsening layers, and global pooling\nlayers to obtain invariant representations for downstream tasks. Finally, we\ndemonstrate the effectiveness of RotaTouille through experiments in shape\nclassification, reconstruction, and contour regression.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16359.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e0a09f5cc7e39aba11eb21",
            "avatarUrl": "/avatars/297775c57d820a40f762ace9456abcd4.svg",
            "fullname": "Odin HG",
            "name": "odinhg",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.16072",
            "authors": [
                {
                    "_id": "68abe30186b21a0e2e358b05",
                    "name": "Zizhen Li",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b06",
                    "name": "Chuanhao Li",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b07",
                    "name": "Yibin Wang",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b08",
                    "name": "Qi Chen",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b09",
                    "user": {
                        "_id": "6469aa23dcbb937d56ba5077",
                        "avatarUrl": "/avatars/aa45f2dacf1c0925c5dd7614e31f9ea0.svg",
                        "isPro": false,
                        "fullname": "Diping Song",
                        "user": "dpsong",
                        "type": "user"
                    },
                    "name": "Diping Song",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-25T20:34:42.539Z",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b0a",
                    "name": "Yukang Feng",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b0b",
                    "name": "Jianwen Sun",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b0c",
                    "name": "Jiaxin Ai",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b0d",
                    "name": "Fanrui Zhang",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b0e",
                    "name": "Mingzhu Sun",
                    "hidden": false
                },
                {
                    "_id": "68abe30186b21a0e2e358b0f",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T04:04:00.000Z",
            "submittedOnDailyAt": "2025-08-25T02:44:11.309Z",
            "title": "InMind: Evaluating LLMs in Capturing and Applying Individual Human\n  Reasoning Styles",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "LLMs have shown strong performance on human-centric reasoning tasks. While\nprevious evaluations have explored whether LLMs can infer intentions or detect\ndeception, they often overlook the individualized reasoning styles that\ninfluence how people interpret and act in social contexts. Social deduction\ngames (SDGs) provide a natural testbed for evaluating individualized reasoning\nstyles, where different players may adopt diverse but contextually valid\nreasoning strategies under identical conditions. To address this, we introduce\nInMind, a cognitively grounded evaluation framework designed to assess whether\nLLMs can capture and apply personalized reasoning styles in SDGs. InMind\nenhances structured gameplay data with round-level strategy traces and\npost-game reflections, collected under both Observer and Participant modes. It\nsupports four cognitively motivated tasks that jointly evaluate both static\nalignment and dynamic adaptation. As a case study, we apply InMind to the game\nAvalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o\nfrequently rely on lexical cues, struggling to anchor reflections in temporal\ngameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs\nlike DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These\nfindings reveal key limitations in current LLMs' capacity for individualized,\nadaptive reasoning, and position InMind as a step toward cognitively aligned\nhuman-AI interaction.",
            "upvotes": 1,
            "discussionId": "68abe30186b21a0e2e358b10",
            "githubRepo": "https://github.com/leroy9472/InMind",
            "ai_summary": "InMind evaluates LLMs' ability to capture and apply personalized reasoning styles in social deduction games, highlighting limitations in current models' adaptive reasoning.",
            "ai_keywords": [
                "LLMs",
                "human-centric reasoning",
                "social deduction games",
                "SDGs",
                "reasoning styles",
                "InMind",
                "strategy traces",
                "post-game reflections",
                "Observer mode",
                "Participant mode",
                "static alignment",
                "dynamic adaptation",
                "Avalon",
                "GPT-4",
                "DeepSeek-R1",
                "style-sensitive reasoning",
                "cognitively aligned human-AI interaction"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-08-22T00:04:00.000Z",
        "title": "InMind: Evaluating LLMs in Capturing and Applying Individual Human\n  Reasoning Styles",
        "summary": "LLMs have shown strong performance on human-centric reasoning tasks. While\nprevious evaluations have explored whether LLMs can infer intentions or detect\ndeception, they often overlook the individualized reasoning styles that\ninfluence how people interpret and act in social contexts. Social deduction\ngames (SDGs) provide a natural testbed for evaluating individualized reasoning\nstyles, where different players may adopt diverse but contextually valid\nreasoning strategies under identical conditions. To address this, we introduce\nInMind, a cognitively grounded evaluation framework designed to assess whether\nLLMs can capture and apply personalized reasoning styles in SDGs. InMind\nenhances structured gameplay data with round-level strategy traces and\npost-game reflections, collected under both Observer and Participant modes. It\nsupports four cognitively motivated tasks that jointly evaluate both static\nalignment and dynamic adaptation. As a case study, we apply InMind to the game\nAvalon, evaluating 11 state-of-the-art LLMs. General-purpose LLMs, even GPT-4o\nfrequently rely on lexical cues, struggling to anchor reflections in temporal\ngameplay or adapt to evolving strategies. In contrast, reasoning-enhanced LLMs\nlike DeepSeek-R1 exhibit early signs of style-sensitive reasoning. These\nfindings reveal key limitations in current LLMs' capacity for individualized,\nadaptive reasoning, and position InMind as a step toward cognitively aligned\nhuman-AI interaction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16072.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.15868",
            "authors": [
                {
                    "_id": "68ac43be86b21a0e2e358bec",
                    "user": {
                        "_id": "648add6aff6123185eb185a8",
                        "avatarUrl": "/avatars/e37dfa680c1bb86c721165f03eb79e97.svg",
                        "isPro": false,
                        "fullname": "WNQzhu",
                        "user": "Qlisp",
                        "type": "user"
                    },
                    "name": "Wenqiao Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T12:37:27.698Z",
                    "hidden": false
                },
                {
                    "_id": "68ac43be86b21a0e2e358bed",
                    "name": "Ji Liu",
                    "hidden": false
                },
                {
                    "_id": "68ac43be86b21a0e2e358bee",
                    "name": "Rongjuncheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ac43be86b21a0e2e358bef",
                    "name": "Haipang Wu",
                    "hidden": false
                },
                {
                    "_id": "68ac43be86b21a0e2e358bf0",
                    "name": "Yulun Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-21T00:20:47.000Z",
            "submittedOnDailyAt": "2025-08-25T09:38:32.798Z",
            "title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated\n  Chain-of-Thought-based Reinforced Fine-Tuning",
            "submittedOnDailyBy": {
                "_id": "648add6aff6123185eb185a8",
                "avatarUrl": "/avatars/e37dfa680c1bb86c721165f03eb79e97.svg",
                "isPro": false,
                "fullname": "WNQzhu",
                "user": "Qlisp",
                "type": "user"
            },
            "summary": "Reasoning capability plays a significantly critical role in the the broad\napplications of Large Language Models (LLMs). To enhance the reasoning\nperformance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning\napproaches have been proposed to address the limited generalization capability\nof LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their\neffectiveness, two major limitations hinder the advancement of LLMs. First,\nvanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and\nincorporate unstable reasoning path sampling, which typically results in model\ncollapse, unstable training process, and suboptimal performance. Second,\nexisting SFT approaches generally overemphasize the annotated CoT, potentially\nleading to performance degradation due to insufficient exploitation of\npotential CoT. In this paper, we propose a Contrastive learning with annotated\nCoT-based Reinforced Fine-Tuning approach, i.e., , to enhance the\nreasoning performance of LLMs while addressing the aforementioned limitations.\nSpecifically, we propose learning a representation for each CoT. Based on this\nrepresentation, we design novel contrastive signals to guide the fine-tuning\nprocess. Our approach not only fully exploits the available annotated CoT but\nalso stabilizes the fine-tuning procedure by incorporating an additional\nunsupervised learning signal. We conduct comprehensive experiments and in-depth\nanalysis with three baseline approaches, two foundation models, and two\ndatasets to demonstrate significant advantages of  in terms of\nrobustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code\nis available at https://github.com/WNQzhu/CARFT.",
            "upvotes": 0,
            "discussionId": "68ac43be86b21a0e2e358bf1",
            "ai_summary": "A contrastive learning approach with annotated Chain-of-Thought enhances the reasoning performance of Large Language Models through stable fine-tuning and robust contrastive signals.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Supervised Fine-Tuning",
                "Chain-of-Thought",
                "model collapse",
                "contrastive learning",
                "contrastive signals",
                "unsupervised learning"
            ]
        },
        "publishedAt": "2025-08-20T20:20:47.000Z",
        "title": "CARFT: Boosting LLM Reasoning via Contrastive Learning with Annotated\n  Chain-of-Thought-based Reinforced Fine-Tuning",
        "summary": "Reasoning capability plays a significantly critical role in the the broad\napplications of Large Language Models (LLMs). To enhance the reasoning\nperformance of LLMs, diverse Reinforcement Learning (RL)-based fine-tuning\napproaches have been proposed to address the limited generalization capability\nof LLMs trained solely via Supervised Fine-Tuning (SFT). Despite their\neffectiveness, two major limitations hinder the advancement of LLMs. First,\nvanilla RL-based approaches ignore annotated Chain-of-Thought (CoT) and\nincorporate unstable reasoning path sampling, which typically results in model\ncollapse, unstable training process, and suboptimal performance. Second,\nexisting SFT approaches generally overemphasize the annotated CoT, potentially\nleading to performance degradation due to insufficient exploitation of\npotential CoT. In this paper, we propose a Contrastive learning with annotated\nCoT-based Reinforced Fine-Tuning approach, i.e., , to enhance the\nreasoning performance of LLMs while addressing the aforementioned limitations.\nSpecifically, we propose learning a representation for each CoT. Based on this\nrepresentation, we design novel contrastive signals to guide the fine-tuning\nprocess. Our approach not only fully exploits the available annotated CoT but\nalso stabilizes the fine-tuning procedure by incorporating an additional\nunsupervised learning signal. We conduct comprehensive experiments and in-depth\nanalysis with three baseline approaches, two foundation models, and two\ndatasets to demonstrate significant advantages of  in terms of\nrobustness, performance (up to 10.15\\%), and efficiency (up to 30.62\\%). Code\nis available at https://github.com/WNQzhu/CARFT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15868.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "648add6aff6123185eb185a8",
            "avatarUrl": "/avatars/e37dfa680c1bb86c721165f03eb79e97.svg",
            "fullname": "WNQzhu",
            "name": "Qlisp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13562",
            "authors": [
                {
                    "_id": "68abf69c86b21a0e2e358b4d",
                    "user": {
                        "_id": "670f667d7d1b0373c63f1620",
                        "avatarUrl": "/avatars/21297c0b00cad7ea37c9167d113e2586.svg",
                        "isPro": false,
                        "fullname": "Yuchen Yang",
                        "user": "Charlie019",
                        "type": "user"
                    },
                    "name": "Yuchen Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T08:18:42.071Z",
                    "hidden": false
                },
                {
                    "_id": "68abf69c86b21a0e2e358b4e",
                    "name": "Linfeng Dong",
                    "hidden": false
                },
                {
                    "_id": "68abf69c86b21a0e2e358b4f",
                    "name": "Wei Wang",
                    "hidden": false
                },
                {
                    "_id": "68abf69c86b21a0e2e358b50",
                    "name": "Zhihang Zhong",
                    "hidden": false
                },
                {
                    "_id": "68abf69c86b21a0e2e358b51",
                    "name": "Xiao Sun",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T06:53:57.000Z",
            "submittedOnDailyAt": "2025-08-25T06:51:15.898Z",
            "title": "Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose\n  Inverse Kinematics",
            "submittedOnDailyBy": {
                "_id": "670f667d7d1b0373c63f1620",
                "avatarUrl": "/avatars/21297c0b00cad7ea37c9167d113e2586.svg",
                "isPro": false,
                "fullname": "Yuchen Yang",
                "user": "Charlie019",
                "type": "user"
            },
            "summary": "In 3D human pose and shape estimation, SMPLify remains a robust baseline that\nsolves inverse kinematics (IK) through iterative optimization. However, its\nhigh computational cost limits its practicality. Recent advances across domains\nhave shown that replacing iterative optimization with data-driven neural\nnetworks can achieve significant runtime improvements without sacrificing\naccuracy. Motivated by this trend, we propose Learnable SMPLify, a neural\nframework that replaces the iterative fitting process in SMPLify with a\nsingle-pass regression model. The design of our framework targets two core\nchallenges in neural IK: data construction and generalization. To enable\neffective training, we propose a temporal sampling strategy that constructs\ninitialization-target pairs from sequential frames. To improve generalization\nacross diverse motions and unseen poses, we propose a human-centric\nnormalization scheme and residual learning to narrow the solution space.\nLearnable SMPLify supports both sequential inference and plug-in\npost-processing to refine existing image-based estimators. Extensive\nexperiments demonstrate that our method establishes itself as a practical and\nsimple baseline: it achieves nearly 200x faster runtime compared to SMPLify,\ngeneralizes well to unseen 3DPW and RICH, and operates in a model-agnostic\nmanner when used as a plug-in tool on LucidAction. The code is available at\nhttps://github.com/Charrrrrlie/Learnable-SMPLify.",
            "upvotes": 0,
            "discussionId": "68abf69c86b21a0e2e358b52",
            "githubRepo": "https://github.com/Charrrrrlie/Learnable-SMPLify",
            "ai_summary": "Learnable SMPLify replaces iterative optimization in SMPLify with a neural network for faster and more generalized 3D human pose and shape estimation.",
            "ai_keywords": [
                "SMPLify",
                "inverse kinematics",
                "iterative optimization",
                "neural networks",
                "single-pass regression model",
                "temporal sampling",
                "human-centric normalization",
                "residual learning",
                "sequential inference",
                "plug-in post-processing",
                "3DPW",
                "RICH",
                "LucidAction"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-08-19T02:53:57.000Z",
        "title": "Learnable SMPLify: A Neural Solution for Optimization-Free Human Pose\n  Inverse Kinematics",
        "summary": "In 3D human pose and shape estimation, SMPLify remains a robust baseline that\nsolves inverse kinematics (IK) through iterative optimization. However, its\nhigh computational cost limits its practicality. Recent advances across domains\nhave shown that replacing iterative optimization with data-driven neural\nnetworks can achieve significant runtime improvements without sacrificing\naccuracy. Motivated by this trend, we propose Learnable SMPLify, a neural\nframework that replaces the iterative fitting process in SMPLify with a\nsingle-pass regression model. The design of our framework targets two core\nchallenges in neural IK: data construction and generalization. To enable\neffective training, we propose a temporal sampling strategy that constructs\ninitialization-target pairs from sequential frames. To improve generalization\nacross diverse motions and unseen poses, we propose a human-centric\nnormalization scheme and residual learning to narrow the solution space.\nLearnable SMPLify supports both sequential inference and plug-in\npost-processing to refine existing image-based estimators. Extensive\nexperiments demonstrate that our method establishes itself as a practical and\nsimple baseline: it achieves nearly 200x faster runtime compared to SMPLify,\ngeneralizes well to unseen 3DPW and RICH, and operates in a model-agnostic\nmanner when used as a plug-in tool on LucidAction. The code is available at\nhttps://github.com/Charrrrrlie/Learnable-SMPLify.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13562.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "670f667d7d1b0373c63f1620",
            "avatarUrl": "/avatars/21297c0b00cad7ea37c9167d113e2586.svg",
            "fullname": "Yuchen Yang",
            "name": "Charlie019",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.10390",
            "authors": [
                {
                    "_id": "689fe738a4caabb4320e6067",
                    "user": {
                        "_id": "637f6b5752229c6392138545",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669295079706-637f6b5752229c6392138545.jpeg",
                        "isPro": false,
                        "fullname": "Zhang",
                        "user": "AlienZhang1996",
                        "type": "user"
                    },
                    "name": "Chiyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-18T06:55:29.169Z",
                    "hidden": false
                },
                {
                    "_id": "689fe738a4caabb4320e6068",
                    "name": "Lu Zhou",
                    "hidden": false
                },
                {
                    "_id": "689fe738a4caabb4320e6069",
                    "name": "Xiaogang Xu",
                    "hidden": false
                },
                {
                    "_id": "689fe738a4caabb4320e606a",
                    "name": "Jiafei Wu",
                    "hidden": false
                },
                {
                    "_id": "689fe738a4caabb4320e606b",
                    "name": "Liming Fang",
                    "hidden": false
                },
                {
                    "_id": "689fe738a4caabb4320e606c",
                    "name": "Zhe Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-14T06:46:56.000Z",
            "submittedOnDailyAt": "2025-08-25T02:19:48.293Z",
            "title": "Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts",
            "submittedOnDailyBy": {
                "_id": "637f6b5752229c6392138545",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669295079706-637f6b5752229c6392138545.jpeg",
                "isPro": false,
                "fullname": "Zhang",
                "user": "AlienZhang1996",
                "type": "user"
            },
            "summary": "Evaluating jailbreak attacks is challenging when prompts are not overtly\nharmful or fail to induce harmful outputs. Unfortunately, many existing\nred-teaming datasets contain such unsuitable prompts. To evaluate attacks\naccurately, these datasets need to be assessed and cleaned for maliciousness.\nHowever, existing malicious content detection methods rely on either manual\nannotation, which is labor-intensive, or large language models (LLMs), which\nhave inconsistent accuracy in harmful types. To balance accuracy and\nefficiency, we propose a hybrid evaluation framework named MDH (Malicious\ncontent Detection based on LLMs with Human assistance) that combines LLM-based\nannotation with minimal human oversight, and apply it to dataset cleaning and\ndetection of jailbroken responses. Furthermore, we find that well-crafted\ndeveloper messages can significantly boost jailbreak success, leading us to\npropose two new strategies: D-Attack, which leverages context simulation, and\nDH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,\njudgements, and detection results will be released in github repository:\nhttps://github.com/AlienZhang1996/DH-CoT.",
            "upvotes": 0,
            "discussionId": "689fe738a4caabb4320e606d",
            "githubRepo": "https://github.com/AlienZhang1996/DH-CoT",
            "ai_summary": "A hybrid framework combining LLMs and human oversight is proposed to clean datasets and detect jailbreak attacks, with new strategies to enhance jailbreak success.",
            "ai_keywords": [
                "red-teaming datasets",
                "large language models",
                "LLMs",
                "hybrid evaluation framework",
                "MDH",
                "malicious content detection",
                "dataset cleaning",
                "jailbroken responses",
                "D-Attack",
                "context simulation",
                "DH-CoT",
                "hijacked chains of thought"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-08-14T02:46:56.000Z",
        "title": "Jailbreaking Commercial Black-Box LLMs with Explicitly Harmful Prompts",
        "summary": "Evaluating jailbreak attacks is challenging when prompts are not overtly\nharmful or fail to induce harmful outputs. Unfortunately, many existing\nred-teaming datasets contain such unsuitable prompts. To evaluate attacks\naccurately, these datasets need to be assessed and cleaned for maliciousness.\nHowever, existing malicious content detection methods rely on either manual\nannotation, which is labor-intensive, or large language models (LLMs), which\nhave inconsistent accuracy in harmful types. To balance accuracy and\nefficiency, we propose a hybrid evaluation framework named MDH (Malicious\ncontent Detection based on LLMs with Human assistance) that combines LLM-based\nannotation with minimal human oversight, and apply it to dataset cleaning and\ndetection of jailbroken responses. Furthermore, we find that well-crafted\ndeveloper messages can significantly boost jailbreak success, leading us to\npropose two new strategies: D-Attack, which leverages context simulation, and\nDH-CoT, which incorporates hijacked chains of thought. The Codes, datasets,\njudgements, and detection results will be released in github repository:\nhttps://github.com/AlienZhang1996/DH-CoT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.10390.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "637f6b5752229c6392138545",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1669295079706-637f6b5752229c6392138545.jpeg",
            "fullname": "Zhang",
            "name": "AlienZhang1996",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]