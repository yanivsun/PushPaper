[
    {
        "paper": {
            "id": "2508.18265",
            "authors": [
                {
                    "_id": "68ad214d86b21a0e2e358d5d",
                    "user": {
                        "_id": "619507e7b74b6c591f794340",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/619507e7b74b6c591f794340/JbPDoy6Ko1V1-6oJJwFV8.jpeg",
                        "isPro": false,
                        "fullname": "Weiyun Wang",
                        "user": "Weiyun1025",
                        "type": "user"
                    },
                    "name": "Weiyun Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:11.076Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d5e",
                    "name": "Zhangwei Gao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d5f",
                    "user": {
                        "_id": "6541efc9109d78427198ea40",
                        "avatarUrl": "/avatars/c1252dd7da2e53b0b6757bf392139cdf.svg",
                        "isPro": false,
                        "fullname": "Lixin Gu",
                        "user": "gulixin0922",
                        "type": "user"
                    },
                    "name": "Lixin Gu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:50:33.244Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d60",
                    "user": {
                        "_id": "648a1e44fe11ebd7489c289c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648a1e44fe11ebd7489c289c/WvszTywGnuY8OV4500_qh.jpeg",
                        "isPro": false,
                        "fullname": "Hengjun Pu",
                        "user": "MIASANMIA",
                        "type": "user"
                    },
                    "name": "Hengjun Pu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:50:39.288Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d61",
                    "name": "Long Cui",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d62",
                    "user": {
                        "_id": "6771045f076645f7feb4798d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/IsYgpeAGvRAcqnYKwnj0T.jpeg",
                        "isPro": false,
                        "fullname": "Xingguang Wei",
                        "user": "WesKwong",
                        "type": "user"
                    },
                    "name": "Xingguang Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:07.170Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d63",
                    "name": "Zhaoyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d64",
                    "user": {
                        "_id": "6692b867c7cfcb719e6ea7fb",
                        "avatarUrl": "/avatars/198b9cb13d2a5ef50ceabba1f007dc4d.svg",
                        "isPro": false,
                        "fullname": "Linglin",
                        "user": "jinglinglin",
                        "type": "user"
                    },
                    "name": "Linglin Jing",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:50:55.076Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d65",
                    "user": {
                        "_id": "64804866c7f87934d082bb25",
                        "avatarUrl": "/avatars/41761226c79ac16e48d4c4cb84362adb.svg",
                        "isPro": false,
                        "fullname": "Yeshenglong",
                        "user": "Yeshenglong",
                        "type": "user"
                    },
                    "name": "Shenglong Ye",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:51:06.121Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d66",
                    "name": "Jie Shao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d67",
                    "user": {
                        "_id": "665d4b515fdfe8f923e347a7",
                        "avatarUrl": "/avatars/d114b24c02dadfca0a8aee104755a8ec.svg",
                        "isPro": false,
                        "fullname": "Zhaokai Wang",
                        "user": "wzk1015",
                        "type": "user"
                    },
                    "name": "Zhaokai Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-26T10:51:17.286Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d68",
                    "name": "Zhe Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d69",
                    "name": "Hongjie Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6a",
                    "user": {
                        "_id": "6565d7149afd51867e55520b",
                        "avatarUrl": "/avatars/027b17651e61df598af53f69b92e7771.svg",
                        "isPro": false,
                        "fullname": "Ganlin Yang",
                        "user": "ganlinyang",
                        "type": "user"
                    },
                    "name": "Ganlin Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:02.487Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6b",
                    "user": {
                        "_id": "64d83ee0763279bb4ddd5ba8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d83ee0763279bb4ddd5ba8/exbvc1hhIXXKPVmJpxRtP.jpeg",
                        "isPro": false,
                        "fullname": "Haomin Wang",
                        "user": "KiyotakaWang",
                        "type": "user"
                    },
                    "name": "Haomin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:12.956Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6c",
                    "name": "Qi Wei",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6d",
                    "name": "Jinhui Yin",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6e",
                    "name": "Wenhao Li",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d6f",
                    "name": "Erfei Cui",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d70",
                    "name": "Guanzhou Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d71",
                    "user": {
                        "_id": "642b9861bb77f8456634b048",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642b9861bb77f8456634b048/VrNmmcdgX7FufQmdP5YaG.jpeg",
                        "isPro": false,
                        "fullname": "Zichen Ding",
                        "user": "heroding77",
                        "type": "user"
                    },
                    "name": "Zichen Ding",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:09.178Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d72",
                    "name": "Changyao Tian",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d73",
                    "name": "Zhenyu Wu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d74",
                    "name": "Jingjing Xie",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d75",
                    "name": "Zehao Li",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d76",
                    "user": {
                        "_id": "66abb1d1930106b4b433f295",
                        "avatarUrl": "/avatars/12e3375fb6aa04fc24d5092bf40cdecd.svg",
                        "isPro": false,
                        "fullname": "ybw",
                        "user": "YYangzzzz",
                        "type": "user"
                    },
                    "name": "Bowen Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T19:06:09.018Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d77",
                    "name": "Yuchen Duan",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d78",
                    "name": "Xuehui Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d79",
                    "user": {
                        "_id": "64acbbd51aee69ece03c6c0c",
                        "avatarUrl": "/avatars/604df1cabc5faeda55022ae4c1997e56.svg",
                        "isPro": false,
                        "fullname": "Songze Li",
                        "user": "SongzeLi",
                        "type": "user"
                    },
                    "name": "Songze Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:04.603Z",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7a",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7b",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7c",
                    "name": "Nianchen Deng",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7d",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7e",
                    "name": "Yinan He",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d7f",
                    "name": "Yi Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d80",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d81",
                    "name": "Botian Shi",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d82",
                    "name": "Junjun He",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d83",
                    "name": "Yingtong Xiong",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d84",
                    "name": "Han Lv",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d85",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d86",
                    "name": "Wenqi Shao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d87",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d88",
                    "name": "Huipeng Deng",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d89",
                    "name": "Biqing Qi",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8a",
                    "name": "Jiaye Ge",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8b",
                    "name": "Qipeng Guo",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8c",
                    "name": "Wenwei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8d",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8e",
                    "name": "Limin Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d8f",
                    "name": "Min Dou",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d90",
                    "name": "Xizhou Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d91",
                    "name": "Tong Lu",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d92",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d93",
                    "name": "Jifeng Dai",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d94",
                    "name": "Bowen Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d95",
                    "name": "Weijie Su",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d96",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d97",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d98",
                    "name": "Wenhai Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad214d86b21a0e2e358d99",
                    "name": "Gen Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T17:58:17.000Z",
            "submittedOnDailyAt": "2025-08-26T01:22:14.339Z",
            "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05times inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
            "upvotes": 117,
            "discussionId": "68ad214d86b21a0e2e358d9a",
            "ai_summary": "InternVL 3.5 introduces Cascade RL, ViR, and DvD to enhance reasoning, efficiency, and performance in multimodal models.",
            "ai_keywords": [
                "Cascade RL",
                "offline RL",
                "online RL",
                "Visual Resolution Router",
                "ViR",
                "Decoupled Vision-Language Deployment",
                "DvD",
                "multimodal models",
                "reasoning performance",
                "inference speedup",
                "GUI interaction",
                "embodied agency"
            ]
        },
        "publishedAt": "2025-08-25T13:58:17.000Z",
        "title": "InternVL3.5: Advancing Open-Source Multimodal Models in Versatility,\n  Reasoning, and Efficiency",
        "summary": "We introduce InternVL 3.5, a new family of open-source multimodal models that\nsignificantly advances versatility, reasoning capability, and inference\nefficiency along the InternVL series. A key innovation is the Cascade\nReinforcement Learning (Cascade RL) framework, which enhances reasoning through\na two-stage process: offline RL for stable convergence and online RL for\nrefined alignment. This coarse-to-fine training strategy leads to substantial\nimprovements on downstream reasoning tasks, e.g., MMMU and MathVista. To\noptimize efficiency, we propose a Visual Resolution Router (ViR) that\ndynamically adjusts the resolution of visual tokens without compromising\nperformance. Coupled with ViR, our Decoupled Vision-Language Deployment (DvD)\nstrategy separates the vision encoder and language model across different GPUs,\neffectively balancing computational load. These contributions collectively\nenable InternVL3.5 to achieve up to a +16.0\\% gain in overall reasoning\nperformance and a 4.05times inference speedup compared to its predecessor,\ni.e., InternVL3. In addition, InternVL3.5 supports novel capabilities such as\nGUI interaction and embodied agency. Notably, our largest model, i.e.,\nInternVL3.5-241B-A28B, attains state-of-the-art results among open-source MLLMs\nacross general multimodal, reasoning, text, and agentic tasks -- narrowing the\nperformance gap with leading commercial models like GPT-5. All models and code\nare publicly released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18265.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.18032",
            "authors": [
                {
                    "_id": "68ad236486b21a0e2e358db3",
                    "name": "Yaqi Li",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db4",
                    "name": "Peng Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db5",
                    "name": "Mingyang Han",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db6",
                    "name": "Bu Pi",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db7",
                    "name": "Haoxiang Shi",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db8",
                    "name": "Runzhou Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358db9",
                    "name": "Yang Yao",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358dba",
                    "name": "Xuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad236486b21a0e2e358dbb",
                    "name": "Jun Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T13:53:02.000Z",
            "submittedOnDailyAt": "2025-08-26T01:31:03.330Z",
            "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.",
            "upvotes": 34,
            "discussionId": "68ad236586b21a0e2e358dbc",
            "ai_summary": "The Visual-Chain of Guidance (Visual-CoG) paradigm enhances text-to-image generation by providing stage-aware rewards, improving performance across multiple benchmarks.",
            "ai_keywords": [
                "autoregressive models",
                "text-to-image (T2I) generation",
                "chain-of-thought (CoT)",
                "reinforcement learning (RL)",
                "stage-aware visual synthesis",
                "semantic reasoning",
                "process refining",
                "outcome evaluation",
                "Visual-Chain of Guidance (Visual-CoG)",
                "VisCog-Bench",
                "GenEval",
                "T2I-CompBench"
            ]
        },
        "publishedAt": "2025-08-25T09:53:02.000Z",
        "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance\n  for Text-to-Image Generation",
        "summary": "Despite the promising progress of recent autoregressive models in\ntext-to-image (T2I) generation, their ability to handle multi-attribute and\nambiguous prompts remains limited. To address these limitations, existing works\nhave applied chain-of-thought (CoT) to enable stage-aware visual synthesis and\nemployed reinforcement learning (RL) to improve reasoning capabilities.\nHowever, most models provide reward signals only at the end of the generation\nstage. This monolithic final-only guidance makes it difficult to identify which\nstages contribute positively to the final outcome and may lead to suboptimal\npolicies. To tackle this issue, we propose a Visual-Chain of Guidance\n(Visual-CoG) paradigm consisting of three stages: semantic reasoning, process\nrefining, and outcome evaluation, with stage-aware rewards providing immediate\nguidance throughout the image generation pipeline. We further construct a\nvisual cognition benchmark, VisCog-Bench, which comprises four subtasks to\nevaluate the effectiveness of semantic reasoning. Comprehensive evaluations on\nGenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%,\n5%, and 19%, respectively, demonstrating the superior performance of the\nproposed Visual-CoG. We will release all the resources soon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18032.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.16577",
            "authors": [
                {
                    "_id": "68ac574486b21a0e2e358c17",
                    "user": {
                        "_id": "680645323889e86c69a3daf6",
                        "avatarUrl": "/avatars/3a361a01ee36965b523d73ff292ce1ef.svg",
                        "isPro": false,
                        "fullname": "Yosef Dayani",
                        "user": "yosepyossi",
                        "type": "user"
                    },
                    "name": "Yosef Dayani",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T12:36:57.099Z",
                    "hidden": false
                },
                {
                    "_id": "68ac574486b21a0e2e358c18",
                    "user": {
                        "_id": "64543a1ccd09ceba0e14ecfd",
                        "avatarUrl": "/avatars/d4f3aca9aa8bb4188f68ffd9e0d1f881.svg",
                        "isPro": false,
                        "fullname": "Omer Benishu",
                        "user": "omerbenishu",
                        "type": "user"
                    },
                    "name": "Omer Benishu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T19:31:52.191Z",
                    "hidden": false
                },
                {
                    "_id": "68ac574486b21a0e2e358c19",
                    "user": {
                        "_id": "6345a9b9a8c2ff9f1377faab",
                        "avatarUrl": "/avatars/a5f2b999ef8b967b2af9f41afcd9d475.svg",
                        "isPro": false,
                        "fullname": "Sagie Benaim",
                        "user": "sagiebenaim",
                        "type": "user"
                    },
                    "name": "Sagie Benaim",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T19:31:54.300Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T17:59:40.000Z",
            "submittedOnDailyAt": "2025-08-26T06:12:31.342Z",
            "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
            "submittedOnDailyBy": {
                "_id": "680645323889e86c69a3daf6",
                "avatarUrl": "/avatars/3a361a01ee36965b523d73ff292ce1ef.svg",
                "isPro": false,
                "fullname": "Yosef Dayani",
                "user": "yosepyossi",
                "type": "user"
            },
            "summary": "Text-to-3D generation approaches have advanced significantly by leveraging\npretrained 2D diffusion priors, producing high-quality and 3D-consistent\noutputs. However, they often fail to produce out-of-domain (OOD) or rare\nconcepts, yielding inconsistent or inaccurate results. To this end, we propose\nMV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images\nfrom a large in-the-wild 2D database and then conditions a multiview diffusion\nmodel on these images to synthesize consistent and accurate multiview outputs.\nTraining such a retrieval-conditioned model is achieved via a novel hybrid\nstrategy bridging structured multiview data and diverse 2D image collections.\nThis involves training on multiview data using augmented conditioning views\nthat simulate retrieval variance for view-specific reconstruction, alongside\ntraining on sets of retrieved real-world 2D images using a distinctive held-out\nview prediction objective: the model predicts the held-out view from the other\nviews to infer 3D consistency from 2D data. To facilitate a rigorous OOD\nevaluation, we introduce a new collection of challenging OOD prompts.\nExperiments against state-of-the-art text-to-3D, image-to-3D, and\npersonalization baselines show that our approach significantly improves 3D\nconsistency, photorealism, and text adherence for OOD/rare concepts, while\nmaintaining competitive performance on standard benchmarks.",
            "upvotes": 29,
            "discussionId": "68ac574486b21a0e2e358c1a",
            "projectPage": "https://yosefdayani.github.io/MV-RAG/",
            "githubRepo": "https://github.com/yosefdayani/MV-RAG",
            "ai_summary": "MV-RAG enhances text-to-3D generation by retrieving 2D images and conditioning a multiview diffusion model to improve consistency and accuracy, especially for out-of-domain concepts.",
            "ai_keywords": [
                "pretrained 2D diffusion priors",
                "MV-RAG",
                "multiview diffusion model",
                "retrieval-conditioned model",
                "hybrid strategy",
                "augmented conditioning views",
                "held-out view prediction objective",
                "OOD prompts",
                "text-to-3D",
                "image-to-3D",
                "personalization baselines",
                "3D consistency",
                "photorealism",
                "text adherence"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-08-22T13:59:40.000Z",
        "title": "MV-RAG: Retrieval Augmented Multiview Diffusion",
        "summary": "Text-to-3D generation approaches have advanced significantly by leveraging\npretrained 2D diffusion priors, producing high-quality and 3D-consistent\noutputs. However, they often fail to produce out-of-domain (OOD) or rare\nconcepts, yielding inconsistent or inaccurate results. To this end, we propose\nMV-RAG, a novel text-to-3D pipeline that first retrieves relevant 2D images\nfrom a large in-the-wild 2D database and then conditions a multiview diffusion\nmodel on these images to synthesize consistent and accurate multiview outputs.\nTraining such a retrieval-conditioned model is achieved via a novel hybrid\nstrategy bridging structured multiview data and diverse 2D image collections.\nThis involves training on multiview data using augmented conditioning views\nthat simulate retrieval variance for view-specific reconstruction, alongside\ntraining on sets of retrieved real-world 2D images using a distinctive held-out\nview prediction objective: the model predicts the held-out view from the other\nviews to infer 3D consistency from 2D data. To facilitate a rigorous OOD\nevaluation, we introduce a new collection of challenging OOD prompts.\nExperiments against state-of-the-art text-to-3D, image-to-3D, and\npersonalization baselines show that our approach significantly improves 3D\nconsistency, photorealism, and text adherence for OOD/rare concepts, while\nmaintaining competitive performance on standard benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16577.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "680645323889e86c69a3daf6",
            "avatarUrl": "/avatars/3a361a01ee36965b523d73ff292ce1ef.svg",
            "fullname": "Yosef Dayani",
            "name": "yosepyossi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.17472",
            "authors": [
                {
                    "_id": "68ad304f86b21a0e2e358e22",
                    "user": {
                        "_id": "63640ce5ff4b318d1b7b6f5c",
                        "avatarUrl": "/avatars/9336d1dab1491f3c8e84b1ea287eb891.svg",
                        "isPro": false,
                        "fullname": "Kaiyue Sun",
                        "user": "Kaiyue",
                        "type": "user"
                    },
                    "name": "Kaiyue Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:28.939Z",
                    "hidden": false
                },
                {
                    "_id": "68ad304f86b21a0e2e358e23",
                    "name": "Rongyao Fang",
                    "hidden": false
                },
                {
                    "_id": "68ad304f86b21a0e2e358e24",
                    "name": "Chengqi Duan",
                    "hidden": false
                },
                {
                    "_id": "68ad304f86b21a0e2e358e25",
                    "name": "Xian Liu",
                    "hidden": false
                },
                {
                    "_id": "68ad304f86b21a0e2e358e26",
                    "name": "Xihui Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/RwCZGuI1OB7yLiQgBLmd_.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/1HhE9m118CtQHrrvgQmbo.png",
                "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/KNE5RJ6_y_em3ZRemlUSW.png"
            ],
            "publishedAt": "2025-08-24T17:59:38.000Z",
            "submittedOnDailyAt": "2025-08-26T02:30:30.581Z",
            "title": "T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image\n  Generation",
            "submittedOnDailyBy": {
                "_id": "63640ce5ff4b318d1b7b6f5c",
                "avatarUrl": "/avatars/9336d1dab1491f3c8e84b1ea287eb891.svg",
                "isPro": false,
                "fullname": "Kaiyue Sun",
                "user": "Kaiyue",
                "type": "user"
            },
            "summary": "We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of\ntext-to-image (T2I) models. It consists of four dimensions: Idiom\nInterpretation, Textual Image Design, Entity-Reasoning and\nScientific-Reasoning. We propose a two-stage evaluation protocol to assess the\nreasoning accuracy and image quality. We benchmark various T2I generation\nmodels, and provide comprehensive analysis on their performances.",
            "upvotes": 20,
            "discussionId": "68ad304f86b21a0e2e358e27",
            "githubRepo": "https://github.com/KaiyueSun98/T2I-ReasonBench",
            "ai_summary": "T2I-ReasonBench evaluates the reasoning capabilities of text-to-image models across four dimensions using a two-stage protocol, analyzing their performance comprehensively.",
            "ai_keywords": [
                "text-to-image",
                "T2I",
                "Idiom Interpretation",
                "Textual Image Design",
                "Entity-Reasoning",
                "Scientific-Reasoning",
                "two-stage evaluation protocol"
            ],
            "githubStars": 20
        },
        "publishedAt": "2025-08-24T13:59:38.000Z",
        "title": "T2I-ReasonBench: Benchmarking Reasoning-Informed Text-to-Image\n  Generation",
        "summary": "We propose T2I-ReasonBench, a benchmark evaluating reasoning capabilities of\ntext-to-image (T2I) models. It consists of four dimensions: Idiom\nInterpretation, Textual Image Design, Entity-Reasoning and\nScientific-Reasoning. We propose a two-stage evaluation protocol to assess the\nreasoning accuracy and image quality. We benchmark various T2I generation\nmodels, and provide comprehensive analysis on their performances.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/RwCZGuI1OB7yLiQgBLmd_.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/1HhE9m118CtQHrrvgQmbo.png",
            "https://cdn-uploads.huggingface.co/production/uploads/63640ce5ff4b318d1b7b6f5c/KNE5RJ6_y_em3ZRemlUSW.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17472.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63640ce5ff4b318d1b7b6f5c",
            "avatarUrl": "/avatars/9336d1dab1491f3c8e84b1ea287eb891.svg",
            "fullname": "Kaiyue Sun",
            "name": "Kaiyue",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.18264",
            "authors": [
                {
                    "_id": "68ae0078364411bea07df667",
                    "name": "Sixun Dong",
                    "hidden": false
                },
                {
                    "_id": "68ae0078364411bea07df668",
                    "name": "Juhua Hu",
                    "hidden": false
                },
                {
                    "_id": "68ae0078364411bea07df669",
                    "name": "Mian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ae0078364411bea07df66a",
                    "name": "Ming Yin",
                    "hidden": false
                },
                {
                    "_id": "68ae0078364411bea07df66b",
                    "name": "Yanjie Fu",
                    "hidden": false
                },
                {
                    "_id": "68ae0078364411bea07df66c",
                    "name": "Qi Qian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T17:57:49.000Z",
            "submittedOnDailyAt": "2025-08-26T17:18:01.869Z",
            "title": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs",
            "submittedOnDailyBy": {
                "_id": "653281ba1995cee54ac7ac4d",
                "avatarUrl": "/avatars/077197d7b00502216b8edb1c87544791.svg",
                "isPro": false,
                "fullname": "Sixun Dong",
                "user": "Ironieser",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) demonstrate impressive performance in\nunderstanding visual content with language instruction by converting visual\ninput to vision tokens. However, redundancy in vision tokens results in the\ndegenerated inference efficiency of VLMs. While many algorithms have been\nproposed to reduce the number of vision tokens, most of them apply only\nunimodal information (i.e., vision/text) for pruning and ignore the inherent\nmultimodal property of vision-language tasks. Moreover, it lacks a generic\ncriterion that can be applied to different modalities. To mitigate this\nlimitation, in this work, we propose to leverage both vision and text tokens to\nselect informative vision tokens by the criterion of coverage. We first\nformulate the subset selection problem as a maximum coverage problem.\nAfterward, a subset of vision tokens is optimized to cover the text tokens and\nthe original set of vision tokens, simultaneously. Finally, a VLM agent can be\nadopted to further improve the quality of text tokens for guiding vision\npruning. The proposed method MMTok is extensively evaluated on benchmark\ndatasets with different VLMs. The comparison illustrates that vision and text\ninformation are complementary, and combining multimodal information can surpass\nthe unimodal baseline with a clear margin. Moreover, under the maximum coverage\ncriterion on the POPE dataset, our method achieves a 1.87x speedup while\nmaintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,\nwith only four vision tokens, it still preserves 87.7% of the original\nperformance on LLaVA-1.5-7B. These results highlight the effectiveness of\ncoverage in token selection.",
            "upvotes": 19,
            "discussionId": "68ae0079364411bea07df66d",
            "ai_summary": "A multimodal method leverages both vision and text tokens to optimize vision token selection, improving inference efficiency in vision-language models.",
            "ai_keywords": [
                "vision tokens",
                "text tokens",
                "multimodal property",
                "maximum coverage problem",
                "VLM agent",
                "POPE dataset",
                "LLaVA-NeXT-13B",
                "LLaVA-1.5-7B"
            ]
        },
        "publishedAt": "2025-08-25T13:57:49.000Z",
        "title": "MMTok: Multimodal Coverage Maximization for Efficient Inference of VLMs",
        "summary": "Vision-Language Models (VLMs) demonstrate impressive performance in\nunderstanding visual content with language instruction by converting visual\ninput to vision tokens. However, redundancy in vision tokens results in the\ndegenerated inference efficiency of VLMs. While many algorithms have been\nproposed to reduce the number of vision tokens, most of them apply only\nunimodal information (i.e., vision/text) for pruning and ignore the inherent\nmultimodal property of vision-language tasks. Moreover, it lacks a generic\ncriterion that can be applied to different modalities. To mitigate this\nlimitation, in this work, we propose to leverage both vision and text tokens to\nselect informative vision tokens by the criterion of coverage. We first\nformulate the subset selection problem as a maximum coverage problem.\nAfterward, a subset of vision tokens is optimized to cover the text tokens and\nthe original set of vision tokens, simultaneously. Finally, a VLM agent can be\nadopted to further improve the quality of text tokens for guiding vision\npruning. The proposed method MMTok is extensively evaluated on benchmark\ndatasets with different VLMs. The comparison illustrates that vision and text\ninformation are complementary, and combining multimodal information can surpass\nthe unimodal baseline with a clear margin. Moreover, under the maximum coverage\ncriterion on the POPE dataset, our method achieves a 1.87x speedup while\nmaintaining 98.7% of the original performance on LLaVA-NeXT-13B. Furthermore,\nwith only four vision tokens, it still preserves 87.7% of the original\nperformance on LLaVA-1.5-7B. These results highlight the effectiveness of\ncoverage in token selection.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18264.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "653281ba1995cee54ac7ac4d",
            "avatarUrl": "/avatars/077197d7b00502216b8edb1c87544791.svg",
            "fullname": "Sixun Dong",
            "name": "Ironieser",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.16949",
            "authors": [
                {
                    "_id": "68ad2b3c86b21a0e2e358de3",
                    "user": {
                        "_id": "670ff71f6b8de497472a81dc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670ff71f6b8de497472a81dc/R2ydPAk6GU5Sk81Qja7E_.jpeg",
                        "isPro": false,
                        "fullname": "YANG ZHOU",
                        "user": "BAOLONGZHANSHEN",
                        "type": "user"
                    },
                    "name": "Yang Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:31.033Z",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358de4",
                    "name": "Sunzhu Li",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358de5",
                    "name": "Shunyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358de6",
                    "name": "Wenkai Fang",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358de7",
                    "name": "Jiale Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358de8",
                    "name": "Jingwen Yang",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358de9",
                    "name": "Jianwei Lv",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358dea",
                    "name": "Kongcheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358deb",
                    "name": "Yihe Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358dec",
                    "name": "Hengtong Lu",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358ded",
                    "name": "Wei Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358dee",
                    "name": "Yan Xie",
                    "hidden": false
                },
                {
                    "_id": "68ad2b3c86b21a0e2e358def",
                    "name": "Mingli Song",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-23T08:47:31.000Z",
            "submittedOnDailyAt": "2025-08-26T02:08:07.486Z",
            "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
            "submittedOnDailyBy": {
                "_id": "670ff71f6b8de497472a81dc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670ff71f6b8de497472a81dc/R2ydPAk6GU5Sk81Qja7E_.jpeg",
                "isPro": false,
                "fullname": "YANG ZHOU",
                "user": "BAOLONGZHANSHEN",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3.",
            "upvotes": 17,
            "discussionId": "68ad2b3c86b21a0e2e358df0",
            "ai_summary": "RuscaRL, a novel instructional scaffolding framework, enhances LLM reasoning by using rubrics for exploration and verifiable rewards, significantly improving performance on reasoning tasks.",
            "ai_keywords": [
                "Large Language Models",
                "Reinforcement Learning",
                "Rubric-Scaffolded Reinforcement Learning",
                "RuscaRL",
                "checklist-style rubrics",
                "rollout generation",
                "LLM-as-a-Judge",
                "best-of-N evaluation",
                "HealthBench-500",
                "Qwen-2.5-7B-Instruct",
                "GPT-4.1",
                "Qwen3-30B-A3B-Instruct",
                "OpenAI-o3"
            ]
        },
        "publishedAt": "2025-08-23T04:47:31.000Z",
        "title": "Breaking the Exploration Bottleneck: Rubric-Scaffolded Reinforcement\n  Learning for General LLM Reasoning",
        "summary": "Recent advances in Large Language Models (LLMs) have underscored the\npotential of Reinforcement Learning (RL) to facilitate the emergence of\nreasoning capabilities. Despite the encouraging results, a fundamental dilemma\npersists as RL improvement relies on learning from high-quality samples, yet\nthe exploration for such samples remains bounded by the inherent limitations of\nLLMs. This, in effect, creates an undesirable cycle in which what cannot be\nexplored cannot be learned. In this work, we propose Rubric-Scaffolded\nReinforcement Learning (RuscaRL), a novel instructional scaffolding framework\ndesigned to break the exploration bottleneck for general LLM reasoning.\nSpecifically, RuscaRL introduces checklist-style rubrics as (1) explicit\nscaffolding for exploration during rollout generation, where different rubrics\nare provided as external guidance within task instructions to steer diverse\nhigh-quality responses. This guidance is gradually decayed over time,\nencouraging the model to internalize the underlying reasoning patterns; (2)\nverifiable rewards for exploitation during model training, where we can obtain\nrobust LLM-as-a-Judge scores using rubrics as references, enabling effective RL\non general reasoning tasks. Extensive experiments demonstrate the superiority\nof the proposed RuscaRL across various benchmarks, effectively expanding\nreasoning boundaries under the best-of-N evaluation. Notably, RuscaRL\nsignificantly boosts Qwen-2.5-7B-Instruct from 23.6 to 50.3 on HealthBench-500,\nsurpassing GPT-4.1. Furthermore, our fine-tuned variant on\nQwen3-30B-A3B-Instruct achieves 61.1 on HealthBench-500, outperforming leading\nLLMs including OpenAI-o3.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16949.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "670ff71f6b8de497472a81dc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/670ff71f6b8de497472a81dc/R2ydPAk6GU5Sk81Qja7E_.jpeg",
            "fullname": "YANG ZHOU",
            "name": "BAOLONGZHANSHEN",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.16745",
            "authors": [
                {
                    "_id": "68ad769486b21a0e2e358f20",
                    "user": {
                        "_id": "63c1ac8cc58fcfeac186bda2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63c1ac8cc58fcfeac186bda2/MajMhLgsvCBN81tmO9PZc.jpeg",
                        "isPro": false,
                        "fullname": "Ivan Rodkin",
                        "user": "irodkin",
                        "type": "user"
                    },
                    "name": "Ivan Rodkin",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:10.644Z",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f21",
                    "name": "Daniil Orel",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f22",
                    "name": "Konstantin Smirnov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f23",
                    "name": "Arman Bolatov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f24",
                    "user": {
                        "_id": "64219641eaad1bcb28af7d2b",
                        "avatarUrl": "/avatars/8edc9e4f889832f4b25e296b2454ce8c.svg",
                        "isPro": true,
                        "fullname": "BIlal Elbouardi",
                        "user": "b1l4lx1",
                        "type": "user"
                    },
                    "name": "Bilal Elbouardi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T11:37:55.584Z",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f25",
                    "name": "Besher Hassan",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f26",
                    "user": {
                        "_id": "618b9540682ec1c38327e586",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/618b9540682ec1c38327e586/v_ZBkfh8O9Zh6C2YQpuBX.jpeg",
                        "isPro": false,
                        "fullname": "Yury Kuratov",
                        "user": "yurakuratov",
                        "type": "user"
                    },
                    "name": "Yuri Kuratov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:12.960Z",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f27",
                    "name": "Aydar Bulatov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f28",
                    "name": "Preslav Nakov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f29",
                    "name": "Timothy Baldwin",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f2a",
                    "name": "Artem Shelmanov",
                    "hidden": false
                },
                {
                    "_id": "68ad769486b21a0e2e358f2b",
                    "user": {
                        "_id": "639c6e978a34ed9a404c6a7b",
                        "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
                        "isPro": false,
                        "fullname": "MIKHAIL BURTSEV",
                        "user": "mbur",
                        "type": "user"
                    },
                    "name": "Mikhail Burtsev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:16.139Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/639c6e978a34ed9a404c6a7b/-_DXOswK6JJc0jmMdFlbP.png"
            ],
            "publishedAt": "2025-08-22T18:57:08.000Z",
            "submittedOnDailyAt": "2025-08-26T07:29:32.605Z",
            "title": "Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory\n  and Test-Time Compute Scaling",
            "submittedOnDailyBy": {
                "_id": "639c6e978a34ed9a404c6a7b",
                "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
                "isPro": false,
                "fullname": "MIKHAIL BURTSEV",
                "user": "mbur",
                "type": "user"
            },
            "summary": "Reasoning is a core capability of large language models, yet understanding\nhow they learn and perform multi-step reasoning remains an open problem. In\nthis study, we explore how different architectures and training methods affect\nmodel multi-step reasoning capabilities within a cellular automata framework.\nBy training on state sequences generated with random Boolean functions for\nrandom initial conditions to exclude memorization, we demonstrate that most\nneural architectures learn to abstract the underlying rules. While models\nachieve high accuracy in next-state prediction, their performance declines\nsharply if multi-step reasoning is required. We confirm that increasing model\ndepth plays a crucial role for sequential computations. We demonstrate that an\nextension of the effective model depth with recurrence, memory, and test-time\ncompute scaling substantially enhances reasoning capabilities.",
            "upvotes": 15,
            "discussionId": "68ad769486b21a0e2e358f2c",
            "githubRepo": "https://github.com/RodkinIvan/associative-recurrent-memory-transformer/tree/ACT",
            "ai_summary": "Models trained on random Boolean functions in a cellular automata framework show that increasing depth, recurrence, memory, and test-time compute scaling enhances multi-step reasoning capabilities.",
            "ai_keywords": [
                "cellular automata",
                "Boolean functions",
                "multi-step reasoning",
                "model depth",
                "recurrence",
                "memory",
                "test-time compute scaling"
            ],
            "githubStars": 45
        },
        "publishedAt": "2025-08-22T14:57:08.000Z",
        "title": "Beyond Memorization: Extending Reasoning Depth with Recurrence, Memory\n  and Test-Time Compute Scaling",
        "summary": "Reasoning is a core capability of large language models, yet understanding\nhow they learn and perform multi-step reasoning remains an open problem. In\nthis study, we explore how different architectures and training methods affect\nmodel multi-step reasoning capabilities within a cellular automata framework.\nBy training on state sequences generated with random Boolean functions for\nrandom initial conditions to exclude memorization, we demonstrate that most\nneural architectures learn to abstract the underlying rules. While models\nachieve high accuracy in next-state prediction, their performance declines\nsharply if multi-step reasoning is required. We confirm that increasing model\ndepth plays a crucial role for sequential computations. We demonstrate that an\nextension of the effective model depth with recurrence, memory, and test-time\ncompute scaling substantially enhances reasoning capabilities.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/639c6e978a34ed9a404c6a7b/-_DXOswK6JJc0jmMdFlbP.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16745.png",
        "numComments": 9,
        "submittedBy": {
            "_id": "639c6e978a34ed9a404c6a7b",
            "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
            "fullname": "MIKHAIL BURTSEV",
            "name": "mbur",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.18255",
            "authors": [
                {
                    "_id": "68ad15b786b21a0e2e358d23",
                    "user": {
                        "_id": "6317aade83d8d2fd903192d9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6317aade83d8d2fd903192d9/erOwgMXc_CZih3uMoyTAp.jpeg",
                        "isPro": false,
                        "fullname": "Teknium",
                        "user": "teknium",
                        "type": "user"
                    },
                    "name": "Ryan Teknium",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T19:06:13.921Z",
                    "hidden": false
                },
                {
                    "_id": "68ad15b786b21a0e2e358d24",
                    "name": "Roger Jin",
                    "hidden": false
                },
                {
                    "_id": "68ad15b786b21a0e2e358d25",
                    "name": "Jai Suphavadeeprasit",
                    "hidden": false
                },
                {
                    "_id": "68ad15b786b21a0e2e358d26",
                    "name": "Dakota Mahan",
                    "hidden": false
                },
                {
                    "_id": "68ad15b786b21a0e2e358d27",
                    "user": {
                        "_id": "630581db99870e13d3e0006f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1676652577978-630581db99870e13d3e0006f.jpeg",
                        "isPro": true,
                        "fullname": "Jeffrey Quesnelle",
                        "user": "emozilla",
                        "type": "user"
                    },
                    "name": "Jeffrey Quesnelle",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T19:06:11.740Z",
                    "hidden": false
                },
                {
                    "_id": "68ad15b786b21a0e2e358d28",
                    "name": "Joe Li",
                    "hidden": false
                },
                {
                    "_id": "68ad15b786b21a0e2e358d29",
                    "name": "Chen Guang",
                    "hidden": false
                },
                {
                    "_id": "68ad15b786b21a0e2e358d2a",
                    "name": "Shannon Sands",
                    "hidden": false
                },
                {
                    "_id": "68ad15b786b21a0e2e358d2b",
                    "name": "Karan Malhotra",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T17:45:06.000Z",
            "submittedOnDailyAt": "2025-08-26T19:15:16.795Z",
            "title": "Hermes 4 Technical Report",
            "submittedOnDailyBy": {
                "_id": "64676c81e7a6a374fd181110",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64676c81e7a6a374fd181110/HEDlAT9FYhJF9Nw__LLNI.jpeg",
                "isPro": true,
                "fullname": "Sumuk Shashidhar",
                "user": "sumuks",
                "type": "user"
            },
            "summary": "We present Hermes 4, a family of hybrid reasoning models that combine\nstructured, multi-turn reasoning with broad instruction-following ability. We\ndescribe the challenges encountered during data curation, synthesis, training,\nand evaluation, and outline the solutions employed to address these challenges\nat scale. We comprehensively evaluate across mathematical reasoning, coding,\nknowledge, comprehension, and alignment benchmarks, and we report both\nquantitative performance and qualitative behavioral analysis. To support open\nresearch, all model weights are published publicly at\nhttps://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728",
            "upvotes": 10,
            "discussionId": "68ad15b886b21a0e2e358d2c",
            "projectPage": "https://hermes4.nousresearch.com/",
            "ai_summary": "Hermes 4, a hybrid reasoning model, integrates structured multi-turn reasoning with broad instruction-following, evaluated across various benchmarks including math, coding, knowledge, comprehension, and alignment.",
            "ai_keywords": [
                "hybrid reasoning models",
                "structured reasoning",
                "multi-turn reasoning",
                "instruction-following",
                "data curation",
                "data synthesis",
                "model training",
                "model evaluation",
                "mathematical reasoning",
                "coding",
                "knowledge benchmarks",
                "comprehension benchmarks",
                "alignment benchmarks"
            ]
        },
        "publishedAt": "2025-08-25T13:45:06.000Z",
        "title": "Hermes 4 Technical Report",
        "summary": "We present Hermes 4, a family of hybrid reasoning models that combine\nstructured, multi-turn reasoning with broad instruction-following ability. We\ndescribe the challenges encountered during data curation, synthesis, training,\nand evaluation, and outline the solutions employed to address these challenges\nat scale. We comprehensively evaluate across mathematical reasoning, coding,\nknowledge, comprehension, and alignment benchmarks, and we report both\nquantitative performance and qualitative behavioral analysis. To support open\nresearch, all model weights are published publicly at\nhttps://huggingface.co/collections/NousResearch/hermes-4-collection-68a731bfd452e20816725728",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18255.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64676c81e7a6a374fd181110",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64676c81e7a6a374fd181110/HEDlAT9FYhJF9Nw__LLNI.jpeg",
            "fullname": "Sumuk Shashidhar",
            "name": "sumuks",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 26
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.17188",
            "authors": [
                {
                    "_id": "68ad136786b21a0e2e358d1c",
                    "user": {
                        "_id": "668aa67dbdc7e8332bb8fa53",
                        "avatarUrl": "/avatars/2ab0b884910ca19953d91a040b8b4d97.svg",
                        "isPro": true,
                        "fullname": "Hadlay Zhang",
                        "user": "HadlayZ",
                        "type": "user"
                    },
                    "name": "Zhilin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:25.983Z",
                    "hidden": false
                },
                {
                    "_id": "68ad136786b21a0e2e358d1d",
                    "name": "Xiang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad136786b21a0e2e358d1e",
                    "user": {
                        "_id": "6539bc7756c9b35961021fa8",
                        "avatarUrl": "/avatars/b0140589c0a435c903c93d93a1a6ee8b.svg",
                        "isPro": false,
                        "fullname": "Jiaqi Wei",
                        "user": "VitaCoco",
                        "type": "user"
                    },
                    "name": "Jiaqi Wei",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:28.158Z",
                    "hidden": false
                },
                {
                    "_id": "68ad136786b21a0e2e358d1f",
                    "name": "Yiwei Xu",
                    "hidden": false
                },
                {
                    "_id": "68ad136786b21a0e2e358d20",
                    "name": "Chenyu You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-24T02:25:45.000Z",
            "submittedOnDailyAt": "2025-08-26T00:23:03.761Z",
            "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "656553d89bf6665f10e3a92d",
                "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                "isPro": false,
                "fullname": "xiang wyatt zhang",
                "user": "Wyattz23",
                "type": "user"
            },
            "summary": "Multi-agent systems built upon large language models (LLMs) have demonstrated\nremarkable capabilities in tackling complex compositional tasks. In this work,\nwe apply this paradigm to the paper-to-poster generation problem, a practical\nyet time-consuming process faced by researchers preparing for conferences.\nWhile recent approaches have attempted to automate this task, most neglect core\ndesign and aesthetic principles, resulting in posters that require substantial\nmanual refinement. To address these design limitations, we propose PosterGen, a\nmulti-agent framework that mirrors the workflow of professional poster\ndesigners. It consists of four collaborative specialized agents: (1) Parser and\nCurator agents extract content from the paper and organize storyboard; (2)\nLayout agent maps the content into a coherent spatial layout; (3) Stylist\nagents apply visual design elements such as color and typography; and (4)\nRenderer composes the final poster. Together, these agents produce posters that\nare both semantically grounded and visually appealing. To evaluate design\nquality, we introduce a vision-language model (VLM)-based rubric that measures\nlayout balance, readability, and aesthetic coherence. Experimental results show\nthat PosterGen consistently matches in content fidelity, and significantly\noutperforms existing methods in visual designs, generating posters that are\npresentation-ready with minimal human refinements.",
            "upvotes": 9,
            "discussionId": "68ad136786b21a0e2e358d21",
            "projectPage": "https://y-research-sbu.github.io/PosterGen/",
            "githubRepo": "https://github.com/Y-Research-SBU/PosterGen",
            "ai_summary": "PosterGen, a multi-agent framework using large language models, automates paper-to-poster generation with high design quality and minimal manual refinement.",
            "ai_keywords": [
                "multi-agent systems",
                "large language models",
                "paper-to-poster generation",
                "Parser",
                "Curator",
                "Layout agent",
                "Stylist agents",
                "Renderer",
                "vision-language model",
                "VLM",
                "layout balance",
                "readability",
                "aesthetic coherence"
            ],
            "githubStars": 36
        },
        "publishedAt": "2025-08-23T22:25:45.000Z",
        "title": "PosterGen: Aesthetic-Aware Paper-to-Poster Generation via Multi-Agent\n  LLMs",
        "summary": "Multi-agent systems built upon large language models (LLMs) have demonstrated\nremarkable capabilities in tackling complex compositional tasks. In this work,\nwe apply this paradigm to the paper-to-poster generation problem, a practical\nyet time-consuming process faced by researchers preparing for conferences.\nWhile recent approaches have attempted to automate this task, most neglect core\ndesign and aesthetic principles, resulting in posters that require substantial\nmanual refinement. To address these design limitations, we propose PosterGen, a\nmulti-agent framework that mirrors the workflow of professional poster\ndesigners. It consists of four collaborative specialized agents: (1) Parser and\nCurator agents extract content from the paper and organize storyboard; (2)\nLayout agent maps the content into a coherent spatial layout; (3) Stylist\nagents apply visual design elements such as color and typography; and (4)\nRenderer composes the final poster. Together, these agents produce posters that\nare both semantically grounded and visually appealing. To evaluate design\nquality, we introduce a vision-language model (VLM)-based rubric that measures\nlayout balance, readability, and aesthetic coherence. Experimental results show\nthat PosterGen consistently matches in content fidelity, and significantly\noutperforms existing methods in visual designs, generating posters that are\npresentation-ready with minimal human refinements.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17188.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "656553d89bf6665f10e3a92d",
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "fullname": "xiang wyatt zhang",
            "name": "Wyattz23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.17580",
            "authors": [
                {
                    "_id": "68ad09b186b21a0e2e358cf5",
                    "name": "Fan Nie",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cf6",
                    "user": {
                        "_id": "64f135ed1e05348eda3edd33",
                        "avatarUrl": "/avatars/69f9d17711c3c619c14e1dce53cebd9d.svg",
                        "isPro": false,
                        "fullname": "Ken Liu",
                        "user": "kzliu",
                        "type": "user"
                    },
                    "name": "Ken Ziyu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:50:32.267Z",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cf7",
                    "name": "Zihao Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cf8",
                    "name": "Rui Sun",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cf9",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cfa",
                    "name": "Weijia Shi",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cfb",
                    "name": "Huaxiu Yao",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cfc",
                    "name": "Linjun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cfd",
                    "name": "Andrew Y. Ng",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cfe",
                    "name": "James Zou",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358cff",
                    "name": "Sanmi Koyejo",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358d00",
                    "name": "Yejin Choi",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358d01",
                    "name": "Percy Liang",
                    "hidden": false
                },
                {
                    "_id": "68ad09b186b21a0e2e358d02",
                    "name": "Niklas Muennighoff",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5f1eb362eec0ad2a071ad6e2/EIc8-2VZpt0d-mBAk6K1z.png"
            ],
            "publishedAt": "2025-08-25T01:07:59.000Z",
            "submittedOnDailyAt": "2025-08-26T00:31:47.088Z",
            "title": "UQ: Assessing Language Models on Unsolved Questions",
            "submittedOnDailyBy": {
                "_id": "5f1eb362eec0ad2a071ad6e2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
                "isPro": false,
                "fullname": "Niklas Muennighoff",
                "user": "Muennighoff",
                "type": "user"
            },
            "summary": "Benchmarks shape progress in AI research. A useful benchmark should be both\ndifficult and realistic: questions should challenge frontier models while also\nreflecting real-world usage. Yet, current paradigms face a difficulty-realism\ntension: exam-style benchmarks are often made artificially difficult with\nlimited real-world value, while benchmarks based on real user interaction often\nskew toward easy, high-frequency problems. In this work, we explore a radically\ndifferent paradigm: assessing models on unsolved questions. Rather than a\nstatic benchmark scored once, we curate unsolved questions and evaluate models\nasynchronously over time with validator-assisted screening and community\nverification. We introduce UQ, a testbed of 500 challenging, diverse questions\nsourced from Stack Exchange, spanning topics from CS theory and math to sci-fi\nand history, probing capabilities including reasoning, factuality, and\nbrowsing. UQ is difficult and realistic by construction: unsolved questions are\noften hard and naturally arise when humans seek answers, thus solving them\nyields direct real-world value. Our contributions are threefold: (1) UQ-Dataset\nand its collection pipeline combining rule-based filters, LLM judges, and human\nreview to ensure question quality (e.g., well-defined and difficult); (2)\nUQ-Validators, compound validation strategies that leverage the\ngenerator-validator gap to provide evaluation signals and pre-screen candidate\nsolutions for human review; and (3) UQ-Platform, an open platform where experts\ncollectively verify questions and solutions. The top model passes UQ-validation\non only 15% of questions, and preliminary human verification has already\nidentified correct answers among those that passed. UQ charts a path for\nevaluating frontier models on real-world, open-ended challenges, where success\npushes the frontier of human knowledge. We release UQ at\nhttps://uq.stanford.edu.",
            "upvotes": 8,
            "discussionId": "68ad09b286b21a0e2e358d03",
            "projectPage": "https://uq.stanford.edu",
            "githubRepo": "https://github.com/uq-project/UQ",
            "ai_summary": "UQ is a benchmark for evaluating AI models on unsolved questions, combining difficulty and realism to assess capabilities like reasoning, factuality, and browsing.",
            "ai_keywords": [
                "unsolved questions",
                "UQ",
                "UQ-Dataset",
                "UQ-Validators",
                "UQ-Platform",
                "generator-validator gap",
                "human verification",
                "reasoning",
                "factuality",
                "browsing"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-08-24T21:07:59.000Z",
        "title": "UQ: Assessing Language Models on Unsolved Questions",
        "summary": "Benchmarks shape progress in AI research. A useful benchmark should be both\ndifficult and realistic: questions should challenge frontier models while also\nreflecting real-world usage. Yet, current paradigms face a difficulty-realism\ntension: exam-style benchmarks are often made artificially difficult with\nlimited real-world value, while benchmarks based on real user interaction often\nskew toward easy, high-frequency problems. In this work, we explore a radically\ndifferent paradigm: assessing models on unsolved questions. Rather than a\nstatic benchmark scored once, we curate unsolved questions and evaluate models\nasynchronously over time with validator-assisted screening and community\nverification. We introduce UQ, a testbed of 500 challenging, diverse questions\nsourced from Stack Exchange, spanning topics from CS theory and math to sci-fi\nand history, probing capabilities including reasoning, factuality, and\nbrowsing. UQ is difficult and realistic by construction: unsolved questions are\noften hard and naturally arise when humans seek answers, thus solving them\nyields direct real-world value. Our contributions are threefold: (1) UQ-Dataset\nand its collection pipeline combining rule-based filters, LLM judges, and human\nreview to ensure question quality (e.g., well-defined and difficult); (2)\nUQ-Validators, compound validation strategies that leverage the\ngenerator-validator gap to provide evaluation signals and pre-screen candidate\nsolutions for human review; and (3) UQ-Platform, an open platform where experts\ncollectively verify questions and solutions. The top model passes UQ-validation\non only 15% of questions, and preliminary human verification has already\nidentified correct answers among those that passed. UQ charts a path for\nevaluating frontier models on real-world, open-ended challenges, where success\npushes the frontier of human knowledge. We release UQ at\nhttps://uq.stanford.edu.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5f1eb362eec0ad2a071ad6e2/EIc8-2VZpt0d-mBAk6K1z.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17580.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1eb362eec0ad2a071ad6e2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5f1eb362eec0ad2a071ad6e2/IXMYkYKuTwn6kBdWnQeeY.png",
            "fullname": "Niklas Muennighoff",
            "name": "Muennighoff",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 146
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.17290",
            "authors": [
                {
                    "_id": "68ad5e2586b21a0e2e358ecd",
                    "user": {
                        "_id": "6250819a5bf543dbd2607081",
                        "avatarUrl": "/avatars/dfade22a578b8349ab544acda5e8bcad.svg",
                        "isPro": false,
                        "fullname": "Omid Ghahroodi",
                        "user": "omidgh",
                        "type": "user"
                    },
                    "name": "Omid Ghahroodi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:22.462Z",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ece",
                    "user": {
                        "_id": "6466bd41425875eca92aadd7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6466bd41425875eca92aadd7/T0OuXplQe20fNWAZUptgr.jpeg",
                        "isPro": false,
                        "fullname": "Arshia Hemmat",
                        "user": "arshiahemmat",
                        "type": "user"
                    },
                    "name": "Arshia Hemmat",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T19:05:52.635Z",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ecf",
                    "name": "Marzia Nouri",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ed0",
                    "name": "Seyed Mohammad Hadi Hosseini",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ed1",
                    "name": "Doratossadat Dastgheib",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ed2",
                    "name": "Mohammad Vali Sanian",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ed3",
                    "user": {
                        "_id": "61ccc2c6176cbfa298afbd58",
                        "avatarUrl": "/avatars/9f89a9190bd4cc402b0b6b0f7abefcf6.svg",
                        "isPro": false,
                        "fullname": "Alireza Sahebi",
                        "user": "AlirezaSahebi",
                        "type": "user"
                    },
                    "name": "Alireza Sahebi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T19:05:49.119Z",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ed4",
                    "name": "Reihaneh Zohrabi",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ed5",
                    "name": "Mohammad Hossein Rohban",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ed6",
                    "name": "Ehsaneddin Asgari",
                    "hidden": false
                },
                {
                    "_id": "68ad5e2586b21a0e2e358ed7",
                    "name": "Mahdieh Soleymani Baghshah",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6250819a5bf543dbd2607081/lHOYrDxy_TTmQUIShbLZL.png"
            ],
            "publishedAt": "2025-08-24T10:32:37.000Z",
            "submittedOnDailyAt": "2025-08-26T05:43:12.601Z",
            "title": "MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for\n  N-level Assessment",
            "submittedOnDailyBy": {
                "_id": "6250819a5bf543dbd2607081",
                "avatarUrl": "/avatars/dfade22a578b8349ab544acda5e8bcad.svg",
                "isPro": false,
                "fullname": "Omid Ghahroodi",
                "user": "omidgh",
                "type": "user"
            },
            "summary": "Recent advancements in large vision-language models (VLMs) have primarily\nfocused on English, with limited attention given to other languages. To address\nthis gap, we introduce MEENA (also known as PersianMMMU), the first dataset\ndesigned to evaluate Persian VLMs across scientific, reasoning, and human-level\nunderstanding tasks. Our dataset comprises approximately 7,500 Persian and\n3,000 English questions, covering a wide range of topics such as reasoning,\nmathematics, physics, diagrams, charts, and Persian art and literature. Key\nfeatures of MEENA include: (1) diverse subject coverage spanning various\neducational levels, from primary to upper secondary school, (2) rich metadata,\nincluding difficulty levels and descriptive answers, (3) original Persian data\nthat preserves cultural nuances, (4) a bilingual structure to assess\ncross-linguistic performance, and (5) a series of diverse experiments assessing\nvarious capabilities, including overall performance, the model's ability to\nattend to images, and its tendency to generate hallucinations. We hope this\nbenchmark contributes to enhancing VLM capabilities beyond English.",
            "upvotes": 6,
            "discussionId": "68ad5e2586b21a0e2e358ed8",
            "ai_summary": "MEENA, a Persian-English dataset, evaluates vision-language models across scientific, reasoning, and human-level understanding tasks, enhancing capabilities beyond English.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "Persian",
                "MEENA",
                "PersianMMMU",
                "bilingual structure",
                "cross-linguistic performance",
                "hallucinations"
            ]
        },
        "publishedAt": "2025-08-24T06:32:37.000Z",
        "title": "MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for\n  N-level Assessment",
        "summary": "Recent advancements in large vision-language models (VLMs) have primarily\nfocused on English, with limited attention given to other languages. To address\nthis gap, we introduce MEENA (also known as PersianMMMU), the first dataset\ndesigned to evaluate Persian VLMs across scientific, reasoning, and human-level\nunderstanding tasks. Our dataset comprises approximately 7,500 Persian and\n3,000 English questions, covering a wide range of topics such as reasoning,\nmathematics, physics, diagrams, charts, and Persian art and literature. Key\nfeatures of MEENA include: (1) diverse subject coverage spanning various\neducational levels, from primary to upper secondary school, (2) rich metadata,\nincluding difficulty levels and descriptive answers, (3) original Persian data\nthat preserves cultural nuances, (4) a bilingual structure to assess\ncross-linguistic performance, and (5) a series of diverse experiments assessing\nvarious capabilities, including overall performance, the model's ability to\nattend to images, and its tendency to generate hallucinations. We hope this\nbenchmark contributes to enhancing VLM capabilities beyond English.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6250819a5bf543dbd2607081/lHOYrDxy_TTmQUIShbLZL.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17290.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6250819a5bf543dbd2607081",
            "avatarUrl": "/avatars/dfade22a578b8349ab544acda5e8bcad.svg",
            "fullname": "Omid Ghahroodi",
            "name": "omidgh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.19201",
            "authors": [
                {
                    "_id": "68ae5c89364411bea07df6ba",
                    "name": "Heng Lin",
                    "hidden": false
                },
                {
                    "_id": "68ae5c89364411bea07df6bb",
                    "name": "Zhongwen Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-26T17:03:46.000Z",
            "submittedOnDailyAt": "2025-08-26T23:48:37.940Z",
            "title": "Understanding Tool-Integrated Reasoning",
            "submittedOnDailyBy": {
                "_id": "672db76fa34d64e774fc42c9",
                "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
                "isPro": false,
                "fullname": "Zhongwen Xu",
                "user": "zhongwenxu",
                "type": "user"
            },
            "summary": "We study why Tool-Integrated Reasoning (TIR) makes Large Language Models\n(LLMs) more capable. While LLMs integrated with tools like Python code\ninterpreters show great promise, a principled theory explaining why this\nparadigm is effective has been missing. This work provides the first formal\nproof that TIR fundamentally expands an LLM's capabilities. We demonstrate that\ntools enable a strict expansion of the model's empirical and feasible support,\nbreaking the capability ceiling of pure-text models by unlocking\nproblem-solving strategies that are otherwise impossible or intractably\nverbose. To guide model behavior without compromising training stability and\nperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), a\nnovel algorithm that directly modifies the advantage function to guide the\npolicy behavior. We conduct comprehensive experiments on challenging\nmathematical benchmarks, leveraging a Python interpreter as the external tool.\nOur results show that the TIR model decisively outperforms its pure-text\ncounterpart on the pass@k metric. Crucially, this advantage is not confined to\ncomputationally-intensive problems but extends to those requiring significant\nabstract insight. We further identify the emergent cognitive patterns that\nillustrate how models learn to think with tools. Finally, we report improved\ntool usage behavior with early code invocation and much more interactive turns\nwith ASPO. Overall, our work provides the first principled explanation for\nTIR's success, shifting the focus from the mere fact that tools work to why and\nhow they enable more powerful reasoning.",
            "upvotes": 3,
            "discussionId": "68ae5c89364411bea07df6bc",
            "projectPage": "https://zhongwenxu.notion.site/Understanding-Tool-Integrated-Reasoning-2551c4e140e3805489fadcc802a1ea83",
            "ai_summary": "Tool-Integrated Reasoning (TIR) enhances Large Language Models (LLMs) by expanding their problem-solving capabilities through the use of external tools, and Advantage Shaping Policy Optimization (ASPO) improves model behavior and tool usage.",
            "ai_keywords": [
                "Tool-Integrated Reasoning",
                "Large Language Models",
                "LLMs",
                "Advantage Shaping Policy Optimization",
                "ASPO",
                "empirical support",
                "feasible support",
                "capability ceiling",
                "problem-solving strategies",
                "pass@k metric",
                "cognitive patterns",
                "code invocation",
                "interactive turns"
            ]
        },
        "publishedAt": "2025-08-26T13:03:46.000Z",
        "title": "Understanding Tool-Integrated Reasoning",
        "summary": "We study why Tool-Integrated Reasoning (TIR) makes Large Language Models\n(LLMs) more capable. While LLMs integrated with tools like Python code\ninterpreters show great promise, a principled theory explaining why this\nparadigm is effective has been missing. This work provides the first formal\nproof that TIR fundamentally expands an LLM's capabilities. We demonstrate that\ntools enable a strict expansion of the model's empirical and feasible support,\nbreaking the capability ceiling of pure-text models by unlocking\nproblem-solving strategies that are otherwise impossible or intractably\nverbose. To guide model behavior without compromising training stability and\nperformance, we also introduce Advantage Shaping Policy Optimization (ASPO), a\nnovel algorithm that directly modifies the advantage function to guide the\npolicy behavior. We conduct comprehensive experiments on challenging\nmathematical benchmarks, leveraging a Python interpreter as the external tool.\nOur results show that the TIR model decisively outperforms its pure-text\ncounterpart on the pass@k metric. Crucially, this advantage is not confined to\ncomputationally-intensive problems but extends to those requiring significant\nabstract insight. We further identify the emergent cognitive patterns that\nillustrate how models learn to think with tools. Finally, we report improved\ntool usage behavior with early code invocation and much more interactive turns\nwith ASPO. Overall, our work provides the first principled explanation for\nTIR's success, shifting the focus from the mere fact that tools work to why and\nhow they enable more powerful reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19201.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "672db76fa34d64e774fc42c9",
            "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
            "fullname": "Zhongwen Xu",
            "name": "zhongwenxu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.18190",
            "authors": [
                {
                    "_id": "68ad223786b21a0e2e358d9c",
                    "name": "Zirui Tang",
                    "hidden": false
                },
                {
                    "_id": "68ad223786b21a0e2e358d9d",
                    "name": "Boyu Niu",
                    "hidden": false
                },
                {
                    "_id": "68ad223786b21a0e2e358d9e",
                    "name": "Xuanhe Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ad223786b21a0e2e358d9f",
                    "name": "Boxiu Li",
                    "hidden": false
                },
                {
                    "_id": "68ad223786b21a0e2e358da0",
                    "name": "Wei Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ad223786b21a0e2e358da1",
                    "name": "Jiannan Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad223786b21a0e2e358da2",
                    "name": "Guoliang Li",
                    "hidden": false
                },
                {
                    "_id": "68ad223786b21a0e2e358da3",
                    "name": "Xinyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad223786b21a0e2e358da4",
                    "name": "Fan Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T16:48:51.000Z",
            "submittedOnDailyAt": "2025-08-26T01:26:03.380Z",
            "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
            "upvotes": 3,
            "discussionId": "68ad223786b21a0e2e358da5",
            "githubRepo": "https://github.com/weAIDB/ST-Raptor",
            "ai_summary": "ST-Raptor, a tree-based framework using large language models, addresses challenges in answering questions from semi-structured tables by introducing a Hierarchical Orthogonal Tree and a two-stage verification mechanism.",
            "ai_keywords": [
                "Hierarchical Orthogonal Tree",
                "tree-based framework",
                "large language models",
                "semi-structured tables",
                "tree operations",
                "query decomposition",
                "operation-table alignment",
                "forward validation",
                "backward validation",
                "SSTQA dataset"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-08-25T12:48:51.000Z",
        "title": "ST-Raptor: LLM-Powered Semi-Structured Table Question Answering",
        "summary": "Semi-structured tables, widely used in real-world applications (e.g.,\nfinancial reports, medical records, transactional orders), often involve\nflexible and complex layouts (e.g., hierarchical headers and merged cells).\nThese tables generally rely on human analysts to interpret table layouts and\nanswer relevant natural language questions, which is costly and inefficient. To\nautomate the procedure, existing methods face significant challenges. First,\nmethods like NL2SQL require converting semi-structured tables into structured\nones, which often causes substantial information loss. Second, methods like\nNL2Code and multi-modal LLM QA struggle to understand the complex layouts of\nsemi-structured tables and cannot accurately answer corresponding questions. To\nthis end, we propose ST-Raptor, a tree-based framework for semi-structured\ntable question answering using large language models. First, we introduce the\nHierarchical Orthogonal Tree (HO-Tree), a structural model that captures\ncomplex semi-structured table layouts, along with an effective algorithm for\nconstructing the tree. Second, we define a set of basic tree operations to\nguide LLMs in executing common QA tasks. Given a user question, ST-Raptor\ndecomposes it into simpler sub-questions, generates corresponding tree\noperation pipelines, and conducts operation-table alignment for accurate\npipeline execution. Third, we incorporate a two-stage verification mechanism:\nforward validation checks the correctness of execution steps, while backward\nvalidation evaluates answer reliability by reconstructing queries from\npredicted answers. To benchmark the performance, we present SSTQA, a dataset of\n764 questions over 102 real-world semi-structured tables. Experiments show that\nST-Raptor outperforms nine baselines by up to 20% in answer accuracy. The code\nis available at https://github.com/weAIDB/ST-Raptor.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18190.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.17298",
            "authors": [
                {
                    "_id": "68ad3dec86b21a0e2e358e46",
                    "name": "Fucai Ke",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e47",
                    "name": "Joy Hsu",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e48",
                    "user": {
                        "_id": "633de44a797cf1b030553d64",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633de44a797cf1b030553d64/uvrAP6uYrKnpRxdcK8RKR.png",
                        "isPro": false,
                        "fullname": "Zhixi Cai",
                        "user": "ControlNet",
                        "type": "user"
                    },
                    "name": "Zhixi Cai",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:27.045Z",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e49",
                    "name": "Zixian Ma",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e4a",
                    "name": "Xin Zheng",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e4b",
                    "name": "Xindi Wu",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e4c",
                    "name": "Sukai Huang",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e4d",
                    "name": "Weiqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e4e",
                    "name": "Pari Delir Haghighi",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e4f",
                    "name": "Gholamreza Haffari",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e50",
                    "name": "Ranjay Krishna",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e51",
                    "name": "Jiajun Wu",
                    "hidden": false
                },
                {
                    "_id": "68ad3dec86b21a0e2e358e52",
                    "name": "Hamid Rezatofighi",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/0gblT2G3J8WFpvJnQtCBD.png",
                "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/qHYMdCq45N3_7rAnjCn-j.png",
                "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/Tl0vUu-164DUOpUyjbGxq.png",
                "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/TNURaGnadgMz-bt8bPdkM.png"
            ],
            "publishedAt": "2025-08-24T11:01:51.000Z",
            "submittedOnDailyAt": "2025-08-26T03:37:29.073Z",
            "title": "Explain Before You Answer: A Survey on Compositional Visual Reasoning",
            "submittedOnDailyBy": {
                "_id": "633de44a797cf1b030553d64",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633de44a797cf1b030553d64/uvrAP6uYrKnpRxdcK8RKR.png",
                "isPro": false,
                "fullname": "Zhixi Cai",
                "user": "ControlNet",
                "type": "user"
            },
            "summary": "Compositional visual reasoning has emerged as a key research frontier in\nmultimodal AI, aiming to endow machines with the human-like ability to\ndecompose visual scenes, ground intermediate concepts, and perform multi-step\nlogical inference. While early surveys focus on monolithic vision-language\nmodels or general multimodal reasoning, a dedicated synthesis of the rapidly\nexpanding compositional visual reasoning literature is still missing. We fill\nthis gap with a comprehensive survey spanning 2023 to 2025 that systematically\nreviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We\nfirst formalize core definitions and describe why compositional approaches\noffer advantages in cognitive alignment, semantic fidelity, robustness,\ninterpretability, and data efficiency. Next, we trace a five-stage paradigm\nshift: from prompt-enhanced language-centric pipelines, through tool-enhanced\nLLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and\nunified agentic VLMs, highlighting their architectural designs, strengths, and\nlimitations. We then catalog 60+ benchmarks and corresponding metrics that\nprobe compositional visual reasoning along dimensions such as grounding\naccuracy, chain-of-thought faithfulness, and high-resolution perception.\nDrawing on these analyses, we distill key insights, identify open challenges\n(e.g., limitations of LLM-based reasoning, hallucination, a bias toward\ndeductive reasoning, scalable supervision, tool integration, and benchmark\nlimitations), and outline future directions, including world-model integration,\nhuman-AI collaborative reasoning, and richer evaluation protocols. By offering\na unified taxonomy, historical roadmap, and critical outlook, this survey aims\nto serve as a foundational reference and inspire the next generation of\ncompositional visual reasoning research.",
            "upvotes": 2,
            "discussionId": "68ad3dec86b21a0e2e358e55",
            "projectPage": "https://github.com/pokerme7777/Compositional-Visual-Reasoning-Survey",
            "githubRepo": "https://github.com/pokerme7777/Compositional-Visual-Reasoning-Survey",
            "ai_summary": "A comprehensive survey of compositional visual reasoning from 2023 to 2025 reviews advancements in multimodal AI, highlighting architectural designs, benchmarks, and future directions.",
            "ai_keywords": [
                "compositional visual reasoning",
                "multimodal AI",
                "cognitive alignment",
                "semantic fidelity",
                "robustness",
                "interpretability",
                "data efficiency",
                "prompt-enhanced pipelines",
                "tool-enhanced LLMs",
                "tool-enhanced VLMs",
                "chain-of-thought reasoning",
                "unified agentic VLMs",
                "grounding accuracy",
                "chain-of-thought faithfulness",
                "high-resolution perception",
                "LLM-based reasoning",
                "hallucination",
                "deductive reasoning",
                "scalable supervision",
                "tool integration",
                "benchmark limitations",
                "world-model integration",
                "human-AI collaborative reasoning",
                "richer evaluation protocols"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-08-24T07:01:51.000Z",
        "title": "Explain Before You Answer: A Survey on Compositional Visual Reasoning",
        "summary": "Compositional visual reasoning has emerged as a key research frontier in\nmultimodal AI, aiming to endow machines with the human-like ability to\ndecompose visual scenes, ground intermediate concepts, and perform multi-step\nlogical inference. While early surveys focus on monolithic vision-language\nmodels or general multimodal reasoning, a dedicated synthesis of the rapidly\nexpanding compositional visual reasoning literature is still missing. We fill\nthis gap with a comprehensive survey spanning 2023 to 2025 that systematically\nreviews 260+ papers from top venues (CVPR, ICCV, NeurIPS, ICML, ACL, etc.). We\nfirst formalize core definitions and describe why compositional approaches\noffer advantages in cognitive alignment, semantic fidelity, robustness,\ninterpretability, and data efficiency. Next, we trace a five-stage paradigm\nshift: from prompt-enhanced language-centric pipelines, through tool-enhanced\nLLMs and tool-enhanced VLMs, to recently minted chain-of-thought reasoning and\nunified agentic VLMs, highlighting their architectural designs, strengths, and\nlimitations. We then catalog 60+ benchmarks and corresponding metrics that\nprobe compositional visual reasoning along dimensions such as grounding\naccuracy, chain-of-thought faithfulness, and high-resolution perception.\nDrawing on these analyses, we distill key insights, identify open challenges\n(e.g., limitations of LLM-based reasoning, hallucination, a bias toward\ndeductive reasoning, scalable supervision, tool integration, and benchmark\nlimitations), and outline future directions, including world-model integration,\nhuman-AI collaborative reasoning, and richer evaluation protocols. By offering\na unified taxonomy, historical roadmap, and critical outlook, this survey aims\nto serve as a foundational reference and inspire the next generation of\ncompositional visual reasoning research.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/0gblT2G3J8WFpvJnQtCBD.png",
            "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/qHYMdCq45N3_7rAnjCn-j.png",
            "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/Tl0vUu-164DUOpUyjbGxq.png",
            "https://cdn-uploads.huggingface.co/production/uploads/633de44a797cf1b030553d64/TNURaGnadgMz-bt8bPdkM.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17298.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "633de44a797cf1b030553d64",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/633de44a797cf1b030553d64/uvrAP6uYrKnpRxdcK8RKR.png",
            "fullname": "Zhixi Cai",
            "name": "ControlNet",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.16790",
            "authors": [
                {
                    "_id": "68ad0ef486b21a0e2e358d05",
                    "name": "Yuancheng Wang",
                    "hidden": false
                },
                {
                    "_id": "68ad0ef486b21a0e2e358d06",
                    "name": "Dekun Chen",
                    "hidden": false
                },
                {
                    "_id": "68ad0ef486b21a0e2e358d07",
                    "name": "Xueyao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad0ef486b21a0e2e358d08",
                    "name": "Junan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ad0ef486b21a0e2e358d09",
                    "name": "Jiaqi Li",
                    "hidden": false
                },
                {
                    "_id": "68ad0ef486b21a0e2e358d0a",
                    "name": "Zhizheng Wu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T20:45:03.000Z",
            "submittedOnDailyAt": "2025-08-26T01:54:58.440Z",
            "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language\n  Modeling",
            "submittedOnDailyBy": {
                "_id": "63072d60cd148dbc5e49f4dd",
                "avatarUrl": "/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg",
                "isPro": false,
                "fullname": "Yuancheng Wang",
                "user": "Hecheng0625",
                "type": "user"
            },
            "summary": "Speech tokenizers serve as foundational components for speech language\nmodels, yet current designs exhibit several limitations, including: 1)\ndependence on multi-layer residual vector quantization structures or high frame\nrates, 2) reliance on auxiliary pre-trained models for semantic distillation,\nand 3) requirements for complex two-stage training processes. In this work, we\nintroduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a\nnovel approach designed to overcome these challenges. TaDiCodec employs\nend-to-end optimization for quantization and reconstruction through a diffusion\nautoencoder, while integrating text guidance into the diffusion decoder to\nenhance reconstruction quality and achieve optimal compression. TaDiCodec\nachieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of\n0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining\nsuperior performance on critical speech generation evaluation metrics such as\nWord Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).\nNotably, TaDiCodec employs a single-stage, end-to-end training paradigm, and\nobviating the need for auxiliary pre-trained models. We also validate the\ncompatibility of TaDiCodec in language model based zero-shot text-to-speech\nwith both autoregressive modeling and masked generative modeling, demonstrating\nits effectiveness and efficiency for speech language modeling, as well as a\nsignificantly small reconstruction-generation gap. We will open source our code\nand model checkpoints. Audio samples are are available at\nhttps:/tadicodec.github.io/. We release code and model checkpoints at\nhttps:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.",
            "upvotes": 2,
            "discussionId": "68ad0ef486b21a0e2e358d0b",
            "projectPage": "https://github.com/HeCheng0625/Diffusion-Speech-Tokenizer",
            "githubRepo": "https://github.com/HeCheng0625/Diffusion-Speech-Tokenizer",
            "ai_summary": "TaDiCodec, a Text-aware Diffusion Transformer Speech Codec, achieves low frame rates and bitrates with superior speech generation performance using end-to-end optimization and text guidance.",
            "ai_keywords": [
                "diffusion autoencoder",
                "text guidance",
                "end-to-end optimization",
                "Word Error Rate (WER)",
                "speaker similarity (SIM)",
                "speech quality (UTMOS)",
                "zero-shot text-to-speech",
                "autoregressive modeling",
                "masked generative modeling"
            ],
            "githubStars": 97
        },
        "publishedAt": "2025-08-22T16:45:03.000Z",
        "title": "TaDiCodec: Text-aware Diffusion Speech Tokenizer for Speech Language\n  Modeling",
        "summary": "Speech tokenizers serve as foundational components for speech language\nmodels, yet current designs exhibit several limitations, including: 1)\ndependence on multi-layer residual vector quantization structures or high frame\nrates, 2) reliance on auxiliary pre-trained models for semantic distillation,\nand 3) requirements for complex two-stage training processes. In this work, we\nintroduce the Text-aware Diffusion Transformer Speech Codec (TaDiCodec), a\nnovel approach designed to overcome these challenges. TaDiCodec employs\nend-to-end optimization for quantization and reconstruction through a diffusion\nautoencoder, while integrating text guidance into the diffusion decoder to\nenhance reconstruction quality and achieve optimal compression. TaDiCodec\nachieves an extremely low frame rate of 6.25 Hz and a corresponding bitrate of\n0.0875 kbps with a single-layer codebook for 24 kHz speech, while maintaining\nsuperior performance on critical speech generation evaluation metrics such as\nWord Error Rate (WER), speaker similarity (SIM), and speech quality (UTMOS).\nNotably, TaDiCodec employs a single-stage, end-to-end training paradigm, and\nobviating the need for auxiliary pre-trained models. We also validate the\ncompatibility of TaDiCodec in language model based zero-shot text-to-speech\nwith both autoregressive modeling and masked generative modeling, demonstrating\nits effectiveness and efficiency for speech language modeling, as well as a\nsignificantly small reconstruction-generation gap. We will open source our code\nand model checkpoints. Audio samples are are available at\nhttps:/tadicodec.github.io/. We release code and model checkpoints at\nhttps:/github.com/HeCheng0625/Diffusion-Speech-Tokenizer.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16790.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63072d60cd148dbc5e49f4dd",
            "avatarUrl": "/avatars/ffa61038c0ff20848fbcde7c1c34570e.svg",
            "fullname": "Yuancheng Wang",
            "name": "Hecheng0625",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.18159",
            "authors": [
                {
                    "_id": "68ad22c586b21a0e2e358da7",
                    "name": "Sara Ghazanfari",
                    "hidden": false
                },
                {
                    "_id": "68ad22c586b21a0e2e358da8",
                    "name": "Wei-An Lin",
                    "hidden": false
                },
                {
                    "_id": "68ad22c586b21a0e2e358da9",
                    "name": "Haitong Tian",
                    "hidden": false
                },
                {
                    "_id": "68ad22c586b21a0e2e358daa",
                    "name": "Ersin Yumer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T16:08:57.000Z",
            "submittedOnDailyAt": "2025-08-26T01:28:26.623Z",
            "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Visually-guided image editing, where edits are conditioned on both visual\ncues and textual prompts, has emerged as a powerful paradigm for fine-grained,\ncontrollable content generation. Although recent generative models have shown\nremarkable capabilities, existing evaluations remain simple and insufficiently\nrepresentative of real-world editing challenges. We present SpotEdit, a\ncomprehensive benchmark designed to systematically assess visually-guided image\nediting methods across diverse diffusion, autoregressive, and hybrid generative\nmodels, uncovering substantial performance disparities. To address a critical\nyet underexplored challenge, our benchmark includes a dedicated component on\nhallucination, highlighting how leading models, such as GPT-4o, often\nhallucinate the existence of a visual cue and erroneously perform the editing\ntask. Our code and benchmark are publicly released at\nhttps://github.com/SaraGhazanfari/SpotEdit.",
            "upvotes": 1,
            "discussionId": "68ad22c586b21a0e2e358dab",
            "githubRepo": "https://github.com/SaraGhazanfari/SpotEdit",
            "ai_summary": "SpotEdit is a benchmark for evaluating visually-guided image editing methods, revealing performance disparities and hallucination issues across diffusion, autoregressive, and hybrid generative models.",
            "ai_keywords": [
                "visually-guided image editing",
                "diffusion models",
                "autoregressive models",
                "hybrid generative models",
                "hallucination"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-08-25T12:08:57.000Z",
        "title": "SpotEdit: Evaluating Visually-Guided Image Editing Methods",
        "summary": "Visually-guided image editing, where edits are conditioned on both visual\ncues and textual prompts, has emerged as a powerful paradigm for fine-grained,\ncontrollable content generation. Although recent generative models have shown\nremarkable capabilities, existing evaluations remain simple and insufficiently\nrepresentative of real-world editing challenges. We present SpotEdit, a\ncomprehensive benchmark designed to systematically assess visually-guided image\nediting methods across diverse diffusion, autoregressive, and hybrid generative\nmodels, uncovering substantial performance disparities. To address a critical\nyet underexplored challenge, our benchmark includes a dedicated component on\nhallucination, highlighting how leading models, such as GPT-4o, often\nhallucinate the existence of a visual cue and erroneously perform the editing\ntask. Our code and benchmark are publicly released at\nhttps://github.com/SaraGhazanfari/SpotEdit.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18159.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.18076",
            "authors": [
                {
                    "_id": "68ad232986b21a0e2e358dad",
                    "name": "Khaoula Chehbouni",
                    "hidden": false
                },
                {
                    "_id": "68ad232986b21a0e2e358dae",
                    "name": "Mohammed Haddou",
                    "hidden": false
                },
                {
                    "_id": "68ad232986b21a0e2e358daf",
                    "name": "Jackie Chi Kit Cheung",
                    "hidden": false
                },
                {
                    "_id": "68ad232986b21a0e2e358db0",
                    "name": "Golnoosh Farnadi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T14:43:10.000Z",
            "submittedOnDailyAt": "2025-08-26T01:30:03.792Z",
            "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Evaluating natural language generation (NLG) systems remains a core challenge\nof natural language processing (NLP), further complicated by the rise of large\nlanguage models (LLMs) that aims to be general-purpose. Recently, large\nlanguage models as judges (LLJs) have emerged as a promising alternative to\ntraditional metrics, but their validity remains underexplored. This position\npaper argues that the current enthusiasm around LLJs may be premature, as their\nadoption has outpaced rigorous scrutiny of their reliability and validity as\nevaluators. Drawing on measurement theory from the social sciences, we identify\nand critically assess four core assumptions underlying the use of LLJs: their\nability to act as proxies for human judgment, their capabilities as evaluators,\ntheir scalability, and their cost-effectiveness. We examine how each of these\nassumptions may be challenged by the inherent limitations of LLMs, LLJs, or\ncurrent practices in NLG evaluation. To ground our analysis, we explore three\napplications of LLJs: text summarization, data annotation, and safety\nalignment. Finally, we highlight the need for more responsible evaluation\npractices in LLJs evaluation, to ensure that their growing role in the field\nsupports, rather than undermines, progress in NLG.",
            "upvotes": 1,
            "discussionId": "68ad232986b21a0e2e358db1",
            "ai_summary": "The paper critiques the use of large language models as judges for evaluating natural language generation systems, questioning their reliability, capabilities, scalability, and cost-effectiveness.",
            "ai_keywords": [
                "large language models",
                "LLJs",
                "natural language generation",
                "NLG",
                "measurement theory",
                "text summarization",
                "data annotation",
                "safety alignment"
            ]
        },
        "publishedAt": "2025-08-25T10:43:10.000Z",
        "title": "Neither Valid nor Reliable? Investigating the Use of LLMs as Judges",
        "summary": "Evaluating natural language generation (NLG) systems remains a core challenge\nof natural language processing (NLP), further complicated by the rise of large\nlanguage models (LLMs) that aims to be general-purpose. Recently, large\nlanguage models as judges (LLJs) have emerged as a promising alternative to\ntraditional metrics, but their validity remains underexplored. This position\npaper argues that the current enthusiasm around LLJs may be premature, as their\nadoption has outpaced rigorous scrutiny of their reliability and validity as\nevaluators. Drawing on measurement theory from the social sciences, we identify\nand critically assess four core assumptions underlying the use of LLJs: their\nability to act as proxies for human judgment, their capabilities as evaluators,\ntheir scalability, and their cost-effectiveness. We examine how each of these\nassumptions may be challenged by the inherent limitations of LLMs, LLJs, or\ncurrent practices in NLG evaluation. To ground our analysis, we explore three\napplications of LLJs: text summarization, data annotation, and safety\nalignment. Finally, we highlight the need for more responsible evaluation\npractices in LLJs evaluation, to ensure that their growing role in the field\nsupports, rather than undermines, progress in NLG.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18076.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 95
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.17973",
            "authors": [
                {
                    "_id": "68ad764a86b21a0e2e358f18",
                    "name": "Miriam Anschütz",
                    "hidden": false
                },
                {
                    "_id": "68ad764a86b21a0e2e358f19",
                    "name": "Thanh Mai Pham",
                    "hidden": false
                },
                {
                    "_id": "68ad764a86b21a0e2e358f1a",
                    "name": "Eslam Nasrallah",
                    "hidden": false
                },
                {
                    "_id": "68ad764a86b21a0e2e358f1b",
                    "name": "Maximilian Müller",
                    "hidden": false
                },
                {
                    "_id": "68ad764a86b21a0e2e358f1c",
                    "name": "Cristian-George Craciun",
                    "hidden": false
                },
                {
                    "_id": "68ad764a86b21a0e2e358f1d",
                    "name": "Georg Groh",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/f0zA3kxkw4slm42W_Y1Q3.png"
            ],
            "publishedAt": "2025-08-25T12:40:32.000Z",
            "submittedOnDailyAt": "2025-08-26T07:28:00.242Z",
            "title": "German4All - A Dataset and Model for Readability-Controlled Paraphrasing\n  in German",
            "submittedOnDailyBy": {
                "_id": "5e6a3d4ea9afd5125d9ec064",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                "isPro": true,
                "fullname": "Stefan Schweter",
                "user": "stefan-it",
                "type": "user"
            },
            "summary": "The ability to paraphrase texts across different complexity levels is\nessential for creating accessible texts that can be tailored toward diverse\nreader groups. Thus, we introduce German4All, the first large-scale German\ndataset of aligned readability-controlled, paragraph-level paraphrases. It\nspans five readability levels and comprises over 25,000 samples. The dataset is\nautomatically synthesized using GPT-4 and rigorously evaluated through both\nhuman and LLM-based judgments. Using German4All, we train an open-source,\nreadability-controlled paraphrasing model that achieves state-of-the-art\nperformance in German text simplification, enabling more nuanced and\nreader-specific adaptations. We opensource both the dataset and the model to\nencourage further research on multi-level paraphrasing",
            "upvotes": 1,
            "discussionId": "68ad764b86b21a0e2e358f1e",
            "githubRepo": "https://github.com/MiriUll/German4All",
            "ai_summary": "A large-scale German dataset and model for readability-controlled paraphrasing are introduced, achieving state-of-the-art performance in text simplification.",
            "ai_keywords": [
                "GPT-4",
                "readability-controlled paraphrases",
                "text simplification"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-08-25T08:40:32.000Z",
        "title": "German4All - A Dataset and Model for Readability-Controlled Paraphrasing\n  in German",
        "summary": "The ability to paraphrase texts across different complexity levels is\nessential for creating accessible texts that can be tailored toward diverse\nreader groups. Thus, we introduce German4All, the first large-scale German\ndataset of aligned readability-controlled, paragraph-level paraphrases. It\nspans five readability levels and comprises over 25,000 samples. The dataset is\nautomatically synthesized using GPT-4 and rigorously evaluated through both\nhuman and LLM-based judgments. Using German4All, we train an open-source,\nreadability-controlled paraphrasing model that achieves state-of-the-art\nperformance in German text simplification, enabling more nuanced and\nreader-specific adaptations. We opensource both the dataset and the model to\nencourage further research on multi-level paraphrasing",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5e6a3d4ea9afd5125d9ec064/f0zA3kxkw4slm42W_Y1Q3.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17973.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "5e6a3d4ea9afd5125d9ec064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3226
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.17821",
            "authors": [
                {
                    "_id": "68ad76c486b21a0e2e358f2e",
                    "user": {
                        "_id": "66348e8fc58f455771063d68",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/1D_v0kPviu-JBViDuzuoC.jpeg",
                        "isPro": false,
                        "fullname": "Timur Mudarisov",
                        "user": "opensapce",
                        "type": "user"
                    },
                    "name": "Timur Mudarisov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T10:34:40.287Z",
                    "hidden": false
                },
                {
                    "_id": "68ad76c486b21a0e2e358f2f",
                    "user": {
                        "_id": "639c6e978a34ed9a404c6a7b",
                        "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
                        "isPro": false,
                        "fullname": "MIKHAIL BURTSEV",
                        "user": "mbur",
                        "type": "user"
                    },
                    "name": "Mikhail Burtsev",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T09:49:08.448Z",
                    "hidden": false
                },
                {
                    "_id": "68ad76c486b21a0e2e358f30",
                    "name": "Tatiana Petrova",
                    "hidden": false
                },
                {
                    "_id": "68ad76c486b21a0e2e358f31",
                    "name": "Radu State",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T09:25:05.000Z",
            "submittedOnDailyAt": "2025-08-26T08:36:17.616Z",
            "title": "Limitations of Normalization in Attention Mechanism",
            "submittedOnDailyBy": {
                "_id": "639c6e978a34ed9a404c6a7b",
                "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
                "isPro": false,
                "fullname": "MIKHAIL BURTSEV",
                "user": "mbur",
                "type": "user"
            },
            "summary": "This paper investigates the limitations of the normalization in attention\nmechanisms. We begin with a theoretical framework that enables the\nidentification of the model's selective ability and the geometric separation\ninvolved in token selection. Our analysis includes explicit bounds on distances\nand separation criteria for token vectors under softmax scaling. Through\nexperiments with pre-trained GPT-2 model, we empirically validate our\ntheoretical results and analyze key behaviors of the attention mechanism.\nNotably, we demonstrate that as the number of selected tokens increases, the\nmodel's ability to distinguish informative tokens declines, often converging\ntoward a uniform selection pattern. We also show that gradient sensitivity\nunder softmax normalization presents challenges during training, especially at\nlow temperature settings. These findings advance current understanding of\nsoftmax-based attention mechanism and motivate the need for more robust\nnormalization and selection strategies in future attention architectures.",
            "upvotes": 1,
            "discussionId": "68ad76c586b21a0e2e358f32",
            "ai_summary": "Theoretical and empirical analysis of softmax normalization in attention mechanisms reveals limitations in token selection and gradient sensitivity, highlighting the need for improved normalization strategies.",
            "ai_keywords": [
                "attention mechanisms",
                "normalization",
                "token selection",
                "softmax scaling",
                "gradient sensitivity",
                "GPT-2",
                "geometric separation",
                "token vectors",
                "uniform selection pattern"
            ]
        },
        "publishedAt": "2025-08-25T05:25:05.000Z",
        "title": "Limitations of Normalization in Attention Mechanism",
        "summary": "This paper investigates the limitations of the normalization in attention\nmechanisms. We begin with a theoretical framework that enables the\nidentification of the model's selective ability and the geometric separation\ninvolved in token selection. Our analysis includes explicit bounds on distances\nand separation criteria for token vectors under softmax scaling. Through\nexperiments with pre-trained GPT-2 model, we empirically validate our\ntheoretical results and analyze key behaviors of the attention mechanism.\nNotably, we demonstrate that as the number of selected tokens increases, the\nmodel's ability to distinguish informative tokens declines, often converging\ntoward a uniform selection pattern. We also show that gradient sensitivity\nunder softmax normalization presents challenges during training, especially at\nlow temperature settings. These findings advance current understanding of\nsoftmax-based attention mechanism and motivate the need for more robust\nnormalization and selection strategies in future attention architectures.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17821.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "639c6e978a34ed9a404c6a7b",
            "avatarUrl": "/avatars/c98ca8c9f9ed8509c2f1bb6aa994fd57.svg",
            "fullname": "MIKHAIL BURTSEV",
            "name": "mbur",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.17811",
            "authors": [
                {
                    "_id": "68ada9e6b9d0280e9ba2aaef",
                    "name": "Hanzhi Chang",
                    "hidden": false
                },
                {
                    "_id": "68ada9e6b9d0280e9ba2aaf0",
                    "user": {
                        "_id": "6697ac8427e4e21a3a92da27",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png",
                        "isPro": false,
                        "fullname": "Ruijie Zhu",
                        "user": "RuijieZhu",
                        "type": "user"
                    },
                    "name": "Ruijie Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T19:05:38.566Z",
                    "hidden": false
                },
                {
                    "_id": "68ada9e6b9d0280e9ba2aaf1",
                    "name": "Wenjie Chang",
                    "hidden": false
                },
                {
                    "_id": "68ada9e6b9d0280e9ba2aaf2",
                    "name": "Mulin Yu",
                    "hidden": false
                },
                {
                    "_id": "68ada9e6b9d0280e9ba2aaf3",
                    "name": "Yanzhe Liang",
                    "hidden": false
                },
                {
                    "_id": "68ada9e6b9d0280e9ba2aaf4",
                    "name": "Jiahao Lu",
                    "hidden": false
                },
                {
                    "_id": "68ada9e6b9d0280e9ba2aaf5",
                    "name": "Zhuoyuan Li",
                    "hidden": false
                },
                {
                    "_id": "68ada9e6b9d0280e9ba2aaf6",
                    "name": "Tianzhu Zhang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6697ac8427e4e21a3a92da27/rxsJzSBYAWsDvjvyvPW05.mp4"
            ],
            "publishedAt": "2025-08-25T09:04:20.000Z",
            "submittedOnDailyAt": "2025-08-26T11:11:02.731Z",
            "title": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian\n  Splatting",
            "submittedOnDailyBy": {
                "_id": "6697ac8427e4e21a3a92da27",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png",
                "isPro": false,
                "fullname": "Ruijie Zhu",
                "user": "RuijieZhu",
                "type": "user"
            },
            "summary": "Surface reconstruction has been widely studied in computer vision and\ngraphics. However, existing surface reconstruction works struggle to recover\naccurate scene geometry when the input views are extremely sparse. To address\nthis issue, we propose MeshSplat, a generalizable sparse-view surface\nreconstruction framework via Gaussian Splatting. Our key idea is to leverage\n2DGS as a bridge, which connects novel view synthesis to learned geometric\npriors and then transfers these priors to achieve surface reconstruction.\nSpecifically, we incorporate a feed-forward network to predict per-view\npixel-aligned 2DGS, which enables the network to synthesize novel view images\nand thus eliminates the need for direct 3D ground-truth supervision. To improve\nthe accuracy of 2DGS position and orientation prediction, we propose a Weighted\nChamfer Distance Loss to regularize the depth maps, especially in overlapping\nareas of input views, and also a normal prediction network to align the\norientation of 2DGS with normal vectors predicted by a monocular normal\nestimator. Extensive experiments validate the effectiveness of our proposed\nimprovement, demonstrating that our method achieves state-of-the-art\nperformance in generalizable sparse-view mesh reconstruction tasks. Project\nPage: https://hanzhichang.github.io/meshsplat_web",
            "upvotes": 1,
            "discussionId": "68ada9e6b9d0280e9ba2aaf7",
            "projectPage": "https://hanzhichang.github.io/meshsplat_web",
            "githubRepo": "https://github.com/HanzhiChang/MeshSplat",
            "ai_summary": "MeshSplat uses Gaussian Splatting and a feed-forward network to reconstruct surfaces from sparse views, improving accuracy with a Weighted Chamfer Distance Loss and normal prediction.",
            "ai_keywords": [
                "Gaussian Splatting",
                "2DGS",
                "feed-forward network",
                "Weighted Chamfer Distance Loss",
                "normal prediction network",
                "monocular normal estimator",
                "sparse-view surface reconstruction"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-08-25T05:04:20.000Z",
        "title": "MeshSplat: Generalizable Sparse-View Surface Reconstruction via Gaussian\n  Splatting",
        "summary": "Surface reconstruction has been widely studied in computer vision and\ngraphics. However, existing surface reconstruction works struggle to recover\naccurate scene geometry when the input views are extremely sparse. To address\nthis issue, we propose MeshSplat, a generalizable sparse-view surface\nreconstruction framework via Gaussian Splatting. Our key idea is to leverage\n2DGS as a bridge, which connects novel view synthesis to learned geometric\npriors and then transfers these priors to achieve surface reconstruction.\nSpecifically, we incorporate a feed-forward network to predict per-view\npixel-aligned 2DGS, which enables the network to synthesize novel view images\nand thus eliminates the need for direct 3D ground-truth supervision. To improve\nthe accuracy of 2DGS position and orientation prediction, we propose a Weighted\nChamfer Distance Loss to regularize the depth maps, especially in overlapping\nareas of input views, and also a normal prediction network to align the\norientation of 2DGS with normal vectors predicted by a monocular normal\nestimator. Extensive experiments validate the effectiveness of our proposed\nimprovement, demonstrating that our method achieves state-of-the-art\nperformance in generalizable sparse-view mesh reconstruction tasks. Project\nPage: https://hanzhichang.github.io/meshsplat_web",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6697ac8427e4e21a3a92da27/rxsJzSBYAWsDvjvyvPW05.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17811.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6697ac8427e4e21a3a92da27",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6697ac8427e4e21a3a92da27/9vn07-1_BBDk9zfDtDpcG.png",
            "fullname": "Ruijie Zhu",
            "name": "RuijieZhu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.17326",
            "authors": [
                {
                    "_id": "68ae137a364411bea07df678",
                    "name": "Tristan S. W. Stevens",
                    "hidden": false
                },
                {
                    "_id": "68ae137a364411bea07df679",
                    "name": "Oisín Nolan",
                    "hidden": false
                },
                {
                    "_id": "68ae137a364411bea07df67a",
                    "name": "Ruud J. G. van Sloun",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6303f618eedc089484c6e772/Zpz56IQZxn8HiKF3h1RPJ.gif"
            ],
            "publishedAt": "2025-08-24T12:20:18.000Z",
            "submittedOnDailyAt": "2025-08-26T19:48:23.651Z",
            "title": "Semantic Diffusion Posterior Sampling for Cardiac Ultrasound Dehazing",
            "submittedOnDailyBy": {
                "_id": "6303f618eedc089484c6e772",
                "avatarUrl": "/avatars/15af57eb7bde2730f7f9ef47d9469780.svg",
                "isPro": false,
                "fullname": "Tristan Stevens",
                "user": "tristan-deep",
                "type": "user"
            },
            "summary": "Echocardiography plays a central role in cardiac imaging, offering dynamic\nviews of the heart that are essential for diagnosis and monitoring. However,\nimage quality can be significantly degraded by haze arising from multipath\nreverberations, particularly in difficult-to-image patients. In this work, we\npropose a semantic-guided, diffusion-based dehazing algorithm developed for the\nMICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method\nintegrates a pixel-wise noise model, derived from semantic segmentation of hazy\ninputs into a diffusion posterior sampling framework guided by a generative\nprior trained on clean ultrasound data. Quantitative evaluation on the\nchallenge dataset demonstrates strong performance across contrast and fidelity\nmetrics. Code for the submitted algorithm is available at\nhttps://github.com/tristan-deep/semantic-diffusion-echo-dehazing.",
            "upvotes": 0,
            "discussionId": "68ae137a364411bea07df67b",
            "githubRepo": "https://github.com/tristan-deep/semantic-diffusion-echo-dehazing",
            "ai_summary": "A semantic-guided diffusion-based dehazing algorithm improves echocardiography image quality by integrating a pixel-wise noise model and a generative prior.",
            "ai_keywords": [
                "semantic-guided",
                "diffusion-based dehazing",
                "pixel-wise noise model",
                "diffusion posterior sampling",
                "generative prior",
                "echocardiography",
                "MICCAI Dehazing Echocardiography Challenge",
                "DehazingEcho2025"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-08-24T08:20:18.000Z",
        "title": "Semantic Diffusion Posterior Sampling for Cardiac Ultrasound Dehazing",
        "summary": "Echocardiography plays a central role in cardiac imaging, offering dynamic\nviews of the heart that are essential for diagnosis and monitoring. However,\nimage quality can be significantly degraded by haze arising from multipath\nreverberations, particularly in difficult-to-image patients. In this work, we\npropose a semantic-guided, diffusion-based dehazing algorithm developed for the\nMICCAI Dehazing Echocardiography Challenge (DehazingEcho2025). Our method\nintegrates a pixel-wise noise model, derived from semantic segmentation of hazy\ninputs into a diffusion posterior sampling framework guided by a generative\nprior trained on clean ultrasound data. Quantitative evaluation on the\nchallenge dataset demonstrates strong performance across contrast and fidelity\nmetrics. Code for the submitted algorithm is available at\nhttps://github.com/tristan-deep/semantic-diffusion-echo-dehazing.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6303f618eedc089484c6e772/Zpz56IQZxn8HiKF3h1RPJ.gif"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17326.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6303f618eedc089484c6e772",
            "avatarUrl": "/avatars/15af57eb7bde2730f7f9ef47d9469780.svg",
            "fullname": "Tristan Stevens",
            "name": "tristan-deep",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.17061",
            "authors": [
                {
                    "_id": "68adb6bb364411bea07df579",
                    "user": {
                        "_id": "67efa4835e6c9ec419b11fea",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ru-dRutLSAOcqAl54Eh0g.png",
                        "isPro": false,
                        "fullname": "Stefanos Pasios",
                        "user": "stefanos50",
                        "type": "user"
                    },
                    "name": "Stefanos Pasios",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T19:05:35.119Z",
                    "hidden": false
                },
                {
                    "_id": "68adb6bb364411bea07df57a",
                    "name": "Nikos Nikolaidis",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67efa4835e6c9ec419b11fea/DWS5mPnnTCoEJ8FNQnoDU.png"
            ],
            "publishedAt": "2025-08-23T15:28:05.000Z",
            "submittedOnDailyAt": "2025-08-26T12:06:48.768Z",
            "title": "REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage\n  Generative Network Framework",
            "submittedOnDailyBy": {
                "_id": "67efa4835e6c9ec419b11fea",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ru-dRutLSAOcqAl54Eh0g.png",
                "isPro": false,
                "fullname": "Stefanos Pasios",
                "user": "stefanos50",
                "type": "user"
            },
            "summary": "Photorealism is an important aspect of modern video games since it can shape\nthe player experience and simultaneously impact the immersion, narrative\nengagement, and visual fidelity. Although recent hardware technological\nbreakthroughs, along with state-of-the-art rendering technologies, have\nsignificantly improved the visual realism of video games, achieving true\nphotorealism in dynamic environments at real-time frame rates still remains a\nmajor challenge due to the tradeoff between visual quality and performance. In\nthis short paper, we present a novel approach for enhancing the photorealism of\nrendered game frames using generative adversarial networks. To this end, we\npropose Real-time photorealism Enhancement in Games via a dual-stage gEnerative\nNetwork framework (REGEN), which employs a robust unpaired image-to-image\ntranslation model to produce semantically consistent photorealistic frames that\ntransform the problem into a simpler paired image-to-image translation task.\nThis enables training with a lightweight method that can achieve real-time\ninference time without compromising visual quality. We demonstrate the\neffectiveness of our framework on Grand Theft Auto V, showing that the approach\nachieves visual results comparable to the ones produced by the robust unpaired\nIm2Im method while improving inference speed by 32.14 times. Our findings also\nindicate that the results outperform the photorealism-enhanced frames produced\nby directly training a lightweight unpaired Im2Im translation method to\ntranslate the video game frames towards the visual characteristics of\nreal-world images. Code, pre-trained models, and demos for this work are\navailable at: https://github.com/stefanos50/REGEN.",
            "upvotes": 0,
            "discussionId": "68adb6bb364411bea07df57b",
            "githubRepo": "https://github.com/stefanos50/REGEN",
            "ai_summary": "A dual-stage generative network framework enhances photorealism in real-time video game rendering by improving inference speed and visual quality.",
            "ai_keywords": [
                "generative adversarial networks",
                "image-to-image translation",
                "photorealism",
                "real-time inference",
                "Grand Theft Auto V",
                "Im2Im method"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-08-23T11:28:05.000Z",
        "title": "REGEN: Real-Time Photorealism Enhancement in Games via a Dual-Stage\n  Generative Network Framework",
        "summary": "Photorealism is an important aspect of modern video games since it can shape\nthe player experience and simultaneously impact the immersion, narrative\nengagement, and visual fidelity. Although recent hardware technological\nbreakthroughs, along with state-of-the-art rendering technologies, have\nsignificantly improved the visual realism of video games, achieving true\nphotorealism in dynamic environments at real-time frame rates still remains a\nmajor challenge due to the tradeoff between visual quality and performance. In\nthis short paper, we present a novel approach for enhancing the photorealism of\nrendered game frames using generative adversarial networks. To this end, we\npropose Real-time photorealism Enhancement in Games via a dual-stage gEnerative\nNetwork framework (REGEN), which employs a robust unpaired image-to-image\ntranslation model to produce semantically consistent photorealistic frames that\ntransform the problem into a simpler paired image-to-image translation task.\nThis enables training with a lightweight method that can achieve real-time\ninference time without compromising visual quality. We demonstrate the\neffectiveness of our framework on Grand Theft Auto V, showing that the approach\nachieves visual results comparable to the ones produced by the robust unpaired\nIm2Im method while improving inference speed by 32.14 times. Our findings also\nindicate that the results outperform the photorealism-enhanced frames produced\nby directly training a lightweight unpaired Im2Im translation method to\ntranslate the video game frames towards the visual characteristics of\nreal-world images. Code, pre-trained models, and demos for this work are\navailable at: https://github.com/stefanos50/REGEN.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67efa4835e6c9ec419b11fea/DWS5mPnnTCoEJ8FNQnoDU.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17061.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67efa4835e6c9ec419b11fea",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ru-dRutLSAOcqAl54Eh0g.png",
            "fullname": "Stefanos Pasios",
            "name": "stefanos50",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.16838",
            "authors": [
                {
                    "_id": "68adeb86364411bea07df5ce",
                    "user": {
                        "_id": "638940232a897944ea5d2a41",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638940232a897944ea5d2a41/G3tmDlB_vagrKj1KjsQP6.jpeg",
                        "isPro": false,
                        "fullname": "Shubhashis Roy Dipta",
                        "user": "dipta007",
                        "type": "user"
                    },
                    "name": "Shubhashis Roy Dipta",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T19:05:27.865Z",
                    "hidden": false
                },
                {
                    "_id": "68adeb86364411bea07df5cf",
                    "name": "Francis Ferraro",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T23:34:24.000Z",
            "submittedOnDailyAt": "2025-08-26T15:45:12.226Z",
            "title": "If We May De-Presuppose: Robustly Verifying Claims through\n  Presupposition-Free Question Decomposition",
            "submittedOnDailyBy": {
                "_id": "638940232a897944ea5d2a41",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638940232a897944ea5d2a41/G3tmDlB_vagrKj1KjsQP6.jpeg",
                "isPro": false,
                "fullname": "Shubhashis Roy Dipta",
                "user": "dipta007",
                "type": "user"
            },
            "summary": "Prior work has shown that presupposition in generated questions can introduce\nunverified assumptions, leading to inconsistencies in claim verification.\nAdditionally, prompt sensitivity remains a significant challenge for large\nlanguage models (LLMs), resulting in performance variance as high as 3-6%.\nWhile recent advancements have reduced this gap, our study demonstrates that\nprompt sensitivity remains a persistent issue. To address this, we propose a\nstructured and robust claim verification framework that reasons through\npresupposition-free, decomposed questions. Extensive experiments across\nmultiple prompts, datasets, and LLMs reveal that even state-of-the-art models\nremain susceptible to prompt variance and presupposition. Our method\nconsistently mitigates these issues, achieving up to a 2-5% improvement.",
            "upvotes": 0,
            "discussionId": "68adeb86364411bea07df5d0",
            "githubRepo": "https://github.com/dipta007/De-Presuppose",
            "ai_summary": "A structured claim verification framework reduces prompt sensitivity and presupposition issues in large language models, improving performance by 2-5%.",
            "ai_keywords": [
                "presupposition",
                "claim verification",
                "prompt sensitivity",
                "large language models",
                "LLMs",
                "decomposed questions"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-08-22T19:34:24.000Z",
        "title": "If We May De-Presuppose: Robustly Verifying Claims through\n  Presupposition-Free Question Decomposition",
        "summary": "Prior work has shown that presupposition in generated questions can introduce\nunverified assumptions, leading to inconsistencies in claim verification.\nAdditionally, prompt sensitivity remains a significant challenge for large\nlanguage models (LLMs), resulting in performance variance as high as 3-6%.\nWhile recent advancements have reduced this gap, our study demonstrates that\nprompt sensitivity remains a persistent issue. To address this, we propose a\nstructured and robust claim verification framework that reasons through\npresupposition-free, decomposed questions. Extensive experiments across\nmultiple prompts, datasets, and LLMs reveal that even state-of-the-art models\nremain susceptible to prompt variance and presupposition. Our method\nconsistently mitigates these issues, achieving up to a 2-5% improvement.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16838.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "638940232a897944ea5d2a41",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/638940232a897944ea5d2a41/G3tmDlB_vagrKj1KjsQP6.jpeg",
            "fullname": "Shubhashis Roy Dipta",
            "name": "dipta007",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": true
    }
]