[
    {
        "paper": {
            "id": "2508.15882",
            "authors": [
                {
                    "_id": "68af59ed245176306494caee",
                    "user": {
                        "_id": "62659a1d24a066c92d76d2ee",
                        "avatarUrl": "/avatars/9ac398922f67350e7be6121f2cbcebcb.svg",
                        "isPro": false,
                        "fullname": "glazer",
                        "user": "netag",
                        "type": "user"
                    },
                    "name": "Neta Glazer",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:55:33.796Z",
                    "hidden": false
                },
                {
                    "_id": "68af59ed245176306494caef",
                    "user": {
                        "_id": "669cc8a75bd3f749a3d09c5b",
                        "avatarUrl": "/avatars/3faf705394ccb3740c78f2caab50ad1d.svg",
                        "isPro": false,
                        "fullname": "Yael Segal-Feldman",
                        "user": "YaelAiola",
                        "type": "user"
                    },
                    "name": "Yael Segal-Feldman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:55:03.027Z",
                    "hidden": false
                },
                {
                    "_id": "68af59ed245176306494caf0",
                    "user": {
                        "_id": "6492bc477ddf9471f55de3b8",
                        "avatarUrl": "/avatars/ba42cf14b1ae558bd29e3ff51e59531a.svg",
                        "isPro": false,
                        "fullname": "Hilit Segev",
                        "user": "Hilit",
                        "type": "user"
                    },
                    "name": "Hilit Segev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:55:09.255Z",
                    "hidden": false
                },
                {
                    "_id": "68af59ed245176306494caf1",
                    "user": {
                        "_id": "6488391759e0256b92194209",
                        "avatarUrl": "/avatars/fbc1c3fbee06e5ef0b8fa5671f111795.svg",
                        "isPro": false,
                        "fullname": "Aviv Shamsian",
                        "user": "AvivSham",
                        "type": "user"
                    },
                    "name": "Aviv Shamsian",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:55:15.902Z",
                    "hidden": false
                },
                {
                    "_id": "68af59ed245176306494caf2",
                    "name": "Asaf Buchnick",
                    "hidden": false
                },
                {
                    "_id": "68af59ed245176306494caf3",
                    "name": "Gill Hetz",
                    "hidden": false
                },
                {
                    "_id": "68af59ed245176306494caf4",
                    "name": "Ethan Fetaya",
                    "hidden": false
                },
                {
                    "_id": "68af59ed245176306494caf5",
                    "name": "Joseph Keshet",
                    "hidden": false
                },
                {
                    "_id": "68af59ed245176306494caf6",
                    "user": {
                        "_id": "63563ac12d14fcd7d83729d6",
                        "avatarUrl": "/avatars/4e78baf7edb286b2518fb4a2fa6dbe04.svg",
                        "isPro": false,
                        "fullname": "Aviv Navon",
                        "user": "AvivNavon",
                        "type": "user"
                    },
                    "name": "Aviv Navon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:55:31.653Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-21T15:42:53.000Z",
            "submittedOnDailyAt": "2025-08-28T08:27:39.741Z",
            "title": "Beyond Transcription: Mechanistic Interpretability in ASR",
            "submittedOnDailyBy": {
                "_id": "62659a1d24a066c92d76d2ee",
                "avatarUrl": "/avatars/9ac398922f67350e7be6121f2cbcebcb.svg",
                "isPro": false,
                "fullname": "glazer",
                "user": "netag",
                "type": "user"
            },
            "summary": "Interpretability methods have recently gained significant attention,\nparticularly in the context of large language models, enabling insights into\nlinguistic representations, error detection, and model behaviors such as\nhallucinations and repetitions. However, these techniques remain underexplored\nin automatic speech recognition (ASR), despite their potential to advance both\nthe performance and interpretability of ASR systems. In this work, we adapt and\nsystematically apply established interpretability methods such as logit lens,\nlinear probing, and activation patching, to examine how acoustic and semantic\ninformation evolves across layers in ASR systems. Our experiments reveal\npreviously unknown internal dynamics, including specific encoder-decoder\ninteractions responsible for repetition hallucinations and semantic biases\nencoded deep within acoustic representations. These insights demonstrate the\nbenefits of extending and applying interpretability techniques to speech\nrecognition, opening promising directions for future research on improving\nmodel transparency and robustness.",
            "upvotes": 68,
            "discussionId": "68af59ed245176306494caf7",
            "ai_summary": "Interpretability methods like logit lens, linear probing, and activation patching are applied to ASR to uncover internal dynamics, repetition hallucinations, and semantic biases, enhancing model transparency and robustness.",
            "ai_keywords": [
                "logit lens",
                "linear probing",
                "activation patching",
                "encoder-decoder interactions",
                "repetition hallucinations",
                "semantic biases"
            ]
        },
        "publishedAt": "2025-08-21T11:42:53.000Z",
        "title": "Beyond Transcription: Mechanistic Interpretability in ASR",
        "summary": "Interpretability methods have recently gained significant attention,\nparticularly in the context of large language models, enabling insights into\nlinguistic representations, error detection, and model behaviors such as\nhallucinations and repetitions. However, these techniques remain underexplored\nin automatic speech recognition (ASR), despite their potential to advance both\nthe performance and interpretability of ASR systems. In this work, we adapt and\nsystematically apply established interpretability methods such as logit lens,\nlinear probing, and activation patching, to examine how acoustic and semantic\ninformation evolves across layers in ASR systems. Our experiments reveal\npreviously unknown internal dynamics, including specific encoder-decoder\ninteractions responsible for repetition hallucinations and semantic biases\nencoded deep within acoustic representations. These insights demonstrate the\nbenefits of extending and applying interpretability techniques to speech\nrecognition, opening promising directions for future research on improving\nmodel transparency and robustness.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.15882.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "62659a1d24a066c92d76d2ee",
            "avatarUrl": "/avatars/9ac398922f67350e7be6121f2cbcebcb.svg",
            "fullname": "glazer",
            "name": "netag",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.19652",
            "authors": [
                {
                    "_id": "68afb491245176306494cbb0",
                    "user": {
                        "_id": "65e643e0e999cde61f9221d0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/T10Y2F7sSgVrKGDmIzjNe.jpeg",
                        "isPro": false,
                        "fullname": "Zongixan Ryan Li",
                        "user": "Zongxian",
                        "type": "user"
                    },
                    "name": "Zongxia Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:55:36.789Z",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbb1",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbb2",
                    "user": {
                        "_id": "62ea79dd01ed9b0e8f61ccd3",
                        "avatarUrl": "/avatars/70af83e0e267be39fcd5f23b85e2dafa.svg",
                        "isPro": false,
                        "fullname": "Chengsong Huang",
                        "user": "ChengsongHuang",
                        "type": "user"
                    },
                    "name": "Chengsong Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:55:08.595Z",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbb3",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbb4",
                    "user": {
                        "_id": "62ffa3f8311cad266f9af236",
                        "avatarUrl": "/avatars/203dac40bc546ee25a01d8715a4b3049.svg",
                        "isPro": false,
                        "fullname": "Zhenwen Liang",
                        "user": "invokerliang",
                        "type": "user"
                    },
                    "name": "Zhenwen Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:55:55.744Z",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbb5",
                    "name": "Fuxiao Liu",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbb6",
                    "name": "Jingxi Che",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbb7",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbb8",
                    "name": "Jordan Boyd-Graber",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbb9",
                    "user": {
                        "_id": "65147a1426fbd558dbd08f1b",
                        "avatarUrl": "/avatars/86574ee2d5c22e940be1c4e50be88675.svg",
                        "isPro": false,
                        "fullname": "Haitao Mi",
                        "user": "haitaominlp",
                        "type": "user"
                    },
                    "name": "Haitao Mi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:56:07.099Z",
                    "hidden": false
                },
                {
                    "_id": "68afb491245176306494cbba",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T08:01:03.000Z",
            "submittedOnDailyAt": "2025-08-28T00:15:36.833Z",
            "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
            "submittedOnDailyBy": {
                "_id": "5feab3a28a3201f8e554c969",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
                "isPro": false,
                "fullname": "Wenhao Yu",
                "user": "wyu1",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying\nthings that are not actually in the image, and language shortcuts, where they\nskip the visual part and just rely on text priors. These issues arise because\nmost post-training methods for VLMs rely on simple verifiable answer matching\nand supervise only final outputs, leaving intermediate visual reasoning without\nexplicit guidance. As a result, VLMs receive sparse visual signals and often\nlearn to prioritize language-based reasoning over visual perception. To\nmitigate this, some existing methods add visual supervision using human\nannotations or distilled labels from external large models. However, human\nannotations are labor-intensive and costly, and because external signals cannot\nadapt to the evolving policy, they cause distributional shifts that can lead to\nreward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method\nthat improves visual reasoning without relying on external visual supervisions\nvia reinforcement learning. Vision-SR1 decomposes VLM reasoning into two\nstages: visual perception and language reasoning. The model is first prompted\nto produce self-contained visual perceptions that are sufficient to answer the\nquestion without referring back the input image. To validate this\nself-containment, the same VLM model is then re-prompted to perform language\nreasoning using only the generated perception as input to compute reward. This\nself-reward is combined with supervision on final outputs, providing a balanced\ntraining signal that strengthens both visual perception and language reasoning.\nOur experiments demonstrate that Vision-SR1 improves visual reasoning,\nmitigates visual hallucinations, and reduces reliance on language shortcuts\nacross diverse vision-language tasks.",
            "upvotes": 62,
            "discussionId": "68afb491245176306494cbbb",
            "githubRepo": "https://github.com/zli12321/Vision-SR1",
            "ai_summary": "Vision-SR1 uses reinforcement learning to enhance visual reasoning in vision-language models by decomposing the process into visual perception and language reasoning stages, improving accuracy and reducing hallucinations.",
            "ai_keywords": [
                "vision-language models",
                "visual hallucinations",
                "language shortcuts",
                "visual reasoning",
                "reinforcement learning",
                "self-rewarding method",
                "visual perception",
                "language reasoning",
                "self-containment",
                "reward hacking"
            ],
            "githubStars": 48
        },
        "publishedAt": "2025-08-27T04:01:03.000Z",
        "title": "Self-Rewarding Vision-Language Model via Reasoning Decomposition",
        "summary": "Vision-Language Models (VLMs) often suffer from visual hallucinations, saying\nthings that are not actually in the image, and language shortcuts, where they\nskip the visual part and just rely on text priors. These issues arise because\nmost post-training methods for VLMs rely on simple verifiable answer matching\nand supervise only final outputs, leaving intermediate visual reasoning without\nexplicit guidance. As a result, VLMs receive sparse visual signals and often\nlearn to prioritize language-based reasoning over visual perception. To\nmitigate this, some existing methods add visual supervision using human\nannotations or distilled labels from external large models. However, human\nannotations are labor-intensive and costly, and because external signals cannot\nadapt to the evolving policy, they cause distributional shifts that can lead to\nreward hacking. In this paper, we introduce Vision-SR1, a self-rewarding method\nthat improves visual reasoning without relying on external visual supervisions\nvia reinforcement learning. Vision-SR1 decomposes VLM reasoning into two\nstages: visual perception and language reasoning. The model is first prompted\nto produce self-contained visual perceptions that are sufficient to answer the\nquestion without referring back the input image. To validate this\nself-containment, the same VLM model is then re-prompted to perform language\nreasoning using only the generated perception as input to compute reward. This\nself-reward is combined with supervision on final outputs, providing a balanced\ntraining signal that strengthens both visual perception and language reasoning.\nOur experiments demonstrate that Vision-SR1 improves visual reasoning,\nmitigates visual hallucinations, and reduces reliance on language shortcuts\nacross diverse vision-language tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19652.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5feab3a28a3201f8e554c969",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1660795228685-5feab3a28a3201f8e554c969.png",
            "fullname": "Wenhao Yu",
            "name": "wyu1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.20072",
            "authors": [
                {
                    "_id": "68afcafd245176306494cc2d",
                    "user": {
                        "_id": "662a471e94baa018b00c0f5c",
                        "avatarUrl": "/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg",
                        "isPro": false,
                        "fullname": "Zhixuan Liang",
                        "user": "Liang-ZX",
                        "type": "user"
                    },
                    "name": "Zhixuan Liang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T19:01:23.534Z",
                    "hidden": false
                },
                {
                    "_id": "68afcafd245176306494cc2e",
                    "user": {
                        "_id": "630b094f8b327c7b8b94d24c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/630b094f8b327c7b8b94d24c/Fd0ugLOHJZIi8oUZScOmX.jpeg",
                        "isPro": false,
                        "fullname": "Yizhuo Li",
                        "user": "liyz",
                        "type": "user"
                    },
                    "name": "Yizhuo Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T19:01:32.296Z",
                    "hidden": false
                },
                {
                    "_id": "68afcafd245176306494cc2f",
                    "user": {
                        "_id": "675875d45f2b9e156b83c57a",
                        "avatarUrl": "/avatars/53f3e400db2692231b4db7eaf23b0575.svg",
                        "isPro": false,
                        "fullname": "Tianshuo Yang",
                        "user": "Soultfz",
                        "type": "user"
                    },
                    "name": "Tianshuo Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T19:01:39.348Z",
                    "hidden": false
                },
                {
                    "_id": "68afcafd245176306494cc30",
                    "user": {
                        "_id": "68ae7557e1effd4a43b675b8",
                        "avatarUrl": "/avatars/870f91c8834bea68e3af002c61c01560.svg",
                        "isPro": false,
                        "fullname": "Chengyue Wu",
                        "user": "chengyuew",
                        "type": "user"
                    },
                    "name": "Chengyue Wu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T19:01:45.408Z",
                    "hidden": false
                },
                {
                    "_id": "68afcafd245176306494cc31",
                    "name": "Sitong Mao",
                    "hidden": false
                },
                {
                    "_id": "68afcafd245176306494cc32",
                    "name": "Liuao Pei",
                    "hidden": false
                },
                {
                    "_id": "68afcafd245176306494cc33",
                    "user": {
                        "_id": "64eaba6b1d8b651a9aed1203",
                        "avatarUrl": "/avatars/a0f26095c262ca1a204c27ff4bfb188c.svg",
                        "isPro": false,
                        "fullname": "yangxiaokang",
                        "user": "zicheqingluo",
                        "type": "user"
                    },
                    "name": "Xiaokang Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T19:01:56.672Z",
                    "hidden": false
                },
                {
                    "_id": "68afcafd245176306494cc34",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "68afcafd245176306494cc35",
                    "name": "Yao Mu",
                    "hidden": false
                },
                {
                    "_id": "68afcafd245176306494cc36",
                    "name": "Ping Luo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T17:39:11.000Z",
            "submittedOnDailyAt": "2025-08-28T01:51:55.123Z",
            "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies",
            "submittedOnDailyBy": {
                "_id": "662a471e94baa018b00c0f5c",
                "avatarUrl": "/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg",
                "isPro": false,
                "fullname": "Zhixuan Liang",
                "user": "Liang-ZX",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions to robot actions. However, prevailing VLA decoders\neither generate actions autoregressively in a fixed left-to-right order or\nattach continuous diffusion or flow matching heads outside the backbone,\ndemanding specialized training and iterative sampling that hinder a unified,\nscalable architecture. We present Discrete Diffusion VLA, a single-transformer\npolicy that models discretized action chunks with discrete diffusion and is\ntrained with the same cross-entropy objective as the VLM backbone. The design\nretains diffusion's progressive refinement paradigm while remaining natively\ncompatible with the discrete token interface of VLMs. Our method achieves an\nadaptive decoding order that resolves easy action elements before harder ones\nand uses secondary remasking to revisit uncertain predictions across refinement\nrounds, which improves consistency and enables robust error correction. This\nunified decoder preserves pretrained vision language priors, supports parallel\ndecoding, breaks the autoregressive bottleneck, and reduces the number of\nfunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,\n71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv\nBridge, improving over both autoregressive and continuous diffusion baselines.\nThese findings indicate that discrete-diffusion action decoder supports precise\naction modeling and consistent training, laying groundwork for scaling VLA to\nlarger models and datasets.",
            "upvotes": 19,
            "discussionId": "68afcafd245176306494cc37",
            "ai_summary": "Discrete Diffusion VLA uses a single-transformer policy with discrete diffusion to model actions, improving decoding order, consistency, and performance over autoregressive and continuous diffusion methods.",
            "ai_keywords": [
                "vision-language-action models",
                "VLA decoders",
                "autoregressive",
                "continuous diffusion",
                "flow matching",
                "single-transformer policy",
                "discrete diffusion",
                "cross-entropy objective",
                "discrete token interface",
                "adaptive decoding order",
                "secondary remasking",
                "pretrained vision language priors",
                "parallel decoding",
                "autoregressive bottleneck",
                "function evaluations",
                "LIBERO",
                "SimplerEnv Fractal",
                "SimplerEnv Bridge"
            ]
        },
        "publishedAt": "2025-08-27T13:39:11.000Z",
        "title": "Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding\n  in Vision-Language-Action Policies",
        "summary": "Vision-Language-Action (VLA) models adapt large vision-language backbones to\nmap images and instructions to robot actions. However, prevailing VLA decoders\neither generate actions autoregressively in a fixed left-to-right order or\nattach continuous diffusion or flow matching heads outside the backbone,\ndemanding specialized training and iterative sampling that hinder a unified,\nscalable architecture. We present Discrete Diffusion VLA, a single-transformer\npolicy that models discretized action chunks with discrete diffusion and is\ntrained with the same cross-entropy objective as the VLM backbone. The design\nretains diffusion's progressive refinement paradigm while remaining natively\ncompatible with the discrete token interface of VLMs. Our method achieves an\nadaptive decoding order that resolves easy action elements before harder ones\nand uses secondary remasking to revisit uncertain predictions across refinement\nrounds, which improves consistency and enables robust error correction. This\nunified decoder preserves pretrained vision language priors, supports parallel\ndecoding, breaks the autoregressive bottleneck, and reduces the number of\nfunction evaluations. Discrete Diffusion VLA achieves 96.3% avg. SR on LIBERO,\n71.2% visual matching on SimplerEnv Fractal and 49.3% overall on SimplerEnv\nBridge, improving over both autoregressive and continuous diffusion baselines.\nThese findings indicate that discrete-diffusion action decoder supports precise\naction modeling and consistent training, laying groundwork for scaling VLA to\nlarger models and datasets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20072.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "662a471e94baa018b00c0f5c",
            "avatarUrl": "/avatars/62a67a2ee6e4b9a7124f8b02b9b3f280.svg",
            "fullname": "Zhixuan Liang",
            "name": "Liang-ZX",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.19827",
            "authors": [
                {
                    "_id": "68b039d1b19c5400014847de",
                    "name": "Samuel Lewis-Lim",
                    "hidden": false
                },
                {
                    "_id": "68b039d1b19c5400014847df",
                    "user": {
                        "_id": "643d0a4d8a55b2bbf4f2a90e",
                        "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg",
                        "isPro": false,
                        "fullname": "Xingwei Tan",
                        "user": "XingweiT",
                        "type": "user"
                    },
                    "name": "Xingwei Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T18:16:41.796Z",
                    "hidden": false
                },
                {
                    "_id": "68b039d1b19c5400014847e0",
                    "name": "Zhixue Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b039d1b19c5400014847e1",
                    "name": "Nikolaos Aletras",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T12:25:29.000Z",
            "submittedOnDailyAt": "2025-08-28T18:58:05.330Z",
            "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful\n  Post-hoc Rationalisation?",
            "submittedOnDailyBy": {
                "_id": "643d0a4d8a55b2bbf4f2a90e",
                "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg",
                "isPro": false,
                "fullname": "Xingwei Tan",
                "user": "XingweiT",
                "type": "user"
            },
            "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.",
            "upvotes": 19,
            "discussionId": "68b039d2b19c5400014847e2",
            "ai_summary": "Investigation into Chain-of-Thought dynamics and faithfulness across various models reveals inconsistencies in their reliance on CoT and its alignment with actual reasoning.",
            "ai_keywords": [
                "Chain-of-Thought",
                "CoT",
                "soft-reasoning",
                "analytical reasoning",
                "commonsense reasoning",
                "instruction-tuned",
                "reasoning models",
                "reasoning-distilled models"
            ]
        },
        "publishedAt": "2025-08-27T08:25:29.000Z",
        "title": "Analysing Chain of Thought Dynamics: Active Guidance or Unfaithful\n  Post-hoc Rationalisation?",
        "summary": "Recent work has demonstrated that Chain-of-Thought (CoT) often yields limited\ngains for soft-reasoning problems such as analytical and commonsense reasoning.\nCoT can also be unfaithful to a model's actual reasoning. We investigate the\ndynamics and faithfulness of CoT in soft-reasoning tasks across\ninstruction-tuned, reasoning and reasoning-distilled models. Our findings\nreveal differences in how these models rely on CoT, and show that CoT influence\nand faithfulness are not always aligned.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19827.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643d0a4d8a55b2bbf4f2a90e",
            "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg",
            "fullname": "Xingwei Tan",
            "name": "XingweiT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.19320",
            "authors": [
                {
                    "_id": "68b008c3245176306494cd22",
                    "user": {
                        "_id": "6507fbecffc738079ca592bf",
                        "avatarUrl": "/avatars/1cb0f39ac6dc2dba2292846a8d7746da.svg",
                        "isPro": false,
                        "fullname": "Ming Chen",
                        "user": "ChenMing-thu14",
                        "type": "user"
                    },
                    "name": "Ming Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:49:49.732Z",
                    "hidden": false
                },
                {
                    "_id": "68b008c3245176306494cd23",
                    "user": {
                        "_id": "67b2a085a8f5fdc2faf61c6c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/9ixmIHL3dqGpmj-ffYv7S.png",
                        "isPro": false,
                        "fullname": "liyuan cui",
                        "user": "Cuiliyuan",
                        "type": "user"
                    },
                    "name": "Liyuan Cui",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:56:54.892Z",
                    "hidden": false
                },
                {
                    "_id": "68b008c3245176306494cd24",
                    "name": "Wenyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b008c3245176306494cd25",
                    "name": "Haoxian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b008c3245176306494cd26",
                    "name": "Yan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68b008c3245176306494cd27",
                    "name": "Xiaohan Li",
                    "hidden": false
                },
                {
                    "_id": "68b008c3245176306494cd28",
                    "name": "Xiaoqiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68b008c3245176306494cd29",
                    "name": "Pengfei Wan",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64d0528459503263d9fb2a2d/87eJrFXOT9FcO6PwV0D2g.mp4",
                "https://cdn-uploads.huggingface.co/production/uploads/64d0528459503263d9fb2a2d/czZyZzMlIrsIgv_mVI_ez.mp4"
            ],
            "publishedAt": "2025-08-26T14:00:16.000Z",
            "submittedOnDailyAt": "2025-08-28T06:48:47.053Z",
            "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time\n  Autoregressive Video Generation",
            "submittedOnDailyBy": {
                "_id": "64d0528459503263d9fb2a2d",
                "avatarUrl": "/avatars/902b44ccbc5074fcf1fa7da373b38f9f.svg",
                "isPro": false,
                "fullname": "Zhang Wenyuan",
                "user": "zParquet",
                "type": "user"
            },
            "summary": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with high latency, heavy\ncomputational cost, and limited controllability. In this work, we introduce an\nautoregressive video generation framework that enables interactive multimodal\ncontrol and low-latency extrapolation in a streaming manner. With minimal\nmodifications to a standard large language model (LLM), our framework accepts\nmultimodal condition encodings including audio, pose, and text, and outputs\nspatially and semantically coherent representations to guide the denoising\nprocess of a diffusion head. To support this, we construct a large-scale\ndialogue dataset of approximately 20,000 hours from multiple sources, providing\nrich conversational scenarios for training. We further introduce a deep\ncompression autoencoder with up to 64times reduction ratio, which\neffectively alleviates the long-horizon inference burden of the autoregressive\nmodel. Extensive experiments on duplex conversation, multilingual human\nsynthesis, and interactive world model highlight the advantages of our approach\nin low latency, high efficiency, and fine-grained multimodal controllability.",
            "upvotes": 18,
            "discussionId": "68b008c3245176306494cd2a",
            "projectPage": "https://chenmingthu.github.io/milm/",
            "ai_summary": "An autoregressive video generation framework with multimodal control and low-latency extrapolation uses a modified large language model and a deep compression autoencoder to achieve high efficiency and fine-grained controllability.",
            "ai_keywords": [
                "autoregressive video generation",
                "multimodal control",
                "low-latency extrapolation",
                "large language model",
                "multimodal condition encodings",
                "denoising process",
                "diffusion head",
                "deep compression autoencoder",
                "duplex conversation",
                "multilingual human synthesis",
                "interactive world model"
            ]
        },
        "publishedAt": "2025-08-26T10:00:16.000Z",
        "title": "MIDAS: Multimodal Interactive Digital-human Synthesis via Real-time\n  Autoregressive Video Generation",
        "summary": "Recently, interactive digital human video generation has attracted widespread\nattention and achieved remarkable progress. However, building such a practical\nsystem that can interact with diverse input signals in real time remains\nchallenging to existing methods, which often struggle with high latency, heavy\ncomputational cost, and limited controllability. In this work, we introduce an\nautoregressive video generation framework that enables interactive multimodal\ncontrol and low-latency extrapolation in a streaming manner. With minimal\nmodifications to a standard large language model (LLM), our framework accepts\nmultimodal condition encodings including audio, pose, and text, and outputs\nspatially and semantically coherent representations to guide the denoising\nprocess of a diffusion head. To support this, we construct a large-scale\ndialogue dataset of approximately 20,000 hours from multiple sources, providing\nrich conversational scenarios for training. We further introduce a deep\ncompression autoencoder with up to 64times reduction ratio, which\neffectively alleviates the long-horizon inference burden of the autoregressive\nmodel. Extensive experiments on duplex conversation, multilingual human\nsynthesis, and interactive world model highlight the advantages of our approach\nin low latency, high efficiency, and fine-grained multimodal controllability.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64d0528459503263d9fb2a2d/87eJrFXOT9FcO6PwV0D2g.mp4",
            "https://cdn-uploads.huggingface.co/production/uploads/64d0528459503263d9fb2a2d/czZyZzMlIrsIgv_mVI_ez.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19320.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d0528459503263d9fb2a2d",
            "avatarUrl": "/avatars/902b44ccbc5074fcf1fa7da373b38f9f.svg",
            "fullname": "Zhang Wenyuan",
            "name": "zParquet",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.20096",
            "authors": [
                {
                    "_id": "68afc826245176306494cbed",
                    "user": {
                        "_id": "63fda3fced9eead590ff6918",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Zeyi Sun",
                        "user": "Zery",
                        "type": "user"
                    },
                    "name": "Zeyi Sun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:54:59.726Z",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbee",
                    "name": "Yuhang Cao",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbef",
                    "name": "Jianze Liang",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbf0",
                    "name": "Qiushi Sun",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbf1",
                    "name": "Ziyu Liu",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbf2",
                    "user": {
                        "_id": "65ab5332043d53781a115475",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65ab5332043d53781a115475/UaxSFDWteYsByzx7G_KKy.jpeg",
                        "isPro": false,
                        "fullname": "Zhixiong Zhang",
                        "user": "rookiexiong",
                        "type": "user"
                    },
                    "name": "Zhixiong Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:54:53.634Z",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbf3",
                    "user": {
                        "_id": "63859cf3b2906edaf83af9f0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63859cf3b2906edaf83af9f0/mCgynEtoXdILvzyoPexii.png",
                        "isPro": false,
                        "fullname": "Yuhang Zang",
                        "user": "yuhangzang",
                        "type": "user"
                    },
                    "name": "Yuhang Zang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:54:56.689Z",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbf4",
                    "name": "Xiaoyi Dong",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbf5",
                    "name": "Kai Chen",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbf6",
                    "name": "Dahua Lin",
                    "hidden": false
                },
                {
                    "_id": "68afc826245176306494cbf7",
                    "name": "Jiaqi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T17:59:50.000Z",
            "submittedOnDailyAt": "2025-08-28T01:40:23.357Z",
            "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer\n  Use Agent with Decoupled Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "63fda3fced9eead590ff6918",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
                "isPro": false,
                "fullname": "Zeyi Sun",
                "user": "Zery",
                "type": "user"
            },
            "summary": "Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.",
            "upvotes": 13,
            "discussionId": "68afc826245176306494cbf8",
            "projectPage": "https://github.com/OpenIXCLab/CODA",
            "githubRepo": "https://github.com/OpenIXCLab/CODA",
            "ai_summary": "CODA, a trainable compositional framework, combines a generalist planner and specialist executor to achieve robust execution and cross-domain generalization in scientific computing GUIs.",
            "ai_keywords": [
                "compositional frameworks",
                "planner",
                "actor",
                "long-horizon planning",
                "precise execution",
                "GRPO",
                "ScienceBoard benchmark"
            ],
            "githubStars": 15
        },
        "publishedAt": "2025-08-27T13:59:50.000Z",
        "title": "CODA: Coordinating the Cerebrum and Cerebellum for a Dual-Brain Computer\n  Use Agent with Decoupled Reinforcement Learning",
        "summary": "Autonomous agents for Graphical User Interfaces (GUIs) face significant\nchallenges in specialized domains such as scientific computing, where both\nlong-horizon planning and precise execution are required. Existing approaches\nsuffer from a trade-off: generalist agents excel at planning but perform poorly\nin execution, while specialized agents demonstrate the opposite weakness.\nRecent compositional frameworks attempt to bridge this gap by combining a\nplanner and an actor, but they are typically static and non-trainable, which\nprevents adaptation from experience. This is a critical limitation given the\nscarcity of high-quality data in scientific domains. To address these\nlimitations, we introduce CODA, a novel and trainable compositional framework\nthat integrates a generalist planner (Cerebrum) with a specialist executor\n(Cerebellum), trained via a dedicated two-stage pipeline. In the first stage,\nSpecialization, we apply a decoupled GRPO approach to train an expert planner\nfor each scientific application individually, bootstrapping from a small set of\ntask trajectories. In the second stage, Generalization, we aggregate all\nsuccessful trajectories from the specialized experts to build a consolidated\ndataset, which is then used for supervised fine-tuning of the final planner.\nThis equips CODA with both robust execution and cross-domain generalization.\nEvaluated on four challenging applications from the ScienceBoard benchmark,\nCODA significantly outperforms baselines and establishes a new state of the art\namong open-source models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20096.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63fda3fced9eead590ff6918",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1677566802735-noauth.jpeg",
            "fullname": "Zeyi Sun",
            "name": "Zery",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 22
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.19228",
            "authors": [
                {
                    "_id": "68afcddc245176306494cc49",
                    "user": {
                        "_id": "60cf8a354061635e43b28f60",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/o8gv9mG5cvnopJZJnNibi.jpeg",
                        "isPro": true,
                        "fullname": "Zayd Muhammad Kawakibi Zuhri",
                        "user": "zaydzuhri",
                        "type": "user"
                    },
                    "name": "Zayd M. K. Zuhri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:54:43.673Z",
                    "hidden": false
                },
                {
                    "_id": "68afcddc245176306494cc4a",
                    "user": {
                        "_id": "6239ac869895e5c2a4345131",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6239ac869895e5c2a4345131/QErVhX2EdKUSg06eRrU7L.jpeg",
                        "isPro": true,
                        "fullname": "Edd",
                        "user": "Erland",
                        "type": "user"
                    },
                    "name": "Erland Hilman Fuadi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:54:41.442Z",
                    "hidden": false
                },
                {
                    "_id": "68afcddc245176306494cc4b",
                    "user": {
                        "_id": "61a4dc053205e107691e0d82",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/61a4dc053205e107691e0d82/BESoEHlHYXstXudh6dOdT.jpeg",
                        "isPro": true,
                        "fullname": "Alham Fikri Aji",
                        "user": "afaji",
                        "type": "user"
                    },
                    "name": "Alham Fikri Aji",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T18:17:09.693Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/GYb7jYmWoAsgJRooKTSUy.jpeg"
            ],
            "publishedAt": "2025-08-26T17:43:30.000Z",
            "submittedOnDailyAt": "2025-08-28T02:07:15.309Z",
            "title": "Predicting the Order of Upcoming Tokens Improves Language Modeling",
            "submittedOnDailyBy": {
                "_id": "60cf8a354061635e43b28f60",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/o8gv9mG5cvnopJZJnNibi.jpeg",
                "isPro": true,
                "fullname": "Zayd Muhammad Kawakibi Zuhri",
                "user": "zaydzuhri",
                "type": "user"
            },
            "summary": "Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to\nimprove next-token prediction (NTP) in language model training but shows\ninconsistent improvements, underperforming in standard NLP benchmarks. We argue\nthat MTP's exact future token prediction is too difficult as an auxiliary loss.\nInstead, we propose Token Order Prediction (TOP), which trains models to order\nupcoming tokens by their proximity using a learning-to-rank loss. TOP requires\nonly a single additional unembedding layer compared to MTP's multiple\ntransformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using\nNTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show\nthat TOP overall outperforms both NTP and MTP even at scale. Our code is\navailable at https://github.com/zaydzuhri/token-order-prediction",
            "upvotes": 13,
            "discussionId": "68afcddc245176306494cc4c",
            "ai_summary": "Token Order Prediction (TOP) improves language model training by ordering upcoming tokens, outperforming both Next-Token Prediction (NTP) and Multi-Token Prediction (MTP) across benchmarks.",
            "ai_keywords": [
                "Multi-Token Prediction",
                "Next-Token Prediction",
                "Token Order Prediction",
                "learning-to-rank loss",
                "unembedding layer",
                "transformer layers"
            ]
        },
        "publishedAt": "2025-08-26T13:43:30.000Z",
        "title": "Predicting the Order of Upcoming Tokens Improves Language Modeling",
        "summary": "Multi-Token Prediction (MTP) has been proposed as an auxiliary objective to\nimprove next-token prediction (NTP) in language model training but shows\ninconsistent improvements, underperforming in standard NLP benchmarks. We argue\nthat MTP's exact future token prediction is too difficult as an auxiliary loss.\nInstead, we propose Token Order Prediction (TOP), which trains models to order\nupcoming tokens by their proximity using a learning-to-rank loss. TOP requires\nonly a single additional unembedding layer compared to MTP's multiple\ntransformer layers. We pretrain models of 340M, 1.8B, and 7B parameters using\nNTP, MTP, and TOP objectives. Results on eight standard NLP benchmarks show\nthat TOP overall outperforms both NTP and MTP even at scale. Our code is\navailable at https://github.com/zaydzuhri/token-order-prediction",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/60cf8a354061635e43b28f60/GYb7jYmWoAsgJRooKTSUy.jpeg"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19228.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60cf8a354061635e43b28f60",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60cf8a354061635e43b28f60/o8gv9mG5cvnopJZJnNibi.jpeg",
            "fullname": "Zayd Muhammad Kawakibi Zuhri",
            "name": "zaydzuhri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.17924",
            "authors": [
                {
                    "_id": "68b01730245176306494cd40",
                    "name": "Konstantin Egorov",
                    "hidden": false
                },
                {
                    "_id": "68b01730245176306494cd41",
                    "name": "Stepan Botman",
                    "hidden": false
                },
                {
                    "_id": "68b01730245176306494cd42",
                    "user": {
                        "_id": "5ee8ed74464d0272c8b245da",
                        "avatarUrl": "/avatars/e9bf7fe8a637b2869d04649cd93dfc78.svg",
                        "isPro": false,
                        "fullname": "Pavel Blinov",
                        "user": "blinoff",
                        "type": "user"
                    },
                    "name": "Pavel Blinov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:49:45.516Z",
                    "hidden": false
                },
                {
                    "_id": "68b01730245176306494cd43",
                    "name": "Galina Zubkova",
                    "hidden": false
                },
                {
                    "_id": "68b01730245176306494cd44",
                    "name": "Anton Ivaschenko",
                    "hidden": false
                },
                {
                    "_id": "68b01730245176306494cd45",
                    "name": "Alexander Kolsanov",
                    "hidden": false
                },
                {
                    "_id": "68b01730245176306494cd46",
                    "name": "Andrey Savchenko",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T11:46:40.000Z",
            "submittedOnDailyAt": "2025-08-28T08:55:35.099Z",
            "title": "Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health\n  Biomarkers Estimation",
            "submittedOnDailyBy": {
                "_id": "5ee8ed74464d0272c8b245da",
                "avatarUrl": "/avatars/e9bf7fe8a637b2869d04649cd93dfc78.svg",
                "isPro": false,
                "fullname": "Pavel Blinov",
                "user": "blinoff",
                "type": "user"
            },
            "summary": "Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical\nissues of existing publicly available datasets: small size, privacy concerns\nwith facial videos, and lack of diversity in conditions. The paper introduces a\nnovel comprehensive large-scale multi-view video dataset for rPPG and health\nbiomarkers estimation. Our dataset comprises 3600 synchronized video recordings\nfrom 600 subjects, captured under varied conditions (resting and post-exercise)\nusing multiple consumer-grade cameras at different angles. To enable multimodal\nanalysis of physiological states, each recording is paired with a 100 Hz PPG\nsignal and extended health metrics, such as electrocardiogram, arterial blood\npressure, biomarkers, temperature, oxygen saturation, respiratory rate, and\nstress level. Using this data, we train an efficient rPPG model and compare its\nquality with existing approaches in cross-dataset scenarios. The public release\nof our dataset and model should significantly speed up the progress in the\ndevelopment of AI medical assistants.",
            "upvotes": 13,
            "discussionId": "68b01730245176306494cd47",
            "projectPage": "https://huggingface.co/datasets/kyegorov/mcd_rppg",
            "githubRepo": "https://github.com/ksyegorov/mcd_rppg",
            "ai_summary": "A large-scale multi-view video dataset for rPPG and health biomarkers estimation is introduced, enabling efficient rPPG model training and comparison with existing approaches.",
            "ai_keywords": [
                "rPPG",
                "multi-view video dataset",
                "PPG signal",
                "electrocardiogram",
                "arterial blood pressure",
                "biomarkers",
                "temperature",
                "oxygen saturation",
                "respiratory rate",
                "stress level"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-08-25T07:46:40.000Z",
        "title": "Gaze into the Heart: A Multi-View Video Dataset for rPPG and Health\n  Biomarkers Estimation",
        "summary": "Progress in remote PhotoPlethysmoGraphy (rPPG) is limited by the critical\nissues of existing publicly available datasets: small size, privacy concerns\nwith facial videos, and lack of diversity in conditions. The paper introduces a\nnovel comprehensive large-scale multi-view video dataset for rPPG and health\nbiomarkers estimation. Our dataset comprises 3600 synchronized video recordings\nfrom 600 subjects, captured under varied conditions (resting and post-exercise)\nusing multiple consumer-grade cameras at different angles. To enable multimodal\nanalysis of physiological states, each recording is paired with a 100 Hz PPG\nsignal and extended health metrics, such as electrocardiogram, arterial blood\npressure, biomarkers, temperature, oxygen saturation, respiratory rate, and\nstress level. Using this data, we train an efficient rPPG model and compare its\nquality with existing approaches in cross-dataset scenarios. The public release\nof our dataset and model should significantly speed up the progress in the\ndevelopment of AI medical assistants.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17924.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5ee8ed74464d0272c8b245da",
            "avatarUrl": "/avatars/e9bf7fe8a637b2869d04649cd93dfc78.svg",
            "fullname": "Pavel Blinov",
            "name": "blinoff",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.19982",
            "authors": [
                {
                    "_id": "68afd84d245176306494ccad",
                    "user": {
                        "_id": "64245f2c089d5fae56b4549a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
                        "isPro": false,
                        "fullname": "Pengxiang Li",
                        "user": "pengxiang",
                        "type": "user"
                    },
                    "name": "Pengxiang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:58:55.979Z",
                    "hidden": false
                },
                {
                    "_id": "68afd84d245176306494ccae",
                    "name": "Yefan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68afd84d245176306494ccaf",
                    "user": {
                        "_id": "631551858be84e1ab41478f4",
                        "avatarUrl": "/avatars/c464f009ec0b7ec8e144ab8fbd07de43.svg",
                        "isPro": false,
                        "fullname": "Dilxat Muhtar",
                        "user": "PumpkinCat",
                        "type": "user"
                    },
                    "name": "Dilxat Muhtar",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:59:02.063Z",
                    "hidden": false
                },
                {
                    "_id": "68afd84d245176306494ccb0",
                    "name": "Lu Yin",
                    "hidden": false
                },
                {
                    "_id": "68afd84d245176306494ccb1",
                    "name": "Shilin Yan",
                    "hidden": false
                },
                {
                    "_id": "68afd84d245176306494ccb2",
                    "name": "Li Shen",
                    "hidden": false
                },
                {
                    "_id": "68afd84d245176306494ccb3",
                    "name": "Yi Liang",
                    "hidden": false
                },
                {
                    "_id": "68afd84d245176306494ccb4",
                    "user": {
                        "_id": "648169d3cb64a2b85a4606e6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/648169d3cb64a2b85a4606e6/6nEeE3TWvO9O5dCJTOG0C.jpeg",
                        "isPro": false,
                        "fullname": "Soroush Vosoughi",
                        "user": "soroushv",
                        "type": "user"
                    },
                    "name": "Soroush Vosoughi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T19:00:53.501Z",
                    "hidden": false
                },
                {
                    "_id": "68afd84d245176306494ccb5",
                    "name": "Shiwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T15:40:25.000Z",
            "submittedOnDailyAt": "2025-08-28T02:49:43.864Z",
            "title": "Diffusion Language Models Know the Answer Before Decoding",
            "submittedOnDailyBy": {
                "_id": "64245f2c089d5fae56b4549a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
                "isPro": false,
                "fullname": "Pengxiang Li",
                "user": "pengxiang",
                "type": "user"
            },
            "summary": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.",
            "upvotes": 10,
            "discussionId": "68afd84e245176306494ccb6",
            "ai_summary": "Prophet, a training-free fast decoding paradigm for diffusion language models, reduces inference time by leveraging early answer convergence without sacrificing quality.",
            "ai_keywords": [
                "diffusion language models",
                "DLMs",
                "autoregressive approaches",
                "parallel sequence generation",
                "bidirectional attention",
                "refinement steps",
                "early answer convergence",
                "semi-autoregressive",
                "random remasking schedules",
                "Prophet",
                "confidence gap",
                "top-2 prediction candidates",
                "early commit decoding",
                "LLaDA-8B",
                "Dream-7B",
                "decoding steps",
                "generation quality"
            ]
        },
        "publishedAt": "2025-08-27T11:40:25.000Z",
        "title": "Diffusion Language Models Know the Answer Before Decoding",
        "summary": "Diffusion language models (DLMs) have recently emerged as an alternative to\nautoregressive approaches, offering parallel sequence generation and flexible\ntoken orders. However, their inference remains slower than that of\nautoregressive models, primarily due to the cost of bidirectional attention and\nthe large number of refinement steps required for high quality outputs. In this\nwork, we highlight and leverage an overlooked property of DLMs early answer\nconvergence: in many cases, the correct answer can be internally identified by\nhalf steps before the final decoding step, both under semi-autoregressive and\nrandom remasking schedules. For example, on GSM8K and MMLU, up to 97% and 99%\nof instances, respectively, can be decoded correctly using only half of the\nrefinement steps. Building on this observation, we introduce Prophet, a\ntraining-free fast decoding paradigm that enables early commit decoding.\nSpecifically, Prophet dynamically decides whether to continue refinement or to\ngo \"all-in\" (i.e., decode all remaining tokens in one step), using the\nconfidence gap between the top-2 prediction candidates as the criterion. It\nintegrates seamlessly into existing DLM implementations, incurs negligible\noverhead, and requires no additional training. Empirical evaluations of\nLLaDA-8B and Dream-7B across multiple tasks show that Prophet reduces the\nnumber of decoding steps by up to 3.4x while preserving high generation\nquality. These results recast DLM decoding as a problem of when to stop\nsampling, and demonstrate that early decode convergence provides a simple yet\npowerful mechanism for accelerating DLM inference, complementary to existing\nspeedup techniques. Our code is publicly available at\nhttps://github.com/pixeli99/Prophet.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19982.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64245f2c089d5fae56b4549a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64245f2c089d5fae56b4549a/qUHFsL9Svwyj5BKpfMtaY.jpeg",
            "fullname": "Pengxiang Li",
            "name": "pengxiang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.19229",
            "authors": [
                {
                    "_id": "68afc7a2245176306494cbe4",
                    "name": "Wei Xiong",
                    "hidden": false
                },
                {
                    "_id": "68afc7a2245176306494cbe5",
                    "name": "Wenting Zhao",
                    "hidden": false
                },
                {
                    "_id": "68afc7a2245176306494cbe6",
                    "name": "Weizhe Yuan",
                    "hidden": false
                },
                {
                    "_id": "68afc7a2245176306494cbe7",
                    "name": "Olga Golovneva",
                    "hidden": false
                },
                {
                    "_id": "68afc7a2245176306494cbe8",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68afc7a2245176306494cbe9",
                    "name": "Jason Weston",
                    "hidden": false
                },
                {
                    "_id": "68afc7a2245176306494cbea",
                    "user": {
                        "_id": "66a8611eb51510d82ed54231",
                        "avatarUrl": "/avatars/ad559e774fee4914091b82c9831ae2a2.svg",
                        "isPro": false,
                        "fullname": "Sainbayar Sukhbaatar",
                        "user": "sainbar",
                        "type": "user"
                    },
                    "name": "Sainbayar Sukhbaatar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:55:02.227Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-26T17:45:05.000Z",
            "submittedOnDailyAt": "2025-08-28T01:36:51.818Z",
            "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
            "submittedOnDailyBy": {
                "_id": "63ffdbaab09f82a81a222c27",
                "avatarUrl": "/avatars/3aca53f6555f4548489844902fe4a80a.svg",
                "isPro": false,
                "fullname": "Wenting Zhao",
                "user": "wentingzhao",
                "type": "user"
            },
            "summary": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search.",
            "upvotes": 10,
            "discussionId": "68afc7a3245176306494cbeb",
            "ai_summary": "A generative judge model, StepWiser, uses reinforcement learning to provide step-by-step reasoning feedback, improving both training and inference performance of policy models.",
            "ai_keywords": [
                "multi-step reasoning",
                "supervising",
                "logical validity",
                "process reward models",
                "step-by-step feedback",
                "classifiers",
                "explanations",
                "supervised fine-tuning",
                "static datasets",
                "generative judge",
                "meta-reasoning",
                "thinking tokens",
                "reinforcement learning",
                "rollouts",
                "judgment accuracy",
                "inference-time search"
            ]
        },
        "publishedAt": "2025-08-26T13:45:05.000Z",
        "title": "StepWiser: Stepwise Generative Judges for Wiser Reasoning",
        "summary": "As models increasingly leverage multi-step reasoning strategies to solve\ncomplex problems, supervising the logical validity of these intermediate steps\nhas become a critical research challenge. Process reward models address this by\nproviding step-by-step feedback, but current approaches have two major\ndrawbacks: they typically function as classifiers without providing\nexplanations, and their reliance on supervised fine-tuning with static datasets\nlimits generalization. Inspired by recent advances, we reframe stepwise reward\nmodeling from a classification task to a reasoning task itself. We thus propose\na generative judge that reasons about the policy model's reasoning steps (i.e.,\nmeta-reasons), outputting thinking tokens before delivering a final verdict.\nOur model, StepWiser, is trained by reinforcement learning using relative\noutcomes of rollouts. We show it provides (i) better judgment accuracy on\nintermediate steps than existing methods; (ii) can be used to improve the\npolicy model at training time; and (iii) improves inference-time search.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19229.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63ffdbaab09f82a81a222c27",
            "avatarUrl": "/avatars/3aca53f6555f4548489844902fe4a80a.svg",
            "fullname": "Wenting Zhao",
            "name": "wentingzhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.19493",
            "authors": [
                {
                    "_id": "68afb4c9245176306494cbbd",
                    "name": "Zhixin Lin",
                    "hidden": false
                },
                {
                    "_id": "68afb4c9245176306494cbbe",
                    "user": {
                        "_id": "64b76528fdb702b3d8641514",
                        "avatarUrl": "/avatars/dcf7987e4c54f11fef0459c46f05ed62.svg",
                        "isPro": false,
                        "fullname": "Jungang Li",
                        "user": "Jungang",
                        "type": "user"
                    },
                    "name": "Jungang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:55:04.751Z",
                    "hidden": false
                },
                {
                    "_id": "68afb4c9245176306494cbbf",
                    "name": "Shidong Pan",
                    "hidden": false
                },
                {
                    "_id": "68afb4c9245176306494cbc0",
                    "name": "Yibo Shi",
                    "hidden": false
                },
                {
                    "_id": "68afb4c9245176306494cbc1",
                    "name": "Yue Yao",
                    "hidden": false
                },
                {
                    "_id": "68afb4c9245176306494cbc2",
                    "name": "Dongliang Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T00:41:28.000Z",
            "submittedOnDailyAt": "2025-08-28T01:20:35.412Z",
            "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered\n  Smartphone Agents",
            "submittedOnDailyBy": {
                "_id": "64b76528fdb702b3d8641514",
                "avatarUrl": "/avatars/dcf7987e4c54f11fef0459c46f05ed62.svg",
                "isPro": false,
                "fullname": "Jungang Li",
                "user": "Jungang",
                "type": "user"
            },
            "summary": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
            "upvotes": 9,
            "discussionId": "68afb4ca245176306494cbc3",
            "projectPage": "https://zhixin-l.github.io/SAPA-Bench",
            "githubRepo": "https://github.com/Zhixin-L/SAPA-Bench",
            "ai_summary": "A large-scale benchmark evaluates the privacy awareness of smartphone agents powered by Multimodal Large Language Models, revealing significant gaps in their ability to protect sensitive user information.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "privacy awareness",
                "benchmark",
                "privacy context",
                "scenario sensitivity level",
                "privacy detection capability",
                "utility-privacy tradeoff"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-08-26T20:41:28.000Z",
        "title": "Mind the Third Eye! Benchmarking Privacy Awareness in MLLM-powered\n  Smartphone Agents",
        "summary": "Smartphones bring significant convenience to users but also enable devices to\nextensively record various types of personal information. Existing smartphone\nagents powered by Multimodal Large Language Models (MLLMs) have achieved\nremarkable performance in automating different tasks. However, as the cost,\nthese agents are granted substantial access to sensitive users' personal\ninformation during this operation. To gain a thorough understanding of the\nprivacy awareness of these agents, we present the first large-scale benchmark\nencompassing 7,138 scenarios to the best of our knowledge. In addition, for\nprivacy context in scenarios, we annotate its type (e.g., Account Credentials),\nsensitivity level, and location. We then carefully benchmark seven available\nmainstream smartphone agents. Our results demonstrate that almost all\nbenchmarked agents show unsatisfying privacy awareness (RA), with performance\nremaining below 60% even with explicit hints. Overall, closed-source agents\nshow better privacy ability than open-source ones, and Gemini 2.0-flash\nachieves the best, achieving an RA of 67%. We also find that the agents'\nprivacy detection capability is highly related to scenario sensitivity level,\ni.e., the scenario with a higher sensitivity level is typically more\nidentifiable. We hope the findings enlighten the research community to rethink\nthe unbalanced utility-privacy tradeoff about smartphone agents. Our code and\nbenchmark are available at https://zhixin-l.github.io/SAPA-Bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19493.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "64b76528fdb702b3d8641514",
            "avatarUrl": "/avatars/dcf7987e4c54f11fef0459c46f05ed62.svg",
            "fullname": "Jungang Li",
            "name": "Jungang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.20088",
            "authors": [
                {
                    "_id": "68afd25f245176306494cc68",
                    "name": "Yuxin Guo",
                    "hidden": false
                },
                {
                    "_id": "68afd25f245176306494cc69",
                    "user": {
                        "_id": "63a9414e32ed73936ec0a0c8",
                        "avatarUrl": "/avatars/f181e1bb480502c2680be5296a036bdd.svg",
                        "isPro": false,
                        "fullname": "wybertwang",
                        "user": "wybertwang",
                        "type": "user"
                    },
                    "name": "Teng Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:49:54.138Z",
                    "hidden": false
                },
                {
                    "_id": "68afd25f245176306494cc6a",
                    "name": "Yuying Ge",
                    "hidden": false
                },
                {
                    "_id": "68afd25f245176306494cc6b",
                    "user": {
                        "_id": "67d30d9ae45dc43004b31425",
                        "avatarUrl": "/avatars/714f4c88710ab4cc75dda49c76dd3b09.svg",
                        "isPro": false,
                        "fullname": "Shijie Ma",
                        "user": "msj9817",
                        "type": "user"
                    },
                    "name": "Shijie Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T18:17:05.402Z",
                    "hidden": false
                },
                {
                    "_id": "68afd25f245176306494cc6c",
                    "name": "Yixiao Ge",
                    "hidden": false
                },
                {
                    "_id": "68afd25f245176306494cc6d",
                    "name": "Wei Zou",
                    "hidden": false
                },
                {
                    "_id": "68afd25f245176306494cc6e",
                    "name": "Ying Shan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T17:55:38.000Z",
            "submittedOnDailyAt": "2025-08-28T02:24:29.122Z",
            "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "63a9414e32ed73936ec0a0c8",
                "avatarUrl": "/avatars/f181e1bb480502c2680be5296a036bdd.svg",
                "isPro": false,
                "fullname": "wybertwang",
                "user": "wybertwang",
                "type": "user"
            },
            "summary": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short\naudio clips but struggle with long-form narrative audio, which requires\ntemporal coherence and compositional reasoning. To address this gap, we propose\nAudioStory, a unified framework that integrates large language models (LLMs)\nwith TTA systems to generate structured, long-form audio narratives. AudioStory\npossesses strong instruction-following reasoning generation capabilities. It\nemploys LLMs to decompose complex narrative queries into temporally ordered\nsub-tasks with contextual cues, enabling coherent scene transitions and\nemotional tone consistency. AudioStory has two appealing features: (1)\nDecoupled bridging mechanism: AudioStory disentangles LLM-diffuser\ncollaboration into two specialized components, i.e., a bridging query for\nintra-event semantic alignment and a residual query for cross-event coherence\npreservation. (2) End-to-end training: By unifying instruction comprehension\nand audio generation within a single end-to-end framework, AudioStory\neliminates the need for modular training pipelines while enhancing synergy\nbetween components. Furthermore, we establish a benchmark AudioStory-10K,\nencompassing diverse domains such as animated soundscapes and natural sound\nnarratives. Extensive experiments show the superiority of AudioStory on both\nsingle-audio generation and narrative audio generation, surpassing prior TTA\nbaselines in both instruction-following ability and audio fidelity. Our code is\navailable at https://github.com/TencentARC/AudioStory",
            "upvotes": 8,
            "discussionId": "68afd260245176306494cc6f",
            "projectPage": "https://github.com/TencentARC/AudioStory",
            "githubRepo": "https://github.com/TencentARC/AudioStory",
            "ai_summary": "AudioStory integrates large language models with text-to-audio systems to generate coherent, long-form audio narratives through a unified framework with decoupled bridging mechanisms and end-to-end training.",
            "ai_keywords": [
                "large language models",
                "text-to-audio",
                "AudioStory",
                "instruction-following reasoning",
                "temporally ordered sub-tasks",
                "contextual cues",
                "coherent scene transitions",
                "emotional tone consistency",
                "bridging query",
                "residual query",
                "end-to-end training",
                "AudioStory-10K",
                "animated soundscapes",
                "natural sound narratives",
                "single-audio generation",
                "narrative audio generation"
            ],
            "githubStars": 24
        },
        "publishedAt": "2025-08-27T13:55:38.000Z",
        "title": "AudioStory: Generating Long-Form Narrative Audio with Large Language\n  Models",
        "summary": "Recent advances in text-to-audio (TTA) generation excel at synthesizing short\naudio clips but struggle with long-form narrative audio, which requires\ntemporal coherence and compositional reasoning. To address this gap, we propose\nAudioStory, a unified framework that integrates large language models (LLMs)\nwith TTA systems to generate structured, long-form audio narratives. AudioStory\npossesses strong instruction-following reasoning generation capabilities. It\nemploys LLMs to decompose complex narrative queries into temporally ordered\nsub-tasks with contextual cues, enabling coherent scene transitions and\nemotional tone consistency. AudioStory has two appealing features: (1)\nDecoupled bridging mechanism: AudioStory disentangles LLM-diffuser\ncollaboration into two specialized components, i.e., a bridging query for\nintra-event semantic alignment and a residual query for cross-event coherence\npreservation. (2) End-to-end training: By unifying instruction comprehension\nand audio generation within a single end-to-end framework, AudioStory\neliminates the need for modular training pipelines while enhancing synergy\nbetween components. Furthermore, we establish a benchmark AudioStory-10K,\nencompassing diverse domains such as animated soundscapes and natural sound\nnarratives. Extensive experiments show the superiority of AudioStory on both\nsingle-audio generation and narrative audio generation, surpassing prior TTA\nbaselines in both instruction-following ability and audio fidelity. Our code is\navailable at https://github.com/TencentARC/AudioStory",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20088.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a9414e32ed73936ec0a0c8",
            "avatarUrl": "/avatars/f181e1bb480502c2680be5296a036bdd.svg",
            "fullname": "wybertwang",
            "name": "wybertwang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.19527",
            "authors": [
                {
                    "_id": "68afd480245176306494cc86",
                    "user": {
                        "_id": "67247961b14bb5ebb2cb2c83",
                        "avatarUrl": "/avatars/dd8c4cb9c0df3a496f4427940130f675.svg",
                        "isPro": false,
                        "fullname": "gaozhiting",
                        "user": "tjugzt",
                        "type": "user"
                    },
                    "name": "Zhiting Gao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T18:17:02.379Z",
                    "hidden": false
                },
                {
                    "_id": "68afd480245176306494cc87",
                    "name": "Dan Song",
                    "hidden": false
                },
                {
                    "_id": "68afd480245176306494cc88",
                    "name": "Diqiong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68afd480245176306494cc89",
                    "name": "Chao Xue",
                    "hidden": false
                },
                {
                    "_id": "68afd480245176306494cc8a",
                    "name": "An-An Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T02:45:09.000Z",
            "submittedOnDailyAt": "2025-08-28T02:31:15.556Z",
            "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified\n  Flow Matching and Preference Alignment",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Motion generation is essential for animating virtual characters and embodied\nagents. While recent text-driven methods have made significant strides, they\noften struggle with achieving precise alignment between linguistic descriptions\nand motion semantics, as well as with the inefficiencies of slow, multi-step\ninference. To address these issues, we introduce TMR++ Aligned Preference\nOptimization (TAPO), an innovative framework that aligns subtle motion\nvariations with textual modifiers and incorporates iterative adjustments to\nreinforce semantic grounding. To further enable real-time synthesis, we propose\nMotionFLUX, a high-speed generation framework based on deterministic rectified\nflow matching. Unlike traditional diffusion models, which require hundreds of\ndenoising steps, MotionFLUX constructs optimal transport paths between noise\ndistributions and motion spaces, facilitating real-time synthesis. The\nlinearized probability paths reduce the need for multi-step sampling typical of\nsequential methods, significantly accelerating inference time without\nsacrificing motion quality. Experimental results demonstrate that, together,\nTAPO and MotionFLUX form a unified system that outperforms state-of-the-art\napproaches in both semantic consistency and motion quality, while also\naccelerating generation speed. The code and pretrained models will be released.",
            "upvotes": 6,
            "discussionId": "68afd481245176306494cc8b",
            "ai_summary": "TAPO and MotionFLUX form a unified system that enhances semantic consistency and motion quality in text-driven motion generation while achieving real-time synthesis.",
            "ai_keywords": [
                "TMR++ Aligned Preference Optimization",
                "TAPO",
                "MotionFLUX",
                "deterministic rectified flow matching",
                "optimal transport paths",
                "noise distributions",
                "motion spaces",
                "linearized probability paths",
                "sequential methods"
            ]
        },
        "publishedAt": "2025-08-26T22:45:09.000Z",
        "title": "MotionFlux: Efficient Text-Guided Motion Generation through Rectified\n  Flow Matching and Preference Alignment",
        "summary": "Motion generation is essential for animating virtual characters and embodied\nagents. While recent text-driven methods have made significant strides, they\noften struggle with achieving precise alignment between linguistic descriptions\nand motion semantics, as well as with the inefficiencies of slow, multi-step\ninference. To address these issues, we introduce TMR++ Aligned Preference\nOptimization (TAPO), an innovative framework that aligns subtle motion\nvariations with textual modifiers and incorporates iterative adjustments to\nreinforce semantic grounding. To further enable real-time synthesis, we propose\nMotionFLUX, a high-speed generation framework based on deterministic rectified\nflow matching. Unlike traditional diffusion models, which require hundreds of\ndenoising steps, MotionFLUX constructs optimal transport paths between noise\ndistributions and motion spaces, facilitating real-time synthesis. The\nlinearized probability paths reduce the need for multi-step sampling typical of\nsequential methods, significantly accelerating inference time without\nsacrificing motion quality. Experimental results demonstrate that, together,\nTAPO and MotionFLUX form a unified system that outperforms state-of-the-art\napproaches in both semantic consistency and motion quality, while also\naccelerating generation speed. The code and pretrained models will be released.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19527.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 97
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.18179",
            "authors": [
                {
                    "_id": "68af95f3245176306494cb5a",
                    "user": {
                        "_id": "653964f156c9b35961f44456",
                        "avatarUrl": "/avatars/0245e7543bff797405a927df5d16aeca.svg",
                        "isPro": false,
                        "fullname": "Joseph Tang",
                        "user": "lilvjosephtang",
                        "type": "user"
                    },
                    "name": "Zhenwei Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-28T08:55:17.678Z",
                    "hidden": false
                },
                {
                    "_id": "68af95f3245176306494cb5b",
                    "name": "Difan Jiao",
                    "hidden": false
                },
                {
                    "_id": "68af95f3245176306494cb5c",
                    "name": "Blair Yang",
                    "hidden": false
                },
                {
                    "_id": "68af95f3245176306494cb5d",
                    "name": "Ashton Anderson",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T16:33:07.000Z",
            "submittedOnDailyAt": "2025-08-28T15:53:00.845Z",
            "title": "SEAM: Semantically Equivalent Across Modalities Benchmark for\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "653964f156c9b35961f44456",
                "avatarUrl": "/avatars/0245e7543bff797405a927df5d16aeca.svg",
                "isPro": false,
                "fullname": "Joseph Tang",
                "user": "lilvjosephtang",
                "type": "user"
            },
            "summary": "Evaluating whether vision-language models (VLMs) reason consistently across\nrepresentations is challenging because modality comparisons are typically\nconfounded by task differences and asymmetric information. We introduce SEAM, a\nbenchmark that pairs semantically equivalent inputs across four domains that\nhave existing standardized textual and visual notations. By employing distinct\nnotation systems across modalities, in contrast to OCR-based image-text\npairing, SEAM provides a rigorous comparative assessment of the\ntextual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21\ncontemporary models, we observe systematic modality imbalance: vision\nfrequently lags language in overall performance, despite the problems\ncontaining semantically equivalent information, and cross-modal agreement is\nrelatively low. Our error analysis reveals two main drivers: textual perception\nfailures from tokenization in domain notation and visual perception failures\nthat induce hallucinations. We also show that our results are largely robust to\nvisual transformations. SEAM establishes a controlled, semantically equivalent\nsetting for measuring and improving modality-agnostic reasoning.",
            "upvotes": 6,
            "discussionId": "68af95f3245176306494cb5e",
            "projectPage": "https://lilv98.github.io/SEAM-Website/",
            "githubRepo": "https://github.com/CSSLab/SEAM",
            "ai_summary": "SEAM is a benchmark that evaluates vision-language models' reasoning consistency across modalities using semantically equivalent inputs, revealing systematic modality imbalance and visual hallucinations.",
            "ai_keywords": [
                "vision-language models",
                "VLMs",
                "benchmark",
                "semantically equivalent inputs",
                "textual-symbolic reasoning",
                "visual-spatial reasoning",
                "OCR-based image-text pairing",
                "tokenization",
                "visual perception failures",
                "visual hallucinations",
                "visual transformations",
                "modality-agnostic reasoning"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-08-25T12:33:07.000Z",
        "title": "SEAM: Semantically Equivalent Across Modalities Benchmark for\n  Vision-Language Models",
        "summary": "Evaluating whether vision-language models (VLMs) reason consistently across\nrepresentations is challenging because modality comparisons are typically\nconfounded by task differences and asymmetric information. We introduce SEAM, a\nbenchmark that pairs semantically equivalent inputs across four domains that\nhave existing standardized textual and visual notations. By employing distinct\nnotation systems across modalities, in contrast to OCR-based image-text\npairing, SEAM provides a rigorous comparative assessment of the\ntextual-symbolic and visual-spatial reasoning capabilities of VLMs. Across 21\ncontemporary models, we observe systematic modality imbalance: vision\nfrequently lags language in overall performance, despite the problems\ncontaining semantically equivalent information, and cross-modal agreement is\nrelatively low. Our error analysis reveals two main drivers: textual perception\nfailures from tokenization in domain notation and visual perception failures\nthat induce hallucinations. We also show that our results are largely robust to\nvisual transformations. SEAM establishes a controlled, semantically equivalent\nsetting for measuring and improving modality-agnostic reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18179.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "653964f156c9b35961f44456",
            "avatarUrl": "/avatars/0245e7543bff797405a927df5d16aeca.svg",
            "fullname": "Joseph Tang",
            "name": "lilvjosephtang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.20033",
            "authors": [
                {
                    "_id": "68afd4f8245176306494cc9d",
                    "name": "Liana Patel",
                    "hidden": false
                },
                {
                    "_id": "68afd4f8245176306494cc9e",
                    "user": {
                        "_id": "5f3741e37e58354338621752",
                        "avatarUrl": "/avatars/b2337760c33ccca1d594824ae7353a8a.svg",
                        "isPro": false,
                        "fullname": "Negar Arabzadeh",
                        "user": "narabzad",
                        "type": "user"
                    },
                    "name": "Negar Arabzadeh",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:57:39.579Z",
                    "hidden": false
                },
                {
                    "_id": "68afd4f8245176306494cc9f",
                    "name": "Harshit Gupta",
                    "hidden": false
                },
                {
                    "_id": "68afd4f8245176306494cca0",
                    "name": "Ankita Sundar",
                    "hidden": false
                },
                {
                    "_id": "68afd4f8245176306494cca1",
                    "name": "Ion Stoica",
                    "hidden": false
                },
                {
                    "_id": "68afd4f8245176306494cca2",
                    "user": {
                        "_id": "642509c6d476e4ad5568dd31",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/642509c6d476e4ad5568dd31/dPiEV70hMeuY-cveLNPQg.jpeg",
                        "isPro": false,
                        "fullname": "Matei Zaharia",
                        "user": "mateiz",
                        "type": "user"
                    },
                    "name": "Matei Zaharia",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:57:59.132Z",
                    "hidden": false
                },
                {
                    "_id": "68afd4f8245176306494cca3",
                    "user": {
                        "_id": "677c45511fbb93b90f1c6f3d",
                        "avatarUrl": "/avatars/3a78fdd8d1debc8d267e80e4b7a6bf77.svg",
                        "isPro": false,
                        "fullname": "Carlos Guestrin",
                        "user": "guestrin",
                        "type": "user"
                    },
                    "name": "Carlos Guestrin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T18:58:04.940Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T16:36:34.000Z",
            "submittedOnDailyAt": "2025-08-28T02:33:27.543Z",
            "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for\n  Generative Research Synthesis",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of 19% across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.",
            "upvotes": 3,
            "discussionId": "68afd4f9245176306494cca4",
            "ai_summary": "DeepScholar-bench evaluates generative research synthesis systems by assessing their performance in creating related work sections from recent ArXiv papers, focusing on knowledge synthesis, retrieval quality, and verifiability.",
            "ai_keywords": [
                "generative research synthesis",
                "DeepScholar-bench",
                "retrieval quality",
                "verifiability",
                "knowledge synthesis",
                "DeepScholar-base",
                "LOTUS API",
                "search AI",
                "OpenAI's DeepResearch"
            ]
        },
        "publishedAt": "2025-08-27T12:36:34.000Z",
        "title": "DeepScholar-Bench: A Live Benchmark and Automated Evaluation for\n  Generative Research Synthesis",
        "summary": "The ability to research and synthesize knowledge is central to human\nexpertise and progress. An emerging class of systems promises these exciting\ncapabilities through generative research synthesis, performing retrieval over\nthe live web and synthesizing discovered sources into long-form, cited\nsummaries. However, evaluating such systems remains an open challenge: existing\nquestion-answering benchmarks focus on short-form factual responses, while\nexpert-curated datasets risk staleness and data contamination. Both fail to\ncapture the complexity and evolving nature of real research synthesis tasks. In\nthis work, we introduce DeepScholar-bench, a live benchmark and holistic,\nautomated evaluation framework designed to evaluate generative research\nsynthesis. DeepScholar-bench draws queries from recent, high-quality ArXiv\npapers and focuses on a real research synthesis task: generating the related\nwork sections of a paper by retrieving, synthesizing, and citing prior\nresearch. Our evaluation framework holistically assesses performance across\nthree key dimensions, knowledge synthesis, retrieval quality, and\nverifiability. We also develop DeepScholar-base, a reference pipeline\nimplemented efficiently using the LOTUS API. Using the DeepScholar-bench\nframework, we perform a systematic evaluation of prior open-source systems,\nsearch AI's, OpenAI's DeepResearch, and DeepScholar-base. We find that\nDeepScholar-base establishes a strong baseline, attaining competitive or higher\nperformance than each other method. We also find that DeepScholar-bench remains\nfar from saturated, with no system exceeding a score of 19% across all\nmetrics. These results underscore the difficulty of DeepScholar-bench, as well\nas its importance for progress towards AI systems capable of generative\nresearch synthesis. We make our code available at\nhttps://github.com/guestrin-lab/deepscholar-bench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20033.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 97
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.19559",
            "authors": [
                {
                    "_id": "68afd493245176306494cc8d",
                    "name": "Rongzhi Li",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc8e",
                    "name": "Ruogu Du",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc8f",
                    "name": "Zefang Chu",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc90",
                    "name": "Sida Zhao",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc91",
                    "name": "Chunlei Han",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc92",
                    "name": "Zuocheng Shi",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc93",
                    "user": {
                        "_id": "6699cf43c7b418d6f33f08ad",
                        "avatarUrl": "/avatars/eca38c827ddc6f83bedd5df83281eeab.svg",
                        "isPro": false,
                        "fullname": "Yiwen Shao",
                        "user": "yshao18",
                        "type": "user"
                    },
                    "name": "Yiwen Shao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T19:02:23.002Z",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc94",
                    "name": "Huanle Han",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc95",
                    "name": "Long Huang",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc96",
                    "name": "Zherui Liu",
                    "hidden": false
                },
                {
                    "_id": "68afd493245176306494cc97",
                    "user": {
                        "_id": "66b15f285e004bafbab0dd4e",
                        "avatarUrl": "/avatars/492abda5902ad271d1f0f30ae29f7ba9.svg",
                        "isPro": false,
                        "fullname": "shufan liu",
                        "user": "liu3766",
                        "type": "user"
                    },
                    "name": "Shufan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-08-28T19:02:40.477Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T04:22:02.000Z",
            "submittedOnDailyAt": "2025-08-28T02:31:35.305Z",
            "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and\n  Disaggregated LLM Inference",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Serving Large Language Models (LLMs) is a GPU-intensive task where\ntraditional autoscalers fall short, particularly for modern Prefill-Decode\n(P/D) disaggregated architectures. This architectural shift, while powerful,\nintroduces significant operational challenges, including inefficient use of\nheterogeneous hardware, network bottlenecks, and critical imbalances between\nprefill and decode stages. We introduce HeteroScale, a coordinated autoscaling\nframework that addresses the core challenges of P/D disaggregated serving.\nHeteroScale combines a topology-aware scheduler that adapts to heterogeneous\nhardware and network constraints with a novel metric-driven policy derived from\nthe first large-scale empirical study of autoscaling signals in production. By\nleveraging a single, robust metric to jointly scale prefill and decode pools,\nHeteroScale maintains architectural balance while ensuring efficient, adaptive\nresource management. Deployed in a massive production environment on tens of\nthousands of GPUs, HeteroScale has proven its effectiveness, increasing average\nGPU utilization by a significant 26.6 percentage points and saving hundreds of\nthousands of GPU-hours daily, all while upholding stringent service level\nobjectives.",
            "upvotes": 3,
            "discussionId": "68afd493245176306494cc98",
            "ai_summary": "HeteroScale, a coordinated autoscaling framework, improves GPU utilization and efficiency in serving large language models by addressing challenges in heterogeneous hardware and network constraints in Prefill-Decode architectures.",
            "ai_keywords": [
                "HeteroScale",
                "topology-aware scheduler",
                "Prefill-Decode",
                "heterogeneous hardware",
                "network bottlenecks",
                "metric-driven policy",
                "GPU utilization",
                "GPU-hours",
                "service level objectives"
            ]
        },
        "publishedAt": "2025-08-27T00:22:02.000Z",
        "title": "Taming the Chaos: Coordinated Autoscaling for Heterogeneous and\n  Disaggregated LLM Inference",
        "summary": "Serving Large Language Models (LLMs) is a GPU-intensive task where\ntraditional autoscalers fall short, particularly for modern Prefill-Decode\n(P/D) disaggregated architectures. This architectural shift, while powerful,\nintroduces significant operational challenges, including inefficient use of\nheterogeneous hardware, network bottlenecks, and critical imbalances between\nprefill and decode stages. We introduce HeteroScale, a coordinated autoscaling\nframework that addresses the core challenges of P/D disaggregated serving.\nHeteroScale combines a topology-aware scheduler that adapts to heterogeneous\nhardware and network constraints with a novel metric-driven policy derived from\nthe first large-scale empirical study of autoscaling signals in production. By\nleveraging a single, robust metric to jointly scale prefill and decode pools,\nHeteroScale maintains architectural balance while ensuring efficient, adaptive\nresource management. Deployed in a massive production environment on tens of\nthousands of GPUs, HeteroScale has proven its effectiveness, increasing average\nGPU utilization by a significant 26.6 percentage points and saving hundreds of\nthousands of GPU-hours daily, all while upholding stringent service level\nobjectives.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19559.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 97
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.16067",
            "authors": [
                {
                    "_id": "68ac77f686b21a0e2e358c4d",
                    "user": {
                        "_id": "6189699017a1bb9ce543cbad",
                        "avatarUrl": "/avatars/abb6721e8e2b279d267988c92f3099df.svg",
                        "isPro": false,
                        "fullname": "Teddy Koker",
                        "user": "teddykoker",
                        "type": "user"
                    },
                    "name": "Teddy Koker",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-25T19:31:49.935Z",
                    "hidden": false
                },
                {
                    "_id": "68ac77f686b21a0e2e358c4e",
                    "name": "Tess Smidt",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-22T03:38:06.000Z",
            "submittedOnDailyAt": "2025-08-28T21:50:54.982Z",
            "title": "Training a Foundation Model for Materials on a Budget",
            "submittedOnDailyBy": {
                "_id": "6189699017a1bb9ce543cbad",
                "avatarUrl": "/avatars/abb6721e8e2b279d267988c92f3099df.svg",
                "isPro": false,
                "fullname": "Teddy Koker",
                "user": "teddykoker",
                "type": "user"
            },
            "summary": "Foundation models for materials modeling are advancing quickly, but their\ntraining remains expensive, often placing state-of-the-art methods out of reach\nfor many research groups. We introduce Nequix, a compact E(3)-equivariant\npotential that pairs a simplified NequIP design with modern training practices,\nincluding equivariant root-mean-square layer normalization and the Muon\noptimizer, to retain accuracy while substantially reducing compute\nrequirements. Built in JAX, Nequix has 700K parameters and was trained in 500\nA100-GPU hours. On the Matbench-Discovery and MDR Phonon benchmarks, Nequix\nranks third overall while requiring less than one quarter of the training cost\nof most other methods, and it delivers an order-of-magnitude faster inference\nspeed than the current top-ranked model. We release model weights and fully\nreproducible codebase at https://github.com/atomicarchitects/nequix",
            "upvotes": 1,
            "discussionId": "68ac77f786b21a0e2e358c4f",
            "githubRepo": "https://github.com/atomicarchitects/nequix",
            "ai_summary": "Nequix, a compact E(3)-equivariant potential, achieves high accuracy with reduced computational cost and faster inference speed compared to other methods on materials modeling benchmarks.",
            "ai_keywords": [
                "E(3)-equivariant",
                "NequIP",
                "equivariant root-mean-square layer normalization",
                "Muon optimizer",
                "JAX",
                "Matbench-Discovery",
                "MDR Phonon benchmarks"
            ],
            "githubStars": 30
        },
        "publishedAt": "2025-08-21T23:38:06.000Z",
        "title": "Training a Foundation Model for Materials on a Budget",
        "summary": "Foundation models for materials modeling are advancing quickly, but their\ntraining remains expensive, often placing state-of-the-art methods out of reach\nfor many research groups. We introduce Nequix, a compact E(3)-equivariant\npotential that pairs a simplified NequIP design with modern training practices,\nincluding equivariant root-mean-square layer normalization and the Muon\noptimizer, to retain accuracy while substantially reducing compute\nrequirements. Built in JAX, Nequix has 700K parameters and was trained in 500\nA100-GPU hours. On the Matbench-Discovery and MDR Phonon benchmarks, Nequix\nranks third overall while requiring less than one quarter of the training cost\nof most other methods, and it delivers an order-of-magnitude faster inference\nspeed than the current top-ranked model. We release model weights and fully\nreproducible codebase at https://github.com/atomicarchitects/nequix",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.16067.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6189699017a1bb9ce543cbad",
            "avatarUrl": "/avatars/abb6721e8e2b279d267988c92f3099df.svg",
            "fullname": "Teddy Koker",
            "name": "teddykoker",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    }
]