[
    {
        "paper": {
            "id": "2508.21113",
            "authors": [
                {
                    "_id": "68b4f36d851c6e7b001ec9c5",
                    "name": "Jie Jiang",
                    "hidden": false
                },
                {
                    "_id": "68b4f36d851c6e7b001ec9c6",
                    "name": "Qi Yang",
                    "hidden": false
                },
                {
                    "_id": "68b4f36d851c6e7b001ec9c7",
                    "name": "Bolin Ni",
                    "hidden": false
                },
                {
                    "_id": "68b4f36d851c6e7b001ec9c8",
                    "name": "Shiming Xiang",
                    "hidden": false
                },
                {
                    "_id": "68b4f36d851c6e7b001ec9c9",
                    "name": "Han Hu",
                    "hidden": false
                },
                {
                    "_id": "68b4f36d851c6e7b001ec9ca",
                    "name": "Houwen Peng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T17:48:19.000Z",
            "submittedOnDailyAt": "2025-09-01T00:10:31.599Z",
            "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning",
            "submittedOnDailyBy": {
                "_id": "643e45c4c639f8bc9727810a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e45c4c639f8bc9727810a/BJR1cvSCxcqxr08iS7GsI.jpeg",
                "isPro": false,
                "fullname": "YannQi",
                "user": "YannQi",
                "type": "user"
            },
            "summary": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking\ncapabilities have demonstrated remarkable performance on complex reasoning\nproblems. However, this thinking process is redundant for simple problems\nsolvable without complex reasoning. To address this inefficiency, we propose\nR-4B, an auto-thinking MLLM, which can adaptively decide when to think based on\nproblem complexity. The central idea of R-4B is to empower the model with both\nthinking and non-thinking capabilities using bi-mode annealing, and apply\nBi-mode Policy Optimization~(BPO) to improve the model's accuracy in\ndetermining whether to activate the thinking process. Specifically, we first\ntrain the model on a carefully curated dataset spanning various topics, which\ncontains samples from both thinking and non-thinking modes. Then it undergoes a\nsecond phase of training under an improved GRPO framework, where the policy\nmodel is forced to generate responses from both modes for each input query.\nExperimental results show that R-4B achieves state-of-the-art performance\nacross 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks\nand achieves performance comparable to larger models such as\nKimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower\ncomputational cost.",
            "upvotes": 84,
            "discussionId": "68b4f36d851c6e7b001ec9cb",
            "githubRepo": "https://github.com/yannqi/R-4B",
            "ai_summary": "R-4B, an auto-thinking multimodal large language model, uses bi-mode annealing and Bi-mode Policy Optimization to adaptively decide on problem-solving strategies, achieving state-of-the-art performance with lower computational cost.",
            "ai_keywords": [
                "multimodal large language models",
                "step-by-step thinking",
                "auto-thinking",
                "bi-mode annealing",
                "Bi-mode Policy Optimization",
                "GRPO framework",
                "policy model",
                "reasoning-intensive benchmarks"
            ],
            "githubStars": 37
        },
        "publishedAt": "2025-08-28T13:48:19.000Z",
        "title": "R-4B: Incentivizing General-Purpose Auto-Thinking Capability in MLLMs\n  via Bi-Mode Annealing and Reinforce Learning",
        "summary": "Multimodal Large Language Models (MLLMs) equipped with step-by-step thinking\ncapabilities have demonstrated remarkable performance on complex reasoning\nproblems. However, this thinking process is redundant for simple problems\nsolvable without complex reasoning. To address this inefficiency, we propose\nR-4B, an auto-thinking MLLM, which can adaptively decide when to think based on\nproblem complexity. The central idea of R-4B is to empower the model with both\nthinking and non-thinking capabilities using bi-mode annealing, and apply\nBi-mode Policy Optimization~(BPO) to improve the model's accuracy in\ndetermining whether to activate the thinking process. Specifically, we first\ntrain the model on a carefully curated dataset spanning various topics, which\ncontains samples from both thinking and non-thinking modes. Then it undergoes a\nsecond phase of training under an improved GRPO framework, where the policy\nmodel is forced to generate responses from both modes for each input query.\nExperimental results show that R-4B achieves state-of-the-art performance\nacross 25 challenging benchmarks. It outperforms Qwen2.5-VL-7B in most tasks\nand achieves performance comparable to larger models such as\nKimi-VL-A3B-Thinking-2506 (16B) on reasoning-intensive benchmarks with lower\ncomputational cost.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21113.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643e45c4c639f8bc9727810a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643e45c4c639f8bc9727810a/BJR1cvSCxcqxr08iS7GsI.jpeg",
            "fullname": "YannQi",
            "name": "YannQi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 4
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.21112",
            "authors": [
                {
                    "_id": "68b4e4e7851c6e7b001ec9a7",
                    "user": {
                        "_id": "64daecec888b7e9c400f59b5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
                        "isPro": false,
                        "fullname": "Delin Qu",
                        "user": "delinqu",
                        "type": "user"
                    },
                    "name": "Delin Qu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T07:50:50.399Z",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9a8",
                    "name": "Haoming Song",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9a9",
                    "name": "Qizhi Chen",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9aa",
                    "name": "Zhaoqing Chen",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9ab",
                    "name": "Xianqiang Gao",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9ac",
                    "name": "Xinyi Ye",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9ad",
                    "name": "Qi Lv",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9ae",
                    "name": "Modi Shi",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9af",
                    "user": {
                        "_id": "646ec9b135f55eb49e405faa",
                        "avatarUrl": "/avatars/a17194be585d20e2a021e77a5a20e213.svg",
                        "isPro": false,
                        "fullname": "Guanghui Ren",
                        "user": "sundrops",
                        "type": "user"
                    },
                    "name": "Guanghui Ren",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T07:50:48.188Z",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9b0",
                    "name": "Cheng Ruan",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9b1",
                    "name": "Maoqing Yao",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9b2",
                    "name": "Haoran Yang",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9b3",
                    "name": "Jiacheng Bao",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9b4",
                    "name": "Bin Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b4e4e7851c6e7b001ec9b5",
                    "name": "Dong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T17:26:15.000Z",
            "submittedOnDailyAt": "2025-09-01T00:04:28.519Z",
            "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control",
            "submittedOnDailyBy": {
                "_id": "64daecec888b7e9c400f59b5",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
                "isPro": false,
                "fullname": "Delin Qu",
                "user": "delinqu",
                "type": "user"
            },
            "summary": "The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.",
            "upvotes": 58,
            "discussionId": "68b4e4e7851c6e7b001ec9b6",
            "projectPage": "https://eo-robotics.ai/eo-1",
            "githubRepo": "https://github.com/EO-Robotics/EO-1",
            "ai_summary": "EO-Robotics, comprising EO-1 model and EO-Data1.5M dataset, advances multimodal embodied reasoning and robot control through interleaved vision-text-action pre-training.",
            "ai_keywords": [
                "vision-language-action (VLA) models",
                "embodied foundation model",
                "multimodal inputs",
                "auto-regressive decoding",
                "flow matching denoising",
                "interleaved vision-text-action comprehension",
                "long-horizon tasks",
                "dexterous manipulation"
            ],
            "githubStars": 123
        },
        "publishedAt": "2025-08-28T13:26:15.000Z",
        "title": "EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for\n  General Robot Control",
        "summary": "The human ability to seamlessly perform multimodal reasoning and physical\ninteraction in the open world is a core goal for general-purpose embodied\nintelligent systems. Recent vision-language-action (VLA) models, which are\nco-trained on large-scale robot and visual-text data, have demonstrated notable\nprogress in general robot control. However, they still fail to achieve\nhuman-level flexibility in interleaved reasoning and interaction. In this work,\nintroduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 is\na unified embodied foundation model that achieves superior performance in\nmultimodal embodied reasoning and robot control through interleaved\nvision-text-action pre-training. The development of EO-1 is based on two key\npillars: (i) a unified architecture that processes multimodal inputs\nindiscriminately (image, text, video, and action), and (ii) a massive,\nhigh-quality multimodal embodied reasoning dataset, EO-Data1.5M, which contains\nover 1.5 million samples with emphasis on interleaved vision-text-action\ncomprehension. EO-1 is trained through synergies between auto-regressive\ndecoding and flow matching denoising on EO-Data1.5M, enabling seamless robot\naction generation and multimodal embodied reasoning. Extensive experiments\ndemonstrate the effectiveness of interleaved vision-text-action learning for\nopen-world understanding and generalization, validated through a variety of\nlong-horizon, dexterous manipulation tasks across multiple embodiments. This\npaper details the architecture of EO-1, the data construction strategy of\nEO-Data1.5M, and the training methodology, offering valuable insights for\ndeveloping advanced embodied foundation models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21112.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64daecec888b7e9c400f59b5",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64daecec888b7e9c400f59b5/f4pfOfWk6jYJX-Nf2-qHn.png",
            "fullname": "Delin Qu",
            "name": "delinqu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.18106",
            "authors": [
                {
                    "_id": "68b50b5f851c6e7b001eca21",
                    "user": {
                        "_id": "67ca63c1e10c4e1bf8262576",
                        "avatarUrl": "/avatars/adfc102d8d3b9222eab7e912cf707076.svg",
                        "isPro": false,
                        "fullname": "Keke Lian",
                        "user": "KekeLian",
                        "type": "user"
                    },
                    "name": "Keke Lian",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T07:50:36.564Z",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca22",
                    "name": "Bin Wang",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca23",
                    "name": "Lei Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca24",
                    "name": "Libo Chen",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca25",
                    "user": {
                        "_id": "62579c55b98dcaa7e0de285d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62579c55b98dcaa7e0de285d/0YUd5nloul_bW9yolDGGo.jpeg",
                        "isPro": false,
                        "fullname": "wangjunjie",
                        "user": "wanng",
                        "type": "user"
                    },
                    "name": "Junjie Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T07:50:42.819Z",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca26",
                    "name": "Ziming Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca27",
                    "name": "Yujiu Yang",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca28",
                    "name": "Haotong Duan",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca29",
                    "name": "Haoran Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca2a",
                    "name": "Shuang Liao",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca2b",
                    "name": "Mingda Guo",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca2c",
                    "user": {
                        "_id": "68b51853a494ea3733e4e851",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68b51853a494ea3733e4e851/-j-nJWeZZpmKzqHsIl7UN.png",
                        "isPro": false,
                        "fullname": "Jiazheng Quan",
                        "user": "jzquan",
                        "type": "user"
                    },
                    "name": "Jiazheng Quan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T13:51:34.413Z",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca2d",
                    "name": "Yilu Zhong",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca2e",
                    "name": "Chenhao He",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca2f",
                    "name": "Zichuan Chen",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca30",
                    "name": "Jie Wu",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca31",
                    "name": "Haoling Li",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca32",
                    "name": "Zhaoxuan Li",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca33",
                    "name": "Jiongchi Yu",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca34",
                    "name": "Hui Li",
                    "hidden": false
                },
                {
                    "_id": "68b50b5f851c6e7b001eca35",
                    "name": "Dong Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T15:11:11.000Z",
            "submittedOnDailyAt": "2025-09-01T01:33:35.016Z",
            "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
            "submittedOnDailyBy": {
                "_id": "62579c55b98dcaa7e0de285d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62579c55b98dcaa7e0de285d/0YUd5nloul_bW9yolDGGo.jpeg",
                "isPro": false,
                "fullname": "wangjunjie",
                "user": "wanng",
                "type": "user"
            },
            "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.",
            "upvotes": 47,
            "discussionId": "68b50b60851c6e7b001eca36",
            "githubRepo": "https://github.com/Tencent/AICGSecEval",
            "ai_summary": "A.S.E is a benchmark for evaluating the security of code generated by large language models using real-world repositories and expert-defined rules, revealing insights into model performance and decoding strategies.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "software engineering",
                "security evaluation",
                "generated code",
                "benchmarks",
                "code snippets",
                "evaluation methods",
                "reproducibility",
                "input context",
                "repository-level",
                "secure code generation",
                "CVEs",
                "build systems",
                "cross-file dependencies",
                "containerized evaluation",
                "expert-defined rules",
                "security",
                "build quality",
                "generation stability",
                "Claude-3.7-Sonnet",
                "Qwen3-235B-A22B-Instruct",
                "decoding strategies",
                "security patching"
            ],
            "githubStars": 133
        },
        "publishedAt": "2025-08-25T11:11:11.000Z",
        "title": "A.S.E: A Repository-Level Benchmark for Evaluating Security in\n  AI-Generated Code",
        "summary": "The increasing adoption of large language models (LLMs) in software\nengineering necessitates rigorous security evaluation of their generated code.\nHowever, existing benchmarks are inadequate, as they focus on isolated code\nsnippets, employ unstable evaluation methods that lack reproducibility, and\nfail to connect the quality of input context with the security of the output.\nTo address these gaps, we introduce A.S.E (AI Code Generation Security\nEvaluation), a benchmark for repository-level secure code generation. A.S.E\nconstructs tasks from real-world repositories with documented CVEs, preserving\nfull repository context like build systems and cross-file dependencies. Its\nreproducible, containerized evaluation framework uses expert-defined rules to\nprovide stable, auditable assessments of security, build quality, and\ngeneration stability. Our evaluation of leading LLMs on A.S.E reveals three key\nfindings: (1) Claude-3.7-Sonnet achieves the best overall performance. (2) The\nsecurity gap between proprietary and open-source models is narrow;\nQwen3-235B-A22B-Instruct attains the top security score. (3) Concise,\n``fast-thinking'' decoding strategies consistently outperform complex,\n``slow-thinking'' reasoning for security patching.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18106.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62579c55b98dcaa7e0de285d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/62579c55b98dcaa7e0de285d/0YUd5nloul_bW9yolDGGo.jpeg",
            "fullname": "wangjunjie",
            "name": "wanng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 30
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.20470",
            "authors": [
                {
                    "_id": "68b50e02851c6e7b001eca38",
                    "user": {
                        "_id": "66b01dc4e48856bb718f2ba8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
                        "isPro": false,
                        "fullname": "Xiaochuan Li",
                        "user": "lixiaochuan",
                        "type": "user"
                    },
                    "name": "Xiaochuan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T07:50:33.701Z",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca39",
                    "name": "Guoguang Du",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca3a",
                    "name": "Runze Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca3b",
                    "name": "Liang Jin",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca3c",
                    "name": "Qi Jia",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca3d",
                    "name": "Lihua Lu",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca3e",
                    "name": "Zhenhua Guo",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca3f",
                    "name": "Yaqian Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca40",
                    "name": "Haiyang Liu",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca41",
                    "name": "Tianqi Wang",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca42",
                    "name": "Changsheng Li",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca43",
                    "name": "Xiaoli Gong",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca44",
                    "name": "Rengang Li",
                    "hidden": false
                },
                {
                    "_id": "68b50e02851c6e7b001eca45",
                    "name": "Baoyu Fan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T06:39:41.000Z",
            "submittedOnDailyAt": "2025-09-01T05:34:52.494Z",
            "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
            "submittedOnDailyBy": {
                "_id": "66b01dc4e48856bb718f2ba8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
                "isPro": false,
                "fullname": "Xiaochuan Li",
                "user": "lixiaochuan",
                "type": "user"
            },
            "summary": "Scaling laws have validated the success and promise of large-data-trained\nmodels in creative generation across text, image, and video domains. However,\nthis paradigm faces data scarcity in the 3D domain, as there is far less of it\navailable on the internet compared to the aforementioned modalities.\nFortunately, there exist adequate videos that inherently contain commonsense\npriors, offering an alternative supervisory signal to mitigate the\ngeneralization bottleneck caused by limited native 3D data. On the one hand,\nvideos capturing multiple views of an object or scene provide a spatial\nconsistency prior for 3D generation. On the other hand, the rich semantic\ninformation contained within the videos enables the generated content to be\nmore faithful to the text prompts and semantically plausible. This paper\nexplores how to apply the video modality in 3D asset generation, spanning\ndatasets to models. We introduce Droplet3D-4M, the first large-scale video\ndataset with multi-view level annotations, and train Droplet3D, a generative\nmodel supporting both image and dense text input. Extensive experiments\nvalidate the effectiveness of our approach, demonstrating its ability to\nproduce spatially consistent and semantically plausible content. Moreover, in\ncontrast to the prevailing 3D solutions, our approach exhibits the potential\nfor extension to scene-level applications. This indicates that the commonsense\npriors from the videos significantly facilitate 3D creation. We have\nopen-sourced all resources including the dataset, code, technical framework,\nand model weights: https://dropletx.github.io/.",
            "upvotes": 39,
            "discussionId": "68b50e03851c6e7b001eca46",
            "ai_summary": "Using video data to provide commonsense priors enhances 3D asset generation, enabling spatial consistency and semantic plausibility in 3D content creation.",
            "ai_keywords": [
                "scaling laws",
                "large-data-trained models",
                "creative generation",
                "3D domain",
                "data scarcity",
                "commonsense priors",
                "supervisory signal",
                "generalization bottleneck",
                "spatial consistency prior",
                "semantic information",
                "Droplet3D-4M",
                "Droplet3D",
                "generative model",
                "image input",
                "dense text input",
                "scene-level applications"
            ]
        },
        "publishedAt": "2025-08-28T02:39:41.000Z",
        "title": "Droplet3D: Commonsense Priors from Videos Facilitate 3D Generation",
        "summary": "Scaling laws have validated the success and promise of large-data-trained\nmodels in creative generation across text, image, and video domains. However,\nthis paradigm faces data scarcity in the 3D domain, as there is far less of it\navailable on the internet compared to the aforementioned modalities.\nFortunately, there exist adequate videos that inherently contain commonsense\npriors, offering an alternative supervisory signal to mitigate the\ngeneralization bottleneck caused by limited native 3D data. On the one hand,\nvideos capturing multiple views of an object or scene provide a spatial\nconsistency prior for 3D generation. On the other hand, the rich semantic\ninformation contained within the videos enables the generated content to be\nmore faithful to the text prompts and semantically plausible. This paper\nexplores how to apply the video modality in 3D asset generation, spanning\ndatasets to models. We introduce Droplet3D-4M, the first large-scale video\ndataset with multi-view level annotations, and train Droplet3D, a generative\nmodel supporting both image and dense text input. Extensive experiments\nvalidate the effectiveness of our approach, demonstrating its ability to\nproduce spatially consistent and semantically plausible content. Moreover, in\ncontrast to the prevailing 3D solutions, our approach exhibits the potential\nfor extension to scene-level applications. This indicates that the commonsense\npriors from the videos significantly facilitate 3D creation. We have\nopen-sourced all resources including the dataset, code, technical framework,\nand model weights: https://dropletx.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20470.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66b01dc4e48856bb718f2ba8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/66b01dc4e48856bb718f2ba8/MHZyIDd3BEtsnH9M2BvzM.jpeg",
            "fullname": "Xiaochuan Li",
            "name": "lixiaochuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.21148",
            "authors": [
                {
                    "_id": "68b5534b851c6e7b001ecb3f",
                    "name": "Ming Hu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb40",
                    "name": "Chenglong Ma",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb41",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb42",
                    "user": {
                        "_id": "65f3f43fc9940817ca9a427b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg",
                        "isPro": false,
                        "fullname": "Wanghan Xu",
                        "user": "CoCoOne",
                        "type": "user"
                    },
                    "name": "Wanghan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T13:51:06.482Z",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb43",
                    "name": "Jiamin Wu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb44",
                    "name": "Jucheng Hu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb45",
                    "name": "Tianbin Li",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb46",
                    "name": "Guohang Zhuang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb47",
                    "user": {
                        "_id": "684ff37fa383bc5d6b0ff77f",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png",
                        "isPro": false,
                        "fullname": "JiaqiLiu",
                        "user": "JiaaqiLiu",
                        "type": "user"
                    },
                    "name": "Jiaqi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T13:51:04.066Z",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb48",
                    "name": "Yingzhou Lu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb49",
                    "name": "Ying Chen",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb4a",
                    "name": "Chaoyang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb4b",
                    "name": "Cheng Tan",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb4c",
                    "name": "Jie Ying",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb4d",
                    "name": "Guocheng Wu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb4e",
                    "name": "Shujian Gao",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb4f",
                    "name": "Pengcheng Chen",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb50",
                    "name": "Jiashi Lin",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb51",
                    "name": "Haitao Wu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb52",
                    "name": "Lulu Chen",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb53",
                    "name": "Fengxiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb54",
                    "name": "Yuanyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb55",
                    "name": "Xiangyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb56",
                    "name": "Feilong Tang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb57",
                    "name": "Encheng Su",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb58",
                    "name": "Junzhi Ning",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb59",
                    "name": "Xinyao Liu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb5a",
                    "name": "Ye Du",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb5b",
                    "name": "Changkai Ji",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb5c",
                    "name": "Cheng Tang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb5d",
                    "name": "Huihui Xu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb5e",
                    "name": "Ziyang Chen",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb5f",
                    "name": "Ziyan Huang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb60",
                    "name": "Jiyao Liu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb61",
                    "name": "Pengfei Jiang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb62",
                    "name": "Yizhou Wang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb63",
                    "name": "Chen Tang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb64",
                    "name": "Jianyu Wu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb65",
                    "name": "Yuchen Ren",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb66",
                    "name": "Siyuan Yan",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb67",
                    "name": "Zhonghua Wang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb68",
                    "name": "Zhongxing Xu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb69",
                    "name": "Shiyan Su",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb6a",
                    "name": "Shangquan Sun",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb6b",
                    "name": "Runkai Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb6c",
                    "name": "Zhisheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb6d",
                    "name": "Yu Liu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb6e",
                    "name": "Fudi Wang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb6f",
                    "name": "Yuanfeng Ji",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb70",
                    "name": "Yanzhou Su",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb71",
                    "name": "Hongming Shan",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb72",
                    "name": "Chunmei Feng",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb73",
                    "name": "Jiahao Xu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb74",
                    "name": "Jiangtao Yan",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb75",
                    "name": "Wenhao Tang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb76",
                    "name": "Diping Song",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb77",
                    "name": "Lihao Liu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb78",
                    "name": "Yanyan Huang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb79",
                    "name": "Lequan Yu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb7a",
                    "name": "Bin Fu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb7b",
                    "name": "Shujun Wang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb7c",
                    "name": "Xiaomeng Li",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb7d",
                    "name": "Xiaowei Hu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb7e",
                    "name": "Yun Gu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb7f",
                    "name": "Ben Fei",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb80",
                    "name": "Zhongying Deng",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb81",
                    "name": "Benyou Wang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb82",
                    "name": "Yuewen Cao",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb83",
                    "name": "Minjie Shen",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb84",
                    "name": "Haodong Duan",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb85",
                    "name": "Jie Xu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb86",
                    "name": "Yirong Chen",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb87",
                    "name": "Fang Yan",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb88",
                    "name": "Hongxia Hao",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb89",
                    "name": "Jielan Li",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb8a",
                    "name": "Jiajun Du",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb8b",
                    "name": "Yanbo Wang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb8c",
                    "name": "Imran Razzak",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb8d",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb8e",
                    "name": "Lijun Wu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb8f",
                    "name": "Conghui He",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb90",
                    "name": "Zhaohui Lu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb91",
                    "name": "Jinhai Huang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb92",
                    "name": "Yihao Liu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb93",
                    "name": "Fenghua Ling",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb94",
                    "name": "Yuqiang Li",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb95",
                    "name": "Aoran Wang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb96",
                    "name": "Qihao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb97",
                    "name": "Nanqing Dong",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb98",
                    "name": "Tianfan Fu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb99",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb9a",
                    "name": "Yan Lu",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb9b",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb9c",
                    "name": "Jin Ye",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb9d",
                    "name": "Jianfei Cai",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb9e",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecb9f",
                    "name": "Yu Qiao",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecba0",
                    "name": "Zongyuan Ge",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecba1",
                    "name": "Shixiang Tang",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecba2",
                    "name": "Junjun He",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecba3",
                    "name": "Chunfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecba4",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68b5534b851c6e7b001ecba5",
                    "name": "Bowen Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T18:30:52.000Z",
            "submittedOnDailyAt": "2025-09-01T08:37:31.450Z",
            "title": "A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers",
            "submittedOnDailyBy": {
                "_id": "65f3f43fc9940817ca9a427b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg",
                "isPro": false,
                "fullname": "Wanghan Xu",
                "user": "CoCoOne",
                "type": "user"
            },
            "summary": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is\nrepresented, integrated, and applied in scientific research, yet their progress\nis shaped by the complex nature of scientific data. This survey presents a\ncomprehensive, data-centric synthesis that reframes the development of Sci-LLMs\nas a co-evolution between models and their underlying data substrate. We\nformulate a unified taxonomy of scientific data and a hierarchical model of\nscientific knowledge, emphasizing the multimodal, cross-scale, and\ndomain-specific challenges that differentiate scientific corpora from general\nnatural language processing datasets. We systematically review recent Sci-LLMs,\nfrom general-purpose foundations to specialized models across diverse\nscientific disciplines, alongside an extensive analysis of over 270\npre-/post-training datasets, showing why Sci-LLMs pose distinct demands --\nheterogeneous, multi-scale, uncertainty-laden corpora that require\nrepresentations preserving domain invariance and enabling cross-modal\nreasoning. On evaluation, we examine over 190 benchmark datasets and trace a\nshift from static exams toward process- and discovery-oriented assessments with\nadvanced evaluation protocols. These data-centric analyses highlight persistent\nissues in scientific data development and discuss emerging solutions involving\nsemi-automated annotation pipelines and expert validation. Finally, we outline\na paradigm shift toward closed-loop systems where autonomous agents based on\nSci-LLMs actively experiment, validate, and contribute to a living, evolving\nknowledge base. Collectively, this work provides a roadmap for building\ntrustworthy, continually evolving artificial intelligence (AI) systems that\nfunction as a true partner in accelerating scientific discovery.",
            "upvotes": 27,
            "discussionId": "68b5534c851c6e7b001ecba6",
            "githubRepo": "https://github.com/open-sciencelab/Awesome-Scientific-Datasets-and-LLMs",
            "ai_summary": "Sci-LLMs are evolving through a co-development with scientific data, addressing unique challenges like multimodal and domain-specific information, and are moving towards autonomous, closed-loop systems in scientific research.",
            "ai_keywords": [
                "Scientific Large Language Models",
                "Sci-LLMs",
                "data-centric synthesis",
                "unified taxonomy",
                "hierarchical model",
                "scientific knowledge",
                "multimodal",
                "cross-scale",
                "domain-specific",
                "pre-/post-training datasets",
                "heterogeneous",
                "multi-scale",
                "uncertainty-laden",
                "domain invariance",
                "cross-modal reasoning",
                "benchmark datasets",
                "process-oriented assessments",
                "semi-automated annotation pipelines",
                "expert validation",
                "closed-loop systems",
                "autonomous agents",
                "living knowledge base",
                "trustworthy AI",
                "scientific discovery"
            ],
            "githubStars": 153
        },
        "publishedAt": "2025-08-28T14:30:52.000Z",
        "title": "A Survey of Scientific Large Language Models: From Data Foundations to\n  Agent Frontiers",
        "summary": "Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is\nrepresented, integrated, and applied in scientific research, yet their progress\nis shaped by the complex nature of scientific data. This survey presents a\ncomprehensive, data-centric synthesis that reframes the development of Sci-LLMs\nas a co-evolution between models and their underlying data substrate. We\nformulate a unified taxonomy of scientific data and a hierarchical model of\nscientific knowledge, emphasizing the multimodal, cross-scale, and\ndomain-specific challenges that differentiate scientific corpora from general\nnatural language processing datasets. We systematically review recent Sci-LLMs,\nfrom general-purpose foundations to specialized models across diverse\nscientific disciplines, alongside an extensive analysis of over 270\npre-/post-training datasets, showing why Sci-LLMs pose distinct demands --\nheterogeneous, multi-scale, uncertainty-laden corpora that require\nrepresentations preserving domain invariance and enabling cross-modal\nreasoning. On evaluation, we examine over 190 benchmark datasets and trace a\nshift from static exams toward process- and discovery-oriented assessments with\nadvanced evaluation protocols. These data-centric analyses highlight persistent\nissues in scientific data development and discuss emerging solutions involving\nsemi-automated annotation pipelines and expert validation. Finally, we outline\na paradigm shift toward closed-loop systems where autonomous agents based on\nSci-LLMs actively experiment, validate, and contribute to a living, evolving\nknowledge base. Collectively, this work provides a roadmap for building\ntrustworthy, continually evolving artificial intelligence (AI) systems that\nfunction as a true partner in accelerating scientific discovery.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21148.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65f3f43fc9940817ca9a427b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg",
            "fullname": "Wanghan Xu",
            "name": "CoCoOne",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.13618",
            "authors": [
                {
                    "_id": "68b510b7851c6e7b001eca5a",
                    "name": "Shunian Chen",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca5b",
                    "name": "Hejin Huang",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca5c",
                    "name": "Yexin Liu",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca5d",
                    "name": "Zihan Ye",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca5e",
                    "name": "Pengcheng Chen",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca5f",
                    "name": "Chenghao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca60",
                    "name": "Michael Guan",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca61",
                    "name": "Rongsheng Wang",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca62",
                    "name": "Junying Chen",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca63",
                    "name": "Guanbin Li",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca64",
                    "name": "Ser-Nam Lim",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca65",
                    "name": "Harry Yang",
                    "hidden": false
                },
                {
                    "_id": "68b510b7851c6e7b001eca66",
                    "name": "Benyou Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T08:31:15.000Z",
            "submittedOnDailyAt": "2025-09-01T01:53:13.364Z",
            "title": "TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis",
            "submittedOnDailyBy": {
                "_id": "623be9e1d1eb227788764959",
                "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
                "isPro": false,
                "fullname": "Shunian Chen",
                "user": "Shunian",
                "type": "user"
            },
            "summary": "Audio-driven talking head synthesis has achieved remarkable photorealism, yet\nstate-of-the-art (SOTA) models exhibit a critical failure: they lack\ngeneralization to the full spectrum of human diversity in ethnicity, language,\nand age groups. We argue that this generalization gap is a direct symptom of\nlimitations in existing training data, which lack the necessary scale, quality,\nand diversity. To address this challenge, we introduce TalkVid, a new\nlarge-scale, high-quality, and diverse dataset containing 1244 hours of video\nfrom 7729 unique speakers. TalkVid is curated through a principled, multi-stage\nautomated pipeline that rigorously filters for motion stability, aesthetic\nquality, and facial detail, and is validated against human judgments to ensure\nits reliability. Furthermore, we construct and release TalkVid-Bench, a\nstratified evaluation set of 500 clips meticulously balanced across key\ndemographic and linguistic axes. Our experiments demonstrate that a model\ntrained on TalkVid outperforms counterparts trained on previous datasets,\nexhibiting superior cross-dataset generalization. Crucially, our analysis on\nTalkVid-Bench reveals performance disparities across subgroups that are\nobscured by traditional aggregate metrics, underscoring its necessity for\nfuture research. Code and data can be found in\nhttps://github.com/FreedomIntelligence/TalkVid",
            "upvotes": 16,
            "discussionId": "68b510b7851c6e7b001eca67",
            "projectPage": "https://freedomintelligence.github.io/talk-vid/",
            "githubRepo": "https://github.com/FreedomIntelligence/TalkVid",
            "ai_summary": "TalkVid, a large-scale, high-quality, and diverse dataset, improves audio-driven talking head synthesis by enhancing generalization across human diversity and revealing subgroup performance disparities.",
            "ai_keywords": [
                "audio-driven talking head synthesis",
                "photorealism",
                "generalization",
                "training data",
                "large-scale dataset",
                "high-quality dataset",
                "diverse dataset",
                "motion stability",
                "aesthetic quality",
                "facial detail",
                "stratified evaluation set",
                "cross-dataset generalization",
                "performance disparities",
                "aggregate metrics"
            ],
            "githubStars": 70
        },
        "publishedAt": "2025-08-19T04:31:15.000Z",
        "title": "TalkVid: A Large-Scale Diversified Dataset for Audio-Driven Talking Head\n  Synthesis",
        "summary": "Audio-driven talking head synthesis has achieved remarkable photorealism, yet\nstate-of-the-art (SOTA) models exhibit a critical failure: they lack\ngeneralization to the full spectrum of human diversity in ethnicity, language,\nand age groups. We argue that this generalization gap is a direct symptom of\nlimitations in existing training data, which lack the necessary scale, quality,\nand diversity. To address this challenge, we introduce TalkVid, a new\nlarge-scale, high-quality, and diverse dataset containing 1244 hours of video\nfrom 7729 unique speakers. TalkVid is curated through a principled, multi-stage\nautomated pipeline that rigorously filters for motion stability, aesthetic\nquality, and facial detail, and is validated against human judgments to ensure\nits reliability. Furthermore, we construct and release TalkVid-Bench, a\nstratified evaluation set of 500 clips meticulously balanced across key\ndemographic and linguistic axes. Our experiments demonstrate that a model\ntrained on TalkVid outperforms counterparts trained on previous datasets,\nexhibiting superior cross-dataset generalization. Crucially, our analysis on\nTalkVid-Bench reveals performance disparities across subgroups that are\nobscured by traditional aggregate metrics, underscoring its necessity for\nfuture research. Code and data can be found in\nhttps://github.com/FreedomIntelligence/TalkVid",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.13618.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "623be9e1d1eb227788764959",
            "avatarUrl": "/avatars/b6521b795a59754dbb40123fd4f63b8c.svg",
            "fullname": "Shunian Chen",
            "name": "Shunian",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.21365",
            "authors": [
                {
                    "_id": "68b51815851c6e7b001eca9f",
                    "name": "Yi Liao",
                    "hidden": false
                },
                {
                    "_id": "68b51815851c6e7b001ecaa0",
                    "name": "Yu Gu",
                    "hidden": false
                },
                {
                    "_id": "68b51815851c6e7b001ecaa1",
                    "name": "Yuan Sui",
                    "hidden": false
                },
                {
                    "_id": "68b51815851c6e7b001ecaa2",
                    "name": "Zining Zhu",
                    "hidden": false
                },
                {
                    "_id": "68b51815851c6e7b001ecaa3",
                    "name": "Yifan Lu",
                    "hidden": false
                },
                {
                    "_id": "68b51815851c6e7b001ecaa4",
                    "name": "Guohua Tang",
                    "hidden": false
                },
                {
                    "_id": "68b51815851c6e7b001ecaa5",
                    "name": "Zhongqian Sun",
                    "hidden": false
                },
                {
                    "_id": "68b51815851c6e7b001ecaa6",
                    "name": "Wei Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-29T07:13:39.000Z",
            "submittedOnDailyAt": "2025-09-01T02:20:52.079Z",
            "title": "Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel at complex reasoning tasks such as\nmathematics and coding, yet they frequently struggle with simple interactive\ntasks that young children perform effortlessly. This discrepancy highlights a\ncritical gap between declarative knowledge (knowing about something) and\nprocedural knowledge (knowing how to do something). Although traditional\nreinforcement learning (RL) agents can acquire procedural knowledge through\nenvironmental interaction, they often operate as black boxes and require\nsubstantial training data. In contrast, LLMs possess extensive world knowledge\nand reasoning capabilities, but are unable to effectively convert this static\nknowledge into dynamic decision-making in interactive settings. To address this\nchallenge, we propose Think in Games (TiG), a novel framework that empowers\nLLMs to develop procedural understanding through direct interaction with game\nenvironments, while retaining their inherent reasoning and explanatory\nabilities. Specifically, TiG reformulates RL-based decision-making as a\nlanguage modeling task: LLMs generate language-guided policies, which are\nrefined iteratively through online reinforcement learning based on\nenvironmental feedback. Our experimental results show that TiG successfully\nbridges the gap between declarative and procedural knowledge, achieving\ncompetitive performance with dramatically lower data and computational demands\ncompared to conventional RL methods. Moreover, TiG provides step-by-step\nnatural language explanations for its decisions, greatly improving transparency\nand interpretability in complex interactive tasks.",
            "upvotes": 11,
            "discussionId": "68b51815851c6e7b001ecaa7",
            "ai_summary": "Think in Games (TiG) framework enables large language models to develop procedural knowledge through interactive game environments, achieving competitive performance with reduced data and computational demands while providing transparent explanations.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "complex reasoning tasks",
                "declarative knowledge",
                "procedural knowledge",
                "reinforcement learning",
                "RL",
                "environmental interaction",
                "black boxes",
                "training data",
                "world knowledge",
                "reasoning capabilities",
                "language modeling task",
                "language-guided policies",
                "online reinforcement learning",
                "environmental feedback",
                "competitive performance",
                "data demands",
                "computational demands",
                "step-by-step natural language explanations",
                "transparency",
                "interpretability"
            ]
        },
        "publishedAt": "2025-08-29T03:13:39.000Z",
        "title": "Think in Games: Learning to Reason in Games via Reinforcement Learning\n  with Large Language Models",
        "summary": "Large language models (LLMs) excel at complex reasoning tasks such as\nmathematics and coding, yet they frequently struggle with simple interactive\ntasks that young children perform effortlessly. This discrepancy highlights a\ncritical gap between declarative knowledge (knowing about something) and\nprocedural knowledge (knowing how to do something). Although traditional\nreinforcement learning (RL) agents can acquire procedural knowledge through\nenvironmental interaction, they often operate as black boxes and require\nsubstantial training data. In contrast, LLMs possess extensive world knowledge\nand reasoning capabilities, but are unable to effectively convert this static\nknowledge into dynamic decision-making in interactive settings. To address this\nchallenge, we propose Think in Games (TiG), a novel framework that empowers\nLLMs to develop procedural understanding through direct interaction with game\nenvironments, while retaining their inherent reasoning and explanatory\nabilities. Specifically, TiG reformulates RL-based decision-making as a\nlanguage modeling task: LLMs generate language-guided policies, which are\nrefined iteratively through online reinforcement learning based on\nenvironmental feedback. Our experimental results show that TiG successfully\nbridges the gap between declarative and procedural knowledge, achieving\ncompetitive performance with dramatically lower data and computational demands\ncompared to conventional RL methods. Moreover, TiG provides step-by-step\nnatural language explanations for its decisions, greatly improving transparency\nand interpretability in complex interactive tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21365.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 98
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.21767",
            "authors": [
                {
                    "_id": "68b5103a851c6e7b001eca4e",
                    "name": "Zhixiong Zeng",
                    "hidden": false
                },
                {
                    "_id": "68b5103a851c6e7b001eca4f",
                    "name": "Jing Huang",
                    "hidden": false
                },
                {
                    "_id": "68b5103a851c6e7b001eca50",
                    "name": "Liming Zheng",
                    "hidden": false
                },
                {
                    "_id": "68b5103a851c6e7b001eca51",
                    "name": "Wenkang Han",
                    "hidden": false
                },
                {
                    "_id": "68b5103a851c6e7b001eca52",
                    "name": "Yufeng Zhong",
                    "hidden": false
                },
                {
                    "_id": "68b5103a851c6e7b001eca53",
                    "name": "Lei Chen",
                    "hidden": false
                },
                {
                    "_id": "68b5103a851c6e7b001eca54",
                    "name": "Longrong Yang",
                    "hidden": false
                },
                {
                    "_id": "68b5103a851c6e7b001eca55",
                    "name": "Yingjie Chu",
                    "hidden": false
                },
                {
                    "_id": "68b5103a851c6e7b001eca56",
                    "name": "Yuzhi He",
                    "hidden": false
                },
                {
                    "_id": "68b5103a851c6e7b001eca57",
                    "name": "Lin Ma",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-29T16:40:57.000Z",
            "submittedOnDailyAt": "2025-09-01T01:47:29.488Z",
            "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "GUI agent aims to enable automated operations on Mobile/PC devices, which is\nan important task toward achieving artificial general intelligence. The rapid\nadvancement of VLMs accelerates the development of GUI agents, owing to their\npowerful capabilities in visual understanding and task planning. However,\nbuilding a GUI agent remains a challenging task due to the scarcity of\noperation trajectories, the availability of interactive infrastructure, and the\nlimitation of initial capabilities in foundation models. In this work, we\nintroduce UItron, an open-source foundational model for automatic GUI agents,\nfeaturing advanced GUI perception, grounding, and planning capabilities. UItron\nhighlights the necessity of systemic data engineering and interactive\ninfrastructure as foundational components for advancing GUI agent development.\nIt not only systematically studies a series of data engineering strategies to\nenhance training effects, but also establishes an interactive environment\nconnecting both Mobile and PC devices. In training, UItron adopts supervised\nfinetuning over perception and planning tasks in various GUI scenarios, and\nthen develop a curriculum reinforcement learning framework to enable complex\nreasoning and exploration for online environments. As a result, UItron achieves\nsuperior performance in benchmarks of GUI perception, grounding, and planning.\nIn particular, UItron highlights the interaction proficiency with top-tier\nChinese mobile APPs, as we identified a general lack of Chinese capabilities\neven in state-of-the-art solutions. To this end, we manually collect over one\nmillion steps of operation trajectories across the top 100 most popular apps,\nand build the offline and online agent evaluation environments. Experimental\nresults demonstrate that UItron achieves significant progress in Chinese app\nscenarios, propelling GUI agents one step closer to real-world application.",
            "upvotes": 8,
            "discussionId": "68b5103b851c6e7b001eca58",
            "ai_summary": "UItron, an open-source foundational model for GUI agents, enhances visual understanding and task planning through advanced perception, grounding, and planning capabilities, achieving superior performance in Chinese app scenarios.",
            "ai_keywords": [
                "GUI agents",
                "VLMs",
                "visual understanding",
                "task planning",
                "UItron",
                "GUI perception",
                "grounding",
                "planning",
                "supervised finetuning",
                "curriculum reinforcement learning",
                "offline and online agent evaluation environments"
            ]
        },
        "publishedAt": "2025-08-29T12:40:57.000Z",
        "title": "UItron: Foundational GUI Agent with Advanced Perception and Planning",
        "summary": "GUI agent aims to enable automated operations on Mobile/PC devices, which is\nan important task toward achieving artificial general intelligence. The rapid\nadvancement of VLMs accelerates the development of GUI agents, owing to their\npowerful capabilities in visual understanding and task planning. However,\nbuilding a GUI agent remains a challenging task due to the scarcity of\noperation trajectories, the availability of interactive infrastructure, and the\nlimitation of initial capabilities in foundation models. In this work, we\nintroduce UItron, an open-source foundational model for automatic GUI agents,\nfeaturing advanced GUI perception, grounding, and planning capabilities. UItron\nhighlights the necessity of systemic data engineering and interactive\ninfrastructure as foundational components for advancing GUI agent development.\nIt not only systematically studies a series of data engineering strategies to\nenhance training effects, but also establishes an interactive environment\nconnecting both Mobile and PC devices. In training, UItron adopts supervised\nfinetuning over perception and planning tasks in various GUI scenarios, and\nthen develop a curriculum reinforcement learning framework to enable complex\nreasoning and exploration for online environments. As a result, UItron achieves\nsuperior performance in benchmarks of GUI perception, grounding, and planning.\nIn particular, UItron highlights the interaction proficiency with top-tier\nChinese mobile APPs, as we identified a general lack of Chinese capabilities\neven in state-of-the-art solutions. To this end, we manually collect over one\nmillion steps of operation trajectories across the top 100 most popular apps,\nand build the offline and online agent evaluation environments. Experimental\nresults demonstrate that UItron achieves significant progress in Chinese app\nscenarios, propelling GUI agents one step closer to real-world application.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21767.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 98
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.17677",
            "authors": [
                {
                    "_id": "68aec6138a2eb0ba9b54e982",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "68aec6138a2eb0ba9b54e983",
                    "name": "Binbin Liu",
                    "hidden": false
                },
                {
                    "_id": "68aec6138a2eb0ba9b54e984",
                    "name": "Fengze Liu",
                    "hidden": false
                },
                {
                    "_id": "68aec6138a2eb0ba9b54e985",
                    "name": "Yuanfan Guo",
                    "hidden": false
                },
                {
                    "_id": "68aec6138a2eb0ba9b54e986",
                    "name": "Jiyao Deng",
                    "hidden": false
                },
                {
                    "_id": "68aec6138a2eb0ba9b54e987",
                    "name": "Xuecheng Wu",
                    "hidden": false
                },
                {
                    "_id": "68aec6138a2eb0ba9b54e988",
                    "name": "Weidong Zhou",
                    "hidden": false
                },
                {
                    "_id": "68aec6138a2eb0ba9b54e989",
                    "name": "Xiaohuan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68aec6138a2eb0ba9b54e98a",
                    "name": "Taifeng Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-25T05:18:32.000Z",
            "submittedOnDailyAt": "2025-09-01T02:09:59.539Z",
            "title": "TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training",
            "submittedOnDailyBy": {
                "_id": "668f5875b5b3081d776e4094",
                "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
                "isPro": false,
                "fullname": "Xiaohuan Zhou",
                "user": "XiaohuanZhou",
                "type": "user"
            },
            "summary": "The data mixture used in the pre-training of a language model is a\ncornerstone of its final performance. However, a static mixing strategy is\nsuboptimal, as the model's learning preferences for various data domains shift\ndynamically throughout training. Crucially, observing these evolving\npreferences in a computationally efficient manner remains a significant\nchallenge. To address this, we propose TiKMiX, a method that dynamically\nadjusts the data mixture according to the model's evolving preferences. TiKMiX\nintroduces Group Influence, an efficient metric for evaluating the impact of\ndata domains on the model. This metric enables the formulation of the data\nmixing problem as a search for an optimal, influence-maximizing distribution.\nWe solve this via two approaches: TiKMiX-D for direct optimization, and\nTiKMiX-M, which uses a regression model to predict a superior mixture. We\ntrained models with different numbers of parameters, on up to 1 trillion\ntokens. TiKMiX-D exceeds the performance of state-of-the-art methods like\nREGMIX while using just 20% of the computational resources. TiKMiX-M leads to\nan average performance gain of 2% across 9 downstream benchmarks. Our\nexperiments reveal that a model's data preferences evolve with training\nprogress and scale, and we demonstrate that dynamically adjusting the data\nmixture based on Group Influence, a direct measure of these preferences,\nsignificantly improves performance by mitigating the underdigestion of data\nseen with static ratios.",
            "upvotes": 8,
            "discussionId": "68aec6138a2eb0ba9b54e98b",
            "ai_summary": "Dynamic adjustment of data mixture based on Group Influence metric improves language model performance by adapting to evolving learning preferences.",
            "ai_keywords": [
                "Group Influence",
                "TiKMiX",
                "TiKMiX-D",
                "TiKMiX-M",
                "data mixture",
                "language model",
                "pre-training",
                "computational resources",
                "downstream benchmarks",
                "data preferences",
                "underdigestion"
            ]
        },
        "publishedAt": "2025-08-25T01:18:32.000Z",
        "title": "TiKMiX: Take Data Influence into Dynamic Mixture for Language Model\n  Pre-training",
        "summary": "The data mixture used in the pre-training of a language model is a\ncornerstone of its final performance. However, a static mixing strategy is\nsuboptimal, as the model's learning preferences for various data domains shift\ndynamically throughout training. Crucially, observing these evolving\npreferences in a computationally efficient manner remains a significant\nchallenge. To address this, we propose TiKMiX, a method that dynamically\nadjusts the data mixture according to the model's evolving preferences. TiKMiX\nintroduces Group Influence, an efficient metric for evaluating the impact of\ndata domains on the model. This metric enables the formulation of the data\nmixing problem as a search for an optimal, influence-maximizing distribution.\nWe solve this via two approaches: TiKMiX-D for direct optimization, and\nTiKMiX-M, which uses a regression model to predict a superior mixture. We\ntrained models with different numbers of parameters, on up to 1 trillion\ntokens. TiKMiX-D exceeds the performance of state-of-the-art methods like\nREGMIX while using just 20% of the computational resources. TiKMiX-M leads to\nan average performance gain of 2% across 9 downstream benchmarks. Our\nexperiments reveal that a model's data preferences evolve with training\nprogress and scale, and we demonstrate that dynamically adjusting the data\nmixture based on Group Influence, a direct measure of these preferences,\nsignificantly improves performance by mitigating the underdigestion of data\nseen with static ratios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17677.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "668f5875b5b3081d776e4094",
            "avatarUrl": "/avatars/8c763393f25afbe5fb8b132f775e746a.svg",
            "fullname": "Xiaohuan Zhou",
            "name": "XiaohuanZhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.21376",
            "authors": [
                {
                    "_id": "68b51220851c6e7b001eca69",
                    "name": "Tony Lee",
                    "hidden": false
                },
                {
                    "_id": "68b51220851c6e7b001eca6a",
                    "name": "Haoqin Tu",
                    "hidden": false
                },
                {
                    "_id": "68b51220851c6e7b001eca6b",
                    "name": "Chi Heem Wong",
                    "hidden": false
                },
                {
                    "_id": "68b51220851c6e7b001eca6c",
                    "name": "Zijun Wang",
                    "hidden": false
                },
                {
                    "_id": "68b51220851c6e7b001eca6d",
                    "name": "Siwei Yang",
                    "hidden": false
                },
                {
                    "_id": "68b51220851c6e7b001eca6e",
                    "name": "Yifan Mai",
                    "hidden": false
                },
                {
                    "_id": "68b51220851c6e7b001eca6f",
                    "name": "Yuyin Zhou",
                    "hidden": false
                },
                {
                    "_id": "68b51220851c6e7b001eca70",
                    "name": "Cihang Xie",
                    "hidden": false
                },
                {
                    "_id": "68b51220851c6e7b001eca71",
                    "name": "Percy Liang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-29T07:40:39.000Z",
            "submittedOnDailyAt": "2025-09-01T01:55:37.031Z",
            "title": "AHELM: A Holistic Evaluation of Audio-Language Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Evaluations of audio-language models (ALMs) -- multimodal models that take\ninterleaved audio and text as input and output text -- are hindered by the lack\nof standardized benchmarks; most benchmarks measure only one or two\ncapabilities and omit evaluative aspects such as fairness or safety.\nFurthermore, comparison across models is difficult as separate evaluations test\na limited number of models and use different prompting methods and inference\nparameters. To address these shortfalls, we introduce AHELM, a benchmark that\naggregates various datasets -- including 2 new synthetic audio-text datasets\ncalled PARADE, which evaluates the ALMs on avoiding stereotypes, and\nCoRe-Bench, which measures reasoning over conversational audio through\ninferential multi-turn question answering -- to holistically measure the\nperformance of ALMs across 10 aspects we have identified as important to the\ndevelopment and usage of ALMs: audio perception, knowledge, reasoning, emotion\ndetection, bias, fairness, multilinguality, robustness, toxicity, and safety.\nWe also standardize the prompts, inference parameters, and evaluation metrics\nto ensure equitable comparisons across models. We test 14 open-weight and\nclosed-API ALMs from 3 developers and 3 additional simple baseline systems each\nconsisting of an automatic speech recognizer and a language model. Our results\nshow that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits\ngroup unfairness (p=0.01) on ASR tasks whereas most of the other models do\nnot. We also find that the baseline systems perform reasonably well on AHELM,\nwith one ranking 5th overall despite having only speech-to-text capabilities.\nFor transparency, all raw prompts, model generations, and outputs are available\non our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is\nintended to be a living benchmark and new datasets and models will be added\nover time.",
            "upvotes": 6,
            "discussionId": "68b51221851c6e7b001eca72",
            "projectPage": "https://crfm.stanford.edu/helm/audio/v1.0.0/",
            "githubRepo": "https://github.com/stanford-crfm/helm",
            "ai_summary": "AHELM is a comprehensive benchmark for audio-language models that evaluates multiple aspects including fairness, safety, and reasoning across various datasets and models.",
            "ai_keywords": [
                "audio-language models",
                "multimodal models",
                "standardized benchmarks",
                "audio perception",
                "knowledge",
                "reasoning",
                "emotion detection",
                "bias",
                "fairness",
                "multilinguality",
                "robustness",
                "toxicity",
                "safety",
                "AHELM",
                "PARADE",
                "CoRe-Bench",
                "automatic speech recognizer",
                "language model",
                "Gemini 2.5 Pro",
                "group unfairness"
            ],
            "githubStars": 2443
        },
        "publishedAt": "2025-08-29T03:40:39.000Z",
        "title": "AHELM: A Holistic Evaluation of Audio-Language Models",
        "summary": "Evaluations of audio-language models (ALMs) -- multimodal models that take\ninterleaved audio and text as input and output text -- are hindered by the lack\nof standardized benchmarks; most benchmarks measure only one or two\ncapabilities and omit evaluative aspects such as fairness or safety.\nFurthermore, comparison across models is difficult as separate evaluations test\na limited number of models and use different prompting methods and inference\nparameters. To address these shortfalls, we introduce AHELM, a benchmark that\naggregates various datasets -- including 2 new synthetic audio-text datasets\ncalled PARADE, which evaluates the ALMs on avoiding stereotypes, and\nCoRe-Bench, which measures reasoning over conversational audio through\ninferential multi-turn question answering -- to holistically measure the\nperformance of ALMs across 10 aspects we have identified as important to the\ndevelopment and usage of ALMs: audio perception, knowledge, reasoning, emotion\ndetection, bias, fairness, multilinguality, robustness, toxicity, and safety.\nWe also standardize the prompts, inference parameters, and evaluation metrics\nto ensure equitable comparisons across models. We test 14 open-weight and\nclosed-API ALMs from 3 developers and 3 additional simple baseline systems each\nconsisting of an automatic speech recognizer and a language model. Our results\nshow that while Gemini 2.5 Pro ranks top in 5 out of 10 aspects, it exhibits\ngroup unfairness (p=0.01) on ASR tasks whereas most of the other models do\nnot. We also find that the baseline systems perform reasonably well on AHELM,\nwith one ranking 5th overall despite having only speech-to-text capabilities.\nFor transparency, all raw prompts, model generations, and outputs are available\non our website at https://crfm.stanford.edu/helm/audio/v1.0.0. AHELM is\nintended to be a living benchmark and new datasets and models will be added\nover time.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21376.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 98
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.21290",
            "authors": [
                {
                    "_id": "68b5221d851c6e7b001ecaa9",
                    "user": {
                        "_id": "676e17d6fb9d2aa0ad730d0a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/jC_o2O9JfK5-1Y99IgDmT.png",
                        "isPro": false,
                        "fullname": "Daria Kryvosheieva",
                        "user": "dariakryvosheieva",
                        "type": "user"
                    },
                    "name": "Daria Kryvosheieva",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T07:50:30.541Z",
                    "hidden": false
                },
                {
                    "_id": "68b5221d851c6e7b001ecaaa",
                    "name": "Saba Sturua",
                    "hidden": false
                },
                {
                    "_id": "68b5221d851c6e7b001ecaab",
                    "name": "Michael Gnther",
                    "hidden": false
                },
                {
                    "_id": "68b5221d851c6e7b001ecaac",
                    "name": "Scott Martens",
                    "hidden": false
                },
                {
                    "_id": "68b5221d851c6e7b001ecaad",
                    "name": "Han Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-29T01:18:15.000Z",
            "submittedOnDailyAt": "2025-09-01T03:03:59.758Z",
            "title": "Efficient Code Embeddings from Code Generation Models",
            "submittedOnDailyBy": {
                "_id": "603763514de52ff951d89793",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png",
                "isPro": false,
                "fullname": "Han Xiao",
                "user": "hanxiao",
                "type": "user"
            },
            "summary": "jina-code-embeddings is a novel code embedding model suite designed to\nretrieve code from natural language queries, perform technical\nquestion-answering, and identify semantically similar code snippets across\nprogramming languages. It makes innovative use of an autoregressive backbone\npre-trained on both text and code, generating embeddings via last-token\npooling. We outline the training recipe and demonstrate state-of-the-art\nperformance despite the relatively small size of the models, validating this\napproach to code embedding model construction.",
            "upvotes": 6,
            "discussionId": "68b5221e851c6e7b001ecaae",
            "ai_summary": "Jina-code-embeddings uses an autoregressive backbone pre-trained on text and code to generate embeddings for code retrieval, question-answering, and similarity identification.",
            "ai_keywords": [
                "autoregressive backbone",
                "last-token pooling",
                "code embedding model"
            ]
        },
        "publishedAt": "2025-08-28T21:18:15.000Z",
        "title": "Efficient Code Embeddings from Code Generation Models",
        "summary": "jina-code-embeddings is a novel code embedding model suite designed to\nretrieve code from natural language queries, perform technical\nquestion-answering, and identify semantically similar code snippets across\nprogramming languages. It makes innovative use of an autoregressive backbone\npre-trained on both text and code, generating embeddings via last-token\npooling. We outline the training recipe and demonstrate state-of-the-art\nperformance despite the relatively small size of the models, validating this\napproach to code embedding model construction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21290.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "603763514de52ff951d89793",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/603763514de52ff951d89793/n-QouGYg7oE5QeDaAb3Ns.png",
            "fullname": "Han Xiao",
            "name": "hanxiao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 12
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.14197",
            "authors": [
                {
                    "_id": "68b53dcf851c6e7b001ecad7",
                    "name": "Tinghan Yang",
                    "hidden": false
                },
                {
                    "_id": "68b53dcf851c6e7b001ecad8",
                    "name": "Md Ashiqur Rahman",
                    "hidden": false
                },
                {
                    "_id": "68b53dcf851c6e7b001ecad9",
                    "name": "Raymond A. Yeh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-19T18:43:14.000Z",
            "submittedOnDailyAt": "2025-09-01T05:03:51.327Z",
            "title": "CLIPSym: Delving into Symmetry Detection with CLIP",
            "submittedOnDailyBy": {
                "_id": "661e07e02a8496916011c08a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
                "isPro": false,
                "fullname": "Md Ashiqur Rahman",
                "user": "ashiq24",
                "type": "user"
            },
            "summary": "Symmetry is one of the most fundamental geometric cues in computer vision,\nand detecting it has been an ongoing challenge. With the recent advances in\nvision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP\nmodel can aid symmetry detection by leveraging the additional symmetry cues\nfound in the natural image descriptions. We propose CLIPSym, which leverages\nCLIP's image and language encoders and a rotation-equivariant decoder based on\na hybrid of Transformer and G-Convolution to detect rotation and reflection\nsymmetries. To fully utilize CLIP's language encoder, we have developed a novel\nprompting technique called Semantic-Aware Prompt Grouping (SAPG), which\naggregates a diverse set of frequent object-based prompts to better integrate\nthe semantic cues for symmetry detection. Empirically, we show that CLIPSym\noutperforms the current state-of-the-art on three standard symmetry detection\ndatasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations\nverifying the benefits of CLIP's pre-training, the proposed equivariant\ndecoder, and the SAPG technique. The code is available at\nhttps://github.com/timyoung2333/CLIPSym.",
            "upvotes": 4,
            "discussionId": "68b53dcf851c6e7b001ecada",
            "ai_summary": "CLIPSym, a vision-language model using CLIP, enhances symmetry detection through a rotation-equivariant decoder and semantic-aware prompting, outperforming existing methods on standard datasets.",
            "ai_keywords": [
                "CLIP",
                "CLIPSym",
                "rotation-equivariant decoder",
                "Transformer",
                "G-Convolution",
                "Semantic-Aware Prompt Grouping",
                "SAPG",
                "symmetry detection",
                "DENDI",
                "SDRW",
                "LDRS"
            ]
        },
        "publishedAt": "2025-08-19T14:43:14.000Z",
        "title": "CLIPSym: Delving into Symmetry Detection with CLIP",
        "summary": "Symmetry is one of the most fundamental geometric cues in computer vision,\nand detecting it has been an ongoing challenge. With the recent advances in\nvision-language models,~i.e., CLIP, we investigate whether a pre-trained CLIP\nmodel can aid symmetry detection by leveraging the additional symmetry cues\nfound in the natural image descriptions. We propose CLIPSym, which leverages\nCLIP's image and language encoders and a rotation-equivariant decoder based on\na hybrid of Transformer and G-Convolution to detect rotation and reflection\nsymmetries. To fully utilize CLIP's language encoder, we have developed a novel\nprompting technique called Semantic-Aware Prompt Grouping (SAPG), which\naggregates a diverse set of frequent object-based prompts to better integrate\nthe semantic cues for symmetry detection. Empirically, we show that CLIPSym\noutperforms the current state-of-the-art on three standard symmetry detection\ndatasets (DENDI, SDRW, and LDRS). Finally, we conduct detailed ablations\nverifying the benefits of CLIP's pre-training, the proposed equivariant\ndecoder, and the SAPG technique. The code is available at\nhttps://github.com/timyoung2333/CLIPSym.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.14197.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "661e07e02a8496916011c08a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/6vIkL4LJ0bLeJL99E20F_.jpeg",
            "fullname": "Md Ashiqur Rahman",
            "name": "ashiq24",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.21456",
            "authors": [
                {
                    "_id": "68b50fd6851c6e7b001eca48",
                    "user": {
                        "_id": "63edc468679c2cc40abd4b16",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63edc468679c2cc40abd4b16/G6dSJ_tFG28QIRt9MaxHv.jpeg",
                        "isPro": false,
                        "fullname": "Yi-Hao",
                        "user": "yihaopeng",
                        "type": "user"
                    },
                    "name": "Yi-Hao Peng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T13:51:08.657Z",
                    "hidden": false
                },
                {
                    "_id": "68b50fd6851c6e7b001eca49",
                    "name": "Dingzeyu Li",
                    "hidden": false
                },
                {
                    "_id": "68b50fd6851c6e7b001eca4a",
                    "name": "Jeffrey P. Bigham",
                    "hidden": false
                },
                {
                    "_id": "68b50fd6851c6e7b001eca4b",
                    "name": "Amy Pavel",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-29T09:39:00.000Z",
            "submittedOnDailyAt": "2025-09-01T01:45:43.503Z",
            "title": "Morae: Proactively Pausing UI Agents for User Choices",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "User interface (UI) agents promise to make inaccessible or complex UIs easier\nto access for blind and low-vision (BLV) users. However, current UI agents\ntypically perform tasks end-to-end without involving users in critical choices\nor making them aware of important contextual information, thus reducing user\nagency. For example, in our field study, a BLV participant asked to buy the\ncheapest available sparkling water, and the agent automatically chose one from\nseveral equally priced options, without mentioning alternative products with\ndifferent flavors or better ratings. To address this problem, we introduce\nMorae, a UI agent that automatically identifies decision points during task\nexecution and pauses so that users can make choices. Morae uses large\nmultimodal models to interpret user queries alongside UI code and screenshots,\nand prompt users for clarification when there is a choice to be made. In a\nstudy over real-world web tasks with BLV participants, Morae helped users\ncomplete more tasks and select options that better matched their preferences,\nas compared to baseline agents, including OpenAI Operator. More broadly, this\nwork exemplifies a mixed-initiative approach in which users benefit from the\nautomation of UI agents while being able to express their preferences.",
            "upvotes": 3,
            "discussionId": "68b50fd6851c6e7b001eca4c",
            "ai_summary": "Morae, a UI agent, enhances accessibility for BLV users by involving them in decision-making processes during task execution, using large multimodal models to interpret user queries and UI elements.",
            "ai_keywords": [
                "UI agents",
                "blind and low-vision (BLV) users",
                "decision points",
                "large multimodal models",
                "user queries",
                "UI code",
                "screenshots",
                "mixed-initiative approach"
            ]
        },
        "publishedAt": "2025-08-29T05:39:00.000Z",
        "title": "Morae: Proactively Pausing UI Agents for User Choices",
        "summary": "User interface (UI) agents promise to make inaccessible or complex UIs easier\nto access for blind and low-vision (BLV) users. However, current UI agents\ntypically perform tasks end-to-end without involving users in critical choices\nor making them aware of important contextual information, thus reducing user\nagency. For example, in our field study, a BLV participant asked to buy the\ncheapest available sparkling water, and the agent automatically chose one from\nseveral equally priced options, without mentioning alternative products with\ndifferent flavors or better ratings. To address this problem, we introduce\nMorae, a UI agent that automatically identifies decision points during task\nexecution and pauses so that users can make choices. Morae uses large\nmultimodal models to interpret user queries alongside UI code and screenshots,\nand prompt users for clarification when there is a choice to be made. In a\nstudy over real-world web tasks with BLV participants, Morae helped users\ncomplete more tasks and select options that better matched their preferences,\nas compared to baseline agents, including OpenAI Operator. More broadly, this\nwork exemplifies a mixed-initiative approach in which users benefit from the\nautomation of UI agents while being able to express their preferences.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21456.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 98
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.21188",
            "authors": [
                {
                    "_id": "68b54fe6851c6e7b001ecaf2",
                    "name": "Haoze Wu",
                    "hidden": false
                },
                {
                    "_id": "68b54fe6851c6e7b001ecaf3",
                    "name": "Cheng Wang",
                    "hidden": false
                },
                {
                    "_id": "68b54fe6851c6e7b001ecaf4",
                    "name": "Wenshuo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b54fe6851c6e7b001ecaf5",
                    "name": "Junxian He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T20:02:10.000Z",
            "submittedOnDailyAt": "2025-09-01T11:38:55.881Z",
            "title": "Model-Task Alignment Drives Distinct RL Outcomes",
            "submittedOnDailyBy": {
                "_id": "65c1cbac88462619b2e191aa",
                "avatarUrl": "/avatars/97e1c2b049282169d1f1053a452c6844.svg",
                "isPro": false,
                "fullname": "Cheng Wang",
                "user": "LLucass",
                "type": "user"
            },
            "summary": "Recent advances in applying reinforcement learning (RL) to large language\nmodels (LLMs) have led to substantial progress. In particular, a series of\nremarkable yet often counterintuitive phenomena have been reported in LLMs,\nexhibiting patterns not typically observed in traditional RL settings. For\nexample, notable claims include that a single training example can match the\nperformance achieved with an entire dataset, that the reward signal does not\nneed to be very accurate, and that training solely with negative samples can\nmatch or even surpass sophisticated reward-based methods. However, the precise\nconditions under which these observations hold - and, critically, when they\nfail - remain unclear. In this work, we identify a key factor that\ndifferentiates RL observations: whether the pretrained model already exhibits\nstrong Model-Task Alignment, as measured by pass@k accuracy on the evaluated\ntask. Through a systematic and comprehensive examination of a series of\ncounterintuitive claims, supported by rigorous experimental validation across\ndifferent model architectures and task domains, our findings show that while\nstandard RL training remains consistently robust across settings, many of these\ncounterintuitive results arise only when the model and task already exhibit\nstrong model-task alignment. In contrast, these techniques fail to drive\nsubstantial learning in more challenging regimes, where standard RL methods\nremain effective.",
            "upvotes": 3,
            "discussionId": "68b54fe6851c6e7b001ecaf6",
            "ai_summary": "Reinforcement learning applied to large language models shows counterintuitive results that depend on pre-existing model-task alignment, with standard RL methods remaining robust in challenging scenarios.",
            "ai_keywords": [
                "reinforcement learning",
                "large language models",
                "Model-Task Alignment",
                "pass@k accuracy",
                "standard RL training"
            ]
        },
        "publishedAt": "2025-08-28T16:02:10.000Z",
        "title": "Model-Task Alignment Drives Distinct RL Outcomes",
        "summary": "Recent advances in applying reinforcement learning (RL) to large language\nmodels (LLMs) have led to substantial progress. In particular, a series of\nremarkable yet often counterintuitive phenomena have been reported in LLMs,\nexhibiting patterns not typically observed in traditional RL settings. For\nexample, notable claims include that a single training example can match the\nperformance achieved with an entire dataset, that the reward signal does not\nneed to be very accurate, and that training solely with negative samples can\nmatch or even surpass sophisticated reward-based methods. However, the precise\nconditions under which these observations hold - and, critically, when they\nfail - remain unclear. In this work, we identify a key factor that\ndifferentiates RL observations: whether the pretrained model already exhibits\nstrong Model-Task Alignment, as measured by pass@k accuracy on the evaluated\ntask. Through a systematic and comprehensive examination of a series of\ncounterintuitive claims, supported by rigorous experimental validation across\ndifferent model architectures and task domains, our findings show that while\nstandard RL training remains consistently robust across settings, many of these\ncounterintuitive results arise only when the model and task already exhibit\nstrong model-task alignment. In contrast, these techniques fail to drive\nsubstantial learning in more challenging regimes, where standard RL methods\nremain effective.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21188.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65c1cbac88462619b2e191aa",
            "avatarUrl": "/avatars/97e1c2b049282169d1f1053a452c6844.svg",
            "fullname": "Cheng Wang",
            "name": "LLucass",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.20085",
            "authors": [
                {
                    "_id": "68b523c6851c6e7b001ecab0",
                    "name": "Zhecheng Yuan",
                    "hidden": false
                },
                {
                    "_id": "68b523c6851c6e7b001ecab1",
                    "name": "Tianming Wei",
                    "hidden": false
                },
                {
                    "_id": "68b523c6851c6e7b001ecab2",
                    "name": "Langzhe Gu",
                    "hidden": false
                },
                {
                    "_id": "68b523c6851c6e7b001ecab3",
                    "name": "Pu Hua",
                    "hidden": false
                },
                {
                    "_id": "68b523c6851c6e7b001ecab4",
                    "name": "Tianhai Liang",
                    "hidden": false
                },
                {
                    "_id": "68b523c6851c6e7b001ecab5",
                    "name": "Yuanpei Chen",
                    "hidden": false
                },
                {
                    "_id": "68b523c6851c6e7b001ecab6",
                    "name": "Huazhe Xu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T17:53:46.000Z",
            "submittedOnDailyAt": "2025-09-01T03:14:52.379Z",
            "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation",
            "submittedOnDailyBy": {
                "_id": "64a0dd823886663ff8455f7a",
                "avatarUrl": "/avatars/a8560f9c9de59cbab4ef5ed9494e4051.svg",
                "isPro": false,
                "fullname": "Zhecheng Yuan",
                "user": "gemcollector",
                "type": "user"
            },
            "summary": "Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.",
            "upvotes": 1,
            "discussionId": "68b523c6851c6e7b001ecab7",
            "projectPage": "https://gemcollector.github.io/HERMES/",
            "ai_summary": "HERMES is a human-to-robot learning framework that translates human hand motions into robotic behaviors, using reinforcement learning and sim2real transfer for versatile manipulation in diverse environments.",
            "ai_keywords": [
                "reinforcement learning",
                "sim2real transfer",
                "Perspective-n-Point (PnP) localization",
                "mobile bimanual dexterous manipulation"
            ]
        },
        "publishedAt": "2025-08-27T13:53:46.000Z",
        "title": "HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data\n  for Mobile Dexterous Manipulation",
        "summary": "Leveraging human motion data to impart robots with versatile manipulation\nskills has emerged as a promising paradigm in robotic manipulation.\nNevertheless, translating multi-source human hand motions into feasible robot\nbehaviors remains challenging, particularly for robots equipped with\nmulti-fingered dexterous hands characterized by complex, high-dimensional\naction spaces. Moreover, existing approaches often struggle to produce policies\ncapable of adapting to diverse environmental conditions. In this paper, we\nintroduce HERMES, a human-to-robot learning framework for mobile bimanual\ndexterous manipulation. First, HERMES formulates a unified reinforcement\nlearning approach capable of seamlessly transforming heterogeneous human hand\nmotions from multiple sources into physically plausible robotic behaviors.\nSubsequently, to mitigate the sim2real gap, we devise an end-to-end, depth\nimage-based sim2real transfer method for improved generalization to real-world\nscenarios. Furthermore, to enable autonomous operation in varied and\nunstructured environments, we augment the navigation foundation model with a\nclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precise\nalignment of visual goals and effectively bridging autonomous navigation and\ndexterous manipulation. Extensive experimental results demonstrate that HERMES\nconsistently exhibits generalizable behaviors across diverse, in-the-wild\nscenarios, successfully performing numerous complex mobile bimanual dexterous\nmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20085.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a0dd823886663ff8455f7a",
            "avatarUrl": "/avatars/a8560f9c9de59cbab4ef5ed9494e4051.svg",
            "fullname": "Zhecheng Yuan",
            "name": "gemcollector",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.17380",
            "authors": [
                {
                    "_id": "68b595ffdc94ad77a7bfcfbb",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfbc",
                    "name": "Songning Lai",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfbd",
                    "name": "Pengze Li",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfbe",
                    "name": "Di Yu",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfbf",
                    "name": "Wenjie Zhou",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc0",
                    "name": "Yiyang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc1",
                    "name": "Peng Xia",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc2",
                    "name": "Zijun Wang",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc3",
                    "name": "Xi Chen",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc4",
                    "name": "Shixiang Tang",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc5",
                    "name": "Lei Bai",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc6",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc7",
                    "name": "Mingyu Ding",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc8",
                    "name": "Huaxiu Yao",
                    "hidden": false
                },
                {
                    "_id": "68b595ffdc94ad77a7bfcfc9",
                    "name": "Aoran Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-24T14:34:21.000Z",
            "submittedOnDailyAt": "2025-09-01T11:20:54.647Z",
            "title": "Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula\n  Discovery",
            "submittedOnDailyBy": {
                "_id": "684ff37fa383bc5d6b0ff77f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png",
                "isPro": false,
                "fullname": "JiaqiLiu",
                "user": "JiaaqiLiu",
                "type": "user"
            },
            "summary": "Automated discovery of physical laws from observational data in the real\nworld is a grand challenge in AI. Current methods, relying on symbolic\nregression or LLMs, are limited to uni-modal data and overlook the rich, visual\nphenomenological representations of motion that are indispensable to\nphysicists. This \"sensory deprivation\" severely weakens their ability to\ninterpret the inherent spatio-temporal patterns within dynamic phenomena. To\naddress this gap, we propose VIPER-R1, a multimodal model that performs Visual\nInduction for Physics-based Equation Reasoning to discover fundamental symbolic\nformulas. It integrates visual perception, trajectory data, and symbolic\nreasoning to emulate the scientific discovery process. The model is trained via\na curriculum of Motion Structure Induction (MSI), using supervised fine-tuning\nto interpret kinematic phase portraits and to construct hypotheses guided by a\nCausal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration\n(RGSC) to refine the formula structure with reinforcement learning. During\ninference, the trained VIPER-R1 acts as an agent: it first posits a\nhigh-confidence symbolic ansatz, then proactively invokes an external symbolic\nregression tool to perform Symbolic Residual Realignment (SR^2). This final\nstep, analogous to a physicist's perturbation analysis, reconciles the\ntheoretical model with empirical data. To support this research, we introduce\nPhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that\nVIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy\nand interpretability, enabling more precise discovery of physical laws. Project\npage: https://jiaaqiliu.github.io/VIPER-R1/",
            "upvotes": 1,
            "discussionId": "68b595ffdc94ad77a7bfcfca",
            "ai_summary": "VIPER-R1, a multimodal model combining visual perception, trajectory data, and symbolic reasoning, discovers physical laws with higher accuracy and interpretability than existing methods.",
            "ai_keywords": [
                "Visual Induction",
                "Physics-based Equation Reasoning",
                "Motion Structure Induction",
                "Causal Chain of Thought",
                "Reward-Guided Symbolic Calibration",
                "Symbolic Residual Realignment",
                "PhysSymbol",
                "multimodal model",
                "symbolic regression",
                "reinforcement learning"
            ]
        },
        "publishedAt": "2025-08-24T10:34:21.000Z",
        "title": "Mimicking the Physicist's Eye:A VLM-centric Approach for Physics Formula\n  Discovery",
        "summary": "Automated discovery of physical laws from observational data in the real\nworld is a grand challenge in AI. Current methods, relying on symbolic\nregression or LLMs, are limited to uni-modal data and overlook the rich, visual\nphenomenological representations of motion that are indispensable to\nphysicists. This \"sensory deprivation\" severely weakens their ability to\ninterpret the inherent spatio-temporal patterns within dynamic phenomena. To\naddress this gap, we propose VIPER-R1, a multimodal model that performs Visual\nInduction for Physics-based Equation Reasoning to discover fundamental symbolic\nformulas. It integrates visual perception, trajectory data, and symbolic\nreasoning to emulate the scientific discovery process. The model is trained via\na curriculum of Motion Structure Induction (MSI), using supervised fine-tuning\nto interpret kinematic phase portraits and to construct hypotheses guided by a\nCausal Chain of Thought (C-CoT), followed by Reward-Guided Symbolic Calibration\n(RGSC) to refine the formula structure with reinforcement learning. During\ninference, the trained VIPER-R1 acts as an agent: it first posits a\nhigh-confidence symbolic ansatz, then proactively invokes an external symbolic\nregression tool to perform Symbolic Residual Realignment (SR^2). This final\nstep, analogous to a physicist's perturbation analysis, reconciles the\ntheoretical model with empirical data. To support this research, we introduce\nPhysSymbol, a new 5,000-instance multimodal corpus. Experiments show that\nVIPER-R1 consistently outperforms state-of-the-art VLM baselines in accuracy\nand interpretability, enabling more precise discovery of physical laws. Project\npage: https://jiaaqiliu.github.io/VIPER-R1/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17380.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "684ff37fa383bc5d6b0ff77f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/0JPr-cd_rxQz3k6rmzBOF.png",
            "fullname": "JiaqiLiu",
            "name": "JiaaqiLiu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2508.21172",
            "authors": [
                {
                    "_id": "68b59b05dc94ad77a7bfcffc",
                    "user": {
                        "_id": "6898c8c3e9e1c6671e64a8ff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6898c8c3e9e1c6671e64a8ff/NLRxGoqH036cq5x55wvk6.jpeg",
                        "isPro": false,
                        "fullname": "Matteo Pinna",
                        "user": "nennomp",
                        "type": "user"
                    },
                    "name": "Matteo Pinna",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T13:46:18.949Z",
                    "hidden": false
                },
                {
                    "_id": "68b59b05dc94ad77a7bfcffd",
                    "name": "Andrea Ceni",
                    "hidden": false
                },
                {
                    "_id": "68b59b05dc94ad77a7bfcffe",
                    "name": "Claudio Gallicchio",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-28T19:22:02.000Z",
            "submittedOnDailyAt": "2025-09-01T13:14:54.333Z",
            "title": "Deep Residual Echo State Networks: exploring residual orthogonal\n  connections in untrained Recurrent Neural Networks",
            "submittedOnDailyBy": {
                "_id": "6898c8c3e9e1c6671e64a8ff",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6898c8c3e9e1c6671e64a8ff/NLRxGoqH036cq5x55wvk6.jpeg",
                "isPro": false,
                "fullname": "Matteo Pinna",
                "user": "nennomp",
                "type": "user"
            },
            "summary": "Echo State Networks (ESNs) are a particular type of untrained Recurrent\nNeural Networks (RNNs) within the Reservoir Computing (RC) framework, popular\nfor their fast and efficient learning. However, traditional ESNs often struggle\nwith long-term information processing. In this paper, we introduce a novel\nclass of deep untrained RNNs based on temporal residual connections, called\nDeep Residual Echo State Networks (DeepResESNs). We show that leveraging a\nhierarchy of untrained residual recurrent layers significantly boosts memory\ncapacity and long-term temporal modeling. For the temporal residual\nconnections, we consider different orthogonal configurations, including\nrandomly generated and fixed-structure configurations, and we study their\neffect on network dynamics. A thorough mathematical analysis outlines necessary\nand sufficient conditions to ensure stable dynamics within DeepResESN. Our\nexperiments on a variety of time series tasks showcase the advantages of the\nproposed approach over traditional shallow and deep RC.",
            "upvotes": 0,
            "discussionId": "68b59b05dc94ad77a7bfcfff",
            "githubRepo": "https://github.com/NennoMP/deepresesn",
            "ai_summary": "Deep Residual Echo State Networks (DeepResESNs) enhance long-term temporal modeling and memory capacity through hierarchical untrained residual layers, outperforming traditional shallow and deep reservoir computing methods.",
            "ai_keywords": [
                "Echo State Networks",
                "Recurrent Neural Networks",
                "Reservoir Computing",
                "temporal residual connections",
                "Deep Residual Echo State Networks",
                "DeepResESNs",
                "network dynamics",
                "time series tasks"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-08-28T15:22:02.000Z",
        "title": "Deep Residual Echo State Networks: exploring residual orthogonal\n  connections in untrained Recurrent Neural Networks",
        "summary": "Echo State Networks (ESNs) are a particular type of untrained Recurrent\nNeural Networks (RNNs) within the Reservoir Computing (RC) framework, popular\nfor their fast and efficient learning. However, traditional ESNs often struggle\nwith long-term information processing. In this paper, we introduce a novel\nclass of deep untrained RNNs based on temporal residual connections, called\nDeep Residual Echo State Networks (DeepResESNs). We show that leveraging a\nhierarchy of untrained residual recurrent layers significantly boosts memory\ncapacity and long-term temporal modeling. For the temporal residual\nconnections, we consider different orthogonal configurations, including\nrandomly generated and fixed-structure configurations, and we study their\neffect on network dynamics. A thorough mathematical analysis outlines necessary\nand sufficient conditions to ensure stable dynamics within DeepResESN. Our\nexperiments on a variety of time series tasks showcase the advantages of the\nproposed approach over traditional shallow and deep RC.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.21172.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6898c8c3e9e1c6671e64a8ff",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6898c8c3e9e1c6671e64a8ff/NLRxGoqH036cq5x55wvk6.jpeg",
            "fullname": "Matteo Pinna",
            "name": "nennomp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.19600",
            "authors": [
                {
                    "_id": "68b339d0851c6e7b001ec763",
                    "name": "Toghrul Karimov",
                    "hidden": false
                },
                {
                    "_id": "68b339d0851c6e7b001ec764",
                    "name": "Hassan Imani",
                    "hidden": false
                },
                {
                    "_id": "68b339d0851c6e7b001ec765",
                    "user": {
                        "_id": "64fc60fe74574268a5723ec0",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eT2iHJx2WKQWRIW17egrE.jpeg",
                        "isPro": false,
                        "fullname": "Allan Kazakov",
                        "user": "AllanK24",
                        "type": "user"
                    },
                    "name": "Allan Kazakov",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-01T07:52:29.225Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-27T06:20:38.000Z",
            "submittedOnDailyAt": "2025-09-01T12:17:51.523Z",
            "title": "Quantization Robustness to Input Degradations for Object Detection",
            "submittedOnDailyBy": {
                "_id": "64fc60fe74574268a5723ec0",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eT2iHJx2WKQWRIW17egrE.jpeg",
                "isPro": false,
                "fullname": "Allan Kazakov",
                "user": "AllanK24",
                "type": "user"
            },
            "summary": "Post-training quantization (PTQ) is crucial for deploying efficient object\ndetection models, like YOLO, on resource-constrained devices. However, the\nimpact of reduced precision on model robustness to real-world input\ndegradations such as noise, blur, and compression artifacts is a significant\nconcern. This paper presents a comprehensive empirical study evaluating the\nrobustness of YOLO models (nano to extra-large scales) across multiple\nprecision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8\n(TensorRT). We introduce and evaluate a degradation-aware calibration strategy\nfor Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix\nof clean and synthetically degraded images. Models were benchmarked on the COCO\ndataset under seven distinct degradation conditions (including various types\nand levels of noise, blur, low contrast, and JPEG compression) and a\nmixed-degradation scenario. Results indicate that while Static INT8 TensorRT\nengines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop\n(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did\nnot yield consistent, broad improvements in robustness over standard clean-data\ncalibration across most models and degradations. A notable exception was\nobserved for larger model scales under specific noise conditions, suggesting\nmodel capacity may influence the efficacy of this calibration approach. These\nfindings highlight the challenges in enhancing PTQ robustness and provide\ninsights for deploying quantized detectors in uncontrolled environments. All\ncode and evaluation tables are available at https://github.com/AllanK24/QRID.",
            "upvotes": 0,
            "discussionId": "68b339d0851c6e7b001ec766",
            "ai_summary": "Post-training quantization of YOLO models is evaluated for robustness to real-world degradations, with a focus on the effectiveness of a degradation-aware calibration strategy for Static INT8 quantization.",
            "ai_keywords": [
                "post-training quantization",
                "YOLO",
                "FP32",
                "FP16",
                "Dynamic UINT8",
                "Static INT8",
                "TensorRT",
                "calibration",
                "degradation-aware",
                "COCO dataset",
                "noise",
                "blur",
                "low contrast",
                "JPEG compression",
                "mAP50-95"
            ]
        },
        "publishedAt": "2025-08-27T02:20:38.000Z",
        "title": "Quantization Robustness to Input Degradations for Object Detection",
        "summary": "Post-training quantization (PTQ) is crucial for deploying efficient object\ndetection models, like YOLO, on resource-constrained devices. However, the\nimpact of reduced precision on model robustness to real-world input\ndegradations such as noise, blur, and compression artifacts is a significant\nconcern. This paper presents a comprehensive empirical study evaluating the\nrobustness of YOLO models (nano to extra-large scales) across multiple\nprecision formats: FP32, FP16 (TensorRT), Dynamic UINT8 (ONNX), and Static INT8\n(TensorRT). We introduce and evaluate a degradation-aware calibration strategy\nfor Static INT8 PTQ, where the TensorRT calibration process is exposed to a mix\nof clean and synthetically degraded images. Models were benchmarked on the COCO\ndataset under seven distinct degradation conditions (including various types\nand levels of noise, blur, low contrast, and JPEG compression) and a\nmixed-degradation scenario. Results indicate that while Static INT8 TensorRT\nengines offer substantial speedups (~1.5-3.3x) with a moderate accuracy drop\n(~3-7% mAP50-95) on clean data, the proposed degradation-aware calibration did\nnot yield consistent, broad improvements in robustness over standard clean-data\ncalibration across most models and degradations. A notable exception was\nobserved for larger model scales under specific noise conditions, suggesting\nmodel capacity may influence the efficacy of this calibration approach. These\nfindings highlight the challenges in enhancing PTQ robustness and provide\ninsights for deploying quantized detectors in uncontrolled environments. All\ncode and evaluation tables are available at https://github.com/AllanK24/QRID.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.19600.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64fc60fe74574268a5723ec0",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/eT2iHJx2WKQWRIW17egrE.jpeg",
            "fullname": "Allan Kazakov",
            "name": "AllanK24",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.17008",
            "authors": [
                {
                    "_id": "68ad9952b9d0280e9ba2aabf",
                    "user": {
                        "_id": "68ad9530fc1e3c0742b452f8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68ad9530fc1e3c0742b452f8/ABDx5BLdHL0rOtOGR8HFl.png",
                        "isPro": false,
                        "fullname": "yhua219",
                        "user": "yhua219",
                        "type": "user"
                    },
                    "name": "Yan Cathy Hua",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-08-26T11:37:57.613Z",
                    "hidden": false
                },
                {
                    "_id": "68ad9952b9d0280e9ba2aac0",
                    "name": "Paul Denny",
                    "hidden": false
                },
                {
                    "_id": "68ad9952b9d0280e9ba2aac1",
                    "name": "Jrg Wicker",
                    "hidden": false
                },
                {
                    "_id": "68ad9952b9d0280e9ba2aac2",
                    "name": "Katerina Taskova",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-23T12:38:40.000Z",
            "submittedOnDailyAt": "2025-09-01T12:38:12.897Z",
            "title": "EduRABSA: An Education Review Dataset for Aspect-based Sentiment\n  Analysis Tasks",
            "submittedOnDailyBy": {
                "_id": "68ad9530fc1e3c0742b452f8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68ad9530fc1e3c0742b452f8/ABDx5BLdHL0rOtOGR8HFl.png",
                "isPro": false,
                "fullname": "yhua219",
                "user": "yhua219",
                "type": "user"
            },
            "summary": "Every year, most educational institutions seek and receive an enormous volume\nof text feedback from students on courses, teaching, and overall experience.\nYet, turning this raw feedback into useful insights is far from\nstraightforward. It has been a long-standing challenge to adopt automatic\nopinion mining solutions for such education review text data due to the content\ncomplexity and low-granularity reporting requirements. Aspect-based Sentiment\nAnalysis (ABSA) offers a promising solution with its rich, sub-sentence-level\nopinion mining capabilities. However, existing ABSA research and resources are\nvery heavily focused on the commercial domain. In education, they are scarce\nand hard to develop due to limited public datasets and strict data protection.\nA high-quality, annotated dataset is urgently needed to advance research in\nthis under-resourced area. In this work, we present EduRABSA (Education Review\nABSA), the first public, annotated ABSA education review dataset that covers\nthree review subject types (course, teaching staff, university) in the English\nlanguage and all main ABSA tasks, including the under-explored implicit aspect\nand implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool),\nan offline, lightweight, installation-free manual data annotation tool that\ngenerates labelled datasets for comprehensive ABSA tasks from a single-task\nannotation. Together, these resources contribute to the ABSA community and\neducation domain by removing the dataset barrier, supporting research\ntransparency and reproducibility, and enabling the creation and sharing of\nfurther resources. The dataset, annotation tool, and scripts and statistics for\ndataset processing and sampling are available at\nhttps://github.com/yhua219/edurabsa_dataset_and_annotation_tool.",
            "upvotes": 0,
            "discussionId": "68ad9953b9d0280e9ba2aac3",
            "githubRepo": "https://github.com/yhua219/edurabsa_dataset_and_annotation_tool",
            "ai_summary": "EduRABSA is a public dataset and ASQE-DPT is a tool for aspect-based sentiment analysis in education reviews, addressing the lack of resources in this domain.",
            "ai_keywords": [
                "Aspect-based Sentiment Analysis (ABSA)",
                "implicit aspect extraction",
                "implicit opinion extraction",
                "EduRABSA",
                "ASQE-DPT",
                "data annotation tool"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-08-23T08:38:40.000Z",
        "title": "EduRABSA: An Education Review Dataset for Aspect-based Sentiment\n  Analysis Tasks",
        "summary": "Every year, most educational institutions seek and receive an enormous volume\nof text feedback from students on courses, teaching, and overall experience.\nYet, turning this raw feedback into useful insights is far from\nstraightforward. It has been a long-standing challenge to adopt automatic\nopinion mining solutions for such education review text data due to the content\ncomplexity and low-granularity reporting requirements. Aspect-based Sentiment\nAnalysis (ABSA) offers a promising solution with its rich, sub-sentence-level\nopinion mining capabilities. However, existing ABSA research and resources are\nvery heavily focused on the commercial domain. In education, they are scarce\nand hard to develop due to limited public datasets and strict data protection.\nA high-quality, annotated dataset is urgently needed to advance research in\nthis under-resourced area. In this work, we present EduRABSA (Education Review\nABSA), the first public, annotated ABSA education review dataset that covers\nthree review subject types (course, teaching staff, university) in the English\nlanguage and all main ABSA tasks, including the under-explored implicit aspect\nand implicit opinion extraction. We also share ASQE-DPT (Data Processing Tool),\nan offline, lightweight, installation-free manual data annotation tool that\ngenerates labelled datasets for comprehensive ABSA tasks from a single-task\nannotation. Together, these resources contribute to the ABSA community and\neducation domain by removing the dataset barrier, supporting research\ntransparency and reproducibility, and enabling the creation and sharing of\nfurther resources. The dataset, annotation tool, and scripts and statistics for\ndataset processing and sampling are available at\nhttps://github.com/yhua219/edurabsa_dataset_and_annotation_tool.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.17008.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68ad9530fc1e3c0742b452f8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/68ad9530fc1e3c0742b452f8/ABDx5BLdHL0rOtOGR8HFl.png",
            "fullname": "yhua219",
            "name": "yhua219",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]