[
    {
        "paper": {
            "id": "2509.00375",
            "authors": [
                {
                    "_id": "68b902b5d43cadaf7a688b4c",
                    "user": {
                        "_id": "6540617c7cadb2d1b42007c5",
                        "avatarUrl": "/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg",
                        "isPro": false,
                        "fullname": "Ziyi Xia",
                        "user": "ZiyiXia",
                        "type": "user"
                    },
                    "name": "Ziyi Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T08:43:56.322Z",
                    "hidden": false
                },
                {
                    "_id": "68b902b5d43cadaf7a688b4d",
                    "name": "Kun Luo",
                    "hidden": false
                },
                {
                    "_id": "68b902b5d43cadaf7a688b4e",
                    "name": "Hongjin Qian",
                    "hidden": false
                },
                {
                    "_id": "68b902b5d43cadaf7a688b4f",
                    "name": "Zheng Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-30T06:02:56.000Z",
            "submittedOnDailyAt": "2025-09-04T01:42:16.942Z",
            "title": "Open Data Synthesis For Deep Research",
            "submittedOnDailyBy": {
                "_id": "6540617c7cadb2d1b42007c5",
                "avatarUrl": "/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg",
                "isPro": false,
                "fullname": "Ziyi Xia",
                "user": "ZiyiXia",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.",
            "upvotes": 36,
            "discussionId": "68b902b5d43cadaf7a688b50",
            "ai_summary": "InfoSeek is a scalable framework for generating complex Deep Research tasks by synthesizing hierarchical constraint satisfaction problems, enabling models to outperform larger baselines on challenging benchmarks.",
            "ai_keywords": [
                "Hierarchical Constraint Satisfaction Problems",
                "HCSPs",
                "dual-agent system",
                "Research Tree",
                "natural language questions",
                "reasoning trajectories",
                "reject sampling",
                "compound reward design",
                "trajectory-level exploration"
            ]
        },
        "publishedAt": "2025-08-30T02:02:56.000Z",
        "title": "Open Data Synthesis For Deep Research",
        "summary": "Large language models (LLMs) are increasingly expected to go beyond simple\nfactual queries toward Deep Research-tasks that require decomposing questions\ninto sub-problems, coordinating multi-step reasoning, and synthesizing evidence\nfrom diverse sources. We formalize Deep Research tasks with verifiable answers\nas Hierarchical Constraint Satisfaction Problems (HCSPs), which are\nfundamentally different from single-constraint, multi-hop, or flat CSP\nformulations. However, existing benchmarks (e.g., Natural Questions, HotpotQA)\nfail to capture this complexity, while recent synthetic datasets often\nintroduce shortcut reasoning, knowledge leakage, or lack sufficient structural\ndepth. To address this gap, we introduce InfoSeek, a scalable framework for\nsynthesizing complex Deep Research tasks. InfoSeek uses a dual-agent system to\nrecursively build a Research Tree from large-scale webpages, blurring\nintermediate nodes into valid sub-problems, and converting these trees into\nnatural language questions that require traversing the full hierarchy. It also\nenables rapid scaling, yielding over 50K training examples, a curated test set,\nand reasoning trajectories generated via reject sampling. Experiments show that\nmodels trained on InfoSeek consistently outperform strong baselines. On a\nchallenging benchmark BrowseComp-Plus, 3B LLMs optimized with InfoSeek surpass\nmuch larger 32B models and lightweight commercial APIs (e.g., Gemini2.5-Flash),\nwhile achieving performance comparable to stronger APIs (e.g., Gemini2.5-Pro).\nBy preserving meta-information such as intermediate steps and retrieval labels,\nInfoSeek further supports advanced optimization strategies, including compound\nreward design and trajectory-level exploration. We provide our codes and\ndatasets in https://github.com/VectorSpaceLab/InfoSeek{this repository}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00375.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6540617c7cadb2d1b42007c5",
            "avatarUrl": "/avatars/b1877fd0564c362a0d4a064d4ec43a73.svg",
            "fullname": "Ziyi Xia",
            "name": "ZiyiXia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.01106",
            "authors": [
                {
                    "_id": "68b8f795d43cadaf7a688b04",
                    "name": "Huang Fang",
                    "hidden": false
                },
                {
                    "_id": "68b8f795d43cadaf7a688b05",
                    "user": {
                        "_id": "65f2a7267d5cdbce8882548b",
                        "avatarUrl": "/avatars/f0244fa4e71403d05ea8bfd25986a02b.svg",
                        "isPro": false,
                        "fullname": "mengxizhang",
                        "user": "mengxizhang",
                        "type": "user"
                    },
                    "name": "Mengxi Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T20:11:34.556Z",
                    "hidden": false
                },
                {
                    "_id": "68b8f795d43cadaf7a688b06",
                    "user": {
                        "_id": "64315a3ddd466752c73c2073",
                        "avatarUrl": "/avatars/d9dd119354960bfba424809dab911840.svg",
                        "isPro": false,
                        "fullname": "Heng Dong",
                        "user": "drdh",
                        "type": "user"
                    },
                    "name": "Heng Dong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T19:36:52.088Z",
                    "hidden": false
                },
                {
                    "_id": "68b8f795d43cadaf7a688b07",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68b8f795d43cadaf7a688b08",
                    "name": "Zixuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68b8f795d43cadaf7a688b09",
                    "name": "Qifeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b8f795d43cadaf7a688b0a",
                    "name": "Xueyun Tian",
                    "hidden": false
                },
                {
                    "_id": "68b8f795d43cadaf7a688b0b",
                    "name": "Yucheng Hu",
                    "hidden": false
                },
                {
                    "_id": "68b8f795d43cadaf7a688b0c",
                    "name": "Hang Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-01T03:53:47.000Z",
            "submittedOnDailyAt": "2025-09-04T00:57:07.946Z",
            "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
            "submittedOnDailyBy": {
                "_id": "63044e025c70c21d0eaf08bc",
                "avatarUrl": "/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg",
                "isPro": false,
                "fullname": "Wei Li",
                "user": "Wiley085",
                "type": "user"
            },
            "summary": "We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.",
            "upvotes": 33,
            "discussionId": "68b8f796d43cadaf7a688b0d",
            "projectPage": "https://robix-seed.github.io/robix/",
            "ai_summary": "Robix, a unified vision-language model, integrates robot reasoning, task planning, and natural language interaction, demonstrating superior performance in interactive task execution through chain-of-thought reasoning and a three-stage training strategy.",
            "ai_keywords": [
                "chain-of-thought reasoning",
                "three-stage training strategy",
                "continued pretraining",
                "supervised finetuning",
                "reinforcement learning",
                "embodied reasoning",
                "3D spatial understanding",
                "visual grounding",
                "task-centric reasoning",
                "human-robot interaction",
                "task planning",
                "reasoning-action sequence",
                "reasoning-action consistency",
                "long-horizon task coherence"
            ]
        },
        "publishedAt": "2025-08-31T23:53:47.000Z",
        "title": "Robix: A Unified Model for Robot Interaction, Reasoning and Planning",
        "summary": "We introduce Robix, a unified model that integrates robot reasoning, task\nplanning, and natural language interaction within a single vision-language\narchitecture. Acting as the high-level cognitive layer in a hierarchical robot\nsystem, Robix dynamically generates atomic commands for the low-level\ncontroller and verbal responses for human interaction, enabling robots to\nfollow complex instructions, plan long-horizon tasks, and interact naturally\nwith human within an end-to-end framework. Robix further introduces novel\ncapabilities such as proactive dialogue, real-time interruption handling, and\ncontext-aware commonsense reasoning during task execution. At its core, Robix\nleverages chain-of-thought reasoning and adopts a three-stage training\nstrategy: (1) continued pretraining to enhance foundational embodied reasoning\nabilities including 3D spatial understanding, visual grounding, and\ntask-centric reasoning; (2) supervised finetuning to model human-robot\ninteraction and task planning as a unified reasoning-action sequence; and (3)\nreinforcement learning to improve reasoning-action consistency and long-horizon\ntask coherence. Extensive experiments demonstrate that Robix outperforms both\nopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) in\ninteractive task execution, demonstrating strong generalization across diverse\ninstruction types (e.g., open-ended, multi-stage, constrained, invalid, and\ninterrupted) and various user-involved tasks such as table bussing, grocery\nshopping, and dietary filtering.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01106.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63044e025c70c21d0eaf08bc",
            "avatarUrl": "/avatars/a2d39973d7fbcbe9d4cce5648b3149c2.svg",
            "fullname": "Wei Li",
            "name": "Wiley085",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.03405",
            "authors": [
                {
                    "_id": "68b94c5ed43cadaf7a688c0a",
                    "name": "Daniela Gottesman",
                    "hidden": false
                },
                {
                    "_id": "68b94c5ed43cadaf7a688c0b",
                    "name": "Alon Gilae-Dotan",
                    "hidden": false
                },
                {
                    "_id": "68b94c5ed43cadaf7a688c0c",
                    "name": "Ido Cohen",
                    "hidden": false
                },
                {
                    "_id": "68b94c5ed43cadaf7a688c0d",
                    "name": "Yoav Gur-Arieh",
                    "hidden": false
                },
                {
                    "_id": "68b94c5ed43cadaf7a688c0e",
                    "name": "Marius Mosbach",
                    "hidden": false
                },
                {
                    "_id": "68b94c5ed43cadaf7a688c0f",
                    "name": "Ori Yoran",
                    "hidden": false
                },
                {
                    "_id": "68b94c5ed43cadaf7a688c10",
                    "user": {
                        "_id": "610b729f9da682cd54ad9adf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1628140189042-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Mor Geva",
                        "user": "mega",
                        "type": "user"
                    },
                    "name": "Mor Geva",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T08:42:49.827Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-03T15:31:18.000Z",
            "submittedOnDailyAt": "2025-09-04T07:03:36.263Z",
            "title": "LMEnt: A Suite for Analyzing Knowledge in Language Models from\n  Pretraining Data to Representations",
            "submittedOnDailyBy": {
                "_id": "637f37b5b3cb8158e1725697",
                "avatarUrl": "/avatars/18780d63b12aa8a5171130d09e214b25.svg",
                "isPro": false,
                "fullname": "Daniela",
                "user": "dhgottesman",
                "type": "user"
            },
            "summary": "Language models (LMs) increasingly drive real-world applications that require\nworld knowledge. However, the internal processes through which models turn data\ninto representations of knowledge and beliefs about the world, are poorly\nunderstood. Insights into these processes could pave the way for developing LMs\nwith knowledge representations that are more consistent, robust, and complete.\nTo facilitate studying these questions, we present LMEnt, a suite for analyzing\nknowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a\nknowledge-rich pretraining corpus, fully annotated with entity mentions, based\non Wikipedia, (2) an entity-based retrieval method over pretraining data that\noutperforms previous approaches by as much as 80.4%, and (3) 12 pretrained\nmodels with up to 1B parameters and 4K intermediate checkpoints, with\ncomparable performance to popular open-sourced models on knowledge benchmarks.\nTogether, these resources provide a controlled environment for analyzing\nconnections between entity mentions in pretraining and downstream performance,\nand the effects of causal interventions in pretraining data. We show the\nutility of LMEnt by studying knowledge acquisition across checkpoints, finding\nthat fact frequency is key, but does not fully explain learning trends. We\nrelease LMEnt to support studies of knowledge in LMs, including knowledge\nrepresentations, plasticity, editing, attribution, and learning dynamics.",
            "upvotes": 16,
            "discussionId": "68b94c5fd43cadaf7a688c11",
            "ai_summary": "LMEnt is a suite for analyzing knowledge acquisition in language models during pretraining, providing annotated corpora, retrieval methods, and pretrained models to study knowledge representations and learning dynamics.",
            "ai_keywords": [
                "language models",
                "knowledge acquisition",
                "pretraining",
                "knowledge-rich pretraining corpus",
                "entity mentions",
                "entity-based retrieval",
                "pretrained models",
                "knowledge benchmarks",
                "knowledge representations",
                "knowledge plasticity",
                "knowledge editing",
                "knowledge attribution",
                "learning dynamics"
            ]
        },
        "publishedAt": "2025-09-03T11:31:18.000Z",
        "title": "LMEnt: A Suite for Analyzing Knowledge in Language Models from\n  Pretraining Data to Representations",
        "summary": "Language models (LMs) increasingly drive real-world applications that require\nworld knowledge. However, the internal processes through which models turn data\ninto representations of knowledge and beliefs about the world, are poorly\nunderstood. Insights into these processes could pave the way for developing LMs\nwith knowledge representations that are more consistent, robust, and complete.\nTo facilitate studying these questions, we present LMEnt, a suite for analyzing\nknowledge acquisition in LMs during pretraining. LMEnt introduces: (1) a\nknowledge-rich pretraining corpus, fully annotated with entity mentions, based\non Wikipedia, (2) an entity-based retrieval method over pretraining data that\noutperforms previous approaches by as much as 80.4%, and (3) 12 pretrained\nmodels with up to 1B parameters and 4K intermediate checkpoints, with\ncomparable performance to popular open-sourced models on knowledge benchmarks.\nTogether, these resources provide a controlled environment for analyzing\nconnections between entity mentions in pretraining and downstream performance,\nand the effects of causal interventions in pretraining data. We show the\nutility of LMEnt by studying knowledge acquisition across checkpoints, finding\nthat fact frequency is key, but does not fully explain learning trends. We\nrelease LMEnt to support studies of knowledge in LMs, including knowledge\nrepresentations, plasticity, editing, attribution, and learning dynamics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03405.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "637f37b5b3cb8158e1725697",
            "avatarUrl": "/avatars/18780d63b12aa8a5171130d09e214b25.svg",
            "fullname": "Daniela",
            "name": "dhgottesman",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.02722",
            "authors": [
                {
                    "_id": "68b9940c736018af705e8d58",
                    "user": {
                        "_id": "630491107424d937fa3258be",
                        "avatarUrl": "/avatars/b8bd81bc8544674ee26b78702afdb87c.svg",
                        "isPro": false,
                        "fullname": "Delong Chen",
                        "user": "chendelong",
                        "type": "user"
                    },
                    "name": "Delong Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T19:39:22.774Z",
                    "hidden": false
                },
                {
                    "_id": "68b9940c736018af705e8d59",
                    "user": {
                        "_id": "639111acb73a0f8c02a92844",
                        "avatarUrl": "/avatars/58766701471be66fd784bedd319741a4.svg",
                        "isPro": false,
                        "fullname": "Theo Moutakanni",
                        "user": "TheoM",
                        "type": "user"
                    },
                    "name": "Theo Moutakanni",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T19:39:32.391Z",
                    "hidden": false
                },
                {
                    "_id": "68b9940c736018af705e8d5a",
                    "user": {
                        "_id": "6749afd958bd26b6c821d336",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/4idAccbPGO2okny1ru4RQ.png",
                        "isPro": false,
                        "fullname": "Willy Chung",
                        "user": "whcchung",
                        "type": "user"
                    },
                    "name": "Willy Chung",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T19:39:39.541Z",
                    "hidden": false
                },
                {
                    "_id": "68b9940c736018af705e8d5b",
                    "name": "Yejin Bang",
                    "hidden": false
                },
                {
                    "_id": "68b9940c736018af705e8d5c",
                    "name": "Ziwei Ji",
                    "hidden": false
                },
                {
                    "_id": "68b9940c736018af705e8d5d",
                    "user": {
                        "_id": "65f08f0bcd30f1ed6ea6cbd1",
                        "avatarUrl": "/avatars/5eb2c2032dc2054d4ca86665ba0d428c.svg",
                        "isPro": false,
                        "fullname": "Allen Bolourchi",
                        "user": "allen-ml",
                        "type": "user"
                    },
                    "name": "Allen Bolourchi",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T19:39:45.340Z",
                    "hidden": false
                },
                {
                    "_id": "68b9940c736018af705e8d5e",
                    "name": "Pascale Fung",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/630491107424d937fa3258be/WFrDJM0y73ZkwHoHHEEGi.mp4"
            ],
            "publishedAt": "2025-09-02T18:18:57.000Z",
            "submittedOnDailyAt": "2025-09-04T12:20:43.590Z",
            "title": "Planning with Reasoning using Vision Language World Model",
            "submittedOnDailyBy": {
                "_id": "630491107424d937fa3258be",
                "avatarUrl": "/avatars/b8bd81bc8544674ee26b78702afdb87c.svg",
                "isPro": false,
                "fullname": "Delong Chen",
                "user": "chendelong",
                "type": "user"
            },
            "summary": "Effective planning requires strong world models, but high-level world models\nthat can understand and reason about actions with semantic and temporal\nabstraction remain largely underdeveloped. We introduce the Vision Language\nWorld Model (VLWM), a foundation model trained for language-based world\nmodeling on natural videos. Given visual observations, the VLWM first infers\nthe overall goal achievements then predicts a trajectory composed of\ninterleaved actions and world state changes. Those targets are extracted by\niterative LLM Self-Refine conditioned on compressed future observations\nrepresented by Tree of Captions. The VLWM learns both an action policy and a\ndynamics model, which respectively facilitates reactive system-1 plan decoding\nand reflective system-2 planning via cost minimization. The cost evaluates the\nsemantic distance between the hypothetical future states given by VLWM\nroll-outs and the expected goal state, and is measured by a critic model that\nwe trained in a self-supervised manner. The VLWM achieves state-of-the-art\nVisual Planning for Assistance (VPA) performance on both benchmark evaluations\nand our proposed PlannerArena human evaluations, where system-2 improves the\nElo score by +27% upon system-1. The VLWM models also outperforms strong VLM\nbaselines on RoboVQA and WorldPrediction benchmark.",
            "upvotes": 8,
            "discussionId": "68b9940c736018af705e8d5f",
            "ai_summary": "The Vision Language World Model (VLWM) achieves state-of-the-art performance in visual planning by integrating language-based world modeling, action policy learning, and dynamics modeling with semantic and temporal abstraction.",
            "ai_keywords": [
                "Vision Language World Model",
                "VLWM",
                "language-based world modeling",
                "natural videos",
                "iterative LLM Self-Refine",
                "Tree of Captions",
                "action policy",
                "dynamics model",
                "system-1 plan decoding",
                "system-2 planning",
                "cost minimization",
                "semantic distance",
                "critic model",
                "self-supervised manner",
                "Visual Planning for Assistance",
                "VPA",
                "PlannerArena",
                "Elo score",
                "RoboVQA",
                "WorldPrediction benchmark"
            ]
        },
        "publishedAt": "2025-09-02T14:18:57.000Z",
        "title": "Planning with Reasoning using Vision Language World Model",
        "summary": "Effective planning requires strong world models, but high-level world models\nthat can understand and reason about actions with semantic and temporal\nabstraction remain largely underdeveloped. We introduce the Vision Language\nWorld Model (VLWM), a foundation model trained for language-based world\nmodeling on natural videos. Given visual observations, the VLWM first infers\nthe overall goal achievements then predicts a trajectory composed of\ninterleaved actions and world state changes. Those targets are extracted by\niterative LLM Self-Refine conditioned on compressed future observations\nrepresented by Tree of Captions. The VLWM learns both an action policy and a\ndynamics model, which respectively facilitates reactive system-1 plan decoding\nand reflective system-2 planning via cost minimization. The cost evaluates the\nsemantic distance between the hypothetical future states given by VLWM\nroll-outs and the expected goal state, and is measured by a critic model that\nwe trained in a self-supervised manner. The VLWM achieves state-of-the-art\nVisual Planning for Assistance (VPA) performance on both benchmark evaluations\nand our proposed PlannerArena human evaluations, where system-2 improves the\nElo score by +27% upon system-1. The VLWM models also outperforms strong VLM\nbaselines on RoboVQA and WorldPrediction benchmark.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/630491107424d937fa3258be/WFrDJM0y73ZkwHoHHEEGi.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02722.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630491107424d937fa3258be",
            "avatarUrl": "/avatars/b8bd81bc8544674ee26b78702afdb87c.svg",
            "fullname": "Delong Chen",
            "name": "chendelong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.01977",
            "authors": [
                {
                    "_id": "68b8f158d43cadaf7a688afb",
                    "name": "Dong She",
                    "hidden": false
                },
                {
                    "_id": "68b8f158d43cadaf7a688afc",
                    "user": {
                        "_id": "6485dd6d07a2c1915060f603",
                        "avatarUrl": "/avatars/8594d647359a7d19ab29b8ec91d1444e.svg",
                        "isPro": false,
                        "fullname": "fu",
                        "user": "simingfu",
                        "type": "user"
                    },
                    "name": "Siming Fu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T19:41:37.052Z",
                    "hidden": false
                },
                {
                    "_id": "68b8f158d43cadaf7a688afd",
                    "name": "Mushui Liu",
                    "hidden": false
                },
                {
                    "_id": "68b8f158d43cadaf7a688afe",
                    "user": {
                        "_id": "656066e3d3f1b94f3b923239",
                        "avatarUrl": "/avatars/445c187d39b45ff5570cbd743817c193.svg",
                        "isPro": false,
                        "fullname": "Qiaoqiao Jin",
                        "user": "Au233",
                        "type": "user"
                    },
                    "name": "Qiaoqiao Jin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T19:41:42.767Z",
                    "hidden": false
                },
                {
                    "_id": "68b8f158d43cadaf7a688aff",
                    "user": {
                        "_id": "651cdb3dfafb0777f212c5ef",
                        "avatarUrl": "/avatars/dc319a87eff9e609360ef2fce51e755d.svg",
                        "isPro": true,
                        "fullname": "Wang Hualiang",
                        "user": "HualiangWang",
                        "type": "user"
                    },
                    "name": "Hualiang Wang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T19:41:54.752Z",
                    "hidden": false
                },
                {
                    "_id": "68b8f158d43cadaf7a688b00",
                    "name": "Mu Liu",
                    "hidden": false
                },
                {
                    "_id": "68b8f158d43cadaf7a688b01",
                    "name": "Jidong Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-02T05:40:07.000Z",
            "submittedOnDailyAt": "2025-09-04T05:05:42.538Z",
            "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement",
            "submittedOnDailyBy": {
                "_id": "6485dd6d07a2c1915060f603",
                "avatarUrl": "/avatars/8594d647359a7d19ab29b8ec91d1444e.svg",
                "isPro": false,
                "fullname": "fu",
                "user": "simingfu",
                "type": "user"
            },
            "summary": "Multi-subject personalized generation presents unique challenges in\nmaintaining identity fidelity and semantic coherence when synthesizing images\nconditioned on multiple reference subjects. Existing methods often suffer from\nidentity blending and attribute leakage due to inadequate modeling of how\ndifferent subjects should interact within shared representation spaces. We\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\ngeneration through explicit semantic correspondence and orthogonal feature\ndisentanglement. Our key insight is that multi-subject generation requires\nprecise semantic alignment at the representation level - knowing exactly which\nregions in the generated image should attend to which parts of each reference.\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\nproviding fine-grained semantic correspondences between multiple reference\nsubjects and target images, previously unavailable in this domain. Building on\nthis foundation, we propose the semantic correspondence attention loss to\nenforce precise point-to-point semantic alignment, ensuring high consistency\nfrom each reference to its designated regions. Furthermore, we develop the\nmulti-reference disentanglement loss to push different subjects into orthogonal\nattention subspaces, preventing feature interference while preserving\nindividual identity characteristics. Extensive experiments demonstrate that\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\nmulti-subject synthesis applications.",
            "upvotes": 7,
            "discussionId": "68b8f158d43cadaf7a688b02",
            "projectPage": "https://bytedance-fanqie-ai.github.io/MOSAIC/",
            "githubRepo": "https://github.com/bytedance-fanqie-ai/MOSAIC",
            "ai_summary": "MOSAIC framework enhances multi-subject image generation by ensuring precise semantic alignment and orthogonal feature disentanglement, achieving high fidelity even with multiple references.",
            "ai_keywords": [
                "representation-centric framework",
                "semantic correspondence",
                "orthogonal feature disentanglement",
                "SemAlign-MS",
                "semantic correspondence attention loss",
                "multi-reference disentanglement loss"
            ],
            "githubStars": 276
        },
        "publishedAt": "2025-09-02T01:40:07.000Z",
        "title": "MOSAIC: Multi-Subject Personalized Generation via Correspondence-Aware\n  Alignment and Disentanglement",
        "summary": "Multi-subject personalized generation presents unique challenges in\nmaintaining identity fidelity and semantic coherence when synthesizing images\nconditioned on multiple reference subjects. Existing methods often suffer from\nidentity blending and attribute leakage due to inadequate modeling of how\ndifferent subjects should interact within shared representation spaces. We\npresent MOSAIC, a representation-centric framework that rethinks multi-subject\ngeneration through explicit semantic correspondence and orthogonal feature\ndisentanglement. Our key insight is that multi-subject generation requires\nprecise semantic alignment at the representation level - knowing exactly which\nregions in the generated image should attend to which parts of each reference.\nTo enable this, we introduce SemAlign-MS, a meticulously annotated dataset\nproviding fine-grained semantic correspondences between multiple reference\nsubjects and target images, previously unavailable in this domain. Building on\nthis foundation, we propose the semantic correspondence attention loss to\nenforce precise point-to-point semantic alignment, ensuring high consistency\nfrom each reference to its designated regions. Furthermore, we develop the\nmulti-reference disentanglement loss to push different subjects into orthogonal\nattention subspaces, preventing feature interference while preserving\nindividual identity characteristics. Extensive experiments demonstrate that\nMOSAIC achieves state-of-the-art performance on multiple benchmarks. Notably,\nwhile existing methods typically degrade beyond 3 subjects, MOSAIC maintains\nhigh fidelity with 4+ reference subjects, opening new possibilities for complex\nmulti-subject synthesis applications.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01977.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6485dd6d07a2c1915060f603",
            "avatarUrl": "/avatars/8594d647359a7d19ab29b8ec91d1444e.svg",
            "fullname": "fu",
            "name": "simingfu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.00428",
            "authors": [
                {
                    "_id": "68b93229d43cadaf7a688bc6",
                    "user": {
                        "_id": "6617af2beab5eef6b1e8bb9e",
                        "avatarUrl": "/avatars/d939c02027916331d4c44119565f2ca6.svg",
                        "isPro": false,
                        "fullname": "XavierJiezou",
                        "user": "XavierJiezou",
                        "type": "user"
                    },
                    "name": "Xuechao Zou",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T19:37:38.778Z",
                    "hidden": false
                },
                {
                    "_id": "68b93229d43cadaf7a688bc7",
                    "name": "Shun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b93229d43cadaf7a688bc8",
                    "name": "Xing Fu",
                    "hidden": false
                },
                {
                    "_id": "68b93229d43cadaf7a688bc9",
                    "name": "Yue Li",
                    "hidden": false
                },
                {
                    "_id": "68b93229d43cadaf7a688bca",
                    "name": "Kai Li",
                    "hidden": false
                },
                {
                    "_id": "68b93229d43cadaf7a688bcb",
                    "name": "Yushe Cao",
                    "hidden": false
                },
                {
                    "_id": "68b93229d43cadaf7a688bcc",
                    "name": "Congyan Lang",
                    "hidden": false
                },
                {
                    "_id": "68b93229d43cadaf7a688bcd",
                    "name": "Pin Tao",
                    "hidden": false
                },
                {
                    "_id": "68b93229d43cadaf7a688bce",
                    "name": "Junliang Xing",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/sJrvZF6pJNPec1coRwsLz.jpeg",
                "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/9Chxv_NqnjXFhIVDrKyK3.png",
                "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/Q6nmMSG1YTJDEfG8ySInJ.png"
            ],
            "publishedAt": "2025-08-30T09:21:07.000Z",
            "submittedOnDailyAt": "2025-09-04T05:17:48.471Z",
            "title": "Mixture of Global and Local Experts with Diffusion Transformer for\n  Controllable Face Generation",
            "submittedOnDailyBy": {
                "_id": "6617af2beab5eef6b1e8bb9e",
                "avatarUrl": "/avatars/d939c02027916331d4c44119565f2ca6.svg",
                "isPro": false,
                "fullname": "XavierJiezou",
                "user": "XavierJiezou",
                "type": "user"
            },
            "summary": "Controllable face generation poses critical challenges in generative modeling\ndue to the intricate balance required between semantic controllability and\nphotorealism. While existing approaches struggle with disentangling semantic\ncontrols from generation pipelines, we revisit the architectural potential of\nDiffusion Transformers (DiTs) through the lens of expert specialization. This\npaper introduces Face-MoGLE, a novel framework featuring: (1)\nSemantic-decoupled latent modeling through mask-conditioned space\nfactorization, enabling precise attribute manipulation; (2) A mixture of global\nand local experts that captures holistic structure and region-level semantics\nfor fine-grained controllability; (3) A dynamic gating network producing\ntime-dependent coefficients that evolve with diffusion steps and spatial\nlocations. Face-MoGLE provides a powerful and flexible solution for\nhigh-quality, controllable face generation, with strong potential in generative\nmodeling and security applications. Extensive experiments demonstrate its\neffectiveness in multimodal and monomodal face generation settings and its\nrobust zero-shot generalization capability. Project page is available at\nhttps://github.com/XavierJiezou/Face-MoGLE.",
            "upvotes": 7,
            "discussionId": "68b93229d43cadaf7a688bcf",
            "projectPage": "https://xavierjiezou.github.io/Face-MoGLE/",
            "githubRepo": "https://github.com/XavierJiezou/Face-MoGLE",
            "ai_summary": "Face-MoGLE, a novel framework using Diffusion Transformers, achieves high-quality, controllable face generation through semantic-decoupled latent modeling, expert specialization, and dynamic gating.",
            "ai_keywords": [
                "Diffusion Transformers",
                "Semantic-decoupled latent modeling",
                "mask-conditioned space factorization",
                "global experts",
                "local experts",
                "dynamic gating network",
                "multimodal face generation",
                "monomodal face generation",
                "zero-shot generalization"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-08-30T05:21:07.000Z",
        "title": "Mixture of Global and Local Experts with Diffusion Transformer for\n  Controllable Face Generation",
        "summary": "Controllable face generation poses critical challenges in generative modeling\ndue to the intricate balance required between semantic controllability and\nphotorealism. While existing approaches struggle with disentangling semantic\ncontrols from generation pipelines, we revisit the architectural potential of\nDiffusion Transformers (DiTs) through the lens of expert specialization. This\npaper introduces Face-MoGLE, a novel framework featuring: (1)\nSemantic-decoupled latent modeling through mask-conditioned space\nfactorization, enabling precise attribute manipulation; (2) A mixture of global\nand local experts that captures holistic structure and region-level semantics\nfor fine-grained controllability; (3) A dynamic gating network producing\ntime-dependent coefficients that evolve with diffusion steps and spatial\nlocations. Face-MoGLE provides a powerful and flexible solution for\nhigh-quality, controllable face generation, with strong potential in generative\nmodeling and security applications. Extensive experiments demonstrate its\neffectiveness in multimodal and monomodal face generation settings and its\nrobust zero-shot generalization capability. Project page is available at\nhttps://github.com/XavierJiezou/Face-MoGLE.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/sJrvZF6pJNPec1coRwsLz.jpeg",
            "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/9Chxv_NqnjXFhIVDrKyK3.png",
            "https://cdn-uploads.huggingface.co/production/uploads/6617af2beab5eef6b1e8bb9e/Q6nmMSG1YTJDEfG8ySInJ.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00428.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6617af2beab5eef6b1e8bb9e",
            "avatarUrl": "/avatars/d939c02027916331d4c44119565f2ca6.svg",
            "fullname": "XavierJiezou",
            "name": "XavierJiezou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.02530",
            "authors": [
                {
                    "_id": "68b9eccf736018af705e8ddf",
                    "user": {
                        "_id": "630c8f0c8e3ff0c72327140a",
                        "avatarUrl": "/avatars/04a5f2effd0ff25d9fb531b195fd8846.svg",
                        "isPro": false,
                        "fullname": "Minghuan Liu",
                        "user": "ericonaldo",
                        "type": "user"
                    },
                    "name": "Minghuan Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T20:12:43.594Z",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de0",
                    "user": {
                        "_id": "64ec1840c2bcaa4525eabc20",
                        "avatarUrl": "/avatars/bcd5be2ccf6527292d66d0a985ff663f.svg",
                        "isPro": false,
                        "fullname": "Zhengbang Zhu",
                        "user": "Avada11",
                        "type": "user"
                    },
                    "name": "Zhengbang Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T20:12:49.069Z",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de1",
                    "user": {
                        "_id": "64256c3bdaa3502ee357380c",
                        "avatarUrl": "/avatars/d47a0268c4c3366cbfb4d621017e9b1e.svg",
                        "isPro": false,
                        "fullname": "Xiaoshen Han",
                        "user": "xshenhan",
                        "type": "user"
                    },
                    "name": "Xiaoshen Han",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T20:12:54.694Z",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de2",
                    "name": "Peng Hu",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de3",
                    "name": "Haotong Lin",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de4",
                    "name": "Xinyao Li",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de5",
                    "user": {
                        "_id": "67b477a81ae54de19c087823",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/T7r-L7FNVKnymSo7kYqqX.png",
                        "isPro": false,
                        "fullname": "Jingxiao Chen",
                        "user": "TimerChen",
                        "type": "user"
                    },
                    "name": "Jingxiao Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T20:13:08.421Z",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de6",
                    "name": "Jiafeng Xu",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de7",
                    "name": "Yichu Yang",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de8",
                    "user": {
                        "_id": "6889daab799265067d8aeedf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/b-CqqzpCDqg7SgIRrhXkk.png",
                        "isPro": false,
                        "fullname": "YunFeng_Lin",
                        "user": "Edward0000",
                        "type": "user"
                    },
                    "name": "Yunfeng Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T20:13:20.218Z",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8de9",
                    "user": {
                        "_id": "646c907297819a8be93a2aad",
                        "avatarUrl": "/avatars/b76734ab89d8446e12a5b679716aee78.svg",
                        "isPro": false,
                        "fullname": "Xinghang Li",
                        "user": "hywslxh",
                        "type": "user"
                    },
                    "name": "Xinghang Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T20:13:26.028Z",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8dea",
                    "name": "Yong Yu",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8deb",
                    "name": "Weinan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8dec",
                    "name": "Tao Kong",
                    "hidden": false
                },
                {
                    "_id": "68b9eccf736018af705e8ded",
                    "user": {
                        "_id": "647b5fef6a79fbf5e996c47c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647b5fef6a79fbf5e996c47c/IkSMnDsCY_CyEFCiMDuxe.jpeg",
                        "isPro": false,
                        "fullname": "Bingyi Kang",
                        "user": "bykang",
                        "type": "user"
                    },
                    "name": "Bingyi Kang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-04T20:12:37.779Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-02T17:29:38.000Z",
            "submittedOnDailyAt": "2025-09-04T18:20:41.305Z",
            "title": "Manipulation as in Simulation: Enabling Accurate Geometry Perception in\n  Robots",
            "submittedOnDailyBy": {
                "_id": "630c8f0c8e3ff0c72327140a",
                "avatarUrl": "/avatars/04a5f2effd0ff25d9fb531b195fd8846.svg",
                "isPro": false,
                "fullname": "Minghuan Liu",
                "user": "ericonaldo",
                "type": "user"
            },
            "summary": "Modern robotic manipulation primarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that using depth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we propose Camera Depth Models (CDMs) as\na simple plugin on daily-use depth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop a neural data engine that generates high-quality paired data from\nsimulation by modeling a depth camera's noise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy in depth prediction, effectively\nbridging the sim-to-real gap for manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on two challenging long-horizon tasks involving\narticulated, reflective, and slender objects, with little to no performance\ndegradation. We hope our findings will inspire future research in utilizing\nsimulation data and 3D information in general robot policies.",
            "upvotes": 2,
            "discussionId": "68b9eccf736018af705e8dee",
            "ai_summary": "Camera Depth Models (CDMs) enhance depth camera accuracy by denoising and improving metric depth prediction, enabling better generalization of robotic manipulation policies from simulation to real-world tasks.",
            "ai_keywords": [
                "Camera Depth Models",
                "neural data engine",
                "depth camera",
                "noise pattern",
                "depth prediction",
                "sim-to-real gap",
                "robotic manipulation",
                "articulated objects",
                "reflective objects",
                "slender objects"
            ]
        },
        "publishedAt": "2025-09-02T13:29:38.000Z",
        "title": "Manipulation as in Simulation: Enabling Accurate Geometry Perception in\n  Robots",
        "summary": "Modern robotic manipulation primarily relies on visual observations in a 2D\ncolor space for skill learning but suffers from poor generalization. In\ncontrast, humans, living in a 3D world, depend more on physical properties-such\nas distance, size, and shape-than on texture when interacting with objects.\nSince such 3D geometric information can be acquired from widely available depth\ncameras, it appears feasible to endow robots with similar perceptual\ncapabilities. Our pilot study found that using depth cameras for manipulation\nis challenging, primarily due to their limited accuracy and susceptibility to\nvarious types of noise. In this work, we propose Camera Depth Models (CDMs) as\na simple plugin on daily-use depth cameras, which take RGB images and raw depth\nsignals as input and output denoised, accurate metric depth. To achieve this,\nwe develop a neural data engine that generates high-quality paired data from\nsimulation by modeling a depth camera's noise pattern. Our results show that\nCDMs achieve nearly simulation-level accuracy in depth prediction, effectively\nbridging the sim-to-real gap for manipulation tasks. Notably, our experiments\ndemonstrate, for the first time, that a policy trained on raw simulated depth,\nwithout the need for adding noise or real-world fine-tuning, generalizes\nseamlessly to real-world robots on two challenging long-horizon tasks involving\narticulated, reflective, and slender objects, with little to no performance\ndegradation. We hope our findings will inspire future research in utilizing\nsimulation data and 3D information in general robot policies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02530.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "630c8f0c8e3ff0c72327140a",
            "avatarUrl": "/avatars/04a5f2effd0ff25d9fb531b195fd8846.svg",
            "fullname": "Minghuan Liu",
            "name": "ericonaldo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.03403",
            "authors": [
                {
                    "_id": "68ba320f736018af705e8e1b",
                    "name": "Chenlu Ye",
                    "hidden": false
                },
                {
                    "_id": "68ba320f736018af705e8e1c",
                    "name": "Zhou Yu",
                    "hidden": false
                },
                {
                    "_id": "68ba320f736018af705e8e1d",
                    "name": "Ziji Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ba320f736018af705e8e1e",
                    "name": "Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "68ba320f736018af705e8e1f",
                    "name": "Narayanan Sadagopan",
                    "hidden": false
                },
                {
                    "_id": "68ba320f736018af705e8e20",
                    "name": "Jing Huang",
                    "hidden": false
                },
                {
                    "_id": "68ba320f736018af705e8e21",
                    "name": "Tong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ba320f736018af705e8e22",
                    "name": "Anurag Beniwal",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-03T15:28:51.000Z",
            "submittedOnDailyAt": "2025-09-04T23:14:30.126Z",
            "title": "Beyond Correctness: Harmonizing Process and Outcome Rewards through RL\n  Training",
            "submittedOnDailyBy": {
                "_id": "65eec5c1d7d63c2ed0615421",
                "avatarUrl": "/avatars/8c32f5e7d4b1940088bdec73c0b86fab.svg",
                "isPro": false,
                "fullname": "Chenlu Ye",
                "user": "Chenlu123",
                "type": "user"
            },
            "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged to be a\npredominant paradigm for mathematical reasoning tasks, offering stable\nimprovements in reasoning ability. However, Outcome Reward Models (ORMs) in\nRLVR are too coarse-grained to distinguish flawed reasoning within correct\nanswers or valid reasoning within incorrect answers. This lack of granularity\nintroduces noisy and misleading gradients significantly and hinders further\nprogress in reasoning process quality. While Process Reward Models (PRMs) offer\nfine-grained guidance for intermediate steps, they frequently suffer from\ninaccuracies and are susceptible to reward hacking.\n  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an\neffective data process curation method that harmonizes noisy, fine-grained\nprocess rewards with accurate, coarse-grained outcome rewards. Rather than\nnaively blending PRM and ORM in the objective function\n(arXiv:archive/2506.18896), PROF leverages their complementary strengths\nthrough consistency-driven sample selection. Our approach retains correct\nresponses with higher averaged process values and incorrect responses with\nlower averaged process values, while maintaining positive/negative training\nsample balance. Extensive experiments demonstrate that our method not only\nconsistently improves the final accuracy over 4% compared to the blending\napproaches, but also strengthens the quality of intermediate reasoning steps.\nCodes and training recipes are available at https://github.com/Chenluye99/PROF.",
            "upvotes": 1,
            "discussionId": "68ba3210736018af705e8e23",
            "ai_summary": "PROF, a data curation method, improves reinforcement learning for mathematical reasoning by harmonizing process and outcome rewards, enhancing both final accuracy and intermediate reasoning quality.",
            "ai_keywords": [
                "Reinforcement learning",
                "verifiable rewards",
                "RLVR",
                "Outcome Reward Models",
                "ORMs",
                "Process Reward Models",
                "PRMs",
                "PRocess cOnsistency Filter",
                "PROF",
                "consistency-driven sample selection",
                "intermediate reasoning steps"
            ]
        },
        "publishedAt": "2025-09-03T11:28:51.000Z",
        "title": "Beyond Correctness: Harmonizing Process and Outcome Rewards through RL\n  Training",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has emerged to be a\npredominant paradigm for mathematical reasoning tasks, offering stable\nimprovements in reasoning ability. However, Outcome Reward Models (ORMs) in\nRLVR are too coarse-grained to distinguish flawed reasoning within correct\nanswers or valid reasoning within incorrect answers. This lack of granularity\nintroduces noisy and misleading gradients significantly and hinders further\nprogress in reasoning process quality. While Process Reward Models (PRMs) offer\nfine-grained guidance for intermediate steps, they frequently suffer from\ninaccuracies and are susceptible to reward hacking.\n  To resolve this dilemma, we introduce PRocess cOnsistency Filter (PROF), an\neffective data process curation method that harmonizes noisy, fine-grained\nprocess rewards with accurate, coarse-grained outcome rewards. Rather than\nnaively blending PRM and ORM in the objective function\n(arXiv:archive/2506.18896), PROF leverages their complementary strengths\nthrough consistency-driven sample selection. Our approach retains correct\nresponses with higher averaged process values and incorrect responses with\nlower averaged process values, while maintaining positive/negative training\nsample balance. Extensive experiments demonstrate that our method not only\nconsistently improves the final accuracy over 4% compared to the blending\napproaches, but also strengthens the quality of intermediate reasoning steps.\nCodes and training recipes are available at https://github.com/Chenluye99/PROF.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03403.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65eec5c1d7d63c2ed0615421",
            "avatarUrl": "/avatars/8c32f5e7d4b1940088bdec73c0b86fab.svg",
            "fullname": "Chenlu Ye",
            "name": "Chenlu123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.00930",
            "authors": [
                {
                    "_id": "68b9c9be736018af705e8d9a",
                    "user": {
                        "_id": "63935b14b567032fe13b030d",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670601442899-noauth.jpeg",
                        "isPro": false,
                        "fullname": "Adam Yanxiao Zhao",
                        "user": "sdpkjc",
                        "type": "user"
                    },
                    "name": "Yanxiao Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T19:32:45.230Z",
                    "hidden": false
                },
                {
                    "_id": "68b9c9be736018af705e8d9b",
                    "name": "Yaqian Li",
                    "hidden": false
                },
                {
                    "_id": "68b9c9be736018af705e8d9c",
                    "name": "Zihao Bo",
                    "hidden": false
                },
                {
                    "_id": "68b9c9be736018af705e8d9d",
                    "name": "Rinyoichi Takezoe",
                    "hidden": false
                },
                {
                    "_id": "68b9c9be736018af705e8d9e",
                    "name": "Haojia Hui",
                    "hidden": false
                },
                {
                    "_id": "68b9c9be736018af705e8d9f",
                    "name": "Mo Guang",
                    "hidden": false
                },
                {
                    "_id": "68b9c9be736018af705e8da0",
                    "name": "Lei Ren",
                    "hidden": false
                },
                {
                    "_id": "68b9c9be736018af705e8da1",
                    "name": "Xiaolin Qin",
                    "hidden": false
                },
                {
                    "_id": "68b9c9be736018af705e8da2",
                    "name": "Kaiwen Long",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63935b14b567032fe13b030d/0JxBeM0BFlJ7VpVAsPqqz.png"
            ],
            "publishedAt": "2025-08-31T16:56:06.000Z",
            "submittedOnDailyAt": "2025-09-04T15:53:49.673Z",
            "title": "SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement\n  Fine-Tuning of LLMs",
            "submittedOnDailyBy": {
                "_id": "63935b14b567032fe13b030d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670601442899-noauth.jpeg",
                "isPro": false,
                "fullname": "Adam Yanxiao Zhao",
                "user": "sdpkjc",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\ngeneral reasoning capabilities. However, systematically evaluating and\nenhancing these reasoning capabilities is challenging due to the lack of\ncontrollable and scalable tools for fine-grained analysis. Existing benchmarks\nand datasets often lack the necessary variable control for multi-dimensional,\nsystematic analysis and training, or have narrow problem types and formats. To\naddress these limitations, we introduce SATQuest, a systematic verifier\ndesigned to evaluate and enhance logical reasoning in LLMs by generating\ndiverse, Satisfiability-based logical reasoning problems directly from\nConjunctive Normal Form (CNF) instances. SATQuest structures these problems\nalong three orthogonal dimensions: instance scale, problem type, and question\nformat, employing randomized, SAT-based problem generation and objective answer\nverification via PySAT. This design mitigates memorization issues, allows for\nnuanced insights into reasoning performance, and enables effective\nreinforcement fine-tuning. Our extensive evaluation of various LLMs using\nSATQuest identified significant limitations in their logical reasoning,\nparticularly in generalizing beyond familiar mathematical formats. Furthermore,\nwe show that reinforcement fine-tuning with SATQuest rewards substantially\nimproves targeted task performance and generalizes to more complex instances,\nwhile highlighting remaining challenges in cross-format adaptation. Through\nthese demonstrations, we showcase SATQuest's potential as a foundational tool\nand a valuable starting point for advancing LLM logical reasoning.",
            "upvotes": 1,
            "discussionId": "68b9c9be736018af705e8da3",
            "githubRepo": "https://github.com/sdpkjc/SATQuest",
            "ai_summary": "SATQuest evaluates and enhances LLM logical reasoning by generating diverse SAT-based problems, offering insights into reasoning performance and enabling effective fine-tuning.",
            "ai_keywords": [
                "Large Language Models",
                "SATQuest",
                "logical reasoning",
                "Conjunctive Normal Form",
                "SAT-based problem generation",
                "PySAT",
                "reinforcement fine-tuning",
                "cross-format adaptation"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-08-31T12:56:06.000Z",
        "title": "SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement\n  Fine-Tuning of LLMs",
        "summary": "Recent advances in Large Language Models (LLMs) have demonstrated remarkable\ngeneral reasoning capabilities. However, systematically evaluating and\nenhancing these reasoning capabilities is challenging due to the lack of\ncontrollable and scalable tools for fine-grained analysis. Existing benchmarks\nand datasets often lack the necessary variable control for multi-dimensional,\nsystematic analysis and training, or have narrow problem types and formats. To\naddress these limitations, we introduce SATQuest, a systematic verifier\ndesigned to evaluate and enhance logical reasoning in LLMs by generating\ndiverse, Satisfiability-based logical reasoning problems directly from\nConjunctive Normal Form (CNF) instances. SATQuest structures these problems\nalong three orthogonal dimensions: instance scale, problem type, and question\nformat, employing randomized, SAT-based problem generation and objective answer\nverification via PySAT. This design mitigates memorization issues, allows for\nnuanced insights into reasoning performance, and enables effective\nreinforcement fine-tuning. Our extensive evaluation of various LLMs using\nSATQuest identified significant limitations in their logical reasoning,\nparticularly in generalizing beyond familiar mathematical formats. Furthermore,\nwe show that reinforcement fine-tuning with SATQuest rewards substantially\nimproves targeted task performance and generalizes to more complex instances,\nwhile highlighting remaining challenges in cross-format adaptation. Through\nthese demonstrations, we showcase SATQuest's potential as a foundational tool\nand a valuable starting point for advancing LLM logical reasoning.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63935b14b567032fe13b030d/0JxBeM0BFlJ7VpVAsPqqz.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00930.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "63935b14b567032fe13b030d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1670601442899-noauth.jpeg",
            "fullname": "Adam Yanxiao Zhao",
            "name": "sdpkjc",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    }
]