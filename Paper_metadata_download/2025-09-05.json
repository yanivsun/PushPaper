[
    {
        "paper": {
            "id": "2509.03867",
            "authors": [
                {
                    "_id": "68ba3302736018af705e8e25",
                    "user": {
                        "_id": "60f313f4adf471cbdf8bb66a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f313f4adf471cbdf8bb66a/5NJFqnldE_0fdE_mEvz9V.jpeg",
                        "isPro": false,
                        "fullname": "Yang Wang",
                        "user": "yangwang825",
                        "type": "user"
                    },
                    "name": "Yang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T14:16:16.939Z",
                    "hidden": false
                },
                {
                    "_id": "68ba3302736018af705e8e26",
                    "user": {
                        "_id": "63108cc834c7d77420b0fd68",
                        "avatarUrl": "/avatars/2721e573a417a8ec0b81ee048c4b42ba.svg",
                        "isPro": false,
                        "fullname": "chenghao xiao",
                        "user": "gowitheflow",
                        "type": "user"
                    },
                    "name": "Chenghao Xiao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:29:05.255Z",
                    "hidden": false
                },
                {
                    "_id": "68ba3302736018af705e8e27",
                    "name": "Chia-Yi Hsiao",
                    "hidden": false
                },
                {
                    "_id": "68ba3302736018af705e8e28",
                    "user": {
                        "_id": "68baafd4074aa57dade0f63e",
                        "avatarUrl": "/avatars/cd8befad1ac9952e854d7b00590a82cc.svg",
                        "isPro": false,
                        "fullname": "Zi Yan Chang",
                        "user": "ZYC101",
                        "type": "user"
                    },
                    "name": "Zi Yan Chang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:29:14.766Z",
                    "hidden": false
                },
                {
                    "_id": "68ba3302736018af705e8e29",
                    "user": {
                        "_id": "68baa91c4d1376777b5c946a",
                        "avatarUrl": "/avatars/f2d7392e2d52d57d531420f87803c875.svg",
                        "isPro": false,
                        "fullname": "Chi Li Chen",
                        "user": "Chilly0105",
                        "type": "user"
                    },
                    "name": "Chi-Li Chen",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:29:21.489Z",
                    "hidden": false
                },
                {
                    "_id": "68ba3302736018af705e8e2a",
                    "user": {
                        "_id": "634e94582cd84978c49089c1",
                        "avatarUrl": "/avatars/c41640cda7a8a76555e3410b67ef064a.svg",
                        "isPro": false,
                        "fullname": "Tyler Loakman",
                        "user": "tylerl404",
                        "type": "user"
                    },
                    "name": "Tyler Loakman",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:29:27.470Z",
                    "hidden": false
                },
                {
                    "_id": "68ba3302736018af705e8e2b",
                    "name": "Chenghua Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T03:58:55.000Z",
            "submittedOnDailyAt": "2025-09-05T06:33:17.605Z",
            "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth",
            "submittedOnDailyBy": {
                "_id": "60f313f4adf471cbdf8bb66a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f313f4adf471cbdf8bb66a/5NJFqnldE_0fdE_mEvz9V.jpeg",
                "isPro": false,
                "fullname": "Yang Wang",
                "user": "yangwang825",
                "type": "user"
            },
            "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\", utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a small but diverse benchmark dataset of over 1,200 meticulously\ncurated examples, with select instances in English, Mandarin, Spanish, French,\nJapanese, and Korean. Annotation was especially challenging: each of the\nexamples required careful expert review to verify that it truly reflected\nDrivelological characteristics. The process involved multiple rounds of\ndiscussion and adjudication to address disagreements, highlighting the subtle\nand subjective nature of the Drivelology. We evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss the implied rhetorical function\naltogether. These findings highlight a deeper representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.",
            "upvotes": 97,
            "discussionId": "68ba3303736018af705e8e2c",
            "projectPage": "https://huggingface.co/datasets/extraordinarylab/drivel-hub",
            "githubRepo": "https://github.com/ExtraOrdinaryLab/drivelology",
            "ai_summary": "LLMs struggle with understanding the nuanced, context-dependent meanings of Drivelological text, which appears nonsensical but contains deeper semantic layers.",
            "ai_keywords": [
                "Drivelology",
                "large language models",
                "LLMs",
                "natural language processing",
                "NLP",
                "benchmark dataset",
                "Drivelological text",
                "classification",
                "generation",
                "reasoning tasks",
                "pragmatic understanding",
                "cognitive comprehension"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-03T23:58:55.000Z",
        "title": "Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth",
        "summary": "We introduce Drivelology, a unique linguistic phenomenon characterised as\n\"nonsense with depth\", utterances that are syntactically coherent yet\npragmatically paradoxical, emotionally loaded, or rhetorically subversive.\nWhile such expressions may resemble surface-level nonsense, they encode\nimplicit meaning requiring contextual inference, moral reasoning, or emotional\ninterpretation. We find that current large language models (LLMs), despite\nexcelling at many natural language processing (NLP) tasks, consistently fail to\ngrasp the layered semantics of Drivelological text. To investigate this, we\nconstruct a small but diverse benchmark dataset of over 1,200 meticulously\ncurated examples, with select instances in English, Mandarin, Spanish, French,\nJapanese, and Korean. Annotation was especially challenging: each of the\nexamples required careful expert review to verify that it truly reflected\nDrivelological characteristics. The process involved multiple rounds of\ndiscussion and adjudication to address disagreements, highlighting the subtle\nand subjective nature of the Drivelology. We evaluate a range of LLMs on\nclassification, generation, and reasoning tasks. Our results reveal clear\nlimitations of LLMs: models often confuse Drivelology with shallow nonsense,\nproduce incoherent justifications, or miss the implied rhetorical function\naltogether. These findings highlight a deeper representational gap in LLMs'\npragmatic understanding and challenge the assumption that statistical fluency\nimplies cognitive comprehension. We release our dataset and code to facilitate\nfurther research in modelling linguistic depth beyond surface-level coherence.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03867.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60f313f4adf471cbdf8bb66a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/60f313f4adf471cbdf8bb66a/5NJFqnldE_0fdE_mEvz9V.jpeg",
            "fullname": "Yang Wang",
            "name": "yangwang825",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.04338",
            "authors": [
                {
                    "_id": "68ba64a6736018af705e8ea9",
                    "user": {
                        "_id": "63cf4ceb7332dafae2b9884c",
                        "avatarUrl": "/avatars/652255476e1898c175c28f7a1d67cc06.svg",
                        "isPro": false,
                        "fullname": "JiYuan Wang",
                        "user": "exander",
                        "type": "user"
                    },
                    "name": "JiYuan Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T08:01:34.119Z",
                    "hidden": false
                },
                {
                    "_id": "68ba64a6736018af705e8eaa",
                    "user": {
                        "_id": "67aea8aabdb9354439534e49",
                        "avatarUrl": "/avatars/79d245ab62aae59279ccd366d7f1de5e.svg",
                        "isPro": false,
                        "fullname": "Chris Lin",
                        "user": "chunyulin",
                        "type": "user"
                    },
                    "name": "Chunyu Lin",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:30:01.810Z",
                    "hidden": false
                },
                {
                    "_id": "68ba64a6736018af705e8eab",
                    "name": "Lei Sun",
                    "hidden": false
                },
                {
                    "_id": "68ba64a6736018af705e8eac",
                    "user": {
                        "_id": "68288895682a9738b93b1d2b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/w_nnDk938berOQvlZsByM.jpeg",
                        "isPro": false,
                        "fullname": "Rongying Liu",
                        "user": "Roinnnn11",
                        "type": "user"
                    },
                    "name": "Rongying Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:30:07.625Z",
                    "hidden": false
                },
                {
                    "_id": "68ba64a6736018af705e8ead",
                    "name": "Lang Nie",
                    "hidden": false
                },
                {
                    "_id": "68ba64a6736018af705e8eae",
                    "user": {
                        "_id": "6513a0f14f1682e4407758a9",
                        "avatarUrl": "/avatars/b2a6886114492944cfa235363817565f.svg",
                        "isPro": false,
                        "fullname": "Mingxing Li",
                        "user": "MingxingLi",
                        "type": "user"
                    },
                    "name": "Mingxing Li",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:30:32.021Z",
                    "hidden": false
                },
                {
                    "_id": "68ba64a6736018af705e8eaf",
                    "user": {
                        "_id": "65bc98383b879593a5a2f5e5",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65bc98383b879593a5a2f5e5/p2ZtoTFN6tW-QkcPJf7YT.jpeg",
                        "isPro": true,
                        "fullname": "Kang Liao",
                        "user": "KangLiao",
                        "type": "user"
                    },
                    "name": "Kang Liao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:30:13.569Z",
                    "hidden": false
                },
                {
                    "_id": "68ba64a6736018af705e8eb0",
                    "name": "Xiangxiang Chu",
                    "hidden": false
                },
                {
                    "_id": "68ba64a6736018af705e8eb1",
                    "name": "Yao Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T15:58:50.000Z",
            "submittedOnDailyAt": "2025-09-05T02:49:35.984Z",
            "title": "From Editor to Dense Geometry Estimator",
            "submittedOnDailyBy": {
                "_id": "66d255e3947594430c723ff6",
                "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
                "isPro": false,
                "fullname": "xiaochonglinghu",
                "user": "xiaochonglinghu",
                "type": "user"
            },
            "summary": "Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce FE2E, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100times data.\nThe project page can be accessed https://amap-ml.github.io/FE2E/{here}.",
            "upvotes": 62,
            "discussionId": "68ba64a7736018af705e8eb2",
            "projectPage": "https://amap-ml.github.io/FE2E/",
            "githubRepo": "https://github.com/AMAP-ML/FE2E/",
            "ai_summary": "FE2E, a framework using a Diffusion Transformer for dense geometry prediction, outperforms generative models in zero-shot monocular depth and normal estimation with improved performance and efficiency.",
            "ai_keywords": [
                "text-to-image",
                "dense prediction",
                "image editing models",
                "generative models",
                "fine-tuning",
                "dense geometry estimation",
                "structural priors",
                "flow matching loss",
                "consistent velocity",
                "logarithmic quantization",
                "BFloat16",
                "global attention",
                "ETH3D",
                "DepthAnything"
            ],
            "githubStars": 20
        },
        "publishedAt": "2025-09-04T11:58:50.000Z",
        "title": "From Editor to Dense Geometry Estimator",
        "summary": "Leveraging visual priors from pre-trained text-to-image (T2I) generative\nmodels has shown success in dense prediction. However, dense prediction is\ninherently an image-to-image task, suggesting that image editing models, rather\nthan T2I generative models, may be a more suitable foundation for fine-tuning.\n  Motivated by this, we conduct a systematic analysis of the fine-tuning\nbehaviors of both editors and generators for dense geometry estimation. Our\nfindings show that editing models possess inherent structural priors, which\nenable them to converge more stably by ``refining\" their innate features, and\nultimately achieve higher performance than their generative counterparts.\n  Based on these findings, we introduce FE2E, a framework that\npioneeringly adapts an advanced editing model based on Diffusion Transformer\n(DiT) architecture for dense geometry prediction. Specifically, to tailor the\neditor for this deterministic task, we reformulate the editor's original flow\nmatching loss into the ``consistent velocity\" training objective. And we use\nlogarithmic quantization to resolve the precision conflict between the editor's\nnative BFloat16 format and the high precision demand of our tasks.\nAdditionally, we leverage the DiT's global attention for a cost-free joint\nestimation of depth and normals in a single forward pass, enabling their\nsupervisory signals to mutually enhance each other.\n  Without scaling up the training data, FE2E achieves impressive performance\nimprovements in zero-shot monocular depth and normal estimation across multiple\ndatasets. Notably, it achieves over 35\\% performance gains on the ETH3D dataset\nand outperforms the DepthAnything series, which is trained on 100times data.\nThe project page can be accessed https://amap-ml.github.io/FE2E/{here}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04338.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66d255e3947594430c723ff6",
            "avatarUrl": "/avatars/c56e4792332a01bf34085a75ee64916e.svg",
            "fullname": "xiaochonglinghu",
            "name": "xiaochonglinghu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.04419",
            "authors": [
                {
                    "_id": "68ba4a27736018af705e8e47",
                    "user": {
                        "_id": "663f07d029be04778ba97871",
                        "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
                        "isPro": false,
                        "fullname": "Xingtai Lv",
                        "user": "XingtaiHF",
                        "type": "user"
                    },
                    "name": "Xingtai Lv",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T15:21:46.656Z",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e48",
                    "name": "Yuxin Zuo",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e49",
                    "name": "Youbang Sun",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e4a",
                    "name": "Hongyi Liu",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e4b",
                    "name": "Yuntian Wei",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e4c",
                    "name": "Zhekai Chen",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e4d",
                    "name": "Lixuan He",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e4e",
                    "name": "Xuekai Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e4f",
                    "user": {
                        "_id": "60bc94cd85a3ab33829b6211",
                        "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
                        "isPro": false,
                        "fullname": "Kaiyan Zhang",
                        "user": "iseesaw",
                        "type": "user"
                    },
                    "name": "Kaiyan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T08:01:42.328Z",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e50",
                    "name": "Bingning Wang",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e51",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "68ba4a27736018af705e8e52",
                    "name": "Bowen Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T17:40:33.000Z",
            "submittedOnDailyAt": "2025-09-05T01:00:55.060Z",
            "title": "Towards a Unified View of Large Language Model Post-Training",
            "submittedOnDailyBy": {
                "_id": "663f07d029be04778ba97871",
                "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
                "isPro": false,
                "fullname": "Xingtai Lv",
                "user": "XingtaiHF",
                "type": "user"
            },
            "summary": "Two major sources of training data exist for post-training modern language\nmodels: online (model-generated rollouts) data, and offline (human or\nother-model demonstrations) data. These two types of data are typically used by\napproaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),\nrespectively. In this paper, we show that these approaches are not in\ncontradiction, but are instances of a single optimization process. We derive a\nUnified Policy Gradient Estimator, and present the calculations of a wide\nspectrum of post-training approaches as the gradient of a common objective\nunder different data distribution assumptions and various bias-variance\ntradeoffs. The gradient estimator is constructed with four interchangeable\nparts: stabilization mask, reference policy denominator, advantage estimate,\nand likelihood gradient. Motivated by our theoretical findings, we propose\nHybrid Post-Training (HPT), an algorithm that dynamically selects different\ntraining signals. HPT is designed to yield both effective exploitation of\ndemonstration and stable exploration without sacrificing learned reasoning\npatterns. We provide extensive experiments and ablation studies to verify the\neffectiveness of our unified theoretical framework and HPT. Across six\nmathematical reasoning benchmarks and two out-of-distribution suites, HPT\nconsistently surpasses strong baselines across models of varying scales and\nfamilies.",
            "upvotes": 41,
            "discussionId": "68ba4a27736018af705e8e53",
            "githubRepo": "https://github.com/TsinghuaC3I/Unify-Post-Training",
            "ai_summary": "A unified policy gradient estimator and Hybrid Post-Training algorithm effectively combine online and offline data for post-training language models, improving performance across various benchmarks.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Supervised Fine-Tuning",
                "Unified Policy Gradient Estimator",
                "stabilization mask",
                "reference policy denominator",
                "advantage estimate",
                "likelihood gradient",
                "Hybrid Post-Training",
                "mathematical reasoning benchmarks",
                "out-of-distribution suites"
            ],
            "githubStars": 42
        },
        "publishedAt": "2025-09-04T13:40:33.000Z",
        "title": "Towards a Unified View of Large Language Model Post-Training",
        "summary": "Two major sources of training data exist for post-training modern language\nmodels: online (model-generated rollouts) data, and offline (human or\nother-model demonstrations) data. These two types of data are typically used by\napproaches like Reinforcement Learning (RL) and Supervised Fine-Tuning (SFT),\nrespectively. In this paper, we show that these approaches are not in\ncontradiction, but are instances of a single optimization process. We derive a\nUnified Policy Gradient Estimator, and present the calculations of a wide\nspectrum of post-training approaches as the gradient of a common objective\nunder different data distribution assumptions and various bias-variance\ntradeoffs. The gradient estimator is constructed with four interchangeable\nparts: stabilization mask, reference policy denominator, advantage estimate,\nand likelihood gradient. Motivated by our theoretical findings, we propose\nHybrid Post-Training (HPT), an algorithm that dynamically selects different\ntraining signals. HPT is designed to yield both effective exploitation of\ndemonstration and stable exploration without sacrificing learned reasoning\npatterns. We provide extensive experiments and ablation studies to verify the\neffectiveness of our unified theoretical framework and HPT. Across six\nmathematical reasoning benchmarks and two out-of-distribution suites, HPT\nconsistently surpasses strong baselines across models of varying scales and\nfamilies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04419.png",
        "numComments": 6,
        "submittedBy": {
            "_id": "663f07d029be04778ba97871",
            "avatarUrl": "/avatars/fb7c9d4a2c537d918a3267e7cbc03f04.svg",
            "fullname": "Xingtai Lv",
            "name": "XingtaiHF",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.01396",
            "authors": [
                {
                    "_id": "68b98e69736018af705e8d41",
                    "user": {
                        "_id": "65a8bcb717d869bb7487c2a1",
                        "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
                        "isPro": false,
                        "fullname": "Haiyuan Wan",
                        "user": "haiyuanwan",
                        "type": "user"
                    },
                    "name": "Haiyuan Wan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T19:36:56.793Z",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d42",
                    "name": "Chen Yang",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d43",
                    "name": "Junchi Yu",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d44",
                    "name": "Meiqi Tu",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d45",
                    "name": "Jiaxuan Lu",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d46",
                    "name": "Di Yu",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d47",
                    "name": "Jianbao Cao",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d48",
                    "name": "Ben Gao",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d49",
                    "name": "Jiaqing Xie",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d4a",
                    "user": {
                        "_id": "666839475ddf78d3bc5b80f4",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/j4OBskJ_GMS2QGE_nEuEJ.jpeg",
                        "isPro": false,
                        "fullname": "Aoran Wang",
                        "user": "RalfWang",
                        "type": "user"
                    },
                    "name": "Aoran Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T14:16:19.262Z",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d4b",
                    "name": "Wenlong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d4c",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "68b98e69736018af705e8d4d",
                    "name": "Dongzhan Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-01T11:42:47.000Z",
            "submittedOnDailyAt": "2025-09-05T00:41:12.849Z",
            "title": "DeepResearch Arena: The First Exam of LLMs' Research Abilities via\n  Seminar-Grounded Tasks",
            "submittedOnDailyBy": {
                "_id": "65a8bcb717d869bb7487c2a1",
                "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
                "isPro": false,
                "fullname": "Haiyuan Wan",
                "user": "haiyuanwan",
                "type": "user"
            },
            "summary": "Deep research agents have attracted growing attention for their potential to\norchestrate multi-stage research workflows, spanning literature synthesis,\nmethodological design, and empirical verification. Despite these strides,\nevaluating their research capability faithfully is rather challenging due to\nthe difficulty of collecting frontier research questions that genuinely capture\nresearchers' attention and intellectual curiosity. To address this gap, we\nintroduce DeepResearch Arena, a benchmark grounded in academic seminars that\ncapture rich expert discourse and interaction, better reflecting real-world\nresearch environments and reducing the risk of data leakage. To automatically\nconstruct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task\nGeneration (MAHTG) system that extracts research-worthy inspirations from\nseminar transcripts. The MAHTG system further translates research-worthy\ninspirations into high-quality research tasks, ensuring the traceability of\nresearch task formulation while filtering noise. With the MAHTG system, we\ncurate DeepResearch Arena with over 10,000 high-quality research tasks from\nover 200 academic seminars, spanning 12 disciplines, such as literature,\nhistory, and science. Our extensive evaluation shows that DeepResearch Arena\npresents substantial challenges for current state-of-the-art agents, with clear\nperformance gaps observed across different models.",
            "upvotes": 40,
            "discussionId": "68b98e6a736018af705e8d4e",
            "ai_summary": "DeepResearch Arena, a benchmark using academic seminar transcripts, provides high-quality research tasks to evaluate deep research agents across multiple disciplines.",
            "ai_keywords": [
                "DeepResearch Arena",
                "Multi-Agent Hierarchical Task Generation",
                "MAHTG",
                "research tasks",
                "academic seminars",
                "research agents"
            ]
        },
        "publishedAt": "2025-09-01T07:42:47.000Z",
        "title": "DeepResearch Arena: The First Exam of LLMs' Research Abilities via\n  Seminar-Grounded Tasks",
        "summary": "Deep research agents have attracted growing attention for their potential to\norchestrate multi-stage research workflows, spanning literature synthesis,\nmethodological design, and empirical verification. Despite these strides,\nevaluating their research capability faithfully is rather challenging due to\nthe difficulty of collecting frontier research questions that genuinely capture\nresearchers' attention and intellectual curiosity. To address this gap, we\nintroduce DeepResearch Arena, a benchmark grounded in academic seminars that\ncapture rich expert discourse and interaction, better reflecting real-world\nresearch environments and reducing the risk of data leakage. To automatically\nconstruct DeepResearch Arena, we propose a Multi-Agent Hierarchical Task\nGeneration (MAHTG) system that extracts research-worthy inspirations from\nseminar transcripts. The MAHTG system further translates research-worthy\ninspirations into high-quality research tasks, ensuring the traceability of\nresearch task formulation while filtering noise. With the MAHTG system, we\ncurate DeepResearch Arena with over 10,000 high-quality research tasks from\nover 200 academic seminars, spanning 12 disciplines, such as literature,\nhistory, and science. Our extensive evaluation shows that DeepResearch Arena\npresents substantial challenges for current state-of-the-art agents, with clear\nperformance gaps observed across different models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01396.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "65a8bcb717d869bb7487c2a1",
            "avatarUrl": "/avatars/261c28f7e616a8482970f50c1f8919fd.svg",
            "fullname": "Haiyuan Wan",
            "name": "haiyuanwan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.04292",
            "authors": [
                {
                    "_id": "68ba6843736018af705e8eb4",
                    "user": {
                        "_id": "68ba81e3d389ea993bf35893",
                        "avatarUrl": "/avatars/67a790f8cd8423de63196adda1ec12a3.svg",
                        "isPro": false,
                        "fullname": "Qinyan Zhang",
                        "user": "zqyzqyzqy",
                        "type": "user"
                    },
                    "name": "Qinyan Zhang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:20:01.213Z",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8eb5",
                    "user": {
                        "_id": "646319712538819c72a1b699",
                        "avatarUrl": "/avatars/19ace41095877f5a485d9100b393ca13.svg",
                        "isPro": false,
                        "fullname": "lei xin ping",
                        "user": "leixp",
                        "type": "user"
                    },
                    "name": "Xinping Lei",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:20:17.772Z",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8eb6",
                    "user": {
                        "_id": "66cd5ebadbfa8dff219acb70",
                        "avatarUrl": "/avatars/fd4225dca0d4c3ba7d86304bd1d826b3.svg",
                        "isPro": false,
                        "fullname": "Ruijie Miao",
                        "user": "Baton6257",
                        "type": "user"
                    },
                    "name": "Ruijie Miao",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:20:24.037Z",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8eb7",
                    "name": "Yu Fu",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8eb8",
                    "name": "Haojie Fan",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8eb9",
                    "name": "Le Chang",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8eba",
                    "name": "Jiafan Hou",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ebb",
                    "name": "Dingling Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ebc",
                    "name": "Zhongfei Hou",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ebd",
                    "user": {
                        "_id": "68a00f7b5140d6d8305ea065",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/ShETxmglZOsaHN7q9Gzfv.png",
                        "isPro": false,
                        "fullname": "杨子强",
                        "user": "ziqiangyang",
                        "type": "user"
                    },
                    "name": "Ziqiang Yang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:20:51.949Z",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ebe",
                    "name": "Changxin Pu",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ebf",
                    "name": "Fei Hu",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ec0",
                    "name": "Jingkai Liu",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ec1",
                    "user": {
                        "_id": "665929223498a9ef525d7b1c",
                        "avatarUrl": "/avatars/445f5298228d54ed5cfc27b6a92a339e.svg",
                        "isPro": false,
                        "fullname": "Mengyun Liu",
                        "user": "liumengyun",
                        "type": "user"
                    },
                    "name": "Mengyun Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:21:17.429Z",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ec2",
                    "name": "Yang Liu",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ec3",
                    "name": "Xiang Gao",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ec4",
                    "user": {
                        "_id": "65377c30e48353201e6fdda0",
                        "avatarUrl": "/avatars/a8f803b6f2e598eaee9c52c0d2ddfc16.svg",
                        "isPro": false,
                        "fullname": "Jiaheng Liu",
                        "user": "CheeryLJH",
                        "type": "user"
                    },
                    "name": "Jiaheng Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:21:09.876Z",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ec5",
                    "name": "Tong Yang",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ec6",
                    "name": "Zaiyuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ec7",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T08:01:32.173Z",
                    "hidden": false
                },
                {
                    "_id": "68ba6843736018af705e8ec8",
                    "user": {
                        "_id": "686c983f8c812deb40e83270",
                        "avatarUrl": "/avatars/7d1e4a38929b7ac79004463fd0060e9b.svg",
                        "isPro": false,
                        "fullname": "Wenhao Huang",
                        "user": "WenhaoHuang",
                        "type": "user"
                    },
                    "name": "Wenhao Huang",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:19:54.443Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T15:03:02.000Z",
            "submittedOnDailyAt": "2025-09-05T03:04:17.131Z",
            "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.",
            "upvotes": 39,
            "discussionId": "68ba6844736018af705e8ec9",
            "projectPage": "https://huggingface.co/datasets/m-a-p/Inverse_IFEval",
            "ai_summary": "Inverse IFEval evaluates Large Language Models' ability to override training biases and follow unconventional instructions, highlighting the need for adaptability in diverse contexts.",
            "ai_keywords": [
                "Large Language Models",
                "cognitive inertia",
                "supervised fine-tuning",
                "Inverse IFEval",
                "Counter-intuitive Ability",
                "Question Correction",
                "Intentional Textual Flaws",
                "Code without Comments",
                "Counterfactual Answering",
                "LLM-as-a-Judge"
            ]
        },
        "publishedAt": "2025-09-04T11:03:02.000Z",
        "title": "Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow\n  Real Instructions?",
        "summary": "Large Language Models (LLMs) achieve strong performance on diverse tasks but\noften exhibit cognitive inertia, struggling to follow instructions that\nconflict with the standardized patterns learned during supervised fine-tuning\n(SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that\nmeasures models Counter-intuitive Abilitytheir capacity to override\ntraining-induced biases and comply with adversarial instructions. Inverse\nIFEval introduces eight types of such challenges, including Question\nCorrection, Intentional Textual Flaws, Code without Comments, and\nCounterfactual Answering. Using a human-in-the-loop pipeline, we construct a\ndataset of 1012 high-quality Chinese and English questions across 23 domains,\nevaluated under an optimized LLM-as-a-Judge framework. Experiments on existing\nleading LLMs demonstrate the necessity of our proposed Inverse IFEval\nbenchmark. Our findings emphasize that future alignment efforts should not only\npursue fluency and factual correctness but also account for adaptability under\nunconventional contexts. We hope that Inverse IFEval serves as both a\ndiagnostic tool and a foundation for developing methods that mitigate cognitive\ninertia, reduce overfitting to narrow patterns, and ultimately enhance the\ninstruction-following reliability of LLMs in diverse and unpredictable\nreal-world scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04292.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 98
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.04394",
            "authors": [
                {
                    "_id": "68ba5465736018af705e8e71",
                    "user": {
                        "_id": "64b7aa374df206a3ed1947d2",
                        "avatarUrl": "/avatars/a7c7e703ccf8824259fc5a8a90a25746.svg",
                        "isPro": false,
                        "fullname": "wzd",
                        "user": "GoodEnough",
                        "type": "user"
                    },
                    "name": "Zidong Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T08:01:39.870Z",
                    "hidden": false
                },
                {
                    "_id": "68ba5465736018af705e8e72",
                    "name": "Yiyuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ba5465736018af705e8e73",
                    "name": "Xiaoyu Yue",
                    "hidden": false
                },
                {
                    "_id": "68ba5465736018af705e8e74",
                    "name": "Xiangyu Yue",
                    "hidden": false
                },
                {
                    "_id": "68ba5465736018af705e8e75",
                    "name": "Yangguang Li",
                    "hidden": false
                },
                {
                    "_id": "68ba5465736018af705e8e76",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68ba5465736018af705e8e77",
                    "name": "Lei Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T17:05:59.000Z",
            "submittedOnDailyAt": "2025-09-05T01:49:34.223Z",
            "title": "Transition Models: Rethinking the Generative Learning Objective",
            "submittedOnDailyBy": {
                "_id": "63176933b58b0184630d2c74",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
                "isPro": false,
                "fullname": "Yiyuan Zhang",
                "user": "Yiyuan",
                "type": "user"
            },
            "summary": "A fundamental dilemma in generative modeling persists: iterative diffusion\nmodels achieve outstanding fidelity, but at a significant computational cost,\nwhile efficient few-step alternatives are constrained by a hard quality\nceiling. This conflict between generation steps and output quality arises from\nrestrictive training objectives that focus exclusively on either infinitesimal\ndynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by\nintroducing an exact, continuous-time dynamics equation that analytically\ndefines state transitions across any finite time interval. This leads to a\nnovel generative paradigm, Transition Models (TiM), which adapt to\narbitrary-step transitions, seamlessly traversing the generative trajectory\nfrom single leaps to fine-grained refinement with more steps. Despite having\nonly 865M parameters, TiM achieves state-of-the-art performance, surpassing\nleading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across\nall evaluated step counts. Importantly, unlike previous few-step generators,\nTiM demonstrates monotonic quality improvement as the sampling budget\nincreases. Additionally, when employing our native-resolution strategy, TiM\ndelivers exceptional fidelity at resolutions up to 4096x4096.",
            "upvotes": 15,
            "discussionId": "68ba5465736018af705e8e78",
            "githubRepo": "https://github.com/WZDTHU/TiM",
            "ai_summary": "A novel generative paradigm, Transition Models (TiM), addresses the trade-off between computational cost and output quality in generative modeling by using a continuous-time dynamics equation.",
            "ai_keywords": [
                "iterative diffusion models",
                "continuous-time dynamics equation",
                "Transition Models (TiM)",
                "PF-ODEs",
                "direct endpoint prediction",
                "state-of-the-art performance",
                "SD3.5",
                "FLUX.1",
                "monotonic quality improvement",
                "native-resolution strategy"
            ],
            "githubStars": 30
        },
        "publishedAt": "2025-09-04T13:05:59.000Z",
        "title": "Transition Models: Rethinking the Generative Learning Objective",
        "summary": "A fundamental dilemma in generative modeling persists: iterative diffusion\nmodels achieve outstanding fidelity, but at a significant computational cost,\nwhile efficient few-step alternatives are constrained by a hard quality\nceiling. This conflict between generation steps and output quality arises from\nrestrictive training objectives that focus exclusively on either infinitesimal\ndynamics (PF-ODEs) or direct endpoint prediction. We address this challenge by\nintroducing an exact, continuous-time dynamics equation that analytically\ndefines state transitions across any finite time interval. This leads to a\nnovel generative paradigm, Transition Models (TiM), which adapt to\narbitrary-step transitions, seamlessly traversing the generative trajectory\nfrom single leaps to fine-grained refinement with more steps. Despite having\nonly 865M parameters, TiM achieves state-of-the-art performance, surpassing\nleading models such as SD3.5 (8B parameters) and FLUX.1 (12B parameters) across\nall evaluated step counts. Importantly, unlike previous few-step generators,\nTiM demonstrates monotonic quality improvement as the sampling budget\nincreases. Additionally, when employing our native-resolution strategy, TiM\ndelivers exceptional fidelity at resolutions up to 4096x4096.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04394.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63176933b58b0184630d2c74",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63176933b58b0184630d2c74/53b5EASwW76zeyyqeJA3O.jpeg",
            "fullname": "Yiyuan Zhang",
            "name": "Yiyuan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.20478",
            "authors": [
                {
                    "_id": "68b7f100295f15ff6091152c",
                    "name": "Yuan Xie",
                    "hidden": false
                },
                {
                    "_id": "68b7f100295f15ff6091152d",
                    "user": {
                        "_id": "66ebf632a867b95ddb5b9414",
                        "avatarUrl": "/avatars/07a00952128d7fc031260d23e1923500.svg",
                        "isPro": false,
                        "fullname": "Tianshui Chen",
                        "user": "TianshuiChen",
                        "type": "user"
                    },
                    "name": "Tianshui Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T14:16:34.464Z",
                    "hidden": false
                },
                {
                    "_id": "68b7f100295f15ff6091152e",
                    "name": "Zheng Ge",
                    "hidden": false
                },
                {
                    "_id": "68b7f100295f15ff6091152f",
                    "name": "Lionel Ni",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66ebf632a867b95ddb5b9414/TBvOS323hyr9zmQR_48H8.png"
            ],
            "publishedAt": "2025-08-28T06:55:08.000Z",
            "submittedOnDailyAt": "2025-09-05T09:48:01.129Z",
            "title": "Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding",
            "submittedOnDailyBy": {
                "_id": "66ebf632a867b95ddb5b9414",
                "avatarUrl": "/avatars/07a00952128d7fc031260d23e1923500.svg",
                "isPro": false,
                "fullname": "Tianshui Chen",
                "user": "TianshuiChen",
                "type": "user"
            },
            "summary": "Long-form video understanding, characterized by long-range temporal\ndependencies and multiple events, remains a challenge. Existing methods often\nrely on static reasoning or external visual-language models (VLMs), which face\nissues like complexity and sub-optimal performance due to the lack of\nend-to-end training. In this paper, we propose Video-MTR, a reinforced\nmulti-turn reasoning framework designed to enable iterative key video segment\nselection and question comprehension. Unlike traditional video reasoning\npipeline, which generate predictions in a single turn, Video-MTR performs\nreasoning in multiple turns, selecting video segments progressively based on\nthe evolving understanding of previously processed segments and the current\nquestion. This iterative process allows for a more refined and contextually\naware analysis of the video. To ensure intermediate reasoning process, we\nintroduce a novel gated bi-level reward system, combining trajectory-level\nrewards based on answer correctness and turn-level rewards emphasizing\nframe-query relevance. This system optimizes both video segment selection and\nquestion comprehension, eliminating the need for external VLMs and allowing\nend-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,\nand EgoSchema demonstrate that Video-MTR outperforms existing methods in both\naccuracy and efficiency, advancing the state-of-the-art in long video\nunderstanding.",
            "upvotes": 13,
            "discussionId": "68b7f100295f15ff60911530",
            "ai_summary": "Video-MTR, a reinforced multi-turn reasoning framework, improves long-form video understanding by iteratively selecting key segments and comprehending questions, outperforming existing methods in accuracy and efficiency.",
            "ai_keywords": [
                "multi-turn reasoning",
                "key video segment selection",
                "question comprehension",
                "gated bi-level reward system",
                "trajectory-level rewards",
                "turn-level rewards",
                "frame-query relevance",
                "end-to-end training",
                "VideoMME",
                "MLVU",
                "EgoSchema"
            ]
        },
        "publishedAt": "2025-08-28T02:55:08.000Z",
        "title": "Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding",
        "summary": "Long-form video understanding, characterized by long-range temporal\ndependencies and multiple events, remains a challenge. Existing methods often\nrely on static reasoning or external visual-language models (VLMs), which face\nissues like complexity and sub-optimal performance due to the lack of\nend-to-end training. In this paper, we propose Video-MTR, a reinforced\nmulti-turn reasoning framework designed to enable iterative key video segment\nselection and question comprehension. Unlike traditional video reasoning\npipeline, which generate predictions in a single turn, Video-MTR performs\nreasoning in multiple turns, selecting video segments progressively based on\nthe evolving understanding of previously processed segments and the current\nquestion. This iterative process allows for a more refined and contextually\naware analysis of the video. To ensure intermediate reasoning process, we\nintroduce a novel gated bi-level reward system, combining trajectory-level\nrewards based on answer correctness and turn-level rewards emphasizing\nframe-query relevance. This system optimizes both video segment selection and\nquestion comprehension, eliminating the need for external VLMs and allowing\nend-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,\nand EgoSchema demonstrate that Video-MTR outperforms existing methods in both\naccuracy and efficiency, advancing the state-of-the-art in long video\nunderstanding.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66ebf632a867b95ddb5b9414/TBvOS323hyr9zmQR_48H8.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.20478.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66ebf632a867b95ddb5b9414",
            "avatarUrl": "/avatars/07a00952128d7fc031260d23e1923500.svg",
            "fullname": "Tianshui Chen",
            "name": "TianshuiChen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.04011",
            "authors": [
                {
                    "_id": "68ba4f71736018af705e8e5c",
                    "name": "Or Shachar",
                    "hidden": false
                },
                {
                    "_id": "68ba4f71736018af705e8e5d",
                    "name": "Uri Katz",
                    "hidden": false
                },
                {
                    "_id": "68ba4f71736018af705e8e5e",
                    "name": "Yoav Goldberg",
                    "hidden": false
                },
                {
                    "_id": "68ba4f71736018af705e8e5f",
                    "name": "Oren Glickman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T08:42:23.000Z",
            "submittedOnDailyAt": "2025-09-05T03:44:27.859Z",
            "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings",
            "submittedOnDailyBy": {
                "_id": "604e1c120fe8ff3ec13d71e8",
                "avatarUrl": "/avatars/22f6463216904fb0ec8306e704432ab7.svg",
                "isPro": false,
                "fullname": "Uri Katz",
                "user": "Uri-ka",
                "type": "user"
            },
            "summary": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever",
            "upvotes": 10,
            "discussionId": "68ba4f71736018af705e8e60",
            "githubRepo": "https://github.com/ShacharOr100/ner_retriever",
            "ai_summary": "NER Retriever uses internal representations from large language models to perform zero-shot named entity retrieval by embedding entity mentions and type descriptions into a shared semantic space, outperforming lexical and dense sentence-level retrieval methods.",
            "ai_keywords": [
                "NER Retriever",
                "zero-shot retrieval",
                "Named Entity Retrieval",
                "Named Entity Recognition",
                "large language models",
                "internal representations",
                "value vectors",
                "mid-layer transformer blocks",
                "contrastive projection network",
                "nearest-neighbor search"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-04T04:42:23.000Z",
        "title": "NER Retriever: Zero-Shot Named Entity Retrieval with Type-Aware\n  Embeddings",
        "summary": "We present NER Retriever, a zero-shot retrieval framework for ad-hoc Named\nEntity Retrieval, a variant of Named Entity Recognition (NER), where the types\nof interest are not provided in advance, and a user-defined type description is\nused to retrieve documents mentioning entities of that type. Instead of relying\non fixed schemas or fine-tuned models, our method builds on internal\nrepresentations of large language models (LLMs) to embed both entity mentions\nand user-provided open-ended type descriptions into a shared semantic space. We\nshow that internal representations, specifically the value vectors from\nmid-layer transformer blocks, encode fine-grained type information more\neffectively than commonly used top-layer embeddings. To refine these\nrepresentations, we train a lightweight contrastive projection network that\naligns type-compatible entities while separating unrelated types. The resulting\nentity embeddings are compact, type-aware, and well-suited for nearest-neighbor\nsearch. Evaluated on three benchmarks, NER Retriever significantly outperforms\nboth lexical and dense sentence-level retrieval baselines. Our findings provide\nempirical support for representation selection within LLMs and demonstrate a\npractical solution for scalable, schema-free entity retrieval. The NER\nRetriever Codebase is publicly available at\nhttps://github.com/ShacharOr100/ner_retriever",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04011.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "604e1c120fe8ff3ec13d71e8",
            "avatarUrl": "/avatars/22f6463216904fb0ec8306e704432ab7.svg",
            "fullname": "Uri Katz",
            "name": "Uri-ka",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.04406",
            "authors": [
                {
                    "_id": "68ba68de736018af705e8ecb",
                    "name": "Zanwei Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ba68de736018af705e8ecc",
                    "user": {
                        "_id": "64e5ee4b93d04e3439f4e988",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5ee4b93d04e3439f4e988/g-THJvunoSPUyQ2v_fChU.jpeg",
                        "isPro": false,
                        "fullname": "taoranyi",
                        "user": "thewhole",
                        "type": "user"
                    },
                    "name": "Taoran Yi",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T08:01:29.900Z",
                    "hidden": false
                },
                {
                    "_id": "68ba68de736018af705e8ecd",
                    "name": "Jiemin Fang",
                    "hidden": false
                },
                {
                    "_id": "68ba68de736018af705e8ece",
                    "name": "Chen Yang",
                    "hidden": false
                },
                {
                    "_id": "68ba68de736018af705e8ecf",
                    "name": "Lingxi Xie",
                    "hidden": false
                },
                {
                    "_id": "68ba68de736018af705e8ed0",
                    "name": "Xinggang Wang",
                    "hidden": false
                },
                {
                    "_id": "68ba68de736018af705e8ed1",
                    "name": "Wei Shen",
                    "hidden": false
                },
                {
                    "_id": "68ba68de736018af705e8ed2",
                    "name": "Qi Tian",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T17:24:31.000Z",
            "submittedOnDailyAt": "2025-09-05T03:09:11.246Z",
            "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
            "submittedOnDailyBy": {
                "_id": "64e5ee4b93d04e3439f4e988",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5ee4b93d04e3439f4e988/g-THJvunoSPUyQ2v_fChU.jpeg",
                "isPro": false,
                "fullname": "taoranyi",
                "user": "thewhole",
                "type": "user"
            },
            "summary": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.",
            "upvotes": 8,
            "discussionId": "68ba68de736018af705e8ed3",
            "ai_summary": "A novel framework, MDT-dist, accelerates 3D flow generation by distilling pretrained models to learn Marginal-Data Transport through Velocity Matching and Velocity Distillation, reducing sampling steps and improving speed and fidelity.",
            "ai_keywords": [
                "flow-based 3D generation models",
                "few-step distillation",
                "Consistency Models",
                "MDT-dist",
                "Marginal-Data Transport",
                "Velocity Matching",
                "Velocity Distillation",
                "TRELLIS",
                "sampling steps",
                "flow transformer",
                "latency",
                "speedup",
                "visual fidelity",
                "geometric fidelity"
            ]
        },
        "publishedAt": "2025-09-04T13:24:31.000Z",
        "title": "Few-step Flow for 3D Generation via Marginal-Data Transport Distillation",
        "summary": "Flow-based 3D generation models typically require dozens of sampling steps\nduring inference. Though few-step distillation methods, particularly\nConsistency Models (CMs), have achieved substantial advancements in\naccelerating 2D diffusion models, they remain under-explored for more complex\n3D generation tasks. In this study, we propose a novel framework, MDT-dist, for\nfew-step 3D flow distillation. Our approach is built upon a primary objective:\ndistilling the pretrained model to learn the Marginal-Data Transport. Directly\nlearning this objective needs to integrate the velocity fields, while this\nintegral is intractable to be implemented. Therefore, we propose two\noptimizable objectives, Velocity Matching (VM) and Velocity Distillation (VD),\nto equivalently convert the optimization target from the transport level to the\nvelocity and the distribution level respectively. Velocity Matching (VM) learns\nto stably match the velocity fields between the student and the teacher, but\ninevitably provides biased gradient estimates. Velocity Distillation (VD)\nfurther enhances the optimization process by leveraging the learned velocity\nfields to perform probability density distillation. When evaluated on the\npioneer 3D generation framework TRELLIS, our method reduces sampling steps of\neach flow transformer from 25 to 1 or 2, achieving 0.68s (1 step x 2) and 0.94s\n(2 steps x 2) latency with 9.0x and 6.5x speedup on A800, while preserving high\nvisual and geometric fidelity. Extensive experiments demonstrate that our\nmethod significantly outperforms existing CM distillation methods, and enables\nTRELLIS to achieve superior performance in few-step 3D generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04406.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "64e5ee4b93d04e3439f4e988",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e5ee4b93d04e3439f4e988/g-THJvunoSPUyQ2v_fChU.jpeg",
            "fullname": "taoranyi",
            "name": "thewhole",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.03059",
            "authors": [
                {
                    "_id": "68b913eed43cadaf7a688b6c",
                    "user": {
                        "_id": "62cb06206193ba3ced7afa05",
                        "avatarUrl": "/avatars/3265c0523642b2463b7c6c7a4181681c.svg",
                        "isPro": false,
                        "fullname": "Xingyue Huang",
                        "user": "hxyscott",
                        "type": "user"
                    },
                    "name": "Xingyue Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T08:43:53.847Z",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b6d",
                    "name": "Rishabh",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b6e",
                    "name": "Gregor Franke",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b6f",
                    "name": "Ziyi Yang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b70",
                    "name": "Jiamu Bai",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b71",
                    "name": "Weijie Bai",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b72",
                    "name": "Jinhe Bi",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b73",
                    "name": "Zifeng Ding",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b74",
                    "name": "Yiqun Duan",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b75",
                    "name": "Chengyu Fan",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b76",
                    "name": "Wendong Fan",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b77",
                    "name": "Xin Gao",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b78",
                    "name": "Ruohao Guo",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b79",
                    "name": "Yuan He",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b7a",
                    "name": "Zhuangzhuang He",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b7b",
                    "name": "Xianglong Hu",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b7c",
                    "name": "Neil Johnson",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b7d",
                    "name": "Bowen Li",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b7e",
                    "name": "Fangru Lin",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b7f",
                    "name": "Siyu Lin",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b80",
                    "name": "Tong Liu",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b81",
                    "name": "Yunpu Ma",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b82",
                    "name": "Hao Shen",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b83",
                    "name": "Hao Sun",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b84",
                    "name": "Beibei Wang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b85",
                    "name": "Fangyijie Wang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b86",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b87",
                    "name": "Haoran Wang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b88",
                    "name": "Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b89",
                    "name": "Yifeng Wang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b8a",
                    "name": "Zhaowei Wang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b8b",
                    "name": "Ziyang Wang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b8c",
                    "name": "Yifan Wu",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b8d",
                    "name": "Zikai Xiao",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b8e",
                    "name": "Chengxing Xie",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b8f",
                    "name": "Fan Yang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b90",
                    "user": {
                        "_id": "65d859a3661492b25c46a117",
                        "avatarUrl": "/avatars/06a547a00b472f512a25eef4ddd047bf.svg",
                        "isPro": false,
                        "fullname": "Junxiao Yang",
                        "user": "yangjunxiao2021",
                        "type": "user"
                    },
                    "name": "Junxiao Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T12:51:29.776Z",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b91",
                    "name": "Qianshuo Ye",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b92",
                    "name": "Ziyu Ye",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b93",
                    "name": "Guangtao Zeng",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b94",
                    "name": "Yuwen Ebony Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b95",
                    "name": "Zeyu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b96",
                    "user": {
                        "_id": "64030d9956038547951c7d55",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64030d9956038547951c7d55/gRCzhy1yd7cOV6GN-KGHJ.png",
                        "isPro": false,
                        "fullname": "Zihao Zhu",
                        "user": "ZihaoZhu",
                        "type": "user"
                    },
                    "name": "Zihao Zhu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T12:51:31.659Z",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b97",
                    "name": "Bernard Ghanem",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b98",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "68b913eed43cadaf7a688b99",
                    "name": "Guohao Li",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-03T06:42:40.000Z",
            "submittedOnDailyAt": "2025-09-05T19:36:35.301Z",
            "title": "Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers",
            "submittedOnDailyBy": {
                "_id": "62cb06206193ba3ced7afa05",
                "avatarUrl": "/avatars/3265c0523642b2463b7c6c7a4181681c.svg",
                "isPro": false,
                "fullname": "Xingyue Huang",
                "user": "hxyscott",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) have shown that their\nreasoning capabilities can be significantly improved through Reinforcement\nLearning with Verifiable Reward (RLVR), particularly in domains like\nmathematics and programming, where ground-truth correctness can be\nautomatically evaluated. However, extending this success to other\nreasoning-intensive domains remains challenging due to the scarcity of\nhigh-quality, verifiable datasets and the high cost of human supervision. In\nthis work, we introduce the Loong Project: an open-source framework for\nscalable synthetic data generation and verification across a diverse range of\nreasoning-intensive domains. The framework consists of two key components: (1)\nLoongBench, a curated seed dataset containing 8,729 human-vetted examples\nacross 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired\nwith executable code and rich metadata; and (2) LoongEnv, a modular synthetic\ndata generation environment that supports multiple prompting strategies to\nproduce new question-answer-code triples. Together, these components form an\nagent-environment loop that enables reinforcement learning, where an LLM-based\nagent is rewarded for generating Chain-of-Thought (CoT) solutions that align\nwith code-executed answers. Empirically, we benchmark LoongBench on a broad\nsuite of both open-source and proprietary LLMs to evaluate domain coverage and\nreveal performance bottlenecks. In addition, we conduct a comprehensive\nanalysis of synthetic data generated by LoongEnv, examining correctness,\ndifficulty, and diversity. Code and documentation are available at\nhttps://github.com/camel-ai/loong.",
            "upvotes": 4,
            "discussionId": "68b913eed43cadaf7a688b9a",
            "ai_summary": "The Loong Project introduces a framework for generating and verifying synthetic data to improve reasoning capabilities in Large Language Models through Reinforcement Learning with Verifiable Reward.",
            "ai_keywords": [
                "Large Language Models",
                "Reinforcement Learning with Verifiable Reward",
                "synthetic data generation",
                "verification",
                "LoongBench",
                "LoongEnv",
                "Chain-of-Thought",
                "domain coverage",
                "performance bottlenecks",
                "correctness",
                "difficulty",
                "diversity"
            ]
        },
        "publishedAt": "2025-09-03T02:42:40.000Z",
        "title": "Loong: Synthesize Long Chain-of-Thoughts at Scale through Verifiers",
        "summary": "Recent advances in Large Language Models (LLMs) have shown that their\nreasoning capabilities can be significantly improved through Reinforcement\nLearning with Verifiable Reward (RLVR), particularly in domains like\nmathematics and programming, where ground-truth correctness can be\nautomatically evaluated. However, extending this success to other\nreasoning-intensive domains remains challenging due to the scarcity of\nhigh-quality, verifiable datasets and the high cost of human supervision. In\nthis work, we introduce the Loong Project: an open-source framework for\nscalable synthetic data generation and verification across a diverse range of\nreasoning-intensive domains. The framework consists of two key components: (1)\nLoongBench, a curated seed dataset containing 8,729 human-vetted examples\nacross 12 domains (e.g., Advanced Mathematics, Chemistry, Logic), each paired\nwith executable code and rich metadata; and (2) LoongEnv, a modular synthetic\ndata generation environment that supports multiple prompting strategies to\nproduce new question-answer-code triples. Together, these components form an\nagent-environment loop that enables reinforcement learning, where an LLM-based\nagent is rewarded for generating Chain-of-Thought (CoT) solutions that align\nwith code-executed answers. Empirically, we benchmark LoongBench on a broad\nsuite of both open-source and proprietary LLMs to evaluate domain coverage and\nreveal performance bottlenecks. In addition, we conduct a comprehensive\nanalysis of synthetic data generated by LoongEnv, examining correctness,\ndifficulty, and diversity. Code and documentation are available at\nhttps://github.com/camel-ai/loong.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03059.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62cb06206193ba3ced7afa05",
            "avatarUrl": "/avatars/3265c0523642b2463b7c6c7a4181681c.svg",
            "fullname": "Xingyue Huang",
            "name": "hxyscott",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.04434",
            "authors": [
                {
                    "_id": "68ba5959736018af705e8e93",
                    "user": {
                        "_id": "62414e2b585605a4079c2f38",
                        "avatarUrl": "/avatars/db1dc5dd2164b7ecbd789104329296bd.svg",
                        "isPro": false,
                        "fullname": "Hyunsoo Cha",
                        "user": "HyunsooCha",
                        "type": "user"
                    },
                    "name": "Hyunsoo Cha",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:34:48.828Z",
                    "hidden": false
                },
                {
                    "_id": "68ba5959736018af705e8e94",
                    "name": "Byungjun Kim",
                    "hidden": false
                },
                {
                    "_id": "68ba5959736018af705e8e95",
                    "user": {
                        "_id": "65b0ebe1165e69e0e6dc6ddd",
                        "avatarUrl": "/avatars/d2e95b2040c761200df1293db2949da7.svg",
                        "isPro": false,
                        "fullname": "Hanbyul Joo",
                        "user": "jhugestar",
                        "type": "user"
                    },
                    "name": "Hanbyul Joo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-05T14:34:59.097Z",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62414e2b585605a4079c2f38/iNxn7368E7JViGz3UhDOo.mp4"
            ],
            "publishedAt": "2025-09-04T17:53:03.000Z",
            "submittedOnDailyAt": "2025-09-05T02:02:16.975Z",
            "title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer",
            "submittedOnDailyBy": {
                "_id": "62414e2b585605a4079c2f38",
                "avatarUrl": "/avatars/db1dc5dd2164b7ecbd789104329296bd.svg",
                "isPro": false,
                "fullname": "Hyunsoo Cha",
                "user": "HyunsooCha",
                "type": "user"
            },
            "summary": "We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.",
            "upvotes": 3,
            "discussionId": "68ba595a736018af705e8e96",
            "projectPage": "https://hyunsoocha.github.io/durian/",
            "githubRepo": "https://github.com/snuvclab/durian",
            "ai_summary": "Durian uses dual reference networks and a diffusion model to generate high-fidelity portrait animations with attribute transfer from a reference image to a target portrait in a zero-shot manner.",
            "ai_keywords": [
                "dual reference networks",
                "denoising process",
                "diffusion model",
                "self-reconstruction formulation",
                "mask expansion strategy",
                "keypoint-conditioned image generation",
                "spatial and appearance-level transformations",
                "multi-attribute composition"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-09-04T13:53:03.000Z",
        "title": "Durian: Dual Reference-guided Portrait Animation with Attribute Transfer",
        "summary": "We present Durian, the first method for generating portrait animation videos\nwith facial attribute transfer from a given reference image to a target\nportrait in a zero-shot manner. To enable high-fidelity and spatially\nconsistent attribute transfer across frames, we introduce dual reference\nnetworks that inject spatial features from both the portrait and attribute\nimages into the denoising process of a diffusion model. We train the model\nusing a self-reconstruction formulation, where two frames are sampled from the\nsame portrait video: one is treated as the attribute reference and the other as\nthe target portrait, and the remaining frames are reconstructed conditioned on\nthese inputs and their corresponding masks. To support the transfer of\nattributes with varying spatial extent, we propose a mask expansion strategy\nusing keypoint-conditioned image generation for training. In addition, we\nfurther augment the attribute and portrait images with spatial and\nappearance-level transformations to improve robustness to positional\nmisalignment between them. These strategies allow the model to effectively\ngeneralize across diverse attributes and in-the-wild reference combinations,\ndespite being trained without explicit triplet supervision. Durian achieves\nstate-of-the-art performance on portrait animation with attribute transfer, and\nnotably, its dual reference design enables multi-attribute composition in a\nsingle generation pass without additional training.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62414e2b585605a4079c2f38/iNxn7368E7JViGz3UhDOo.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04434.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "62414e2b585605a4079c2f38",
            "avatarUrl": "/avatars/db1dc5dd2164b7ecbd789104329296bd.svg",
            "fullname": "Hyunsoo Cha",
            "name": "HyunsooCha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2508.18733",
            "authors": [
                {
                    "_id": "68b993e5736018af705e8d50",
                    "name": "Feiwei Qin",
                    "hidden": false
                },
                {
                    "_id": "68b993e5736018af705e8d51",
                    "name": "Shichao Lu",
                    "hidden": false
                },
                {
                    "_id": "68b993e5736018af705e8d52",
                    "user": {
                        "_id": "64258e73120a3ed32333319e",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64258e73120a3ed32333319e/f1_eZ6slEoVq8Kn4WbQDC.png",
                        "isPro": false,
                        "fullname": "Junhao Hou",
                        "user": "1nnoh",
                        "type": "user"
                    },
                    "name": "Junhao Hou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T19:34:40.271Z",
                    "hidden": false
                },
                {
                    "_id": "68b993e5736018af705e8d53",
                    "name": "Changmiao Wang",
                    "hidden": false
                },
                {
                    "_id": "68b993e5736018af705e8d54",
                    "name": "Meie Fang",
                    "hidden": false
                },
                {
                    "_id": "68b993e5736018af705e8d55",
                    "name": "Ligang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-26T07:01:58.000Z",
            "submittedOnDailyAt": "2025-09-05T00:39:47.038Z",
            "title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from\n  Vector Drawings",
            "submittedOnDailyBy": {
                "_id": "64258e73120a3ed32333319e",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64258e73120a3ed32333319e/f1_eZ6slEoVq8Kn4WbQDC.png",
                "isPro": false,
                "fullname": "Junhao Hou",
                "user": "1nnoh",
                "type": "user"
            },
            "summary": "Computer-Aided Design (CAD) generative modeling is driving significant\ninnovations across industrial applications. Recent works have shown remarkable\nprogress in creating solid models from various inputs such as point clouds,\nmeshes, and text descriptions. However, these methods fundamentally diverge\nfrom traditional industrial workflows that begin with 2D engineering drawings.\nThe automatic generation of parametric CAD models from these 2D vector drawings\nremains underexplored despite being a critical step in engineering design. To\naddress this gap, our key insight is to reframe CAD generation as a\nsequence-to-sequence learning problem where vector drawing primitives directly\ninform the generation of parametric CAD operations, preserving geometric\nprecision and design intent throughout the transformation process. We propose\nDrawing2CAD, a framework with three key technical components: a\nnetwork-friendly vector primitive representation that preserves precise\ngeometric information, a dual-decoder transformer architecture that decouples\ncommand type and parameter generation while maintaining precise correspondence,\nand a soft target distribution loss function accommodating inherent flexibility\nin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,\na dataset of paired engineering drawings and parametric CAD models, and conduct\nthorough experiments to demonstrate the effectiveness of our method. Code and\ndataset are available at https://github.com/lllssc/Drawing2CAD.",
            "upvotes": 2,
            "discussionId": "68b993e5736018af705e8d56",
            "githubRepo": "https://github.com/lllssc/Drawing2CAD",
            "ai_summary": "Drawing2CAD is a framework that converts 2D vector drawings into parametric CAD models using a sequence-to-sequence learning approach with a dual-decoder transformer architecture and a soft target distribution loss function.",
            "ai_keywords": [
                "sequence-to-sequence learning",
                "vector primitive representation",
                "dual-decoder transformer architecture",
                "soft target distribution loss function",
                "parametric CAD models"
            ],
            "githubStars": 50
        },
        "publishedAt": "2025-08-26T03:01:58.000Z",
        "title": "Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from\n  Vector Drawings",
        "summary": "Computer-Aided Design (CAD) generative modeling is driving significant\ninnovations across industrial applications. Recent works have shown remarkable\nprogress in creating solid models from various inputs such as point clouds,\nmeshes, and text descriptions. However, these methods fundamentally diverge\nfrom traditional industrial workflows that begin with 2D engineering drawings.\nThe automatic generation of parametric CAD models from these 2D vector drawings\nremains underexplored despite being a critical step in engineering design. To\naddress this gap, our key insight is to reframe CAD generation as a\nsequence-to-sequence learning problem where vector drawing primitives directly\ninform the generation of parametric CAD operations, preserving geometric\nprecision and design intent throughout the transformation process. We propose\nDrawing2CAD, a framework with three key technical components: a\nnetwork-friendly vector primitive representation that preserves precise\ngeometric information, a dual-decoder transformer architecture that decouples\ncommand type and parameter generation while maintaining precise correspondence,\nand a soft target distribution loss function accommodating inherent flexibility\nin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,\na dataset of paired engineering drawings and parametric CAD models, and conduct\nthorough experiments to demonstrate the effectiveness of our method. Code and\ndataset are available at https://github.com/lllssc/Drawing2CAD.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2508.18733.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64258e73120a3ed32333319e",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64258e73120a3ed32333319e/f1_eZ6slEoVq8Kn4WbQDC.png",
            "fullname": "Junhao Hou",
            "name": "1nnoh",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.04442",
            "authors": [
                {
                    "_id": "68ba6fd7736018af705e8ed5",
                    "user": {
                        "_id": "648f276cdb6279384d1b3231",
                        "avatarUrl": "/avatars/9137b367cf2da707cf78363e9f3af73b.svg",
                        "isPro": false,
                        "fullname": "Zhiqiu (Oscar) Xu",
                        "user": "OscarXZQ",
                        "type": "user"
                    },
                    "name": "Zhiqiu Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-05T07:58:07.308Z",
                    "hidden": false
                },
                {
                    "_id": "68ba6fd7736018af705e8ed6",
                    "name": "Amish Sethi",
                    "hidden": false
                },
                {
                    "_id": "68ba6fd7736018af705e8ed7",
                    "name": "Mayur Naik",
                    "hidden": false
                },
                {
                    "_id": "68ba6fd7736018af705e8ed8",
                    "name": "Ser-Nam Lim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T17:59:06.000Z",
            "submittedOnDailyAt": "2025-09-05T06:41:51.744Z",
            "title": "Delta Activations: A Representation for Finetuned Large Language Models",
            "submittedOnDailyBy": {
                "_id": "648f276cdb6279384d1b3231",
                "avatarUrl": "/avatars/9137b367cf2da707cf78363e9f3af73b.svg",
                "isPro": false,
                "fullname": "Zhiqiu (Oscar) Xu",
                "user": "OscarXZQ",
                "type": "user"
            },
            "summary": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.",
            "upvotes": 1,
            "discussionId": "68ba6fd7736018af705e8ed9",
            "projectPage": "https://oscarxzq.github.io/delta_activation/",
            "githubRepo": "https://github.com/OscarXZQ/delta_activations",
            "ai_summary": "Delta Activations represent fine-tuned models as vector embeddings based on internal activation shifts, enabling effective clustering and model reuse.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "Delta Activations",
                "vector embeddings",
                "internal activations",
                "base model",
                "clustering",
                "domain",
                "task",
                "robust",
                "additive property",
                "few-shot finetuning",
                "model selection",
                "model merging"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-09-04T13:59:06.000Z",
        "title": "Delta Activations: A Representation for Finetuned Large Language Models",
        "summary": "The success of powerful open source Large Language Models (LLMs) has enabled\nthe community to create a vast collection of post-trained models adapted to\nspecific tasks and domains. However, navigating and understanding these models\nremains challenging due to inconsistent metadata and unstructured repositories.\nWe introduce Delta Activations, a method to represent finetuned models as\nvector embeddings by measuring shifts in their internal activations relative to\na base model. This representation allows for effective clustering by domain and\ntask, revealing structure in the model landscape. Delta Activations also\ndemonstrate desirable properties: it is robust across finetuning settings and\nexhibits an additive property when finetuning datasets are mixed. In addition,\nwe show that Delta Activations can embed tasks via few-shot finetuning, and\nfurther explore its use for model selection and merging. We hope Delta\nActivations can facilitate the practice of reusing publicly available models.\nCode is available at https://github.com/OscarXZQ/delta_activations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04442.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "648f276cdb6279384d1b3231",
            "avatarUrl": "/avatars/9137b367cf2da707cf78363e9f3af73b.svg",
            "fullname": "Zhiqiu (Oscar) Xu",
            "name": "OscarXZQ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.03888",
            "authors": [
                {
                    "_id": "68ba4063736018af705e8e2e",
                    "name": "Cheng Wang",
                    "hidden": false
                },
                {
                    "_id": "68ba4063736018af705e8e2f",
                    "name": "Zeming Wei",
                    "hidden": false
                },
                {
                    "_id": "68ba4063736018af705e8e30",
                    "name": "Qin Liu",
                    "hidden": false
                },
                {
                    "_id": "68ba4063736018af705e8e31",
                    "name": "Muhao Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T05:15:55.000Z",
            "submittedOnDailyAt": "2025-09-05T00:43:07.219Z",
            "title": "False Sense of Security: Why Probing-based Malicious Input Detection\n  Fails to Generalize",
            "submittedOnDailyBy": {
                "_id": "6329c57e6813868fa49efeaa",
                "avatarUrl": "/avatars/59f46607d3bd33d98a35679e99a00b32.svg",
                "isPro": false,
                "fullname": "Zeming Wei",
                "user": "ZemingWei",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising\nserious safety concerns despite their impressive capabilities. Recent work has\nleveraged probing-based approaches to study the separability of malicious and\nbenign inputs in LLMs' internal representations, and researchers have proposed\nusing such probing methods for safety detection. We systematically re-examine\nthis paradigm. Motivated by poor out-of-distribution performance, we\nhypothesize that probes learn superficial patterns rather than semantic\nharmfulness. Through controlled experiments, we confirm this hypothesis and\nidentify the specific patterns learned: instructional patterns and trigger\nwords. Our investigation follows a systematic approach, progressing from\ndemonstrating comparable performance of simple n-gram methods, to controlled\nexperiments with semantically cleaned datasets, to detailed analysis of pattern\ndependencies. These results reveal a false sense of security around current\nprobing-based approaches and highlight the need to redesign both models and\nevaluation protocols, for which we provide further discussions in the hope of\nsuggesting responsible further research in this direction. We have open-sourced\nthe project at https://github.com/WangCheng0116/Why-Probe-Fails.",
            "upvotes": 0,
            "discussionId": "68ba4063736018af705e8e32",
            "ai_summary": "Probing-based approaches for detecting harmful instructions in LLMs are found to rely on superficial patterns rather than semantic understanding, indicating a need for redesigning models and evaluation methods.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "probing-based approaches",
                "internal representations",
                "safety detection",
                "out-of-distribution performance",
                "instructional patterns",
                "trigger words",
                "n-gram methods",
                "semantically cleaned datasets",
                "pattern dependencies"
            ]
        },
        "publishedAt": "2025-09-04T01:15:55.000Z",
        "title": "False Sense of Security: Why Probing-based Malicious Input Detection\n  Fails to Generalize",
        "summary": "Large Language Models (LLMs) can comply with harmful instructions, raising\nserious safety concerns despite their impressive capabilities. Recent work has\nleveraged probing-based approaches to study the separability of malicious and\nbenign inputs in LLMs' internal representations, and researchers have proposed\nusing such probing methods for safety detection. We systematically re-examine\nthis paradigm. Motivated by poor out-of-distribution performance, we\nhypothesize that probes learn superficial patterns rather than semantic\nharmfulness. Through controlled experiments, we confirm this hypothesis and\nidentify the specific patterns learned: instructional patterns and trigger\nwords. Our investigation follows a systematic approach, progressing from\ndemonstrating comparable performance of simple n-gram methods, to controlled\nexperiments with semantically cleaned datasets, to detailed analysis of pattern\ndependencies. These results reveal a false sense of security around current\nprobing-based approaches and highlight the need to redesign both models and\nevaluation protocols, for which we provide further discussions in the hope of\nsuggesting responsible further research in this direction. We have open-sourced\nthe project at https://github.com/WangCheng0116/Why-Probe-Fails.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03888.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6329c57e6813868fa49efeaa",
            "avatarUrl": "/avatars/59f46607d3bd33d98a35679e99a00b32.svg",
            "fullname": "Zeming Wei",
            "name": "ZemingWei",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    }
]