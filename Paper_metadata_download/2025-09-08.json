[
    {
        "paper": {
            "id": "2509.04664",
            "authors": [
                {
                    "_id": "68be5810c123124955ef60ac",
                    "name": "Adam Tauman Kalai",
                    "hidden": false
                },
                {
                    "_id": "68be5810c123124955ef60ad",
                    "name": "Ofir Nachum",
                    "hidden": false
                },
                {
                    "_id": "68be5810c123124955ef60ae",
                    "name": "Santosh S. Vempala",
                    "hidden": false
                },
                {
                    "_id": "68be5810c123124955ef60af",
                    "name": "Edwin Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T21:26:31.000Z",
            "submittedOnDailyAt": "2025-09-08T02:44:20.973Z",
            "title": "Why Language Models Hallucinate",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Like students facing hard exam questions, large language models sometimes\nguess when uncertain, producing plausible yet incorrect statements instead of\nadmitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art\nsystems and undermine trust. We argue that language models hallucinate because\nthe training and evaluation procedures reward guessing over acknowledging\nuncertainty, and we analyze the statistical causes of hallucinations in the\nmodern training pipeline. Hallucinations need not be mysterious -- they\noriginate simply as errors in binary classification. If incorrect statements\ncannot be distinguished from facts, then hallucinations in pretrained language\nmodels will arise through natural statistical pressures. We then argue that\nhallucinations persist due to the way most evaluations are graded -- language\nmodels are optimized to be good test-takers, and guessing when uncertain\nimproves test performance. This \"epidemic\" of penalizing uncertain responses\ncan only be addressed through a socio-technical mitigation: modifying the\nscoring of existing benchmarks that are misaligned but dominate leaderboards,\nrather than introducing additional hallucination evaluations. This change may\nsteer the field toward more trustworthy AI systems.",
            "upvotes": 80,
            "discussionId": "68be5810c123124955ef60b0",
            "ai_summary": "Language models produce incorrect statements due to training and evaluation procedures that reward guessing over acknowledging uncertainty, leading to a need for socio-technical changes in benchmark scoring.",
            "ai_keywords": [
                "hallucinations",
                "binary classification",
                "uncertain responses",
                "trustworthy AI systems"
            ]
        },
        "publishedAt": "2025-09-04T17:26:31.000Z",
        "title": "Why Language Models Hallucinate",
        "summary": "Like students facing hard exam questions, large language models sometimes\nguess when uncertain, producing plausible yet incorrect statements instead of\nadmitting uncertainty. Such \"hallucinations\" persist even in state-of-the-art\nsystems and undermine trust. We argue that language models hallucinate because\nthe training and evaluation procedures reward guessing over acknowledging\nuncertainty, and we analyze the statistical causes of hallucinations in the\nmodern training pipeline. Hallucinations need not be mysterious -- they\noriginate simply as errors in binary classification. If incorrect statements\ncannot be distinguished from facts, then hallucinations in pretrained language\nmodels will arise through natural statistical pressures. We then argue that\nhallucinations persist due to the way most evaluations are graded -- language\nmodels are optimized to be good test-takers, and guessing when uncertain\nimproves test performance. This \"epidemic\" of penalizing uncertain responses\ncan only be addressed through a socio-technical mitigation: modifying the\nscoring of existing benchmarks that are misaligned but dominate leaderboards,\nrather than introducing additional hallucination evaluations. This change may\nsteer the field toward more trustworthy AI systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04664.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 98
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.05208",
            "authors": [
                {
                    "_id": "68be49a7c123124955ef5fd0",
                    "name": "Yamei Chen",
                    "hidden": false
                },
                {
                    "_id": "68be49a7c123124955ef5fd1",
                    "name": "Haoquan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68be49a7c123124955ef5fd2",
                    "user": {
                        "_id": "630461923926de1f7ec7a93a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GoUU2qRfLB2wlcciwVBrz.png",
                        "isPro": false,
                        "fullname": "Yangyi, Huang",
                        "user": "YangyiH",
                        "type": "user"
                    },
                    "name": "Yangyi Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-08T07:01:53.437Z",
                    "hidden": false
                },
                {
                    "_id": "68be49a7c123124955ef5fd3",
                    "name": "Zeju Qiu",
                    "hidden": false
                },
                {
                    "_id": "68be49a7c123124955ef5fd4",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68be49a7c123124955ef5fd5",
                    "name": "Yandong Wen",
                    "hidden": false
                },
                {
                    "_id": "68be49a7c123124955ef5fd6",
                    "name": "Weiyang Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-05T16:10:53.000Z",
            "submittedOnDailyAt": "2025-09-08T07:41:47.549Z",
            "title": "Symbolic Graphics Programming with Large Language Models",
            "submittedOnDailyBy": {
                "_id": "630461923926de1f7ec7a93a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GoUU2qRfLB2wlcciwVBrz.png",
                "isPro": false,
                "fullname": "Yangyi, Huang",
                "user": "YangyiH",
                "type": "user"
            },
            "summary": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding.",
            "upvotes": 30,
            "discussionId": "68be49a8c123124955ef5fd7",
            "projectPage": "https://spherelab.ai/SGP-Gen/",
            "githubRepo": "https://github.com/Sphere-AI-Lab/SGP-RL",
            "ai_summary": "LLMs generate SVGs from natural-language descriptions using a reinforcement learning approach with verifiable rewards, improving performance and scene coherence.",
            "ai_keywords": [
                "symbolic graphics programs",
                "SGPs",
                "scalable vector graphics",
                "SVGs",
                "SGP-GenBench",
                "reinforcement learning",
                "RL",
                "format-validity gate",
                "cross-modal reward",
                "SigLIP",
                "DINO",
                "Qwen-2.5-7B",
                "cross-modal grounding"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-09-05T12:10:53.000Z",
        "title": "Symbolic Graphics Programming with Large Language Models",
        "summary": "Large language models (LLMs) excel at program synthesis, yet their ability to\nproduce symbolic graphics programs (SGPs) that render into precise visual\ncontent remains underexplored. We study symbolic graphics programming, where\nthe goal is to generate an SGP from a natural-language description. This task\nalso serves as a lens into how LLMs understand the visual world by prompting\nthem to generate images rendered from SGPs. Among various SGPs, our paper\nsticks to scalable vector graphics (SVGs). We begin by examining the extent to\nwhich LLMs can generate SGPs. To this end, we introduce SGP-GenBench, a\ncomprehensive benchmark covering object fidelity, scene fidelity, and\ncompositionality (attribute binding, spatial relations, numeracy). On\nSGP-GenBench, we discover that frontier proprietary models substantially\noutperform open-source models, and performance correlates well with general\ncoding capabilities. Motivated by this gap, we aim to improve LLMs' ability to\ngenerate SGPs. We propose a reinforcement learning (RL) with verifiable rewards\napproach, where a format-validity gate ensures renderable SVG, and a\ncross-modal reward aligns text and the rendered image via strong vision\nencoders (e.g., SigLIP for text-image and DINO for image-image). Applied to\nQwen-2.5-7B, our method substantially improves SVG generation quality and\nsemantics, achieving performance on par with frontier systems. We further\nanalyze training dynamics, showing that RL induces (i) finer decomposition of\nobjects into controllable primitives and (ii) contextual details that improve\nscene coherence. Our results demonstrate that symbolic graphics programming\noffers a precise and interpretable lens on cross-modal grounding.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05208.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "630461923926de1f7ec7a93a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/GoUU2qRfLB2wlcciwVBrz.png",
            "fullname": "Yangyi, Huang",
            "name": "YangyiH",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.04185",
            "authors": [
                {
                    "_id": "68beb7f7c123124955ef61b5",
                    "name": "Itai Gat",
                    "hidden": false
                },
                {
                    "_id": "68beb7f7c123124955ef61b6",
                    "name": "Heli Ben-Hamu",
                    "hidden": false
                },
                {
                    "_id": "68beb7f7c123124955ef61b7",
                    "name": "Marton Havasi",
                    "hidden": false
                },
                {
                    "_id": "68beb7f7c123124955ef61b8",
                    "name": "Daniel Haziza",
                    "hidden": false
                },
                {
                    "_id": "68beb7f7c123124955ef61b9",
                    "name": "Jeremy Reizenstein",
                    "hidden": false
                },
                {
                    "_id": "68beb7f7c123124955ef61ba",
                    "name": "Gabriel Synnaeve",
                    "hidden": false
                },
                {
                    "_id": "68beb7f7c123124955ef61bb",
                    "name": "David Lopez-Paz",
                    "hidden": false
                },
                {
                    "_id": "68beb7f7c123124955ef61bc",
                    "name": "Brian Karrer",
                    "hidden": false
                },
                {
                    "_id": "68beb7f7c123124955ef61bd",
                    "name": "Yaron Lipman",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62a8fa984d933c74bf410c16/VDDCsutVgInVtJlvVV2WJ.png",
                "https://cdn-uploads.huggingface.co/production/uploads/62a8fa984d933c74bf410c16/dGa0ikEOY5Ove2n2-DnKH.qt"
            ],
            "publishedAt": "2025-09-04T13:02:39.000Z",
            "submittedOnDailyAt": "2025-09-08T09:44:37.535Z",
            "title": "Set Block Decoding is a Language Model Inference Accelerator",
            "submittedOnDailyBy": {
                "_id": "62a8fa984d933c74bf410c16",
                "avatarUrl": "/avatars/73519deba3176be9c23d49f749aee5da.svg",
                "isPro": false,
                "fullname": "Itai Gat",
                "user": "itaigat",
                "type": "user"
            },
            "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
            "upvotes": 30,
            "discussionId": "68beb7f7c123124955ef61be",
            "ai_summary": "Set Block Decoding accelerates language model generation by integrating next token prediction and masked token prediction, enabling parallel sampling of future tokens and reducing computational cost without sacrificing accuracy.",
            "ai_keywords": [
                "autoregressive next token prediction",
                "masked token prediction",
                "Set Block Decoding",
                "discrete diffusion",
                "forward passes",
                "Llama-3.1 8B",
                "Qwen-3 8B"
            ]
        },
        "publishedAt": "2025-09-04T09:02:39.000Z",
        "title": "Set Block Decoding is a Language Model Inference Accelerator",
        "summary": "Autoregressive next token prediction language models offer powerful\ncapabilities but face significant challenges in practical deployment due to the\nhigh computational and memory costs of inference, particularly during the\ndecoding stage. We introduce Set Block Decoding (SBD), a simple and flexible\nparadigm that accelerates generation by integrating standard next token\nprediction (NTP) and masked token prediction (MATP) within a single\narchitecture. SBD allows the model to sample multiple, not necessarily\nconsecutive, future tokens in parallel, a key distinction from previous\nacceleration methods. This flexibility allows the use of advanced solvers from\nthe discrete diffusion literature, offering significant speedups without\nsacrificing accuracy. SBD requires no architectural changes or extra training\nhyperparameters, maintains compatibility with exact KV-caching, and can be\nimplemented by fine-tuning existing next token prediction models. By\nfine-tuning Llama-3.1 8B and Qwen-3 8B, we demonstrate that SBD enables a 3-5x\nreduction in the number of forward passes required for generation while\nachieving same performance as equivalent NTP training.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62a8fa984d933c74bf410c16/VDDCsutVgInVtJlvVV2WJ.png",
            "https://cdn-uploads.huggingface.co/production/uploads/62a8fa984d933c74bf410c16/dGa0ikEOY5Ove2n2-DnKH.qt"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04185.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62a8fa984d933c74bf410c16",
            "avatarUrl": "/avatars/73519deba3176be9c23d49f749aee5da.svg",
            "fullname": "Itai Gat",
            "name": "itaigat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 19
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.04744",
            "authors": [
                {
                    "_id": "68be3e13c123124955ef5f99",
                    "name": "Gagan Mundada",
                    "hidden": false
                },
                {
                    "_id": "68be3e13c123124955ef5f9a",
                    "name": "Yash Vishe",
                    "hidden": false
                },
                {
                    "_id": "68be3e13c123124955ef5f9b",
                    "name": "Amit Namburi",
                    "hidden": false
                },
                {
                    "_id": "68be3e13c123124955ef5f9c",
                    "user": {
                        "_id": "6190ab805ca89a28e9f66873",
                        "avatarUrl": "/avatars/3c7ecc398fbf851acd2a132e947a92be.svg",
                        "isPro": false,
                        "fullname": "Xin Xu",
                        "user": "XinXuNLPer",
                        "type": "user"
                    },
                    "name": "Xin Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-08T07:01:58.002Z",
                    "hidden": false
                },
                {
                    "_id": "68be3e13c123124955ef5f9d",
                    "user": {
                        "_id": "643060c6cb3fe707b24c53a2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
                        "isPro": false,
                        "fullname": "Zachary Novack",
                        "user": "ZacharyNovack",
                        "type": "user"
                    },
                    "name": "Zachary Novack",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-08T07:02:00.032Z",
                    "hidden": false
                },
                {
                    "_id": "68be3e13c123124955ef5f9e",
                    "name": "Julian McAuley",
                    "hidden": false
                },
                {
                    "_id": "68be3e13c123124955ef5f9f",
                    "name": "Junda Wu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/i7xvFhvJz98DfSA92HH8z.png",
                "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/jb9rmpc4sKJdwj48aO16M.png"
            ],
            "publishedAt": "2025-09-05T01:54:50.000Z",
            "submittedOnDailyAt": "2025-09-08T00:59:18.510Z",
            "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning",
            "submittedOnDailyBy": {
                "_id": "643060c6cb3fe707b24c53a2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
                "isPro": false,
                "fullname": "Zachary Novack",
                "user": "ZacharyNovack",
                "type": "user"
            },
            "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated\nimpressive capabilities across various vision-language tasks. However, their\nreasoning abilities in the multimodal symbolic music domain remain largely\nunexplored. We introduce WildScore, the first in-the-wild multimodal symbolic\nmusic reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to\ninterpret real-world music scores and answer complex musicological queries.\nEach instance in WildScore is sourced from genuine musical compositions and\naccompanied by authentic user-generated questions and discussions, capturing\nthe intricacies of practical music analysis. To facilitate systematic\nevaluation, we propose a systematic taxonomy, comprising both high-level and\nfine-grained musicological ontologies. Furthermore, we frame complex music\nreasoning as multiple-choice question answering, enabling controlled and\nscalable assessment of MLLMs' symbolic music understanding. Empirical\nbenchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns\nin their visual-symbolic reasoning, uncovering both promising directions and\npersistent challenges for MLLMs in symbolic music reasoning and analysis. We\nrelease the dataset and code.",
            "upvotes": 8,
            "discussionId": "68be3e13c123124955ef5fa0",
            "ai_summary": "WildScore evaluates MLLMs' symbolic music reasoning through a benchmark of real-world music scores and user-generated queries, revealing both strengths and challenges.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "multimodal symbolic music reasoning",
                "WildScore",
                "musicological queries",
                "musicological ontologies",
                "multiple-choice question answering",
                "visual-symbolic reasoning"
            ]
        },
        "publishedAt": "2025-09-04T21:54:50.000Z",
        "title": "WildScore: Benchmarking MLLMs in-the-Wild Symbolic Music Reasoning",
        "summary": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated\nimpressive capabilities across various vision-language tasks. However, their\nreasoning abilities in the multimodal symbolic music domain remain largely\nunexplored. We introduce WildScore, the first in-the-wild multimodal symbolic\nmusic reasoning and analysis benchmark, designed to evaluate MLLMs' capacity to\ninterpret real-world music scores and answer complex musicological queries.\nEach instance in WildScore is sourced from genuine musical compositions and\naccompanied by authentic user-generated questions and discussions, capturing\nthe intricacies of practical music analysis. To facilitate systematic\nevaluation, we propose a systematic taxonomy, comprising both high-level and\nfine-grained musicological ontologies. Furthermore, we frame complex music\nreasoning as multiple-choice question answering, enabling controlled and\nscalable assessment of MLLMs' symbolic music understanding. Empirical\nbenchmarking of state-of-the-art MLLMs on WildScore reveals intriguing patterns\nin their visual-symbolic reasoning, uncovering both promising directions and\npersistent challenges for MLLMs in symbolic music reasoning and analysis. We\nrelease the dataset and code.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/i7xvFhvJz98DfSA92HH8z.png",
            "https://cdn-uploads.huggingface.co/production/uploads/643060c6cb3fe707b24c53a2/jb9rmpc4sKJdwj48aO16M.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04744.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643060c6cb3fe707b24c53a2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643060c6cb3fe707b24c53a2/MIoM9hrX0vV4XRyrm-4Kz.jpeg",
            "fullname": "Zachary Novack",
            "name": "ZacharyNovack",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 11
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.05263",
            "authors": [
                {
                    "_id": "68be4a9dc123124955ef5fd9",
                    "name": "Yinglin Duan",
                    "hidden": false
                },
                {
                    "_id": "68be4a9dc123124955ef5fda",
                    "name": "Zhengxia Zou",
                    "hidden": false
                },
                {
                    "_id": "68be4a9dc123124955ef5fdb",
                    "name": "Tongwei Gu",
                    "hidden": false
                },
                {
                    "_id": "68be4a9dc123124955ef5fdc",
                    "name": "Wei Jia",
                    "hidden": false
                },
                {
                    "_id": "68be4a9dc123124955ef5fdd",
                    "name": "Zhan Zhao",
                    "hidden": false
                },
                {
                    "_id": "68be4a9dc123124955ef5fde",
                    "name": "Luyi Xu",
                    "hidden": false
                },
                {
                    "_id": "68be4a9dc123124955ef5fdf",
                    "name": "Xinzhu Liu",
                    "hidden": false
                },
                {
                    "_id": "68be4a9dc123124955ef5fe0",
                    "name": "Hao Jiang",
                    "hidden": false
                },
                {
                    "_id": "68be4a9dc123124955ef5fe1",
                    "name": "Kang Chen",
                    "hidden": false
                },
                {
                    "_id": "68be4a9dc123124955ef5fe2",
                    "name": "Shuang Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-05T17:22:33.000Z",
            "submittedOnDailyAt": "2025-09-08T01:46:57.372Z",
            "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n90times increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
            "upvotes": 6,
            "discussionId": "68be4a9dc123124955ef5fe3",
            "ai_summary": "LatticeWorld, a 3D world generation framework using lightweight LLMs and Unreal Engine 5, creates dynamic, interactive environments from textual and visual inputs, achieving high accuracy and efficiency.",
            "ai_keywords": [
                "LLMs",
                "LLaMA-2-7B",
                "Unreal Engine 5",
                "multimodal inputs",
                "dynamic agents",
                "multi-agent interaction",
                "high-fidelity physics simulation",
                "real-time rendering",
                "scene layout generation",
                "visual fidelity"
            ]
        },
        "publishedAt": "2025-09-05T13:22:33.000Z",
        "title": "LatticeWorld: A Multimodal Large Language Model-Empowered Framework for\n  Interactive Complex World Generation",
        "summary": "Recent research has been increasingly focusing on developing 3D world models\nthat simulate complex real-world scenarios. World models have found broad\napplications across various domains, including embodied AI, autonomous driving,\nentertainment, etc. A more realistic simulation with accurate physics will\neffectively narrow the sim-to-real gap and allow us to gather rich information\nabout the real world conveniently. While traditional manual modeling has\nenabled the creation of virtual 3D scenes, modern approaches have leveraged\nadvanced machine learning algorithms for 3D world generation, with most recent\nadvances focusing on generative methods that can create virtual worlds based on\nuser instructions. This work explores such a research direction by proposing\nLatticeWorld, a simple yet effective 3D world generation framework that\nstreamlines the industrial production pipeline of 3D environments. LatticeWorld\nleverages lightweight LLMs (LLaMA-2-7B) alongside the industry-grade rendering\nengine (e.g., Unreal Engine 5) to generate a dynamic environment. Our proposed\nframework accepts textual descriptions and visual instructions as multimodal\ninputs and creates large-scale 3D interactive worlds with dynamic agents,\nfeaturing competitive multi-agent interaction, high-fidelity physics\nsimulation, and real-time rendering. We conduct comprehensive experiments to\nevaluate LatticeWorld, showing that it achieves superior accuracy in scene\nlayout generation and visual fidelity. Moreover, LatticeWorld achieves over a\n90times increase in industrial production efficiency while maintaining high\ncreative quality compared with traditional manual production methods. Our demo\nvideo is available at https://youtu.be/8VWZXpERR18",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05263.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 98
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.03680",
            "authors": [
                {
                    "_id": "68be8f39c123124955ef615a",
                    "name": "Ruofan Liang",
                    "hidden": false
                },
                {
                    "_id": "68be8f39c123124955ef615b",
                    "name": "Kai He",
                    "hidden": false
                },
                {
                    "_id": "68be8f39c123124955ef615c",
                    "name": "Zan Gojcic",
                    "hidden": false
                },
                {
                    "_id": "68be8f39c123124955ef615d",
                    "name": "Igor Gilitschenski",
                    "hidden": false
                },
                {
                    "_id": "68be8f39c123124955ef615e",
                    "name": "Sanja Fidler",
                    "hidden": false
                },
                {
                    "_id": "68be8f39c123124955ef615f",
                    "name": "Nandita Vijaykumar",
                    "hidden": false
                },
                {
                    "_id": "68be8f39c123124955ef6160",
                    "name": "Zian Wang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/dx3psEmrYlHPrdESlmTQo.mp4"
            ],
            "publishedAt": "2025-09-03T19:59:20.000Z",
            "submittedOnDailyAt": "2025-09-08T06:40:25.571Z",
            "title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer",
            "submittedOnDailyBy": {
                "_id": "5f1158120c833276f61f1a84",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
                "isPro": true,
                "fullname": "Niels Rogge",
                "user": "nielsr",
                "type": "user"
            },
            "summary": "Estimating scene lighting from a single image or video remains a longstanding\nchallenge in computer vision and graphics. Learning-based approaches are\nconstrained by the scarcity of ground-truth HDR environment maps, which are\nexpensive to capture and limited in diversity. While recent generative models\noffer strong priors for image synthesis, lighting estimation remains difficult\ndue to its reliance on indirect visual cues, the need to infer global\n(non-local) context, and the recovery of high-dynamic-range outputs. We propose\nLuxDiT, a novel data-driven approach that fine-tunes a video diffusion\ntransformer to generate HDR environment maps conditioned on visual input.\nTrained on a large synthetic dataset with diverse lighting conditions, our\nmodel learns to infer illumination from indirect visual cues and generalizes\neffectively to real-world scenes. To improve semantic alignment between the\ninput and the predicted environment map, we introduce a low-rank adaptation\nfinetuning strategy using a collected dataset of HDR panoramas. Our method\nproduces accurate lighting predictions with realistic angular high-frequency\ndetails, outperforming existing state-of-the-art techniques in both\nquantitative and qualitative evaluations.",
            "upvotes": 6,
            "discussionId": "68be8f3ac123124955ef6161",
            "ai_summary": "LuxDiT, a video diffusion transformer fine-tuned with low-rank adaptation, generates accurate HDR environment maps from visual input, outperforming existing methods.",
            "ai_keywords": [
                "video diffusion transformer",
                "HDR environment maps",
                "low-rank adaptation",
                "synthetic dataset",
                "indirect visual cues",
                "global context",
                "high-dynamic-range outputs",
                "HDR panoramas",
                "angular high-frequency details"
            ]
        },
        "publishedAt": "2025-09-03T15:59:20.000Z",
        "title": "LuxDiT: Lighting Estimation with Video Diffusion Transformer",
        "summary": "Estimating scene lighting from a single image or video remains a longstanding\nchallenge in computer vision and graphics. Learning-based approaches are\nconstrained by the scarcity of ground-truth HDR environment maps, which are\nexpensive to capture and limited in diversity. While recent generative models\noffer strong priors for image synthesis, lighting estimation remains difficult\ndue to its reliance on indirect visual cues, the need to infer global\n(non-local) context, and the recovery of high-dynamic-range outputs. We propose\nLuxDiT, a novel data-driven approach that fine-tunes a video diffusion\ntransformer to generate HDR environment maps conditioned on visual input.\nTrained on a large synthetic dataset with diverse lighting conditions, our\nmodel learns to infer illumination from indirect visual cues and generalizes\neffectively to real-world scenes. To improve semantic alignment between the\ninput and the predicted environment map, we introduce a low-rank adaptation\nfinetuning strategy using a collected dataset of HDR panoramas. Our method\nproduces accurate lighting predictions with realistic angular high-frequency\ndetails, outperforming existing state-of-the-art techniques in both\nquantitative and qualitative evaluations.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/5f1158120c833276f61f1a84/dx3psEmrYlHPrdESlmTQo.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03680.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5f1158120c833276f61f1a84",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1608042047613-5f1158120c833276f61f1a84.jpeg",
            "fullname": "Niels Rogge",
            "name": "nielsr",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 959
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.05296",
            "authors": [
                {
                    "_id": "68be737bc123124955ef60d8",
                    "name": "Zizun Li",
                    "hidden": false
                },
                {
                    "_id": "68be737bc123124955ef60d9",
                    "name": "Jianjun Zhou",
                    "hidden": false
                },
                {
                    "_id": "68be737bc123124955ef60da",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "68be737bc123124955ef60db",
                    "user": {
                        "_id": "652ce0d4c543a08aa92e010f",
                        "avatarUrl": "/avatars/7978304e3fe99b0d4d0712441c6a24f3.svg",
                        "isPro": false,
                        "fullname": "Haoyu Guo",
                        "user": "ghy0324",
                        "type": "user"
                    },
                    "name": "Haoyu Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-08T07:01:51.542Z",
                    "hidden": false
                },
                {
                    "_id": "68be737bc123124955ef60dc",
                    "name": "Wenzheng Chang",
                    "hidden": false
                },
                {
                    "_id": "68be737bc123124955ef60dd",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68be737bc123124955ef60de",
                    "name": "Haoyi Zhu",
                    "hidden": false
                },
                {
                    "_id": "68be737bc123124955ef60df",
                    "name": "Junyi Chen",
                    "hidden": false
                },
                {
                    "_id": "68be737bc123124955ef60e0",
                    "name": "Chunhua Shen",
                    "hidden": false
                },
                {
                    "_id": "68be737bc123124955ef60e1",
                    "name": "Tong He",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65e7eb86c7a0617cc71d3df4/FbisPQymZ9xE9u1ePHpSq.mp4"
            ],
            "publishedAt": "2025-09-05T17:59:47.000Z",
            "submittedOnDailyAt": "2025-09-08T04:43:48.454Z",
            "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
            "submittedOnDailyBy": {
                "_id": "65e7eb86c7a0617cc71d3df4",
                "avatarUrl": "/avatars/01020b6b5ccb08bf8aa10fd5f8b2701d.svg",
                "isPro": false,
                "fullname": "lizizun",
                "user": "lizizun",
                "type": "user"
            },
            "summary": "We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.",
            "upvotes": 3,
            "discussionId": "68be737bc123124955ef60e2",
            "ai_summary": "WinT3R, a feed-forward reconstruction model, achieves high-quality camera pose estimation and real-time performance using a sliding window mechanism and a global camera token pool.",
            "ai_keywords": [
                "feed-forward reconstruction model",
                "sliding window mechanism",
                "geometric predictions",
                "compact representation",
                "global camera token pool",
                "camera pose estimation",
                "real-time performance"
            ]
        },
        "publishedAt": "2025-09-05T13:59:47.000Z",
        "title": "WinT3R: Window-Based Streaming Reconstruction with Camera Token Pool",
        "summary": "We present WinT3R, a feed-forward reconstruction model capable of online\nprediction of precise camera poses and high-quality point maps. Previous\nmethods suffer from a trade-off between reconstruction quality and real-time\nperformance. To address this, we first introduce a sliding window mechanism\nthat ensures sufficient information exchange among frames within the window,\nthereby improving the quality of geometric predictions without large\ncomputation. In addition, we leverage a compact representation of cameras and\nmaintain a global camera token pool, which enhances the reliability of camera\npose estimation without sacrificing efficiency. These designs enable WinT3R to\nachieve state-of-the-art performance in terms of online reconstruction quality,\ncamera pose estimation, and reconstruction speed, as validated by extensive\nexperiments on diverse datasets. Code and model are publicly available at\nhttps://github.com/LiZizun/WinT3R.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65e7eb86c7a0617cc71d3df4/FbisPQymZ9xE9u1ePHpSq.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05296.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65e7eb86c7a0617cc71d3df4",
            "avatarUrl": "/avatars/01020b6b5ccb08bf8aa10fd5f8b2701d.svg",
            "fullname": "lizizun",
            "name": "lizizun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.03800",
            "authors": [
                {
                    "_id": "68be5223c123124955ef5fec",
                    "name": "Yuheng Li",
                    "hidden": false
                },
                {
                    "_id": "68be5223c123124955ef5fed",
                    "name": "Yenho Chen",
                    "hidden": false
                },
                {
                    "_id": "68be5223c123124955ef5fee",
                    "name": "Yuxiang Lai",
                    "hidden": false
                },
                {
                    "_id": "68be5223c123124955ef5fef",
                    "name": "Jike Zhong",
                    "hidden": false
                },
                {
                    "_id": "68be5223c123124955ef5ff0",
                    "name": "Vanessa Wildman",
                    "hidden": false
                },
                {
                    "_id": "68be5223c123124955ef5ff1",
                    "name": "Xiaofeng Yang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T01:28:44.000Z",
            "submittedOnDailyAt": "2025-09-08T02:19:21.951Z",
            "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.",
            "upvotes": 3,
            "discussionId": "68be5224c123124955ef5ff2",
            "ai_summary": "MedVista3D is a multi-scale semantic-enriched vision-language pretraining framework for 3D CT analysis that addresses local-global understanding, report variability, and achieves state-of-the-art performance in disease classification, report retrieval, and medical visual question answering.",
            "ai_keywords": [
                "3D vision-language models",
                "local-global understanding",
                "spatial reasoning",
                "language model rewrites",
                "Radiology Semantic Matching Bank",
                "zero-shot disease classification",
                "report retrieval",
                "medical visual question answering",
                "organ segmentation",
                "prognosis prediction"
            ]
        },
        "publishedAt": "2025-09-03T21:28:44.000Z",
        "title": "MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in\n  3D CT Disease Detection, Understanding and Reporting",
        "summary": "Radiologic diagnostic errors-under-reading errors, inattentional blindness,\nand communication failures-remain prevalent in clinical practice. These issues\noften stem from missed localized abnormalities, limited global context, and\nvariability in report language. These challenges are amplified in 3D imaging,\nwhere clinicians must examine hundreds of slices per scan. Addressing them\nrequires systems with precise localized detection, global volume-level\nreasoning, and semantically consistent natural language reporting. However,\nexisting 3D vision-language models are unable to meet all three needs jointly,\nlacking local-global understanding for spatial reasoning and struggling with\nthe variability and noise of uncurated radiology reports. We present\nMedVista3D, a multi-scale semantic-enriched vision-language pretraining\nframework for 3D CT analysis. To enable joint disease detection and holistic\ninterpretation, MedVista3D performs local and global image-text alignment for\nfine-grained representation learning within full-volume context. To address\nreport variability, we apply language model rewrites and introduce a Radiology\nSemantic Matching Bank for semantics-aware alignment. MedVista3D achieves\nstate-of-the-art performance on zero-shot disease classification, report\nretrieval, and medical visual question answering, while transferring well to\norgan segmentation and prognosis prediction. Code and datasets will be\nreleased.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03800.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 98
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.04013",
            "authors": [
                {
                    "_id": "68be6ac4c123124955ef60cb",
                    "name": "Riccardo Lunardi",
                    "hidden": false
                },
                {
                    "_id": "68be6ac4c123124955ef60cc",
                    "name": "Vincenzo Della Mea",
                    "hidden": false
                },
                {
                    "_id": "68be6ac4c123124955ef60cd",
                    "name": "Stefano Mizzaro",
                    "hidden": false
                },
                {
                    "_id": "68be6ac4c123124955ef60ce",
                    "name": "Kevin Roitero",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T08:43:27.000Z",
            "submittedOnDailyAt": "2025-09-08T04:05:06.987Z",
            "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
            "submittedOnDailyBy": {
                "_id": "620cca6f06a4320dbf3b50d8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
                "isPro": false,
                "fullname": "Kevin Roitero",
                "user": "kevinr",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.",
            "upvotes": 2,
            "discussionId": "68be6ac4c123124955ef60cf",
            "ai_summary": "LLMs show reduced effectiveness on paraphrased benchmark questions, indicating limitations in handling linguistic variability and suggesting the need for more robust evaluation methods.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "MMLU",
                "ARC-C",
                "HellaSwag",
                "paraphrased benchmark questions",
                "linguistic variability",
                "generalization abilities",
                "benchmark-based evaluations",
                "robustness-aware benchmarks"
            ]
        },
        "publishedAt": "2025-09-04T04:43:27.000Z",
        "title": "On Robustness and Reliability of Benchmark-Based Evaluation of LLMs",
        "summary": "Large Language Models (LLMs) effectiveness is usually evaluated by means of\nbenchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in\ntheir original wording, thus in a fixed, standardized format. However,\nreal-world applications involve linguistic variability, requiring models to\nmaintain their effectiveness across diverse rewordings of the same question or\nquery. In this study, we systematically assess the robustness of LLMs to\nparaphrased benchmark questions and investigate whether benchmark-based\nevaluations provide a reliable measure of model capabilities. We systematically\ngenerate various paraphrases of all the questions across six different common\nbenchmarks, and measure the resulting variations in effectiveness of 34\nstate-of-the-art LLMs, of different size and effectiveness. Our findings reveal\nthat while LLM rankings remain relatively stable across paraphrased inputs,\nabsolute effectiveness scores change, and decline significantly. This suggests\nthat LLMs struggle with linguistic variability, raising concerns about their\ngeneralization abilities and evaluation methodologies. Furthermore, the\nobserved performance drop challenges the reliability of benchmark-based\nevaluations, indicating that high benchmark scores may not fully capture a\nmodel's robustness to real-world input variations. We discuss the implications\nof these findings for LLM evaluation methodologies, emphasizing the need for\nrobustness-aware benchmarks that better reflect practical deployment scenarios.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04013.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "620cca6f06a4320dbf3b50d8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1654011487582-620cca6f06a4320dbf3b50d8.png",
            "fullname": "Kevin Roitero",
            "name": "kevinr",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.04504",
            "authors": [
                {
                    "_id": "68be5b08c123124955ef60bb",
                    "name": "Zehua Pei",
                    "hidden": false
                },
                {
                    "_id": "68be5b08c123124955ef60bc",
                    "name": "Hui-Ling Zhen",
                    "hidden": false
                },
                {
                    "_id": "68be5b08c123124955ef60bd",
                    "name": "Ying Zhang",
                    "hidden": false
                },
                {
                    "_id": "68be5b08c123124955ef60be",
                    "name": "Zhiyuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68be5b08c123124955ef60bf",
                    "name": "Xing Li",
                    "hidden": false
                },
                {
                    "_id": "68be5b08c123124955ef60c0",
                    "name": "Xianzhi Yu",
                    "hidden": false
                },
                {
                    "_id": "68be5b08c123124955ef60c1",
                    "name": "Mingxuan Yuan",
                    "hidden": false
                },
                {
                    "_id": "68be5b08c123124955ef60c2",
                    "name": "Bei Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-02T07:03:20.000Z",
            "submittedOnDailyAt": "2025-09-08T02:57:59.995Z",
            "title": "Behavioral Fingerprinting of Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6527c063e86758eb6ca800a1",
                "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
                "isPro": false,
                "fullname": "JarvisPei",
                "user": "Eleven-P",
                "type": "user"
            },
            "summary": "Current benchmarks for Large Language Models (LLMs) primarily focus on\nperformance metrics, often failing to capture the nuanced behavioral\ncharacteristics that differentiate them. This paper introduces a novel\n``Behavioral Fingerprinting'' framework designed to move beyond traditional\nevaluation by creating a multi-faceted profile of a model's intrinsic cognitive\nand interactive styles. Using a curated Diagnostic Prompt Suite and an\ninnovative, automated evaluation pipeline where a powerful LLM acts as an\nimpartial judge, we analyze eighteen models across capability tiers. Our\nresults reveal a critical divergence in the LLM landscape: while core\ncapabilities like abstract and causal reasoning are converging among top\nmodels, alignment-related behaviors such as sycophancy and semantic robustness\nvary dramatically. We further document a cross-model default persona clustering\n(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,\nthis suggests that a model's interactive nature is not an emergent property of\nits scale or reasoning power, but a direct consequence of specific, and highly\nvariable, developer alignment strategies. Our framework provides a reproducible\nand scalable methodology for uncovering these deep behavioral differences.\nProject: https://github.com/JarvisPei/Behavioral-Fingerprinting",
            "upvotes": 2,
            "discussionId": "68be5b08c123124955ef60c3",
            "projectPage": "https://github.com/JarvisPei/Behavioral-Fingerprinting",
            "githubRepo": "https://github.com/JarvisPei/Behavioral-Fingerprinting",
            "ai_summary": "A Behavioral Fingerprinting framework evaluates Large Language Models using a Diagnostic Prompt Suite and automated pipeline, revealing divergent alignment behaviors and clustering patterns.",
            "ai_keywords": [
                "Behavioral Fingerprinting",
                "Diagnostic Prompt Suite",
                "automated evaluation pipeline",
                "Large Language Models",
                "abstract reasoning",
                "causal reasoning",
                "sycophancy",
                "semantic robustness",
                "default persona clustering",
                "ISTJ/ESTJ",
                "alignment strategies"
            ],
            "githubStars": 2
        },
        "publishedAt": "2025-09-02T03:03:20.000Z",
        "title": "Behavioral Fingerprinting of Large Language Models",
        "summary": "Current benchmarks for Large Language Models (LLMs) primarily focus on\nperformance metrics, often failing to capture the nuanced behavioral\ncharacteristics that differentiate them. This paper introduces a novel\n``Behavioral Fingerprinting'' framework designed to move beyond traditional\nevaluation by creating a multi-faceted profile of a model's intrinsic cognitive\nand interactive styles. Using a curated Diagnostic Prompt Suite and an\ninnovative, automated evaluation pipeline where a powerful LLM acts as an\nimpartial judge, we analyze eighteen models across capability tiers. Our\nresults reveal a critical divergence in the LLM landscape: while core\ncapabilities like abstract and causal reasoning are converging among top\nmodels, alignment-related behaviors such as sycophancy and semantic robustness\nvary dramatically. We further document a cross-model default persona clustering\n(ISTJ/ESTJ) that likely reflects common alignment incentives. Taken together,\nthis suggests that a model's interactive nature is not an emergent property of\nits scale or reasoning power, but a direct consequence of specific, and highly\nvariable, developer alignment strategies. Our framework provides a reproducible\nand scalable methodology for uncovering these deep behavioral differences.\nProject: https://github.com/JarvisPei/Behavioral-Fingerprinting",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04504.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6527c063e86758eb6ca800a1",
            "avatarUrl": "/avatars/9091be87eea518209c1de9eebfa663c0.svg",
            "fullname": "JarvisPei",
            "name": "Eleven-P",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.04575",
            "authors": [
                {
                    "_id": "68bea66dc123124955ef61aa",
                    "name": "Minqi Jiang",
                    "hidden": false
                },
                {
                    "_id": "68bea66dc123124955ef61ab",
                    "name": "Andrei Lupu",
                    "hidden": false
                },
                {
                    "_id": "68bea66dc123124955ef61ac",
                    "name": "Yoram Bachrach",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T18:01:00.000Z",
            "submittedOnDailyAt": "2025-09-08T08:20:19.574Z",
            "title": "Bootstrapping Task Spaces for Self-Improvement",
            "submittedOnDailyBy": {
                "_id": "630368320547362a22a21798",
                "avatarUrl": "/avatars/035b468584e43b4fbbf4c4aa1d2bbac7.svg",
                "isPro": false,
                "fullname": "Minqi Jiang",
                "user": "minqi",
                "type": "user"
            },
            "summary": "Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training.",
            "upvotes": 1,
            "discussionId": "68bea66dc123124955ef61ad",
            "ai_summary": "Exploratory Iteration (ExIt) is an autocurriculum RL method that trains LLMs to perform multi-step self-improvement at inference-time by selectively sampling informative intermediate histories, enabling strong self-improvement on unseen tasks.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "autocurriculum RL",
                "LLMs",
                "self-improvement",
                "inference-time",
                "multi-step self-improvement",
                "task space",
                "intermediate histories",
                "explicit exploration mechanisms",
                "competition math",
                "multi-turn tool-use",
                "machine learning engineering"
            ]
        },
        "publishedAt": "2025-09-04T14:01:00.000Z",
        "title": "Bootstrapping Task Spaces for Self-Improvement",
        "summary": "Progress in many task domains emerges from repeated revisions to previous\nsolution attempts. Training agents that can reliably self-improve over such\nsequences at inference-time is a natural target for reinforcement learning\n(RL), yet the naive approach assumes a fixed maximum iteration depth, which can\nbe both costly and arbitrary. We present Exploratory Iteration (ExIt), a family\nof autocurriculum RL methods that directly exploits the recurrent structure of\nself-improvement tasks to train LLMs to perform multi-step self-improvement at\ninference-time while only training on the most informative single-step\niterations. ExIt grows a task space by selectively sampling the most\ninformative intermediate, partial histories encountered during an episode for\ncontinued iteration, treating these starting points as new self-iteration task\ninstances to train a self-improvement policy. ExIt can further pair with\nexplicit exploration mechanisms to sustain greater task diversity. Across\nseveral domains, encompassing competition math, multi-turn tool-use, and\nmachine learning engineering, we demonstrate that ExIt strategies, starting\nfrom either a single or many task instances, can produce policies exhibiting\nstrong inference-time self-improvement on held-out task instances, and the\nability to iterate towards higher performance over a step budget extending\nbeyond the average iteration depth encountered during training.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04575.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "630368320547362a22a21798",
            "avatarUrl": "/avatars/035b468584e43b4fbbf4c4aa1d2bbac7.svg",
            "fullname": "Minqi Jiang",
            "name": "minqi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.02437",
            "authors": [
                {
                    "_id": "68be910dc123124955ef616b",
                    "name": "Yanwen Zou",
                    "hidden": false
                },
                {
                    "_id": "68be910dc123124955ef616c",
                    "name": "Zhaoye Zhou",
                    "hidden": false
                },
                {
                    "_id": "68be910dc123124955ef616d",
                    "name": "Chenyang Shi",
                    "hidden": false
                },
                {
                    "_id": "68be910dc123124955ef616e",
                    "name": "Zewei Ye",
                    "hidden": false
                },
                {
                    "_id": "68be910dc123124955ef616f",
                    "name": "Junda Huang",
                    "hidden": false
                },
                {
                    "_id": "68be910dc123124955ef6170",
                    "name": "Yan Ding",
                    "hidden": false
                },
                {
                    "_id": "68be910dc123124955ef6171",
                    "name": "Bo Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-02T15:39:38.000Z",
            "submittedOnDailyAt": "2025-09-08T06:47:39.249Z",
            "title": "U-ARM : Ultra low-cost general teleoperation interface for robot\n  manipulation",
            "submittedOnDailyBy": {
                "_id": "64acafac1aee69ece03af05a",
                "avatarUrl": "/avatars/6380777852546dbd432fed5544d5db0b.svg",
                "isPro": false,
                "fullname": "Bo Zhao",
                "user": "Z-LIRA",
                "type": "user"
            },
            "summary": "We propose U-Arm, a low-cost and rapidly adaptable leader-follower\nteleoperation framework designed to interface with most of commercially\navailable robotic arms. Our system supports teleoperation through three\nstructurally distinct 3D-printed leader arms that share consistent control\nlogic, enabling seamless compatibility with diverse commercial robot\nconfigurations. Compared with previous open-source leader-follower interfaces,\nwe further optimized both the mechanical design and servo selection, achieving\na bill of materials (BOM) cost of only \\50.5 for the 6-DoF leader arm and\n56.8 for the 7-DoF version. To enhance usability, we mitigate the common\nchallenge in controlling redundant degrees of freedom by %engineering methods\nmechanical and control optimizations. Experimental results demonstrate that\nU-Arm achieves 39\\% higher data collection efficiency and comparable task\nsuccess rates across multiple manipulation scenarios compared with Joycon,\nanother low-cost teleoperation interface. We have open-sourced all CAD models\nof three configs and also provided simulation support for validating\nteleoperation workflows. We also open-sourced real-world manipulation data\ncollected with U-Arm. The project website is\nhttps://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.",
            "upvotes": 1,
            "discussionId": "68be910dc123124955ef6172",
            "projectPage": "https://github.com/MINT-SJTU/LeRobot-Anything-U-Arm",
            "githubRepo": "https://github.com/MINT-SJTU/LeRobot-Anything-U-Arm",
            "ai_summary": "U-Arm is a low-cost, adaptable teleoperation framework for robotic arms that optimizes mechanical design and control logic to enhance data collection efficiency and task success rates.",
            "ai_keywords": [
                "leader-follower teleoperation",
                "3D-printed leader arms",
                "6-DoF",
                "7-DoF",
                "servo selection",
                "mechanical design",
                "control logic",
                "data collection efficiency",
                "task success rates",
                "CAD models",
                "simulation support"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-09-02T11:39:38.000Z",
        "title": "U-ARM : Ultra low-cost general teleoperation interface for robot\n  manipulation",
        "summary": "We propose U-Arm, a low-cost and rapidly adaptable leader-follower\nteleoperation framework designed to interface with most of commercially\navailable robotic arms. Our system supports teleoperation through three\nstructurally distinct 3D-printed leader arms that share consistent control\nlogic, enabling seamless compatibility with diverse commercial robot\nconfigurations. Compared with previous open-source leader-follower interfaces,\nwe further optimized both the mechanical design and servo selection, achieving\na bill of materials (BOM) cost of only \\50.5 for the 6-DoF leader arm and\n56.8 for the 7-DoF version. To enhance usability, we mitigate the common\nchallenge in controlling redundant degrees of freedom by %engineering methods\nmechanical and control optimizations. Experimental results demonstrate that\nU-Arm achieves 39\\% higher data collection efficiency and comparable task\nsuccess rates across multiple manipulation scenarios compared with Joycon,\nanother low-cost teleoperation interface. We have open-sourced all CAD models\nof three configs and also provided simulation support for validating\nteleoperation workflows. We also open-sourced real-world manipulation data\ncollected with U-Arm. The project website is\nhttps://github.com/MINT-SJTU/LeRobot-Anything-U-Arm.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02437.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64acafac1aee69ece03af05a",
            "avatarUrl": "/avatars/6380777852546dbd432fed5544d5db0b.svg",
            "fullname": "Bo Zhao",
            "name": "Z-LIRA",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    }
]