[
    {
        "paper": {
            "id": "2509.06160",
            "authors": [
                {
                    "_id": "68bf936f207285de11b07b79",
                    "name": "Haozhe Wang",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b7a",
                    "name": "Haoran Que",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b7b",
                    "user": {
                        "_id": "6680f0b20b72be136708af26",
                        "avatarUrl": "/avatars/5d8fd5be0cf94e246b46abb9d3cc8f5c.svg",
                        "isPro": false,
                        "fullname": "XuQixin",
                        "user": "Racktic",
                        "type": "user"
                    },
                    "name": "Qixin Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:04.349Z",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b7c",
                    "user": {
                        "_id": "6417d9ea8f689506e7148417",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6417d9ea8f689506e7148417/bAYcruWNw4WvmuQcGgcwC.jpeg",
                        "isPro": false,
                        "fullname": "minghao",
                        "user": "Liam-Liu",
                        "type": "user"
                    },
                    "name": "Minghao Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:08.356Z",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b7d",
                    "name": "Wangchunshu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b7e",
                    "name": "Jiazhan Feng",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b7f",
                    "name": "Wanjun Zhong",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b80",
                    "name": "Wei Ye",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b81",
                    "name": "Tong Yang",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b82",
                    "name": "Wenhao Huang",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b83",
                    "user": {
                        "_id": "638efcf4c67af472d316d424",
                        "avatarUrl": "/avatars/97a57859d7d87a3a8f1bb41d32a72bc2.svg",
                        "isPro": false,
                        "fullname": "Ge Zhang",
                        "user": "zhangysk",
                        "type": "user"
                    },
                    "name": "Ge Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:57.642Z",
                    "hidden": false
                },
                {
                    "_id": "68bf936f207285de11b07b84",
                    "name": "Fangzhen Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-07T18:07:58.000Z",
            "submittedOnDailyAt": "2025-09-09T01:10:01.643Z",
            "title": "Reverse-Engineered Reasoning for Open-Ended Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "While the ``deep reasoning'' paradigm has spurred significant advances in\nverifiable domains like mathematics, its application to open-ended, creative\ngeneration remains a critical challenge. The two dominant methods for\ninstilling reasoning -- reinforcement learning (RL) and instruction\ndistillation -- falter in this area; RL struggles with the absence of clear\nreward signals and high-quality reward models, while distillation is\nprohibitively expensive and capped by the teacher model's capabilities. To\novercome these limitations, we introduce REverse-Engineered Reasoning (REER), a\nnew paradigm that fundamentally shifts the approach. Instead of building a\nreasoning process ``forwards'' through trial-and-error or imitation, REER works\n``backwards'' from known-good solutions to computationally discover the latent,\nstep-by-step deep reasoning process that could have produced them. Using this\nscalable, gradient-free approach, we curate and open-source DeepWriting-20K, a\nlarge-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.\nOur model, DeepWriter-8B, trained on this data, not only surpasses strong\nopen-source baselines but also achieves performance competitive with, and at\ntimes superior to, leading proprietary models like GPT-4o and Claude 3.5.",
            "upvotes": 105,
            "discussionId": "68bf9370207285de11b07b85",
            "projectPage": "https://m-a-p.ai/REER_DeepWriter/",
            "ai_summary": "REER, a new paradigm for deep reasoning, uses reverse engineering to discover step-by-step reasoning processes, enabling a model to perform competitively on open-ended tasks.",
            "ai_keywords": [
                "deep reasoning",
                "reinforcement learning",
                "instruction distillation",
                "REER",
                "gradient-free",
                "DeepWriting-20K",
                "DeepWriter-8B",
                "GPT-4o",
                "Claude 3.5"
            ]
        },
        "publishedAt": "2025-09-07T14:07:58.000Z",
        "title": "Reverse-Engineered Reasoning for Open-Ended Generation",
        "summary": "While the ``deep reasoning'' paradigm has spurred significant advances in\nverifiable domains like mathematics, its application to open-ended, creative\ngeneration remains a critical challenge. The two dominant methods for\ninstilling reasoning -- reinforcement learning (RL) and instruction\ndistillation -- falter in this area; RL struggles with the absence of clear\nreward signals and high-quality reward models, while distillation is\nprohibitively expensive and capped by the teacher model's capabilities. To\novercome these limitations, we introduce REverse-Engineered Reasoning (REER), a\nnew paradigm that fundamentally shifts the approach. Instead of building a\nreasoning process ``forwards'' through trial-and-error or imitation, REER works\n``backwards'' from known-good solutions to computationally discover the latent,\nstep-by-step deep reasoning process that could have produced them. Using this\nscalable, gradient-free approach, we curate and open-source DeepWriting-20K, a\nlarge-scale dataset of 20,000 deep reasoning trajectories for open-ended tasks.\nOur model, DeepWriter-8B, trained on this data, not only surpasses strong\nopen-source baselines but also achieves performance competitive with, and at\ntimes superior to, leading proprietary models like GPT-4o and Claude 3.5.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06160.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 102
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06501",
            "authors": [
                {
                    "_id": "68bfb768207285de11b07d02",
                    "name": "Junteng Liu",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d03",
                    "user": {
                        "_id": "62ac3bcec35bb36ff0785962",
                        "avatarUrl": "/avatars/edd4f14556abc39739bac951043a3065.svg",
                        "isPro": false,
                        "fullname": "李云济",
                        "user": "awdrgyjilplij",
                        "type": "user"
                    },
                    "name": "Yunji Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:46:35.234Z",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d04",
                    "name": "Chi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d05",
                    "name": "Jingyang Li",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d06",
                    "user": {
                        "_id": "63f86b099f87cc3e645b51d9",
                        "avatarUrl": "/avatars/27ca5ba425640bf67474cee871e8e53a.svg",
                        "isPro": false,
                        "fullname": "Ellie Chen",
                        "user": "sheep33333",
                        "type": "user"
                    },
                    "name": "Aili Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:46:37.221Z",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d07",
                    "name": "Ke Ji",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d08",
                    "name": "Weiyu Cheng",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d09",
                    "name": "Zijia Wu",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d0a",
                    "name": "Chengyu Du",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d0b",
                    "name": "Qidi Xu",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d0c",
                    "name": "Jiayuan Song",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d0d",
                    "name": "Zhengmao Zhu",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d0e",
                    "name": "Wenhu Chen",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d0f",
                    "name": "Pengyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68bfb768207285de11b07d10",
                    "name": "Junxian He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T10:07:03.000Z",
            "submittedOnDailyAt": "2025-09-09T03:45:45.200Z",
            "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents",
            "submittedOnDailyBy": {
                "_id": "6493fbb3085e14d7933b936d",
                "avatarUrl": "/avatars/85723bedac9e81fecc33b36ff94ecada.svg",
                "isPro": false,
                "fullname": "Junteng Liu",
                "user": "Junteng",
                "type": "user"
            },
            "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.",
            "upvotes": 55,
            "discussionId": "68bfb768207285de11b07d11",
            "ai_summary": "WebExplorer, a data-driven approach for developing advanced web agents, achieves state-of-the-art performance in information-seeking tasks through systematic data generation and reinforcement learning.",
            "ai_keywords": [
                "Large Language Models",
                "agentic applications",
                "web browsing capabilities",
                "information-seeking abilities",
                "model-based exploration",
                "iterative query evolution",
                "query-answer pairs",
                "multi-step reasoning",
                "complex web navigation",
                "supervised fine-tuning",
                "reinforcement learning",
                "context length",
                "tool calling turns",
                "long-horizon problem solving",
                "BrowseComp-en/zh",
                "WebWalkerQA",
                "FRAMES",
                "HLE benchmark"
            ]
        },
        "publishedAt": "2025-09-08T06:07:03.000Z",
        "title": "WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents",
        "summary": "The paradigm of Large Language Models (LLMs) has increasingly shifted toward\nagentic applications, where web browsing capabilities are fundamental for\nretrieving information from diverse online sources. However, existing\nopen-source web agents either demonstrate limited information-seeking abilities\non complex tasks or lack transparent implementations. In this work, we identify\nthat the key challenge lies in the scarcity of challenging data for information\nseeking. To address this limitation, we introduce WebExplorer: a systematic\ndata generation approach using model-based exploration and iterative,\nlong-to-short query evolution. This method creates challenging query-answer\npairs that require multi-step reasoning and complex web navigation. By\nleveraging our curated high-quality dataset, we successfully develop advanced\nweb agent WebExplorer-8B through supervised fine-tuning followed by\nreinforcement learning. Our model supports 128K context length and up to 100\ntool calling turns, enabling long-horizon problem solving. Across diverse\ninformation-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art\nperformance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able\nto effectively search over an average of 16 turns after RL training, achieving\nhigher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best\nperformance among models up to 100B parameters on WebWalkerQA and FRAMES.\nBeyond these information-seeking tasks, our model also achieves strong\ngeneralization on the HLE benchmark even though it is only trained on\nknowledge-intensive QA data. These results highlight our approach as a\npractical path toward long-horizon web agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06501.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6493fbb3085e14d7933b936d",
            "avatarUrl": "/avatars/85723bedac9e81fecc33b36ff94ecada.svg",
            "fullname": "Junteng Liu",
            "name": "Junteng",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06949",
            "authors": [
                {
                    "_id": "68bf87ff207285de11b07b39",
                    "name": "Yinjie Wang",
                    "hidden": false
                },
                {
                    "_id": "68bf87ff207285de11b07b3a",
                    "user": {
                        "_id": "64fde4e252e82dd432b74ce9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
                        "isPro": false,
                        "fullname": "Ling Yang",
                        "user": "Lingaaaaaaa",
                        "type": "user"
                    },
                    "name": "Ling Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:22.774Z",
                    "hidden": false
                },
                {
                    "_id": "68bf87ff207285de11b07b3b",
                    "name": "Bowen Li",
                    "hidden": false
                },
                {
                    "_id": "68bf87ff207285de11b07b3c",
                    "name": "Ye Tian",
                    "hidden": false
                },
                {
                    "_id": "68bf87ff207285de11b07b3d",
                    "name": "Ke Shen",
                    "hidden": false
                },
                {
                    "_id": "68bf87ff207285de11b07b3e",
                    "name": "Mengdi Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T17:58:06.000Z",
            "submittedOnDailyAt": "2025-09-09T00:25:38.536Z",
            "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
            "submittedOnDailyBy": {
                "_id": "64fde4e252e82dd432b74ce9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
                "isPro": false,
                "fullname": "Ling Yang",
                "user": "Lingaaaaaaa",
                "type": "user"
            },
            "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
            "upvotes": 37,
            "discussionId": "68bf87ff207285de11b07b3f",
            "projectPage": "https://huggingface.co/collections/Gen-Verse/trado-series-68beb6cd6a26c27cde9fe3af",
            "githubRepo": "https://github.com/Gen-Verse/dLLM-RL",
            "ai_summary": "TraceRL enhances diffusion language models with trajectory-aware reinforcement learning, improving reasoning performance on complex tasks and enabling flexible sampling.",
            "ai_keywords": [
                "trajectory-aware reinforcement learning",
                "diffusion language models",
                "diffusion-based value model",
                "sampling flexibility",
                "curriculum learning",
                "long-CoT DLM",
                "KV-cache techniques",
                "inference engines",
                "supervised fine-tuning",
                "RL methods"
            ],
            "githubStars": 89
        },
        "publishedAt": "2025-09-08T13:58:06.000Z",
        "title": "Revolutionizing Reinforcement Learning Framework for Diffusion Large\n  Language Models",
        "summary": "We propose TraceRL, a trajectory-aware reinforcement learning framework for\ndiffusion language models (DLMs) that incorporates preferred inference\ntrajectory into post-training, and is applicable across different\narchitectures. Equipped with a diffusion-based value model that enhances\ntraining stability, we demonstrate improved reasoning performance on complex\nmath and coding tasks. Besides, it can also be applied to adapt block-specific\nmodels to larger blocks, which improves sampling flexibility. Employing\nTraceRL, we derive a series of state-of-the-art diffusion language models,\nnamely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still\nconsistently outperforms them across complex math reasoning tasks.\nTraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over\nQwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical\nreasoning benchmarks. Through curriculum learning, we also derive the first\nlong-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1%\nrelative accuracy gain. To facilitate reproducible research and practical\napplications, we release a comprehensive open-source framework for building,\ntraining, and deploying diffusion LLMs across diverse architectures. The\nframework integrates accelerated KV-cache techniques and inference engines for\nboth inference and reinforcement learning, and includes implementations of\nvarious supervised fine-tuning and RL methods for mathematics, coding, and\ngeneral tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06949.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "64fde4e252e82dd432b74ce9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64fde4e252e82dd432b74ce9/-CQZbBP7FsPPyawYrsi4z.jpeg",
            "fullname": "Ling Yang",
            "name": "Lingaaaaaaa",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06467",
            "authors": [
                {
                    "_id": "68bf97aa207285de11b07ba5",
                    "name": "Che Liu",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07ba6",
                    "name": "Yinda Chen",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07ba7",
                    "name": "Haoyuan Shi",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07ba8",
                    "name": "Jinpeng Lu",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07ba9",
                    "name": "Bailiang Jian",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07baa",
                    "name": "Jiazhen Pan",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bab",
                    "name": "Linghan Cai",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bac",
                    "name": "Jiayi Wang",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bad",
                    "name": "Yundi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bae",
                    "name": "Jun Li",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07baf",
                    "name": "Cosmin I. Bercea",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bb0",
                    "name": "Cheng Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bb1",
                    "name": "Chen Chen",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bb2",
                    "name": "Zhiwei Xiong",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bb3",
                    "name": "Benedikt Wiestler",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bb4",
                    "name": "Christian Wachinger",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bb5",
                    "name": "Daniel Rueckert",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bb6",
                    "name": "Wenjia Bai",
                    "hidden": false
                },
                {
                    "_id": "68bf97aa207285de11b07bb7",
                    "name": "Rossella Arcucci",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T09:28:57.000Z",
            "submittedOnDailyAt": "2025-09-09T01:28:11.725Z",
            "title": "Does DINOv3 Set a New Medical Vision Standard?",
            "submittedOnDailyBy": {
                "_id": "631b9ff5824f2502e3557c7e",
                "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
                "isPro": true,
                "fullname": "liu",
                "user": "che111",
                "type": "user"
            },
            "summary": "The advent of large-scale vision foundation models, pre-trained on diverse\nnatural images, has marked a paradigm shift in computer vision. However, how\nthe frontier vision foundation models' efficacies transfer to specialized\ndomains remains such as medical imaging remains an open question. This report\ninvestigates whether DINOv3, a state-of-the-art self-supervised vision\ntransformer (ViT) that features strong capability in dense prediction tasks,\ncan directly serve as a powerful, unified encoder for medical vision tasks\nwithout domain-specific pre-training. To answer this, we benchmark DINOv3\nacross common medical vision tasks, including 2D/3D classification and\nsegmentation on a wide range of medical imaging modalities. We systematically\nanalyze its scalability by varying model sizes and input image resolutions. Our\nfindings reveal that DINOv3 shows impressive performance and establishes a\nformidable new baseline. Remarkably, it can even outperform medical-specific\nfoundation models like BiomedCLIP and CT-Net on several tasks, despite being\ntrained solely on natural images. However, we identify clear limitations: The\nmodel's features degrade in scenarios requiring deep domain specialization,\nsuch as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),\nand Positron Emission Tomography (PET). Furthermore, we observe that DINOv3\ndoes not consistently obey scaling law in the medical domain; performance does\nnot reliably increase with larger models or finer feature resolutions, showing\ndiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3\nas a strong baseline, whose powerful visual features can serve as a robust\nprior for multiple complex medical tasks. This opens promising future\ndirections, such as leveraging its features to enforce multiview consistency in\n3D reconstruction.",
            "upvotes": 28,
            "discussionId": "68bf97ab207285de11b07bb8",
            "ai_summary": "DINOv3, a self-supervised vision transformer, demonstrates strong performance across various medical vision tasks without domain-specific pre-training, though it shows limitations in deeply specialized domains and does not consistently follow scaling laws in the medical domain.",
            "ai_keywords": [
                "self-supervised vision transformer",
                "ViT",
                "dense prediction tasks",
                "medical vision tasks",
                "2D/3D classification",
                "segmentation",
                "medical imaging modalities",
                "scalability",
                "model sizes",
                "input image resolutions",
                "BiomedCLIP",
                "CT-Net",
                "Whole-Slide Pathological Images",
                "Electron Microscopy",
                "Positron Emission Tomography",
                "scaling law",
                "multiview consistency",
                "3D reconstruction"
            ]
        },
        "publishedAt": "2025-09-08T05:28:57.000Z",
        "title": "Does DINOv3 Set a New Medical Vision Standard?",
        "summary": "The advent of large-scale vision foundation models, pre-trained on diverse\nnatural images, has marked a paradigm shift in computer vision. However, how\nthe frontier vision foundation models' efficacies transfer to specialized\ndomains remains such as medical imaging remains an open question. This report\ninvestigates whether DINOv3, a state-of-the-art self-supervised vision\ntransformer (ViT) that features strong capability in dense prediction tasks,\ncan directly serve as a powerful, unified encoder for medical vision tasks\nwithout domain-specific pre-training. To answer this, we benchmark DINOv3\nacross common medical vision tasks, including 2D/3D classification and\nsegmentation on a wide range of medical imaging modalities. We systematically\nanalyze its scalability by varying model sizes and input image resolutions. Our\nfindings reveal that DINOv3 shows impressive performance and establishes a\nformidable new baseline. Remarkably, it can even outperform medical-specific\nfoundation models like BiomedCLIP and CT-Net on several tasks, despite being\ntrained solely on natural images. However, we identify clear limitations: The\nmodel's features degrade in scenarios requiring deep domain specialization,\nsuch as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),\nand Positron Emission Tomography (PET). Furthermore, we observe that DINOv3\ndoes not consistently obey scaling law in the medical domain; performance does\nnot reliably increase with larger models or finer feature resolutions, showing\ndiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3\nas a strong baseline, whose powerful visual features can serve as a robust\nprior for multiple complex medical tasks. This opens promising future\ndirections, such as leveraging its features to enforce multiview consistency in\n3D reconstruction.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06467.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "631b9ff5824f2502e3557c7e",
            "avatarUrl": "/avatars/076043c9dba07644a570692563ef8114.svg",
            "fullname": "liu",
            "name": "che111",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.01656",
            "authors": [
                {
                    "_id": "68beda55c123124955ef6267",
                    "name": "Zetong Zhou",
                    "hidden": false
                },
                {
                    "_id": "68beda55c123124955ef6268",
                    "name": "Dongping Chen",
                    "hidden": false
                },
                {
                    "_id": "68beda55c123124955ef6269",
                    "name": "Zixian Ma",
                    "hidden": false
                },
                {
                    "_id": "68beda55c123124955ef626a",
                    "name": "Zhihan Hu",
                    "hidden": false
                },
                {
                    "_id": "68beda55c123124955ef626b",
                    "name": "Mingyang Fu",
                    "hidden": false
                },
                {
                    "_id": "68beda55c123124955ef626c",
                    "name": "Sinan Wang",
                    "hidden": false
                },
                {
                    "_id": "68beda55c123124955ef626d",
                    "name": "Yao Wan",
                    "hidden": false
                },
                {
                    "_id": "68beda55c123124955ef626e",
                    "name": "Zhou Zhao",
                    "hidden": false
                },
                {
                    "_id": "68beda55c123124955ef626f",
                    "name": "Ranjay Krishna",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-01T17:57:49.000Z",
            "submittedOnDailyAt": "2025-09-09T00:10:59.899Z",
            "title": "Reinforced Visual Perception with Tools",
            "submittedOnDailyBy": {
                "_id": "643be8879f5d314db2d9ed23",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
                "isPro": false,
                "fullname": "Chen Dongping",
                "user": "shuaishuaicdp",
                "type": "user"
            },
            "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
            "upvotes": 26,
            "discussionId": "68beda55c123124955ef6270",
            "githubRepo": "https://github.com/ls-kelvin/REVPT",
            "ai_summary": "ReVPT enhances multi-modal LLMs' visual reasoning capabilities using reinforcement learning, achieving state-of-the-art performance on visual benchmarks.",
            "ai_keywords": [
                "LLMs",
                "vision models",
                "supervised finetuning",
                "reinforcement learning",
                "GRPO",
                "visual tools",
                "SAT",
                "CV-Bench",
                "BLINK",
                "MMStar",
                "instruct models"
            ],
            "githubStars": 28
        },
        "publishedAt": "2025-09-01T13:57:49.000Z",
        "title": "Reinforced Visual Perception with Tools",
        "summary": "Visual reasoning, a cornerstone of human intelligence, encompasses complex\nperceptual and logical processes essential for solving diverse visual problems.\nWhile advances in computer vision have produced powerful models for various\nperceptual tasks, leveraging these for general visual reasoning remains\nchallenging. Prior work demonstrates that augmenting LLMs with vision models\nvia supervised finetuning improves performance, but faces key limitations such\nas expensive data generation, reliance on careful data filtering, and poor\ngeneralization. To address these issues, we propose ReVPT to enhance\nmulti-modal LLMs' abilities to reason about and use visual tools through\nreinforcement learning. We introduce a novel RL algorithm based on GRPO,\ndesigned to train models to reason with a suite of four visual tools. Through\nextensive experiments, we show that our method achieves state-of-the-art\nperformance on several perception-heavy benchmarks, including SAT, CV-Bench,\nBLINK and MMStar, significantly outperforming the supervised and text-based RL\nfinetuning baselines. Notably, Our ReVPT-3B and ReVPT-7B outperform the\ninstruct models by 9.03% and 9.44% on CV-Bench. Finally, we bring to the\ncommunity new insights on RL-based visual tool-usage through extensive\nablations. Our code is available at https://github.com/ls-kelvin/REVPT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01656.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643be8879f5d314db2d9ed23",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643be8879f5d314db2d9ed23/VrW2UtJ7ppOnGIYjTWd7b.png",
            "fullname": "Chen Dongping",
            "name": "shuaishuaicdp",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 9
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06733",
            "authors": [
                {
                    "_id": "68bf8ec6207285de11b07b51",
                    "user": {
                        "_id": "6622245224f3842a31f7c58a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6622245224f3842a31f7c58a/EcvwtHamYwMdzRT9pevdE.jpeg",
                        "isPro": false,
                        "fullname": "wenjun",
                        "user": "wenjun-li",
                        "type": "user"
                    },
                    "name": "Wenjun Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:16.941Z",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b52",
                    "name": "Zhi Chen",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b53",
                    "name": "Jingru Lin",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b54",
                    "name": "Hannan Cao",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b55",
                    "name": "Wei Han",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b56",
                    "user": {
                        "_id": "6634aec8ed309a52116b0b17",
                        "avatarUrl": "/avatars/a913a312d0613089427734e6f491a784.svg",
                        "isPro": false,
                        "fullname": "Shengl",
                        "user": "Shengl02",
                        "type": "user"
                    },
                    "name": "Sheng Liang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:19.211Z",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b57",
                    "name": "Zhi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b58",
                    "name": "Kuicai Dong",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b59",
                    "name": "Dexun Li",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b5a",
                    "name": "Chen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68bf8ec6207285de11b07b5b",
                    "name": "Yong Liu",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6622245224f3842a31f7c58a/oCfHkt6BkeTDm98jxWtNm.png"
            ],
            "publishedAt": "2025-09-08T14:27:23.000Z",
            "submittedOnDailyAt": "2025-09-09T00:53:51.403Z",
            "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
            "submittedOnDailyBy": {
                "_id": "6622245224f3842a31f7c58a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6622245224f3842a31f7c58a/EcvwtHamYwMdzRT9pevdE.jpeg",
                "isPro": false,
                "fullname": "wenjun",
                "user": "wenjun-li",
                "type": "user"
            },
            "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.",
            "upvotes": 22,
            "discussionId": "68bf8ec6207285de11b07b5c",
            "githubRepo": "https://github.com/wenjunli-0/deepresearch-survey",
            "ai_summary": "Reinforcement learning is explored as a foundational approach for training deep research systems, addressing limitations of supervised and preference alignment methods by optimizing policies for tool interaction and exploration.",
            "ai_keywords": [
                "Reinforcement learning",
                "trajectory-level policies",
                "exploration",
                "recovery behaviors",
                "credit assignment",
                "multi-objective optimization",
                "multimodal integration",
                "agent architecture",
                "coordination",
                "evaluation",
                "benchmarks",
                "QA",
                "VQA",
                "long-form synthesis",
                "domain-grounded tasks"
            ],
            "githubStars": 10
        },
        "publishedAt": "2025-09-08T10:27:23.000Z",
        "title": "Reinforcement Learning Foundations for Deep Research Systems: A Survey",
        "summary": "Deep research systems, agentic AI that solve complex, multi-step tasks by\ncoordinating reasoning, search across the open web and user files, and tool\nuse, are moving toward hierarchical deployments with a Planner, Coordinator,\nand Executors. In practice, training entire stacks end-to-end remains\nimpractical, so most work trains a single planner connected to core tools such\nas search, browsing, and code. While SFT imparts protocol fidelity, it suffers\nfrom imitation and exposure biases and underuses environment feedback.\nPreference alignment methods such as DPO are schema and proxy-dependent,\noff-policy, and weak for long-horizon credit assignment and multi-objective\ntrade-offs. A further limitation of SFT and DPO is their reliance on human\ndefined decision points and subskills through schema design and labeled\ncomparisons. Reinforcement learning aligns with closed-loop, tool-interaction\nresearch by optimizing trajectory-level policies, enabling exploration,\nrecovery behaviors, and principled credit assignment, and it reduces dependence\non such human priors and rater biases.\n  This survey is, to our knowledge, the first dedicated to the RL foundations\nof deep research systems. It systematizes work after DeepSeek-R1 along three\naxes: (i) data synthesis and curation; (ii) RL methods for agentic research\ncovering stability, sample efficiency, long context handling, reward and credit\ndesign, multi-objective optimization, and multimodal integration; and (iii)\nagentic RL training systems and frameworks. We also cover agent architecture\nand coordination, as well as evaluation and benchmarks, including recent QA,\nVQA, long-form synthesis, and domain-grounded, tool-interaction tasks. We\ndistill recurring patterns, surface infrastructure bottlenecks, and offer\npractical guidance for training robust, transparent deep research agents with\nRL.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6622245224f3842a31f7c58a/oCfHkt6BkeTDm98jxWtNm.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06733.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6622245224f3842a31f7c58a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6622245224f3842a31f7c58a/EcvwtHamYwMdzRT9pevdE.jpeg",
            "fullname": "wenjun",
            "name": "wenjun-li",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06461",
            "authors": [
                {
                    "_id": "68bfa200207285de11b07beb",
                    "user": {
                        "_id": "656ad93853703dd78f3de7b8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
                        "isPro": false,
                        "fullname": "YuyaoGe",
                        "user": "YuyaoGe",
                        "type": "user"
                    },
                    "name": "Yuyao Ge",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:32.746Z",
                    "hidden": false
                },
                {
                    "_id": "68bfa200207285de11b07bec",
                    "name": "Shenghua Liu",
                    "hidden": false
                },
                {
                    "_id": "68bfa200207285de11b07bed",
                    "name": "Yiwei Wang",
                    "hidden": false
                },
                {
                    "_id": "68bfa200207285de11b07bee",
                    "name": "Lingrui Mei",
                    "hidden": false
                },
                {
                    "_id": "68bfa200207285de11b07bef",
                    "name": "Baolong Bi",
                    "hidden": false
                },
                {
                    "_id": "68bfa200207285de11b07bf0",
                    "name": "Xuanshan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68bfa200207285de11b07bf1",
                    "name": "Jiayu Yao",
                    "hidden": false
                },
                {
                    "_id": "68bfa200207285de11b07bf2",
                    "name": "Jiafeng Guo",
                    "hidden": false
                },
                {
                    "_id": "68bfa200207285de11b07bf3",
                    "name": "Xueqi Cheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T09:20:04.000Z",
            "submittedOnDailyAt": "2025-09-09T02:36:55.458Z",
            "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning",
            "submittedOnDailyBy": {
                "_id": "656ad93853703dd78f3de7b8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
                "isPro": false,
                "fullname": "YuyaoGe",
                "user": "YuyaoGe",
                "type": "user"
            },
            "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success across\ndiverse visual tasks, yet their performance degrades in complex visual\nenvironments. While existing enhancement approaches require additional\ntraining, rely on external segmentation tools, or operate at coarse-grained\nlevels, they overlook the innate ability within VLMs. To bridge this gap, we\ninvestigate VLMs' attention patterns and discover that: (1) visual complexity\nstrongly correlates with attention entropy, negatively impacting reasoning\nperformance; (2) attention progressively refines from global scanning in\nshallow layers to focused convergence in deeper layers, with convergence degree\ndetermined by visual complexity. (3) Theoretically, we prove that the contrast\nof attention maps between general queries and task-specific queries enables the\ndecomposition of visual signal into semantic signals and visual noise\ncomponents. Building on these insights, we propose Contrastive Attention\nRefinement for Visual Enhancement (CARVE), a training-free method that extracts\ntask-relevant visual signals through attention contrasting at the pixel level.\nExtensive experiments demonstrate that CARVE consistently enhances performance,\nachieving up to 75% improvement on open-source models. Our work provides\ncritical insights into the interplay between visual complexity and attention\nmechanisms, offering an efficient pathway for improving visual reasoning with\ncontrasting attention.",
            "upvotes": 14,
            "discussionId": "68bfa201207285de11b07bf4",
            "ai_summary": "Contrastive Attention Refinement for Visual Enhancement (CARVE) improves VLM performance by extracting task-relevant visual signals through attention contrasting, addressing issues with visual complexity and attention mechanisms.",
            "ai_keywords": [
                "Vision-Language Models",
                "attention patterns",
                "attention entropy",
                "reasoning performance",
                "attention maps",
                "general queries",
                "task-specific queries",
                "semantic signals",
                "visual noise",
                "Contrastive Attention Refinement",
                "CARVE",
                "visual enhancement",
                "visual complexity",
                "attention mechanisms",
                "visual reasoning"
            ]
        },
        "publishedAt": "2025-09-08T05:20:04.000Z",
        "title": "Focusing by Contrastive Attention: Enhancing VLMs' Visual Reasoning",
        "summary": "Vision-Language Models (VLMs) have demonstrated remarkable success across\ndiverse visual tasks, yet their performance degrades in complex visual\nenvironments. While existing enhancement approaches require additional\ntraining, rely on external segmentation tools, or operate at coarse-grained\nlevels, they overlook the innate ability within VLMs. To bridge this gap, we\ninvestigate VLMs' attention patterns and discover that: (1) visual complexity\nstrongly correlates with attention entropy, negatively impacting reasoning\nperformance; (2) attention progressively refines from global scanning in\nshallow layers to focused convergence in deeper layers, with convergence degree\ndetermined by visual complexity. (3) Theoretically, we prove that the contrast\nof attention maps between general queries and task-specific queries enables the\ndecomposition of visual signal into semantic signals and visual noise\ncomponents. Building on these insights, we propose Contrastive Attention\nRefinement for Visual Enhancement (CARVE), a training-free method that extracts\ntask-relevant visual signals through attention contrasting at the pixel level.\nExtensive experiments demonstrate that CARVE consistently enhances performance,\nachieving up to 75% improvement on open-source models. Our work provides\ncritical insights into the interplay between visual complexity and attention\nmechanisms, offering an efficient pathway for improving visual reasoning with\ncontrasting attention.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06461.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656ad93853703dd78f3de7b8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/656ad93853703dd78f3de7b8/r6VB6ICND1td3wFOy8pnz.jpeg",
            "fullname": "YuyaoGe",
            "name": "YuyaoGe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06155",
            "authors": [
                {
                    "_id": "68bf827d207285de11b07b2d",
                    "user": {
                        "_id": "64ae9b88a22a179fc4d07992",
                        "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "dorni",
                        "type": "user"
                    },
                    "name": "Duomin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:24.856Z",
                    "hidden": false
                },
                {
                    "_id": "68bf827d207285de11b07b2e",
                    "name": "Wei Zuo",
                    "hidden": false
                },
                {
                    "_id": "68bf827d207285de11b07b2f",
                    "name": "Aojie Li",
                    "hidden": false
                },
                {
                    "_id": "68bf827d207285de11b07b30",
                    "name": "Ling-Hao Chen",
                    "hidden": false
                },
                {
                    "_id": "68bf827d207285de11b07b31",
                    "name": "Xinyao Liao",
                    "hidden": false
                },
                {
                    "_id": "68bf827d207285de11b07b32",
                    "name": "Deyu Zhou",
                    "hidden": false
                },
                {
                    "_id": "68bf827d207285de11b07b33",
                    "name": "Zixin Yin",
                    "hidden": false
                },
                {
                    "_id": "68bf827d207285de11b07b34",
                    "name": "Xili Dai",
                    "hidden": false
                },
                {
                    "_id": "68bf827d207285de11b07b35",
                    "name": "Daxin Jiang",
                    "hidden": false
                },
                {
                    "_id": "68bf827d207285de11b07b36",
                    "name": "Gang Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-07T17:55:03.000Z",
            "submittedOnDailyAt": "2025-09-09T00:08:23.306Z",
            "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
            "submittedOnDailyBy": {
                "_id": "64ae9b88a22a179fc4d07992",
                "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
                "isPro": false,
                "fullname": "wang",
                "user": "dorni",
                "type": "user"
            },
            "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.",
            "upvotes": 14,
            "discussionId": "68bf827d207285de11b07b37",
            "projectPage": "https://dorniwang.github.io/UniVerse-1/",
            "githubRepo": "https://github.com/Dorniwang/UniVerse-1-code/",
            "ai_summary": "UniVerse-1, a unified audio-video generation model, uses a stitching of experts technique to combine pre-trained video and music models, ensuring accurate temporal alignment and producing high-quality audio-visual outputs.",
            "ai_keywords": [
                "unified model",
                "Veo-3-like model",
                "stitching of experts (SoE)",
                "pre-trained video generation",
                "pre-trained music generation",
                "online annotation pipeline",
                "temporal alignment",
                "ambient sounds",
                "speech generation",
                "Verse-Bench",
                "audio-video generation"
            ],
            "githubStars": 42
        },
        "publishedAt": "2025-09-07T13:55:03.000Z",
        "title": "UniVerse-1: Unified Audio-Video Generation via Stitching of Experts",
        "summary": "We introduce UniVerse-1, a unified, Veo-3-like model capable of\nsimultaneously generating coordinated audio and video. To enhance training\nefficiency, we bypass training from scratch and instead employ a stitching of\nexperts (SoE) technique. This approach deeply fuses the corresponding blocks of\npre-trained video and music generation experts models, thereby fully leveraging\ntheir foundational capabilities. To ensure accurate annotations and temporal\nalignment for both ambient sounds and speech with video content, we developed\nan online annotation pipeline that processes the required training data and\ngenerates labels during training process. This strategy circumvents the\nperformance degradation often caused by misalignment text-based annotations.\nThrough the synergy of these techniques, our model, after being finetuned on\napproximately 7,600 hours of audio-video data, produces results with\nwell-coordinated audio-visuals for ambient sounds generation and strong\nalignment for speech generation. To systematically evaluate our proposed\nmethod, we introduce Verse-Bench, a new benchmark dataset. In an effort to\nadvance research in audio-video generation and to close the performance gap\nwith state-of-the-art models such as Veo3, we make our model and code publicly\navailable. We hope this contribution will benefit the broader research\ncommunity. Project page: https://dorniwang.github.io/UniVerse-1/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06155.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ae9b88a22a179fc4d07992",
            "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
            "fullname": "wang",
            "name": "dorni",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06917",
            "authors": [
                {
                    "_id": "68bf93a8207285de11b07b87",
                    "name": "Jiacheng Miao",
                    "hidden": false
                },
                {
                    "_id": "68bf93a8207285de11b07b88",
                    "name": "Joe R. Davis",
                    "hidden": false
                },
                {
                    "_id": "68bf93a8207285de11b07b89",
                    "name": "Jonathan K. Pritchard",
                    "hidden": false
                },
                {
                    "_id": "68bf93a8207285de11b07b8a",
                    "name": "James Zou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T17:28:42.000Z",
            "submittedOnDailyAt": "2025-09-09T01:10:59.723Z",
            "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.",
            "upvotes": 12,
            "discussionId": "68bf93a8207285de11b07b8b",
            "ai_summary": "Paper2Agent converts research papers into interactive AI agents to facilitate knowledge dissemination and enable complex scientific queries through natural language.",
            "ai_keywords": [
                "AI agents",
                "Model Context Protocol (MCP)",
                "chat agent",
                "AlphaGenome",
                "ScanPy",
                "TISSUE",
                "single-cell analysis",
                "spatial transcriptomics"
            ]
        },
        "publishedAt": "2025-09-08T13:28:42.000Z",
        "title": "Paper2Agent: Reimagining Research Papers As Interactive and Reliable AI\n  Agents",
        "summary": "We introduce Paper2Agent, an automated framework that converts research\npapers into AI agents. Paper2Agent transforms research output from passive\nartifacts into active systems that can accelerate downstream use, adoption, and\ndiscovery. Conventional research papers require readers to invest substantial\neffort to understand and adapt a paper's code, data, and methods to their own\nwork, creating barriers to dissemination and reuse. Paper2Agent addresses this\nchallenge by automatically converting a paper into an AI agent that acts as a\nknowledgeable research assistant. It systematically analyzes the paper and the\nassociated codebase using multiple agents to construct a Model Context Protocol\n(MCP) server, then iteratively generates and runs tests to refine and robustify\nthe resulting MCP. These paper MCPs can then be flexibly connected to a chat\nagent (e.g. Claude Code) to carry out complex scientific queries through\nnatural language while invoking tools and workflows from the original paper. We\ndemonstrate Paper2Agent's effectiveness in creating reliable and capable paper\nagents through in-depth case studies. Paper2Agent created an agent that\nleverages AlphaGenome to interpret genomic variants and agents based on ScanPy\nand TISSUE to carry out single-cell and spatial transcriptomics analyses. We\nvalidate that these paper agents can reproduce the original paper's results and\ncan correctly carry out novel user queries. By turning static papers into\ndynamic, interactive AI agents, Paper2Agent introduces a new paradigm for\nknowledge dissemination and a foundation for the collaborative ecosystem of AI\nco-scientists.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06917.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 102
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.02108",
            "authors": [
                {
                    "_id": "68c04e3456a3252309f75ce7",
                    "name": "Touayouch Brahim",
                    "hidden": false
                },
                {
                    "_id": "68c04e3456a3252309f75ce8",
                    "name": "Fosse Loïc",
                    "hidden": false
                },
                {
                    "_id": "68c04e3456a3252309f75ce9",
                    "name": "Damnati Géraldine",
                    "hidden": false
                },
                {
                    "_id": "68c04e3456a3252309f75cea",
                    "name": "Lecorvé Gwénolé",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/62bab79c7a8e2d222fb3dc1a/aU3Nsw07GCQGhH4E77S_p.png"
            ],
            "publishedAt": "2025-09-02T09:04:41.000Z",
            "submittedOnDailyAt": "2025-09-09T14:43:08.213Z",
            "title": "DivMerge: A divergence-based model merging method for multi-tasking",
            "submittedOnDailyBy": {
                "_id": "62bab79c7a8e2d222fb3dc1a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656403850135-noauth.png",
                "isPro": false,
                "fullname": "Gwénolé Lecorvé",
                "user": "glecorve",
                "type": "user"
            },
            "summary": "Multi-task learning (MTL) is often achieved by merging datasets before\nfine-tuning, but the growing availability of fine-tuned models has led to new\napproaches such as model merging via task arithmetic. A major challenge in this\nsetting is task interference, which worsens as the number of tasks increases.\nWe propose a method that merges models trained on different tasks into a single\nmodel, maintaining strong performance across all tasks. Our approach leverages\nJensen-Shannon divergence to guide the merging process without requiring\nadditional labelled data, and automatically balances task importance. Unlike\nexisting methods, our approach remains robust as the number of tasks grows and\nconsistently outperforms prior work.",
            "upvotes": 11,
            "discussionId": "68c04e3456a3252309f75ceb"
        },
        "publishedAt": "2025-09-02T05:04:41.000Z",
        "title": "DivMerge: A divergence-based model merging method for multi-tasking",
        "summary": "Multi-task learning (MTL) is often achieved by merging datasets before\nfine-tuning, but the growing availability of fine-tuned models has led to new\napproaches such as model merging via task arithmetic. A major challenge in this\nsetting is task interference, which worsens as the number of tasks increases.\nWe propose a method that merges models trained on different tasks into a single\nmodel, maintaining strong performance across all tasks. Our approach leverages\nJensen-Shannon divergence to guide the merging process without requiring\nadditional labelled data, and automatically balances task importance. Unlike\nexisting methods, our approach remains robust as the number of tasks grows and\nconsistently outperforms prior work.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/62bab79c7a8e2d222fb3dc1a/aU3Nsw07GCQGhH4E77S_p.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.02108.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62bab79c7a8e2d222fb3dc1a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1656403850135-noauth.png",
            "fullname": "Gwénolé Lecorvé",
            "name": "glecorve",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.03516",
            "authors": [
                {
                    "_id": "68b93427d43cadaf7a688bd1",
                    "user": {
                        "_id": "666c06ca9ed9e91df03e7e27",
                        "avatarUrl": "/avatars/3101ad4f1a3a243b75080d06c5e59e9c.svg",
                        "isPro": false,
                        "fullname": "Ouxiang Li",
                        "user": "lioooox",
                        "type": "user"
                    },
                    "name": "Ouxiang Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-04T08:42:54.336Z",
                    "hidden": false
                },
                {
                    "_id": "68b93427d43cadaf7a688bd2",
                    "name": "Yuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68b93427d43cadaf7a688bd3",
                    "name": "Xinting Hu",
                    "hidden": false
                },
                {
                    "_id": "68b93427d43cadaf7a688bd4",
                    "name": "Huijuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68b93427d43cadaf7a688bd5",
                    "name": "Rui Chen",
                    "hidden": false
                },
                {
                    "_id": "68b93427d43cadaf7a688bd6",
                    "name": "Jiarong Ou",
                    "hidden": false
                },
                {
                    "_id": "68b93427d43cadaf7a688bd7",
                    "name": "Xin Tao",
                    "hidden": false
                },
                {
                    "_id": "68b93427d43cadaf7a688bd8",
                    "name": "Pengfei Wan",
                    "hidden": false
                },
                {
                    "_id": "68b93427d43cadaf7a688bd9",
                    "name": "Fuli Feng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-03T17:58:12.000Z",
            "submittedOnDailyAt": "2025-09-09T01:08:55.234Z",
            "title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?",
            "submittedOnDailyBy": {
                "_id": "666c06ca9ed9e91df03e7e27",
                "avatarUrl": "/avatars/3101ad4f1a3a243b75080d06c5e59e9c.svg",
                "isPro": false,
                "fullname": "Ouxiang Li",
                "user": "lioooox",
                "type": "user"
            },
            "summary": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/.",
            "upvotes": 10,
            "discussionId": "68b93428d43cadaf7a688bda",
            "projectPage": "https://t2i-corebench.github.io/",
            "githubRepo": "https://github.com/KwaiVGI/T2I-CoReBench",
            "ai_summary": "T2I-CoReBench is a benchmark that evaluates the composition and reasoning capabilities of text-to-image models using a comprehensive and complex set of prompts and checklist questions.",
            "ai_keywords": [
                "text-to-image (T2I) generation",
                "composition",
                "reasoning",
                "scene graph elements",
                "inference",
                "deductive",
                "inductive",
                "abductive",
                "evaluation taxonomy",
                "compositional density",
                "multi-step inference",
                "checklist questions"
            ],
            "githubStars": 12
        },
        "publishedAt": "2025-09-03T13:58:12.000Z",
        "title": "Easier Painting Than Thinking: Can Text-to-Image Models Set the Stage,\n  but Not Direct the Play?",
        "summary": "Text-to-image (T2I) generation aims to synthesize images from textual\nprompts, which jointly specify what must be shown and imply what can be\ninferred, thereby corresponding to two core capabilities: composition and\nreasoning. However, with the emerging advances of T2I models in reasoning\nbeyond composition, existing benchmarks reveal clear limitations in providing\ncomprehensive evaluations across and within these capabilities. Meanwhile,\nthese advances also enable models to handle more complex prompts, whereas\ncurrent benchmarks remain limited to low scene density and simplified\none-to-one reasoning. To address these limitations, we propose T2I-CoReBench, a\ncomprehensive and complex benchmark that evaluates both composition and\nreasoning capabilities of T2I models. To ensure comprehensiveness, we structure\ncomposition around scene graph elements (instance, attribute, and relation) and\nreasoning around the philosophical framework of inference (deductive,\ninductive, and abductive), formulating a 12-dimensional evaluation taxonomy. To\nincrease complexity, driven by the inherent complexities of real-world\nscenarios, we curate each prompt with high compositional density for\ncomposition and multi-step inference for reasoning. We also pair each prompt\nwith a checklist that specifies individual yes/no questions to assess each\nintended element independently to facilitate fine-grained and reliable\nevaluation. In statistics, our benchmark comprises 1,080 challenging prompts\nand around 13,500 checklist questions. Experiments across 27 current T2I models\nreveal that their composition capability still remains limited in complex\nhigh-density scenarios, while the reasoning capability lags even further behind\nas a critical bottleneck, with all models struggling to infer implicit elements\nfrom prompts. Our project page: https://t2i-corebench.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03516.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "666c06ca9ed9e91df03e7e27",
            "avatarUrl": "/avatars/3101ad4f1a3a243b75080d06c5e59e9c.svg",
            "fullname": "Ouxiang Li",
            "name": "lioooox",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06945",
            "authors": [
                {
                    "_id": "68bfaf59207285de11b07cb4",
                    "user": {
                        "_id": "67dc162ec8c00778e8689f42",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
                        "isPro": false,
                        "fullname": "Wenxuan Huang",
                        "user": "Osilly",
                        "type": "user"
                    },
                    "name": "Wenxuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:14.259Z",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cb5",
                    "name": "Shuang Chen",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cb6",
                    "name": "Zheyong Xie",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cb7",
                    "name": "Shaosheng Cao",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cb8",
                    "name": "Shixiang Tang",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cb9",
                    "name": "Yufan Shen",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cba",
                    "name": "Qingyu Yin",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cbb",
                    "name": "Wenbo Hu",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cbc",
                    "name": "Xiaoman Wang",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cbd",
                    "name": "Yuntian Tang",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cbe",
                    "name": "Junbo Qiao",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cbf",
                    "name": "Yue Guo",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cc0",
                    "name": "Yao Hu",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cc1",
                    "name": "Zhenfei Yin",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cc2",
                    "name": "Philip Torr",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cc3",
                    "name": "Yu Cheng",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cc4",
                    "name": "Wanli Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68bfaf59207285de11b07cc5",
                    "name": "Shaohui Lin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T17:56:23.000Z",
            "submittedOnDailyAt": "2025-09-09T03:17:00.247Z",
            "title": "Interleaving Reasoning for Better Text-to-Image Generation",
            "submittedOnDailyBy": {
                "_id": "67dc162ec8c00778e8689f42",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
                "isPro": false,
                "fullname": "Wenxuan Huang",
                "user": "Osilly",
                "type": "user"
            },
            "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
            "upvotes": 8,
            "discussionId": "68bfaf59207285de11b07cc6",
            "githubRepo": "https://github.com/Osilly/Interleaving-Reasoning-Generation",
            "ai_summary": "Interleaving Reasoning Generation (IRG) framework alternates between text-based thinking and image synthesis to improve Text-to-Image generation, achieving state-of-the-art performance and enhanced visual quality.",
            "ai_keywords": [
                "Interleaving Reasoning Generation",
                "IRG",
                "Interleaving Reasoning Generation Learning",
                "IRGL",
                "text-based thinking",
                "image synthesis",
                "GenEval",
                "WISE",
                "TIIF",
                "GenAI-Bench",
                "OneIG-EN"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-09-08T13:56:23.000Z",
        "title": "Interleaving Reasoning for Better Text-to-Image Generation",
        "summary": "Unified multimodal understanding and generation models recently have achieve\nsignificant improvement in image generation capability, yet a large gap remains\nin instruction following and detail preservation compared to systems that\ntightly couple comprehension with generation such as GPT-4o. Motivated by\nrecent advances in interleaving reasoning, we explore whether such reasoning\ncan further improve Text-to-Image (T2I) generation. We introduce Interleaving\nReasoning Generation (IRG), a framework that alternates between text-based\nthinking and image synthesis: the model first produces a text-based thinking to\nguide an initial image, then reflects on the result to refine fine-grained\ndetails, visual quality, and aesthetics while preserving semantics. To train\nIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),\nwhich targets two sub-goals: (1) strengthening the initial think-and-generate\nstage to establish core content and base quality, and (2) enabling high-quality\ntextual reflection and faithful implementation of those refinements in a\nsubsequent image. We curate IRGL-300K, a dataset organized into six decomposed\nlearning modes that jointly cover learning text-based thinking, and full\nthinking-image trajectories. Starting from a unified foundation model that\nnatively emits interleaved text-image outputs, our two-stage training first\nbuilds robust thinking and reflection, then efficiently tunes the IRG pipeline\nin the full thinking-image trajectory data. Extensive experiments show SoTA\nperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,\nGenAI-Bench, and OneIG-EN, alongside substantial improvements in visual quality\nand fine-grained fidelity. The code, model weights and datasets will be\nreleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06945.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67dc162ec8c00778e8689f42",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67dc162ec8c00778e8689f42/_y_tO6W3ONOkOWbumAFXA.png",
            "fullname": "Wenxuan Huang",
            "name": "Osilly",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06631",
            "authors": [
                {
                    "_id": "68bf9f54207285de11b07be2",
                    "name": "Özgür Uğur",
                    "hidden": false
                },
                {
                    "_id": "68bf9f54207285de11b07be3",
                    "name": "Musa Yılmaz",
                    "hidden": false
                },
                {
                    "_id": "68bf9f54207285de11b07be4",
                    "name": "Esra Şavirdi",
                    "hidden": false
                },
                {
                    "_id": "68bf9f54207285de11b07be5",
                    "user": {
                        "_id": "6464e76894327a238f56d7ff",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6464e76894327a238f56d7ff/pKsurCt64lH_0pb_nvjoE.jpeg",
                        "isPro": false,
                        "fullname": "Özay Ezerceli",
                        "user": "ozayezerceli",
                        "type": "user"
                    },
                    "name": "Özay Ezerceli",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:35.221Z",
                    "hidden": false
                },
                {
                    "_id": "68bf9f54207285de11b07be6",
                    "user": {
                        "_id": "6422eab8e2029ade06eeee2c",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                        "isPro": false,
                        "fullname": "Mahmud ElHuseyni 🇵🇸",
                        "user": "MElHuseyni",
                        "type": "user"
                    },
                    "name": "Mahmut El Huseyni",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:44.299Z",
                    "hidden": false
                },
                {
                    "_id": "68bf9f54207285de11b07be7",
                    "name": "Selva Taş",
                    "hidden": false
                },
                {
                    "_id": "68bf9f54207285de11b07be8",
                    "user": {
                        "_id": "64a52413fa840dd18c2f76fe",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64a52413fa840dd18c2f76fe/5PD-BHnKm2aQWFZhE7P2I.jpeg",
                        "isPro": false,
                        "fullname": "Reyhan Bayraktar",
                        "user": "byrayhana",
                        "type": "user"
                    },
                    "name": "Reyhan Bayraktar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:42.094Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T12:51:40.000Z",
            "submittedOnDailyAt": "2025-09-09T12:22:02.261Z",
            "title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation",
            "submittedOnDailyBy": {
                "_id": "6422eab8e2029ade06eeee2c",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
                "isPro": false,
                "fullname": "Mahmud ElHuseyni 🇵🇸",
                "user": "MElHuseyni",
                "type": "user"
            },
            "summary": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment.",
            "upvotes": 5,
            "discussionId": "68bf9f55207285de11b07be9",
            "ai_summary": "Guided decoding methods in Retrieval-Augmented Generation (RAG) systems are evaluated for structured output generation, revealing performance variations across different prompting setups.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "Retrieval-Augmented Generation (RAG)",
                "guided decoding",
                "Outlines",
                "XGrammar",
                "LM Format Enforcer",
                "multi-turn prompting",
                "success rates",
                "hallucination rates",
                "output quality"
            ]
        },
        "publishedAt": "2025-09-08T08:51:40.000Z",
        "title": "Guided Decoding and Its Critical Role in Retrieval-Augmented Generation",
        "summary": "The integration of Large Language Models (LLMs) into various applications has\ndriven the need for structured and reliable responses. A key challenge in\nRetrieval-Augmented Generation (RAG) systems is ensuring that outputs align\nwith expected formats while minimizing hallucinations. This study examines the\nrole of guided decoding in RAG systems, comparing three methods, Outlines,\nXGrammar, and LM Format Enforcer, across different multi-turn prompting setups\n(0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates,\nand output quality, we provide insights into their performance and\napplicability. Our findings reveal how multi-turn interactions influence guided\ndecoding, uncovering unexpected performance variations that can inform method\nselection for specific use cases. This work advances the understanding of\nstructured output generation in RAG systems, offering both theoretical insights\nand practical guidance for LLM deployment.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06631.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6422eab8e2029ade06eeee2c",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6422eab8e2029ade06eeee2c/Gai3BHr2WJ0YuhdumqQ_z.png",
            "fullname": "Mahmud ElHuseyni 🇵🇸",
            "name": "MElHuseyni",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06493",
            "authors": [
                {
                    "_id": "68bf940c207285de11b07b8d",
                    "name": "Ran Xin",
                    "hidden": false
                },
                {
                    "_id": "68bf940c207285de11b07b8e",
                    "name": "Zeyu Zheng",
                    "hidden": false
                },
                {
                    "_id": "68bf940c207285de11b07b8f",
                    "name": "Yanchen Nie",
                    "hidden": false
                },
                {
                    "_id": "68bf940c207285de11b07b90",
                    "name": "Kun Yuan",
                    "hidden": false
                },
                {
                    "_id": "68bf940c207285de11b07b91",
                    "name": "Xia Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T09:54:18.000Z",
            "submittedOnDailyAt": "2025-09-09T01:12:27.071Z",
            "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces BFS-Prover-V2, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. BFS-Prover-V2 achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
            "upvotes": 5,
            "discussionId": "68bf940d207285de11b07b92",
            "ai_summary": "BFS-Prover-V2 addresses scaling challenges in automated theorem proving by integrating a multi-turn off-policy RL framework and a planner-enhanced multi-agent search architecture, achieving state-of-the-art results on formal mathematics benchmarks.",
            "ai_keywords": [
                "Large Language Models",
                "automated theorem proving",
                "reinforcement learning",
                "off-policy RL",
                "AlphaZero",
                "multi-stage expert iteration",
                "adaptive tactic-level data filtering",
                "periodic retraining",
                "planner-enhanced multi-agent search",
                "general reasoning model",
                "high-level planner",
                "subgoals",
                "shared proof cache",
                "MiniF2F",
                "ProofNet"
            ]
        },
        "publishedAt": "2025-09-08T05:54:18.000Z",
        "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
        "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces BFS-Prover-V2, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. BFS-Prover-V2 achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06493.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 102
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06861",
            "authors": [
                {
                    "_id": "68bfded0207285de11b07df2",
                    "name": "James Xu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68bfded0207285de11b07df3",
                    "name": "Bryan Hooi",
                    "hidden": false
                },
                {
                    "_id": "68bfded0207285de11b07df4",
                    "name": "See-Kiong Ng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T16:28:25.000Z",
            "submittedOnDailyAt": "2025-09-09T06:31:58.268Z",
            "title": "Test-Time Scaling in Reasoning Models Is Not Effective for\n  Knowledge-Intensive Tasks Yet",
            "submittedOnDailyBy": {
                "_id": "64ca18318d2d187c24df20ec",
                "avatarUrl": "/avatars/cada297547bf4c84934c6196d2ee6abd.svg",
                "isPro": false,
                "fullname": "James X. Zhao",
                "user": "JamesXZ",
                "type": "user"
            },
            "summary": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge",
            "upvotes": 4,
            "discussionId": "68bfded1207285de11b07df5",
            "githubRepo": "https://github.com/XuZhao0/tts-knowledge",
            "ai_summary": "Test-time scaling does not consistently improve accuracy or reduce hallucinations in knowledge-intensive tasks, often leading to overconfident errors.",
            "ai_keywords": [
                "test-time scaling",
                "inference-time computation",
                "reasoning chains",
                "knowledge-intensive tasks",
                "factual accuracy",
                "hallucination rates",
                "reasoning models",
                "confirmation bias"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-08T12:28:25.000Z",
        "title": "Test-Time Scaling in Reasoning Models Is Not Effective for\n  Knowledge-Intensive Tasks Yet",
        "summary": "Test-time scaling increases inference-time computation by allowing models to\ngenerate long reasoning chains, and has shown strong performance across many\ndomains. However, in this work, we show that this approach is not yet effective\nfor knowledge-intensive tasks, where high factual accuracy and low\nhallucination rates are essential. We conduct a comprehensive evaluation of\ntest-time scaling using 12 reasoning models on two knowledge-intensive\nbenchmarks. Our results reveal that increasing test-time computation does not\nconsistently improve accuracy and, in many cases, it even leads to more\nhallucinations. We then analyze how extended reasoning affects hallucination\nbehavior. We find that reduced hallucinations often result from the model\nchoosing to abstain after thinking more, rather than from improved factual\nrecall. Conversely, for some models, longer reasoning encourages attempts on\npreviously unanswered questions, many of which result in hallucinations. Case\nstudies show that extended reasoning can induce confirmation bias, leading to\noverconfident hallucinations. Despite these limitations, we observe that\ncompared to non-thinking, enabling thinking remains beneficial. Code and data\nare available at https://github.com/XuZhao0/tts-knowledge",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06861.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ca18318d2d187c24df20ec",
            "avatarUrl": "/avatars/cada297547bf4c84934c6196d2ee6abd.svg",
            "fullname": "James X. Zhao",
            "name": "JamesXZ",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06786",
            "authors": [
                {
                    "_id": "68bf8d90207285de11b07b4a",
                    "name": "Youbang Sun",
                    "hidden": false
                },
                {
                    "_id": "68bf8d90207285de11b07b4b",
                    "user": {
                        "_id": "65fca775fa59bdf4737b1a84",
                        "avatarUrl": "/avatars/a161b510bde8f57e7686cbb0b4aa6a52.svg",
                        "isPro": false,
                        "fullname": "Xiang Wang",
                        "user": "xiangwang1223",
                        "type": "user"
                    },
                    "name": "Xiang Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:21.076Z",
                    "hidden": false
                },
                {
                    "_id": "68bf8d90207285de11b07b4c",
                    "name": "Jie Fu",
                    "hidden": false
                },
                {
                    "_id": "68bf8d90207285de11b07b4d",
                    "name": "Chaochao Lu",
                    "hidden": false
                },
                {
                    "_id": "68bf8d90207285de11b07b4e",
                    "name": "Bowen Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T15:13:23.000Z",
            "submittedOnDailyAt": "2025-09-09T00:56:07.515Z",
            "title": "R^textbf{2AI}: Towards Resistant and Resilient AI in an\n  Evolving World",
            "submittedOnDailyBy": {
                "_id": "679ce8c048ebd7903d76a832",
                "avatarUrl": "/avatars/5f3fecaacfee6e2d5a72dd19fe87055a.svg",
                "isPro": false,
                "fullname": "Youbang Sun",
                "user": "Youbang",
                "type": "user"
            },
            "summary": "In this position paper, we address the persistent gap between rapidly growing\nAI capabilities and lagging safety progress. Existing paradigms divide into\n``Make AI Safe'', which applies post-hoc alignment and guardrails but remains\nbrittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety\nbut struggles to address unforeseen risks in open-ended environments. We\ntherefore propose safe-by-coevolution as a new formulation of the\n``Make Safe AI'' paradigm, inspired by biological immunity, in which safety\nbecomes a dynamic, adversarial, and ongoing learning process. To operationalize\nthis vision, we introduce R^2AI -- Resistant and Resilient\nAI -- as a practical framework that unites resistance against known threats\nwith resilience to unforeseen risks. R^2AI integrates fast\nand slow safe models, adversarial simulation and verification through a\nsafety wind tunnel, and continual feedback loops that guide safety and\ncapability to coevolve. We argue that this framework offers a scalable and\nproactive path to maintain continual safety in dynamic environments, addressing\nboth near-term vulnerabilities and long-term existential risks as AI advances\ntoward AGI and ASI.",
            "upvotes": 3,
            "discussionId": "68bf8d90207285de11b07b4f",
            "ai_summary": "A new framework, R²AI, is proposed to enhance AI safety through coevolution, combining resistance to known threats with resilience to unforeseen risks using fast and slow safe models and adversarial simulation.",
            "ai_keywords": [
                "safe-by-coevolution",
                "R²AI",
                "Resistant and Resilient AI",
                "fast and slow safe models",
                "safety wind tunnel",
                "continual feedback loops",
                "AGI",
                "ASI"
            ]
        },
        "publishedAt": "2025-09-08T11:13:23.000Z",
        "title": "R^textbf{2AI}: Towards Resistant and Resilient AI in an\n  Evolving World",
        "summary": "In this position paper, we address the persistent gap between rapidly growing\nAI capabilities and lagging safety progress. Existing paradigms divide into\n``Make AI Safe'', which applies post-hoc alignment and guardrails but remains\nbrittle and reactive, and ``Make Safe AI'', which emphasizes intrinsic safety\nbut struggles to address unforeseen risks in open-ended environments. We\ntherefore propose safe-by-coevolution as a new formulation of the\n``Make Safe AI'' paradigm, inspired by biological immunity, in which safety\nbecomes a dynamic, adversarial, and ongoing learning process. To operationalize\nthis vision, we introduce R^2AI -- Resistant and Resilient\nAI -- as a practical framework that unites resistance against known threats\nwith resilience to unforeseen risks. R^2AI integrates fast\nand slow safe models, adversarial simulation and verification through a\nsafety wind tunnel, and continual feedback loops that guide safety and\ncapability to coevolve. We argue that this framework offers a scalable and\nproactive path to maintain continual safety in dynamic environments, addressing\nboth near-term vulnerabilities and long-term existential risks as AI advances\ntoward AGI and ASI.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06786.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "679ce8c048ebd7903d76a832",
            "avatarUrl": "/avatars/5f3fecaacfee6e2d5a72dd19fe87055a.svg",
            "fullname": "Youbang Sun",
            "name": "Youbang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06771",
            "authors": [
                {
                    "_id": "68bf971c207285de11b07b9d",
                    "user": {
                        "_id": "651692d718f3a57f869a5a0a",
                        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
                        "isPro": false,
                        "fullname": "Sai Kartheek Reddy",
                        "user": "UVSKKR",
                        "type": "user"
                    },
                    "name": "Sai Kartheek Reddy Kasu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:52.150Z",
                    "hidden": false
                },
                {
                    "_id": "68bf971c207285de11b07b9e",
                    "name": "Mohammad Zia Ur Rehman",
                    "hidden": false
                },
                {
                    "_id": "68bf971c207285de11b07b9f",
                    "name": "Shahid Shafi Dar",
                    "hidden": false
                },
                {
                    "_id": "68bf971c207285de11b07ba0",
                    "name": "Rishi Bharat Junghare",
                    "hidden": false
                },
                {
                    "_id": "68bf971c207285de11b07ba1",
                    "name": "Dhanvin Sanjay Namboodiri",
                    "hidden": false
                },
                {
                    "_id": "68bf971c207285de11b07ba2",
                    "name": "Nagendra Kumar",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T14:55:16.000Z",
            "submittedOnDailyAt": "2025-09-09T01:28:57.435Z",
            "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning",
            "submittedOnDailyBy": {
                "_id": "651692d718f3a57f869a5a0a",
                "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
                "isPro": false,
                "fullname": "Sai Kartheek Reddy",
                "user": "UVSKKR",
                "type": "user"
            },
            "summary": "Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
            "upvotes": 3,
            "discussionId": "68bf971c207285de11b07ba3",
            "githubRepo": "https://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning/",
            "ai_summary": "A reasoning-augmented framework using a Large Vision-Language Model and a Tri-stream Cross-Reasoning Network achieves superior performance in detecting dark humor, identifying targets, and predicting intensity in multimodal memes.",
            "ai_keywords": [
                "Large Vision-Language Model",
                "Role-Reversal Self-Loop",
                "text encoder",
                "vision transformer",
                "Tri-stream Cross-Reasoning Network",
                "pairwise attention mechanisms"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-08T10:55:16.000Z",
        "title": "D-HUMOR: Dark Humor Understanding via Multimodal Open-ended Reasoning",
        "summary": "Dark humor in online memes poses unique challenges due to its reliance on\nimplicit, sensitive, and culturally contextual cues. To address the lack of\nresources and methods for detecting dark humor in multimodal content, we\nintroduce a novel dataset of 4,379 Reddit memes annotated for dark humor,\ntarget category (gender, mental health, violence, race, disability, and other),\nand a three-level intensity rating (mild, moderate, severe). Building on this\nresource, we propose a reasoning-augmented framework that first generates\nstructured explanations for each meme using a Large Vision-Language Model\n(VLM). Through a Role-Reversal Self-Loop, VLM adopts the author's perspective\nto iteratively refine its explanations, ensuring completeness and alignment. We\nthen extract textual features from both the OCR transcript and the self-refined\nreasoning via a text encoder, while visual features are obtained using a vision\ntransformer. A Tri-stream Cross-Reasoning Network (TCRNet) fuses these three\nstreams, text, image, and reasoning, via pairwise attention mechanisms,\nproducing a unified representation for classification. Experimental results\ndemonstrate that our approach outperforms strong baselines across three tasks:\ndark humor detection, target identification, and intensity prediction. The\ndataset, annotations, and code are released to facilitate further research in\nmultimodal humor understanding and content moderation. Code and Dataset are\navailable at:\nhttps://github.com/Sai-Kartheek-Reddy/D-Humor-Dark-Humor-Understanding-via-Multimodal-Open-ended-Reasoning",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06771.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651692d718f3a57f869a5a0a",
            "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
            "fullname": "Sai Kartheek Reddy",
            "name": "UVSKKR",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.05668",
            "authors": [
                {
                    "_id": "68bf9108207285de11b07b62",
                    "name": "Michael Hoffmann",
                    "hidden": false
                },
                {
                    "_id": "68bf9108207285de11b07b63",
                    "name": "Jophin John",
                    "hidden": false
                },
                {
                    "_id": "68bf9108207285de11b07b64",
                    "user": {
                        "_id": "5e6a3d4ea9afd5125d9ec064",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                        "isPro": true,
                        "fullname": "Stefan Schweter",
                        "user": "stefan-it",
                        "type": "user"
                    },
                    "name": "Stefan Schweter",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:14.824Z",
                    "hidden": false
                },
                {
                    "_id": "68bf9108207285de11b07b65",
                    "name": "Gokul Ramakrishnan",
                    "hidden": false
                },
                {
                    "_id": "68bf9108207285de11b07b66",
                    "name": "Hoi-Fong Mak",
                    "hidden": false
                },
                {
                    "_id": "68bf9108207285de11b07b67",
                    "name": "Alice Zhang",
                    "hidden": false
                },
                {
                    "_id": "68bf9108207285de11b07b68",
                    "name": "Dmitry Gaynullin",
                    "hidden": false
                },
                {
                    "_id": "68bf9108207285de11b07b69",
                    "name": "Nicolay J. Hammer",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-06T10:12:52.000Z",
            "submittedOnDailyAt": "2025-09-09T01:00:32.896Z",
            "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English\n  and Bavarian",
            "submittedOnDailyBy": {
                "_id": "5e6a3d4ea9afd5125d9ec064",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
                "isPro": true,
                "fullname": "Stefan Schweter",
                "user": "stefan-it",
                "type": "user"
            },
            "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.",
            "upvotes": 3,
            "discussionId": "68bf9108207285de11b07b6a",
            "ai_summary": "Llama-GENBA-10B, a trilingual foundation model, addresses English-centric bias by balancing English, German, and Bavarian training, achieving strong cross-lingual performance and setting new benchmarks for Bavarian.",
            "ai_keywords": [
                "trilingual foundation model",
                "Llama 3.1-8B",
                "Llama-GENBA-10B",
                "multilingual corpus",
                "unified tokenizer",
                "cross-lingual transfer",
                "trilingual evaluation suite",
                "fine-tuned variant",
                "Apertus-8B-2509",
                "gemma-2-9b",
                "EuroLLM",
                "Cerebras CS-2",
                "large-scale multilingual pretraining"
            ]
        },
        "publishedAt": "2025-09-06T06:12:52.000Z",
        "title": "Llama-GENBA-10B: A Trilingual Large Language Model for German, English\n  and Bavarian",
        "summary": "We present Llama-GENBA-10B, a trilingual foundation model addressing\nEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaled\nto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens\n(82B English, 82B German, and 80M Bavarian), balancing resources while\npreventing English dominance. Targeted at the German NLP community, the model\nalso promotes Bavarian as a low-resource language. Development tackled four\nchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)\ncreating a unified tokenizer for English, German, and Bavarian, (3) optimizing\narchitecture and language-ratio hyperparameters for cross-lingual transfer, and\n(4) establishing the first standardized trilingual evaluation suite by\ntranslating German benchmarks into Bavarian. Evaluations show that\nLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned\nvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing\nitself as the best model in its class for this language, while also\noutperforming EuroLLM in English and matching its results in German. Training\non the Cerebras CS-2 demonstrated efficient large-scale multilingual\npretraining with documented energy use, offering a blueprint for inclusive\nfoundation models that integrate low-resource languages.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05668.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5e6a3d4ea9afd5125d9ec064",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1584020801691-noauth.jpeg",
            "fullname": "Stefan Schweter",
            "name": "stefan-it",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3360
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06477",
            "authors": [
                {
                    "_id": "68bf993b207285de11b07bc9",
                    "user": {
                        "_id": "65a088f4300957620ba45c70",
                        "avatarUrl": "/avatars/56ed45e10d3455531979f30881b2d3f9.svg",
                        "isPro": false,
                        "fullname": "pengxiang zhao",
                        "user": "Pengxiangzhao",
                        "type": "user"
                    },
                    "name": "Pengxiang Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:46.445Z",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bca",
                    "user": {
                        "_id": "64d761b98ebc40443831f82a",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
                        "isPro": false,
                        "fullname": "Guangyi Liu",
                        "user": "lgy0404",
                        "type": "user"
                    },
                    "name": "Guangyi Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:49.267Z",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bcb",
                    "name": "Yaozhen Liang",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bcc",
                    "name": "Weiqing He",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bcd",
                    "name": "Zhengxi Lu",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bce",
                    "name": "Yuehao Huang",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bcf",
                    "name": "Yaxuan Guo",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bd0",
                    "name": "Kexin Zhang",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bd1",
                    "name": "Hao Wang",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bd2",
                    "name": "Liang Liu",
                    "hidden": false
                },
                {
                    "_id": "68bf993b207285de11b07bd3",
                    "name": "Yong Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T09:43:48.000Z",
            "submittedOnDailyAt": "2025-09-09T01:35:27.046Z",
            "title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI\n  Agents",
            "submittedOnDailyBy": {
                "_id": "64d761b98ebc40443831f82a",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
                "isPro": false,
                "fullname": "Guangyi Liu",
                "user": "lgy0404",
                "type": "user"
            },
            "summary": "To enhance the efficiency of GUI agents on various platforms like smartphones\nand computers, a hybrid paradigm that combines flexible GUI operations with\nefficient shortcuts (e.g., API, deep links) is emerging as a promising\ndirection. However, a framework for systematically benchmarking these hybrid\nagents is still underexplored. To take the first step in bridging this gap, we\nintroduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut\nhybrid agents with a specific focus on the mobile domain. Beyond merely using\npredefined shortcuts, MAS-Bench assesses an agent's capability to autonomously\ngenerate shortcuts by discovering and creating reusable, low-cost workflows. It\nfeatures 139 complex tasks across 11 real-world applications, a knowledge base\nof 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation\nmetrics. The tasks are designed to be solvable via GUI-only operations, but can\nbe significantly accelerated by intelligently embedding shortcuts. Experiments\nshow that hybrid agents achieve significantly higher success rates and\nefficiency than their GUI-only counterparts. This result also demonstrates the\neffectiveness of our method for evaluating an agent's shortcut generation\ncapabilities. MAS-Bench fills a critical evaluation gap, providing a\nfoundational platform for future advancements in creating more efficient and\nrobust intelligent agents.",
            "upvotes": 2,
            "discussionId": "68bf993c207285de11b07bd4",
            "projectPage": "https://pengxiang-zhao.github.io/MAS-Bench",
            "githubRepo": "https://github.com/Pengxiang-zhao/MAS-Bench",
            "ai_summary": "MAS-Bench evaluates GUI-shortcut hybrid agents on mobile devices, demonstrating their superior performance and efficiency over GUI-only agents through a comprehensive benchmarking framework.",
            "ai_keywords": [
                "GUI agents",
                "hybrid paradigm",
                "efficient shortcuts",
                "API",
                "deep links",
                "MAS-Bench",
                "autonomous generation",
                "reusable workflows",
                "evaluation metrics",
                "success rates",
                "efficiency"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-09-08T05:43:48.000Z",
        "title": "MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI\n  Agents",
        "summary": "To enhance the efficiency of GUI agents on various platforms like smartphones\nand computers, a hybrid paradigm that combines flexible GUI operations with\nefficient shortcuts (e.g., API, deep links) is emerging as a promising\ndirection. However, a framework for systematically benchmarking these hybrid\nagents is still underexplored. To take the first step in bridging this gap, we\nintroduce MAS-Bench, a benchmark that pioneers the evaluation of GUI-shortcut\nhybrid agents with a specific focus on the mobile domain. Beyond merely using\npredefined shortcuts, MAS-Bench assesses an agent's capability to autonomously\ngenerate shortcuts by discovering and creating reusable, low-cost workflows. It\nfeatures 139 complex tasks across 11 real-world applications, a knowledge base\nof 88 predefined shortcuts (APIs, deep-links, RPA scripts), and 7 evaluation\nmetrics. The tasks are designed to be solvable via GUI-only operations, but can\nbe significantly accelerated by intelligently embedding shortcuts. Experiments\nshow that hybrid agents achieve significantly higher success rates and\nefficiency than their GUI-only counterparts. This result also demonstrates the\neffectiveness of our method for evaluating an agent's shortcut generation\ncapabilities. MAS-Bench fills a critical evaluation gap, providing a\nfoundational platform for future advancements in creating more efficient and\nrobust intelligent agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06477.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64d761b98ebc40443831f82a",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64d761b98ebc40443831f82a/DHBOtOstiFp2-lDY6b9gb.png",
            "fullname": "Guangyi Liu",
            "name": "lgy0404",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06283",
            "authors": [
                {
                    "_id": "68c0b7f73912ed54cf5431b8",
                    "name": "Xuan-Phi Nguyen",
                    "hidden": false
                },
                {
                    "_id": "68c0b7f73912ed54cf5431b9",
                    "name": "Shrey Pandit",
                    "hidden": false
                },
                {
                    "_id": "68c0b7f73912ed54cf5431ba",
                    "name": "Revanth Gangi Reddy",
                    "hidden": false
                },
                {
                    "_id": "68c0b7f73912ed54cf5431bb",
                    "name": "Austin Xu",
                    "hidden": false
                },
                {
                    "_id": "68c0b7f73912ed54cf5431bc",
                    "name": "Silvio Savarese",
                    "hidden": false
                },
                {
                    "_id": "68c0b7f73912ed54cf5431bd",
                    "name": "Caiming Xiong",
                    "hidden": false
                },
                {
                    "_id": "68c0b7f73912ed54cf5431be",
                    "name": "Shafiq Joty",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T02:07:09.000Z",
            "submittedOnDailyAt": "2025-09-09T21:59:58.301Z",
            "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for\n  Autonomously Reasoning Single Agents",
            "submittedOnDailyBy": {
                "_id": "649dbcc4e0fff1ed099dc80a",
                "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
                "isPro": false,
                "fullname": "Caiming Xiong",
                "user": "cxiong",
                "type": "user"
            },
            "summary": "Equipping large language models (LLMs) with complex, interleaved reasoning\nand tool-use capabilities has become a key focus in agentic AI research,\nespecially with recent advances in reasoning-oriented (``thinking'') models.\nSuch capabilities are key to unlocking a number of important applications. One\nsuch application is Deep Research (DR), which requires extensive search and\nreasoning over many sources. Our work in this paper focuses on the development\nof native Autonomous Single-Agent models for DR featuring minimal web crawling\nand Python tool integration. Unlike multi-agent systems, where agents take up\npre-defined roles and are told what to do at each step in a static workflow, an\nautonomous single-agent determines its next action dynamically based on\ncontext, without manual directive. While prior work has proposed training\nrecipes for base or instruction-tuned LLMs, we focus on continual reinforcement\nlearning (RL) of reasoning-optimized models to further enhance agentic skills\nwhile preserving reasoning ability. Towards this end, we propose a simple RL\nrecipe with entirely synthetic data, which we apply to various open-source\nLLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam\nbenchmark. In addition, we conduct key analysis experiments to provide more\ninsights into our methodologies.",
            "upvotes": 2,
            "discussionId": "68c0b7f83912ed54cf5431bf",
            "ai_summary": "Continual reinforcement learning enhances autonomous single-agent models for deep research tasks, improving agentic skills and reasoning ability using synthetic data.",
            "ai_keywords": [
                "large language models",
                "reasoning-oriented models",
                "deep research",
                "autonomous single-agent models",
                "reinforcement learning",
                "reasoning-optimized models",
                "Humanity's Last Exam benchmark"
            ]
        },
        "publishedAt": "2025-09-07T22:07:09.000Z",
        "title": "SFR-DeepResearch: Towards Effective Reinforcement Learning for\n  Autonomously Reasoning Single Agents",
        "summary": "Equipping large language models (LLMs) with complex, interleaved reasoning\nand tool-use capabilities has become a key focus in agentic AI research,\nespecially with recent advances in reasoning-oriented (``thinking'') models.\nSuch capabilities are key to unlocking a number of important applications. One\nsuch application is Deep Research (DR), which requires extensive search and\nreasoning over many sources. Our work in this paper focuses on the development\nof native Autonomous Single-Agent models for DR featuring minimal web crawling\nand Python tool integration. Unlike multi-agent systems, where agents take up\npre-defined roles and are told what to do at each step in a static workflow, an\nautonomous single-agent determines its next action dynamically based on\ncontext, without manual directive. While prior work has proposed training\nrecipes for base or instruction-tuned LLMs, we focus on continual reinforcement\nlearning (RL) of reasoning-optimized models to further enhance agentic skills\nwhile preserving reasoning ability. Towards this end, we propose a simple RL\nrecipe with entirely synthetic data, which we apply to various open-source\nLLMs. Our best variant SFR-DR-20B achieves up to 28.7% on Humanity's Last Exam\nbenchmark. In addition, we conduct key analysis experiments to provide more\ninsights into our methodologies.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06283.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649dbcc4e0fff1ed099dc80a",
            "avatarUrl": "/avatars/c87c273ca628dbcddccbf1ee19b2ce33.svg",
            "fullname": "Caiming Xiong",
            "name": "cxiong",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.00328",
            "authors": [
                {
                    "_id": "68bf4715207285de11b07adf",
                    "user": {
                        "_id": "643a3857de858bcf53c30cd8",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a3857de858bcf53c30cd8/XQ7FtuOOOjx6PNLJh5uNM.jpeg",
                        "isPro": false,
                        "fullname": "Bear Häon",
                        "user": "bearhaon",
                        "type": "user"
                    },
                    "name": "Bear Häon",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:29.386Z",
                    "hidden": false
                },
                {
                    "_id": "68bf4715207285de11b07ae0",
                    "name": "Kaylene Stocking",
                    "hidden": false
                },
                {
                    "_id": "68bf4715207285de11b07ae1",
                    "name": "Ian Chuang",
                    "hidden": false
                },
                {
                    "_id": "68bf4715207285de11b07ae2",
                    "name": "Claire Tomlin",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-08-30T03:01:57.000Z",
            "submittedOnDailyAt": "2025-09-09T14:44:08.897Z",
            "title": "Mechanistic interpretability for steering vision-language-action models",
            "submittedOnDailyBy": {
                "_id": "643a3857de858bcf53c30cd8",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a3857de858bcf53c30cd8/XQ7FtuOOOjx6PNLJh5uNM.jpeg",
                "isPro": false,
                "fullname": "Bear Häon",
                "user": "bearhaon",
                "type": "user"
            },
            "summary": "Vision-Language-Action (VLA) models are a promising path to realizing\ngeneralist embodied agents that can quickly adapt to new tasks, modalities, and\nenvironments. However, methods for interpreting and steering VLAs fall far\nshort of classical robotics pipelines, which are grounded in explicit models of\nkinematics, dynamics, and control. This lack of mechanistic insight is a\ncentral challenge for deploying learned policies in real-world robotics, where\nrobustness and explainability are critical. Motivated by advances in\nmechanistic interpretability for large language models, we introduce the first\nframework for interpreting and steering VLAs via their internal\nrepresentations, enabling direct intervention in model behavior at inference\ntime. We project feedforward activations within transformer layers onto the\ntoken embedding basis, identifying sparse semantic directions - such as speed\nand direction - that are causally linked to action selection. Leveraging these\nfindings, we introduce a general-purpose activation steering method that\nmodulates behavior in real time, without fine-tuning, reward signals, or\nenvironment interaction. We evaluate this method on two recent open-source\nVLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in\nsimulation (LIBERO) and on a physical robot (UR5). This work demonstrates that\ninterpretable components of embodied VLAs can be systematically harnessed for\ncontrol - establishing a new paradigm for transparent and steerable foundation\nmodels in robotics.",
            "upvotes": 2,
            "discussionId": "68bf4715207285de11b07ae3",
            "projectPage": "https://vla-mech-interp.github.io",
            "ai_summary": "A framework for interpreting and steering Vision-Language-Action (VLA) models via internal representations enables real-time behavioral control without fine-tuning or environment interaction.",
            "ai_keywords": [
                "transformer layers",
                "token embedding basis",
                "sparse semantic directions",
                "activation steering method",
                "Pi0",
                "OpenVLA",
                "LIBERO",
                "UR5"
            ]
        },
        "publishedAt": "2025-08-29T23:01:57.000Z",
        "title": "Mechanistic interpretability for steering vision-language-action models",
        "summary": "Vision-Language-Action (VLA) models are a promising path to realizing\ngeneralist embodied agents that can quickly adapt to new tasks, modalities, and\nenvironments. However, methods for interpreting and steering VLAs fall far\nshort of classical robotics pipelines, which are grounded in explicit models of\nkinematics, dynamics, and control. This lack of mechanistic insight is a\ncentral challenge for deploying learned policies in real-world robotics, where\nrobustness and explainability are critical. Motivated by advances in\nmechanistic interpretability for large language models, we introduce the first\nframework for interpreting and steering VLAs via their internal\nrepresentations, enabling direct intervention in model behavior at inference\ntime. We project feedforward activations within transformer layers onto the\ntoken embedding basis, identifying sparse semantic directions - such as speed\nand direction - that are causally linked to action selection. Leveraging these\nfindings, we introduce a general-purpose activation steering method that\nmodulates behavior in real time, without fine-tuning, reward signals, or\nenvironment interaction. We evaluate this method on two recent open-source\nVLAs, Pi0 and OpenVLA, and demonstrate zero-shot behavioral control in\nsimulation (LIBERO) and on a physical robot (UR5). This work demonstrates that\ninterpretable components of embodied VLAs can be systematically harnessed for\ncontrol - establishing a new paradigm for transparent and steerable foundation\nmodels in robotics.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.00328.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643a3857de858bcf53c30cd8",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/643a3857de858bcf53c30cd8/XQ7FtuOOOjx6PNLJh5uNM.jpeg",
            "fullname": "Bear Häon",
            "name": "bearhaon",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06809",
            "authors": [
                {
                    "_id": "68bfdc49207285de11b07dd6",
                    "name": "Valentin Quesnel",
                    "hidden": false
                },
                {
                    "_id": "68bfdc49207285de11b07dd7",
                    "name": "Damien Sileo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T15:43:29.000Z",
            "submittedOnDailyAt": "2025-09-09T06:21:05.936Z",
            "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in\n  the TPTP Ecosystem",
            "submittedOnDailyBy": {
                "_id": "5fc0bcb41160c47d1d43856b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc0bcb41160c47d1d43856b/C1j1sc3BH5FWB9JRK7jqM.png",
                "isPro": false,
                "fullname": "Damien Sileo",
                "user": "sileod",
                "type": "user"
            },
            "summary": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1",
            "upvotes": 1,
            "discussionId": "68bfdc49207285de11b07dd8",
            "githubRepo": "https://github.com/sileod/reasoning_core/",
            "ai_summary": "A framework generates a large corpus of valid theorems using automated theorem proving to create symbolic training data for improving LLMs' mathematical reasoning.",
            "ai_keywords": [
                "Large Language Models",
                "automated theorem proving",
                "E-prover",
                "saturation capabilities",
                "TPTP axiom library",
                "entailment verification",
                "premise selection",
                "proof reconstruction",
                "zero-shot experiments"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-09-08T11:43:29.000Z",
        "title": "Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in\n  the TPTP Ecosystem",
        "summary": "The scarcity of high-quality, logically sound data is a critical bottleneck\nfor advancing the mathematical reasoning of Large Language Models (LLMs). Our\nwork confronts this challenge by turning decades of automated theorem proving\nresearch into a scalable data engine. Rather than relying on error-prone LLMs\nor complex proof-assistant syntax like Lean and Isabelle, our framework\nleverages E-prover's saturation capabilities on the vast TPTP axiom library to\nderive a massive, guaranteed-valid corpus of theorems. Our pipeline is\nprincipled and simple: saturate axioms, filter for \"interesting\" theorems, and\ngenerate tasks. With no LLMs in the loop, we eliminate factual errors by\nconstruction. This purely symbolic data is then transformed into three\ndifficulty-controlled challenges: entailment verification, premise selection,\nand proof reconstruction. Our zero-shot experiments on frontier models reveal a\nclear weakness: performance collapses on tasks requiring deep, structural\nreasoning. Our framework provides both the diagnostic tool to measure this gap\nand a scalable source of symbolic training data to address it. We make the code\nand data publicly available.\n  https://github.com/sileod/reasoning_core\nhttps://hf.co/datasets/reasoning-core/rc1",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06809.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5fc0bcb41160c47d1d43856b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/5fc0bcb41160c47d1d43856b/C1j1sc3BH5FWB9JRK7jqM.png",
            "fullname": "Damien Sileo",
            "name": "sileod",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 27
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06285",
            "authors": [
                {
                    "_id": "68bffcd0207285de11b07e47",
                    "name": "Xiangcheng Hu",
                    "hidden": false
                },
                {
                    "_id": "68bffcd0207285de11b07e48",
                    "name": "Xieyuanli Chen",
                    "hidden": false
                },
                {
                    "_id": "68bffcd0207285de11b07e49",
                    "name": "Mingkai Jia",
                    "hidden": false
                },
                {
                    "_id": "68bffcd0207285de11b07e4a",
                    "name": "Jin Wu",
                    "hidden": false
                },
                {
                    "_id": "68bffcd0207285de11b07e4b",
                    "name": "Ping Tan",
                    "hidden": false
                },
                {
                    "_id": "68bffcd0207285de11b07e4c",
                    "name": "Steven L. Waslander",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T02:12:54.000Z",
            "submittedOnDailyAt": "2025-09-09T12:36:40.702Z",
            "title": "DCReg: Decoupled Characterization for Efficient Degenerate LiDAR\n  Registration",
            "submittedOnDailyBy": {
                "_id": "6434298c7b824748010a06a6",
                "avatarUrl": "/avatars/274a1f744e0b35d803af37d7bb8d94d5.svg",
                "isPro": false,
                "fullname": "xchu",
                "user": "xchu123",
                "type": "user"
            },
            "summary": "LiDAR point cloud registration is fundamental to robotic perception and\nnavigation. However, in geometrically degenerate or narrow environments,\nregistration problems become ill-conditioned, leading to unstable solutions and\ndegraded accuracy. While existing approaches attempt to handle these issues,\nthey fail to address the core challenge: accurately detection, interpret, and\nresolve this ill-conditioning, leading to missed detections or corrupted\nsolutions. In this study, we introduce DCReg, a principled framework that\nsystematically addresses the ill-conditioned registration problems through\nthree integrated innovations. First, DCReg achieves reliable ill-conditioning\ndetection by employing a Schur complement decomposition to the hessian matrix.\nThis technique decouples the registration problem into clean rotational and\ntranslational subspaces, eliminating coupling effects that mask degeneracy\npatterns in conventional analyses. Second, within these cleanly subspaces, we\ndevelop quantitative characterization techniques that establish explicit\nmappings between mathematical eigenspaces and physical motion directions,\nproviding actionable insights about which specific motions lack constraints.\nFinally, leveraging this clean subspace, we design a targeted mitigation\nstrategy: a novel preconditioner that selectively stabilizes only the\nidentified ill-conditioned directions while preserving all well-constrained\ninformation in observable space. This enables efficient and robust optimization\nvia the Preconditioned Conjugate Gradient method with a single physical\ninterpretable parameter. Extensive experiments demonstrate DCReg achieves at\nleast 20% - 50% improvement in localization accuracy and 5-100 times speedup\nover state-of-the-art methods across diverse environments. Our implementation\nwill be available at https://github.com/JokerJohn/DCReg.",
            "upvotes": 1,
            "discussionId": "68bffcd1207285de11b07e4d",
            "projectPage": "https://github.com/JokerJohn/DCReg",
            "githubRepo": "https://github.com/JokerJohn/DCReg",
            "ai_summary": "DCReg addresses ill-conditioned LiDAR point cloud registration by detecting, characterizing, and mitigating degeneracies through Schur complement decomposition and a novel preconditioner.",
            "ai_keywords": [
                "Schur complement decomposition",
                "hessian matrix",
                "rotational subspace",
                "translational subspace",
                "eigenspaces",
                "motion directions",
                "preconditioner",
                "Preconditioned Conjugate Gradient method"
            ],
            "githubStars": 38
        },
        "publishedAt": "2025-09-07T22:12:54.000Z",
        "title": "DCReg: Decoupled Characterization for Efficient Degenerate LiDAR\n  Registration",
        "summary": "LiDAR point cloud registration is fundamental to robotic perception and\nnavigation. However, in geometrically degenerate or narrow environments,\nregistration problems become ill-conditioned, leading to unstable solutions and\ndegraded accuracy. While existing approaches attempt to handle these issues,\nthey fail to address the core challenge: accurately detection, interpret, and\nresolve this ill-conditioning, leading to missed detections or corrupted\nsolutions. In this study, we introduce DCReg, a principled framework that\nsystematically addresses the ill-conditioned registration problems through\nthree integrated innovations. First, DCReg achieves reliable ill-conditioning\ndetection by employing a Schur complement decomposition to the hessian matrix.\nThis technique decouples the registration problem into clean rotational and\ntranslational subspaces, eliminating coupling effects that mask degeneracy\npatterns in conventional analyses. Second, within these cleanly subspaces, we\ndevelop quantitative characterization techniques that establish explicit\nmappings between mathematical eigenspaces and physical motion directions,\nproviding actionable insights about which specific motions lack constraints.\nFinally, leveraging this clean subspace, we design a targeted mitigation\nstrategy: a novel preconditioner that selectively stabilizes only the\nidentified ill-conditioned directions while preserving all well-constrained\ninformation in observable space. This enables efficient and robust optimization\nvia the Preconditioned Conjugate Gradient method with a single physical\ninterpretable parameter. Extensive experiments demonstrate DCReg achieves at\nleast 20% - 50% improvement in localization accuracy and 5-100 times speedup\nover state-of-the-art methods across diverse environments. Our implementation\nwill be available at https://github.com/JokerJohn/DCReg.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06285.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6434298c7b824748010a06a6",
            "avatarUrl": "/avatars/274a1f744e0b35d803af37d7bb8d94d5.svg",
            "fullname": "xchu",
            "name": "xchu123",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.04582",
            "authors": [
                {
                    "_id": "68bebd40c123124955ef61c6",
                    "user": {
                        "_id": "65260483b5cccc82eab1a559",
                        "avatarUrl": "/avatars/bcda44ab0b46f394b18ad4469154f38f.svg",
                        "isPro": false,
                        "fullname": "LU Jingyi",
                        "user": "LuJingyi",
                        "type": "user"
                    },
                    "name": "Jingyi Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:49:32.977Z",
                    "hidden": false
                },
                {
                    "_id": "68bebd40c123124955ef61c7",
                    "name": "Kai Han",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T18:04:47.000Z",
            "submittedOnDailyAt": "2025-09-09T12:36:42.640Z",
            "title": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing\n  via Bidirectional Warping",
            "submittedOnDailyBy": {
                "_id": "65260483b5cccc82eab1a559",
                "avatarUrl": "/avatars/bcda44ab0b46f394b18ad4469154f38f.svg",
                "isPro": false,
                "fullname": "LU Jingyi",
                "user": "LuJingyi",
                "type": "user"
            },
            "summary": "Drag-based image editing has emerged as a powerful paradigm for intuitive\nimage manipulation. However, existing approaches predominantly rely on\nmanipulating the latent space of generative models, leading to limited\nprecision, delayed feedback, and model-specific constraints. Accordingly, we\npresent Inpaint4Drag, a novel framework that decomposes drag-based editing into\npixel-space bidirectional warping and image inpainting. Inspired by elastic\nobject deformation in the physical world, we treat image regions as deformable\nmaterials that maintain natural shape under user manipulation. Our method\nachieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at\n512x512 resolution, significantly improving the interaction experience compared\nto existing methods that require minutes per edit. By transforming drag inputs\ndirectly into standard inpainting formats, our approach serves as a universal\nadapter for any inpainting model without architecture modification,\nautomatically inheriting all future improvements in inpainting technology.\nExtensive experiments demonstrate that our method achieves superior visual\nquality and precise control while maintaining real-time performance. Project\npage: https://visual-ai.github.io/inpaint4drag/",
            "upvotes": 1,
            "discussionId": "68bebd40c123124955ef61c8",
            "projectPage": "https://visual-ai.github.io/inpaint4drag/",
            "githubRepo": "https://github.com/Visual-AI/Inpaint4Drag/",
            "ai_summary": "Inpaint4Drag enhances drag-based image editing by decomposing it into pixel-space warping and inpainting, offering real-time performance and superior visual quality.",
            "ai_keywords": [
                "drag-based editing",
                "latent space",
                "generative models",
                "pixel-space bidirectional warping",
                "image inpainting",
                "elastic object deformation",
                "real-time warping",
                "real-time inpainting",
                "inpainting formats",
                "universal adapter"
            ],
            "githubStars": 18
        },
        "publishedAt": "2025-09-04T14:04:47.000Z",
        "title": "Inpaint4Drag: Repurposing Inpainting Models for Drag-Based Image Editing\n  via Bidirectional Warping",
        "summary": "Drag-based image editing has emerged as a powerful paradigm for intuitive\nimage manipulation. However, existing approaches predominantly rely on\nmanipulating the latent space of generative models, leading to limited\nprecision, delayed feedback, and model-specific constraints. Accordingly, we\npresent Inpaint4Drag, a novel framework that decomposes drag-based editing into\npixel-space bidirectional warping and image inpainting. Inspired by elastic\nobject deformation in the physical world, we treat image regions as deformable\nmaterials that maintain natural shape under user manipulation. Our method\nachieves real-time warping previews (0.01s) and efficient inpainting (0.3s) at\n512x512 resolution, significantly improving the interaction experience compared\nto existing methods that require minutes per edit. By transforming drag inputs\ndirectly into standard inpainting formats, our approach serves as a universal\nadapter for any inpainting model without architecture modification,\nautomatically inheriting all future improvements in inpainting technology.\nExtensive experiments demonstrate that our method achieves superior visual\nquality and precise control while maintaining real-time performance. Project\npage: https://visual-ai.github.io/inpaint4drag/",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04582.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65260483b5cccc82eab1a559",
            "avatarUrl": "/avatars/bcda44ab0b46f394b18ad4469154f38f.svg",
            "fullname": "LU Jingyi",
            "name": "LuJingyi",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.03740",
            "authors": [
                {
                    "_id": "68c0423d56a3252309f75cb9",
                    "name": "Taha Koleilat",
                    "hidden": false
                },
                {
                    "_id": "68c0423d56a3252309f75cba",
                    "name": "Hassan Rivaz",
                    "hidden": false
                },
                {
                    "_id": "68c0423d56a3252309f75cbb",
                    "name": "Yiming Xiao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-03T22:00:23.000Z",
            "submittedOnDailyAt": "2025-09-09T14:12:01.359Z",
            "title": "Singular Value Few-shot Adaptation of Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "67d33a3b1d5d1fd92c3cfce4",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d33a3b1d5d1fd92c3cfce4/MFBB29WXxNz5vgwPY_GkK.png",
                "isPro": false,
                "fullname": "Taha Koleilat",
                "user": "TahaKoleilat",
                "type": "user"
            },
            "summary": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and\nfew-shot learning capabilities across diverse applications. However, adapting\nthese models to new fine-grained domains remains difficult due to reliance on\nprompt engineering and the high cost of full model fine-tuning. Existing\nadaptation approaches rely on augmented components, such as prompt tokens and\nadapter modules, which could limit adaptation quality, destabilize the model,\nand compromise the rich knowledge learned during pretraining. In this work, we\npresent CLIP-SVD, a novel multi-modal and\nparameter-efficient adaptation technique that leverages Singular Value\nDecomposition (SVD) to modify the internal parameter space of CLIP without\ninjecting additional modules. Specifically, we fine-tune only the singular\nvalues of the CLIP parameter matrices to rescale the basis vectors for domain\nadaptation while retaining the pretrained model. This design enables enhanced\nadaptation performance using only 0.04\\% of the model's total\nparameters and better preservation of its generalization ability. CLIP-SVD\nachieves state-of-the-art classification results on 11 natural and 10\nbiomedical datasets, outperforming previous methods in both accuracy and\ngeneralization under few-shot settings. Additionally, we leverage a natural\nlanguage-based approach to analyze the effectiveness and dynamics of the CLIP\nadaptation to allow interpretability of CLIP-SVD. The code is publicly\navailable at https://github.com/HealthX-Lab/CLIP-SVD.",
            "upvotes": 1,
            "discussionId": "68c0423d56a3252309f75cbc",
            "githubRepo": "https://github.com/HealthX-Lab/CLIP-SVD",
            "githubStars": 5
        },
        "publishedAt": "2025-09-03T18:00:23.000Z",
        "title": "Singular Value Few-shot Adaptation of Vision-Language Models",
        "summary": "Vision-language models (VLMs) like CLIP have shown impressive zero-shot and\nfew-shot learning capabilities across diverse applications. However, adapting\nthese models to new fine-grained domains remains difficult due to reliance on\nprompt engineering and the high cost of full model fine-tuning. Existing\nadaptation approaches rely on augmented components, such as prompt tokens and\nadapter modules, which could limit adaptation quality, destabilize the model,\nand compromise the rich knowledge learned during pretraining. In this work, we\npresent CLIP-SVD, a novel multi-modal and\nparameter-efficient adaptation technique that leverages Singular Value\nDecomposition (SVD) to modify the internal parameter space of CLIP without\ninjecting additional modules. Specifically, we fine-tune only the singular\nvalues of the CLIP parameter matrices to rescale the basis vectors for domain\nadaptation while retaining the pretrained model. This design enables enhanced\nadaptation performance using only 0.04\\% of the model's total\nparameters and better preservation of its generalization ability. CLIP-SVD\nachieves state-of-the-art classification results on 11 natural and 10\nbiomedical datasets, outperforming previous methods in both accuracy and\ngeneralization under few-shot settings. Additionally, we leverage a natural\nlanguage-based approach to analyze the effectiveness and dynamics of the CLIP\nadaptation to allow interpretability of CLIP-SVD. The code is publicly\navailable at https://github.com/HealthX-Lab/CLIP-SVD.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03740.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d33a3b1d5d1fd92c3cfce4",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/67d33a3b1d5d1fd92c3cfce4/MFBB29WXxNz5vgwPY_GkK.png",
            "fullname": "Taha Koleilat",
            "name": "TahaKoleilat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    }
]