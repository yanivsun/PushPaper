[
    {
        "paper": {
            "id": "2509.07980",
            "authors": [
                {
                    "_id": "68c0d8e13912ed54cf543209",
                    "name": "Tong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf54320a",
                    "name": "Hongming Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf54320b",
                    "name": "Wenhao Yu",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf54320c",
                    "name": "Xiaoyang Wang",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf54320d",
                    "name": "Xinyu Yang",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf54320e",
                    "name": "Runpeng Dai",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf54320f",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf543210",
                    "name": "Huiwen Bao",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf543211",
                    "name": "Chengsong Huang",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf543212",
                    "name": "Heng Huang",
                    "hidden": false
                },
                {
                    "_id": "68c0d8e13912ed54cf543213",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T17:59:35.000Z",
            "submittedOnDailyAt": "2025-09-10T00:20:34.994Z",
            "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "64f58b970b24e548a85522bc",
                "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
                "isPro": false,
                "fullname": "Xinyu Yang",
                "user": "Hanyuezhuohua",
                "type": "user"
            },
            "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose Parallel-R1, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a mid-training exploration scaffold, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.",
            "upvotes": 66,
            "discussionId": "68c0d8e23912ed54cf543214",
            "githubRepo": "https://github.com/zhengkid/Parallel-R1",
            "ai_summary": "Parallel-R1, a reinforcement learning framework, enhances large language models' reasoning capabilities by enabling parallel thinking through a progressive curriculum, leading to significant performance improvements on math benchmarks.",
            "ai_keywords": [
                "parallel thinking",
                "reinforcement learning",
                "progressive curriculum",
                "cold-start problem",
                "supervised fine-tuning",
                "prompt-generated trajectories",
                "sequential thinking",
                "multi-perspective verification",
                "mid-training exploration scaffold"
            ],
            "githubStars": 35
        },
        "publishedAt": "2025-09-09T13:59:35.000Z",
        "title": "Parallel-R1: Towards Parallel Thinking via Reinforcement Learning",
        "summary": "Parallel thinking has emerged as a novel approach for enhancing the reasoning\ncapabilities of large language models (LLMs) by exploring multiple reasoning\npaths concurrently. However, activating such capabilities through training\nremains challenging, as existing methods predominantly rely on supervised\nfine-tuning (SFT) over synthetic data, which encourages teacher-forced\nimitation rather than exploration and generalization. Different from them, we\npropose Parallel-R1, the first reinforcement learning (RL) framework\nthat enables parallel thinking behaviors for complex real-world reasoning\ntasks. Our framework employs a progressive curriculum that explicitly addresses\nthe cold-start problem in training parallel thinking with RL. We first use SFT\non prompt-generated trajectories from easier tasks to instill the parallel\nthinking ability, then transition to RL to explore and generalize this skill on\nharder problems. Experiments on various math benchmarks, including MATH, AMC23,\nand AIME, show that Parallel-R1 successfully instills parallel thinking,\nleading to 8.4% accuracy improvements over the sequential thinking model\ntrained directly on challenging tasks with RL. Further analysis reveals a clear\nshift in the model's thinking behavior: at an early stage, it uses parallel\nthinking as an exploration strategy, while in a later stage, it uses the same\ncapability for multi-perspective verification. Most significantly, we validate\nparallel thinking as a mid-training exploration scaffold, where this\ntemporary exploratory phase unlocks a higher performance ceiling after RL,\nyielding a 42.9% improvement over the baseline on AIME25. Our model, data, and\ncode will be open-source at https://github.com/zhengkid/Parallel-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07980.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "64f58b970b24e548a85522bc",
            "avatarUrl": "/avatars/c8ca1294b5a1edd609694877e335b22f.svg",
            "fullname": "Xinyu Yang",
            "name": "Hanyuezhuohua",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07979",
            "authors": [
                {
                    "_id": "68c0d8f13912ed54cf543216",
                    "name": "Heeji Yoon",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf543217",
                    "name": "Jaewoo Jung",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf543218",
                    "name": "Junwan Kim",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf543219",
                    "name": "Hyungyu Choi",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf54321a",
                    "name": "Heeseong Shin",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf54321b",
                    "name": "Sangbeom Lim",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf54321c",
                    "name": "Honggyu An",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf54321d",
                    "name": "Chaehyun Kim",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf54321e",
                    "name": "Jisang Han",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf54321f",
                    "name": "Donghyun Kim",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf543220",
                    "name": "Chanho Eom",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf543221",
                    "name": "Sunghwan Hong",
                    "hidden": false
                },
                {
                    "_id": "68c0d8f13912ed54cf543222",
                    "name": "Seungryong Kim",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T17:59:14.000Z",
            "submittedOnDailyAt": "2025-09-10T00:18:48.073Z",
            "title": "Visual Representation Alignment for Multimodal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Multimodal large language models (MLLMs) trained with visual instruction\ntuning have achieved strong performance across diverse tasks, yet they remain\nlimited in vision-centric tasks such as object counting or spatial reasoning.\nWe attribute this gap to the prevailing text-only supervision paradigm, which\nprovides only indirect guidance for the visual pathway and often leads MLLMs to\ndiscard fine-grained visual details during training. In this paper, we present\nVIsual Representation ALignment (VIRAL), a simple yet effective regularization\nstrategy that aligns the internal visual representations of MLLMs with those of\npre-trained vision foundation models (VFMs). By explicitly enforcing this\nalignment, VIRAL enables the model not only to retain critical visual details\nfrom the input vision encoder but also to complement additional visual\nknowledge from VFMs, thereby enhancing its ability to reason over complex\nvisual inputs. Our experiments demonstrate consistent improvements across all\ntasks on widely adopted multimodal benchmarks. Furthermore, we conduct\ncomprehensive ablation studies to validate the key design choices underlying\nour framework. We believe this simple finding opens up an important direction\nfor the effective integration of visual information in training MLLMs.",
            "upvotes": 54,
            "discussionId": "68c0d8f23912ed54cf543223",
            "projectPage": "https://cvlab-kaist.github.io/VIRAL/",
            "githubRepo": "https://github.com/cvlab-kaist/VIRAL",
            "ai_summary": "VIRAL, a regularization strategy, aligns MLLMs' visual representations with pre-trained VFMs, enhancing performance on vision-centric tasks.",
            "ai_keywords": [
                "multimodal large language models",
                "MLLMs",
                "visual instruction tuning",
                "vision-centric tasks",
                "object counting",
                "spatial reasoning",
                "text-only supervision",
                "visual pathway",
                "fine-grained visual details",
                "Visual Representation ALignment",
                "VIRAL",
                "pre-trained vision foundation models",
                "VFMs",
                "internal visual representations",
                "visual encoder",
                "visual knowledge",
                "reasoning over complex visual inputs",
                "multimodal benchmarks",
                "ablation studies"
            ],
            "githubStars": 59
        },
        "publishedAt": "2025-09-09T13:59:14.000Z",
        "title": "Visual Representation Alignment for Multimodal Large Language Models",
        "summary": "Multimodal large language models (MLLMs) trained with visual instruction\ntuning have achieved strong performance across diverse tasks, yet they remain\nlimited in vision-centric tasks such as object counting or spatial reasoning.\nWe attribute this gap to the prevailing text-only supervision paradigm, which\nprovides only indirect guidance for the visual pathway and often leads MLLMs to\ndiscard fine-grained visual details during training. In this paper, we present\nVIsual Representation ALignment (VIRAL), a simple yet effective regularization\nstrategy that aligns the internal visual representations of MLLMs with those of\npre-trained vision foundation models (VFMs). By explicitly enforcing this\nalignment, VIRAL enables the model not only to retain critical visual details\nfrom the input vision encoder but also to complement additional visual\nknowledge from VFMs, thereby enhancing its ability to reason over complex\nvisual inputs. Our experiments demonstrate consistent improvements across all\ntasks on widely adopted multimodal benchmarks. Furthermore, we conduct\ncomprehensive ablation studies to validate the key design choices underlying\nour framework. We believe this simple finding opens up an important direction\nfor the effective integration of visual information in training MLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07979.png",
        "numComments": 7,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 102
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07969",
            "authors": [
                {
                    "_id": "68c0d77d3912ed54cf543201",
                    "name": "Xin Lai",
                    "hidden": false
                },
                {
                    "_id": "68c0d77d3912ed54cf543202",
                    "name": "Junyi Li",
                    "hidden": false
                },
                {
                    "_id": "68c0d77d3912ed54cf543203",
                    "name": "Wei Li",
                    "hidden": false
                },
                {
                    "_id": "68c0d77d3912ed54cf543204",
                    "name": "Tao Liu",
                    "hidden": false
                },
                {
                    "_id": "68c0d77d3912ed54cf543205",
                    "name": "Tianjian Li",
                    "hidden": false
                },
                {
                    "_id": "68c0d77d3912ed54cf543206",
                    "name": "Hengshuang Zhao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T17:54:21.000Z",
            "submittedOnDailyAt": "2025-09-10T00:12:34.256Z",
            "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems.",
            "upvotes": 45,
            "discussionId": "68c0d77d3912ed54cf543207",
            "projectPage": "https://mini-o3.github.io/",
            "githubRepo": "https://github.com/Mini-o3/Mini-o3",
            "ai_summary": "Mini-o3, a system for deep, multi-turn reasoning in visual search tasks, uses an iterative data collection pipeline and over-turn masking strategy to achieve state-of-the-art performance with rich reasoning patterns.",
            "ai_keywords": [
                "reinforcement learning",
                "Visual Probe Dataset",
                "iterative data collection pipeline",
                "depth-first search",
                "trial-and-error",
                "goal maintenance",
                "over-turn masking strategy"
            ],
            "githubStars": 122
        },
        "publishedAt": "2025-09-09T13:54:21.000Z",
        "title": "Mini-o3: Scaling Up Reasoning Patterns and Interaction Turns for Visual\n  Search",
        "summary": "Recent advances in large multimodal models have leveraged image-based tools\nwith reinforcement learning to tackle visual problems. However, existing\nopen-source approaches often exhibit monotonous reasoning patterns and allow\nonly a limited number of interaction turns, making them inadequate for\ndifficult tasks that require trial-and-error exploration. In this work, we\naddress this limitation by scaling up tool-based interactions and introduce\nMini-o3, a system that executes deep, multi-turn reasoning -- spanning tens of\nsteps -- and achieves state-of-the-art performance on challenging visual search\ntasks. Our recipe for reproducing OpenAI o3-style behaviors comprises three key\ncomponents. First, we construct the Visual Probe Dataset, a collection of\nthousands of challenging visual search problems designed for exploratory\nreasoning. Second, we develop an iterative data collection pipeline to obtain\ncold-start trajectories that exhibit diverse reasoning patterns, including\ndepth-first search, trial-and-error, and goal maintenance. Third, we propose an\nover-turn masking strategy that prevents penalization of over-turn responses\n(those that hit the maximum number of turns) during reinforcement learning,\nthereby balancing training-time efficiency with test-time scalability. Despite\ntraining with an upper bound of only six interaction turns, our model generates\ntrajectories that naturally scale to tens of turns at inference time, with\naccuracy improving as the number of turns increases. Extensive experiments\ndemonstrate that Mini-o3 produces rich reasoning patterns and deep thinking\npaths, effectively solving challenging visual search problems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07969.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 102
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07295",
            "authors": [
                {
                    "_id": "68c0e2033912ed54cf543276",
                    "name": "Ji Xie",
                    "hidden": false
                },
                {
                    "_id": "68c0e2033912ed54cf543277",
                    "name": "Trevor Darrell",
                    "hidden": false
                },
                {
                    "_id": "68c0e2033912ed54cf543278",
                    "name": "Luke Zettlemoyer",
                    "hidden": false
                },
                {
                    "_id": "68c0e2033912ed54cf543279",
                    "name": "XuDong Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T23:59:32.000Z",
            "submittedOnDailyAt": "2025-09-10T00:59:36.913Z",
            "title": "Reconstruction Alignment Improves Unified Multimodal Models",
            "submittedOnDailyBy": {
                "_id": "64e99fc07e2ec711a7138262",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e99fc07e2ec711a7138262/FmP3F8_UXgh9K-0gwS99A.jpeg",
                "isPro": true,
                "fullname": "Ji Xie",
                "user": "sanaka87",
                "type": "user"
            },
            "summary": "Unified multimodal models (UMMs) unify visual understanding and generation\nwithin a single architecture. However, conventional training relies on\nimage-text pairs (or sequences) whose captions are typically sparse and miss\nfine-grained visual details--even when they use hundreds of words to describe a\nsimple image. We introduce Reconstruction Alignment (RecA), a\nresource-efficient post-training method that leverages visual understanding\nencoder embeddings as dense \"text prompts,\" providing rich supervision without\ncaptions. Concretely, RecA conditions a UMM on its own visual understanding\nembeddings and optimizes it to reconstruct the input image with a\nself-supervised reconstruction loss, thereby realigning understanding and\ngeneration. Despite its simplicity, RecA is broadly applicable: across\nautoregressive, masked-autoregressive, and diffusion-based UMMs, it\nconsistently improves generation and editing fidelity. With only 27 GPU-hours,\npost-training with RecA substantially improves image generation performance on\nGenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while\nalso boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit\n6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models\nand applies broadly across diverse UMM architectures, establishing it as an\nefficient and general post-training alignment strategy for UMMs",
            "upvotes": 31,
            "discussionId": "68c0e2033912ed54cf54327a",
            "projectPage": "https://reconstruction-alignment.github.io/",
            "githubRepo": "https://github.com/HorizonWind2004/reconstruction-alignment",
            "ai_summary": "Reconstruction Alignment (RecA) is a post-training method that enhances multimodal models by using visual embeddings as dense prompts, improving image generation and editing fidelity.",
            "ai_keywords": [
                "Unified multimodal models",
                "Reconstruction Alignment",
                "visual understanding encoder embeddings",
                "self-supervised reconstruction loss",
                "autoregressive",
                "masked-autoregressive",
                "diffusion-based",
                "GenEval",
                "DPGBench",
                "ImgEdit",
                "GEdit"
            ],
            "githubStars": 62
        },
        "publishedAt": "2025-09-08T19:59:32.000Z",
        "title": "Reconstruction Alignment Improves Unified Multimodal Models",
        "summary": "Unified multimodal models (UMMs) unify visual understanding and generation\nwithin a single architecture. However, conventional training relies on\nimage-text pairs (or sequences) whose captions are typically sparse and miss\nfine-grained visual details--even when they use hundreds of words to describe a\nsimple image. We introduce Reconstruction Alignment (RecA), a\nresource-efficient post-training method that leverages visual understanding\nencoder embeddings as dense \"text prompts,\" providing rich supervision without\ncaptions. Concretely, RecA conditions a UMM on its own visual understanding\nembeddings and optimizes it to reconstruct the input image with a\nself-supervised reconstruction loss, thereby realigning understanding and\ngeneration. Despite its simplicity, RecA is broadly applicable: across\nautoregressive, masked-autoregressive, and diffusion-based UMMs, it\nconsistently improves generation and editing fidelity. With only 27 GPU-hours,\npost-training with RecA substantially improves image generation performance on\nGenEval (0.73rightarrow0.90) and DPGBench (80.93rightarrow88.15), while\nalso boosting editing benchmarks (ImgEdit 3.38rightarrow3.75, GEdit\n6.94rightarrow7.25). Notably, RecA surpasses much larger open-source models\nand applies broadly across diverse UMM architectures, establishing it as an\nefficient and general post-training alignment strategy for UMMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07295.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64e99fc07e2ec711a7138262",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64e99fc07e2ec711a7138262/FmP3F8_UXgh9K-0gwS99A.jpeg",
            "fullname": "Ji Xie",
            "name": "sanaka87",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 39
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06818",
            "authors": [
                {
                    "_id": "68bfa87b207285de11b07c7e",
                    "user": {
                        "_id": "66a0aed210de1ccd3d409721",
                        "avatarUrl": "/avatars/bc6e6c1d60b6601dfb3fe3a697e02ce9.svg",
                        "isPro": false,
                        "fullname": "Yufeng Cheng",
                        "user": "cb1cyf",
                        "type": "user"
                    },
                    "name": "Yufeng Cheng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:18.629Z",
                    "hidden": false
                },
                {
                    "_id": "68bfa87b207285de11b07c7f",
                    "name": "Wenxu Wu",
                    "hidden": false
                },
                {
                    "_id": "68bfa87b207285de11b07c80",
                    "user": {
                        "_id": "660114b38ae190912a61be5d",
                        "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
                        "isPro": false,
                        "fullname": "ShaojinWu",
                        "user": "fenfan",
                        "type": "user"
                    },
                    "name": "Shaojin Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:20.947Z",
                    "hidden": false
                },
                {
                    "_id": "68bfa87b207285de11b07c81",
                    "name": "Mengqi Huang",
                    "hidden": false
                },
                {
                    "_id": "68bfa87b207285de11b07c82",
                    "name": "Fei Ding",
                    "hidden": false
                },
                {
                    "_id": "68bfa87b207285de11b07c83",
                    "name": "Qian He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T15:54:55.000Z",
            "submittedOnDailyAt": "2025-09-10T01:45:26.368Z",
            "title": "UMO: Scaling Multi-Identity Consistency for Image Customization via\n  Matching Reward",
            "submittedOnDailyBy": {
                "_id": "660114b38ae190912a61be5d",
                "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
                "isPro": false,
                "fullname": "ShaojinWu",
                "user": "fenfan",
                "type": "user"
            },
            "summary": "Recent advancements in image customization exhibit a wide range of\napplication prospects due to stronger customization capabilities. However,\nsince we humans are more sensitive to faces, a significant challenge remains in\npreserving consistent identity while avoiding identity confusion with\nmulti-reference images, limiting the identity scalability of customization\nmodels. To address this, we present UMO, a Unified Multi-identity Optimization\nframework, designed to maintain high-fidelity identity preservation and\nalleviate identity confusion with scalability. With \"multi-to-multi matching\"\nparadigm, UMO reformulates multi-identity generation as a global assignment\noptimization problem and unleashes multi-identity consistency for existing\nimage customization methods generally through reinforcement learning on\ndiffusion models. To facilitate the training of UMO, we develop a scalable\ncustomization dataset with multi-reference images, consisting of both\nsynthesised and real parts. Additionally, we propose a new metric to measure\nidentity confusion. Extensive experiments demonstrate that UMO not only\nimproves identity consistency significantly, but also reduces identity\nconfusion on several image customization methods, setting a new\nstate-of-the-art among open-source methods along the dimension of identity\npreserving. Code and model: https://github.com/bytedance/UMO",
            "upvotes": 24,
            "discussionId": "68bfa87b207285de11b07c84",
            "projectPage": "https://bytedance.github.io/UMO/",
            "githubRepo": "https://github.com/bytedance/UMO",
            "ai_summary": "UMO, a Unified Multi-identity Optimization framework, enhances identity consistency and reduces confusion in multi-reference image customization using reinforcement learning on diffusion models.",
            "ai_keywords": [
                "Unified Multi-identity Optimization",
                "multi-to-multi matching",
                "global assignment optimization",
                "reinforcement learning",
                "diffusion models",
                "identity consistency",
                "identity confusion"
            ],
            "githubStars": 43
        },
        "publishedAt": "2025-09-08T11:54:55.000Z",
        "title": "UMO: Scaling Multi-Identity Consistency for Image Customization via\n  Matching Reward",
        "summary": "Recent advancements in image customization exhibit a wide range of\napplication prospects due to stronger customization capabilities. However,\nsince we humans are more sensitive to faces, a significant challenge remains in\npreserving consistent identity while avoiding identity confusion with\nmulti-reference images, limiting the identity scalability of customization\nmodels. To address this, we present UMO, a Unified Multi-identity Optimization\nframework, designed to maintain high-fidelity identity preservation and\nalleviate identity confusion with scalability. With \"multi-to-multi matching\"\nparadigm, UMO reformulates multi-identity generation as a global assignment\noptimization problem and unleashes multi-identity consistency for existing\nimage customization methods generally through reinforcement learning on\ndiffusion models. To facilitate the training of UMO, we develop a scalable\ncustomization dataset with multi-reference images, consisting of both\nsynthesised and real parts. Additionally, we propose a new metric to measure\nidentity confusion. Extensive experiments demonstrate that UMO not only\nimproves identity consistency significantly, but also reduces identity\nconfusion on several image customization methods, setting a new\nstate-of-the-art among open-source methods along the dimension of identity\npreserving. Code and model: https://github.com/bytedance/UMO",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06818.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "660114b38ae190912a61be5d",
            "avatarUrl": "/avatars/abc4ab10d6f9769d2b5e697ccbf3fb70.svg",
            "fullname": "ShaojinWu",
            "name": "fenfan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.07414",
            "authors": [
                {
                    "_id": "68c0dee53912ed54cf543256",
                    "name": "Jakub Grudzien Kuba",
                    "hidden": false
                },
                {
                    "_id": "68c0dee53912ed54cf543257",
                    "name": "Mengting Gu",
                    "hidden": false
                },
                {
                    "_id": "68c0dee53912ed54cf543258",
                    "name": "Qi Ma",
                    "hidden": false
                },
                {
                    "_id": "68c0dee53912ed54cf543259",
                    "name": "Yuandong Tian",
                    "hidden": false
                },
                {
                    "_id": "68c0dee53912ed54cf54325a",
                    "name": "Vijai Mohan",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T05:51:34.000Z",
            "submittedOnDailyAt": "2025-09-10T00:44:10.481Z",
            "title": "Language Self-Play For Data-Free Training",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have advanced rapidly in recent years, driven by\nscale, abundant high-quality training data, and reinforcement learning. Yet\nthis progress faces a fundamental bottleneck: the need for ever more data from\nwhich models can continue to learn. In this work, we propose a reinforcement\nlearning approach that removes this dependency by enabling models to improve\nwithout additional data. Our method leverages a game-theoretic framework of\nself-play, where a model's capabilities are cast as performance in a\ncompetitive game and stronger policies emerge by having the model play against\nitself - a process we call Language Self-Play (LSP). Experiments with\nLlama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained\nmodels can not only enhance their performance on challenging tasks through\nself-play alone, but can also do so more effectively than data-driven\nbaselines.",
            "upvotes": 19,
            "discussionId": "68c0dee53912ed54cf54325b",
            "ai_summary": "Language Self-Play (LSP) enhances large language models' performance on instruction-following tasks through self-play, surpassing data-driven methods.",
            "ai_keywords": [
                "large language models",
                "reinforcement learning",
                "self-play",
                "Language Self-Play (LSP)",
                "instruction-following benchmarks",
                "pretrained models"
            ]
        },
        "publishedAt": "2025-09-09T01:51:34.000Z",
        "title": "Language Self-Play For Data-Free Training",
        "summary": "Large language models (LLMs) have advanced rapidly in recent years, driven by\nscale, abundant high-quality training data, and reinforcement learning. Yet\nthis progress faces a fundamental bottleneck: the need for ever more data from\nwhich models can continue to learn. In this work, we propose a reinforcement\nlearning approach that removes this dependency by enabling models to improve\nwithout additional data. Our method leverages a game-theoretic framework of\nself-play, where a model's capabilities are cast as performance in a\ncompetitive game and stronger policies emerge by having the model play against\nitself - a process we call Language Self-Play (LSP). Experiments with\nLlama-3.2-3B-Instruct on instruction-following benchmarks show that pretrained\nmodels can not only enhance their performance on challenging tasks through\nself-play alone, but can also do so more effectively than data-driven\nbaselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07414.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 102
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06951",
            "authors": [
                {
                    "_id": "68bfa3f2207285de11b07c07",
                    "user": {
                        "_id": "678123194248fde89e4fc9bf",
                        "avatarUrl": "/avatars/c207e25e12a138eed048c72b143b9bcf.svg",
                        "isPro": false,
                        "fullname": "aopolin-lv",
                        "user": "aopolin-lv",
                        "type": "user"
                    },
                    "name": "Qi Lv",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:30.614Z",
                    "hidden": false
                },
                {
                    "_id": "68bfa3f2207285de11b07c08",
                    "name": "Weijie Kong",
                    "hidden": false
                },
                {
                    "_id": "68bfa3f2207285de11b07c09",
                    "name": "Hao Li",
                    "hidden": false
                },
                {
                    "_id": "68bfa3f2207285de11b07c0a",
                    "name": "Jia Zeng",
                    "hidden": false
                },
                {
                    "_id": "68bfa3f2207285de11b07c0b",
                    "name": "Zherui Qiu",
                    "hidden": false
                },
                {
                    "_id": "68bfa3f2207285de11b07c0c",
                    "name": "Delin Qu",
                    "hidden": false
                },
                {
                    "_id": "68bfa3f2207285de11b07c0d",
                    "name": "Haoming Song",
                    "hidden": false
                },
                {
                    "_id": "68bfa3f2207285de11b07c0e",
                    "name": "Qizhi Chen",
                    "hidden": false
                },
                {
                    "_id": "68bfa3f2207285de11b07c0f",
                    "name": "Xiang Deng",
                    "hidden": false
                },
                {
                    "_id": "68bfa3f2207285de11b07c10",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T17:58:30.000Z",
            "submittedOnDailyAt": "2025-09-10T00:49:21.614Z",
            "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation\n  to Actions",
            "submittedOnDailyBy": {
                "_id": "678123194248fde89e4fc9bf",
                "avatarUrl": "/avatars/c207e25e12a138eed048c72b143b9bcf.svg",
                "isPro": false,
                "fullname": "aopolin-lv",
                "user": "aopolin-lv",
                "type": "user"
            },
            "summary": "Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.",
            "upvotes": 19,
            "discussionId": "68bfa3f2207285de11b07c11",
            "projectPage": "https://aopolin-lv.github.io/F1-VLA/",
            "githubRepo": "https://github.com/InternRobotics/F1-VLA",
            "ai_summary": "F1, a pretrained VLA framework with foresight generation, improves task success and generalization in dynamic environments through a Mixture-of-Transformer architecture and next-scale prediction.",
            "ai_keywords": [
                "Mixture-of-Transformer",
                "foresight generation",
                "next-scale prediction",
                "inverse dynamics problem",
                "modular reasoning",
                "transferable visual foresight"
            ],
            "githubStars": 44
        },
        "publishedAt": "2025-09-08T13:58:30.000Z",
        "title": "F1: A Vision-Language-Action Model Bridging Understanding and Generation\n  to Actions",
        "summary": "Executing language-conditioned tasks in dynamic visual environments remains a\ncentral challenge in embodied AI. Existing Vision-Language-Action (VLA) models\npredominantly adopt reactive state-to-action mappings, often leading to\nshort-sighted behaviors and poor robustness in dynamic scenes. In this paper,\nwe introduce F1, a pretrained VLA framework which integrates the visual\nforesight generation into decision-making pipeline. F1 adopts a\nMixture-of-Transformer architecture with dedicated modules for perception,\nforesight generation, and control, thereby bridging understanding, generation,\nand actions. At its core, F1 employs a next-scale prediction mechanism to\nsynthesize goal-conditioned visual foresight as explicit planning targets. By\nforecasting plausible future visual states, F1 reformulates action generation\nas a foresight-guided inverse dynamics problem, enabling actions that\nimplicitly achieve visual goals. To endow F1 with robust and generalizable\ncapabilities, we propose a three-stage training recipe on an extensive dataset\ncomprising over 330k trajectories across 136 diverse tasks. This training\nscheme enhances modular reasoning and equips the model with transferable visual\nforesight, which is critical for complex and dynamic environments. Extensive\nevaluations on real-world tasks and simulation benchmarks demonstrate F1\nconsistently outperforms existing approaches, achieving substantial gains in\nboth task success rate and generalization ability.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06951.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "678123194248fde89e4fc9bf",
            "avatarUrl": "/avatars/c207e25e12a138eed048c72b143b9bcf.svg",
            "fullname": "aopolin-lv",
            "name": "aopolin-lv",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06923",
            "authors": [
                {
                    "_id": "68bf933e207285de11b07b6c",
                    "user": {
                        "_id": "636a7459eb076ec3f4030e7d",
                        "avatarUrl": "/avatars/832dc709211e3a2ea5e93caea3768122.svg",
                        "isPro": false,
                        "fullname": "Ziheng Li",
                        "user": "ChillingDream",
                        "type": "user"
                    },
                    "name": "Ziheng Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:12.520Z",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b6d",
                    "name": "Zexu Sun",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b6e",
                    "user": {
                        "_id": "6546ecc9cd0a56213929fd18",
                        "avatarUrl": "/avatars/9f4d4e369f0ca6e5669b038badc4e622.svg",
                        "isPro": false,
                        "fullname": "jinman",
                        "user": "zhaojinm",
                        "type": "user"
                    },
                    "name": "Jinman Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:48:10.358Z",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b6f",
                    "name": "Erxue Min",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b70",
                    "name": "Yongcheng Zeng",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b71",
                    "name": "Hui Wu",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b72",
                    "name": "Hengyi Cai",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b73",
                    "name": "Shuaiqiang Wang",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b74",
                    "name": "Dawei Yin",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b75",
                    "name": "Xu Chen",
                    "hidden": false
                },
                {
                    "_id": "68bf933e207285de11b07b76",
                    "name": "Zhi-Hong Deng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T17:36:21.000Z",
            "submittedOnDailyAt": "2025-09-10T00:16:30.819Z",
            "title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding",
            "submittedOnDailyBy": {
                "_id": "636a7459eb076ec3f4030e7d",
                "avatarUrl": "/avatars/832dc709211e3a2ea5e93caea3768122.svg",
                "isPro": false,
                "fullname": "Ziheng Li",
                "user": "ChillingDream",
                "type": "user"
            },
            "summary": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable\nsuccess in enhancing the reasoning capabilities of large language models\n(LLMs). However, existing RLVR methods often suffer from exploration\ninefficiency due to mismatches between the training data's difficulty and the\nmodel's capability. LLMs fail to discover viable reasoning paths when problems\nare overly difficult, while learning little new capability when problems are\ntoo simple. In this work, we formalize the impact of problem difficulty by\nquantifying the relationship between loss descent speed and rollout accuracy.\nBuilding on this analysis, we propose SEELE, a novel supervision-aided RLVR\nframework that dynamically adjusts problem difficulty to stay within the\nhigh-efficiency region. SEELE augments each training sample by appending a hint\n(part of a full solution) after the original problem. Unlike previous\nhint-based approaches, SEELE deliberately and adaptively adjusts the hint\nlength for each problem to achieve an optimal difficulty. To determine the\noptimal hint length, SEELE employs a multi-round rollout sampling strategy. In\neach round, it fits an item response theory model to the accuracy-hint pairs\ncollected in preceding rounds to predict the required hint length for the next\nround. This instance-level, real-time difficulty adjustment aligns problem\ndifficulty with the evolving model capability, thereby improving exploration\nefficiency. Experimental results show that SEELE outperforms Group Relative\nPolicy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5\npoints, respectively, and surpasses the best previous supervision-aided\napproach by +3.6 points on average across six math reasoning benchmarks.",
            "upvotes": 17,
            "discussionId": "68bf933f207285de11b07b77",
            "projectPage": "https://github.com/ChillingDream/seele",
            "githubRepo": "https://github.com/ChillingDream/seele",
            "ai_summary": "SEELE, a novel RLVR framework, dynamically adjusts problem difficulty using adaptive hint lengths to enhance exploration efficiency and improve performance in math reasoning tasks.",
            "ai_keywords": [
                "reinforcement learning with verifiable rewards",
                "RLVR",
                "large language models",
                "LLMs",
                "exploration inefficiency",
                "problem difficulty",
                "loss descent speed",
                "rollout accuracy",
                "SEELE",
                "supervision-aided RLVR",
                "hint length",
                "multi-round rollout sampling",
                "item response theory model",
                "Group Relative Policy Optimization",
                "GRPO",
                "Supervised Fine-tuning",
                "SFT"
            ],
            "githubStars": 5
        },
        "publishedAt": "2025-09-08T13:36:21.000Z",
        "title": "Staying in the Sweet Spot: Responsive Reasoning Evolution via\n  Capability-Adaptive Hint Scaffolding",
        "summary": "Reinforcement learning with verifiable rewards (RLVR) has achieved remarkable\nsuccess in enhancing the reasoning capabilities of large language models\n(LLMs). However, existing RLVR methods often suffer from exploration\ninefficiency due to mismatches between the training data's difficulty and the\nmodel's capability. LLMs fail to discover viable reasoning paths when problems\nare overly difficult, while learning little new capability when problems are\ntoo simple. In this work, we formalize the impact of problem difficulty by\nquantifying the relationship between loss descent speed and rollout accuracy.\nBuilding on this analysis, we propose SEELE, a novel supervision-aided RLVR\nframework that dynamically adjusts problem difficulty to stay within the\nhigh-efficiency region. SEELE augments each training sample by appending a hint\n(part of a full solution) after the original problem. Unlike previous\nhint-based approaches, SEELE deliberately and adaptively adjusts the hint\nlength for each problem to achieve an optimal difficulty. To determine the\noptimal hint length, SEELE employs a multi-round rollout sampling strategy. In\neach round, it fits an item response theory model to the accuracy-hint pairs\ncollected in preceding rounds to predict the required hint length for the next\nround. This instance-level, real-time difficulty adjustment aligns problem\ndifficulty with the evolving model capability, thereby improving exploration\nefficiency. Experimental results show that SEELE outperforms Group Relative\nPolicy Optimization (GRPO) and Supervised Fine-tuning (SFT) by +11.8 and +10.5\npoints, respectively, and surpasses the best previous supervision-aided\napproach by +3.6 points on average across six math reasoning benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06923.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "636a7459eb076ec3f4030e7d",
            "avatarUrl": "/avatars/832dc709211e3a2ea5e93caea3768122.svg",
            "fullname": "Ziheng Li",
            "name": "ChillingDream",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06830",
            "authors": [
                {
                    "_id": "68bfd7b3207285de11b07da5",
                    "name": "Corentin Dancette",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07da6",
                    "name": "Julien Khlaut",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07da7",
                    "name": "Antoine Saporta",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07da8",
                    "name": "Helene Philippe",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07da9",
                    "name": "Elodie Ferreres",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07daa",
                    "name": "Baptiste Callard",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07dab",
                    "name": "Tho Danielou",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07dac",
                    "name": "Lo Alberge",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07dad",
                    "name": "Lo Machado",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07dae",
                    "name": "Daniel Tordjman",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07daf",
                    "name": "Julie Dupuis",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db0",
                    "name": "Korentin Le Floch",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db1",
                    "name": "Jean Du Terrail",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db2",
                    "name": "Mariam Moshiri",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db3",
                    "name": "Laurent Dercle",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db4",
                    "name": "Tom Boeken",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db5",
                    "name": "Jules Gregory",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db6",
                    "name": "Maxime Ronot",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db7",
                    "name": "Franois Legou",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db8",
                    "name": "Pascal Roux",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07db9",
                    "name": "Marc Sapoval",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07dba",
                    "name": "Pierre Manceron",
                    "hidden": false
                },
                {
                    "_id": "68bfd7b3207285de11b07dbb",
                    "name": "Paul Hrent",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T16:04:12.000Z",
            "submittedOnDailyAt": "2025-09-10T06:33:10.373Z",
            "title": "Curia: A Multi-Modal Foundation Model for Radiology",
            "submittedOnDailyBy": {
                "_id": "62cdea59a9be5c195561c2b8",
                "avatarUrl": "/avatars/959b702e57718b9029634cb41772dcef.svg",
                "isPro": false,
                "fullname": "Corentin Dancette",
                "user": "cdancette",
                "type": "user"
            },
            "summary": "AI-assisted radiological interpretation is based on predominantly narrow,\nsingle-task models. This approach is impractical for covering the vast spectrum\nof imaging modalities, diseases, and radiological findings. Foundation models\n(FMs) hold the promise of broad generalization across modalities and in\nlow-data settings. However, this potential has remained largely unrealized in\nradiology. We introduce Curia, a foundation model trained on the entire\ncross-sectional imaging output of a major hospital over several years, which to\nour knowledge is the largest such corpus of real-world data-encompassing\n150,000 exams (130 TB). On a newly curated 19-task external validation\nbenchmark, Curia accurately identifies organs, detects conditions like brain\nhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.\nCuria meets or surpasses the performance of radiologists and recent foundation\nmodels, and exhibits clinically significant emergent properties in\ncross-modality, and low-data regimes. To accelerate progress, we release our\nbase model's weights at https://huggingface.co/raidium/curia.",
            "upvotes": 17,
            "discussionId": "68bfd7b4207285de11b07dbc",
            "projectPage": "https://huggingface.co/raidium/curia",
            "ai_summary": "Curia, a foundation model trained on extensive cross-sectional imaging data, demonstrates superior performance across multiple radiological tasks and shows emergent properties in cross-modality and low-data settings.",
            "ai_keywords": [
                "foundation models",
                "cross-sectional imaging",
                "external validation benchmark",
                "organ identification",
                "condition detection",
                "outcome prediction",
                "tumor staging",
                "cross-modality",
                "low-data regimes"
            ]
        },
        "publishedAt": "2025-09-08T12:04:12.000Z",
        "title": "Curia: A Multi-Modal Foundation Model for Radiology",
        "summary": "AI-assisted radiological interpretation is based on predominantly narrow,\nsingle-task models. This approach is impractical for covering the vast spectrum\nof imaging modalities, diseases, and radiological findings. Foundation models\n(FMs) hold the promise of broad generalization across modalities and in\nlow-data settings. However, this potential has remained largely unrealized in\nradiology. We introduce Curia, a foundation model trained on the entire\ncross-sectional imaging output of a major hospital over several years, which to\nour knowledge is the largest such corpus of real-world data-encompassing\n150,000 exams (130 TB). On a newly curated 19-task external validation\nbenchmark, Curia accurately identifies organs, detects conditions like brain\nhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.\nCuria meets or surpasses the performance of radiologists and recent foundation\nmodels, and exhibits clinically significant emergent properties in\ncross-modality, and low-data regimes. To accelerate progress, we release our\nbase model's weights at https://huggingface.co/raidium/curia.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06830.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62cdea59a9be5c195561c2b8",
            "avatarUrl": "/avatars/959b702e57718b9029634cb41772dcef.svg",
            "fullname": "Corentin Dancette",
            "name": "cdancette",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.03646",
            "authors": [
                {
                    "_id": "68baebf4e41474e84d6ddd07",
                    "name": "Haozhe Wang",
                    "hidden": false
                },
                {
                    "_id": "68baebf4e41474e84d6ddd08",
                    "user": {
                        "_id": "6680f0b20b72be136708af26",
                        "avatarUrl": "/avatars/5d8fd5be0cf94e246b46abb9d3cc8f5c.svg",
                        "isPro": false,
                        "fullname": "XuQixin",
                        "user": "Racktic",
                        "type": "user"
                    },
                    "name": "Qixin Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:49:47.551Z",
                    "hidden": false
                },
                {
                    "_id": "68baebf4e41474e84d6ddd09",
                    "name": "Che Liu",
                    "hidden": false
                },
                {
                    "_id": "68baebf4e41474e84d6ddd0a",
                    "name": "Junhong Wu",
                    "hidden": false
                },
                {
                    "_id": "68baebf4e41474e84d6ddd0b",
                    "name": "Fangzhen Lin",
                    "hidden": false
                },
                {
                    "_id": "68baebf4e41474e84d6ddd0c",
                    "name": "Wenhu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-03T18:52:49.000Z",
            "submittedOnDailyAt": "2025-09-10T12:54:35.029Z",
            "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "65bf52f0259bc6caeb74f8bf",
                "avatarUrl": "/avatars/b38392e954466df784a5760ded5df804.svg",
                "isPro": false,
                "fullname": "Haozhe Wang",
                "user": "JasperHaozhe",
                "type": "user"
            },
            "summary": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy.",
            "upvotes": 14,
            "discussionId": "68baebf4e41474e84d6ddd0d",
            "projectPage": "https://tiger-ai-lab.github.io/Hierarchical-Reasoner/",
            "githubRepo": "https://github.com/TIGER-AI-Lab/Hierarchical-Reasoner",
            "ai_summary": "Reinforcement Learning enhances LLM reasoning through a two-phase process involving procedural correctness and strategic planning, with HICRA algorithm focusing on high-impact planning tokens to improve performance.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Large Language Models",
                "aha moments",
                "length-scaling",
                "entropy dynamics",
                "reasoning hierarchy",
                "procedural correctness",
                "high-level strategic planning",
                "low-level procedural execution",
                "GRPO",
                "HIerarchy-Aware Credit Assignment",
                "HICRA",
                "semantic entropy",
                "token-level entropy"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-03T14:52:49.000Z",
        "title": "Emergent Hierarchical Reasoning in LLMs through Reinforcement Learning",
        "summary": "Reinforcement Learning (RL) has proven highly effective at enhancing the\ncomplex reasoning abilities of Large Language Models (LLMs), yet underlying\nmechanisms driving this success remain largely opaque. Our analysis reveals\nthat puzzling phenomena like ``aha moments\", ``length-scaling'' and entropy\ndynamics are not disparate occurrences but hallmarks of an emergent reasoning\nhierarchy, akin to the separation of high-level strategic planning from\nlow-level procedural execution in human cognition. We uncover a compelling\ntwo-phase dynamic: initially, a model is constrained by procedural correctness\nand must improve its low-level skills. The learning bottleneck then decisively\nshifts, with performance gains being driven by the exploration and mastery of\nhigh-level strategic planning. This insight exposes a core inefficiency in\nprevailing RL algorithms like GRPO, which apply optimization pressure\nagnostically and dilute the learning signal across all tokens. To address this,\nwe propose HIerarchy-Aware Credit Assignment (HICRA), an algorithm that\nconcentrates optimization efforts on high-impact planning tokens. HICRA\nsignificantly outperforms strong baselines, demonstrating that focusing on this\nstrategic bottleneck is key to unlocking advanced reasoning. Furthermore, we\nvalidate semantic entropy as a superior compass for measuring strategic\nexploration over misleading metrics such as token-level entropy.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.03646.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65bf52f0259bc6caeb74f8bf",
            "avatarUrl": "/avatars/b38392e954466df784a5760ded5df804.svg",
            "fullname": "Haozhe Wang",
            "name": "JasperHaozhe",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07301",
            "authors": [
                {
                    "_id": "68c0e08a3912ed54cf54325d",
                    "name": "Zhuoqing Song",
                    "hidden": false
                },
                {
                    "_id": "68c0e08a3912ed54cf54325e",
                    "name": "Peng Sun",
                    "hidden": false
                },
                {
                    "_id": "68c0e08a3912ed54cf54325f",
                    "name": "Huizhuo Yuan",
                    "hidden": false
                },
                {
                    "_id": "68c0e08a3912ed54cf543260",
                    "name": "Quanquan Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T00:15:23.000Z",
            "submittedOnDailyAt": "2025-09-10T01:33:57.243Z",
            "title": "Causal Attention with Lookahead Keys",
            "submittedOnDailyBy": {
                "_id": "64c039128e2612254356bba5",
                "avatarUrl": "/avatars/06cc76feebba0cc80ebb8f4ff86f6d9b.svg",
                "isPro": false,
                "fullname": "Quanquan Gu",
                "user": "thughost",
                "type": "user"
            },
            "summary": "In standard causal attention, each token's query, key, and value (QKV) are\nstatic and encode only preceding context. We introduce CAuSal aTtention with\nLookahead kEys (CASTLE), an attention mechanism that continually updates each\ntoken's keys as the context unfolds. We term these updated keys lookahead keys\nbecause they belong to earlier positions yet integrate information from tokens\nthat appear later relative to those positions, while strictly preserving the\nautoregressive property. Although the mechanism appears sequential, we derive a\nmathematical equivalence that avoids explicitly materializing lookahead keys at\neach position and enables efficient parallel training. On language modeling\nbenchmarks, CASTLE consistently outperforms standard causal attention across\nmodel scales, reducing validation perplexity and improving performance on a\nrange of downstream tasks.",
            "upvotes": 10,
            "discussionId": "68c0e08a3912ed54cf543261",
            "ai_summary": "CASTLE, an attention mechanism that updates keys with future context while maintaining autoregressive properties, outperforms standard causal attention in language modeling.",
            "ai_keywords": [
                "causal attention",
                "QKV",
                "lookahead keys",
                "autoregressive property",
                "parallel training",
                "validation perplexity",
                "downstream tasks"
            ]
        },
        "publishedAt": "2025-09-08T20:15:23.000Z",
        "title": "Causal Attention with Lookahead Keys",
        "summary": "In standard causal attention, each token's query, key, and value (QKV) are\nstatic and encode only preceding context. We introduce CAuSal aTtention with\nLookahead kEys (CASTLE), an attention mechanism that continually updates each\ntoken's keys as the context unfolds. We term these updated keys lookahead keys\nbecause they belong to earlier positions yet integrate information from tokens\nthat appear later relative to those positions, while strictly preserving the\nautoregressive property. Although the mechanism appears sequential, we derive a\nmathematical equivalence that avoids explicitly materializing lookahead keys at\neach position and enables efficient parallel training. On language modeling\nbenchmarks, CASTLE consistently outperforms standard causal attention across\nmodel scales, reducing validation perplexity and improving performance on a\nrange of downstream tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07301.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64c039128e2612254356bba5",
            "avatarUrl": "/avatars/06cc76feebba0cc80ebb8f4ff86f6d9b.svg",
            "fullname": "Quanquan Gu",
            "name": "thughost",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 25
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.08721",
            "authors": [
                {
                    "_id": "68c221ac29b8ec9932cd08b9",
                    "name": "Jeffrey Amico",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08ba",
                    "name": "Gabriel Passamani Andrade",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08bb",
                    "name": "John Donaghy",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08bc",
                    "name": "Ben Fielding",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08bd",
                    "name": "Tristin Forbus",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08be",
                    "name": "Harry Grieve",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08bf",
                    "name": "Semih Kara",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08c0",
                    "name": "Jari Kolehmainen",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08c1",
                    "name": "Yihua Lou",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08c2",
                    "name": "Christopher Nies",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08c3",
                    "name": "Edward Phillip Flores Nuo",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08c4",
                    "name": "Diogo Ortega",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08c5",
                    "name": "Shikhar Rastogi",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08c6",
                    "name": "Austin Virts",
                    "hidden": false
                },
                {
                    "_id": "68c221ac29b8ec9932cd08c7",
                    "name": "Matthew J. Wright",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/66d252ec8a438492b0d6e4ce/9ens83HsK7Ap7JR7Ael7-.png"
            ],
            "publishedAt": "2025-09-10T16:14:20.000Z",
            "submittedOnDailyAt": "2025-09-10T23:44:27.824Z",
            "title": "Sharing is Caring: Efficient LM Post-Training with Collective RL\n  Experience Sharing",
            "submittedOnDailyBy": {
                "_id": "66d252ec8a438492b0d6e4ce",
                "avatarUrl": "/avatars/4f0fc7fbe8afae463741f786ea774e95.svg",
                "isPro": false,
                "fullname": "Ben",
                "user": "benfielding",
                "type": "user"
            },
            "summary": "Post-training language models (LMs) with reinforcement learning (RL) can\nenhance their complex reasoning capabilities without supervised fine-tuning, as\ndemonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs\nrequires significant parallelization to scale-up inference, which introduces\nnon-trivial technical challenges (e.g. latency, memory, and reliability)\nalongside ever-growing financial costs. We present Swarm sAmpling Policy\nOptimization (SAPO), a fully decentralized and asynchronous RL post-training\nalgorithm. SAPO is designed for decentralized networks of heterogenous compute\nnodes, where each node manages its own policy model(s) while \"sharing\" rollouts\nwith others in the network; no explicit assumptions about latency, model\nhomogeneity, or hardware are required and nodes can operate in silo if desired.\nAs a result, the algorithm avoids common bottlenecks in scaling RL\npost-training while also allowing (and even encouraging) new possibilities. By\nsampling rollouts \"shared\" across the network, it enables \"Aha moments\" to\npropagate, thereby bootstrapping the learning process. In this paper we show\nSAPO achieved cumulative reward gains of up to 94% in controlled experiments.\nWe also share insights from tests on a network with thousands of nodes\ncontributed by Gensyn community members running the algorithm on diverse\nhardware and models during an open-source demo.",
            "upvotes": 7,
            "discussionId": "68c221ad29b8ec9932cd08c8",
            "githubRepo": "https://github.com/gensyn-ai/rl-swarm",
            "ai_summary": "Swarm sAmpling Policy Optimization (SAPO) is a decentralized and asynchronous RL algorithm that enhances post-training language models without supervised fine-tuning, achieving significant reward gains and scalability across diverse hardware.",
            "ai_keywords": [
                "reinforcement learning",
                "post-training",
                "language models",
                "DeepSeek-R1-Zero",
                "Swarm sAmpling Policy Optimization",
                "SAPO",
                "decentralized networks",
                "heterogenous compute nodes",
                "policy models",
                "rollouts",
                "cumulative reward gains"
            ]
        },
        "publishedAt": "2025-09-10T12:14:20.000Z",
        "title": "Sharing is Caring: Efficient LM Post-Training with Collective RL\n  Experience Sharing",
        "summary": "Post-training language models (LMs) with reinforcement learning (RL) can\nenhance their complex reasoning capabilities without supervised fine-tuning, as\ndemonstrated by DeepSeek-R1-Zero. However, effectively utilizing RL for LMs\nrequires significant parallelization to scale-up inference, which introduces\nnon-trivial technical challenges (e.g. latency, memory, and reliability)\nalongside ever-growing financial costs. We present Swarm sAmpling Policy\nOptimization (SAPO), a fully decentralized and asynchronous RL post-training\nalgorithm. SAPO is designed for decentralized networks of heterogenous compute\nnodes, where each node manages its own policy model(s) while \"sharing\" rollouts\nwith others in the network; no explicit assumptions about latency, model\nhomogeneity, or hardware are required and nodes can operate in silo if desired.\nAs a result, the algorithm avoids common bottlenecks in scaling RL\npost-training while also allowing (and even encouraging) new possibilities. By\nsampling rollouts \"shared\" across the network, it enables \"Aha moments\" to\npropagate, thereby bootstrapping the learning process. In this paper we show\nSAPO achieved cumulative reward gains of up to 94% in controlled experiments.\nWe also share insights from tests on a network with thousands of nodes\ncontributed by Gensyn community members running the algorithm on diverse\nhardware and models during an open-source demo.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/66d252ec8a438492b0d6e4ce/9ens83HsK7Ap7JR7Ael7-.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08721.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "66d252ec8a438492b0d6e4ce",
            "avatarUrl": "/avatars/4f0fc7fbe8afae463741f786ea774e95.svg",
            "fullname": "Ben",
            "name": "benfielding",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07968",
            "authors": [
                {
                    "_id": "68c0db913912ed54cf543246",
                    "name": "Lukas Haas",
                    "hidden": false
                },
                {
                    "_id": "68c0db913912ed54cf543247",
                    "name": "Gal Yona",
                    "hidden": false
                },
                {
                    "_id": "68c0db913912ed54cf543248",
                    "name": "Giovanni D'Antonio",
                    "hidden": false
                },
                {
                    "_id": "68c0db913912ed54cf543249",
                    "name": "Sasha Goldshtein",
                    "hidden": false
                },
                {
                    "_id": "68c0db913912ed54cf54324a",
                    "name": "Dipanjan Das",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T17:53:58.000Z",
            "submittedOnDailyAt": "2025-09-10T00:30:00.404Z",
            "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.",
            "upvotes": 7,
            "discussionId": "68c0db913912ed54cf54324b",
            "projectPage": "https://www.kaggle.com/benchmarks/deepmind/simpleqa-verified",
            "ai_summary": "SimpleQA Verified is a refined benchmark for evaluating the factuality of Large Language Models, addressing issues in previous benchmarks and providing a more reliable evaluation tool.",
            "ai_keywords": [
                "Large Language Model",
                "LLM",
                "SimpleQA",
                "SimpleQA Verified",
                "F1-score",
                "parametric model factuality",
                "hallucinations"
            ]
        },
        "publishedAt": "2025-09-09T13:53:58.000Z",
        "title": "SimpleQA Verified: A Reliable Factuality Benchmark to Measure Parametric\n  Knowledge",
        "summary": "We introduce SimpleQA Verified, a 1,000-prompt benchmark for evaluating Large\nLanguage Model (LLM) short-form factuality based on OpenAI's SimpleQA. It\naddresses critical limitations in OpenAI's benchmark, including noisy and\nincorrect labels, topical biases, and question redundancy. SimpleQA Verified\nwas created through a rigorous multi-stage filtering process involving\nde-duplication, topic balancing, and source reconciliation to produce a more\nreliable and challenging evaluation set, alongside improvements in the\nautorater prompt. On this new benchmark, Gemini 2.5 Pro achieves a\nstate-of-the-art F1-score of 55.6, outperforming other frontier models,\nincluding GPT-5. This work provides the research community with a\nhigher-fidelity tool to track genuine progress in parametric model factuality\nand to mitigate hallucinations. The benchmark dataset, evaluation code, and\nleaderboard are available at:\nhttps://www.kaggle.com/benchmarks/deepmind/simpleqa-verified.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07968.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 102
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06942",
            "authors": [
                {
                    "_id": "68bfd5e9207285de11b07d9a",
                    "name": "Xiangwei Shen",
                    "hidden": false
                },
                {
                    "_id": "68bfd5e9207285de11b07d9b",
                    "name": "Zhimin Li",
                    "hidden": false
                },
                {
                    "_id": "68bfd5e9207285de11b07d9c",
                    "name": "Zhantao Yang",
                    "hidden": false
                },
                {
                    "_id": "68bfd5e9207285de11b07d9d",
                    "name": "Shiyi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68bfd5e9207285de11b07d9e",
                    "name": "Yingfang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68bfd5e9207285de11b07d9f",
                    "name": "Donghao Li",
                    "hidden": false
                },
                {
                    "_id": "68bfd5e9207285de11b07da0",
                    "name": "Chunyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68bfd5e9207285de11b07da1",
                    "name": "Qinglin Lu",
                    "hidden": false
                },
                {
                    "_id": "68bfd5e9207285de11b07da2",
                    "name": "Yansong Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T17:54:08.000Z",
            "submittedOnDailyAt": "2025-09-10T08:50:11.989Z",
            "title": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human\n  Preference",
            "submittedOnDailyBy": {
                "_id": "62d22496c58f969c152bcefd",
                "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
                "isPro": false,
                "fullname": "Tiezhen WANG",
                "user": "xianbao",
                "type": "user"
            },
            "summary": "Recent studies have demonstrated the effectiveness of directly aligning\ndiffusion models with human preferences using differentiable reward. However,\nthey exhibit two primary challenges: (1) they rely on multistep denoising with\ngradient computation for reward scoring, which is computationally expensive,\nthus restricting optimization to only a few diffusion steps; (2) they often\nneed continuous offline adaptation of reward models in order to achieve desired\naesthetic quality, such as photorealism or precise lighting effects. To address\nthe limitation of multistep denoising, we propose Direct-Align, a method that\npredefines a noise prior to effectively recover original images from any time\nsteps via interpolation, leveraging the equation that diffusion states are\ninterpolations between noise and target images, which effectively avoids\nover-optimization in late timesteps. Furthermore, we introduce Semantic\nRelative Preference Optimization (SRPO), in which rewards are formulated as\ntext-conditioned signals. This approach enables online adjustment of rewards in\nresponse to positive and negative prompt augmentation, thereby reducing the\nreliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model\nwith optimized denoising and online reward adjustment, we improve its\nhuman-evaluated realism and aesthetic quality by over 3x.",
            "upvotes": 6,
            "discussionId": "68bfd5ea207285de11b07da3",
            "ai_summary": "Direct-Align and Semantic Relative Preference Optimization improve diffusion models' alignment with human preferences by reducing computational costs and minimizing offline reward adaptation.",
            "ai_keywords": [
                "diffusion models",
                "differentiable reward",
                "multistep denoising",
                "gradient computation",
                "noise prior",
                "interpolation",
                "diffusion states",
                "Semantic Relative Preference Optimization",
                "text-conditioned signals",
                "prompt augmentation",
                "FLUX.1.dev model",
                "human-evaluated realism",
                "aesthetic quality"
            ]
        },
        "publishedAt": "2025-09-08T13:54:08.000Z",
        "title": "Directly Aligning the Full Diffusion Trajectory with Fine-Grained Human\n  Preference",
        "summary": "Recent studies have demonstrated the effectiveness of directly aligning\ndiffusion models with human preferences using differentiable reward. However,\nthey exhibit two primary challenges: (1) they rely on multistep denoising with\ngradient computation for reward scoring, which is computationally expensive,\nthus restricting optimization to only a few diffusion steps; (2) they often\nneed continuous offline adaptation of reward models in order to achieve desired\naesthetic quality, such as photorealism or precise lighting effects. To address\nthe limitation of multistep denoising, we propose Direct-Align, a method that\npredefines a noise prior to effectively recover original images from any time\nsteps via interpolation, leveraging the equation that diffusion states are\ninterpolations between noise and target images, which effectively avoids\nover-optimization in late timesteps. Furthermore, we introduce Semantic\nRelative Preference Optimization (SRPO), in which rewards are formulated as\ntext-conditioned signals. This approach enables online adjustment of rewards in\nresponse to positive and negative prompt augmentation, thereby reducing the\nreliance on offline reward fine-tuning. By fine-tuning the FLUX.1.dev model\nwith optimized denoising and online reward adjustment, we improve its\nhuman-evaluated realism and aesthetic quality by over 3x.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06942.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62d22496c58f969c152bcefd",
            "avatarUrl": "/avatars/76c3b70e312f25e1e610473475553c5c.svg",
            "fullname": "Tiezhen WANG",
            "name": "xianbao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 132
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.01624",
            "authors": [
                {
                    "_id": "68b88f34d43cadaf7a688a87",
                    "name": "Natalia Frumkin",
                    "hidden": false
                },
                {
                    "_id": "68b88f34d43cadaf7a688a88",
                    "name": "Diana Marculescu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-01T17:09:22.000Z",
            "submittedOnDailyAt": "2025-09-10T02:50:00.657Z",
            "title": "Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with\n  Quantization-Aware Scheduling",
            "submittedOnDailyBy": {
                "_id": "6503ceb74279ba57791edc35",
                "avatarUrl": "/avatars/a7211b74747bf1aecb9d401e726c5bc1.svg",
                "isPro": false,
                "fullname": "Natalia Frumkin",
                "user": "nfrumkin",
                "type": "user"
            },
            "summary": "Text-to-image diffusion models are computationally intensive, often requiring\ndozens of forward passes through large transformer backbones. For instance,\nStable Diffusion XL generates high-quality images with 50 evaluations of a\n2.6B-parameter model, an expensive process even for a single batch. Few-step\ndiffusion models reduce this cost to 2-8 denoising steps but still depend on\nlarge, uncompressed U-Net or diffusion transformer backbones, which are often\ntoo costly for full-precision inference without datacenter GPUs. These\nrequirements also limit existing post-training quantization methods that rely\non full-precision calibration. We introduce Q-Sched, a new paradigm for\npost-training quantization that modifies the diffusion model scheduler rather\nthan model weights. By adjusting the few-step sampling trajectory, Q-Sched\nachieves full-precision accuracy with a 4x reduction in model size. To learn\nquantization-aware pre-conditioning coefficients, we propose the JAQ loss,\nwhich combines text-image compatibility with an image quality metric for\nfine-grained optimization. JAQ is reference-free and requires only a handful of\ncalibration prompts, avoiding full-precision inference during calibration.\nQ-Sched delivers substantial gains: a 15.5% FID improvement over the FP16\n4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step\nPhased Consistency Model, showing that quantization and few-step distillation\nare complementary for high-fidelity generation. A large-scale user study with\nmore than 80,000 annotations further confirms Q-Sched's effectiveness on both\nFLUX.1[schnell] and SDXL-Turbo.",
            "upvotes": 5,
            "discussionId": "68b88f35d43cadaf7a688a89",
            "githubRepo": "https://github.com/enyac-group/q-sched",
            "ai_summary": "Q-Sched, a novel post-training quantization method for diffusion models, reduces model size by 4x while maintaining full-precision accuracy and improving image quality metrics.",
            "ai_keywords": [
                "text-to-image diffusion models",
                "transformer backbones",
                "Stable Diffusion XL",
                "few-step diffusion models",
                "U-Net",
                "diffusion transformer backbones",
                "post-training quantization",
                "diffusion model scheduler",
                "quantization-aware pre-conditioning coefficients",
                "JAQ loss",
                "text-image compatibility",
                "image quality metric",
                "FID improvement",
                "Latent Consistency Model",
                "Phased Consistency Model",
                "FLUX.1",
                "SDXL-Turbo"
            ],
            "githubStars": 6
        },
        "publishedAt": "2025-09-01T13:09:22.000Z",
        "title": "Q-Sched: Pushing the Boundaries of Few-Step Diffusion Models with\n  Quantization-Aware Scheduling",
        "summary": "Text-to-image diffusion models are computationally intensive, often requiring\ndozens of forward passes through large transformer backbones. For instance,\nStable Diffusion XL generates high-quality images with 50 evaluations of a\n2.6B-parameter model, an expensive process even for a single batch. Few-step\ndiffusion models reduce this cost to 2-8 denoising steps but still depend on\nlarge, uncompressed U-Net or diffusion transformer backbones, which are often\ntoo costly for full-precision inference without datacenter GPUs. These\nrequirements also limit existing post-training quantization methods that rely\non full-precision calibration. We introduce Q-Sched, a new paradigm for\npost-training quantization that modifies the diffusion model scheduler rather\nthan model weights. By adjusting the few-step sampling trajectory, Q-Sched\nachieves full-precision accuracy with a 4x reduction in model size. To learn\nquantization-aware pre-conditioning coefficients, we propose the JAQ loss,\nwhich combines text-image compatibility with an image quality metric for\nfine-grained optimization. JAQ is reference-free and requires only a handful of\ncalibration prompts, avoiding full-precision inference during calibration.\nQ-Sched delivers substantial gains: a 15.5% FID improvement over the FP16\n4-step Latent Consistency Model and a 16.6% improvement over the FP16 8-step\nPhased Consistency Model, showing that quantization and few-step distillation\nare complementary for high-fidelity generation. A large-scale user study with\nmore than 80,000 annotations further confirms Q-Sched's effectiveness on both\nFLUX.1[schnell] and SDXL-Turbo.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01624.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6503ceb74279ba57791edc35",
            "avatarUrl": "/avatars/a7211b74747bf1aecb9d401e726c5bc1.svg",
            "fullname": "Natalia Frumkin",
            "name": "nfrumkin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07558",
            "authors": [
                {
                    "_id": "68c132223912ed54cf5432f8",
                    "name": "Zhiyuan He",
                    "hidden": false
                },
                {
                    "_id": "68c132223912ed54cf5432f9",
                    "name": "Xufang Luo",
                    "hidden": false
                },
                {
                    "_id": "68c132223912ed54cf5432fa",
                    "name": "Yike Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c132223912ed54cf5432fb",
                    "name": "Yuqing Yang",
                    "hidden": false
                },
                {
                    "_id": "68c132223912ed54cf5432fc",
                    "name": "Lili Qiu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T09:52:34.000Z",
            "submittedOnDailyAt": "2025-09-10T06:40:01.132Z",
            "title": "L Normalization: Rethink Loss Aggregation in RLVR",
            "submittedOnDailyBy": {
                "_id": "6455f5cababbbbd3486d6ee3",
                "avatarUrl": "/avatars/b6c8f65fd2bef8a00aa3269856ea238e.svg",
                "isPro": false,
                "fullname": "Zhiyuan He",
                "user": "hzy46",
                "type": "user"
            },
            "summary": "We propose Delta L Normalization, a simple yet effective loss aggregation\nmethod tailored to the characteristic of dynamic generation lengths in\nReinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has\ndemonstrated strong potential in improving the reasoning capabilities of large\nlanguage models (LLMs), but a major challenge lies in the large variability of\nresponse lengths during training, which leads to high gradient variance and\nunstable optimization. Although previous methods such as GRPO, DAPO, and Dr.\nGRPO introduce different loss normalization terms to address this issue, they\neither produce biased estimates or still suffer from high gradient variance. By\nanalyzing the effect of varying lengths on policy loss both theoretically and\nempirically, we reformulate the problem as finding a minimum-variance unbiased\nestimator. Our proposed Delta L Normalization not only provides an unbiased\nestimate of the true policy loss but also minimizes gradient variance in\ntheory. Extensive experiments show that it consistently achieves superior\nresults across different model sizes, maximum lengths, and tasks. Our code will\nbe made public at https://github.com/zerolllin/Delta-L-Normalization.",
            "upvotes": 4,
            "discussionId": "68c132223912ed54cf5432fd",
            "ai_summary": "L Normalization addresses gradient variance in Reinforcement Learning with Verifiable Rewards by providing an unbiased policy loss estimate with minimal variance.",
            "ai_keywords": [
                "L Normalization",
                "loss aggregation",
                "dynamic generation lengths",
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "large language models",
                "LLMs",
                "gradient variance",
                "policy loss",
                "unbiased estimate",
                "GRPO",
                "DAPO",
                "Dr. GRPO"
            ]
        },
        "publishedAt": "2025-09-09T05:52:34.000Z",
        "title": "L Normalization: Rethink Loss Aggregation in RLVR",
        "summary": "We propose Delta L Normalization, a simple yet effective loss aggregation\nmethod tailored to the characteristic of dynamic generation lengths in\nReinforcement Learning with Verifiable Rewards (RLVR). Recently, RLVR has\ndemonstrated strong potential in improving the reasoning capabilities of large\nlanguage models (LLMs), but a major challenge lies in the large variability of\nresponse lengths during training, which leads to high gradient variance and\nunstable optimization. Although previous methods such as GRPO, DAPO, and Dr.\nGRPO introduce different loss normalization terms to address this issue, they\neither produce biased estimates or still suffer from high gradient variance. By\nanalyzing the effect of varying lengths on policy loss both theoretically and\nempirically, we reformulate the problem as finding a minimum-variance unbiased\nestimator. Our proposed Delta L Normalization not only provides an unbiased\nestimate of the true policy loss but also minimizes gradient variance in\ntheory. Extensive experiments show that it consistently achieves superior\nresults across different model sizes, maximum lengths, and tasks. Our code will\nbe made public at https://github.com/zerolllin/Delta-L-Normalization.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07558.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6455f5cababbbbd3486d6ee3",
            "avatarUrl": "/avatars/b6c8f65fd2bef8a00aa3269856ea238e.svg",
            "fullname": "Zhiyuan He",
            "name": "hzy46",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07253",
            "authors": [
                {
                    "_id": "68c189c729b8ec9932cd07c6",
                    "name": "Julian Killingback",
                    "hidden": false
                },
                {
                    "_id": "68c189c729b8ec9932cd07c7",
                    "name": "Hamed Zamani",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T22:11:10.000Z",
            "submittedOnDailyAt": "2025-09-10T13:05:00.742Z",
            "title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks",
            "submittedOnDailyBy": {
                "_id": "635582fb6b58fa7cc8701580",
                "avatarUrl": "/avatars/a00791ba70a2de40dacac4582307c0f2.svg",
                "isPro": false,
                "fullname": "Julian Killingback",
                "user": "jfkback",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are incredible and versatile tools for\ntext-based tasks that have enabled countless, previously unimaginable,\napplications. Retrieval models, in contrast, have not yet seen such capable\ngeneral-purpose models emerge. To achieve this goal, retrieval models must be\nable to perform complex retrieval tasks, where queries contain multiple parts,\nconstraints, or requirements in natural language. These tasks represent a\nnatural progression from the simple, single-aspect queries that are used in the\nvast majority of existing, commonly used evaluation sets. Complex queries\nnaturally arise as people expect search systems to handle more specific and\noften ambitious information requests, as is demonstrated by how people use\nLLM-based information systems. Despite the growing desire for retrieval models\nto expand their capabilities in complex retrieval tasks, there exist limited\nresources to assess the ability of retrieval models on a comprehensive set of\ndiverse complex tasks. The few resources that do exist feature a limited scope\nand often lack realistic settings making it hard to know the true capabilities\nof retrieval models on complex real-world retrieval tasks. To address this\nshortcoming and spur innovation in next-generation retrieval models, we\nconstruct a diverse and realistic set of complex retrieval tasks and benchmark\na representative set of state-of-the-art retrieval models. Additionally, we\nexplore the impact of LLM-based query expansion and rewriting on retrieval\nquality. Our results show that even the best models struggle to produce\nhigh-quality retrieval results with the highest average nDCG@10 of only 0.346\nand R@100 of only 0.587 across all tasks. Although LLM augmentation can help\nweaker models, the strongest model has decreased performance across all metrics\nwith all rewriting techniques.",
            "upvotes": 3,
            "discussionId": "68c189c729b8ec9932cd07c8",
            "ai_summary": "A benchmark of complex retrieval tasks reveals that even state-of-the-art models struggle with high-quality retrieval, and LLM-based query expansion does not consistently improve performance.",
            "ai_keywords": [
                "retrieval models",
                "complex retrieval tasks",
                "LLM-based query expansion",
                "nDCG@10",
                "R@100"
            ]
        },
        "publishedAt": "2025-09-08T18:11:10.000Z",
        "title": "Benchmarking Information Retrieval Models on Complex Retrieval Tasks",
        "summary": "Large language models (LLMs) are incredible and versatile tools for\ntext-based tasks that have enabled countless, previously unimaginable,\napplications. Retrieval models, in contrast, have not yet seen such capable\ngeneral-purpose models emerge. To achieve this goal, retrieval models must be\nable to perform complex retrieval tasks, where queries contain multiple parts,\nconstraints, or requirements in natural language. These tasks represent a\nnatural progression from the simple, single-aspect queries that are used in the\nvast majority of existing, commonly used evaluation sets. Complex queries\nnaturally arise as people expect search systems to handle more specific and\noften ambitious information requests, as is demonstrated by how people use\nLLM-based information systems. Despite the growing desire for retrieval models\nto expand their capabilities in complex retrieval tasks, there exist limited\nresources to assess the ability of retrieval models on a comprehensive set of\ndiverse complex tasks. The few resources that do exist feature a limited scope\nand often lack realistic settings making it hard to know the true capabilities\nof retrieval models on complex real-world retrieval tasks. To address this\nshortcoming and spur innovation in next-generation retrieval models, we\nconstruct a diverse and realistic set of complex retrieval tasks and benchmark\na representative set of state-of-the-art retrieval models. Additionally, we\nexplore the impact of LLM-based query expansion and rewriting on retrieval\nquality. Our results show that even the best models struggle to produce\nhigh-quality retrieval results with the highest average nDCG@10 of only 0.346\nand R@100 of only 0.587 across all tasks. Although LLM augmentation can help\nweaker models, the strongest model has decreased performance across all metrics\nwith all rewriting techniques.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07253.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "635582fb6b58fa7cc8701580",
            "avatarUrl": "/avatars/a00791ba70a2de40dacac4582307c0f2.svg",
            "fullname": "Julian Killingback",
            "name": "jfkback",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06938",
            "authors": [
                {
                    "_id": "68c1bc3a29b8ec9932cd080e",
                    "name": "Praneet Suresh",
                    "hidden": false
                },
                {
                    "_id": "68c1bc3a29b8ec9932cd080f",
                    "name": "Jack Stanley",
                    "hidden": false
                },
                {
                    "_id": "68c1bc3a29b8ec9932cd0810",
                    "name": "Sonia Joseph",
                    "hidden": false
                },
                {
                    "_id": "68c1bc3a29b8ec9932cd0811",
                    "name": "Luca Scimeca",
                    "hidden": false
                },
                {
                    "_id": "68c1bc3a29b8ec9932cd0812",
                    "name": "Danilo Bzdok",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T17:50:45.000Z",
            "submittedOnDailyAt": "2025-09-10T16:30:14.002Z",
            "title": "From Noise to Narrative: Tracing the Origins of Hallucinations in\n  Transformers",
            "submittedOnDailyBy": {
                "_id": "62f678d2d278a8f3e7842621",
                "avatarUrl": "/avatars/330cc1fb81f5c7c9f8276316c2ec186d.svg",
                "isPro": false,
                "fullname": "Praneet Suresh",
                "user": "PraneetNeuro",
                "type": "user"
            },
            "summary": "As generative AI systems become competent and democratized in science,\nbusiness, and government, deeper insight into their failure modes now poses an\nacute need. The occasional volatility in their behavior, such as the propensity\nof transformer models to hallucinate, impedes trust and adoption of emerging AI\nsolutions in high-stakes areas. In the present work, we establish how and when\nhallucinations arise in pre-trained transformer models through concept\nrepresentations captured by sparse autoencoders, under scenarios with\nexperimentally controlled uncertainty in the input space. Our systematic\nexperiments reveal that the number of semantic concepts used by the transformer\nmodel grows as the input information becomes increasingly unstructured. In the\nface of growing uncertainty in the input space, the transformer model becomes\nprone to activate coherent yet input-insensitive semantic features, leading to\nhallucinated output. At its extreme, for pure-noise inputs, we identify a wide\nvariety of robustly triggered and meaningful concepts in the intermediate\nactivations of pre-trained transformer models, whose functional integrity we\nconfirm through targeted steering. We also show that hallucinations in the\noutput of a transformer model can be reliably predicted from the concept\npatterns embedded in transformer layer activations. This collection of insights\non transformer internal processing mechanics has immediate consequences for\naligning AI models with human values, AI safety, opening the attack surface for\npotential adversarial attacks, and providing a basis for automatic\nquantification of a model's hallucination risk.",
            "upvotes": 1,
            "discussionId": "68c1bc3b29b8ec9932cd0813",
            "ai_summary": "Transformer models tend to activate input-insensitive semantic features under uncertainty, leading to hallucinations that can be predicted from their internal activations.",
            "ai_keywords": [
                "transformer models",
                "hallucinations",
                "sparse autoencoders",
                "input space",
                "semantic concepts",
                "intermediate activations",
                "adversarial attacks",
                "hallucination risk"
            ]
        },
        "publishedAt": "2025-09-08T13:50:45.000Z",
        "title": "From Noise to Narrative: Tracing the Origins of Hallucinations in\n  Transformers",
        "summary": "As generative AI systems become competent and democratized in science,\nbusiness, and government, deeper insight into their failure modes now poses an\nacute need. The occasional volatility in their behavior, such as the propensity\nof transformer models to hallucinate, impedes trust and adoption of emerging AI\nsolutions in high-stakes areas. In the present work, we establish how and when\nhallucinations arise in pre-trained transformer models through concept\nrepresentations captured by sparse autoencoders, under scenarios with\nexperimentally controlled uncertainty in the input space. Our systematic\nexperiments reveal that the number of semantic concepts used by the transformer\nmodel grows as the input information becomes increasingly unstructured. In the\nface of growing uncertainty in the input space, the transformer model becomes\nprone to activate coherent yet input-insensitive semantic features, leading to\nhallucinated output. At its extreme, for pure-noise inputs, we identify a wide\nvariety of robustly triggered and meaningful concepts in the intermediate\nactivations of pre-trained transformer models, whose functional integrity we\nconfirm through targeted steering. We also show that hallucinations in the\noutput of a transformer model can be reliably predicted from the concept\npatterns embedded in transformer layer activations. This collection of insights\non transformer internal processing mechanics has immediate consequences for\naligning AI models with human values, AI safety, opening the attack surface for\npotential adversarial attacks, and providing a basis for automatic\nquantification of a model's hallucination risk.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06938.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f678d2d278a8f3e7842621",
            "avatarUrl": "/avatars/330cc1fb81f5c7c9f8276316c2ec186d.svg",
            "fullname": "Praneet Suresh",
            "name": "PraneetNeuro",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": false
    }
]