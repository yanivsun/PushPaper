[
    {
        "paper": {
            "id": "2509.08827",
            "authors": [
                {
                    "_id": "68c228e829b8ec9932cd08ca",
                    "name": "Kaiyan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08cb",
                    "name": "Yuxin Zuo",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08cc",
                    "name": "Bingxiang He",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08cd",
                    "name": "Youbang Sun",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ce",
                    "name": "Runze Liu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08cf",
                    "name": "Che Jiang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d0",
                    "name": "Yuchen Fan",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d1",
                    "name": "Kai Tian",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d2",
                    "name": "Guoli Jia",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d3",
                    "name": "Pengfei Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d4",
                    "name": "Yu Fu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d5",
                    "name": "Xingtai Lv",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d6",
                    "name": "Yuchen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d7",
                    "name": "Sihang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d8",
                    "name": "Shang Qu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08d9",
                    "name": "Haozhan Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08da",
                    "name": "Shijie Wang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08db",
                    "name": "Yuru Wang",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08dc",
                    "name": "Xinwei Long",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08dd",
                    "name": "Fangfu Liu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08de",
                    "name": "Xiang Xu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08df",
                    "name": "Jiaze Ma",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e0",
                    "name": "Xuekai Zhu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e1",
                    "name": "Ermo Hua",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e2",
                    "name": "Yihao Liu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e3",
                    "name": "Zonglin Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e4",
                    "name": "Huayu Chen",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e5",
                    "name": "Xiaoye Qu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e6",
                    "name": "Yafu Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e7",
                    "name": "Weize Chen",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e8",
                    "name": "Zhenzhao Yuan",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08e9",
                    "name": "Junqi Gao",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ea",
                    "name": "Dong Li",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08eb",
                    "name": "Zhiyuan Ma",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ec",
                    "name": "Ganqu Cui",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ed",
                    "name": "Zhiyuan Liu",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ee",
                    "name": "Biqing Qi",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08ef",
                    "name": "Ning Ding",
                    "hidden": false
                },
                {
                    "_id": "68c228e829b8ec9932cd08f0",
                    "name": "Bowen Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T17:59:43.000Z",
            "submittedOnDailyAt": "2025-09-11T00:49:36.249Z",
            "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
            "submittedOnDailyBy": {
                "_id": "60bc94cd85a3ab33829b6211",
                "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
                "isPro": false,
                "fullname": "Kaiyan Zhang",
                "user": "iseesaw",
                "type": "user"
            },
            "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
            "upvotes": 98,
            "discussionId": "68c228e929b8ec9932cd08f1",
            "projectPage": "https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
            "githubRepo": "https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
            "ai_summary": "Reinforcement Learning enhances Large Language Models for complex reasoning tasks, facing challenges in scalability and infrastructure as the field advances.",
            "ai_keywords": [
                "Reinforcement Learning",
                "Large Language Models",
                "LRMs",
                "DeepSeek-R1",
                "Artificial SuperIntelligence"
            ],
            "githubStars": 889
        },
        "publishedAt": "2025-09-10T13:59:43.000Z",
        "title": "A Survey of Reinforcement Learning for Large Reasoning Models",
        "summary": "In this paper, we survey recent advances in Reinforcement Learning (RL) for\nreasoning with Large Language Models (LLMs). RL has achieved remarkable success\nin advancing the frontier of LLM capabilities, particularly in addressing\ncomplex logical tasks such as mathematics and coding. As a result, RL has\nemerged as a foundational methodology for transforming LLMs into LRMs. With the\nrapid progress of the field, further scaling of RL for LRMs now faces\nfoundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely\nto revisit the development of this domain, reassess its trajectory, and explore\nstrategies to enhance the scalability of RL toward Artificial SuperIntelligence\n(ASI). In particular, we examine research applying RL to LLMs and LRMs for\nreasoning abilities, especially since the release of DeepSeek-R1, including\nfoundational components, core problems, training resources, and downstream\napplications, to identify future opportunities and directions for this rapidly\nevolving area. We hope this review will promote future research on RL for\nbroader reasoning models. Github:\nhttps://github.com/TsinghuaC3I/Awesome-RL-for-LRMs",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08827.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "60bc94cd85a3ab33829b6211",
            "avatarUrl": "/avatars/b57d36c7577fbbb42ea5b963eef4144a.svg",
            "fullname": "Kaiyan Zhang",
            "name": "iseesaw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.08826",
            "authors": [
                {
                    "_id": "68c2308c29b8ec9932cd08f3",
                    "name": "Jie Wu",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f4",
                    "name": "Yu Gao",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f5",
                    "name": "Zilyu Ye",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f6",
                    "name": "Ming Li",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f7",
                    "name": "Liang Li",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f8",
                    "name": "Hanzhong Guo",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08f9",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fa",
                    "name": "Zeyue Xue",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fb",
                    "name": "Xiaoxia Hou",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fc",
                    "name": "Wei Liu",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fd",
                    "name": "Yan Zeng",
                    "hidden": false
                },
                {
                    "_id": "68c2308c29b8ec9932cd08fe",
                    "name": "Weilin Huang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T17:59:31.000Z",
            "submittedOnDailyAt": "2025-09-11T00:44:48.257Z",
            "title": "RewardDance: Reward Scaling in Visual Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Reward Models (RMs) are critical for improving generation models via\nReinforcement Learning (RL), yet the RM scaling paradigm in visual generation\nremains largely unexplored. It primarily due to fundamental limitations in\nexisting approaches: CLIP-based RMs suffer from architectural and input\nmodality constraints, while prevalent Bradley-Terry losses are fundamentally\nmisaligned with the next-token prediction mechanism of Vision-Language Models\n(VLMs), hindering effective scaling. More critically, the RLHF optimization\nprocess is plagued by Reward Hacking issue, where models exploit flaws in the\nreward signal without improving true quality. To address these challenges, we\nintroduce RewardDance, a scalable reward modeling framework that overcomes\nthese barriers through a novel generative reward paradigm. By reformulating the\nreward score as the model's probability of predicting a \"yes\" token, indicating\nthat the generated image outperforms a reference image according to specific\ncriteria, RewardDance intrinsically aligns reward objectives with VLM\narchitectures. This alignment unlocks scaling across two dimensions: (1) Model\nScaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context\nScaling: Integration of task-specific instructions, reference examples, and\nchain-of-thought (CoT) reasoning. Extensive experiments demonstrate that\nRewardDance significantly surpasses state-of-the-art methods in text-to-image,\ntext-to-video, and image-to-video generation. Crucially, we resolve the\npersistent challenge of \"reward hacking\": Our large-scale RMs exhibit and\nmaintain high reward variance during RL fine-tuning, proving their resistance\nto hacking and ability to produce diverse, high-quality outputs. It greatly\nrelieves the mode collapse problem that plagues smaller models.",
            "upvotes": 50,
            "discussionId": "68c2308c29b8ec9932cd08ff",
            "ai_summary": "RewardDance is a scalable reward modeling framework that aligns with VLM architectures, enabling effective scaling of RMs and resolving reward hacking issues in generation models.",
            "ai_keywords": [
                "CLIP-based RMs",
                "Bradley-Terry losses",
                "Vision-Language Models (VLMs)",
                "RLHF optimization",
                "Reward Hacking",
                "RewardDance",
                "generative reward paradigm",
                "model scaling",
                "context scaling",
                "task-specific instructions",
                "reference examples",
                "chain-of-thought (CoT) reasoning",
                "text-to-image",
                "text-to-video",
                "image-to-video generation",
                "mode collapse"
            ]
        },
        "publishedAt": "2025-09-10T13:59:31.000Z",
        "title": "RewardDance: Reward Scaling in Visual Generation",
        "summary": "Reward Models (RMs) are critical for improving generation models via\nReinforcement Learning (RL), yet the RM scaling paradigm in visual generation\nremains largely unexplored. It primarily due to fundamental limitations in\nexisting approaches: CLIP-based RMs suffer from architectural and input\nmodality constraints, while prevalent Bradley-Terry losses are fundamentally\nmisaligned with the next-token prediction mechanism of Vision-Language Models\n(VLMs), hindering effective scaling. More critically, the RLHF optimization\nprocess is plagued by Reward Hacking issue, where models exploit flaws in the\nreward signal without improving true quality. To address these challenges, we\nintroduce RewardDance, a scalable reward modeling framework that overcomes\nthese barriers through a novel generative reward paradigm. By reformulating the\nreward score as the model's probability of predicting a \"yes\" token, indicating\nthat the generated image outperforms a reference image according to specific\ncriteria, RewardDance intrinsically aligns reward objectives with VLM\narchitectures. This alignment unlocks scaling across two dimensions: (1) Model\nScaling: Systematic scaling of RMs up to 26 billion parameters; (2) Context\nScaling: Integration of task-specific instructions, reference examples, and\nchain-of-thought (CoT) reasoning. Extensive experiments demonstrate that\nRewardDance significantly surpasses state-of-the-art methods in text-to-image,\ntext-to-video, and image-to-video generation. Crucially, we resolve the\npersistent challenge of \"reward hacking\": Our large-scale RMs exhibit and\nmaintain high reward variance during RL fine-tuning, proving their resistance\nto hacking and ability to produce diverse, high-quality outputs. It greatly\nrelieves the mode collapse problem that plagues smaller models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08826.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 103
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07996",
            "authors": [
                {
                    "_id": "68c23b0629b8ec9932cd095b",
                    "name": "Lingdong Kong",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd095c",
                    "name": "Wesley Yang",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd095d",
                    "name": "Jianbiao Mei",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd095e",
                    "name": "Youquan Liu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd095f",
                    "name": "Ao Liang",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0960",
                    "name": "Dekai Zhu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0961",
                    "name": "Dongyue Lu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0962",
                    "name": "Wei Yin",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0963",
                    "name": "Xiaotao Hu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0964",
                    "name": "Mingkai Jia",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0965",
                    "name": "Junyuan Deng",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0966",
                    "name": "Kaiwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0967",
                    "name": "Yang Wu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0968",
                    "name": "Tianyi Yan",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0969",
                    "name": "Shenyuan Gao",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096a",
                    "name": "Song Wang",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096b",
                    "name": "Linfeng Li",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096c",
                    "name": "Liang Pan",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096d",
                    "name": "Yong Liu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096e",
                    "name": "Jianke Zhu",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd096f",
                    "name": "Wei Tsang Ooi",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0970",
                    "name": "Steven C. H. Hoi",
                    "hidden": false
                },
                {
                    "_id": "68c23b0629b8ec9932cd0971",
                    "name": "Ziwei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-04T17:59:58.000Z",
            "submittedOnDailyAt": "2025-09-11T01:29:36.490Z",
            "title": "3D and 4D World Modeling: A Survey",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "World modeling has become a cornerstone in AI research, enabling agents to\nunderstand, represent, and predict the dynamic environments they inhabit. While\nprior work largely emphasizes generative methods for 2D image and video data,\nthey overlook the rapidly growing body of work that leverages native 3D and 4D\nrepresentations such as RGB-D imagery, occupancy grids, and LiDAR point clouds\nfor large-scale scene modeling. At the same time, the absence of a standardized\ndefinition and taxonomy for ``world models'' has led to fragmented and\nsometimes inconsistent claims in the literature. This survey addresses these\ngaps by presenting the first comprehensive review explicitly dedicated to 3D\nand 4D world modeling and generation. We establish precise definitions,\nintroduce a structured taxonomy spanning video-based (VideoGen),\noccupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and\nsystematically summarize datasets and evaluation metrics tailored to 3D/4D\nsettings. We further discuss practical applications, identify open challenges,\nand highlight promising research directions, aiming to provide a coherent and\nfoundational reference for advancing the field. A systematic summary of\nexisting literature is available at https://github.com/worldbench/survey",
            "upvotes": 39,
            "discussionId": "68c23b0629b8ec9932cd0972",
            "githubRepo": "https://github.com/worldbench/survey",
            "ai_summary": "This survey provides a comprehensive review of 3D and 4D world modeling and generation, establishing definitions, taxonomy, datasets, and evaluation metrics, and discussing applications and challenges.",
            "ai_keywords": [
                "world models",
                "3D world modeling",
                "4D world modeling",
                "generative methods",
                "RGB-D imagery",
                "occupancy grids",
                "LiDAR point clouds",
                "VideoGen",
                "OccGen",
                "LiDARGen",
                "datasets",
                "evaluation metrics",
                "applications",
                "open challenges",
                "research directions"
            ],
            "githubStars": 189
        },
        "publishedAt": "2025-09-04T13:59:58.000Z",
        "title": "3D and 4D World Modeling: A Survey",
        "summary": "World modeling has become a cornerstone in AI research, enabling agents to\nunderstand, represent, and predict the dynamic environments they inhabit. While\nprior work largely emphasizes generative methods for 2D image and video data,\nthey overlook the rapidly growing body of work that leverages native 3D and 4D\nrepresentations such as RGB-D imagery, occupancy grids, and LiDAR point clouds\nfor large-scale scene modeling. At the same time, the absence of a standardized\ndefinition and taxonomy for ``world models'' has led to fragmented and\nsometimes inconsistent claims in the literature. This survey addresses these\ngaps by presenting the first comprehensive review explicitly dedicated to 3D\nand 4D world modeling and generation. We establish precise definitions,\nintroduce a structured taxonomy spanning video-based (VideoGen),\noccupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, and\nsystematically summarize datasets and evaluation metrics tailored to 3D/4D\nsettings. We further discuss practical applications, identify open challenges,\nand highlight promising research directions, aiming to provide a coherent and\nfoundational reference for advancing the field. A systematic summary of\nexisting literature is available at https://github.com/worldbench/survey",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07996.png",
        "numComments": 4,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 103
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.08755",
            "authors": [
                {
                    "_id": "68c2329529b8ec9932cd090a",
                    "name": "Zhiheng Xi",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090b",
                    "name": "Jixuan Huang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090c",
                    "name": "Chenyang Liao",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090d",
                    "name": "Baodai Huang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090e",
                    "name": "Honglin Guo",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd090f",
                    "name": "Jiaqi Liu",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0910",
                    "name": "Rui Zheng",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0911",
                    "name": "Junjie Ye",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0912",
                    "name": "Jiazheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0913",
                    "name": "Wenxiang Chen",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0914",
                    "name": "Wei He",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0915",
                    "name": "Yiwen Ding",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0916",
                    "name": "Guanyu Li",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0917",
                    "name": "Zehui Chen",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0918",
                    "name": "Zhengyin Du",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0919",
                    "name": "Xuesong Yao",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091a",
                    "name": "Yufei Xu",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091b",
                    "name": "Jiecao Chen",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091c",
                    "name": "Tao Gui",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091d",
                    "name": "Zuxuan Wu",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091e",
                    "name": "Qi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd091f",
                    "name": "Xuanjing Huang",
                    "hidden": false
                },
                {
                    "_id": "68c2329529b8ec9932cd0920",
                    "name": "Yu-Gang Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T16:46:11.000Z",
            "submittedOnDailyAt": "2025-09-11T00:53:25.051Z",
            "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
            "upvotes": 21,
            "discussionId": "68c2329529b8ec9932cd0921",
            "projectPage": "https://agentgym-rl.github.io/",
            "githubRepo": "https://github.com/WooooDyy/AgentGym-RL",
            "ai_summary": "AgentGym-RL is a modular RL framework for training LLM agents in diverse environments without supervised fine-tuning, featuring ScalingInter-RL for balanced exploration-exploitation.",
            "ai_keywords": [
                "reinforcement learning",
                "RL",
                "modular architecture",
                "decoupled architecture",
                "exploration-exploitation balance",
                "ScalingInter-RL",
                "LLM agents",
                "multi-turn decision-making",
                "diverse environments",
                "commercial models"
            ],
            "githubStars": 116
        },
        "publishedAt": "2025-09-10T12:46:11.000Z",
        "title": "AgentGym-RL: Training LLM Agents for Long-Horizon Decision Making\n  through Multi-Turn Reinforcement Learning",
        "summary": "Developing autonomous LLM agents capable of making a series of intelligent\ndecisions to solve complex, real-world tasks is a fast-evolving frontier. Like\nhuman cognitive development, agents are expected to acquire knowledge and\nskills through exploration and interaction with the environment. Despite\nadvances, the community still lacks a unified, interactive reinforcement\nlearning (RL) framework that can effectively train such agents from scratch --\nwithout relying on supervised fine-tuning (SFT) -- across diverse and realistic\nenvironments. To bridge this gap, we introduce AgentGym-RL, a new framework to\ntrain LLM agents for multi-turn interactive decision-making through RL. The\nframework features a modular and decoupled architecture, ensuring high\nflexibility and extensibility. It encompasses a wide variety of real-world\nscenarios, and supports mainstream RL algorithms. Furthermore, we propose\nScalingInter-RL, a training approach designed for exploration-exploitation\nbalance and stable RL optimization. In early stages, it emphasizes exploitation\nby restricting the number of interactions, and gradually shifts towards\nexploration with larger horizons to encourage diverse problem-solving\nstrategies. In this way, the agent develops more diverse behaviors and is less\nprone to collapse under long horizons. We perform extensive experiments to\nvalidate the stability and effectiveness of both the AgentGym-RL framework and\nthe ScalingInter-RL approach. Our agents match or surpass commercial models on\n27 tasks across diverse environments. We offer key insights and will\nopen-source the complete AgentGym-RL framework -- including code and datasets\n-- to empower the research community in developing the next generation of\nintelligent agents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08755.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 103
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06784",
            "authors": [
                {
                    "_id": "68bfa444207285de11b07c13",
                    "user": {
                        "_id": "66fe2aadd5ede3018ab704bf",
                        "avatarUrl": "/avatars/46f2ae3717d1dfa5773d419e4dc9a891.svg",
                        "isPro": false,
                        "fullname": "Changhangfeng MA",
                        "user": "murcherful",
                        "type": "user"
                    },
                    "name": "Changfeng Ma",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:47:28.483Z",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c14",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c15",
                    "name": "Xinhao Yan",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c16",
                    "name": "Jiachen Xu",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c17",
                    "name": "Yunhan Yang",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c18",
                    "name": "Chunshi Wang",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c19",
                    "name": "Zibo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c1a",
                    "name": "Yanwen Guo",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c1b",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68bfa444207285de11b07c1c",
                    "name": "Chunchao Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T15:12:17.000Z",
            "submittedOnDailyAt": "2025-09-11T01:06:10.468Z",
            "title": "P3-SAM: Native 3D Part Segmentation",
            "submittedOnDailyBy": {
                "_id": "66fe2aadd5ede3018ab704bf",
                "avatarUrl": "/avatars/46f2ae3717d1dfa5773d419e4dc9a891.svg",
                "isPro": false,
                "fullname": "Changhangfeng MA",
                "user": "murcherful",
                "type": "user"
            },
            "summary": "Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.",
            "upvotes": 13,
            "discussionId": "68bfa444207285de11b07c1d",
            "ai_summary": "P3-SAM, a native 3D point-promptable part segmentation model, achieves precise and robust segmentation of complex 3D objects using a feature extractor, multiple segmentation heads, and an IoU predictor.",
            "ai_keywords": [
                "3D point-promptable part segmentation",
                "P3-SAM",
                "feature extractor",
                "segmentation heads",
                "IoU predictor",
                "interactive segmentation",
                "part instance segmentation",
                "dataset"
            ]
        },
        "publishedAt": "2025-09-08T11:12:17.000Z",
        "title": "P3-SAM: Native 3D Part Segmentation",
        "summary": "Segmenting 3D assets into their constituent parts is crucial for enhancing 3D\nunderstanding, facilitating model reuse, and supporting various applications\nsuch as part generation. However, current methods face limitations such as poor\nrobustness when dealing with complex objects and cannot fully automate the\nprocess. In this paper, we propose a native 3D point-promptable part\nsegmentation model termed P3-SAM, designed to fully automate the segmentation\nof any 3D objects into components. Inspired by SAM, P3-SAM consists of a\nfeature extractor, multiple segmentation heads, and an IoU predictor, enabling\ninteractive segmentation for users. We also propose an algorithm to\nautomatically select and merge masks predicted by our model for part instance\nsegmentation. Our model is trained on a newly built dataset containing nearly\n3.7 million models with reasonable segmentation labels. Comparisons show that\nour method achieves precise segmentation results and strong robustness on any\ncomplex objects, attaining state-of-the-art performance. Our code will be\nreleased soon.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06784.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66fe2aadd5ede3018ab704bf",
            "avatarUrl": "/avatars/46f2ae3717d1dfa5773d419e4dc9a891.svg",
            "fullname": "Changhangfeng MA",
            "name": "murcherful",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.05209",
            "authors": [
                {
                    "_id": "68be5ad2c123124955ef60b2",
                    "name": "Mao Zheng",
                    "hidden": false
                },
                {
                    "_id": "68be5ad2c123124955ef60b3",
                    "name": "Zheng Li",
                    "hidden": false
                },
                {
                    "_id": "68be5ad2c123124955ef60b4",
                    "name": "Bingxin Qu",
                    "hidden": false
                },
                {
                    "_id": "68be5ad2c123124955ef60b5",
                    "name": "Mingyang Song",
                    "hidden": false
                },
                {
                    "_id": "68be5ad2c123124955ef60b6",
                    "name": "Yang Du",
                    "hidden": false
                },
                {
                    "_id": "68be5ad2c123124955ef60b7",
                    "name": "Mingrui Sun",
                    "hidden": false
                },
                {
                    "_id": "68be5ad2c123124955ef60b8",
                    "name": "Di Wang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-05T16:11:05.000Z",
            "submittedOnDailyAt": "2025-09-11T00:58:23.616Z",
            "title": "Hunyuan-MT Technical Report",
            "submittedOnDailyBy": {
                "_id": "617051728db4a760d912d81f",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617051728db4a760d912d81f/4dKG176GPk2pPRK7ctaT-.jpeg",
                "isPro": false,
                "fullname": "Mingyang Song",
                "user": "Nickyang",
                "type": "user"
            },
            "summary": "In this report, we introduce Hunyuan-MT-7B, our first open-source\nmultilingual translation model, which supports bidirectional translation across\n33 major languages and places a special emphasis on translation between\nMandarin and several ethnic minority languages as well as dialects.\nFurthermore, to serve and address diverse translation scenarios and enhance\nmodel performance at test time, we introduce Hunyuan-MT-Chimera-7B, a\ntranslation model inspired by the slow thinking mode. This model integrates\nmultiple outputs generated by the Hunyuan-MT-7B model under varying parameter\nsettings, thereby achieving performance superior to that of conventional\nslow-thinking models based on Chain-of-Thought (CoT). The development of our\nmodels follows a holistic training process specifically engineered for\nmultilingual translation, which begins with general and MT-oriented\npre-training to build foundational capabilities, proceeds to Supervised\nFine-Tuning (SFT) for task-specific adaptation, and culminates in advanced\nalignment through Reinforcement Learning (RL) and weak-to-strong RL. Through\ncomprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and\nHunyuan-MT-Chimera-7B significantly outperform all translation-specific models\nof comparable parameter size and most of the SOTA large models, particularly on\nthe task of translation between Mandarin and minority languages as well as\ndialects. In the WMT2025 shared task (General Machine Translation), our models\ndemonstrate state-of-the-art performance, ranking first in 30 out of 31\nlanguage pairs. This result highlights the robustness of our models across a\ndiverse linguistic spectrum, encompassing high-resource languages such as\nChinese, English, and Japanese, as well as low-resource languages including\nCzech, Marathi, Estonian, and Icelandic.",
            "upvotes": 9,
            "discussionId": "68be5ad2c123124955ef60b9",
            "projectPage": "https://github.com/Tencent-Hunyuan/Hunyuan-MT",
            "githubRepo": "https://github.com/Tencent-Hunyuan/Hunyuan-MT",
            "ai_summary": "Hunyuan-MT-7B and Hunyuan-MT-Chimera-7B are multilingual translation models that outperform existing models, especially in translating between Mandarin and minority languages, through a combination of pre-training, supervised fine-tuning, and reinforcement learning.",
            "ai_keywords": [
                "multilingual translation",
                "bidirectional translation",
                "Hunyuan-MT-7B",
                "Hunyuan-MT-Chimera-7B",
                "slow thinking mode",
                "Chain-of-Thought (CoT)",
                "Supervised Fine-Tuning (SFT)",
                "Reinforcement Learning (RL)",
                "weak-to-strong RL",
                "WMT2025",
                "state-of-the-art performance"
            ],
            "githubStars": 482
        },
        "publishedAt": "2025-09-05T12:11:05.000Z",
        "title": "Hunyuan-MT Technical Report",
        "summary": "In this report, we introduce Hunyuan-MT-7B, our first open-source\nmultilingual translation model, which supports bidirectional translation across\n33 major languages and places a special emphasis on translation between\nMandarin and several ethnic minority languages as well as dialects.\nFurthermore, to serve and address diverse translation scenarios and enhance\nmodel performance at test time, we introduce Hunyuan-MT-Chimera-7B, a\ntranslation model inspired by the slow thinking mode. This model integrates\nmultiple outputs generated by the Hunyuan-MT-7B model under varying parameter\nsettings, thereby achieving performance superior to that of conventional\nslow-thinking models based on Chain-of-Thought (CoT). The development of our\nmodels follows a holistic training process specifically engineered for\nmultilingual translation, which begins with general and MT-oriented\npre-training to build foundational capabilities, proceeds to Supervised\nFine-Tuning (SFT) for task-specific adaptation, and culminates in advanced\nalignment through Reinforcement Learning (RL) and weak-to-strong RL. Through\ncomprehensive experimentation, we demonstrate that both Hunyuan-MT-7B and\nHunyuan-MT-Chimera-7B significantly outperform all translation-specific models\nof comparable parameter size and most of the SOTA large models, particularly on\nthe task of translation between Mandarin and minority languages as well as\ndialects. In the WMT2025 shared task (General Machine Translation), our models\ndemonstrate state-of-the-art performance, ranking first in 30 out of 31\nlanguage pairs. This result highlights the robustness of our models across a\ndiverse linguistic spectrum, encompassing high-resource languages such as\nChinese, English, and Japanese, as well as low-resource languages including\nCzech, Marathi, Estonian, and Icelandic.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.05209.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "617051728db4a760d912d81f",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/617051728db4a760d912d81f/4dKG176GPk2pPRK7ctaT-.jpeg",
            "fullname": "Mingyang Song",
            "name": "Nickyang",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.09675",
            "authors": [
                {
                    "_id": "68c376fbfc1747b912403987",
                    "name": "Runpeng Dai",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b912403988",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b912403989",
                    "name": "Haolin Liu",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b91240398a",
                    "name": "Zhenwen Liang",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b91240398b",
                    "name": "Dian Yu",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b91240398c",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b91240398d",
                    "name": "Zhaopeng Tu",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b91240398e",
                    "name": "Rui Liu",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b91240398f",
                    "name": "Tong Zheng",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b912403990",
                    "name": "Hongtu Zhu",
                    "hidden": false
                },
                {
                    "_id": "68c376fbfc1747b912403991",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-11T17:59:17.000Z",
            "submittedOnDailyAt": "2025-09-11T23:57:39.917Z",
            "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6623ea65b642e29cdf90a1b4",
                "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
                "isPro": true,
                "fullname": "TongZheng",
                "user": "TongZheng1999",
                "type": "user"
            },
            "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.",
            "upvotes": 7,
            "discussionId": "68c376fcfc1747b912403992",
            "ai_summary": "Curiosity-Driven Exploration (CDE) enhances Reinforcement Learning with Verifiable Rewards (RLVR) by using intrinsic curiosity signals from the actor and critic to improve exploration and reduce premature convergence in Large Language Models (LLMs).",
            "ai_keywords": [
                "Reinforcement Learning with Verifiable Rewards",
                "RLVR",
                "Large Language Models",
                "LLMs",
                "Curiosity-Driven Exploration",
                "CDE",
                "actor",
                "critic",
                "perplexity",
                "value estimates",
                "multi-head architecture",
                "exploration bonus",
                "overconfident errors",
                "diversity",
                "count-based exploration bonus",
                "calibration collapse"
            ]
        },
        "publishedAt": "2025-09-11T13:59:17.000Z",
        "title": "CDE: Curiosity-Driven Exploration for Efficient Reinforcement Learning\n  in Large Language Models",
        "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful paradigm\nfor enhancing the reasoning ability of Large Language Models (LLMs). Yet\ncurrent RLVR methods often explore poorly, leading to premature convergence and\nentropy collapse. To address this challenge, we introduce Curiosity-Driven\nExploration (CDE), a framework that leverages the model's own intrinsic sense\nof curiosity to guide exploration. We formalize curiosity with signals from\nboth the actor and the critic: for the actor, we use perplexity over its\ngenerated response, and for the critic, we use the variance of value estimates\nfrom a multi-head architecture. Both signals serve as an exploration bonus\nwithin the RLVR framework to guide the model. Our theoretical analysis shows\nthat the actor-wise bonus inherently penalizes overconfident errors and\npromotes diversity among correct responses; moreover, we connect the\ncritic-wise bonus to the well-established count-based exploration bonus in RL.\nEmpirically, our method achieves an approximate +3 point improvement over\nstandard RLVR using GRPO/PPO on AIME benchmarks. Further analysis identifies a\ncalibration collapse mechanism within RLVR, shedding light on common LLM\nfailure modes.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09675.png",
        "numComments": 1,
        "submittedBy": {
            "_id": "6623ea65b642e29cdf90a1b4",
            "avatarUrl": "/avatars/e32e90574c1162b2be87ed78604e3e4d.svg",
            "fullname": "TongZheng",
            "name": "TongZheng1999",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.06870",
            "authors": [
                {
                    "_id": "68bfa818207285de11b07c76",
                    "name": "Wenting Zhao",
                    "hidden": false
                },
                {
                    "_id": "68bfa818207285de11b07c77",
                    "name": "Pranjal Aggarwal",
                    "hidden": false
                },
                {
                    "_id": "68bfa818207285de11b07c78",
                    "name": "Swarnadeep Saha",
                    "hidden": false
                },
                {
                    "_id": "68bfa818207285de11b07c79",
                    "name": "Asli Celikyilmaz",
                    "hidden": false
                },
                {
                    "_id": "68bfa818207285de11b07c7a",
                    "name": "Jason Weston",
                    "hidden": false
                },
                {
                    "_id": "68bfa818207285de11b07c7b",
                    "name": "Ilia Kulikov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T16:39:38.000Z",
            "submittedOnDailyAt": "2025-09-11T12:35:25.749Z",
            "title": "The Majority is not always right: RL training for solution aggregation",
            "submittedOnDailyBy": {
                "_id": "62f023a36a027498eaa2f9cc",
                "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
                "isPro": false,
                "fullname": "Jason Weston",
                "user": "spermwhale",
                "type": "user"
            },
            "summary": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions.",
            "upvotes": 6,
            "discussionId": "68bfa819207285de11b07c7c",
            "ai_summary": "A reinforcement learning approach to aggregating multiple solutions for large language models improves performance on reasoning tasks by learning to synthesize correct answers from candidate solutions.",
            "ai_keywords": [
                "large language models",
                "reinforcement learning",
                "aggregation",
                "reasoning tasks",
                "verifiable rewards",
                "AggLM"
            ]
        },
        "publishedAt": "2025-09-08T12:39:38.000Z",
        "title": "The Majority is not always right: RL training for solution aggregation",
        "summary": "Scaling up test-time compute, by generating multiple independent solutions\nand selecting or aggregating among them, has become a central paradigm for\nimproving large language models (LLMs) on challenging reasoning tasks. While\nmost prior work relies on simple majority voting or reward model ranking to\naggregate solutions, these approaches may only yield limited benefits. In this\nwork, we propose to learn aggregation as an explicit reasoning skill: given a\nset of candidate solutions, we train an aggregator model to review, reconcile,\nand synthesize a final, correct answer using reinforcement learning from\nverifiable rewards. A key ingredient is careful balancing of easy and hard\ntraining examples, allowing the model to learn both to recover\nminority-but-correct answers as well as easy majority-correct answers.\nEmpirically, we find our method, AggLM, outperforms both strong rule-based and\nreward-model baselines, across multiple benchmarks. Furthermore, it generalizes\neffectively to solutions from differing models, including stronger ones than\ncontained in the training data, all while requiring substantially fewer tokens\nthan majority voting with larger numbers of solutions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06870.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "62f023a36a027498eaa2f9cc",
            "avatarUrl": "/avatars/8ac1c5c74d0957e3c6cc94b3a7795c37.svg",
            "fullname": "Jason Weston",
            "name": "spermwhale",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 18
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.08358",
            "authors": [
                {
                    "_id": "68c287b329b8ec9932cd0a3a",
                    "name": "Sergey Pletenev",
                    "hidden": false
                },
                {
                    "_id": "68c287b329b8ec9932cd0a3b",
                    "name": "Daniil Moskovskiy",
                    "hidden": false
                },
                {
                    "_id": "68c287b329b8ec9932cd0a3c",
                    "name": "Alexander Panchenko",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T07:48:24.000Z",
            "submittedOnDailyAt": "2025-09-11T08:55:17.858Z",
            "title": "<think> So let's replace this phrase with insult... </think> Lessons\n  learned from generation of toxic texts with LLMs",
            "submittedOnDailyBy": {
                "_id": "5dfa8e07da6d0311fd3d5430",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
                "isPro": false,
                "fullname": "Sergey Pletenev",
                "user": "memyprokotow",
                "type": "user"
            },
            "summary": "Modern Large Language Models (LLMs) are excellent at generating synthetic\ndata. However, their performance in sensitive domains such as text\ndetoxification has not received proper attention from the scientific community.\nThis paper explores the possibility of using LLM-generated synthetic toxic data\nas an alternative to human-generated data for training models for\ndetoxification. Using Llama 3 and Qwen activation-patched models, we generated\nsynthetic toxic counterparts for neutral texts from ParaDetox and SST-2\ndatasets. Our experiments show that models fine-tuned on synthetic data\nconsistently perform worse than those trained on human data, with a drop in\nperformance of up to 30% in joint metrics. The root cause is identified as a\ncritical lexical diversity gap: LLMs generate toxic content using a small,\nrepetitive vocabulary of insults that fails to capture the nuances and variety\nof human toxicity. These findings highlight the limitations of current LLMs in\nthis domain and emphasize the continued importance of diverse, human-annotated\ndata for building robust detoxification systems.",
            "upvotes": 5,
            "discussionId": "68c287b429b8ec9932cd0a3d",
            "ai_summary": "Models fine-tuned on synthetic toxic data generated by LLMs perform worse than those trained on human data due to a lexical diversity gap in the synthetic content.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "text detoxification",
                "Llama 3",
                "Qwen",
                "ParaDetox",
                "SST-2",
                "fine-tuned",
                "lexical diversity gap"
            ]
        },
        "publishedAt": "2025-09-10T03:48:24.000Z",
        "title": "<think> So let's replace this phrase with insult... </think> Lessons\n  learned from generation of toxic texts with LLMs",
        "summary": "Modern Large Language Models (LLMs) are excellent at generating synthetic\ndata. However, their performance in sensitive domains such as text\ndetoxification has not received proper attention from the scientific community.\nThis paper explores the possibility of using LLM-generated synthetic toxic data\nas an alternative to human-generated data for training models for\ndetoxification. Using Llama 3 and Qwen activation-patched models, we generated\nsynthetic toxic counterparts for neutral texts from ParaDetox and SST-2\ndatasets. Our experiments show that models fine-tuned on synthetic data\nconsistently perform worse than those trained on human data, with a drop in\nperformance of up to 30% in joint metrics. The root cause is identified as a\ncritical lexical diversity gap: LLMs generate toxic content using a small,\nrepetitive vocabulary of insults that fails to capture the nuances and variety\nof human toxicity. These findings highlight the limitations of current LLMs in\nthis domain and emphasize the continued importance of diverse, human-annotated\ndata for building robust detoxification systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08358.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5dfa8e07da6d0311fd3d5430",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1651090418656-5dfa8e07da6d0311fd3d5430.png",
            "fullname": "Sergey Pletenev",
            "name": "memyprokotow",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07054",
            "authors": [
                {
                    "_id": "68c2ee46fc1747b9124037d4",
                    "name": "Edgar Dobriban",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T17:42:59.000Z",
            "submittedOnDailyAt": "2025-09-11T18:58:14.942Z",
            "title": "Statistical Methods in Generative AI",
            "submittedOnDailyBy": {
                "_id": "6388f57bec1f539adc0b1a35",
                "avatarUrl": "/avatars/a39311ec70a79adedd7a9302fc32eb7f.svg",
                "isPro": false,
                "fullname": "Edgar Dobriban",
                "user": "ed13",
                "type": "user"
            },
            "summary": "Generative Artificial Intelligence is emerging as an important technology,\npromising to be transformative in many areas. At the same time, generative AI\ntechniques are based on sampling from probabilistic models, and by default,\nthey come with no guarantees about correctness, safety, fairness, or other\nproperties. Statistical methods offer a promising potential approach to improve\nthe reliability of generative AI techniques. In addition, statistical methods\nare also promising for improving the quality and efficiency of AI evaluation,\nas well as for designing interventions and experiments in AI.\n  In this paper, we review some of the existing work on these topics,\nexplaining both the general statistical techniques used, as well as their\napplications to generative AI. We also discuss limitations and potential future\ndirections.",
            "upvotes": 3,
            "discussionId": "68c2ee46fc1747b9124037d5",
            "ai_summary": "Statistical methods are reviewed for improving the reliability, quality, and efficiency of generative AI techniques, highlighting their applications and limitations.",
            "ai_keywords": [
                ""
            ]
        },
        "publishedAt": "2025-09-08T13:42:59.000Z",
        "title": "Statistical Methods in Generative AI",
        "summary": "Generative Artificial Intelligence is emerging as an important technology,\npromising to be transformative in many areas. At the same time, generative AI\ntechniques are based on sampling from probabilistic models, and by default,\nthey come with no guarantees about correctness, safety, fairness, or other\nproperties. Statistical methods offer a promising potential approach to improve\nthe reliability of generative AI techniques. In addition, statistical methods\nare also promising for improving the quality and efficiency of AI evaluation,\nas well as for designing interventions and experiments in AI.\n  In this paper, we review some of the existing work on these topics,\nexplaining both the general statistical techniques used, as well as their\napplications to generative AI. We also discuss limitations and potential future\ndirections.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07054.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6388f57bec1f539adc0b1a35",
            "avatarUrl": "/avatars/a39311ec70a79adedd7a9302fc32eb7f.svg",
            "fullname": "Edgar Dobriban",
            "name": "ed13",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.08088",
            "authors": [
                {
                    "_id": "68c2337029b8ec9932cd0929",
                    "name": "Linyao Chen",
                    "hidden": false
                },
                {
                    "_id": "68c2337029b8ec9932cd092a",
                    "name": "Zimian Peng",
                    "hidden": false
                },
                {
                    "_id": "68c2337029b8ec9932cd092b",
                    "name": "Yingxuan Yang",
                    "hidden": false
                },
                {
                    "_id": "68c2337029b8ec9932cd092c",
                    "name": "Yikun Wang",
                    "hidden": false
                },
                {
                    "_id": "68c2337029b8ec9932cd092d",
                    "name": "Wenzheng Tom Tang",
                    "hidden": false
                },
                {
                    "_id": "68c2337029b8ec9932cd092e",
                    "name": "Hiroki H. Kobayashi",
                    "hidden": false
                },
                {
                    "_id": "68c2337029b8ec9932cd092f",
                    "name": "Weinan Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T18:51:36.000Z",
            "submittedOnDailyAt": "2025-09-11T00:57:10.362Z",
            "title": "EnvX: Agentize Everything with Agentic AI",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The widespread availability of open-source repositories has led to a vast\ncollection of reusable software components, yet their utilization remains\nmanual, error-prone, and disconnected. Developers must navigate documentation,\nunderstand APIs, and write integration code, creating significant barriers to\nefficient software reuse. To address this, we present EnvX, a framework that\nleverages Agentic AI to agentize GitHub repositories, transforming them into\nintelligent, autonomous agents capable of natural language interaction and\ninter-agent collaboration. Unlike existing approaches that treat repositories\nas static code resources, EnvX reimagines them as active agents through a\nthree-phase process: (1) TODO-guided environment initialization, which sets up\nthe necessary dependencies, data, and validation datasets; (2) human-aligned\nagentic automation, allowing repository-specific agents to autonomously perform\nreal-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple\nagents to collaborate. By combining large language model capabilities with\nstructured tool integration, EnvX automates not just code generation, but the\nentire process of understanding, initializing, and operationalizing repository\nfunctionality. We evaluate EnvX on the GitTaskBench benchmark, using 18\nrepositories across domains such as image processing, speech recognition,\ndocument analysis, and video manipulation. Our results show that EnvX achieves\na 74.07% execution completion rate and 51.85% task pass rate, outperforming\nexisting frameworks. Case studies further demonstrate EnvX's ability to enable\nmulti-repository collaboration via the A2A protocol. This work marks a shift\nfrom treating repositories as passive code resources to intelligent,\ninteractive agents, fostering greater accessibility and collaboration within\nthe open-source ecosystem.",
            "upvotes": 2,
            "discussionId": "68c2337029b8ec9932cd0930",
            "ai_summary": "EnvX leverages Agentic AI to transform GitHub repositories into intelligent agents capable of natural language interaction and collaboration, automating the entire process of understanding, initializing, and operationalizing repository functionality.",
            "ai_keywords": [
                "Agentic AI",
                "agentization",
                "natural language interaction",
                "inter-agent collaboration",
                "TODO-guided environment initialization",
                "human-aligned agentic automation",
                "Agent-to-Agent (A2A) protocol",
                "large language model",
                "structured tool integration",
                "GitTaskBench benchmark"
            ]
        },
        "publishedAt": "2025-09-09T14:51:36.000Z",
        "title": "EnvX: Agentize Everything with Agentic AI",
        "summary": "The widespread availability of open-source repositories has led to a vast\ncollection of reusable software components, yet their utilization remains\nmanual, error-prone, and disconnected. Developers must navigate documentation,\nunderstand APIs, and write integration code, creating significant barriers to\nefficient software reuse. To address this, we present EnvX, a framework that\nleverages Agentic AI to agentize GitHub repositories, transforming them into\nintelligent, autonomous agents capable of natural language interaction and\ninter-agent collaboration. Unlike existing approaches that treat repositories\nas static code resources, EnvX reimagines them as active agents through a\nthree-phase process: (1) TODO-guided environment initialization, which sets up\nthe necessary dependencies, data, and validation datasets; (2) human-aligned\nagentic automation, allowing repository-specific agents to autonomously perform\nreal-world tasks; and (3) Agent-to-Agent (A2A) protocol, enabling multiple\nagents to collaborate. By combining large language model capabilities with\nstructured tool integration, EnvX automates not just code generation, but the\nentire process of understanding, initializing, and operationalizing repository\nfunctionality. We evaluate EnvX on the GitTaskBench benchmark, using 18\nrepositories across domains such as image processing, speech recognition,\ndocument analysis, and video manipulation. Our results show that EnvX achieves\na 74.07% execution completion rate and 51.85% task pass rate, outperforming\nexisting frameworks. Case studies further demonstrate EnvX's ability to enable\nmulti-repository collaboration via the A2A protocol. This work marks a shift\nfrom treating repositories as passive code resources to intelligent,\ninteractive agents, fostering greater accessibility and collaboration within\nthe open-source ecosystem.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08088.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 103
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.08494",
            "authors": [
                {
                    "_id": "68c232fc29b8ec9932cd0923",
                    "name": "Benjamin Sturgeon",
                    "hidden": false
                },
                {
                    "_id": "68c232fc29b8ec9932cd0924",
                    "name": "Daniel Samuelson",
                    "hidden": false
                },
                {
                    "_id": "68c232fc29b8ec9932cd0925",
                    "name": "Jacob Haimes",
                    "hidden": false
                },
                {
                    "_id": "68c232fc29b8ec9932cd0926",
                    "name": "Jacy Reese Anthis",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T11:10:10.000Z",
            "submittedOnDailyAt": "2025-09-11T00:55:10.506Z",
            "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets.",
            "upvotes": 0,
            "discussionId": "68c232fc29b8ec9932cd0927",
            "ai_summary": "A benchmark evaluates human agency in AI assistants using large language models, finding varying support across systems and dimensions.",
            "ai_keywords": [
                "large language models",
                "HumanAgencyBench",
                "human agency",
                "Ask Clarifying Questions",
                "Avoid Value Manipulation",
                "Correct Misinformation",
                "Defer Important Decisions",
                "Encourage Learning",
                "Maintain Social Boundaries",
                "RLHF"
            ]
        },
        "publishedAt": "2025-09-10T07:10:10.000Z",
        "title": "HumanAgencyBench: Scalable Evaluation of Human Agency Support in AI\n  Assistants",
        "summary": "As humans delegate more tasks and decisions to artificial intelligence (AI),\nwe risk losing control of our individual and collective futures. Relatively\nsimple algorithmic systems already steer human decision-making, such as social\nmedia feed algorithms that lead people to unintentionally and absent-mindedly\nscroll through engagement-optimized content. In this paper, we develop the idea\nof human agency by integrating philosophical and scientific theories of agency\nwith AI-assisted evaluation methods: using large language models (LLMs) to\nsimulate and validate user queries and to evaluate AI responses. We develop\nHumanAgencyBench (HAB), a scalable and adaptive benchmark with six dimensions\nof human agency based on typical AI use cases. HAB measures the tendency of an\nAI assistant or agent to Ask Clarifying Questions, Avoid Value Manipulation,\nCorrect Misinformation, Defer Important Decisions, Encourage Learning, and\nMaintain Social Boundaries. We find low-to-moderate agency support in\ncontemporary LLM-based assistants and substantial variation across system\ndevelopers and dimensions. For example, while Anthropic LLMs most support human\nagency overall, they are the least supportive LLMs in terms of Avoid Value\nManipulation. Agency support does not appear to consistently result from\nincreasing LLM capabilities or instruction-following behavior (e.g., RLHF), and\nwe encourage a shift towards more robust safety and alignment targets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08494.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 103
        },
        "isAuthorParticipating": false
    }
]