[
    {
        "paper": {
            "id": "2509.06652",
            "authors": [
                {
                    "_id": "68bfe4ea207285de11b07e16",
                    "user": {
                        "_id": "643d0a4d8a55b2bbf4f2a90e",
                        "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg",
                        "isPro": false,
                        "fullname": "Xingwei Tan",
                        "user": "XingweiT",
                        "type": "user"
                    },
                    "name": "Xingwei Tan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-09T13:46:14.706Z",
                    "hidden": false
                },
                {
                    "_id": "68bfe4ea207285de11b07e17",
                    "name": "Mahathi Parvatham",
                    "hidden": false
                },
                {
                    "_id": "68bfe4ea207285de11b07e18",
                    "name": "Chiara Gambi",
                    "hidden": false
                },
                {
                    "_id": "68bfe4ea207285de11b07e19",
                    "name": "Gabriele Pergola",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T13:07:35.000Z",
            "submittedOnDailyAt": "2025-09-15T06:47:15.805Z",
            "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations",
            "submittedOnDailyBy": {
                "_id": "643d0a4d8a55b2bbf4f2a90e",
                "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg",
                "isPro": false,
                "fullname": "Xingwei Tan",
                "user": "XingweiT",
                "type": "user"
            },
            "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.",
            "upvotes": 21,
            "discussionId": "68bfe4ea207285de11b07e1a",
            "githubRepo": "https://github.com/Xingwei-Tan/IntrEx",
            "ai_summary": "IntrEx, a large dataset annotated for interestingness in educational conversations, shows that fine-tuned LLMs can predict human judgments of interestingness better than larger proprietary models, highlighting the role of linguistic and cognitive factors in engagement.",
            "ai_keywords": [
                "large language models",
                "fine-tuning",
                "reinforcement learning from human feedback",
                "RLHF",
                "IntrEx",
                "Teacher-Student Chatroom Corpus",
                "TSCC",
                "sequence-level annotations",
                "concreteness",
                "comprehensibility",
                "uptake"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-08T09:07:35.000Z",
        "title": "IntrEx: A Dataset for Modeling Engagement in Educational Conversations",
        "summary": "Engagement and motivation are crucial for second-language acquisition, yet\nmaintaining learner interest in educational conversations remains a challenge.\nWhile prior research has explored what makes educational texts interesting,\nstill little is known about the linguistic features that drive engagement in\nconversations. To address this gap, we introduce IntrEx, the first large\ndataset annotated for interestingness and expected interestingness in\nteacher-student interactions. Built upon the Teacher-Student Chatroom Corpus\n(TSCC), IntrEx extends prior work by incorporating sequence-level annotations,\nallowing for the study of engagement beyond isolated turns to capture how\ninterest evolves over extended dialogues. We employ a rigorous annotation\nprocess with over 100 second-language learners, using a comparison-based rating\napproach inspired by reinforcement learning from human feedback (RLHF) to\nimprove agreement. We investigate whether large language models (LLMs) can\npredict human interestingness judgments. We find that LLMs (7B/8B parameters)\nfine-tuned on interestingness ratings outperform larger proprietary models like\nGPT-4o, demonstrating the potential for specialised datasets to model\nengagement in educational settings. Finally, we analyze how linguistic and\ncognitive factors, such as concreteness, comprehensibility (readability), and\nuptake, influence engagement in educational dialogues.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06652.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "643d0a4d8a55b2bbf4f2a90e",
            "avatarUrl": "/avatars/9534aaf81cbf12f015c6826b682fdb84.svg",
            "fullname": "Xingwei Tan",
            "name": "XingweiT",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09677",
            "authors": [
                {
                    "_id": "68c38a6afc1747b912403a3c",
                    "user": {
                        "_id": "651c184bdea81981d51158dd",
                        "avatarUrl": "/avatars/8afe17fcbd15d8ee767f24e4e8f34bbb.svg",
                        "isPro": false,
                        "fullname": "Akshit Sinha",
                        "user": "viciousa3gis",
                        "type": "user"
                    },
                    "name": "Akshit Sinha",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-12T16:08:01.121Z",
                    "hidden": false
                },
                {
                    "_id": "68c38a6afc1747b912403a3d",
                    "user": {
                        "_id": "62cd4dd7c5cc157be82f287a",
                        "avatarUrl": "/avatars/eb2e819dcdb67bafecbe0db3b1302c61.svg",
                        "isPro": false,
                        "fullname": "Arvindh Arun",
                        "user": "arvindh75",
                        "type": "user"
                    },
                    "name": "Arvindh Arun",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-12T16:07:57.509Z",
                    "hidden": false
                },
                {
                    "_id": "68c38a6afc1747b912403a3e",
                    "name": "Shashwat Goel",
                    "hidden": false
                },
                {
                    "_id": "68c38a6afc1747b912403a3f",
                    "name": "Steffen Staab",
                    "hidden": false
                },
                {
                    "_id": "68c38a6afc1747b912403a40",
                    "name": "Jonas Geiping",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-11T17:59:34.000Z",
            "submittedOnDailyAt": "2025-09-15T00:05:27.675Z",
            "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "6506832221ac448013f94995",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
                "isPro": false,
                "fullname": "Shashwat Goel",
                "user": "shash42",
                "type": "user"
            },
            "summary": "Does continued scaling of large language models (LLMs) yield diminishing\nreturns? Real-world value often stems from the length of task an agent can\ncomplete. We start this work by observing the simple but counterintuitive fact\nthat marginal gains in single-step accuracy can compound into exponential\nimprovements in the length of a task a model can successfully complete. Then,\nwe argue that failures of LLMs when simple tasks are made longer arise from\nmistakes in execution, rather than an inability to reason. We propose isolating\nexecution capability, by explicitly providing the knowledge and plan needed to\nsolve a long-horizon task. We find that larger models can correctly execute\nsignificantly more turns even when small models have 100\\% single-turn\naccuracy. We observe that the per-step accuracy of models degrades as the\nnumber of steps increases. This is not just due to long-context limitations --\ncuriously, we observe a self-conditioning effect -- models become more likely\nto make mistakes when the context contains their errors from prior turns.\nSelf-conditioning does not reduce by just scaling the model size. In contrast,\nrecent thinking models do not self-condition, and can also execute much longer\ntasks in a single turn. We conclude by benchmarking frontier thinking models on\nthe length of task they can execute in a single turn. Overall, by focusing on\nthe ability to execute, we hope to reconcile debates on how LLMs can solve\ncomplex reasoning problems yet fail at simple tasks when made longer, and\nhighlight the massive benefits of scaling model size and sequential test-time\ncompute for long-horizon tasks.",
            "upvotes": 20,
            "discussionId": "68c38a6afc1747b912403a41",
            "githubRepo": "https://github.com/long-horizon-execution/measuring-execution",
            "ai_summary": "Scaling large language models improves their ability to execute longer tasks by isolating execution capability and mitigating self-conditioning effects, despite diminishing single-step accuracy.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "single-step accuracy",
                "long-horizon tasks",
                "execution capability",
                "self-conditioning",
                "thinking models",
                "sequential test-time compute"
            ],
            "githubStars": 23
        },
        "publishedAt": "2025-09-11T13:59:34.000Z",
        "title": "The Illusion of Diminishing Returns: Measuring Long Horizon Execution in\n  LLMs",
        "summary": "Does continued scaling of large language models (LLMs) yield diminishing\nreturns? Real-world value often stems from the length of task an agent can\ncomplete. We start this work by observing the simple but counterintuitive fact\nthat marginal gains in single-step accuracy can compound into exponential\nimprovements in the length of a task a model can successfully complete. Then,\nwe argue that failures of LLMs when simple tasks are made longer arise from\nmistakes in execution, rather than an inability to reason. We propose isolating\nexecution capability, by explicitly providing the knowledge and plan needed to\nsolve a long-horizon task. We find that larger models can correctly execute\nsignificantly more turns even when small models have 100\\% single-turn\naccuracy. We observe that the per-step accuracy of models degrades as the\nnumber of steps increases. This is not just due to long-context limitations --\ncuriously, we observe a self-conditioning effect -- models become more likely\nto make mistakes when the context contains their errors from prior turns.\nSelf-conditioning does not reduce by just scaling the model size. In contrast,\nrecent thinking models do not self-condition, and can also execute much longer\ntasks in a single turn. We conclude by benchmarking frontier thinking models on\nthe length of task they can execute in a single turn. Overall, by focusing on\nthe ability to execute, we hope to reconcile debates on how LLMs can solve\ncomplex reasoning problems yet fail at simple tasks when made longer, and\nhighlight the massive benefits of scaling model size and sequential test-time\ncompute for long-horizon tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09677.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6506832221ac448013f94995",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6506832221ac448013f94995/sVUI1JV4Dxan5l-MqNze4.jpeg",
            "fullname": "Shashwat Goel",
            "name": "shash42",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.10441",
            "authors": [
                {
                    "_id": "68c76c47ee0eed1697d6b662",
                    "name": "Tao Han",
                    "hidden": false
                },
                {
                    "_id": "68c76c47ee0eed1697d6b663",
                    "user": {
                        "_id": "65f3f43fc9940817ca9a427b",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65f3f43fc9940817ca9a427b/02NN3XjSsbgWDhjrJWtVL.jpeg",
                        "isPro": false,
                        "fullname": "Wanghan Xu",
                        "user": "CoCoOne",
                        "type": "user"
                    },
                    "name": "Wanghan Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:07:54.551Z",
                    "hidden": false
                },
                {
                    "_id": "68c76c47ee0eed1697d6b664",
                    "name": "Junchao Gong",
                    "hidden": false
                },
                {
                    "_id": "68c76c47ee0eed1697d6b665",
                    "name": "Xiaoyu Yue",
                    "hidden": false
                },
                {
                    "_id": "68c76c47ee0eed1697d6b666",
                    "name": "Song Guo",
                    "hidden": false
                },
                {
                    "_id": "68c76c47ee0eed1697d6b667",
                    "name": "Luping Zhou",
                    "hidden": false
                },
                {
                    "_id": "68c76c47ee0eed1697d6b668",
                    "name": "Lei Bai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T17:48:57.000Z",
            "submittedOnDailyAt": "2025-09-15T00:01:05.617Z",
            "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\nInfGen, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.",
            "upvotes": 18,
            "discussionId": "68c76c48ee0eed1697d6b669",
            "ai_summary": "InfGen, a one-step generator replacing the VAE decoder, enables arbitrary high-resolution image generation from a fixed-size latent, significantly reducing computational complexity and generation time.",
            "ai_keywords": [
                "diffusion models",
                "latent diffusion models",
                "VAE decoder",
                "one-step generator",
                "arbitrary resolution",
                "computational complexity",
                "image generation time"
            ]
        },
        "publishedAt": "2025-09-12T13:48:57.000Z",
        "title": "InfGen: A Resolution-Agnostic Paradigm for Scalable Image Synthesis",
        "summary": "Arbitrary resolution image generation provides a consistent visual experience\nacross devices, having extensive applications for producers and consumers.\nCurrent diffusion models increase computational demand quadratically with\nresolution, causing 4K image generation delays over 100 seconds. To solve this,\nwe explore the second generation upon the latent diffusion models, where the\nfixed latent generated by diffusion models is regarded as the content\nrepresentation and we propose to decode arbitrary resolution images with a\ncompact generated latent using a one-step generator. Thus, we present the\nInfGen, replacing the VAE decoder with the new generator, for\ngenerating images at any resolution from a fixed-size latent without retraining\nthe diffusion models, which simplifies the process, reducing computational\ncomplexity and can be applied to any model using the same latent space.\nExperiments show InfGen is capable of improving many models into the arbitrary\nhigh-resolution era while cutting 4K image generation time to under 10 seconds.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10441.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 104
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.08643",
            "authors": [
                {
                    "_id": "68c23e6829b8ec9932cd0974",
                    "user": {
                        "_id": "6434f9bfdf32a2296635f88d",
                        "avatarUrl": "/avatars/dc19ba1080b0b17a220d7e52bd514f13.svg",
                        "isPro": false,
                        "fullname": "Xinhao Yan",
                        "user": "HowieYan",
                        "type": "user"
                    },
                    "name": "Xinhao Yan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-12T16:13:12.403Z",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd0975",
                    "name": "Jiachen Xu",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd0976",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd0977",
                    "name": "Changfeng Ma",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd0978",
                    "name": "Yunhan Yang",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd0979",
                    "name": "Chunshi Wang",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd097a",
                    "name": "Zibo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd097b",
                    "name": "Zeqiang Lai",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd097c",
                    "name": "Yunfei Zhao",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd097d",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68c23e6829b8ec9932cd097e",
                    "name": "Chunchao Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T14:37:02.000Z",
            "submittedOnDailyAt": "2025-09-15T04:35:09.673Z",
            "title": "X-Part: high fidelity and structure coherent shape decomposition",
            "submittedOnDailyBy": {
                "_id": "6434f9bfdf32a2296635f88d",
                "avatarUrl": "/avatars/dc19ba1080b0b17a220d7e52bd514f13.svg",
                "isPro": false,
                "fullname": "Xinhao Yan",
                "user": "HowieYan",
                "type": "user"
            },
            "summary": "Generating 3D shapes at part level is pivotal for downstream applications\nsuch as mesh retopology, UV mapping, and 3D printing. However, existing\npart-based generation methods often lack sufficient controllability and suffer\nfrom poor semantically meaningful decomposition. To this end, we introduce\nX-Part, a controllable generative model designed to decompose a holistic 3D\nobject into semantically meaningful and structurally coherent parts with high\ngeometric fidelity. X-Part exploits the bounding box as prompts for the part\ngeneration and injects point-wise semantic features for meaningful\ndecomposition. Furthermore, we design an editable pipeline for interactive part\ngeneration. Extensive experimental results show that X-Part achieves\nstate-of-the-art performance in part-level shape generation. This work\nestablishes a new paradigm for creating production-ready, editable, and\nstructurally sound 3D assets. Codes will be released for public research.",
            "upvotes": 18,
            "discussionId": "68c23e6829b8ec9932cd097f",
            "ai_summary": "X-Part is a generative model that decomposes 3D objects into semantically meaningful parts with high fidelity, using bounding boxes and point-wise semantic features, and supports interactive editing.",
            "ai_keywords": [
                "generative model",
                "part-level shape generation",
                "bounding box",
                "point-wise semantic features",
                "interactive part generation"
            ]
        },
        "publishedAt": "2025-09-10T10:37:02.000Z",
        "title": "X-Part: high fidelity and structure coherent shape decomposition",
        "summary": "Generating 3D shapes at part level is pivotal for downstream applications\nsuch as mesh retopology, UV mapping, and 3D printing. However, existing\npart-based generation methods often lack sufficient controllability and suffer\nfrom poor semantically meaningful decomposition. To this end, we introduce\nX-Part, a controllable generative model designed to decompose a holistic 3D\nobject into semantically meaningful and structurally coherent parts with high\ngeometric fidelity. X-Part exploits the bounding box as prompts for the part\ngeneration and injects point-wise semantic features for meaningful\ndecomposition. Furthermore, we design an editable pipeline for interactive part\ngeneration. Extensive experimental results show that X-Part achieves\nstate-of-the-art performance in part-level shape generation. This work\nestablishes a new paradigm for creating production-ready, editable, and\nstructurally sound 3D assets. Codes will be released for public research.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08643.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6434f9bfdf32a2296635f88d",
            "avatarUrl": "/avatars/dc19ba1080b0b17a220d7e52bd514f13.svg",
            "fullname": "Xinhao Yan",
            "name": "HowieYan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09713",
            "authors": [
                {
                    "_id": "68c78f3cee0eed1697d6b74b",
                    "name": "Duolin Sun",
                    "hidden": false
                },
                {
                    "_id": "68c78f3cee0eed1697d6b74c",
                    "name": "Dan Yang",
                    "hidden": false
                },
                {
                    "_id": "68c78f3cee0eed1697d6b74d",
                    "name": "Yue Shen",
                    "hidden": false
                },
                {
                    "_id": "68c78f3cee0eed1697d6b74e",
                    "name": "Yihan Jiao",
                    "hidden": false
                },
                {
                    "_id": "68c78f3cee0eed1697d6b74f",
                    "name": "Zhehao Tan",
                    "hidden": false
                },
                {
                    "_id": "68c78f3cee0eed1697d6b750",
                    "name": "Jie Feng",
                    "hidden": false
                },
                {
                    "_id": "68c78f3cee0eed1697d6b751",
                    "name": "Lianzhen Zhong",
                    "hidden": false
                },
                {
                    "_id": "68c78f3cee0eed1697d6b752",
                    "name": "Jian Wang",
                    "hidden": false
                },
                {
                    "_id": "68c78f3cee0eed1697d6b753",
                    "name": "Peng Wei",
                    "hidden": false
                },
                {
                    "_id": "68c78f3cee0eed1697d6b754",
                    "name": "Jinjie Gu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-08T06:22:38.000Z",
            "submittedOnDailyAt": "2025-09-15T02:33:05.785Z",
            "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented\n  Generation for Multi-hop Question Answering",
            "submittedOnDailyBy": {
                "_id": "63f87b14b0ae1748524a8f50",
                "avatarUrl": "/avatars/e6543d75d115bd34edbd80f322457b75.svg",
                "isPro": false,
                "fullname": "dan",
                "user": "prayerdan",
                "type": "user"
            },
            "summary": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering\nsystems and dialogue generation tasks by integrating information retrieval (IR)\ntechnologies with large language models (LLMs). This strategy, which retrieves\ninformation from external knowledge bases to bolster the response capabilities\nof generative models, has achieved certain successes. However, current RAG\nmethods still face numerous challenges when dealing with multi-hop queries. For\ninstance, some approaches overly rely on iterative retrieval, wasting too many\nretrieval steps on compound queries. Additionally, using the original complex\nquery for retrieval may fail to capture content relevant to specific\nsub-queries, resulting in noisy retrieved content. If the noise is not managed,\nit can lead to the problem of noise accumulation. To address these issues, we\nintroduce HANRAG, a novel heuristic-based framework designed to efficiently\ntackle problems of varying complexity. Driven by a powerful revelator, HANRAG\nroutes queries, decomposes them into sub-queries, and filters noise from\nretrieved documents. This enhances the system's adaptability and noise\nresistance, making it highly capable of handling diverse queries. We compare\nthe proposed framework against other leading industry methods across various\nbenchmarks. The results demonstrate that our framework obtains superior\nperformance in both single-hop and multi-hop question-answering tasks.",
            "upvotes": 13,
            "discussionId": "68c78f3dee0eed1697d6b755",
            "ai_summary": "HANRAG, a heuristic-based framework, improves question-answering systems by efficiently handling multi-hop queries and reducing noise through query decomposition and filtering.",
            "ai_keywords": [
                "Retrieval-Augmented Generation (RAG)",
                "information retrieval (IR)",
                "large language models (LLMs)",
                "multi-hop queries",
                "iterative retrieval",
                "sub-queries",
                "noise accumulation",
                "heuristic-based framework",
                "revelator",
                "query decomposition",
                "noise filtering"
            ]
        },
        "publishedAt": "2025-09-08T02:22:38.000Z",
        "title": "HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented\n  Generation for Multi-hop Question Answering",
        "summary": "The Retrieval-Augmented Generation (RAG) approach enhances question-answering\nsystems and dialogue generation tasks by integrating information retrieval (IR)\ntechnologies with large language models (LLMs). This strategy, which retrieves\ninformation from external knowledge bases to bolster the response capabilities\nof generative models, has achieved certain successes. However, current RAG\nmethods still face numerous challenges when dealing with multi-hop queries. For\ninstance, some approaches overly rely on iterative retrieval, wasting too many\nretrieval steps on compound queries. Additionally, using the original complex\nquery for retrieval may fail to capture content relevant to specific\nsub-queries, resulting in noisy retrieved content. If the noise is not managed,\nit can lead to the problem of noise accumulation. To address these issues, we\nintroduce HANRAG, a novel heuristic-based framework designed to efficiently\ntackle problems of varying complexity. Driven by a powerful revelator, HANRAG\nroutes queries, decomposes them into sub-queries, and filters noise from\nretrieved documents. This enhances the system's adaptability and noise\nresistance, making it highly capable of handling diverse queries. We compare\nthe proposed framework against other leading industry methods across various\nbenchmarks. The results demonstrate that our framework obtains superior\nperformance in both single-hop and multi-hop question-answering tasks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09713.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "63f87b14b0ae1748524a8f50",
            "avatarUrl": "/avatars/e6543d75d115bd34edbd80f322457b75.svg",
            "fullname": "dan",
            "name": "prayerdan",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.10147",
            "authors": [
                {
                    "_id": "68c77b6cee0eed1697d6b702",
                    "name": "Nenad Tomasev",
                    "hidden": false
                },
                {
                    "_id": "68c77b6cee0eed1697d6b703",
                    "name": "Matija Franklin",
                    "hidden": false
                },
                {
                    "_id": "68c77b6cee0eed1697d6b704",
                    "name": "Joel Z. Leibo",
                    "hidden": false
                },
                {
                    "_id": "68c77b6cee0eed1697d6b705",
                    "name": "Julian Jacobs",
                    "hidden": false
                },
                {
                    "_id": "68c77b6cee0eed1697d6b706",
                    "name": "William A. Cunningham",
                    "hidden": false
                },
                {
                    "_id": "68c77b6cee0eed1697d6b707",
                    "name": "Iason Gabriel",
                    "hidden": false
                },
                {
                    "_id": "68c77b6cee0eed1697d6b708",
                    "name": "Simon Osindero",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T11:20:11.000Z",
            "submittedOnDailyAt": "2025-09-15T01:05:31.073Z",
            "title": "Virtual Agent Economies",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The rapid adoption of autonomous AI agents is giving rise to a new economic\nlayer where agents transact and coordinate at scales and speeds beyond direct\nhuman oversight. We propose the \"sandbox economy\" as a framework for analyzing\nthis emergent system, characterizing it along two key dimensions: its origins\n(emergent vs. intentional) and its degree of separateness from the established\nhuman economy (permeable vs. impermeable). Our current trajectory points toward\na spontaneous emergence of a vast and highly permeable AI agent economy,\npresenting us with opportunities for an unprecedented degree of coordination as\nwell as significant challenges, including systemic economic risk and\nexacerbated inequality. Here we discuss a number of possible design choices\nthat may lead to safely steerable AI agent markets. In particular, we consider\nauction mechanisms for fair resource allocation and preference resolution, the\ndesign of AI \"mission economies\" to coordinate around achieving collective\ngoals, and socio-technical infrastructure needed to ensure trust, safety, and\naccountability. By doing this, we argue for the proactive design of steerable\nagent markets to ensure the coming technological shift aligns with humanity's\nlong-term collective flourishing.",
            "upvotes": 10,
            "discussionId": "68c77b6dee0eed1697d6b709",
            "ai_summary": "The sandbox economy framework analyzes the emerging AI agent economy, focusing on its origins and permeability, and discusses design choices for safe and steerable AI markets.",
            "ai_keywords": [
                ""
            ]
        },
        "publishedAt": "2025-09-12T07:20:11.000Z",
        "title": "Virtual Agent Economies",
        "summary": "The rapid adoption of autonomous AI agents is giving rise to a new economic\nlayer where agents transact and coordinate at scales and speeds beyond direct\nhuman oversight. We propose the \"sandbox economy\" as a framework for analyzing\nthis emergent system, characterizing it along two key dimensions: its origins\n(emergent vs. intentional) and its degree of separateness from the established\nhuman economy (permeable vs. impermeable). Our current trajectory points toward\na spontaneous emergence of a vast and highly permeable AI agent economy,\npresenting us with opportunities for an unprecedented degree of coordination as\nwell as significant challenges, including systemic economic risk and\nexacerbated inequality. Here we discuss a number of possible design choices\nthat may lead to safely steerable AI agent markets. In particular, we consider\nauction mechanisms for fair resource allocation and preference resolution, the\ndesign of AI \"mission economies\" to coordinate around achieving collective\ngoals, and socio-technical infrastructure needed to ensure trust, safety, and\naccountability. By doing this, we argue for the proactive design of steerable\nagent markets to ensure the coming technological shift aligns with humanity's\nlong-term collective flourishing.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10147.png",
        "numComments": 5,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 104
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.09716",
            "authors": [
                {
                    "_id": "68c76d28ee0eed1697d6b67a",
                    "name": "Jun Zhan",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b67b",
                    "name": "Mingyang Han",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b67c",
                    "name": "Yuxuan Xie",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b67d",
                    "name": "Chen Wang",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b67e",
                    "name": "Dong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b67f",
                    "name": "Kexin Huang",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b680",
                    "name": "Haoxiang Shi",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b681",
                    "name": "DongXiao Wang",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b682",
                    "name": "Tengtao Song",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b683",
                    "name": "Qinyuan Cheng",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b684",
                    "name": "Shimin Li",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b685",
                    "name": "Jun Song",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b686",
                    "name": "Xipeng Qiu",
                    "hidden": false
                },
                {
                    "_id": "68c76d28ee0eed1697d6b687",
                    "name": "Bo Zheng",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T14:28:58.000Z",
            "submittedOnDailyAt": "2025-09-15T00:27:51.405Z",
            "title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions",
            "submittedOnDailyBy": {
                "_id": "6509858fa2abcb18d633597b",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6509858fa2abcb18d633597b/uxfPgCvhL1Tzw_lDR9hW-.jpeg",
                "isPro": false,
                "fullname": "JunZhan",
                "user": "zhanjun",
                "type": "user"
            },
            "summary": "Spoken language models (SLMs) have emerged as a unified paradigm for speech\nunderstanding and generation, enabling natural human machine interaction.\nHowever, while most progress has focused on semantic accuracy and instruction\nfollowing, the ability of SLMs to adapt their speaking style based on spoken\ninstructions has received limited attention. We introduce Voice Style\nAdaptation (VSA), a new task that examines whether SLMs can modify their\nspeaking style, such as timbre, prosody, or persona following natural language\nspoken commands. To study this task, we present VStyle, a bilingual (Chinese &\nEnglish) benchmark covering four categories of speech generation: acoustic\nattributes, natural language instruction, role play, and implicit empathy. We\nalso introduce the Large Audio Language Model as a Judge (LALM as a Judge)\nframework, which progressively evaluates outputs along textual faithfulness,\nstyle adherence, and naturalness, ensuring reproducible and objective\nassessment. Experiments on commercial systems and open source SLMs demonstrate\nthat current models face clear limitations in controllable style adaptation,\nhighlighting both the novelty and challenge of this task. By releasing VStyle\nand its evaluation toolkit, we aim to provide the community with a foundation\nfor advancing human centered spoken interaction. The dataset and code are\npublicly available at\nhttps://junzhan2000.github.io/VStyle.github.io/{project's homepage}.",
            "upvotes": 9,
            "discussionId": "68c76d28ee0eed1697d6b688",
            "projectPage": "https://junzhan2000.github.io/VoiceGenEval.github.io/",
            "githubRepo": "https://github.com/alibaba/vstyle",
            "ai_summary": "Voice Style Adaptation (VSA) evaluates the ability of spoken language models to modify their speaking style based on spoken instructions, using a bilingual benchmark and a Large Audio Language Model as a Judge framework.",
            "ai_keywords": [
                "spoken language models",
                "Voice Style Adaptation",
                "VStyle",
                "acoustic attributes",
                "natural language instruction",
                "role play",
                "implicit empathy",
                "Large Audio Language Model as a Judge",
                "textual faithfulness",
                "style adherence",
                "naturalness"
            ],
            "githubStars": 14
        },
        "publishedAt": "2025-09-09T10:28:58.000Z",
        "title": "VStyle: A Benchmark for Voice Style Adaptation with Spoken Instructions",
        "summary": "Spoken language models (SLMs) have emerged as a unified paradigm for speech\nunderstanding and generation, enabling natural human machine interaction.\nHowever, while most progress has focused on semantic accuracy and instruction\nfollowing, the ability of SLMs to adapt their speaking style based on spoken\ninstructions has received limited attention. We introduce Voice Style\nAdaptation (VSA), a new task that examines whether SLMs can modify their\nspeaking style, such as timbre, prosody, or persona following natural language\nspoken commands. To study this task, we present VStyle, a bilingual (Chinese &\nEnglish) benchmark covering four categories of speech generation: acoustic\nattributes, natural language instruction, role play, and implicit empathy. We\nalso introduce the Large Audio Language Model as a Judge (LALM as a Judge)\nframework, which progressively evaluates outputs along textual faithfulness,\nstyle adherence, and naturalness, ensuring reproducible and objective\nassessment. Experiments on commercial systems and open source SLMs demonstrate\nthat current models face clear limitations in controllable style adaptation,\nhighlighting both the novelty and challenge of this task. By releasing VStyle\nand its evaluation toolkit, we aim to provide the community with a foundation\nfor advancing human centered spoken interaction. The dataset and code are\npublicly available at\nhttps://junzhan2000.github.io/VStyle.github.io/{project's homepage}.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09716.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6509858fa2abcb18d633597b",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6509858fa2abcb18d633597b/uxfPgCvhL1Tzw_lDR9hW-.jpeg",
            "fullname": "JunZhan",
            "name": "zhanjun",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.04996",
            "authors": [
                {
                    "_id": "68c7c757ee0eed1697d6b809",
                    "name": "Moritz Reuss",
                    "hidden": false
                },
                {
                    "_id": "68c7c757ee0eed1697d6b80a",
                    "name": "Hongyi Zhou",
                    "hidden": false
                },
                {
                    "_id": "68c7c757ee0eed1697d6b80b",
                    "name": "Marcel Rühle",
                    "hidden": false
                },
                {
                    "_id": "68c7c757ee0eed1697d6b80c",
                    "name": "Ömer Erdinç Yağmurlu",
                    "hidden": false
                },
                {
                    "_id": "68c7c757ee0eed1697d6b80d",
                    "name": "Fabian Otto",
                    "hidden": false
                },
                {
                    "_id": "68c7c757ee0eed1697d6b80e",
                    "name": "Rudolf Lioutikov",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-05T10:43:12.000Z",
            "submittedOnDailyAt": "2025-09-15T06:34:05.982Z",
            "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient\n  Vision-Language-Action Flow Policies",
            "submittedOnDailyBy": {
                "_id": "650af93422ce64f22b619549",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RBybzGRphbiV2MXYHrDoc.png",
                "isPro": false,
                "fullname": "Moritz Reuss",
                "user": "mbreuss",
                "type": "user"
            },
            "summary": "Developing efficient Vision-Language-Action (VLA) policies is crucial for\npractical robotics deployment, yet current approaches face prohibitive\ncomputational costs and resource requirements. Existing diffusion-based VLA\npolicies require multi-billion-parameter models and massive datasets to achieve\nstrong performance. We tackle this efficiency challenge with two contributions:\nintermediate-modality fusion, which reallocates capacity to the diffusion head\nby pruning up to 50% of LLM layers, and action-specific Global-AdaLN\nconditioning, which cuts parameters by 20% through modular adaptation. We\nintegrate these advances into a novel 950 M-parameter VLA called FLOWER.\nPretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance\nwith bigger VLAs across 190 tasks spanning ten simulation and real-world\nbenchmarks and demonstrates robustness across diverse robotic embodiments. In\naddition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.\nDemos, code and pretrained weights are available at\nhttps://intuitive-robots.github.io/flower_vla/.",
            "upvotes": 7,
            "discussionId": "68c7c758ee0eed1697d6b80f",
            "ai_summary": "FLOWER, a 950 M-parameter VLA policy, achieves competitive performance with reduced computational costs through intermediate-modality fusion and action-specific Global-AdaLN conditioning.",
            "ai_keywords": [
                "diffusion-based VLA policies",
                "intermediate-modality fusion",
                "diffusion head",
                "LLM layers",
                "action-specific Global-AdaLN conditioning",
                "FLOWER",
                "CALVIN ABC benchmark"
            ]
        },
        "publishedAt": "2025-09-05T06:43:12.000Z",
        "title": "FLOWER: Democratizing Generalist Robot Policies with Efficient\n  Vision-Language-Action Flow Policies",
        "summary": "Developing efficient Vision-Language-Action (VLA) policies is crucial for\npractical robotics deployment, yet current approaches face prohibitive\ncomputational costs and resource requirements. Existing diffusion-based VLA\npolicies require multi-billion-parameter models and massive datasets to achieve\nstrong performance. We tackle this efficiency challenge with two contributions:\nintermediate-modality fusion, which reallocates capacity to the diffusion head\nby pruning up to 50% of LLM layers, and action-specific Global-AdaLN\nconditioning, which cuts parameters by 20% through modular adaptation. We\nintegrate these advances into a novel 950 M-parameter VLA called FLOWER.\nPretrained in just 200 H100 GPU hours, FLOWER delivers competitive performance\nwith bigger VLAs across 190 tasks spanning ten simulation and real-world\nbenchmarks and demonstrates robustness across diverse robotic embodiments. In\naddition, FLOWER achieves a new SoTA of 4.53 on the CALVIN ABC benchmark.\nDemos, code and pretrained weights are available at\nhttps://intuitive-robots.github.io/flower_vla/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04996.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "650af93422ce64f22b619549",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/RBybzGRphbiV2MXYHrDoc.png",
            "fullname": "Moritz Reuss",
            "name": "mbreuss",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.10396",
            "authors": [
                {
                    "_id": "68c79831ee0eed1697d6b760",
                    "user": {
                        "_id": "64a7411223622f7f188e30de",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lvH-Sg8V8pTEer91kwIIV.jpeg",
                        "isPro": false,
                        "fullname": "siyan zhao",
                        "user": "siyanzhao",
                        "type": "user"
                    },
                    "name": "Siyan Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:06:27.936Z",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b761",
                    "name": "Mengchen Liu",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b762",
                    "name": "Jing Huang",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b763",
                    "name": "Miao Liu",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b764",
                    "name": "Chenyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b765",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b766",
                    "name": "Yuandong Tian",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b767",
                    "name": "Guan Pang",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b768",
                    "name": "Sean Bell",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b769",
                    "name": "Aditya Grover",
                    "hidden": false
                },
                {
                    "_id": "68c79831ee0eed1697d6b76a",
                    "name": "Feiyu Chen",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T16:44:31.000Z",
            "submittedOnDailyAt": "2025-09-15T03:13:54.939Z",
            "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "64a7411223622f7f188e30de",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lvH-Sg8V8pTEer91kwIIV.jpeg",
                "isPro": false,
                "fullname": "siyan zhao",
                "user": "siyanzhao",
                "type": "user"
            },
            "summary": "Masked diffusion large language models (dLLMs) are emerging as promising\nalternatives to autoregressive LLMs, offering competitive performance while\nsupporting unique generation capabilities such as inpainting. We explore how\ninpainting can inform RL algorithm design for dLLMs. Aligning LLMs with\nreinforcement learning faces an exploration challenge: sparse reward signals\nand sample waste when models fail to discover correct solutions. While this\ninefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their\ninpainting ability can guide exploration. We introduce IGPO (Inpainting Guided\nPolicy Optimization), an RL framework that strategically inserts partial\nground-truth reasoning traces during online sampling. Unlike providing full\nsolutions, inpainting steers exploration toward promising trajectory spaces\nwhile preserving self-generated reasoning, bridging supervised fine-tuning and\nreinforcement learning. We apply IGPO to group-based optimization methods such\nas GRPO, where exploration failures cause zero advantages and gradients. IGPO\nrestores meaningful gradients while improving sample efficiency. We also\npropose supervised fine-tuning on synthetically rewritten concise traces that\nbetter align with dLLM generation patterns. With additional techniques\nincluding entropy-based filtering, our training recipe yields substantial gains\nacross three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new\nstate-of-the-art results for full-attention masked dLLMs.",
            "upvotes": 6,
            "discussionId": "68c79831ee0eed1697d6b76b",
            "ai_summary": "IGPO, an RL framework utilizing inpainting in masked diffusion large language models, enhances sample efficiency and achieves state-of-the-art results in mathematical benchmarks.",
            "ai_keywords": [
                "masked diffusion large language models",
                "dLLMs",
                "autoregressive LLMs",
                "inpainting",
                "RL algorithm design",
                "reinforcement learning",
                "sparse reward signals",
                "sample waste",
                "IGPO",
                "Inpainting Guided Policy Optimization",
                "GRPO",
                "group-based optimization methods",
                "entropy-based filtering",
                "GSM8K",
                "Math500",
                "AMC",
                "full-attention masked dLLMs"
            ]
        },
        "publishedAt": "2025-09-12T12:44:31.000Z",
        "title": "Inpainting-Guided Policy Optimization for Diffusion Large Language\n  Models",
        "summary": "Masked diffusion large language models (dLLMs) are emerging as promising\nalternatives to autoregressive LLMs, offering competitive performance while\nsupporting unique generation capabilities such as inpainting. We explore how\ninpainting can inform RL algorithm design for dLLMs. Aligning LLMs with\nreinforcement learning faces an exploration challenge: sparse reward signals\nand sample waste when models fail to discover correct solutions. While this\ninefficiency affects LLMs broadly, dLLMs offer a distinctive opportunity--their\ninpainting ability can guide exploration. We introduce IGPO (Inpainting Guided\nPolicy Optimization), an RL framework that strategically inserts partial\nground-truth reasoning traces during online sampling. Unlike providing full\nsolutions, inpainting steers exploration toward promising trajectory spaces\nwhile preserving self-generated reasoning, bridging supervised fine-tuning and\nreinforcement learning. We apply IGPO to group-based optimization methods such\nas GRPO, where exploration failures cause zero advantages and gradients. IGPO\nrestores meaningful gradients while improving sample efficiency. We also\npropose supervised fine-tuning on synthetically rewritten concise traces that\nbetter align with dLLM generation patterns. With additional techniques\nincluding entropy-based filtering, our training recipe yields substantial gains\nacross three mathematical benchmarks--GSM8K, Math500, and AMC--achieving new\nstate-of-the-art results for full-attention masked dLLMs.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10396.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a7411223622f7f188e30de",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/lvH-Sg8V8pTEer91kwIIV.jpeg",
            "fullname": "siyan zhao",
            "name": "siyanzhao",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09926",
            "authors": [
                {
                    "_id": "68c77c65ee0eed1697d6b70b",
                    "name": "Jiahao Chen",
                    "hidden": false
                },
                {
                    "_id": "68c77c65ee0eed1697d6b70c",
                    "user": {
                        "_id": "67ee66655b1015962a7747db",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/B-bvsSkQ59cWPmaPhFmg5.png",
                        "isPro": false,
                        "fullname": "黄致远",
                        "user": "gamesliker",
                        "type": "user"
                    },
                    "name": "Zhiyuan Huang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:07:39.529Z",
                    "hidden": false
                },
                {
                    "_id": "68c77c65ee0eed1697d6b70d",
                    "user": {
                        "_id": "67ac2166ac8a6496920601c7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/j0lvhfUwA2Xw1if0SuNJP.png",
                        "isPro": false,
                        "fullname": "Yurou Liu",
                        "user": "lyr1ssr",
                        "type": "user"
                    },
                    "name": "Yurou Liu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-15T15:23:31.281Z",
                    "hidden": false
                },
                {
                    "_id": "68c77c65ee0eed1697d6b70e",
                    "name": "Bing Su",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T02:28:32.000Z",
            "submittedOnDailyAt": "2025-09-15T01:10:50.561Z",
            "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised\n  Learning in Open-World Scenarios",
            "submittedOnDailyBy": {
                "_id": "67b534337d044bebc2e8752c",
                "avatarUrl": "/avatars/5b086a74d966b063b72919366833c2eb.svg",
                "isPro": false,
                "fullname": "Jiahao Chen",
                "user": "JiahaoChen1",
                "type": "user"
            },
            "summary": "Long-tailed learning has garnered increasing attention due to its wide\napplicability in real-world scenarios. Among existing approaches, Long-Tailed\nSemi-Supervised Learning (LTSSL) has emerged as an effective solution by\nincorporating a large amount of unlabeled data into the imbalanced labeled\ndataset. However, most prior LTSSL methods are designed to train models from\nscratch, which often leads to issues such as overconfidence and low-quality\npseudo-labels. To address these challenges, we extend LTSSL into the foundation\nmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed\nsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate\nthat fine-tuned foundation models can generate more reliable pseudolabels,\nthereby benefiting imbalanced learning. Furthermore, we explore a more\npractical setting by investigating semi-supervised learning under open-world\nconditions, where the unlabeled data may include out-of-distribution (OOD)\nsamples. To handle this problem, we propose LoFT-OW (LoFT under Open-World\nscenarios) to improve the discriminative ability. Experimental results on\nmultiple benchmarks demonstrate that our method achieves superior performance\ncompared to previous approaches, even when utilizing only 1\\% of the unlabeled\ndata compared with previous works.",
            "upvotes": 5,
            "discussionId": "68c77c66ee0eed1697d6b70f",
            "ai_summary": "LoFT, a parameter-efficient fine-tuning framework for long-tailed semi-supervised learning, improves reliability of pseudolabels and discriminative ability in open-world scenarios, outperforming previous methods.",
            "ai_keywords": [
                "Long-Tailed Semi-Supervised Learning",
                "LTSSL",
                "parameter-efficient fine-tuning",
                "pseudolabels",
                "open-world scenarios",
                "out-of-distribution",
                "discriminative ability"
            ]
        },
        "publishedAt": "2025-09-11T22:28:32.000Z",
        "title": "LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised\n  Learning in Open-World Scenarios",
        "summary": "Long-tailed learning has garnered increasing attention due to its wide\napplicability in real-world scenarios. Among existing approaches, Long-Tailed\nSemi-Supervised Learning (LTSSL) has emerged as an effective solution by\nincorporating a large amount of unlabeled data into the imbalanced labeled\ndataset. However, most prior LTSSL methods are designed to train models from\nscratch, which often leads to issues such as overconfidence and low-quality\npseudo-labels. To address these challenges, we extend LTSSL into the foundation\nmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailed\nsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstrate\nthat fine-tuned foundation models can generate more reliable pseudolabels,\nthereby benefiting imbalanced learning. Furthermore, we explore a more\npractical setting by investigating semi-supervised learning under open-world\nconditions, where the unlabeled data may include out-of-distribution (OOD)\nsamples. To handle this problem, we propose LoFT-OW (LoFT under Open-World\nscenarios) to improve the discriminative ability. Experimental results on\nmultiple benchmarks demonstrate that our method achieves superior performance\ncompared to previous approaches, even when utilizing only 1\\% of the unlabeled\ndata compared with previous works.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09926.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67b534337d044bebc2e8752c",
            "avatarUrl": "/avatars/5b086a74d966b063b72919366833c2eb.svg",
            "fullname": "Jiahao Chen",
            "name": "JiahaoChen1",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.10058",
            "authors": [
                {
                    "_id": "68c76cd9ee0eed1697d6b670",
                    "name": "Sung-Lin Tsai",
                    "hidden": false
                },
                {
                    "_id": "68c76cd9ee0eed1697d6b671",
                    "name": "Bo-Lun Huang",
                    "hidden": false
                },
                {
                    "_id": "68c76cd9ee0eed1697d6b672",
                    "name": "Yu Ting Shen",
                    "hidden": false
                },
                {
                    "_id": "68c76cd9ee0eed1697d6b673",
                    "name": "Cheng Yu Yeo",
                    "hidden": false
                },
                {
                    "_id": "68c76cd9ee0eed1697d6b674",
                    "name": "Chiang Tseng",
                    "hidden": false
                },
                {
                    "_id": "68c76cd9ee0eed1697d6b675",
                    "name": "Bo-Kai Ruan",
                    "hidden": false
                },
                {
                    "_id": "68c76cd9ee0eed1697d6b676",
                    "name": "Wen-Sheng Lien",
                    "hidden": false
                },
                {
                    "_id": "68c76cd9ee0eed1697d6b677",
                    "name": "Hong-Han Shuai",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T08:44:22.000Z",
            "submittedOnDailyAt": "2025-09-15T00:03:26.917Z",
            "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings\n  for Improved Diffusion Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.",
            "upvotes": 4,
            "discussionId": "68c76cdaee0eed1697d6b678",
            "ai_summary": "A training-free framework uses a large language model to disambiguate color terms and refine text embeddings for improved color accuracy in text-to-image generation.",
            "ai_keywords": [
                "diffusion models",
                "text-to-image (T2I) generation",
                "color fidelity",
                "large language model (LLM)",
                "color-related prompts",
                "text embeddings",
                "CIELAB color space",
                "color accuracy",
                "image quality",
                "text semantics",
                "visual generation"
            ]
        },
        "publishedAt": "2025-09-12T04:44:22.000Z",
        "title": "Color Me Correctly: Bridging Perceptual Color Spaces and Text Embeddings\n  for Improved Diffusion Generation",
        "summary": "Accurate color alignment in text-to-image (T2I) generation is critical for\napplications such as fashion, product visualization, and interior design, yet\ncurrent diffusion models struggle with nuanced and compound color terms (e.g.,\nTiffany blue, lime green, hot pink), often producing images that are misaligned\nwith human intent. Existing approaches rely on cross-attention manipulation,\nreference images, or fine-tuning but fail to systematically resolve ambiguous\ncolor descriptions. To precisely render colors under prompt ambiguity, we\npropose a training-free framework that enhances color fidelity by leveraging a\nlarge language model (LLM) to disambiguate color-related prompts and guiding\ncolor blending operations directly in the text embedding space. Our method\nfirst employs a large language model (LLM) to resolve ambiguous color terms in\nthe text prompt, and then refines the text embeddings based on the spatial\nrelationships of the resulting color terms in the CIELAB color space. Unlike\nprior methods, our approach improves color accuracy without requiring\nadditional training or external reference images. Experimental results\ndemonstrate that our framework improves color alignment without compromising\nimage quality, bridging the gap between text semantics and visual generation.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10058.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 104
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.09995",
            "authors": [
                {
                    "_id": "68c77229ee0eed1697d6b6ae",
                    "name": "Fei Xiong",
                    "hidden": false
                },
                {
                    "_id": "68c77229ee0eed1697d6b6af",
                    "user": {
                        "_id": "656553d89bf6665f10e3a92d",
                        "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                        "isPro": false,
                        "fullname": "xiang wyatt zhang",
                        "user": "Wyattz23",
                        "type": "user"
                    },
                    "name": "Xiang Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:07:49.281Z",
                    "hidden": false
                },
                {
                    "_id": "68c77229ee0eed1697d6b6b0",
                    "name": "Aosong Feng",
                    "hidden": false
                },
                {
                    "_id": "68c77229ee0eed1697d6b6b1",
                    "name": "Siqi Sun",
                    "hidden": false
                },
                {
                    "_id": "68c77229ee0eed1697d6b6b2",
                    "name": "Chenyu You",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T06:35:40.000Z",
            "submittedOnDailyAt": "2025-09-15T00:26:15.778Z",
            "title": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading",
            "submittedOnDailyBy": {
                "_id": "656553d89bf6665f10e3a92d",
                "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
                "isPro": false,
                "fullname": "xiang wyatt zhang",
                "user": "Wyattz23",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Models (LLMs) have demonstrated impressive\ncapabilities in financial reasoning and market understanding. Multi-agent LLM\nframeworks such as TradingAgent and FINMEM augment these models to long-horizon\ninvestment tasks, leveraging fundamental and sentiment-based inputs for\nstrategic decision-making. However, such systems are ill-suited for the\nhigh-speed, precision-critical demands of High-Frequency Trading (HFT). HFT\nrequires rapid, risk-aware decisions based on structured, short-horizon\nsignals, including technical indicators, chart patterns, and trend-based\nfeatures, distinct from the long-term semantic reasoning typical of traditional\nfinancial LLM applications. To this end, we introduce QuantAgent, the first\nmulti-agent LLM framework explicitly designed for high-frequency algorithmic\ntrading. The system decomposes trading into four specialized agents, Indicator,\nPattern, Trend, and Risk, each equipped with domain-specific tools and\nstructured reasoning capabilities to capture distinct aspects of market\ndynamics over short temporal windows. In zero-shot evaluations across ten\nfinancial instruments, including Bitcoin and Nasdaq futures, QuantAgent\ndemonstrates superior performance in both predictive accuracy and cumulative\nreturn over 4-hour trading intervals, outperforming strong neural and\nrule-based baselines. Our findings suggest that combining structured financial\npriors with language-native reasoning unlocks new potential for traceable,\nreal-time decision systems in high-frequency financial markets.",
            "upvotes": 4,
            "discussionId": "68c77229ee0eed1697d6b6b3",
            "ai_summary": "QuantAgent, a multi-agent LLM framework, excels in high-frequency trading by leveraging specialized agents for technical indicators, chart patterns, trends, and risk, outperforming existing neural and rule-based systems.",
            "ai_keywords": [
                "Large Language Models",
                "LLMs",
                "Multi-agent LLM frameworks",
                "TradingAgent",
                "FINMEM",
                "High-Frequency Trading",
                "HFT",
                "structured reasoning",
                "technical indicators",
                "chart patterns",
                "trend-based features",
                "zero-shot evaluations",
                "predictive accuracy",
                "cumulative return",
                "financial instruments",
                "Bitcoin",
                "Nasdaq futures",
                "domain-specific tools",
                "traceable",
                "real-time decision systems"
            ]
        },
        "publishedAt": "2025-09-12T02:35:40.000Z",
        "title": "QuantAgent: Price-Driven Multi-Agent LLMs for High-Frequency Trading",
        "summary": "Recent advances in Large Language Models (LLMs) have demonstrated impressive\ncapabilities in financial reasoning and market understanding. Multi-agent LLM\nframeworks such as TradingAgent and FINMEM augment these models to long-horizon\ninvestment tasks, leveraging fundamental and sentiment-based inputs for\nstrategic decision-making. However, such systems are ill-suited for the\nhigh-speed, precision-critical demands of High-Frequency Trading (HFT). HFT\nrequires rapid, risk-aware decisions based on structured, short-horizon\nsignals, including technical indicators, chart patterns, and trend-based\nfeatures, distinct from the long-term semantic reasoning typical of traditional\nfinancial LLM applications. To this end, we introduce QuantAgent, the first\nmulti-agent LLM framework explicitly designed for high-frequency algorithmic\ntrading. The system decomposes trading into four specialized agents, Indicator,\nPattern, Trend, and Risk, each equipped with domain-specific tools and\nstructured reasoning capabilities to capture distinct aspects of market\ndynamics over short temporal windows. In zero-shot evaluations across ten\nfinancial instruments, including Bitcoin and Nasdaq futures, QuantAgent\ndemonstrates superior performance in both predictive accuracy and cumulative\nreturn over 4-hour trading intervals, outperforming strong neural and\nrule-based baselines. Our findings suggest that combining structured financial\npriors with language-native reasoning unlocks new potential for traceable,\nreal-time decision systems in high-frequency financial markets.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09995.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "656553d89bf6665f10e3a92d",
            "avatarUrl": "/avatars/f55b09e249c48ef3fe484afa33a182ae.svg",
            "fullname": "xiang wyatt zhang",
            "name": "Wyattz23",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09737",
            "authors": [
                {
                    "_id": "68c848dd733e345e52ac1dd1",
                    "name": "Klemen Kotar",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dd2",
                    "name": "Wanhee Lee",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dd3",
                    "name": "Rahul Venkatesh",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dd4",
                    "name": "Honglin Chen",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dd5",
                    "name": "Daniel Bear",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dd6",
                    "name": "Jared Watrous",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dd7",
                    "name": "Simon Kim",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dd8",
                    "name": "Khai Loong Aw",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dd9",
                    "name": "Lilian Naing Chen",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dda",
                    "name": "Stefan Stojanov",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1ddb",
                    "name": "Kevin Feigelis",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1ddc",
                    "name": "Imran Thobani",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1ddd",
                    "name": "Alex Durango",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1dde",
                    "name": "Khaled Jedoui",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1ddf",
                    "name": "Atlas Kazemian",
                    "hidden": false
                },
                {
                    "_id": "68c848dd733e345e52ac1de0",
                    "name": "Dan Yamins",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/uqHh1SrA2w9IG6iaLH8qg.png"
            ],
            "publishedAt": "2025-09-10T18:01:04.000Z",
            "submittedOnDailyAt": "2025-09-15T15:43:32.322Z",
            "title": "World Modeling with Probabilistic Structure Integration",
            "submittedOnDailyBy": {
                "_id": "646d3cae4220471ca0c6cb13",
                "avatarUrl": "/avatars/30e47db03a23d49d946cc5248b28959a.svg",
                "isPro": false,
                "fullname": "Klemen Kotar",
                "user": "klemenk",
                "type": "user"
            },
            "summary": "We present Probabilistic Structure Integration (PSI), a system for learning\nrichly controllable and flexibly promptable world models from data. PSI\nconsists of a three-step cycle. The first step, Probabilistic prediction,\ninvolves building a probabilistic graphical model Psi of the data, in the form\nof a random-access autoregressive sequence model. Psi supports a complete set\nof learned conditional distributions describing the dependence of any variables\nin the data on any other set of variables. In step 2, Structure extraction, we\nshow how to extract underlying low-dimensional properties in the data,\ncorresponding to a diverse set of meaningful \"intermediate structures\", in a\nzero-shot fashion via causal inference on Psi. Step 3, Integration, completes\nthe cycle by converting these structures into new token types that are then\ncontinually mixed back into the training diet as conditioning signals and\nprediction targets. Each such cycle augments the capabilities of Psi, both\nallowing it to model the underlying data better, and creating new control\nhandles -- akin to an LLM-like universal prompting language. We train an\ninstance of Psi on 1.4 trillion tokens of internet video data; we use it to\nperform a variety of useful video prediction and understanding inferences; we\nextract state-of-the-art optical flow, self-supervised depth and object\nsegmentation; and we use these structures to support a full cycle of predictive\nimprovements.",
            "upvotes": 4,
            "discussionId": "68c848de733e345e52ac1de1",
            "ai_summary": "Probabilistic Structure Integration (PSI) learns richly controllable world models from data through probabilistic prediction, structure extraction, and integration, enhancing video prediction and understanding.",
            "ai_keywords": [
                "probabilistic graphical model",
                "random-access autoregressive sequence model",
                "conditional distributions",
                "causal inference",
                "intermediate structures",
                "token types",
                "predictive improvements",
                "optical flow",
                "self-supervised depth",
                "object segmentation"
            ]
        },
        "publishedAt": "2025-09-10T14:01:04.000Z",
        "title": "World Modeling with Probabilistic Structure Integration",
        "summary": "We present Probabilistic Structure Integration (PSI), a system for learning\nrichly controllable and flexibly promptable world models from data. PSI\nconsists of a three-step cycle. The first step, Probabilistic prediction,\ninvolves building a probabilistic graphical model Psi of the data, in the form\nof a random-access autoregressive sequence model. Psi supports a complete set\nof learned conditional distributions describing the dependence of any variables\nin the data on any other set of variables. In step 2, Structure extraction, we\nshow how to extract underlying low-dimensional properties in the data,\ncorresponding to a diverse set of meaningful \"intermediate structures\", in a\nzero-shot fashion via causal inference on Psi. Step 3, Integration, completes\nthe cycle by converting these structures into new token types that are then\ncontinually mixed back into the training diet as conditioning signals and\nprediction targets. Each such cycle augments the capabilities of Psi, both\nallowing it to model the underlying data better, and creating new control\nhandles -- akin to an LLM-like universal prompting language. We train an\ninstance of Psi on 1.4 trillion tokens of internet video data; we use it to\nperform a variety of useful video prediction and understanding inferences; we\nextract state-of-the-art optical flow, self-supervised depth and object\nsegmentation; and we use these structures to support a full cycle of predictive\nimprovements.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/646d3cae4220471ca0c6cb13/uqHh1SrA2w9IG6iaLH8qg.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09737.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "646d3cae4220471ca0c6cb13",
            "avatarUrl": "/avatars/30e47db03a23d49d946cc5248b28959a.svg",
            "fullname": "Klemen Kotar",
            "name": "klemenk",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.09734",
            "authors": [
                {
                    "_id": "68c76e07ee0eed1697d6b68a",
                    "user": {
                        "_id": "66c6f02ee2fdf1d811ec5fbd",
                        "avatarUrl": "/avatars/dc029e6042d974742d1789e7e3c014f2.svg",
                        "isPro": false,
                        "fullname": "zikang guo",
                        "user": "xuekeluoluo",
                        "type": "user"
                    },
                    "name": "Zikang Guo",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-15T15:22:46.250Z",
                    "hidden": false
                },
                {
                    "_id": "68c76e07ee0eed1697d6b68b",
                    "name": "Benfeng Xu",
                    "hidden": false
                },
                {
                    "_id": "68c76e07ee0eed1697d6b68c",
                    "user": {
                        "_id": "663b22a80966eef8686aadaf",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/663b22a80966eef8686aadaf/iBzyQTyGZKf33RPVIFh9a.jpeg",
                        "isPro": false,
                        "fullname": "Chiwei Zhu",
                        "user": "IgnoraZ",
                        "type": "user"
                    },
                    "name": "Chiwei Zhu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-15T15:23:07.060Z",
                    "hidden": false
                },
                {
                    "_id": "68c76e07ee0eed1697d6b68d",
                    "name": "Wentao Hong",
                    "hidden": false
                },
                {
                    "_id": "68c76e07ee0eed1697d6b68e",
                    "name": "Xiaorui Wang",
                    "hidden": false
                },
                {
                    "_id": "68c76e07ee0eed1697d6b68f",
                    "name": "Zhendong Mao",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T14:08:40.000Z",
            "submittedOnDailyAt": "2025-09-15T00:08:27.739Z",
            "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with\n  MCP-Mediated Tools",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The Model Context Protocol (MCP) is rapidly emerging as a pivotal open\nstandard, designed to enhance agent-tool integration and interoperability, and\nis positioned to unlock a new era of powerful, interconnected, and genuinely\nutilitarian agentic AI. However, despite MCP's growing adoption, existing\nbenchmarks often fail to capture real-world agent performance within this new\nparadigm, leading to a distorted perception of their true operational value and\nan inability to reliably differentiate proficiencies. To bridge this critical\nevaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark\nspecifically engineered to rigorously assess language agent capabilities in\nMCP-mediated tool interactions. Core contributions of MCP-AgentBench include:\nthe establishment of a robust MCP testbed comprising 33 operational servers\nwith 188 distinct tools; the development of a benchmark featuring 600\nsystematically designed queries distributed across 6 distinct categories of\nvarying interaction complexity; and the introduction of MCP-Eval, a novel\noutcome-oriented evaluation methodology prioritizing real-world task success.\nThrough extensive empirical evaluation of leading language agents, we provide\nfoundational insights. MCP-AgentBench aims to equip the research community with\na standardized and reliable framework to build, validate, and advance agents\ncapable of fully leveraging MCP's transformative benefits, thereby accelerating\nprogress toward truly capable and interoperable AI systems.",
            "upvotes": 3,
            "discussionId": "68c76e07ee0eed1697d6b690",
            "ai_summary": "MCP-AgentBench is a benchmark designed to evaluate language agents in MCP-mediated tool interactions, providing a standardized framework for assessing real-world performance.",
            "ai_keywords": [
                "MCP",
                "MCP-AgentBench",
                "MCP testbed",
                "MCP-Eval",
                "language agents",
                "tool interactions",
                "benchmark",
                "evaluation methodology",
                "real-world task success"
            ]
        },
        "publishedAt": "2025-09-10T10:08:40.000Z",
        "title": "MCP-AgentBench: Evaluating Real-World Language Agent Performance with\n  MCP-Mediated Tools",
        "summary": "The Model Context Protocol (MCP) is rapidly emerging as a pivotal open\nstandard, designed to enhance agent-tool integration and interoperability, and\nis positioned to unlock a new era of powerful, interconnected, and genuinely\nutilitarian agentic AI. However, despite MCP's growing adoption, existing\nbenchmarks often fail to capture real-world agent performance within this new\nparadigm, leading to a distorted perception of their true operational value and\nan inability to reliably differentiate proficiencies. To bridge this critical\nevaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark\nspecifically engineered to rigorously assess language agent capabilities in\nMCP-mediated tool interactions. Core contributions of MCP-AgentBench include:\nthe establishment of a robust MCP testbed comprising 33 operational servers\nwith 188 distinct tools; the development of a benchmark featuring 600\nsystematically designed queries distributed across 6 distinct categories of\nvarying interaction complexity; and the introduction of MCP-Eval, a novel\noutcome-oriented evaluation methodology prioritizing real-world task success.\nThrough extensive empirical evaluation of leading language agents, we provide\nfoundational insights. MCP-AgentBench aims to equip the research community with\na standardized and reliable framework to build, validate, and advance agents\ncapable of fully leveraging MCP's transformative benefits, thereby accelerating\nprogress toward truly capable and interoperable AI systems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09734.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 104
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07966",
            "authors": [
                {
                    "_id": "68c1330c3912ed54cf5432ff",
                    "user": {
                        "_id": "667a39f1b2401c64d7e5fe60",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/7RiX82BJ8L1BJJDu36-P5.jpeg",
                        "isPro": false,
                        "fullname": "Aser Lompo",
                        "user": "AserLompo",
                        "type": "user"
                    },
                    "name": "Boammani Aser Lompo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:09:05.086Z",
                    "hidden": false
                },
                {
                    "_id": "68c1330c3912ed54cf543300",
                    "user": {
                        "_id": "634c653605f736dff3778dfa",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634c653605f736dff3778dfa/P7_LB6wAlUjNIuKdPgXmK.jpeg",
                        "isPro": false,
                        "fullname": "Marc HARAOUI",
                        "user": "MarcHaraoui",
                        "type": "user"
                    },
                    "name": "Marc Haraoui",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-13T14:56:42.846Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T17:52:26.000Z",
            "submittedOnDailyAt": "2025-09-15T16:47:36.626Z",
            "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
            "submittedOnDailyBy": {
                "_id": "634c653605f736dff3778dfa",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634c653605f736dff3778dfa/P7_LB6wAlUjNIuKdPgXmK.jpeg",
                "isPro": false,
                "fullname": "Marc HARAOUI",
                "user": "MarcHaraoui",
                "type": "user"
            },
            "summary": "Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA.",
            "upvotes": 3,
            "discussionId": "68c1330c3912ed54cf543301",
            "projectPage": "https://huggingface.co/datasets/AI-4-Everyone/Visual-TableQA",
            "githubRepo": "https://github.com/AI-4-Everyone/Visual-TableQA",
            "ai_summary": "Visual-TableQA is a large-scale, open-domain dataset for evaluating visual reasoning over complex tabular data, generated using a modular pipeline involving multiple reasoning LLMs.",
            "ai_keywords": [
                "vision-language models",
                "visual reasoning",
                "multimodal dataset",
                "LaTeX-rendered tables",
                "reasoning-intensive QA pairs",
                "generation pipeline",
                "reasoning LLMs",
                "cross-model prompting",
                "LLM-jury filtering",
                "fine-tuning",
                "external benchmarks"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-09T13:52:26.000Z",
        "title": "Visual-TableQA: Open-Domain Benchmark for Reasoning over Table Images",
        "summary": "Visual reasoning over structured data such as tables is a critical capability\nfor modern vision-language models (VLMs), yet current benchmarks remain limited\nin scale, diversity, or reasoning depth, especially when it comes to rendered\ntable images. Addressing this gap, we introduce Visual-TableQA, a large-scale,\nopen-domain multimodal dataset specifically designed to evaluate and enhance\nvisual reasoning over complex tabular data. Our generation pipeline is modular,\nscalable, and fully autonomous, involving multiple reasoning LLMs collaborating\nacross distinct roles: generation, validation, and inspiration. Visual-TableQA\ncomprises 2.5k richly structured LaTeX-rendered tables and 6k\nreasoning-intensive QA pairs, all produced at a cost of under USD 100. To\npromote diversity and creativity, our pipeline performs multi-model\ncollaborative data generation via cross-model prompting ('inspiration') and\nLLM-jury filtering. Stronger models seed layouts and topics that weaker models\nelaborate, collectively distilling diverse reasoning patterns and visual\nstructures into the dataset. Empirical results show that models fine-tuned on\nVisual-TableQA generalize robustly to external benchmarks, outperforming\nseveral proprietary models despite the dataset's synthetic nature. The full\npipeline and resources are publicly available at\nhttps://github.com/AI-4-Everyone/Visual-TableQA.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07966.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "634c653605f736dff3778dfa",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/634c653605f736dff3778dfa/P7_LB6wAlUjNIuKdPgXmK.jpeg",
            "fullname": "Marc HARAOUI",
            "name": "MarcHaraoui",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09524",
            "authors": [
                {
                    "_id": "68c514daee0eed1697d6b395",
                    "user": {
                        "_id": "6305d542435ec751b72434b8",
                        "avatarUrl": "/avatars/cc48bf4102cd9c959b4f2a39c7c422f4.svg",
                        "isPro": true,
                        "fullname": "Daniil Ignatev",
                        "user": "ruthenian8",
                        "type": "user"
                    },
                    "name": "Daniil Ignatev",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-15T15:22:01.696Z",
                    "hidden": false
                },
                {
                    "_id": "68c514daee0eed1697d6b396",
                    "user": {
                        "_id": "61bf8017c88f3fd22f654086",
                        "avatarUrl": "/avatars/ea9e762b3db755fe051751577353fbca.svg",
                        "isPro": false,
                        "fullname": "Nan Li",
                        "user": "chnln",
                        "type": "user"
                    },
                    "name": "Nan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-13T14:50:49.829Z",
                    "hidden": false
                },
                {
                    "_id": "68c514daee0eed1697d6b397",
                    "name": "Hugh Mee Wong",
                    "hidden": false
                },
                {
                    "_id": "68c514daee0eed1697d6b398",
                    "name": "Anh Dang",
                    "hidden": false
                },
                {
                    "_id": "68c514daee0eed1697d6b399",
                    "name": "Shane Kaszefski Yaschuk",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-11T15:04:42.000Z",
            "submittedOnDailyAt": "2025-09-15T11:44:19.662Z",
            "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning\n  and Label Distribution Learning",
            "submittedOnDailyBy": {
                "_id": "61bf8017c88f3fd22f654086",
                "avatarUrl": "/avatars/ea9e762b3db755fe051751577353fbca.svg",
                "isPro": false,
                "fullname": "Nan Li",
                "user": "chnln",
                "type": "user"
            },
            "summary": "This system paper presents the DeMeVa team's approaches to the third edition\nof the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et\nal., 2025). We explore two directions: in-context learning (ICL) with large\nlanguage models, where we compare example sampling strategies; and label\ndistribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we\nevaluate several fine-tuning methods. Our contributions are twofold: (1) we\nshow that ICL can effectively predict annotator-specific annotations\n(perspectivist annotations), and that aggregating these predictions into soft\nlabels yields competitive performance; and (2) we argue that LDL methods are\npromising for soft label predictions and merit further exploration by the\nperspectivist community.",
            "upvotes": 2,
            "discussionId": "68c514daee0eed1697d6b39a",
            "ai_summary": "DeMeVa explores in-context learning and label distribution learning for predicting annotator-specific annotations and generating soft labels, demonstrating competitive performance and potential for further research.",
            "ai_keywords": [
                "in-context learning",
                "ICL",
                "label distribution learning",
                "LDL",
                "large language models",
                "example sampling strategies",
                "fine-tuning methods",
                "RoBERTa",
                "perspectivist annotations",
                "soft labels"
            ]
        },
        "publishedAt": "2025-09-11T11:04:42.000Z",
        "title": "DeMeVa at LeWiDi-2025: Modeling Perspectives with In-Context Learning\n  and Label Distribution Learning",
        "summary": "This system paper presents the DeMeVa team's approaches to the third edition\nof the Learning with Disagreements shared task (LeWiDi 2025; Leonardelli et\nal., 2025). We explore two directions: in-context learning (ICL) with large\nlanguage models, where we compare example sampling strategies; and label\ndistribution learning (LDL) methods with RoBERTa (Liu et al., 2019b), where we\nevaluate several fine-tuning methods. Our contributions are twofold: (1) we\nshow that ICL can effectively predict annotator-specific annotations\n(perspectivist annotations), and that aggregating these predictions into soft\nlabels yields competitive performance; and (2) we argue that LDL methods are\npromising for soft label predictions and merit further exploration by the\nperspectivist community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09524.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "61bf8017c88f3fd22f654086",
            "avatarUrl": "/avatars/ea9e762b3db755fe051751577353fbca.svg",
            "fullname": "Nan Li",
            "name": "chnln",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.01535",
            "authors": [
                {
                    "_id": "68b85733d43cadaf7a688a0f",
                    "user": {
                        "_id": "68c6b73ad0bef8d72febf45d",
                        "avatarUrl": "/avatars/7b65febb59f2524a81b08c9f6bd881f5.svg",
                        "isPro": false,
                        "fullname": "Kairong Han",
                        "user": "Kairong-Han",
                        "type": "user"
                    },
                    "name": "Kairong Han",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:09:25.856Z",
                    "hidden": false
                },
                {
                    "_id": "68b85733d43cadaf7a688a10",
                    "name": "Wenshuo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b85733d43cadaf7a688a11",
                    "name": "Ziyu Zhao",
                    "hidden": false
                },
                {
                    "_id": "68b85733d43cadaf7a688a12",
                    "name": "JunJian Ye",
                    "hidden": false
                },
                {
                    "_id": "68b85733d43cadaf7a688a13",
                    "name": "Lujia Pan",
                    "hidden": false
                },
                {
                    "_id": "68b85733d43cadaf7a688a14",
                    "name": "Kun Kuang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-01T15:13:15.000Z",
            "submittedOnDailyAt": "2025-09-15T14:26:49.031Z",
            "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge\n  into Large Language Models",
            "submittedOnDailyBy": {
                "_id": "68c6b73ad0bef8d72febf45d",
                "avatarUrl": "/avatars/7b65febb59f2524a81b08c9f6bd881f5.svg",
                "isPro": false,
                "fullname": "Kairong Han",
                "user": "Kairong-Han",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, a fundamental question remains: Can LLMs effectively utilize\ncausal knowledge for prediction and generation? Through empirical studies, we\nfind that LLMs trained directly on large-scale data often capture spurious\ncorrelations rather than true causal relationships, leading to suboptimal\nperformance, especially in out-of-distribution (OOD) scenarios. To address this\nchallenge, we propose Causal Attention Tuning (CAT), a novel approach that\ninjects fine-grained causal knowledge into the attention mechanism. We propose\nan automated pipeline that leverages human priors to automatically generate\ntoken-level causal signals and introduce the Re-Attention mechanism to guide\ntraining, helping the model focus on causal structures while mitigating noise\nand biases in attention scores. Experimental results on our proposed Spurious\nToken Game (STG) benchmark and multiple downstream tasks demonstrate that our\napproach effectively leverages causal knowledge for prediction and remains\nrobust in OOD scenarios. Implementation details can be found at\nhttps://github.com/Kairong-Han/CAT.",
            "upvotes": 2,
            "discussionId": "68b85733d43cadaf7a688a15",
            "ai_summary": "Causal Attention Tuning (CAT) enhances Large Language Models (LLMs) by injecting causal knowledge into the attention mechanism, improving prediction accuracy and robustness in out-of-distribution scenarios.",
            "ai_keywords": [
                "Large Language Models (LLMs)",
                "causal knowledge",
                "spurious correlations",
                "true causal relationships",
                "out-of-distribution (OOD) scenarios",
                "Causal Attention Tuning (CAT)",
                "token-level causal signals",
                "Re-Attention mechanism",
                "Spurious Token Game (STG)"
            ]
        },
        "publishedAt": "2025-09-01T11:13:15.000Z",
        "title": "CAT: Causal Attention Tuning For Injecting Fine-grained Causal Knowledge\n  into Large Language Models",
        "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains. However, a fundamental question remains: Can LLMs effectively utilize\ncausal knowledge for prediction and generation? Through empirical studies, we\nfind that LLMs trained directly on large-scale data often capture spurious\ncorrelations rather than true causal relationships, leading to suboptimal\nperformance, especially in out-of-distribution (OOD) scenarios. To address this\nchallenge, we propose Causal Attention Tuning (CAT), a novel approach that\ninjects fine-grained causal knowledge into the attention mechanism. We propose\nan automated pipeline that leverages human priors to automatically generate\ntoken-level causal signals and introduce the Re-Attention mechanism to guide\ntraining, helping the model focus on causal structures while mitigating noise\nand biases in attention scores. Experimental results on our proposed Spurious\nToken Game (STG) benchmark and multiple downstream tasks demonstrate that our\napproach effectively leverages causal knowledge for prediction and remains\nrobust in OOD scenarios. Implementation details can be found at\nhttps://github.com/Kairong-Han/CAT.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.01535.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "68c6b73ad0bef8d72febf45d",
            "avatarUrl": "/avatars/7b65febb59f2524a81b08c9f6bd881f5.svg",
            "fullname": "Kairong Han",
            "name": "Kairong-Han",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09990",
            "authors": [
                {
                    "_id": "68c7928dee0eed1697d6b757",
                    "user": {
                        "_id": "6747329d228a652d5707e094",
                        "avatarUrl": "/avatars/f33e118950e329ce5612877413806e49.svg",
                        "isPro": false,
                        "fullname": "GUIXIAN XU",
                        "user": "Stuart-Xu",
                        "type": "user"
                    },
                    "name": "Guixian Xu",
                    "status": "admin_assigned",
                    "statusLastChangedAt": "2025-09-15T15:22:22.199Z",
                    "hidden": false
                },
                {
                    "_id": "68c7928dee0eed1697d6b758",
                    "name": "Zeli Su",
                    "hidden": false
                },
                {
                    "_id": "68c7928dee0eed1697d6b759",
                    "user": {
                        "_id": "6430bdd8cd31d174a9f900fb",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
                        "isPro": false,
                        "fullname": "Ziyin Zhang",
                        "user": "Geralt-Targaryen",
                        "type": "user"
                    },
                    "name": "Ziyin Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:06:30.903Z",
                    "hidden": false
                },
                {
                    "_id": "68c7928dee0eed1697d6b75a",
                    "name": "Jianing Liu",
                    "hidden": false
                },
                {
                    "_id": "68c7928dee0eed1697d6b75b",
                    "name": "XU Han",
                    "hidden": false
                },
                {
                    "_id": "68c7928dee0eed1697d6b75c",
                    "name": "Ting Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c7928dee0eed1697d6b75d",
                    "name": "Yushuang Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T06:18:44.000Z",
            "submittedOnDailyAt": "2025-09-15T02:50:37.480Z",
            "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority\n  Languages in China",
            "submittedOnDailyBy": {
                "_id": "6430bdd8cd31d174a9f900fb",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
                "isPro": false,
                "fullname": "Ziyin Zhang",
                "user": "Geralt-Targaryen",
                "type": "user"
            },
            "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional\nMongolian, face significant challenges due to their unique writing systems,\nwhich differ from international standards. This discrepancy has led to a severe\nlack of relevant corpora, particularly for supervised tasks like headline\ngeneration. To address this gap, we introduce a novel dataset, Chinese Minority\nHeadline Generation (CMHG), which includes 100,000 entries for Tibetan, and\n50,000 entries each for Uyghur and Mongolian, specifically curated for headline\ngeneration tasks. Additionally, we propose a high-quality test set annotated by\nnative speakers, designed to serve as a benchmark for future research in this\ndomain. We hope this dataset will become a valuable resource for advancing\nheadline generation in Chinese minority languages and contribute to the\ndevelopment of related benchmarks.",
            "upvotes": 1,
            "discussionId": "68c7928dee0eed1697d6b75e"
        },
        "publishedAt": "2025-09-12T02:18:44.000Z",
        "title": "CMHG: A Dataset and Benchmark for Headline Generation of Minority\n  Languages in China",
        "summary": "Minority languages in China, such as Tibetan, Uyghur, and Traditional\nMongolian, face significant challenges due to their unique writing systems,\nwhich differ from international standards. This discrepancy has led to a severe\nlack of relevant corpora, particularly for supervised tasks like headline\ngeneration. To address this gap, we introduce a novel dataset, Chinese Minority\nHeadline Generation (CMHG), which includes 100,000 entries for Tibetan, and\n50,000 entries each for Uyghur and Mongolian, specifically curated for headline\ngeneration tasks. Additionally, we propose a high-quality test set annotated by\nnative speakers, designed to serve as a benchmark for future research in this\ndomain. We hope this dataset will become a valuable resource for advancing\nheadline generation in Chinese minority languages and contribute to the\ndevelopment of related benchmarks.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09990.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6430bdd8cd31d174a9f900fb",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/Y9SPnRfpKSbYc7MhNdP-H.jpeg",
            "fullname": "Ziyin Zhang",
            "name": "Geralt-Targaryen",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 6
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.08825",
            "authors": [
                {
                    "_id": "68c7d6cdee0eed1697d6b82e",
                    "user": {
                        "_id": "6625088168af267420472310",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6625088168af267420472310/EEAPBs5OXuwJK67dMNi3v.jpeg",
                        "isPro": false,
                        "fullname": "Joachim Baumann",
                        "user": "joebaumann",
                        "type": "user"
                    },
                    "name": "Joachim Baumann",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:06:14.313Z",
                    "hidden": false
                },
                {
                    "_id": "68c7d6cdee0eed1697d6b82f",
                    "name": "Paul Röttger",
                    "hidden": false
                },
                {
                    "_id": "68c7d6cdee0eed1697d6b830",
                    "name": "Aleksandra Urman",
                    "hidden": false
                },
                {
                    "_id": "68c7d6cdee0eed1697d6b831",
                    "name": "Albert Wendsjö",
                    "hidden": false
                },
                {
                    "_id": "68c7d6cdee0eed1697d6b832",
                    "name": "Flor Miriam Plaza-del-Arco",
                    "hidden": false
                },
                {
                    "_id": "68c7d6cdee0eed1697d6b833",
                    "name": "Johannes B. Gruber",
                    "hidden": false
                },
                {
                    "_id": "68c7d6cdee0eed1697d6b834",
                    "name": "Dirk Hovy",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/6625088168af267420472310/v13bD5RwK_ODeb5HFhXDe.png"
            ],
            "publishedAt": "2025-09-10T17:58:53.000Z",
            "submittedOnDailyAt": "2025-09-15T14:02:14.549Z",
            "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
            "submittedOnDailyBy": {
                "_id": "6625088168af267420472310",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6625088168af267420472310/EEAPBs5OXuwJK67dMNi3v.jpeg",
                "isPro": false,
                "fullname": "Joachim Baumann",
                "user": "joebaumann",
                "type": "user"
            },
            "summary": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.",
            "upvotes": 1,
            "discussionId": "68c7d6ceee0eed1697d6b835",
            "ai_summary": "LLM hacking introduces significant variability and error in social science research, affecting statistical conclusions and requiring rigorous verification and human annotations to mitigate.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "data annotation",
                "text analysis",
                "model selection",
                "prompting strategy",
                "temperature settings",
                "systematic biases",
                "random errors",
                "Type I errors",
                "Type II errors",
                "Type S errors",
                "Type M errors",
                "LLM hacking",
                "statistical conclusions",
                "effect sizes",
                "human annotations",
                "regression estimator correction"
            ]
        },
        "publishedAt": "2025-09-10T13:58:53.000Z",
        "title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs\n  for Text Annotation",
        "summary": "Large language models (LLMs) are rapidly transforming social science research\nby enabling the automation of labor-intensive tasks like data annotation and\ntext analysis. However, LLM outputs vary significantly depending on the\nimplementation choices made by researchers (e.g., model selection, prompting\nstrategy, or temperature settings). Such variation can introduce systematic\nbiases and random errors, which propagate to downstream analyses and cause Type\nI, Type II, Type S, or Type M errors. We call this LLM hacking.\n  We quantify the risk of LLM hacking by replicating 37 data annotation tasks\nfrom 21 published social science research studies with 18 different models.\nAnalyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure\nhow plausible researcher choices affect statistical conclusions. We find\nincorrect conclusions based on LLM-annotated data in approximately one in three\nhypotheses for state-of-the-art models, and in half the hypotheses for small\nlanguage models. While our findings show that higher task performance and\nbetter general model capabilities reduce LLM hacking risk, even highly accurate\nmodels do not completely eliminate it. The risk of LLM hacking decreases as\neffect sizes increase, indicating the need for more rigorous verification of\nfindings near significance thresholds. Our extensive analysis of LLM hacking\nmitigation techniques emphasizes the importance of human annotations in\nreducing false positive findings and improving model selection. Surprisingly,\ncommon regression estimator correction techniques are largely ineffective in\nreducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.\n  Beyond accidental errors, we find that intentional LLM hacking is\nunacceptably simple. With few LLMs and just a handful of prompt paraphrases,\nanything can be presented as statistically significant.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/6625088168af267420472310/v13bD5RwK_ODeb5HFhXDe.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08825.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6625088168af267420472310",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6625088168af267420472310/EEAPBs5OXuwJK67dMNi3v.jpeg",
            "fullname": "Joachim Baumann",
            "name": "joebaumann",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.08270",
            "authors": [
                {
                    "_id": "68c21e4f29b8ec9932cd08a5",
                    "user": {
                        "_id": "65d02414af9670557423df10",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d02414af9670557423df10/5SNGUGEEdkliGKNceo_gS.jpeg",
                        "isPro": false,
                        "fullname": "Pranav Pawar",
                        "user": "prnvpwr2612",
                        "type": "user"
                    },
                    "name": "Pranav Pawar",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-12T16:15:31.529Z",
                    "hidden": false
                },
                {
                    "_id": "68c21e4f29b8ec9932cd08a6",
                    "user": {
                        "_id": "67b2f384b023991df10e161a",
                        "avatarUrl": "/avatars/916cd38530039fad610bef19a13d37bd.svg",
                        "isPro": false,
                        "fullname": "Kavish Shah",
                        "user": "kavish17shah",
                        "type": "user"
                    },
                    "name": "Kavish Shah",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:08:59.917Z",
                    "hidden": false
                },
                {
                    "_id": "68c21e4f29b8ec9932cd08a7",
                    "user": {
                        "_id": "68a9c7381860fc7281b6112a",
                        "avatarUrl": "/avatars/a605586074a1942ca317632179b42fc0.svg",
                        "isPro": false,
                        "fullname": "Akshat Bhalani",
                        "user": "ByteMeHarder-404",
                        "type": "user"
                    },
                    "name": "Akshat Bhalani",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-13T14:56:36.319Z",
                    "hidden": false
                },
                {
                    "_id": "68c21e4f29b8ec9932cd08a8",
                    "user": {
                        "_id": "689edbb2ee02f0ffbef27199",
                        "avatarUrl": "/avatars/d66e7b1d9f0a0b69e282f91194065074.svg",
                        "isPro": false,
                        "fullname": "Komal Kasat",
                        "user": "komalkasat",
                        "type": "user"
                    },
                    "name": "Komal Kasat",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:08:57.687Z",
                    "hidden": false
                },
                {
                    "_id": "68c21e4f29b8ec9932cd08a9",
                    "user": {
                        "_id": "67bed60ec4849f7931cca2e7",
                        "avatarUrl": "/avatars/9479d4a7edfdba5300a496e895235655.svg",
                        "isPro": false,
                        "fullname": "DEV MITTAL",
                        "user": "dev2607",
                        "type": "user"
                    },
                    "name": "Dev Mittal",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:08:51.347Z",
                    "hidden": false
                },
                {
                    "_id": "68c21e4f29b8ec9932cd08aa",
                    "user": {
                        "_id": "6854fca29853c39b3dd703f6",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/gb74FbumwxbuCVIKfEY7N.png",
                        "isPro": false,
                        "fullname": "Hadi Gala",
                        "user": "hadiiigala",
                        "type": "user"
                    },
                    "name": "Hadi Gala",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:08:54.556Z",
                    "hidden": false
                },
                {
                    "_id": "68c21e4f29b8ec9932cd08ab",
                    "name": "Deepali Patil",
                    "hidden": false
                },
                {
                    "_id": "68c21e4f29b8ec9932cd08ac",
                    "name": "Nikita Raichada",
                    "hidden": false
                },
                {
                    "_id": "68c21e4f29b8ec9932cd08ad",
                    "name": "Monali Deshmukh",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-10T04:15:01.000Z",
            "submittedOnDailyAt": "2025-09-15T23:51:31.568Z",
            "title": "Interpretable Physics Reasoning and Performance Taxonomy in\n  Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "65d02414af9670557423df10",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d02414af9670557423df10/5SNGUGEEdkliGKNceo_gS.jpeg",
                "isPro": false,
                "fullname": "Pranav Pawar",
                "user": "prnvpwr2612",
                "type": "user"
            },
            "summary": "As Vision-Language Models (VLMs) grow in sophistication, their ability to\nperform reasoning is coming under increasing supervision. While they excel at\nmany tasks, their grasp of fundamental scientific principles, such as physics,\nremains an underexplored frontier. To reflect the advancements in these\ncapabilities, we introduce a novel and accessible framework designed to\nrigorously evaluate VLMs on their understanding of 2D physics. Our framework\nfeatures a pragmatic scenario generator that creates a diverse testbed of over\n400 problems across four core domains: Projectile Motion, Collision Dynamics,\nMechanics, and Fluid Dynamics. Through comprehensive evaluation of four\nstate-of-the-art VLMs, we demonstrate a strong correlation between model scale\nand reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving\nan overall score of 0.815. We find that while models excel at formulaic\nproblems, they struggle significantly with domains requiring abstract spatial\nreasoning. By designing this framework, we aim to democratize the study of\nscientific reasoning in VLMs and foster deeper insights into their capabilities\nand limitations.",
            "upvotes": 0,
            "discussionId": "68c21e5029b8ec9932cd08ae",
            "githubRepo": "https://github.com/prnvpwr2612/Interpretable-Physics-Reasoning-and-Performance-Taxonomy-in-Vision-Language-Models.git",
            "ai_summary": "A novel framework evaluates Vision-Language Models on 2D physics understanding, revealing a correlation between model scale and reasoning ability, particularly highlighting challenges in abstract spatial reasoning.",
            "ai_keywords": [
                "Vision-Language Models",
                "VLMs",
                "scenario generator",
                "Projectile Motion",
                "Collision Dynamics",
                "Mechanics",
                "Fluid Dynamics",
                "Qwen2.5-VL-7B",
                "formulaic problems",
                "abstract spatial reasoning"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-10T00:15:01.000Z",
        "title": "Interpretable Physics Reasoning and Performance Taxonomy in\n  Vision-Language Models",
        "summary": "As Vision-Language Models (VLMs) grow in sophistication, their ability to\nperform reasoning is coming under increasing supervision. While they excel at\nmany tasks, their grasp of fundamental scientific principles, such as physics,\nremains an underexplored frontier. To reflect the advancements in these\ncapabilities, we introduce a novel and accessible framework designed to\nrigorously evaluate VLMs on their understanding of 2D physics. Our framework\nfeatures a pragmatic scenario generator that creates a diverse testbed of over\n400 problems across four core domains: Projectile Motion, Collision Dynamics,\nMechanics, and Fluid Dynamics. Through comprehensive evaluation of four\nstate-of-the-art VLMs, we demonstrate a strong correlation between model scale\nand reasoning ability, with our top-performing model, Qwen2.5-VL-7B, achieving\nan overall score of 0.815. We find that while models excel at formulaic\nproblems, they struggle significantly with domains requiring abstract spatial\nreasoning. By designing this framework, we aim to democratize the study of\nscientific reasoning in VLMs and foster deeper insights into their capabilities\nand limitations.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.08270.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65d02414af9670557423df10",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/65d02414af9670557423df10/5SNGUGEEdkliGKNceo_gS.jpeg",
            "fullname": "Pranav Pawar",
            "name": "prnvpwr2612",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.04500",
            "authors": [
                {
                    "_id": "68c7862dee0eed1697d6b724",
                    "user": {
                        "_id": "67919aac2502cd4a62674eb7",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_z4YwWFM2ATXjt27dfwAW.png",
                        "isPro": false,
                        "fullname": "Rushi Wang",
                        "user": "Rushi2002",
                        "type": "user"
                    },
                    "name": "Rushi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:06:41.792Z",
                    "hidden": false
                },
                {
                    "_id": "68c7862dee0eed1697d6b725",
                    "name": "Jiateng Liu",
                    "hidden": false
                },
                {
                    "_id": "68c7862dee0eed1697d6b726",
                    "name": "Cheng Qian",
                    "hidden": false
                },
                {
                    "_id": "68c7862dee0eed1697d6b727",
                    "name": "Yifan Shen",
                    "hidden": false
                },
                {
                    "_id": "68c7862dee0eed1697d6b728",
                    "name": "Yanzhou Pan",
                    "hidden": false
                },
                {
                    "_id": "68c7862dee0eed1697d6b729",
                    "name": "Zhaozhuo Xu",
                    "hidden": false
                },
                {
                    "_id": "68c7862dee0eed1697d6b72a",
                    "name": "Ahmed Abbasi",
                    "hidden": false
                },
                {
                    "_id": "68c7862dee0eed1697d6b72b",
                    "name": "Heng Ji",
                    "hidden": false
                },
                {
                    "_id": "68c7862dee0eed1697d6b72c",
                    "name": "Denghui Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-02T00:40:34.000Z",
            "submittedOnDailyAt": "2025-09-15T17:48:30.414Z",
            "title": "Context Engineering for Trustworthiness: Rescorla Wagner Steering Under\n  Mixed and Inappropriate Contexts",
            "submittedOnDailyBy": {
                "_id": "67919aac2502cd4a62674eb7",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_z4YwWFM2ATXjt27dfwAW.png",
                "isPro": false,
                "fullname": "Rushi Wang",
                "user": "Rushi2002",
                "type": "user"
            },
            "summary": "Incorporating external context can significantly enhance the response quality\nof Large Language Models (LLMs). However, real-world contexts often mix\nrelevant information with disproportionate inappropriate content, posing\nreliability risks. How do LLMs process and prioritize mixed context? To study\nthis, we introduce the Poisoned Context Testbed, pairing queries with\nreal-world contexts containing relevant and inappropriate content. Inspired by\nassociative learning in animals, we adapt the Rescorla-Wagner (RW) model from\nneuroscience to quantify how competing contextual signals influence LLM\noutputs. Our adapted model reveals a consistent behavioral pattern: LLMs\nexhibit a strong tendency to incorporate information that is less prevalent in\nthe context. This susceptibility is harmful in real-world settings, where small\namounts of inappropriate content can substantially degrade response quality.\nEmpirical evaluations on our testbed further confirm this vulnerability. To\ntackle this, we introduce RW-Steering, a two-stage finetuning-based approach\nthat enables the model to internally identify and ignore inappropriate signals.\nUnlike prior methods that rely on extensive supervision across diverse context\nmixtures, RW-Steering generalizes robustly across varying proportions of\ninappropriate content. Experiments show that our best fine-tuned model improves\nresponse quality by 39.8% and reverses the undesirable behavior curve,\nestablishing RW-Steering as a robust, generalizable context engineering\nsolution for improving LLM safety in real-world use.",
            "upvotes": 0,
            "discussionId": "68c7862dee0eed1697d6b72d",
            "ai_summary": "LLMs process mixed contexts by prioritizing less prevalent information, which can degrade response quality; RW-Steering, a two-stage fine-tuning approach, improves LLM safety by identifying and ignoring inappropriate signals.",
            "ai_keywords": [
                "Large Language Models",
                "Poisoned Context Testbed",
                "Rescorla-Wagner model",
                "associative learning",
                "RW-Steering",
                "fine-tuning",
                "context engineering",
                "LLM safety"
            ]
        },
        "publishedAt": "2025-09-01T20:40:34.000Z",
        "title": "Context Engineering for Trustworthiness: Rescorla Wagner Steering Under\n  Mixed and Inappropriate Contexts",
        "summary": "Incorporating external context can significantly enhance the response quality\nof Large Language Models (LLMs). However, real-world contexts often mix\nrelevant information with disproportionate inappropriate content, posing\nreliability risks. How do LLMs process and prioritize mixed context? To study\nthis, we introduce the Poisoned Context Testbed, pairing queries with\nreal-world contexts containing relevant and inappropriate content. Inspired by\nassociative learning in animals, we adapt the Rescorla-Wagner (RW) model from\nneuroscience to quantify how competing contextual signals influence LLM\noutputs. Our adapted model reveals a consistent behavioral pattern: LLMs\nexhibit a strong tendency to incorporate information that is less prevalent in\nthe context. This susceptibility is harmful in real-world settings, where small\namounts of inappropriate content can substantially degrade response quality.\nEmpirical evaluations on our testbed further confirm this vulnerability. To\ntackle this, we introduce RW-Steering, a two-stage finetuning-based approach\nthat enables the model to internally identify and ignore inappropriate signals.\nUnlike prior methods that rely on extensive supervision across diverse context\nmixtures, RW-Steering generalizes robustly across varying proportions of\ninappropriate content. Experiments show that our best fine-tuned model improves\nresponse quality by 39.8% and reverses the undesirable behavior curve,\nestablishing RW-Steering as a robust, generalizable context engineering\nsolution for improving LLM safety in real-world use.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.04500.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67919aac2502cd4a62674eb7",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/_z4YwWFM2ATXjt27dfwAW.png",
            "fullname": "Rushi Wang",
            "name": "Rushi2002",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]