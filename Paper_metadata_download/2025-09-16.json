[
    {
        "paper": {
            "id": "2509.12201",
            "authors": [
                {
                    "_id": "68c8cece733e345e52ac1e82",
                    "name": "Yang Zhou",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e83",
                    "name": "Yifan Wang",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e84",
                    "user": {
                        "_id": "667e81565934c9fae29207ef",
                        "avatarUrl": "/avatars/431e777c71fccf7cf48ce013e5f6f1cb.svg",
                        "isPro": false,
                        "fullname": "Zhou",
                        "user": "ZhouTimeMachine",
                        "type": "user"
                    },
                    "name": "Jianjun Zhou",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:41:56.867Z",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e85",
                    "name": "Wenzheng Chang",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e86",
                    "name": "Haoyu Guo",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e87",
                    "user": {
                        "_id": "65e7eb86c7a0617cc71d3df4",
                        "avatarUrl": "/avatars/01020b6b5ccb08bf8aa10fd5f8b2701d.svg",
                        "isPro": false,
                        "fullname": "lizizun",
                        "user": "lizizun",
                        "type": "user"
                    },
                    "name": "Zizun Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:41:59.507Z",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e88",
                    "name": "Kaijing Ma",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e89",
                    "user": {
                        "_id": "66aba287b0f0b7411f511a47",
                        "avatarUrl": "/avatars/1450f182c38e80066ae5ea5df4fa218f.svg",
                        "isPro": false,
                        "fullname": "Xinyue Li",
                        "user": "Xxxy13",
                        "type": "user"
                    },
                    "name": "Xinyue Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:01.933Z",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e8a",
                    "name": "Yating Wang",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e8b",
                    "name": "Haoyi Zhu",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e8c",
                    "user": {
                        "_id": "652e25d2e647b0ee0a024f26",
                        "avatarUrl": "/avatars/b5c65cf6c8d0ddc9b8ef0226e0295d56.svg",
                        "isPro": false,
                        "fullname": "Mingyu Liu",
                        "user": "MingyuLiu",
                        "type": "user"
                    },
                    "name": "Mingyu Liu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:41:53.915Z",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e8d",
                    "name": "Dingning Liu",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e8e",
                    "name": "Jiange Yang",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e8f",
                    "name": "Zhoujie Fu",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e90",
                    "name": "Junyi Chen",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e91",
                    "name": "Chunhua Shen",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e92",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e93",
                    "name": "Kaipeng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c8cece733e345e52ac1e94",
                    "name": "Tong He",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T17:59:19.000Z",
            "submittedOnDailyAt": "2025-09-16T01:13:36.945Z",
            "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The field of 4D world modeling - aiming to jointly capture spatial geometry\nand temporal dynamics - has witnessed remarkable progress in recent years,\ndriven by advances in large-scale generative models and multimodal learning.\nHowever, the development of truly general 4D world models remains fundamentally\nconstrained by the availability of high-quality data. Existing datasets and\nbenchmarks often lack the dynamic complexity, multi-domain diversity, and\nspatial-temporal annotations required to support key tasks such as 4D geometric\nreconstruction, future prediction, and camera-control video generation. To\naddress this gap, we introduce OmniWorld, a large-scale, multi-domain,\nmulti-modal dataset specifically designed for 4D world modeling. OmniWorld\nconsists of a newly collected OmniWorld-Game dataset and several curated public\ndatasets spanning diverse domains. Compared with existing synthetic datasets,\nOmniWorld-Game provides richer modality coverage, larger scale, and more\nrealistic dynamic interactions. Based on this dataset, we establish a\nchallenging benchmark that exposes the limitations of current state-of-the-art\n(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning\nexisting SOTA methods on OmniWorld leads to significant performance gains\nacross 4D reconstruction and video generation tasks, strongly validating\nOmniWorld as a powerful resource for training and evaluation. We envision\nOmniWorld as a catalyst for accelerating the development of general-purpose 4D\nworld models, ultimately advancing machines' holistic understanding of the\nphysical world.",
            "upvotes": 73,
            "discussionId": "68c8cece733e345e52ac1e95",
            "projectPage": "https://yangzhou24.github.io/OmniWorld/",
            "githubRepo": "https://github.com/yangzhou24/OmniWorld",
            "ai_summary": "OmniWorld, a large-scale, multi-domain, multi-modal dataset, addresses the limitations of existing 4D world modeling datasets and benchmarks, enabling significant performance improvements in 4D reconstruction and video generation.",
            "ai_keywords": [
                "4D world modeling",
                "spatial geometry",
                "temporal dynamics",
                "large-scale generative models",
                "multimodal learning",
                "OmniWorld",
                "OmniWorld-Game",
                "4D geometric reconstruction",
                "future prediction",
                "camera-control video generation",
                "state-of-the-art (SOTA)"
            ],
            "githubStars": 175
        },
        "publishedAt": "2025-09-15T13:59:19.000Z",
        "title": "OmniWorld: A Multi-Domain and Multi-Modal Dataset for 4D World Modeling",
        "summary": "The field of 4D world modeling - aiming to jointly capture spatial geometry\nand temporal dynamics - has witnessed remarkable progress in recent years,\ndriven by advances in large-scale generative models and multimodal learning.\nHowever, the development of truly general 4D world models remains fundamentally\nconstrained by the availability of high-quality data. Existing datasets and\nbenchmarks often lack the dynamic complexity, multi-domain diversity, and\nspatial-temporal annotations required to support key tasks such as 4D geometric\nreconstruction, future prediction, and camera-control video generation. To\naddress this gap, we introduce OmniWorld, a large-scale, multi-domain,\nmulti-modal dataset specifically designed for 4D world modeling. OmniWorld\nconsists of a newly collected OmniWorld-Game dataset and several curated public\ndatasets spanning diverse domains. Compared with existing synthetic datasets,\nOmniWorld-Game provides richer modality coverage, larger scale, and more\nrealistic dynamic interactions. Based on this dataset, we establish a\nchallenging benchmark that exposes the limitations of current state-of-the-art\n(SOTA) approaches in modeling complex 4D environments. Moreover, fine-tuning\nexisting SOTA methods on OmniWorld leads to significant performance gains\nacross 4D reconstruction and video generation tasks, strongly validating\nOmniWorld as a powerful resource for training and evaluation. We envision\nOmniWorld as a catalyst for accelerating the development of general-purpose 4D\nworld models, ultimately advancing machines' holistic understanding of the\nphysical world.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12201.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.11543",
            "authors": [
                {
                    "_id": "68c8c9b3733e345e52ac1e66",
                    "user": {
                        "_id": "676127cf11b19ea602bb202a",
                        "avatarUrl": "/avatars/dfd802a24bd63e509728159ebb1769f6.svg",
                        "isPro": false,
                        "fullname": "Zhengxi Lu",
                        "user": "LZXzju",
                        "type": "user"
                    },
                    "name": "Zhengxi Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:11.100Z",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e67",
                    "user": {
                        "_id": "63cd1e04ff7cd335f0ddfa66",
                        "avatarUrl": "/avatars/8cca4ed96c699f53d4daabff0f6d6b56.svg",
                        "isPro": false,
                        "fullname": "Jiabo Ye",
                        "user": "Mizukiluke",
                        "type": "user"
                    },
                    "name": "Jiabo Ye",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:37.393Z",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e68",
                    "name": "Fei Tang",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e69",
                    "name": "Yongliang Shen",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e6a",
                    "user": {
                        "_id": "645b10e80c73ea27d13f7aca",
                        "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
                        "isPro": false,
                        "fullname": "xuhaiyang",
                        "user": "xhyandwyy",
                        "type": "user"
                    },
                    "name": "Haiyang Xu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:34.060Z",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e6b",
                    "name": "Ziwei Zheng",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e6c",
                    "name": "Weiming Lu",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e6d",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e6e",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e6f",
                    "name": "Jun Xiao",
                    "hidden": false
                },
                {
                    "_id": "68c8c9b3733e345e52ac1e70",
                    "name": "Yueting Zhuang",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/Im2ESDVPPQMlHP6L5owuo.png",
                "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/NchaoNZMIP1fSvmDtQ95e.png",
                "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/_A1UZfDw0U9K7cycraLFv.png"
            ],
            "publishedAt": "2025-09-15T03:24:08.000Z",
            "submittedOnDailyAt": "2025-09-16T00:55:54.867Z",
            "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "645b10e80c73ea27d13f7aca",
                "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
                "isPro": false,
                "fullname": "xuhaiyang",
                "user": "xhyandwyy",
                "type": "user"
            },
            "summary": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.",
            "upvotes": 35,
            "discussionId": "68c8c9b4733e345e52ac1e71",
            "githubRepo": "https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1",
            "ai_summary": "Semi-online Reinforcement Learning addresses the limitations of offline and online RL by simulating online RL on offline trajectories, achieving state-of-the-art performance in dynamic benchmarks.",
            "ai_keywords": [
                "reinforcement learning",
                "offline RL",
                "online RL",
                "multi-step task execution",
                "trajectory-level reward signals",
                "sparse rewards",
                "deployment costs",
                "Patch Module",
                "discounted future returns",
                "step-level advantages",
                "episode-level advantages",
                "Semi-Online Performance (SOP)",
                "dynamic benchmarks",
                "AndroidWorld",
                "AITW"
            ],
            "githubStars": 5644
        },
        "publishedAt": "2025-09-14T23:24:08.000Z",
        "title": "UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning",
        "summary": "Graphical User Interface (GUI) agents have demonstrated remarkable progress\nin automating complex user interface interactions through reinforcement\nlearning. However, current approaches face a fundamental dilemma: offline RL\nenables stable training on pre-collected trajectories, but struggles with\nmulti-step task execution for lack of trajectory-level reward signals; online\nRL captures these signals through environment interaction, but suffers from\nsparse rewards and prohibitive deployment costs. To address it, we present\nSemi-online Reinforcement Learning, a novel paradigm that simulates online RL\non offline trajectories. During each rollout process, we preserve the original\nmodel output within the multi-turn dialogue, where a Patch Module adaptively\nrecovers the divergence between rollout and expert trajectories. To capture\nlong-term training signals, Semi-online RL introduces discounted future returns\ninto the reward computation and optimizes the policy with weighted step-level\nand episode-level advantages. We further introduce Semi-Online Performance\n(SOP), a metric that aligns better with true online performance, serving as a\npractical and effective proxy for real-world evaluation. Experiments show that\nours Semi-online RL achieves SOTA performance among 7B models across four\ndynamic benchmarks, with significant gains over the base model (e.g., +12.0% on\nAndroidWorld, +23.8% on AITW), demonstrating significant progress in bridging\nthe gap between offline training efficiency and online multi-turn reasoning.\nThe code is available at https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/Im2ESDVPPQMlHP6L5owuo.png",
            "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/NchaoNZMIP1fSvmDtQ95e.png",
            "https://cdn-uploads.huggingface.co/production/uploads/645b10e80c73ea27d13f7aca/_A1UZfDw0U9K7cycraLFv.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11543.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "645b10e80c73ea27d13f7aca",
            "avatarUrl": "/avatars/95e565306472a15067440b5b43e07a6f.svg",
            "fullname": "xuhaiyang",
            "name": "xhyandwyy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 5
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.10813",
            "authors": [
                {
                    "_id": "68c8d095733e345e52ac1e97",
                    "name": "Weipeng Zhong",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1e98",
                    "name": "Peizhou Cao",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1e99",
                    "name": "Yichen Jin",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1e9a",
                    "name": "Li Luo",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1e9b",
                    "name": "Wenzhe Cai",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1e9c",
                    "name": "Jingli Lin",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1e9d",
                    "name": "Hanqing Wang",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1e9e",
                    "name": "Zhaoyang Lyu",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1e9f",
                    "name": "Tai Wang",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1ea0",
                    "name": "Bo Dai",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1ea1",
                    "name": "Xudong Xu",
                    "hidden": false
                },
                {
                    "_id": "68c8d095733e345e52ac1ea2",
                    "name": "Jiangmiao Pang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-13T14:25:17.000Z",
            "submittedOnDailyAt": "2025-09-16T01:21:15.577Z",
            "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with\n  Realistic Layouts",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D\nscene datasets characterized by scene diversity and realistic layouts. However,\nexisting datasets typically suffer from limitations in data scale or diversity,\nsanitized layouts lacking small items, and severe object collisions. To address\nthese shortcomings, we introduce InternScenes, a novel large-scale\nsimulatable indoor scene dataset comprising approximately 40,000 diverse scenes\nby integrating three disparate scene sources, real-world scans, procedurally\ngenerated scenes, and designer-created scenes, including 1.96M 3D objects and\ncovering 15 common scene types and 288 object classes. We particularly preserve\nmassive small items in the scenes, resulting in realistic and complex layouts\nwith an average of 41.5 objects per region. Our comprehensive data processing\npipeline ensures simulatability by creating real-to-sim replicas for real-world\nscans, enhances interactivity by incorporating interactive objects into these\nscenes, and resolves object collisions by physical simulations. We demonstrate\nthe value of InternScenes with two benchmark applications: scene layout\ngeneration and point-goal navigation. Both show the new challenges posed by the\ncomplex and realistic layouts. More importantly, InternScenes paves the way for\nscaling up the model training for both tasks, making the generation and\nnavigation in such complex scenes possible. We commit to open-sourcing the\ndata, models, and benchmarks to benefit the whole community.",
            "upvotes": 23,
            "discussionId": "68c8d095733e345e52ac1ea3",
            "projectPage": "https://marjordcpz.github.io/InternScenes.github.io/",
            "githubRepo": "https://github.com/InternRobotics/InternScenes",
            "ai_summary": "InternScenes is a large-scale, diverse, and realistic indoor scene dataset that addresses limitations in existing datasets, enabling better scene layout generation and point-goal navigation.",
            "ai_keywords": [
                "Embodied AI",
                "3D scene datasets",
                "scene diversity",
                "realistic layouts",
                "real-world scans",
                "procedurally generated scenes",
                "designer-created scenes",
                "3D objects",
                "scene types",
                "object classes",
                "small items",
                "real-to-sim replicas",
                "interactive objects",
                "physical simulations",
                "scene layout generation",
                "point-goal navigation"
            ],
            "githubStars": 123
        },
        "publishedAt": "2025-09-13T10:25:17.000Z",
        "title": "InternScenes: A Large-scale Simulatable Indoor Scene Dataset with\n  Realistic Layouts",
        "summary": "The advancement of Embodied AI heavily relies on large-scale, simulatable 3D\nscene datasets characterized by scene diversity and realistic layouts. However,\nexisting datasets typically suffer from limitations in data scale or diversity,\nsanitized layouts lacking small items, and severe object collisions. To address\nthese shortcomings, we introduce InternScenes, a novel large-scale\nsimulatable indoor scene dataset comprising approximately 40,000 diverse scenes\nby integrating three disparate scene sources, real-world scans, procedurally\ngenerated scenes, and designer-created scenes, including 1.96M 3D objects and\ncovering 15 common scene types and 288 object classes. We particularly preserve\nmassive small items in the scenes, resulting in realistic and complex layouts\nwith an average of 41.5 objects per region. Our comprehensive data processing\npipeline ensures simulatability by creating real-to-sim replicas for real-world\nscans, enhances interactivity by incorporating interactive objects into these\nscenes, and resolves object collisions by physical simulations. We demonstrate\nthe value of InternScenes with two benchmark applications: scene layout\ngeneration and point-goal navigation. Both show the new challenges posed by the\ncomplex and realistic layouts. More importantly, InternScenes paves the way for\nscaling up the model training for both tasks, making the generation and\nnavigation in such complex scenes possible. We commit to open-sourcing the\ndata, models, and benchmarks to benefit the whole community.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10813.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.12203",
            "authors": [
                {
                    "_id": "68c8cea2733e345e52ac1e79",
                    "name": "Zixin Yin",
                    "hidden": false
                },
                {
                    "_id": "68c8cea2733e345e52ac1e7a",
                    "name": "Xili Dai",
                    "hidden": false
                },
                {
                    "_id": "68c8cea2733e345e52ac1e7b",
                    "user": {
                        "_id": "64ae9b88a22a179fc4d07992",
                        "avatarUrl": "/avatars/c9065f04a1188ea3129e56a90328ffd3.svg",
                        "isPro": false,
                        "fullname": "wang",
                        "user": "dorni",
                        "type": "user"
                    },
                    "name": "Duomin Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:04.461Z",
                    "hidden": false
                },
                {
                    "_id": "68c8cea2733e345e52ac1e7c",
                    "name": "Xianfang Zeng",
                    "hidden": false
                },
                {
                    "_id": "68c8cea2733e345e52ac1e7d",
                    "name": "Lionel M. Ni",
                    "hidden": false
                },
                {
                    "_id": "68c8cea2733e345e52ac1e7e",
                    "name": "Gang Yu",
                    "hidden": false
                },
                {
                    "_id": "68c8cea2733e345e52ac1e7f",
                    "name": "Heung-Yeung Shum",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T17:59:47.000Z",
            "submittedOnDailyAt": "2025-09-16T01:13:01.794Z",
            "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion\n  Transformers via Explicit Correspondence",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The reliance on implicit point matching via attention has become a core\nbottleneck in drag-based editing, resulting in a fundamental compromise on\nweakened inversion strength and costly test-time optimization (TTO). This\ncompromise severely limits the generative capabilities of diffusion models,\nsuppressing high-fidelity inpainting and text-guided creation. In this paper,\nwe introduce LazyDrag, the first drag-based image editing method for\nMulti-Modal Diffusion Transformers, which directly eliminates the reliance on\nimplicit point matching. In concrete terms, our method generates an explicit\ncorrespondence map from user drag inputs as a reliable reference to boost the\nattention control. This reliable reference opens the potential for a stable\nfull-strength inversion process, which is the first in the drag-based editing\ntask. It obviates the necessity for TTO and unlocks the generative capability\nof models. Therefore, LazyDrag naturally unifies precise geometric control with\ntext guidance, enabling complex edits that were previously out of reach:\nopening the mouth of a dog and inpainting its interior, generating new objects\nlike a ``tennis ball'', or for ambiguous drags, making context-aware changes\nlike moving a hand into a pocket. Additionally, LazyDrag supports multi-round\nworkflows with simultaneous move and scale operations. Evaluated on the\nDragBench, our method outperforms baselines in drag accuracy and perceptual\nquality, as validated by VIEScore and human evaluation. LazyDrag not only\nestablishes new state-of-the-art performance, but also paves a new way to\nediting paradigms.",
            "upvotes": 10,
            "discussionId": "68c8cea2733e345e52ac1e80",
            "projectPage": "https://zxyin.github.io/LazyDrag",
            "ai_summary": "LazyDrag, a drag-based image editing method for Multi-Modal Diffusion Transformers, eliminates implicit point matching, enabling precise geometric control and text guidance without test-time optimization.",
            "ai_keywords": [
                "attention",
                "drag-based editing",
                "Multi-Modal Diffusion Transformers",
                "explicit correspondence map",
                "full-strength inversion",
                "test-time optimization",
                "high-fidelity inpainting",
                "text-guided creation",
                "DragBench",
                "VIEScore"
            ]
        },
        "publishedAt": "2025-09-15T13:59:47.000Z",
        "title": "LazyDrag: Enabling Stable Drag-Based Editing on Multi-Modal Diffusion\n  Transformers via Explicit Correspondence",
        "summary": "The reliance on implicit point matching via attention has become a core\nbottleneck in drag-based editing, resulting in a fundamental compromise on\nweakened inversion strength and costly test-time optimization (TTO). This\ncompromise severely limits the generative capabilities of diffusion models,\nsuppressing high-fidelity inpainting and text-guided creation. In this paper,\nwe introduce LazyDrag, the first drag-based image editing method for\nMulti-Modal Diffusion Transformers, which directly eliminates the reliance on\nimplicit point matching. In concrete terms, our method generates an explicit\ncorrespondence map from user drag inputs as a reliable reference to boost the\nattention control. This reliable reference opens the potential for a stable\nfull-strength inversion process, which is the first in the drag-based editing\ntask. It obviates the necessity for TTO and unlocks the generative capability\nof models. Therefore, LazyDrag naturally unifies precise geometric control with\ntext guidance, enabling complex edits that were previously out of reach:\nopening the mouth of a dog and inpainting its interior, generating new objects\nlike a ``tennis ball'', or for ambiguous drags, making context-aware changes\nlike moving a hand into a pocket. Additionally, LazyDrag supports multi-round\nworkflows with simultaneous move and scale operations. Evaluated on the\nDragBench, our method outperforms baselines in drag accuracy and perceptual\nquality, as validated by VIEScore and human evaluation. LazyDrag not only\nestablishes new state-of-the-art performance, but also paves a new way to\nediting paradigms.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12203.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.10708",
            "authors": [
                {
                    "_id": "68c8f911733e345e52ac1f20",
                    "user": {
                        "_id": "654a90ab55ecd2d37ac99965",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a90ab55ecd2d37ac99965/8lmXmiY0feZ9ORbkMrjJm.jpeg",
                        "isPro": false,
                        "fullname": "Iman Barati",
                        "user": "Iman998",
                        "type": "user"
                    },
                    "name": "Iman Barati",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:41:39.161Z",
                    "hidden": false
                },
                {
                    "_id": "68c8f911733e345e52ac1f21",
                    "name": "Mostafa Amiri",
                    "hidden": false
                },
                {
                    "_id": "68c8f911733e345e52ac1f22",
                    "name": "Heshaam Faili",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T21:50:39.000Z",
            "submittedOnDailyAt": "2025-09-16T11:14:39.321Z",
            "title": "SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based\n  Instruction Dataset Creation",
            "submittedOnDailyBy": {
                "_id": "654a90ab55ecd2d37ac99965",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a90ab55ecd2d37ac99965/8lmXmiY0feZ9ORbkMrjJm.jpeg",
                "isPro": false,
                "fullname": "Iman Barati",
                "user": "Iman998",
                "type": "user"
            },
            "summary": "Supervised Fine-Tuning (SFT) is essential for training large language models\n(LLMs), significantly enhancing critical capabilities such as instruction\nfollowing and in-context learning. Nevertheless, creating suitable training\ndatasets tailored for specific domains remains challenging due to unique domain\nconstraints and data scarcity. In this paper, we propose SearchInstruct, an\ninnovative method explicitly designed to construct high quality instruction\ndatasets for SFT. Our approach begins with a limited set of domain specific,\nhuman generated questions, which are systematically expanded using a large\nlanguage model. Subsequently, domain relevant resources are dynamically\nretrieved to generate accurate and contextually appropriate answers for each\naugmented question. Experimental evaluation demonstrates that SearchInstruct\nenhances both the diversity and quality of SFT datasets, leading to measurable\nimprovements in LLM performance within specialized domains. Additionally, we\nshow that beyond dataset generation, the proposed method can also effectively\nfacilitate tasks such as model editing, enabling efficient updates to existing\nmodels. To facilitate reproducibility and community adoption, we provide full\nimplementation details, the complete set of generated instruction response\npairs, and the source code in a publicly accessible Git repository:\n[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)",
            "upvotes": 9,
            "discussionId": "68c8f911733e345e52ac1f23",
            "githubRepo": "https://github.com/mostafaamiri/SearchInstruct",
            "ai_summary": "SearchInstruct enhances supervised fine-tuning datasets for large language models by expanding domain-specific questions and retrieving accurate answers, improving model performance and enabling efficient model editing.",
            "ai_keywords": [
                "supervised fine-tuning",
                "large language models",
                "instruction datasets",
                "domain-specific",
                "human-generated questions",
                "large language model",
                "domain relevant resources",
                "instruction response pairs",
                "model editing"
            ],
            "githubStars": 8
        },
        "publishedAt": "2025-09-12T17:50:39.000Z",
        "title": "SearchInstruct: Enhancing Domain Adaptation via Retrieval-Based\n  Instruction Dataset Creation",
        "summary": "Supervised Fine-Tuning (SFT) is essential for training large language models\n(LLMs), significantly enhancing critical capabilities such as instruction\nfollowing and in-context learning. Nevertheless, creating suitable training\ndatasets tailored for specific domains remains challenging due to unique domain\nconstraints and data scarcity. In this paper, we propose SearchInstruct, an\ninnovative method explicitly designed to construct high quality instruction\ndatasets for SFT. Our approach begins with a limited set of domain specific,\nhuman generated questions, which are systematically expanded using a large\nlanguage model. Subsequently, domain relevant resources are dynamically\nretrieved to generate accurate and contextually appropriate answers for each\naugmented question. Experimental evaluation demonstrates that SearchInstruct\nenhances both the diversity and quality of SFT datasets, leading to measurable\nimprovements in LLM performance within specialized domains. Additionally, we\nshow that beyond dataset generation, the proposed method can also effectively\nfacilitate tasks such as model editing, enabling efficient updates to existing\nmodels. To facilitate reproducibility and community adoption, we provide full\nimplementation details, the complete set of generated instruction response\npairs, and the source code in a publicly accessible Git repository:\n[https://github.com/mostafaamiri/SearchInstruct](https://github.com/mostafaamiri/SearchInstruct)",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10708.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "654a90ab55ecd2d37ac99965",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654a90ab55ecd2d37ac99965/8lmXmiY0feZ9ORbkMrjJm.jpeg",
            "fullname": "Iman Barati",
            "name": "Iman998",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.11452",
            "authors": [
                {
                    "_id": "68c8c651733e345e52ac1e5b",
                    "user": {
                        "_id": "642f742270daaa6e7209a2c8",
                        "avatarUrl": "/avatars/746fb840c220329f69a17905ea519322.svg",
                        "isPro": false,
                        "fullname": "Yining Lu",
                        "user": "ylu610",
                        "type": "user"
                    },
                    "name": "Yining Lu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:41.060Z",
                    "hidden": false
                },
                {
                    "_id": "68c8c651733e345e52ac1e5c",
                    "name": "Zilong Wang",
                    "hidden": false
                },
                {
                    "_id": "68c8c651733e345e52ac1e5d",
                    "name": "Shiyang Li",
                    "hidden": false
                },
                {
                    "_id": "68c8c651733e345e52ac1e5e",
                    "name": "Xin Liu",
                    "hidden": false
                },
                {
                    "_id": "68c8c651733e345e52ac1e5f",
                    "name": "Changlong Yu",
                    "hidden": false
                },
                {
                    "_id": "68c8c651733e345e52ac1e60",
                    "name": "Qingyu Yin",
                    "hidden": false
                },
                {
                    "_id": "68c8c651733e345e52ac1e61",
                    "name": "Zhan Shi",
                    "hidden": false
                },
                {
                    "_id": "68c8c651733e345e52ac1e62",
                    "name": "Zixuan Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c8c651733e345e52ac1e63",
                    "name": "Meng Jiang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-14T21:56:35.000Z",
            "submittedOnDailyAt": "2025-09-16T00:38:36.427Z",
            "title": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward\n  Weighting",
            "submittedOnDailyBy": {
                "_id": "642f742270daaa6e7209a2c8",
                "avatarUrl": "/avatars/746fb840c220329f69a17905ea519322.svg",
                "isPro": false,
                "fullname": "Yining Lu",
                "user": "ylu610",
                "type": "user"
            },
            "summary": "Prior works in multi-objective reinforcement learning typically use linear\nreward scalarization with fixed weights, which provably fail to capture\nnon-convex Pareto fronts and thus yield suboptimal results. This limitation\nbecomes especially critical in online preference alignment for large language\nmodels. Here, stochastic trajectories generated by parameterized policies\ncreate highly non-linear and non-convex mappings from parameters to objectives\nthat no single static weighting scheme can find optimal trade-offs. We address\nthis limitation by introducing dynamic reward weighting, which adaptively\nadjusts reward weights during the online reinforcement learning process. Unlike\nexisting approaches that rely on fixed-weight interpolation, our dynamic\nweighting continuously balances and prioritizes objectives in training,\nfacilitating effective exploration of Pareto fronts in objective space. We\nintroduce two approaches of increasing sophistication and generalizability: (1)\nhypervolume-guided weight adaptation and (2) gradient-based weight\noptimization, offering a versatile toolkit for online multi-objective\nalignment. Our extensive experiments demonstrate their compatibility with\ncommonly used online reinforcement learning algorithms (including GRPO,\nREINFORCE, and RLOO), effectiveness across multiple mathematical reasoning\ndatasets, and applicability to different model families, consistently achieving\nPareto dominant solutions with fewer training steps than fixed-weight linear\nscalarization baselines.",
            "upvotes": 8,
            "discussionId": "68c8c651733e345e52ac1e64",
            "projectPage": "https://yining610.github.io/dynamic-reward-weighting-webpage/",
            "githubRepo": "https://github.com/yining610/dynamic-reward-weighting",
            "ai_summary": "Dynamic reward weighting in multi-objective reinforcement learning adaptively adjusts weights during training to explore Pareto fronts effectively, outperforming fixed-weight scalarization methods.",
            "ai_keywords": [
                "multi-objective reinforcement learning",
                "linear reward scalarization",
                "non-convex Pareto fronts",
                "parameterized policies",
                "dynamic reward weighting",
                "hypervolume-guided weight adaptation",
                "gradient-based weight optimization",
                "GRPO",
                "REINFORCE",
                "RLOO",
                "Pareto dominant solutions"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-09-14T17:56:35.000Z",
        "title": "Learning to Optimize Multi-Objective Alignment Through Dynamic Reward\n  Weighting",
        "summary": "Prior works in multi-objective reinforcement learning typically use linear\nreward scalarization with fixed weights, which provably fail to capture\nnon-convex Pareto fronts and thus yield suboptimal results. This limitation\nbecomes especially critical in online preference alignment for large language\nmodels. Here, stochastic trajectories generated by parameterized policies\ncreate highly non-linear and non-convex mappings from parameters to objectives\nthat no single static weighting scheme can find optimal trade-offs. We address\nthis limitation by introducing dynamic reward weighting, which adaptively\nadjusts reward weights during the online reinforcement learning process. Unlike\nexisting approaches that rely on fixed-weight interpolation, our dynamic\nweighting continuously balances and prioritizes objectives in training,\nfacilitating effective exploration of Pareto fronts in objective space. We\nintroduce two approaches of increasing sophistication and generalizability: (1)\nhypervolume-guided weight adaptation and (2) gradient-based weight\noptimization, offering a versatile toolkit for online multi-objective\nalignment. Our extensive experiments demonstrate their compatibility with\ncommonly used online reinforcement learning algorithms (including GRPO,\nREINFORCE, and RLOO), effectiveness across multiple mathematical reasoning\ndatasets, and applicability to different model families, consistently achieving\nPareto dominant solutions with fewer training steps than fixed-weight linear\nscalarization baselines.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11452.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "642f742270daaa6e7209a2c8",
            "avatarUrl": "/avatars/746fb840c220329f69a17905ea519322.svg",
            "fullname": "Yining Lu",
            "name": "ylu610",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09672",
            "authors": [
                {
                    "_id": "68c8d0fd733e345e52ac1ea5",
                    "name": "Artem Lukoianov",
                    "hidden": false
                },
                {
                    "_id": "68c8d0fd733e345e52ac1ea6",
                    "name": "Chenyang Yuan",
                    "hidden": false
                },
                {
                    "_id": "68c8d0fd733e345e52ac1ea7",
                    "name": "Justin Solomon",
                    "hidden": false
                },
                {
                    "_id": "68c8d0fd733e345e52ac1ea8",
                    "name": "Vincent Sitzmann",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/63d9528c6b496a404a437507/qYvBgptghbW5et_mmWEDV.png"
            ],
            "publishedAt": "2025-09-11T17:59:08.000Z",
            "submittedOnDailyAt": "2025-09-16T02:23:57.542Z",
            "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
            "submittedOnDailyBy": {
                "_id": "63d9528c6b496a404a437507",
                "avatarUrl": "/avatars/71eebe88297c83020e3446489edfd06b.svg",
                "isPro": false,
                "fullname": "Artem Lukoianov",
                "user": "ottogin",
                "type": "user"
            },
            "summary": "Among generative models, diffusion models are uniquely intriguing due to the\nexistence of a closed-form optimal minimizer of their training objective, often\nreferred to as the optimal denoiser. However, diffusion using this optimal\ndenoiser merely reproduces images in the training set and hence fails to\ncapture the behavior of deep diffusion models. Recent work has attempted to\ncharacterize this gap between the optimal denoiser and deep diffusion models,\nproposing analytical, training-free models that can generate images that\nresemble those generated by a trained UNet. The best-performing method\nhypothesizes that shift equivariance and locality inductive biases of\nconvolutional neural networks are the cause of the performance gap, hence\nincorporating these assumptions into its analytical model. In this work, we\npresent evidence that the locality in deep diffusion models emerges as a\nstatistical property of the image dataset, not due to the inductive bias of\nconvolutional neural networks. Specifically, we demonstrate that an optimal\nparametric linear denoiser exhibits similar locality properties to the deep\nneural denoisers. We further show, both theoretically and experimentally, that\nthis locality arises directly from the pixel correlations present in natural\nimage datasets. Finally, we use these insights to craft an analytical denoiser\nthat better matches scores predicted by a deep diffusion model than the prior\nexpert-crafted alternative.",
            "upvotes": 8,
            "discussionId": "68c8d0fd733e345e52ac1ea9",
            "projectPage": "https://locality.lukoianov.com/",
            "githubRepo": "https://github.com/ottogin/locality-in-diffusion-models",
            "ai_summary": "Research shows that locality in deep diffusion models is a statistical property of image datasets rather than an inductive bias of convolutional neural networks, leading to the development of a more accurate analytical denoiser.",
            "ai_keywords": [
                "diffusion models",
                "optimal denoiser",
                "UNet",
                "shift equivariance",
                "locality inductive biases",
                "convolutional neural networks",
                "parametric linear denoiser",
                "pixel correlations",
                "natural image datasets",
                "analytical denoiser"
            ],
            "githubStars": 13
        },
        "publishedAt": "2025-09-11T13:59:08.000Z",
        "title": "Locality in Image Diffusion Models Emerges from Data Statistics",
        "summary": "Among generative models, diffusion models are uniquely intriguing due to the\nexistence of a closed-form optimal minimizer of their training objective, often\nreferred to as the optimal denoiser. However, diffusion using this optimal\ndenoiser merely reproduces images in the training set and hence fails to\ncapture the behavior of deep diffusion models. Recent work has attempted to\ncharacterize this gap between the optimal denoiser and deep diffusion models,\nproposing analytical, training-free models that can generate images that\nresemble those generated by a trained UNet. The best-performing method\nhypothesizes that shift equivariance and locality inductive biases of\nconvolutional neural networks are the cause of the performance gap, hence\nincorporating these assumptions into its analytical model. In this work, we\npresent evidence that the locality in deep diffusion models emerges as a\nstatistical property of the image dataset, not due to the inductive bias of\nconvolutional neural networks. Specifically, we demonstrate that an optimal\nparametric linear denoiser exhibits similar locality properties to the deep\nneural denoisers. We further show, both theoretically and experimentally, that\nthis locality arises directly from the pixel correlations present in natural\nimage datasets. Finally, we use these insights to craft an analytical denoiser\nthat better matches scores predicted by a deep diffusion model than the prior\nexpert-crafted alternative.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/63d9528c6b496a404a437507/qYvBgptghbW5et_mmWEDV.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09672.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63d9528c6b496a404a437507",
            "avatarUrl": "/avatars/71eebe88297c83020e3446489edfd06b.svg",
            "fullname": "Artem Lukoianov",
            "name": "ottogin",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.11986",
            "authors": [
                {
                    "_id": "68c8d7dd733e345e52ac1edd",
                    "user": {
                        "_id": "619b506f70d03780cbec5806",
                        "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
                        "isPro": false,
                        "fullname": "wenyan li",
                        "user": "lyan62",
                        "type": "user"
                    },
                    "name": "Wenyan Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:41:46.176Z",
                    "hidden": false
                },
                {
                    "_id": "68c8d7dd733e345e52ac1ede",
                    "name": "Raphael Tang",
                    "hidden": false
                },
                {
                    "_id": "68c8d7dd733e345e52ac1edf",
                    "name": "Chengzu Li",
                    "hidden": false
                },
                {
                    "_id": "68c8d7dd733e345e52ac1ee0",
                    "name": "Caiqi Zhang",
                    "hidden": false
                },
                {
                    "_id": "68c8d7dd733e345e52ac1ee1",
                    "name": "Ivan Vuli",
                    "hidden": false
                },
                {
                    "_id": "68c8d7dd733e345e52ac1ee2",
                    "name": "Anders Sgaard",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T14:38:06.000Z",
            "submittedOnDailyAt": "2025-09-16T01:52:58.573Z",
            "title": "Lost in Embeddings: Information Loss in Vision-Language Models",
            "submittedOnDailyBy": {
                "_id": "619b506f70d03780cbec5806",
                "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
                "isPro": false,
                "fullname": "wenyan li",
                "user": "lyan62",
                "type": "user"
            },
            "summary": "Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle.",
            "upvotes": 7,
            "discussionId": "68c8d7dd733e345e52ac1ee3",
            "ai_summary": "Two approaches are introduced to analyze and quantify information loss in vision-language models during the projection of visual inputs into the language model's embedding space, revealing significant distortions and their impact on model performance.",
            "ai_keywords": [
                "vision--language models",
                "pretrained vision encoder",
                "connector component",
                "modality fusion",
                "latent representation space",
                "semantic information preservation",
                "k-nearest neighbor relationships",
                "visual embeddings",
                "patch-level embedding reconstruction",
                "visually grounded question-answering tasks"
            ]
        },
        "publishedAt": "2025-09-15T10:38:06.000Z",
        "title": "Lost in Embeddings: Information Loss in Vision-Language Models",
        "summary": "Vision--language models (VLMs) often process visual inputs through a\npretrained vision encoder, followed by a projection into the language model's\nembedding space via a connector component. While crucial for modality fusion,\nthe potential information loss induced by this projection step and its direct\nimpact on model capabilities remain understudied. We introduce two\ncomplementary approaches to examine and quantify this loss by analyzing the\nlatent representation space. First, we evaluate semantic information\npreservation by analyzing changes in k-nearest neighbor relationships between\nimage representations, before and after projection. Second, we directly measure\ninformation loss by reconstructing visual embeddings from the projected\nrepresentation, localizing loss at an image patch level. Experiments reveal\nthat connectors substantially distort the local geometry of visual\nrepresentations, with k-nearest neighbors diverging by 40--60\\%\npost-projection, correlating with degradation in retrieval performance. The\npatch-level embedding reconstruction provides interpretable insights for model\nbehavior on visually grounded question-answering tasks, finding that areas of\nhigh information loss reliably predict instances where models struggle.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11986.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "619b506f70d03780cbec5806",
            "avatarUrl": "/avatars/ea2b0b8f0a3eb16d53ef40da9981c397.svg",
            "fullname": "wenyan li",
            "name": "lyan62",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 2
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.09658",
            "authors": [
                {
                    "_id": "68c8ccf3733e345e52ac1e73",
                    "name": "Bingkui Tong",
                    "hidden": false
                },
                {
                    "_id": "68c8ccf3733e345e52ac1e74",
                    "user": {
                        "_id": "654c865e17d83697c75f3f90",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/654c865e17d83697c75f3f90/JW79nXWaCKmw2HmELshEw.png",
                        "isPro": false,
                        "fullname": "JiaerXia",
                        "user": "JiaerX",
                        "type": "user"
                    },
                    "name": "Jiaer Xia",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:07.209Z",
                    "hidden": false
                },
                {
                    "_id": "68c8ccf3733e345e52ac1e75",
                    "name": "Sifeng Shang",
                    "hidden": false
                },
                {
                    "_id": "68c8ccf3733e345e52ac1e76",
                    "name": "Kaiyang Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-11T17:54:00.000Z",
            "submittedOnDailyAt": "2025-09-16T01:07:13.664Z",
            "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
            "submittedOnDailyBy": {
                "_id": "62ac6656de8bfbb93094b8fd",
                "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
                "isPro": false,
                "fullname": "Kaiyang Zhou",
                "user": "kaiyangzhou",
                "type": "user"
            },
            "summary": "Hallucinations in multimodal large language models (MLLMs) -- where the model\ngenerates content inconsistent with the input image -- pose significant risks\nin real-world applications, from misinformation in visual question answering to\nunsafe errors in decision-making. Existing benchmarks primarily test\nrecognition accuracy, i.e., evaluating whether models can select the correct\nanswer among distractors. This overlooks an equally critical capability for\ntrustworthy AI: recognizing when none of the provided options are correct, a\nbehavior reflecting epistemic humility. We present HumbleBench, a new\nhallucination benchmark designed to evaluate MLLMs' ability to reject plausible\nbut incorrect answers across three hallucination types: object, relation, and\nattribute. Built from a panoptic scene graph dataset, we leverage fine-grained\nscene graph annotations to extract ground-truth entities and relations, and\nprompt GPT-4-Turbo to generate multiple-choice questions, followed by a\nrigorous manual filtering process. Each question includes a \"None of the above\"\noption, requiring models not only to recognize correct visual information but\nalso to identify when no provided answer is valid. We evaluate a variety of\nstate-of-the-art MLLMs -- including both general-purpose and specialized\nreasoning models -- on HumbleBench and share valuable findings and insights\nwith the community. By incorporating explicit false-option rejection,\nHumbleBench fills a key gap in current evaluation suites, providing a more\nrealistic measure of MLLM reliability in safety-critical settings. Our code and\ndataset are released publicly and can be accessed at\nhttps://github.com/maifoundations/HumbleBench.",
            "upvotes": 5,
            "discussionId": "68c8ccf4733e345e52ac1e77",
            "githubRepo": "https://github.com/maifoundations/HumbleBench",
            "ai_summary": "HumbleBench evaluates multimodal large language models' ability to reject incorrect answers, addressing the issue of hallucinations in visual question answering and decision-making.",
            "ai_keywords": [
                "multimodal large language models",
                "hallucinations",
                "visual question answering",
                "epistemic humility",
                "panoptic scene graph dataset",
                "fine-grained scene graph annotations",
                "multiple-choice questions",
                "false-option rejection"
            ],
            "githubStars": 7
        },
        "publishedAt": "2025-09-11T13:54:00.000Z",
        "title": "Measuring Epistemic Humility in Multimodal Large Language Models",
        "summary": "Hallucinations in multimodal large language models (MLLMs) -- where the model\ngenerates content inconsistent with the input image -- pose significant risks\nin real-world applications, from misinformation in visual question answering to\nunsafe errors in decision-making. Existing benchmarks primarily test\nrecognition accuracy, i.e., evaluating whether models can select the correct\nanswer among distractors. This overlooks an equally critical capability for\ntrustworthy AI: recognizing when none of the provided options are correct, a\nbehavior reflecting epistemic humility. We present HumbleBench, a new\nhallucination benchmark designed to evaluate MLLMs' ability to reject plausible\nbut incorrect answers across three hallucination types: object, relation, and\nattribute. Built from a panoptic scene graph dataset, we leverage fine-grained\nscene graph annotations to extract ground-truth entities and relations, and\nprompt GPT-4-Turbo to generate multiple-choice questions, followed by a\nrigorous manual filtering process. Each question includes a \"None of the above\"\noption, requiring models not only to recognize correct visual information but\nalso to identify when no provided answer is valid. We evaluate a variety of\nstate-of-the-art MLLMs -- including both general-purpose and specialized\nreasoning models -- on HumbleBench and share valuable findings and insights\nwith the community. By incorporating explicit false-option rejection,\nHumbleBench fills a key gap in current evaluation suites, providing a more\nrealistic measure of MLLM reliability in safety-critical settings. Our code and\ndataset are released publicly and can be accessed at\nhttps://github.com/maifoundations/HumbleBench.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.09658.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "62ac6656de8bfbb93094b8fd",
            "avatarUrl": "/avatars/1d8f445b6d5f9d43ddab81056bcb141e.svg",
            "fullname": "Kaiyang Zhou",
            "name": "kaiyangzhou",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.10884",
            "authors": [
                {
                    "_id": "68c8d3b6733e345e52ac1ecb",
                    "name": "Qingxiang Liu",
                    "hidden": false
                },
                {
                    "_id": "68c8d3b6733e345e52ac1ecc",
                    "name": "Ting Huang",
                    "hidden": false
                },
                {
                    "_id": "68c8d3b6733e345e52ac1ecd",
                    "user": {
                        "_id": "64ec877bb93654d4ca5c92e9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/64ec877bb93654d4ca5c92e9/GvHk_KSdE9Rhnk_o-NaZX.jpeg",
                        "isPro": true,
                        "fullname": "Zeyu Zhang",
                        "user": "SteveZeyuZhang",
                        "type": "user"
                    },
                    "name": "Zeyu Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:41:48.984Z",
                    "hidden": false
                },
                {
                    "_id": "68c8d3b6733e345e52ac1ece",
                    "name": "Hao Tang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-13T16:31:03.000Z",
            "submittedOnDailyAt": "2025-09-16T01:34:35.833Z",
            "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Embodied navigation requires agents to integrate perception, reasoning, and\naction for robust interaction in complex 3D environments. Existing approaches\noften suffer from incoherent and unstable reasoning traces that hinder\ngeneralization across diverse environments, and difficulty balancing\nlong-horizon semantic reasoning with low-latency control for real-time\nnavigation. To address these challenges, we propose Nav-R1, an embodied\nfoundation model that unifies reasoning in embodied environments. We first\nconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought\n(CoT) for embodied tasks, which enables cold-start initialization with\nstructured reasoning. Building on this foundation, we design a GRPO-based\nreinforcement learning framework with three complementary rewards: format,\nunderstanding, and navigation, to improve structural adherence, semantic\ngrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow\nreasoning paradigm, decoupling deliberate semantic reasoning from low-latency\nreactive control for efficient yet coherent navigation. Extensive evaluations\non embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms\nstrong baselines, with over 8% average improvement in reasoning and navigation\nperformance. Real-world deployment on a mobile robot further validates its\nrobustness under limited onboard resources. Code:\nhttps://github.com/AIGeeksGroup/Nav-R1. Website:\nhttps://aigeeksgroup.github.io/Nav-R1.",
            "upvotes": 4,
            "discussionId": "68c8d3b6733e345e52ac1ecf",
            "projectPage": "https://aigeeksgroup.github.io/Nav-R1/",
            "githubRepo": "https://github.com/AIGeeksGroup/Nav-R1",
            "ai_summary": "Nav-R1, an embodied foundation model, enhances navigation by integrating structured reasoning and decoupled control mechanisms, outperforming existing approaches on benchmarks and in real-world scenarios.",
            "ai_keywords": [
                "Embodied navigation",
                "Chains-of-Thought (CoT)",
                "GRPO-based reinforcement learning",
                "Fast-in-Slow reasoning",
                "structural adherence",
                "semantic grounding",
                "path fidelity",
                "embodied AI benchmarks",
                "mobile robot deployment"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-09-13T12:31:03.000Z",
        "title": "Nav-R1: Reasoning and Navigation in Embodied Scenes",
        "summary": "Embodied navigation requires agents to integrate perception, reasoning, and\naction for robust interaction in complex 3D environments. Existing approaches\noften suffer from incoherent and unstable reasoning traces that hinder\ngeneralization across diverse environments, and difficulty balancing\nlong-horizon semantic reasoning with low-latency control for real-time\nnavigation. To address these challenges, we propose Nav-R1, an embodied\nfoundation model that unifies reasoning in embodied environments. We first\nconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought\n(CoT) for embodied tasks, which enables cold-start initialization with\nstructured reasoning. Building on this foundation, we design a GRPO-based\nreinforcement learning framework with three complementary rewards: format,\nunderstanding, and navigation, to improve structural adherence, semantic\ngrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slow\nreasoning paradigm, decoupling deliberate semantic reasoning from low-latency\nreactive control for efficient yet coherent navigation. Extensive evaluations\non embodied AI benchmarks demonstrate that Nav-R1 consistently outperforms\nstrong baselines, with over 8% average improvement in reasoning and navigation\nperformance. Real-world deployment on a mobile robot further validates its\nrobustness under limited onboard resources. Code:\nhttps://github.com/AIGeeksGroup/Nav-R1. Website:\nhttps://aigeeksgroup.github.io/Nav-R1.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10884.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.12132",
            "authors": [
                {
                    "_id": "68c9211f733e345e52ac1f5d",
                    "name": "Pu Jian",
                    "hidden": false
                },
                {
                    "_id": "68c9211f733e345e52ac1f5e",
                    "name": "Junhong Wu",
                    "hidden": false
                },
                {
                    "_id": "68c9211f733e345e52ac1f5f",
                    "name": "Wei Sun",
                    "hidden": false
                },
                {
                    "_id": "68c9211f733e345e52ac1f60",
                    "name": "Chen Wang",
                    "hidden": false
                },
                {
                    "_id": "68c9211f733e345e52ac1f61",
                    "name": "Shuo Ren",
                    "hidden": false
                },
                {
                    "_id": "68c9211f733e345e52ac1f62",
                    "name": "Jiajun Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T16:57:25.000Z",
            "submittedOnDailyAt": "2025-09-16T07:07:10.819Z",
            "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language\n  Models",
            "submittedOnDailyBy": {
                "_id": "60d75902dd069f8db73149b4",
                "avatarUrl": "/avatars/dd6ed8b1c51998f1090f10aaf0088111.svg",
                "isPro": false,
                "fullname": "Junhong Wu",
                "user": "Macro",
                "type": "user"
            },
            "summary": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (VRMs). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires visual\nreflection, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM Reflection-V, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, Reflection-V demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, Reflection-V maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities.",
            "upvotes": 3,
            "discussionId": "68c92120733e345e52ac1f63",
            "ai_summary": "Reflection-V enhances visual reasoning by constructing vision-centered data and using a visual attention reward model, improving reliance on visual information.",
            "ai_keywords": [
                "visual reflection",
                "VRMs",
                "reasoning data construction",
                "cold-start",
                "reinforcement learning",
                "reward design",
                "visual attention",
                "visual reasoning benchmarks"
            ]
        },
        "publishedAt": "2025-09-15T12:57:25.000Z",
        "title": "Look Again, Think Slowly: Enhancing Visual Reflection in Vision-Language\n  Models",
        "summary": "Recent advances in text-only \"slow-thinking\" reasoning have prompted efforts\nto transfer this capability to vision-language models (VLMs), for training\nvisual reasoning models (VRMs). owever, such transfer faces critical\nchallenges: Effective \"slow thinking\" in VRMs requires visual\nreflection, the ability to check the reasoning process based on visual\ninformation. Through quantitative analysis, we observe that current VRMs\nexhibit limited visual reflection, as their attention to visual information\ndiminishes rapidly with longer generated responses. To address this challenge,\nwe propose a new VRM Reflection-V, which enhances visual reflection\nbased on reasoning data construction for cold-start and reward design for\nreinforcement learning (RL). Firstly, we construct vision-centered reasoning\ndata by leveraging an agent that interacts between VLMs and reasoning LLMs,\nenabling cold-start learning of visual reflection patterns. Secondly, a visual\nattention based reward model is employed during RL to encourage reasoning based\non visual information. Therefore, Reflection-V demonstrates\nsignificant improvements across multiple visual reasoning benchmarks.\nFurthermore, Reflection-V maintains a stronger and more consistent\nreliance on visual information during visual reasoning, indicating effective\nenhancement in visual reflection capabilities.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12132.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "60d75902dd069f8db73149b4",
            "avatarUrl": "/avatars/dd6ed8b1c51998f1090f10aaf0088111.svg",
            "fullname": "Junhong Wu",
            "name": "Macro",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.11444",
            "authors": [
                {
                    "_id": "68c8bef9733e345e52ac1e38",
                    "user": {
                        "_id": "685204843761e5afb796bc57",
                        "avatarUrl": "/avatars/1224f12ba48912dd59f22a7a6af683bc.svg",
                        "isPro": false,
                        "fullname": "Gaurab Chhetri",
                        "user": "gauravfs-14",
                        "type": "user"
                    },
                    "name": "Gaurab Chhetri",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:53.342Z",
                    "hidden": false
                },
                {
                    "_id": "68c8bef9733e345e52ac1e39",
                    "name": "Anandi Dutta",
                    "hidden": false
                },
                {
                    "_id": "68c8bef9733e345e52ac1e3a",
                    "name": "Subasish Das",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/685204843761e5afb796bc57/4aU7jSLmMB3daVepAE8Fv.png"
            ],
            "publishedAt": "2025-09-14T21:37:24.000Z",
            "submittedOnDailyAt": "2025-09-16T00:35:07.371Z",
            "title": "CognitiveSky: Scalable Sentiment and Narrative Analysis for\n  Decentralized Social Media",
            "submittedOnDailyBy": {
                "_id": "685204843761e5afb796bc57",
                "avatarUrl": "/avatars/1224f12ba48912dd59f22a7a6af683bc.svg",
                "isPro": false,
                "fullname": "Gaurab Chhetri",
                "user": "gauravfs-14",
                "type": "user"
            },
            "summary": "The emergence of decentralized social media platforms presents new\nopportunities and challenges for real-time analysis of public discourse. This\nstudy introduces CognitiveSky, an open-source and scalable framework designed\nfor sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter\nor X.com alternative. By ingesting data through Bluesky's Application\nProgramming Interface (API), CognitiveSky applies transformer-based models to\nannotate large-scale user-generated content and produces structured and\nanalyzable outputs. These summaries drive a dynamic dashboard that visualizes\nevolving patterns in emotion, activity, and conversation topics. Built entirely\non free-tier infrastructure, CognitiveSky achieves both low operational cost\nand high accessibility. While demonstrated here for monitoring mental health\ndiscourse, its modular design enables applications across domains such as\ndisinformation detection, crisis response, and civic sentiment analysis. By\nbridging large language models with decentralized networks, CognitiveSky offers\na transparent, extensible tool for computational social science in an era of\nshifting digital ecosystems.",
            "upvotes": 3,
            "discussionId": "68c8bef9733e345e52ac1e3b",
            "projectPage": "https://www.gaurabchhetri.com.np/projects/cognitivesky",
            "githubRepo": "https://github.com/gauravfs-14/CognitiveSky",
            "ai_summary": "CognitiveSky, a transformer-based framework, analyzes sentiment, emotion, and narratives on Bluesky, providing insights through a dynamic dashboard and supporting various applications in computational social science.",
            "ai_keywords": [
                "transformer-based models",
                "sentiment analysis",
                "emotion analysis",
                "narrative analysis",
                "Bluesky",
                "Application Programming Interface (API)",
                "dynamic dashboard",
                "mental health discourse",
                "disinformation detection",
                "crisis response",
                "civic sentiment analysis",
                "large language models",
                "decentralized networks"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-09-14T17:37:24.000Z",
        "title": "CognitiveSky: Scalable Sentiment and Narrative Analysis for\n  Decentralized Social Media",
        "summary": "The emergence of decentralized social media platforms presents new\nopportunities and challenges for real-time analysis of public discourse. This\nstudy introduces CognitiveSky, an open-source and scalable framework designed\nfor sentiment, emotion, and narrative analysis on Bluesky, a federated Twitter\nor X.com alternative. By ingesting data through Bluesky's Application\nProgramming Interface (API), CognitiveSky applies transformer-based models to\nannotate large-scale user-generated content and produces structured and\nanalyzable outputs. These summaries drive a dynamic dashboard that visualizes\nevolving patterns in emotion, activity, and conversation topics. Built entirely\non free-tier infrastructure, CognitiveSky achieves both low operational cost\nand high accessibility. While demonstrated here for monitoring mental health\ndiscourse, its modular design enables applications across domains such as\ndisinformation detection, crisis response, and civic sentiment analysis. By\nbridging large language models with decentralized networks, CognitiveSky offers\na transparent, extensible tool for computational social science in an era of\nshifting digital ecosystems.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/685204843761e5afb796bc57/4aU7jSLmMB3daVepAE8Fv.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11444.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "685204843761e5afb796bc57",
            "avatarUrl": "/avatars/1224f12ba48912dd59f22a7a6af683bc.svg",
            "fullname": "Gaurab Chhetri",
            "name": "gauravfs-14",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.11362",
            "authors": [
                {
                    "_id": "68c8d175733e345e52ac1eb1",
                    "name": "Loka Li",
                    "hidden": false
                },
                {
                    "_id": "68c8d175733e345e52ac1eb2",
                    "name": "Wong Yu Kang",
                    "hidden": false
                },
                {
                    "_id": "68c8d175733e345e52ac1eb3",
                    "name": "Minghao Fu",
                    "hidden": false
                },
                {
                    "_id": "68c8d175733e345e52ac1eb4",
                    "name": "Guangyi Chen",
                    "hidden": false
                },
                {
                    "_id": "68c8d175733e345e52ac1eb5",
                    "name": "Zhenhao Chen",
                    "hidden": false
                },
                {
                    "_id": "68c8d175733e345e52ac1eb6",
                    "name": "Gongxu Luo",
                    "hidden": false
                },
                {
                    "_id": "68c8d175733e345e52ac1eb7",
                    "name": "Yuewen Sun",
                    "hidden": false
                },
                {
                    "_id": "68c8d175733e345e52ac1eb8",
                    "name": "Salman Khan",
                    "hidden": false
                },
                {
                    "_id": "68c8d175733e345e52ac1eb9",
                    "name": "Peter Spirtes",
                    "hidden": false
                },
                {
                    "_id": "68c8d175733e345e52ac1eba",
                    "name": "Kun Zhang",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-14T17:30:03.000Z",
            "submittedOnDailyAt": "2025-09-16T01:24:53.457Z",
            "title": "PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Understanding human behavior traits is central to applications in\nhuman-computer interaction, computational social science, and personalized AI\nsystems. Such understanding often requires integrating multiple modalities to\ncapture nuanced patterns and relationships. However, existing resources rarely\nprovide datasets that combine behavioral descriptors with complementary\nmodalities such as facial attributes and biographical information. To address\nthis gap, we present PersonaX, a curated collection of multimodal datasets\ndesigned to enable comprehensive analysis of public traits across modalities.\nPersonaX consists of (1) CelebPersona, featuring 9444 public figures from\ndiverse occupations, and (2) AthlePersona, covering 4181 professional athletes\nacross 7 major sports leagues. Each dataset includes behavioral trait\nassessments inferred by three high-performing large language models, alongside\nfacial imagery and structured biographical features. We analyze PersonaX at two\ncomplementary levels. First, we abstract high-level trait scores from text\ndescriptions and apply five statistical independence tests to examine their\nrelationships with other modalities. Second, we introduce a novel causal\nrepresentation learning (CRL) framework tailored to multimodal and\nmulti-measurement data, providing theoretical identifiability guarantees.\nExperiments on both synthetic and real-world data demonstrate the effectiveness\nof our approach. By unifying structured and unstructured analysis, PersonaX\nestablishes a foundation for studying LLM-inferred behavioral traits in\nconjunction with visual and biographical attributes, advancing multimodal trait\nanalysis and causal reasoning.",
            "upvotes": 2,
            "discussionId": "68c8d175733e345e52ac1ebb",
            "ai_summary": "PersonaX, a multimodal dataset, combines behavioral traits, facial imagery, and biographical information to enable comprehensive analysis and causal reasoning using large language models.",
            "ai_keywords": [
                "large language models",
                "causal representation learning",
                "multimodal datasets",
                "behavioral traits",
                "facial imagery",
                "biographical information",
                "statistical independence tests",
                "theoretical identifiability guarantees"
            ]
        },
        "publishedAt": "2025-09-14T13:30:03.000Z",
        "title": "PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits",
        "summary": "Understanding human behavior traits is central to applications in\nhuman-computer interaction, computational social science, and personalized AI\nsystems. Such understanding often requires integrating multiple modalities to\ncapture nuanced patterns and relationships. However, existing resources rarely\nprovide datasets that combine behavioral descriptors with complementary\nmodalities such as facial attributes and biographical information. To address\nthis gap, we present PersonaX, a curated collection of multimodal datasets\ndesigned to enable comprehensive analysis of public traits across modalities.\nPersonaX consists of (1) CelebPersona, featuring 9444 public figures from\ndiverse occupations, and (2) AthlePersona, covering 4181 professional athletes\nacross 7 major sports leagues. Each dataset includes behavioral trait\nassessments inferred by three high-performing large language models, alongside\nfacial imagery and structured biographical features. We analyze PersonaX at two\ncomplementary levels. First, we abstract high-level trait scores from text\ndescriptions and apply five statistical independence tests to examine their\nrelationships with other modalities. Second, we introduce a novel causal\nrepresentation learning (CRL) framework tailored to multimodal and\nmulti-measurement data, providing theoretical identifiability guarantees.\nExperiments on both synthetic and real-world data demonstrate the effectiveness\nof our approach. By unifying structured and unstructured analysis, PersonaX\nestablishes a foundation for studying LLM-inferred behavioral traits in\nconjunction with visual and biographical attributes, advancing multimodal trait\nanalysis and causal reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11362.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.11866",
            "authors": [
                {
                    "_id": "68c954bf4b9025f9f1e0c188",
                    "name": "Meng Luo",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c189",
                    "name": "Shengqiong Wu",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c18a",
                    "name": "Liqiang Jing",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c18b",
                    "name": "Tianjie Ju",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c18c",
                    "name": "Li Zheng",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c18d",
                    "name": "Jinxiang Lai",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c18e",
                    "name": "Tianlong Wu",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c18f",
                    "name": "Xinya Du",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c190",
                    "name": "Jian Li",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c191",
                    "name": "Siyuan Yan",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c192",
                    "name": "Jiebo Luo",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c193",
                    "name": "William Yang Wang",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c194",
                    "name": "Hao Fei",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c195",
                    "name": "Mong-Li Lee",
                    "hidden": false
                },
                {
                    "_id": "68c954bf4b9025f9f1e0c196",
                    "name": "Wynne Hsu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T12:39:19.000Z",
            "submittedOnDailyAt": "2025-09-16T10:45:27.651Z",
            "title": "Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose\n  Video Hallucination by Fine-grained Spatial-Temporal Grounding",
            "submittedOnDailyBy": {
                "_id": "64ad1c0bad6218d51a07b54e",
                "avatarUrl": "/avatars/0f84d9a51c6ca9bcef44de2d7c707d9b.svg",
                "isPro": false,
                "fullname": "LUO MENG",
                "user": "Eureka-Leo",
                "type": "user"
            },
            "summary": "Recent advancements in large video models (LVMs) have significantly enhance\nvideo understanding. However, these models continue to suffer from\nhallucinations, producing content that conflicts with input videos. To address\nthis issue, we propose Dr.V, a hierarchical framework covering perceptive,\ntemporal, and cognitive levels to diagnose video hallucination by fine-grained\nspatial-temporal grounding. Dr.V comprises of two key components: a benchmark\ndataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes\n10k instances drawn from 4,974 videos spanning diverse tasks, each enriched\nwith detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in\nLVMs by systematically applying fine-grained spatial-temporal grounding at the\nperceptive and temporal levels, followed by cognitive level reasoning. This\nstep-by-step pipeline mirrors human-like video comprehension and effectively\nidentifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is\neffective in diagnosing hallucination while enhancing interpretability and\nreliability, offering a practical blueprint for robust video understanding in\nreal-world scenarios. All our data and code are available at\nhttps://github.com/Eurekaleo/Dr.V.",
            "upvotes": 1,
            "discussionId": "68c954bf4b9025f9f1e0c197",
            "ai_summary": "Dr.V, a hierarchical framework with Dr.V-Bench and Dr.V-Agent, addresses video hallucinations through fine-grained spatial-temporal grounding and cognitive reasoning, enhancing video understanding.",
            "ai_keywords": [
                "large video models",
                "LVMs",
                "video hallucinations",
                "hierarchical framework",
                "perceptive levels",
                "temporal levels",
                "cognitive levels",
                "fine-grained spatial-temporal grounding",
                "benchmark dataset",
                "satellite video agent",
                "Dr.V-Bench",
                "Dr.V-Agent",
                "spatial-temporal annotation",
                "human-like video comprehension",
                "interpretability",
                "reliability"
            ]
        },
        "publishedAt": "2025-09-15T08:39:19.000Z",
        "title": "Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose\n  Video Hallucination by Fine-grained Spatial-Temporal Grounding",
        "summary": "Recent advancements in large video models (LVMs) have significantly enhance\nvideo understanding. However, these models continue to suffer from\nhallucinations, producing content that conflicts with input videos. To address\nthis issue, we propose Dr.V, a hierarchical framework covering perceptive,\ntemporal, and cognitive levels to diagnose video hallucination by fine-grained\nspatial-temporal grounding. Dr.V comprises of two key components: a benchmark\ndataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes\n10k instances drawn from 4,974 videos spanning diverse tasks, each enriched\nwith detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations in\nLVMs by systematically applying fine-grained spatial-temporal grounding at the\nperceptive and temporal levels, followed by cognitive level reasoning. This\nstep-by-step pipeline mirrors human-like video comprehension and effectively\nidentifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent is\neffective in diagnosing hallucination while enhancing interpretability and\nreliability, offering a practical blueprint for robust video understanding in\nreal-world scenarios. All our data and code are available at\nhttps://github.com/Eurekaleo/Dr.V.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11866.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64ad1c0bad6218d51a07b54e",
            "avatarUrl": "/avatars/0f84d9a51c6ca9bcef44de2d7c707d9b.svg",
            "fullname": "LUO MENG",
            "name": "Eureka-Leo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.11648",
            "authors": [
                {
                    "_id": "68c8c3ac733e345e52ac1e58",
                    "user": {
                        "_id": "651692d718f3a57f869a5a0a",
                        "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
                        "isPro": false,
                        "fullname": "Sai Kartheek Reddy",
                        "user": "UVSKKR",
                        "type": "user"
                    },
                    "name": "Sai Kartheek Reddy Kasu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:43.840Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T07:35:35.000Z",
            "submittedOnDailyAt": "2025-09-16T00:28:30.435Z",
            "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI",
            "submittedOnDailyBy": {
                "_id": "651692d718f3a57f869a5a0a",
                "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
                "isPro": false,
                "fullname": "Sai Kartheek Reddy",
                "user": "UVSKKR",
                "type": "user"
            },
            "summary": "The deployment of large language models (LLMs) in mental health and other\nsensitive domains raises urgent questions about ethical reasoning, fairness,\nand responsible alignment. Yet, existing benchmarks for moral and clinical\ndecision-making do not adequately capture the unique ethical dilemmas\nencountered in mental health practice, where confidentiality, autonomy,\nbeneficence, and bias frequently intersect. To address this gap, we introduce\nEthical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios\ndesigned to evaluate how AI systems navigate ethically charged situations in\ntherapeutic and psychiatric contexts. Each scenario is enriched with structured\nfields, including multiple decision options, expert-aligned reasoning, expected\nmodel behavior, real-world impact, and multi-stakeholder viewpoints. This\nstructure enables evaluation not only of decision accuracy but also of\nexplanation quality and alignment with professional norms. Although modest in\nscale and developed with model-assisted generation, EthicsMH establishes a task\nframework that bridges AI ethics and mental health decision-making. By\nreleasing this dataset, we aim to provide a seed resource that can be expanded\nthrough community and expert contributions, fostering the development of AI\nsystems capable of responsibly handling some of society's most delicate\ndecisions.",
            "upvotes": 1,
            "discussionId": "68c8c3ad733e345e52ac1e59",
            "ai_summary": "EthicsMH is a dataset of 125 scenarios designed to evaluate AI systems' ethical reasoning in mental health contexts, focusing on decision accuracy, explanation quality, and alignment with professional norms.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "ethical reasoning",
                "fairness",
                "responsible alignment",
                "mental health",
                "ethical dilemmas",
                "confidentiality",
                "autonomy",
                "beneficence",
                "bias",
                "pilot dataset",
                "decision options",
                "expert-aligned reasoning",
                "expected model behavior",
                "real-world impact",
                "multi-stakeholder viewpoints",
                "AI ethics",
                "mental health decision-making"
            ]
        },
        "publishedAt": "2025-09-15T03:35:35.000Z",
        "title": "EthicsMH: A Pilot Benchmark for Ethical Reasoning in Mental Health AI",
        "summary": "The deployment of large language models (LLMs) in mental health and other\nsensitive domains raises urgent questions about ethical reasoning, fairness,\nand responsible alignment. Yet, existing benchmarks for moral and clinical\ndecision-making do not adequately capture the unique ethical dilemmas\nencountered in mental health practice, where confidentiality, autonomy,\nbeneficence, and bias frequently intersect. To address this gap, we introduce\nEthical Reasoning in Mental Health (EthicsMH), a pilot dataset of 125 scenarios\ndesigned to evaluate how AI systems navigate ethically charged situations in\ntherapeutic and psychiatric contexts. Each scenario is enriched with structured\nfields, including multiple decision options, expert-aligned reasoning, expected\nmodel behavior, real-world impact, and multi-stakeholder viewpoints. This\nstructure enables evaluation not only of decision accuracy but also of\nexplanation quality and alignment with professional norms. Although modest in\nscale and developed with model-assisted generation, EthicsMH establishes a task\nframework that bridges AI ethics and mental health decision-making. By\nreleasing this dataset, we aim to provide a seed resource that can be expanded\nthrough community and expert contributions, fostering the development of AI\nsystems capable of responsibly handling some of society's most delicate\ndecisions.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11648.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "651692d718f3a57f869a5a0a",
            "avatarUrl": "/avatars/d5fe48de11e46675b05e1e2e1cf3505c.svg",
            "fullname": "Sai Kartheek Reddy",
            "name": "UVSKKR",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.11492",
            "authors": [
                {
                    "_id": "68c9dc5e6e0073c09bd1dd5e",
                    "name": "Anirban Saha Anik",
                    "hidden": false
                },
                {
                    "_id": "68c9dc5e6e0073c09bd1dd5f",
                    "name": "Md Fahimul Kabir Chowdhury",
                    "hidden": false
                },
                {
                    "_id": "68c9dc5e6e0073c09bd1dd60",
                    "name": "Andrew Wyckoff",
                    "hidden": false
                },
                {
                    "_id": "68c9dc5e6e0073c09bd1dd61",
                    "name": "Sagnik Ray Choudhury",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T01:03:09.000Z",
            "submittedOnDailyAt": "2025-09-16T20:26:58.894Z",
            "title": "ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language\n  Models for Verifying Numerical Claims",
            "submittedOnDailyBy": {
                "_id": "66627df0a7cd190bd5a3bc51",
                "avatarUrl": "/avatars/517076f9f2fcb0a01c04a53a326646b1.svg",
                "isPro": false,
                "fullname": "Anirban Saha Anik",
                "user": "AnirbanSaha",
                "type": "user"
            },
            "summary": "This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,\nwhich focuses on verifying numerical and temporal claims using retrieved\nevidence. We explore two complementary approaches: zero-shot prompting with\ninstruction-tuned large language models (LLMs) and supervised fine-tuning using\nparameter-efficient LoRA. To enhance evidence quality, we investigate several\nselection strategies, including full-document input and top-k sentence\nfiltering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned\nwith LoRA achieves strong performance on the English validation set. However, a\nnotable drop in the test set highlights a generalization challenge. These\nfindings underscore the importance of evidence granularity and model adaptation\nfor robust numerical fact verification.",
            "upvotes": 1,
            "discussionId": "68c9dc5f6e0073c09bd1dd62",
            "ai_summary": "The system uses zero-shot prompting and parameter-efficient fine-tuning to verify numerical and temporal claims, with findings highlighting the importance of evidence granularity and model adaptation.",
            "ai_keywords": [
                "zero-shot prompting",
                "instruction-tuned large language models",
                "supervised fine-tuning",
                "parameter-efficient LoRA",
                "full-document input",
                "top-k sentence filtering",
                "BM25",
                "MiniLM",
                "LLaMA",
                "numerical fact verification"
            ]
        },
        "publishedAt": "2025-09-14T21:03:09.000Z",
        "title": "ClaimIQ at CheckThat! 2025: Comparing Prompted and Fine-Tuned Language\n  Models for Verifying Numerical Claims",
        "summary": "This paper presents our system for Task 3 of the CLEF 2025 CheckThat! Lab,\nwhich focuses on verifying numerical and temporal claims using retrieved\nevidence. We explore two complementary approaches: zero-shot prompting with\ninstruction-tuned large language models (LLMs) and supervised fine-tuning using\nparameter-efficient LoRA. To enhance evidence quality, we investigate several\nselection strategies, including full-document input and top-k sentence\nfiltering using BM25 and MiniLM. Our best-performing model LLaMA fine-tuned\nwith LoRA achieves strong performance on the English validation set. However, a\nnotable drop in the test set highlights a generalization challenge. These\nfindings underscore the importance of evidence granularity and model adaptation\nfor robust numerical fact verification.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11492.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "66627df0a7cd190bd5a3bc51",
            "avatarUrl": "/avatars/517076f9f2fcb0a01c04a53a326646b1.svg",
            "fullname": "Anirban Saha Anik",
            "name": "AnirbanSaha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.11425",
            "authors": [
                {
                    "_id": "68ca01d66e0073c09bd1ddc5",
                    "name": "Md Mubtasim Ahasan",
                    "hidden": false
                },
                {
                    "_id": "68ca01d66e0073c09bd1ddc6",
                    "name": "Rafat Hasan Khan",
                    "hidden": false
                },
                {
                    "_id": "68ca01d66e0073c09bd1ddc7",
                    "name": "Tasnim Mohiuddin",
                    "hidden": false
                },
                {
                    "_id": "68ca01d66e0073c09bd1ddc8",
                    "name": "Aman Chadha",
                    "hidden": false
                },
                {
                    "_id": "68ca01d66e0073c09bd1ddc9",
                    "name": "Tariq Iqbal",
                    "hidden": false
                },
                {
                    "_id": "68ca01d66e0073c09bd1ddca",
                    "name": "M Ashraful Amin",
                    "hidden": false
                },
                {
                    "_id": "68ca01d66e0073c09bd1ddcb",
                    "name": "Amin Ahsan Ali",
                    "hidden": false
                },
                {
                    "_id": "68ca01d66e0073c09bd1ddcc",
                    "name": "Md Mofijul Islam",
                    "hidden": false
                },
                {
                    "_id": "68ca01d66e0073c09bd1ddcd",
                    "name": "A K M Mahbubur Rahman",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-14T20:35:36.000Z",
            "submittedOnDailyAt": "2025-09-16T23:03:55.176Z",
            "title": "FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs",
            "submittedOnDailyBy": {
                "_id": "63a4754927f1f64ed7238dac",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
                "isPro": false,
                "fullname": "Aman Chadha",
                "user": "amanchadha",
                "type": "user"
            },
            "summary": "Speech tokenization enables discrete representation and facilitates speech\nlanguage modeling. However, existing neural codecs capture low-level acoustic\nfeatures, overlooking the semantic and contextual cues inherent to human\nspeech. While recent efforts introduced semantic representations from\nself-supervised speech models or incorporated contextual representations from\npre-trained language models, challenges remain in aligning and unifying the\nsemantic and contextual representations. We introduce FuseCodec, which unifies\nacoustic, semantic, and contextual representations through strong cross-modal\nalignment and globally informed supervision. We propose three complementary\ntechniques: (i) Latent Representation Fusion, integrating semantic and\ncontextual features directly into the encoder latent space for robust and\nunified representation learning; (ii) Global Semantic-Contextual Supervision,\nsupervising discrete tokens with globally pooled and broadcasted\nrepresentations to enhance temporal consistency and cross-modal alignment; and\n(iii) Temporally Aligned Contextual Supervision, strengthening alignment by\ndynamically matching contextual and speech tokens within a local window for\nfine-grained token-level supervision. We further introduce FuseCodec-TTS,\ndemonstrating our methodology's applicability to zero-shot speech synthesis.\nEmpirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,\nsurpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,\nperceptual quality, intelligibility, and speaker similarity. Results highlight\nthe effectiveness of contextually and semantically guided tokenization for\nspeech tokenization and downstream tasks. Code and pretrained models are\navailable at https://github.com/mubtasimahasan/FuseCodec.",
            "upvotes": 1,
            "discussionId": "68ca01d66e0073c09bd1ddce",
            "ai_summary": "FuseCodec unifies acoustic, semantic, and contextual representations in speech tokenization through cross-modal alignment and global supervision, achieving state-of-the-art performance in transcription and synthesis.",
            "ai_keywords": [
                "speech tokenization",
                "discrete representation",
                "speech language modeling",
                "neural codecs",
                "acoustic features",
                "semantic representations",
                "self-supervised speech models",
                "contextual representations",
                "pre-trained language models",
                "cross-modal alignment",
                "globally informed supervision",
                "Latent Representation Fusion",
                "Global Semantic-Contextual Supervision",
                "Temporally Aligned Contextual Supervision",
                "FuseCodec-TTS",
                "LibriSpeech",
                "EnCodec",
                "SpeechTokenizer",
                "DAC",
                "transcription accuracy",
                "perceptual quality",
                "intelligibility",
                "speaker similarity"
            ]
        },
        "publishedAt": "2025-09-14T16:35:36.000Z",
        "title": "FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs",
        "summary": "Speech tokenization enables discrete representation and facilitates speech\nlanguage modeling. However, existing neural codecs capture low-level acoustic\nfeatures, overlooking the semantic and contextual cues inherent to human\nspeech. While recent efforts introduced semantic representations from\nself-supervised speech models or incorporated contextual representations from\npre-trained language models, challenges remain in aligning and unifying the\nsemantic and contextual representations. We introduce FuseCodec, which unifies\nacoustic, semantic, and contextual representations through strong cross-modal\nalignment and globally informed supervision. We propose three complementary\ntechniques: (i) Latent Representation Fusion, integrating semantic and\ncontextual features directly into the encoder latent space for robust and\nunified representation learning; (ii) Global Semantic-Contextual Supervision,\nsupervising discrete tokens with globally pooled and broadcasted\nrepresentations to enhance temporal consistency and cross-modal alignment; and\n(iii) Temporally Aligned Contextual Supervision, strengthening alignment by\ndynamically matching contextual and speech tokens within a local window for\nfine-grained token-level supervision. We further introduce FuseCodec-TTS,\ndemonstrating our methodology's applicability to zero-shot speech synthesis.\nEmpirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,\nsurpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,\nperceptual quality, intelligibility, and speaker similarity. Results highlight\nthe effectiveness of contextually and semantically guided tokenization for\nspeech tokenization and downstream tasks. Code and pretrained models are\navailable at https://github.com/mubtasimahasan/FuseCodec.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11425.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "63a4754927f1f64ed7238dac",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/63a4754927f1f64ed7238dac/aH-eJF-31g4vof9jv2gmI.jpeg",
            "fullname": "Aman Chadha",
            "name": "amanchadha",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 10
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.10844",
            "authors": [
                {
                    "_id": "68c8bedf733e345e52ac1e34",
                    "name": "Yixuan Tang",
                    "hidden": false
                },
                {
                    "_id": "68c8bedf733e345e52ac1e35",
                    "user": {
                        "_id": "647d834618274bce03013cc2",
                        "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
                        "isPro": true,
                        "fullname": "yixuan",
                        "user": "yixuantt",
                        "type": "user"
                    },
                    "name": "Yi Yang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-16T09:42:55.548Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-13T15:03:37.000Z",
            "submittedOnDailyAt": "2025-09-16T01:28:34.281Z",
            "title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings",
            "submittedOnDailyBy": {
                "_id": "647d834618274bce03013cc2",
                "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
                "isPro": true,
                "fullname": "yixuan",
                "user": "yixuantt",
                "type": "user"
            },
            "summary": "Domain-specific embedding models have shown promise for applications that\nrequire specialized semantic understanding, such as coding agents and financial\nretrieval systems, often achieving higher performance gains than general\nmodels. However, state-of-the-art embedding models are typically based on LLMs,\nwhich contain billions of parameters, making deployment challenging in\nresource-constrained environments. Model compression through pruning offers a\npromising solution, but existing pruning methods treat all parameters\nuniformly, failing to distinguish between general semantic representations and\ndomain-specific patterns, leading to suboptimal pruning decisions. Thus, we\npropose GAPrune, a pruning framework that addresses this challenge by\nconsidering both domain importance and preserving general linguistic\nfoundation. Our method uses Fisher Information to measure importance and\ngeneral-domain gradient alignment to assess parameter behavior, then combines\nthese signals using our Domain Alignment Importance (DAI) scoring. Lower DAI\nscores indicate that the parameter is either less important for the domain task\nor creates conflicts between domain and general objectives. Experiments on two\ndomain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance\nwithin 2.5% of dense models in one-shot pruning at 50% sparsity, while\noutperforming all baselines. With retraining in 100 steps, GAPrune achieves\n+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our\npruning strategy not only preserves but enhances domain-specific capabilities.\nOur findings demonstrate that principled pruning strategies can achieve model\ncompression and enhanced domain specialization, providing the research\ncommunity with a new approach for development.",
            "upvotes": 1,
            "discussionId": "68c8bedf733e345e52ac1e36",
            "projectPage": "https://github.com/yixuantt/GAPrune",
            "githubRepo": "https://github.com/yixuantt/GAPrune",
            "ai_summary": "GAPrune, a pruning framework that considers domain importance and general linguistic foundation, effectively compresses models while maintaining and enhancing domain-specific performance.",
            "ai_keywords": [
                "LLMs",
                "pruning",
                "Fisher Information",
                "gradient alignment",
                "Domain Alignment Importance (DAI)",
                "FinMTEB",
                "ChemTEB",
                "one-shot pruning",
                "retraining"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-13T11:03:37.000Z",
        "title": "GAPrune: Gradient-Alignment Pruning for Domain-Aware Embeddings",
        "summary": "Domain-specific embedding models have shown promise for applications that\nrequire specialized semantic understanding, such as coding agents and financial\nretrieval systems, often achieving higher performance gains than general\nmodels. However, state-of-the-art embedding models are typically based on LLMs,\nwhich contain billions of parameters, making deployment challenging in\nresource-constrained environments. Model compression through pruning offers a\npromising solution, but existing pruning methods treat all parameters\nuniformly, failing to distinguish between general semantic representations and\ndomain-specific patterns, leading to suboptimal pruning decisions. Thus, we\npropose GAPrune, a pruning framework that addresses this challenge by\nconsidering both domain importance and preserving general linguistic\nfoundation. Our method uses Fisher Information to measure importance and\ngeneral-domain gradient alignment to assess parameter behavior, then combines\nthese signals using our Domain Alignment Importance (DAI) scoring. Lower DAI\nscores indicate that the parameter is either less important for the domain task\nor creates conflicts between domain and general objectives. Experiments on two\ndomain benchmarks, FinMTEB and ChemTEB, show that GAPrune maintains performance\nwithin 2.5% of dense models in one-shot pruning at 50% sparsity, while\noutperforming all baselines. With retraining in 100 steps, GAPrune achieves\n+4.51% improvement on FinMTEB and +1.73% on ChemTEB, demonstrating that our\npruning strategy not only preserves but enhances domain-specific capabilities.\nOur findings demonstrate that principled pruning strategies can achieve model\ncompression and enhanced domain specialization, providing the research\ncommunity with a new approach for development.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10844.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647d834618274bce03013cc2",
            "avatarUrl": "/avatars/a95c7df96dc4fb6a96193f6dd5068227.svg",
            "fullname": "yixuan",
            "name": "yixuantt",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 7
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.11963",
            "authors": [
                {
                    "_id": "68c9c4d16e0073c09bd1dd22",
                    "name": "Mayank Agarwal",
                    "hidden": false
                },
                {
                    "_id": "68c9c4d16e0073c09bd1dd23",
                    "name": "Ibrahim Abdelaziz",
                    "hidden": false
                },
                {
                    "_id": "68c9c4d16e0073c09bd1dd24",
                    "name": "Kinjal Basu",
                    "hidden": false
                },
                {
                    "_id": "68c9c4d16e0073c09bd1dd25",
                    "name": "Merve Unuvar",
                    "hidden": false
                },
                {
                    "_id": "68c9c4d16e0073c09bd1dd26",
                    "name": "Luis A. Lastras",
                    "hidden": false
                },
                {
                    "_id": "68c9c4d16e0073c09bd1dd27",
                    "name": "Yara Rizk",
                    "hidden": false
                },
                {
                    "_id": "68c9c4d16e0073c09bd1dd28",
                    "name": "Pavan Kapanipathi",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T14:17:17.000Z",
            "submittedOnDailyAt": "2025-09-16T22:35:43.118Z",
            "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models",
            "submittedOnDailyBy": {
                "_id": "6229237ed94a4a3d5efbacb5",
                "avatarUrl": "/avatars/7de370e80cc876b33aee79a9959e7af3.svg",
                "isPro": false,
                "fullname": "Mayank Agarwal",
                "user": "mayankagarwal",
                "type": "user"
            },
            "summary": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering.",
            "upvotes": 0,
            "discussionId": "68c9c4d16e0073c09bd1dd29",
            "ai_summary": "A benchmark and training framework for reward models in tool-calling scenarios improve performance and enable efficient fine-tuning through outcome-based evaluation.",
            "ai_keywords": [
                "large language models",
                "LLMs",
                "reward modeling",
                "tool use",
                "FC-RewardBench",
                "tool-calling scenarios",
                "domain-specific modeling",
                "outcome-based reward models",
                "open-weight LLMs",
                "data-efficient fine-tuning",
                "reward-guided filtering"
            ]
        },
        "publishedAt": "2025-09-15T10:17:17.000Z",
        "title": "ToolRM: Outcome Reward Models for Tool-Calling Large Language Models",
        "summary": "As large language models (LLMs) increasingly interact with external tools,\nreward modeling for tool use has become a critical yet underexplored area.\nExisting reward models, trained primarily on natural language outputs, struggle\nto evaluate tool-based reasoning and execution. To quantify this gap, we\nintroduce FC-RewardBench, the first benchmark designed to systematically assess\nreward models' performance in tool-calling scenarios. Our analysis shows that\ncurrent reward models often miss key signals of effective tool use,\nhighlighting the need for domain-specific modeling. To address this, we propose\na training framework for outcome-based reward models using data synthesized\nfrom permissively licensed, open-weight LLMs. We train models ranging from 1.7B\nto 14B parameters and evaluate them across seven out-of-domain benchmarks.\nThese models consistently outperform general-purpose baselines, achieving up to\n25\\% average improvement in downstream task performance and enabling\ndata-efficient fine-tuning through reward-guided filtering.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11963.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6229237ed94a4a3d5efbacb5",
            "avatarUrl": "/avatars/7de370e80cc876b33aee79a9959e7af3.svg",
            "fullname": "Mayank Agarwal",
            "name": "mayankagarwal",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.07403",
            "authors": [
                {
                    "_id": "68c398f9fc1747b912403aa3",
                    "name": "Weichu Liu",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aa4",
                    "user": {
                        "_id": "60851545a5da133ac6c38686",
                        "avatarUrl": "/avatars/d385fcc513acef80a3b711aa92d898e5.svg",
                        "isPro": false,
                        "fullname": "Jing Xiong",
                        "user": "menik1126",
                        "type": "user"
                    },
                    "name": "Jing Xiong",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-12T16:07:35.882Z",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aa5",
                    "name": "Yuxuan Hu",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aa6",
                    "name": "Zixuan Li",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aa7",
                    "name": "Minghuan Tan",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aa8",
                    "name": "Ningning Mao",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aa9",
                    "name": "Chenyang Zhao",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aaa",
                    "name": "Zhongwei Wan",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aab",
                    "name": "Chaofan Tao",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aac",
                    "name": "Wendong Xu",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aad",
                    "user": {
                        "_id": "640feb91b0ee289c8583dd59",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640feb91b0ee289c8583dd59/CA4UZj2mEsJO2UR0NnN5H.jpeg",
                        "isPro": false,
                        "fullname": "Hui Shen",
                        "user": "Cloudriver",
                        "type": "user"
                    },
                    "name": "Hui Shen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-15T15:08:38.540Z",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aae",
                    "name": "Chengming Li",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403aaf",
                    "name": "Lingpeng Kong",
                    "hidden": false
                },
                {
                    "_id": "68c398f9fc1747b912403ab0",
                    "name": "Ngai Wong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-09T05:32:45.000Z",
            "submittedOnDailyAt": "2025-09-16T17:53:39.128Z",
            "title": "LongEmotion: Measuring Emotional Intelligence of Large Language Models\n  in Long-Context Interaction",
            "submittedOnDailyBy": {
                "_id": "640feb91b0ee289c8583dd59",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640feb91b0ee289c8583dd59/CA4UZj2mEsJO2UR0NnN5H.jpeg",
                "isPro": false,
                "fullname": "Hui Shen",
                "user": "Cloudriver",
                "type": "user"
            },
            "summary": "Large language models (LLMs) make significant progress in Emotional\nIntelligence (EI) and long-context understanding. However, existing benchmarks\ntend to overlook certain aspects of EI in long-context scenarios, especially\nunder realistic, practical settings where interactions are lengthy, diverse,\nand often noisy. To move towards such realistic settings, we present\nLongEmotion, a benchmark specifically designed for long-context EI tasks. It\ncovers a diverse set of tasks, including Emotion Classification, Emotion\nDetection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion\nExpression. On average, the input length for these tasks reaches 8,777 tokens,\nwith long-form generation required for Emotion Expression. To enhance\nperformance under realistic constraints, we incorporate Retrieval-Augmented\nGeneration (RAG) and Collaborative Emotional Modeling (CoEM), and compare them\nwith standard prompt-based methods. Unlike conventional approaches, our RAG\nmethod leverages both the conversation context and the large language model\nitself as retrieval sources, avoiding reliance on external knowledge bases. The\nCoEM method further improves performance by decomposing the task into five\nstages, integrating both retrieval augmentation and limited knowledge\ninjection. Experimental results show that both RAG and CoEM consistently\nenhance EI-related performance across most long-context tasks, advancing LLMs\ntoward more practical and real-world EI applications. Furthermore, we conducted\na comparative case study experiment on the GPT series to demonstrate the\ndifferences among various models in terms of EI. Code is available on GitHub at\nhttps://github.com/LongEmotion/LongEmotion, and the project page can be found\nat https://longemotion.github.io/.",
            "upvotes": 0,
            "discussionId": "68c398f9fc1747b912403ab1",
            "ai_summary": "LongEmotion benchmark enhances large language models' emotional intelligence in long-context scenarios using Retrieval-Augmented Generation and Collaborative Emotional Modeling.",
            "ai_keywords": [
                "Large language models",
                "Emotional Intelligence",
                "long-context understanding",
                "LongEmotion",
                "Emotion Classification",
                "Emotion Detection",
                "Emotion QA",
                "Emotion Conversation",
                "Emotion Summary",
                "Emotion Expression",
                "Retrieval-Augmented Generation",
                "Collaborative Emotional Modeling",
                "GPT series"
            ]
        },
        "publishedAt": "2025-09-09T01:32:45.000Z",
        "title": "LongEmotion: Measuring Emotional Intelligence of Large Language Models\n  in Long-Context Interaction",
        "summary": "Large language models (LLMs) make significant progress in Emotional\nIntelligence (EI) and long-context understanding. However, existing benchmarks\ntend to overlook certain aspects of EI in long-context scenarios, especially\nunder realistic, practical settings where interactions are lengthy, diverse,\nand often noisy. To move towards such realistic settings, we present\nLongEmotion, a benchmark specifically designed for long-context EI tasks. It\ncovers a diverse set of tasks, including Emotion Classification, Emotion\nDetection, Emotion QA, Emotion Conversation, Emotion Summary, and Emotion\nExpression. On average, the input length for these tasks reaches 8,777 tokens,\nwith long-form generation required for Emotion Expression. To enhance\nperformance under realistic constraints, we incorporate Retrieval-Augmented\nGeneration (RAG) and Collaborative Emotional Modeling (CoEM), and compare them\nwith standard prompt-based methods. Unlike conventional approaches, our RAG\nmethod leverages both the conversation context and the large language model\nitself as retrieval sources, avoiding reliance on external knowledge bases. The\nCoEM method further improves performance by decomposing the task into five\nstages, integrating both retrieval augmentation and limited knowledge\ninjection. Experimental results show that both RAG and CoEM consistently\nenhance EI-related performance across most long-context tasks, advancing LLMs\ntoward more practical and real-world EI applications. Furthermore, we conducted\na comparative case study experiment on the GPT series to demonstrate the\ndifferences among various models in terms of EI. Code is available on GitHub at\nhttps://github.com/LongEmotion/LongEmotion, and the project page can be found\nat https://longemotion.github.io/.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.07403.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "640feb91b0ee289c8583dd59",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/640feb91b0ee289c8583dd59/CA4UZj2mEsJO2UR0NnN5H.jpeg",
            "fullname": "Hui Shen",
            "name": "Cloudriver",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    }
]