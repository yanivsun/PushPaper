[
    {
        "paper": {
            "id": "2509.13312",
            "authors": [
                {
                    "_id": "68ca16756e0073c09bd1de0f",
                    "user": {
                        "_id": "64d31c55d8b712baf198602f",
                        "avatarUrl": "/avatars/fad7972744f879116a8dc8b406f8b91c.svg",
                        "isPro": false,
                        "fullname": "Zijian Li",
                        "user": "zli999",
                        "type": "user"
                    },
                    "name": "Zijian Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:56:59.945Z",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de10",
                    "name": "Xin Guan",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de11",
                    "name": "Bo Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de12",
                    "name": "Shen Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de13",
                    "name": "Houquan Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de14",
                    "name": "Shaopeng Lai",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de15",
                    "name": "Ming Yan",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de16",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de17",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de18",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de19",
                    "name": "Jun Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca16756e0073c09bd1de1a",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T17:57:21.000Z",
            "submittedOnDailyAt": "2025-09-17T00:31:34.945Z",
            "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for\n  Open-Ended Deep Research",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "This paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports.",
            "upvotes": 77,
            "discussionId": "68ca16756e0073c09bd1de1b",
            "projectPage": "https://tongyi-agent.github.io/blog/",
            "githubRepo": "https://github.com/Alibaba-NLP/DeepResearch",
            "ai_summary": "WebWeaver, a dual-agent framework, addresses open-ended deep research challenges by integrating adaptive planning and focused synthesis to produce high-quality, reliable reports.",
            "ai_keywords": [
                "open-ended deep research",
                "AI agents",
                "static research pipelines",
                "one-shot generation",
                "long-context failure",
                "loss in the middle",
                "hallucinations",
                "dual-agent framework",
                "human research process",
                "planner",
                "evidence acquisition",
                "outline optimization",
                "memory bank",
                "writer",
                "hierarchical retrieval",
                "writing process",
                "DeepResearch Bench",
                "DeepConsult",
                "DeepResearchGym"
            ],
            "githubStars": 7652
        },
        "publishedAt": "2025-09-16T13:57:21.000Z",
        "title": "WebWeaver: Structuring Web-Scale Evidence with Dynamic Outlines for\n  Open-Ended Deep Research",
        "summary": "This paper tackles open-ended deep research (OEDR), a complex challenge where\nAI agents must synthesize vast web-scale information into insightful reports.\nCurrent approaches are plagued by dual-fold limitations: static research\npipelines that decouple planning from evidence acquisition and one-shot\ngeneration paradigms that easily suffer from long-context failure issues like\n\"loss in the middle\" and hallucinations. To address these challenges, we\nintroduce WebWeaver, a novel dual-agent framework that emulates the human\nresearch process. The planner operates in a dynamic cycle, iteratively\ninterleaving evidence acquisition with outline optimization to produce a\ncomprehensive, source-grounded outline linking to a memory bank of evidence.\nThe writer then executes a hierarchical retrieval and writing process,\ncomposing the report section by section. By performing targeted retrieval of\nonly the necessary evidence from the memory bank for each part, it effectively\nmitigates long-context issues. Our framework establishes a new state-of-the-art\nacross major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and\nDeepResearchGym. These results validate our human-centric, iterative\nmethodology, demonstrating that adaptive planning and focused synthesis are\ncrucial for producing high-quality, reliable, and well-structured reports.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13312.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.13310",
            "authors": [
                {
                    "_id": "68ca3cbe6e0073c09bd1df42",
                    "user": {
                        "_id": "677f945ea82c316db164a180",
                        "avatarUrl": "/avatars/50ec99d971564944de3b1d9c17d50cfd.svg",
                        "isPro": false,
                        "fullname": "Liangcai Su",
                        "user": "HKU-Liangcai",
                        "type": "user"
                    },
                    "name": "Liangcai Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:47:38.981Z",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df43",
                    "name": "Zhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df44",
                    "name": "Guangyu Li",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df45",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df46",
                    "name": "Chenxi Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df47",
                    "name": "Maojia Song",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df48",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df49",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df4a",
                    "user": {
                        "_id": "644a4fbc2166258fccc664bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                        "isPro": false,
                        "fullname": "Jialong Wu",
                        "user": "callanwu",
                        "type": "user"
                    },
                    "name": "Jialong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:47:36.751Z",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df4b",
                    "user": {
                        "_id": "65e6970d135c27ea806526fe",
                        "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg",
                        "isPro": false,
                        "fullname": "Xuanzhong Chen",
                        "user": "chenxz",
                        "type": "user"
                    },
                    "name": "Xuanzhong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:47:41.195Z",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df4c",
                    "name": "Zile Qiao",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df4d",
                    "name": "Zhongwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df4e",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df4f",
                    "name": "Shihao Cai",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df50",
                    "name": "Runnan Fang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df51",
                    "name": "Zhengwei Tao",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df52",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df53",
                    "name": "Chenxiong Qian",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df54",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df55",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df56",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cbe6e0073c09bd1df57",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T17:57:19.000Z",
            "submittedOnDailyAt": "2025-09-17T03:17:41.951Z",
            "title": "Scaling Agents via Continual Pre-training",
            "submittedOnDailyBy": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
            },
            "summary": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.",
            "upvotes": 62,
            "discussionId": "68ca3cbe6e0073c09bd1df58",
            "projectPage": "https://tongyi-agent.github.io/blog/",
            "githubRepo": "https://github.com/Alibaba-NLP/DeepResearch///",
            "ai_summary": "AgentFounder, a deep research agent model incorporating Agentic Continual Pre-training, achieves state-of-the-art performance in agentic tasks while maintaining strong tool-use ability.",
            "ai_keywords": [
                "Large language models",
                "agentic systems",
                "autonomous tool use",
                "multi-step reasoning",
                "post-training approaches",
                "general-purpose foundation models",
                "agentic foundation models",
                "Agentic Continual Pre-training",
                "deep research agents",
                "AgentFounder",
                "BrowseComp-en",
                "BrowseComp-zh",
                "HLE"
            ],
            "githubStars": 7652
        },
        "publishedAt": "2025-09-16T13:57:19.000Z",
        "title": "Scaling Agents via Continual Pre-training",
        "summary": "Large language models (LLMs) have evolved into agentic systems capable of\nautonomous tool use and multi-step reasoning for complex problem-solving.\nHowever, post-training approaches building upon general-purpose foundation\nmodels consistently underperform in agentic tasks, particularly in open-source\nimplementations. We identify the root cause: the absence of robust agentic\nfoundation models forces models during post-training to simultaneously learn\ndiverse agentic behaviors while aligning them to expert demonstrations, thereby\ncreating fundamental optimization tensions. To this end, we are the first to\npropose incorporating Agentic Continual Pre-training (Agentic CPT) into the\ndeep research agents training pipeline to build powerful agentic foundational\nmodels. Based on this approach, we develop a deep research agent model named\nAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achieve\nstate-of-the-art performance while retains strong tool-use ability, notably\n39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13310.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "fullname": "Jialong Wu",
            "name": "callanwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.13305",
            "authors": [
                {
                    "_id": "68ca3cd06e0073c09bd1df5a",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df5b",
                    "name": "Zhongwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df5c",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df5d",
                    "name": "Rui Ye",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df5e",
                    "user": {
                        "_id": "66e4019518a1920fb7ca19d7",
                        "avatarUrl": "/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg",
                        "isPro": false,
                        "fullname": "Yida Zhao",
                        "user": "zhaoyd",
                        "type": "user"
                    },
                    "name": "Yida Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T15:29:20.549Z",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df5f",
                    "name": "Liwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df60",
                    "name": "Litu Ou",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df61",
                    "name": "Dingchu Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df62",
                    "user": {
                        "_id": "6622132f63598534f96ca29d",
                        "avatarUrl": "/avatars/34e61fc3101f8ebce1ef7041f761e108.svg",
                        "isPro": false,
                        "fullname": "Xixi Wu",
                        "user": "xxwu",
                        "type": "user"
                    },
                    "name": "Xixi Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:47:32.127Z",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df63",
                    "user": {
                        "_id": "644a4fbc2166258fccc664bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                        "isPro": false,
                        "fullname": "Jialong Wu",
                        "user": "callanwu",
                        "type": "user"
                    },
                    "name": "Jialong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:47:34.465Z",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df64",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df65",
                    "name": "Zile Qiao",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df66",
                    "name": "Zhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df67",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df68",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df69",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca3cd06e0073c09bd1df6a",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T17:57:03.000Z",
            "submittedOnDailyAt": "2025-09-17T03:16:18.695Z",
            "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning",
            "submittedOnDailyBy": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
            },
            "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
            "upvotes": 53,
            "discussionId": "68ca3cd06e0073c09bd1df6b",
            "projectPage": "https://tongyi-agent.github.io/blog/",
            "githubRepo": "https://github.com/Alibaba-NLP/DeepResearch/",
            "ai_summary": "WebSailor, a post-training methodology, enhances open-source models with systematic uncertainty reduction, matching proprietary agents' performance in complex information-seeking tasks.",
            "ai_keywords": [
                "LLM training",
                "DeepResearch",
                "BrowseComp",
                "reasoning pattern",
                "high-uncertainty tasks",
                "structured sampling",
                "information obfuscation",
                "RFT cold start",
                "agentic RL training",
                "Duplicating Sampling Policy Optimization",
                "DUPO"
            ],
            "githubStars": 7652
        },
        "publishedAt": "2025-09-16T13:57:03.000Z",
        "title": "WebSailor-V2: Bridging the Chasm to Proprietary Agents via Synthetic\n  Data and Scalable Reinforcement Learning",
        "summary": "Transcending human cognitive limitations represents a critical frontier in\nLLM training. Proprietary agentic systems like DeepResearch have demonstrated\nsuperhuman capabilities on extremely complex information-seeking benchmarks\nsuch as BrowseComp, a feat previously unattainable. We posit that their success\nhinges on a sophisticated reasoning pattern absent in open-source models: the\nability to systematically reduce extreme uncertainty when navigating vast\ninformation landscapes. Based on this insight, we introduce WebSailor, a\ncomplete post-training methodology designed to instill this crucial capability.\nOur approach involves generating novel, high-uncertainty tasks through\nstructured sampling and information obfuscation, RFT cold start, and an\nefficient agentic RL training algorithm, Duplicating Sampling Policy\nOptimization (DUPO). With this integrated pipeline, WebSailor significantly\noutperforms all open-source agents in complex information-seeking tasks,\nmatching proprietary agents' performance and closing the capability gap.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13305.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "fullname": "Jialong Wu",
            "name": "callanwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.13311",
            "authors": [
                {
                    "_id": "68ca16c06e0073c09bd1de1d",
                    "name": "Runnan Fang",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de1e",
                    "name": "Shihao Cai",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de1f",
                    "name": "Baixuan Li",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de20",
                    "user": {
                        "_id": "644a4fbc2166258fccc664bc",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                        "isPro": false,
                        "fullname": "Jialong Wu",
                        "user": "callanwu",
                        "type": "user"
                    },
                    "name": "Jialong Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:56:52.444Z",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de21",
                    "name": "Guangyu Li",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de22",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de23",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de24",
                    "name": "Xiaobin Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de25",
                    "user": {
                        "_id": "677f945ea82c316db164a180",
                        "avatarUrl": "/avatars/50ec99d971564944de3b1d9c17d50cfd.svg",
                        "isPro": false,
                        "fullname": "Liangcai Su",
                        "user": "HKU-Liangcai",
                        "type": "user"
                    },
                    "name": "Liangcai Su",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:56:56.416Z",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de26",
                    "name": "Zhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de27",
                    "name": "Shibin Wu",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de28",
                    "name": "Zhengwei Tao",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de29",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de2a",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de2b",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca16c06e0073c09bd1de2c",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T17:57:20.000Z",
            "submittedOnDailyAt": "2025-09-17T00:32:53.838Z",
            "title": "Towards General Agentic Intelligence via Environment Scaling",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Advanced agentic intelligence is a prerequisite for deploying Large Language\nModels in practical, real-world applications. Diverse real-world APIs demand\nprecise, robust function-calling intelligence, which needs agents to develop\nthese capabilities through interaction in varied environments. The breadth of\nfunction-calling competence is closely tied to the diversity of environments in\nwhich agents are trained. In this work, we scale up environments as a step\ntowards advancing general agentic intelligence. This gives rise to two central\nchallenges: (i) how to scale environments in a principled manner, and (ii) how\nto effectively train agentic capabilities from experiences derived through\ninteractions with these environments. To address these, we design a scalable\nframework that automatically constructs heterogeneous environments that are\nfully simulated, systematically broadening the space of function-calling\nscenarios. We further adapt a two-phase agent fine-tuning strategy: first\nendowing agents with fundamental agentic capabilities, then specializing them\nfor domain-specific contexts. Extensive experiments on agentic benchmarks,\ntau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,\nAgentScaler, significantly enhances the function-calling capability of models.",
            "upvotes": 50,
            "discussionId": "68ca16c16e0073c09bd1de2d",
            "ai_summary": "A scalable framework and two-phase fine-tuning strategy enhance function-calling capabilities of agents in diverse environments, improving performance on agentic benchmarks.",
            "ai_keywords": [
                "agentic intelligence",
                "Large Language Models",
                "function-calling intelligence",
                "heterogeneous environments",
                "two-phase agent fine-tuning",
                "tau-bench",
                "tau2-Bench",
                "ACEBench",
                "AgentScaler"
            ]
        },
        "publishedAt": "2025-09-16T13:57:20.000Z",
        "title": "Towards General Agentic Intelligence via Environment Scaling",
        "summary": "Advanced agentic intelligence is a prerequisite for deploying Large Language\nModels in practical, real-world applications. Diverse real-world APIs demand\nprecise, robust function-calling intelligence, which needs agents to develop\nthese capabilities through interaction in varied environments. The breadth of\nfunction-calling competence is closely tied to the diversity of environments in\nwhich agents are trained. In this work, we scale up environments as a step\ntowards advancing general agentic intelligence. This gives rise to two central\nchallenges: (i) how to scale environments in a principled manner, and (ii) how\nto effectively train agentic capabilities from experiences derived through\ninteractions with these environments. To address these, we design a scalable\nframework that automatically constructs heterogeneous environments that are\nfully simulated, systematically broadening the space of function-calling\nscenarios. We further adapt a two-phase agent fine-tuning strategy: first\nendowing agents with fundamental agentic capabilities, then specializing them\nfor domain-specific contexts. Extensive experiments on agentic benchmarks,\ntau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model,\nAgentScaler, significantly enhances the function-calling capability of models.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13311.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.13309",
            "authors": [
                {
                    "_id": "68ca171d6e0073c09bd1de39",
                    "name": "Zile Qiao",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de3a",
                    "name": "Guoxin Chen",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de3b",
                    "user": {
                        "_id": "65e6970d135c27ea806526fe",
                        "avatarUrl": "/avatars/4aced113d9cab055ae06f3945869a280.svg",
                        "isPro": false,
                        "fullname": "Xuanzhong Chen",
                        "user": "chenxz",
                        "type": "user"
                    },
                    "name": "Xuanzhong Chen",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:55:38.926Z",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de3c",
                    "name": "Donglei Yu",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de3d",
                    "name": "Wenbiao Yin",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de3e",
                    "name": "Xinyu Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de3f",
                    "name": "Zhen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de40",
                    "name": "Baixuan Li",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de41",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de42",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de43",
                    "name": "Rui Min",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de44",
                    "name": "Minpeng Liao",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de45",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de46",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de47",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca171d6e0073c09bd1de48",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T17:57:17.000Z",
            "submittedOnDailyAt": "2025-09-17T00:34:22.753Z",
            "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon\n  Agents",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "Recent advances in deep-research systems have demonstrated the potential for\nAI agents to autonomously discover and synthesize knowledge from external\nsources. In this paper, we introduce WebResearcher, a novel framework for\nbuilding such agents through two key components: (1) WebResearcher, an\niterative deep-research paradigm that reformulates deep research as a Markov\nDecision Process, where agents periodically consolidate findings into evolving\nreports while maintaining focused workspaces, overcoming the context\nsuffocation and noise contamination that plague existing mono-contextual\napproaches; and (2) WebFrontier, a scalable data synthesis engine that\ngenerates high-quality training data through tool-augmented complexity\nescalation, enabling systematic creation of research tasks that bridge the gap\nbetween passive knowledge recall and active knowledge construction. Notably, we\nfind that the training data from our paradigm significantly enhances tool-use\ncapabilities even for traditional mono-contextual methods. Furthermore, our\nparadigm naturally scales through parallel thinking, enabling concurrent\nmulti-agent exploration for more comprehensive conclusions. Extensive\nexperiments across 6 challenging benchmarks demonstrate that WebResearcher\nachieves state-of-the-art performance, even surpassing frontier proprietary\nsystems.",
            "upvotes": 49,
            "discussionId": "68ca171d6e0073c09bd1de49",
            "ai_summary": "WebResearcher, a deep-research framework, enhances AI agents' knowledge synthesis by reformulating research as a Markov Decision Process and using a scalable data synthesis engine, achieving superior performance across benchmarks.",
            "ai_keywords": [
                "deep-research systems",
                "AI agents",
                "Markov Decision Process",
                "WebResearcher",
                "WebFrontier",
                "data synthesis engine",
                "tool-augmented complexity escalation",
                "parallel thinking",
                "multi-agent exploration"
            ]
        },
        "publishedAt": "2025-09-16T13:57:17.000Z",
        "title": "WebResearcher: Unleashing unbounded reasoning capability in Long-Horizon\n  Agents",
        "summary": "Recent advances in deep-research systems have demonstrated the potential for\nAI agents to autonomously discover and synthesize knowledge from external\nsources. In this paper, we introduce WebResearcher, a novel framework for\nbuilding such agents through two key components: (1) WebResearcher, an\niterative deep-research paradigm that reformulates deep research as a Markov\nDecision Process, where agents periodically consolidate findings into evolving\nreports while maintaining focused workspaces, overcoming the context\nsuffocation and noise contamination that plague existing mono-contextual\napproaches; and (2) WebFrontier, a scalable data synthesis engine that\ngenerates high-quality training data through tool-augmented complexity\nescalation, enabling systematic creation of research tasks that bridge the gap\nbetween passive knowledge recall and active knowledge construction. Notably, we\nfind that the training data from our paradigm significantly enhances tool-use\ncapabilities even for traditional mono-contextual methods. Furthermore, our\nparadigm naturally scales through parallel thinking, enabling concurrent\nmulti-agent exploration for more comprehensive conclusions. Extensive\nexperiments across 6 challenging benchmarks demonstrate that WebResearcher\nachieves state-of-the-art performance, even surpassing frontier proprietary\nsystems.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13309.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.13313",
            "authors": [
                {
                    "_id": "68ca3c926e0073c09bd1df32",
                    "user": {
                        "_id": "6622132f63598534f96ca29d",
                        "avatarUrl": "/avatars/34e61fc3101f8ebce1ef7041f761e108.svg",
                        "isPro": false,
                        "fullname": "Xixi Wu",
                        "user": "xxwu",
                        "type": "user"
                    },
                    "name": "Xixi Wu",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:47:44.759Z",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df33",
                    "name": "Kuan Li",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df34",
                    "user": {
                        "_id": "66e4019518a1920fb7ca19d7",
                        "avatarUrl": "/avatars/4f8be2f3244239510235c2ec6e8d38a6.svg",
                        "isPro": false,
                        "fullname": "Yida Zhao",
                        "user": "zhaoyd",
                        "type": "user"
                    },
                    "name": "Yida Zhao",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T15:29:22.732Z",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df35",
                    "name": "Liwen Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df36",
                    "name": "Litu Ou",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df37",
                    "name": "Huifeng Yin",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df38",
                    "name": "Zhongwang Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df39",
                    "name": "Yong Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df3a",
                    "name": "Pengjun Xie",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df3b",
                    "name": "Fei Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df3c",
                    "name": "Minhao Cheng",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df3d",
                    "name": "Shuai Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df3e",
                    "name": "Hong Cheng",
                    "hidden": false
                },
                {
                    "_id": "68ca3c926e0073c09bd1df3f",
                    "name": "Jingren Zhou",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T17:57:22.000Z",
            "submittedOnDailyAt": "2025-09-17T03:17:01.731Z",
            "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization",
            "submittedOnDailyBy": {
                "_id": "644a4fbc2166258fccc664bc",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
                "isPro": false,
                "fullname": "Jialong Wu",
                "user": "callanwu",
                "type": "user"
            },
            "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.",
            "upvotes": 44,
            "discussionId": "68ca3c926e0073c09bd1df40",
            "projectPage": "https://tongyi-agent.github.io/blog/",
            "githubRepo": "https://github.com/Alibaba-NLP/DeepResearch//",
            "ai_summary": "ReSum, a novel paradigm with periodic context summarization, enhances web agents' performance on knowledge-intensive tasks by overcoming context window limitations, achieving significant improvements over ReAct.",
            "ai_keywords": [
                "Large Language Model",
                "LLM",
                "web agents",
                "knowledge-intensive tasks",
                "context window limitations",
                "ReAct",
                "ReSum",
                "context summarization",
                "reasoning states",
                "GRPO",
                "segmented trajectory training",
                "advantage broadcasting",
                "ReSum-GRPO",
                "WebResummer-30B",
                "WebSailor-30B",
                "BrowseComp-zh",
                "BrowseComp-en",
                "Pass@1"
            ],
            "githubStars": 7652
        },
        "publishedAt": "2025-09-16T13:57:22.000Z",
        "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization",
        "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching complete solutions. To overcome this\nchallenge, we introduce ReSum, a novel paradigm that enables indefinite\nexploration through periodic context summarization. ReSum converts growing\ninteraction histories into compact reasoning states, maintaining awareness of\nprior discoveries while bypassing context constraints. For paradigm adaptation,\nwe propose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents of varying scales across three\nbenchmarks demonstrate that ReSum delivers an average absolute improvement of\n4.5\\% over ReAct, with further gains of up to 8.2\\% following ReSum-GRPO\ntraining. Notably, with only 1K training samples, our WebResummer-30B (a\nReSum-GRPO-trained version of WebSailor-30B) achieves 33.3\\% Pass@1 on\nBrowseComp-zh and 18.3\\% on BrowseComp-en, surpassing existing open-source web\nagents.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13313.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "644a4fbc2166258fccc664bc",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/noauth/8k3b44MbhQiWuo6i8BnYl.jpeg",
            "fullname": "Jialong Wu",
            "name": "callanwu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 15
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.13232",
            "authors": [
                {
                    "_id": "68ca17a96e0073c09bd1de5f",
                    "name": "Zhongwen Xu",
                    "hidden": false
                },
                {
                    "_id": "68ca17a96e0073c09bd1de60",
                    "name": "Zihan Ding",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T16:39:11.000Z",
            "submittedOnDailyAt": "2025-09-17T00:38:24.543Z",
            "title": "Single-stream Policy Optimization",
            "submittedOnDailyBy": {
                "_id": "672db76fa34d64e774fc42c9",
                "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
                "isPro": false,
                "fullname": "Zhongwen Xu",
                "user": "zhongwenxu",
                "type": "user"
            },
            "summary": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@k across the evaluated k values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.",
            "upvotes": 23,
            "discussionId": "68ca17a96e0073c09bd1de61",
            "projectPage": "https://zhongwenxu.notion.site/Single-stream-Policy-Optimization-26a1c4e140e380d78d51fa4567727f50",
            "ai_summary": "Single-stream Policy Optimization (SPO) improves policy-gradient training for Large Language Models by eliminating group-based issues and providing a stable, low-variance learning signal, leading to better performance and efficiency.",
            "ai_keywords": [
                "policy-gradient optimization",
                "Large Language Models (LLMs)",
                "GRPO",
                "variance reduction",
                "on-the-fly baselines",
                "degenerate groups",
                "synchronization barriers",
                "Single-stream Policy Optimization (SPO)",
                "KL-adaptive value tracker",
                "advantage normalization",
                "adaptive curriculum",
                "prioritized sampling",
                "Qwen3-8B",
                "hard math benchmarks",
                "maj@32",
                "BRUMO 25",
                "AIME 25",
                "HMMT 25",
                "pass@$k$"
            ]
        },
        "publishedAt": "2025-09-16T12:39:11.000Z",
        "title": "Single-stream Policy Optimization",
        "summary": "We revisit policy-gradient optimization for Large Language Models (LLMs) from\na single-stream perspective. Prevailing group-based methods like GRPO reduce\nvariance with on-the-fly baselines but suffer from critical flaws: frequent\ndegenerate groups erase learning signals, and synchronization barriers hinder\nscalability. We introduce Single-stream Policy Optimization (SPO), which\neliminates these issues by design. SPO replaces per-group baselines with a\npersistent, KL-adaptive value tracker and normalizes advantages globally across\nthe batch, providing a stable, low-variance learning signal for every sample.\nBeing group-free, SPO enables higher throughput and scales effectively in\nlong-horizon or tool-integrated settings where generation times vary.\nFurthermore, the persistent value tracker naturally enables an adaptive\ncurriculum via prioritized sampling. Experiments using Qwen3-8B show that SPO\nconverges more smoothly and attains higher accuracy than GRPO, while\neliminating computation wasted on degenerate groups. Ablation studies confirm\nthat SPO's gains stem from its principled approach to baseline estimation and\nadvantage normalization, offering a more robust and efficient path for LLM\nreasoning. Across five hard math benchmarks with Qwen3 8B, SPO improves the\naverage maj@32 by +3.4 percentage points (pp) over GRPO, driven by substantial\nabsolute point gains on challenging datasets, including +7.3 pp on BRUMO 25,\n+4.4 pp on AIME 25, +3.3 pp on HMMT 25, and achieves consistent relative gain\nin pass@k across the evaluated k values. SPO's success challenges the\nprevailing trend of adding incidental complexity to RL algorithms, highlighting\na path where fundamental principles, not architectural workarounds, drive the\nnext wave of progress in LLM reasoning.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13232.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "672db76fa34d64e774fc42c9",
            "avatarUrl": "/avatars/be529f4bfcc40514697facbe8e874735.svg",
            "fullname": "Zhongwen Xu",
            "name": "zhongwenxu",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.12815",
            "authors": [
                {
                    "_id": "68ca18456e0073c09bd1de63",
                    "name": "Biwen Lei",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de64",
                    "name": "Yang Li",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de65",
                    "name": "Xinhai Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de66",
                    "name": "Shuhui Yang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de67",
                    "name": "Lixin Xu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de68",
                    "name": "Jingwei Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de69",
                    "name": "Ruining Tang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de6a",
                    "name": "Haohan Weng",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de6b",
                    "name": "Jian Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de6c",
                    "name": "Jing Xu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de6d",
                    "name": "Zhen Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de6e",
                    "name": "Yiling Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de6f",
                    "name": "Jiankai Xing",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de70",
                    "name": "Jiachen Xu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de71",
                    "name": "Changfeng Ma",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de72",
                    "name": "Xinhao Yan",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de73",
                    "name": "Yunhan Yang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de74",
                    "name": "Chunshi Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de75",
                    "name": "Duoteng Xu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de76",
                    "name": "Xueqi Ma",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de77",
                    "name": "Yuguang Chen",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de78",
                    "name": "Jing Li",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de79",
                    "name": "Mingxin Yang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de7a",
                    "name": "Sheng Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de7b",
                    "name": "Yifei Feng",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de7c",
                    "name": "Xin Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de7d",
                    "name": "Di Luo",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de7e",
                    "name": "Zebin He",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de7f",
                    "name": "Puhua Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de80",
                    "name": "Changrong Hu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de81",
                    "name": "Zihan Qin",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de82",
                    "name": "Shiwei Miao",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de83",
                    "name": "Haolin Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de84",
                    "name": "Yunfei Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de85",
                    "name": "Zeqiang Lai",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de86",
                    "name": "Qingxiang Lin",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de87",
                    "name": "Zibo Zhao",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de88",
                    "name": "Kunhong Li",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de89",
                    "name": "Xianghui Yang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de8a",
                    "name": "Huiwen Shi",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de8b",
                    "name": "Xin Yang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de8c",
                    "name": "Yuxuan Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de8d",
                    "name": "Zebin Yao",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de8e",
                    "name": "Yihang Lian",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de8f",
                    "name": "Sicong Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de90",
                    "name": "Xintong Han",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de91",
                    "name": "Wangchen Qin",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de92",
                    "name": "Caisheng Ouyang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de93",
                    "name": "Jianyin Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de94",
                    "name": "Tianwen Yuan",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de95",
                    "name": "Shuai Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de96",
                    "name": "Hong Duan",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de97",
                    "name": "Yanqi Niu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de98",
                    "name": "Wencong Lin",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de99",
                    "name": "Yifu Sun",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de9a",
                    "name": "Shirui Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de9b",
                    "name": "Lin Niu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de9c",
                    "name": "Gu Gong",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de9d",
                    "name": "Guojian Xiao",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de9e",
                    "name": "Bojian Zheng",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1de9f",
                    "name": "Xiang Yuan",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea0",
                    "name": "Qi Chen",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea1",
                    "name": "Jie Xiao",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea2",
                    "name": "Dongyang Zheng",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea3",
                    "name": "Xiaofeng Yang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea4",
                    "name": "Kai Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea5",
                    "name": "Jianchen Zhu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea6",
                    "name": "Lifu Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea7",
                    "name": "Qinglin Lu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea8",
                    "name": "Jie Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dea9",
                    "name": "Liang Dong",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deaa",
                    "name": "Fan Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deab",
                    "name": "Ruibin Chen",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deac",
                    "name": "Lei Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dead",
                    "name": "Chao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deae",
                    "name": "Jiaxin Lin",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deaf",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb0",
                    "name": "Zheng Ye",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb1",
                    "name": "Peng He",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb2",
                    "name": "Runzhou Wu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb3",
                    "name": "Yinhe Wu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb4",
                    "name": "Jiayao Du",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb5",
                    "name": "Jupeng Chen",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb6",
                    "name": "Xinyue Mao",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb7",
                    "name": "Dongyuan Guo",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb8",
                    "name": "Yixuan Tang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deb9",
                    "name": "Yulin Tsai",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1deba",
                    "name": "Yonghao Tan",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1debb",
                    "name": "Jiaao Yu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1debc",
                    "name": "Junlin Yu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1debd",
                    "name": "Keren Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1debe",
                    "name": "Yifan Li",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1debf",
                    "name": "Peng Chen",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dec0",
                    "name": "Tian Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dec1",
                    "name": "Di Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dec2",
                    "name": "Yuhong Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dec3",
                    "name": "Linus",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dec4",
                    "name": "Jie Jiang",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dec5",
                    "name": "Zhuo Chen",
                    "hidden": false
                },
                {
                    "_id": "68ca18456e0073c09bd1dec6",
                    "name": "Chunchao Guo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T08:33:03.000Z",
            "submittedOnDailyAt": "2025-09-17T00:39:17.325Z",
            "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset\n  Generation",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "The creation of high-quality 3D assets, a cornerstone of modern game\ndevelopment, has long been characterized by labor-intensive and specialized\nworkflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered\ncontent creation platform designed to revolutionize the game production\npipeline by automating and streamlining the generation of game-ready 3D assets.\nAt its core, Hunyuan3D Studio integrates a suite of advanced neural modules\n(such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into\na cohesive and user-friendly system. This unified framework allows for the\nrapid transformation of a single concept image or textual description into a\nfully-realized, production-quality 3D model complete with optimized geometry\nand high-fidelity PBR textures. We demonstrate that assets generated by\nHunyuan3D Studio are not only visually compelling but also adhere to the\nstringent technical requirements of contemporary game engines, significantly\nreducing iteration time and lowering the barrier to entry for 3D content\ncreation. By providing a seamless bridge from creative intent to technical\nasset, Hunyuan3D Studio represents a significant leap forward for AI-assisted\nworkflows in game development and interactive media.",
            "upvotes": 19,
            "discussionId": "68ca18456e0073c09bd1dec7",
            "ai_summary": "Hunyuan3D Studio automates 3D asset creation using AI, integrating neural modules to transform concept images or text into high-quality, game-ready 3D models with optimized geometry and PBR textures.",
            "ai_keywords": [
                "Part-level 3D Generation",
                "Polygon Generation",
                "Semantic UV",
                "PBR textures"
            ]
        },
        "publishedAt": "2025-09-16T04:33:03.000Z",
        "title": "Hunyuan3D Studio: End-to-End AI Pipeline for Game-Ready 3D Asset\n  Generation",
        "summary": "The creation of high-quality 3D assets, a cornerstone of modern game\ndevelopment, has long been characterized by labor-intensive and specialized\nworkflows. This paper presents Hunyuan3D Studio, an end-to-end AI-powered\ncontent creation platform designed to revolutionize the game production\npipeline by automating and streamlining the generation of game-ready 3D assets.\nAt its core, Hunyuan3D Studio integrates a suite of advanced neural modules\n(such as Part-level 3D Generation, Polygon Generation, Semantic UV, etc.) into\na cohesive and user-friendly system. This unified framework allows for the\nrapid transformation of a single concept image or textual description into a\nfully-realized, production-quality 3D model complete with optimized geometry\nand high-fidelity PBR textures. We demonstrate that assets generated by\nHunyuan3D Studio are not only visually compelling but also adhere to the\nstringent technical requirements of contemporary game engines, significantly\nreducing iteration time and lowering the barrier to entry for 3D content\ncreation. By providing a seamless bridge from creative intent to technical\nasset, Hunyuan3D Studio represents a significant leap forward for AI-assisted\nworkflows in game development and interactive media.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12815.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.13317",
            "authors": [
                {
                    "_id": "68ca17616e0073c09bd1de4b",
                    "name": "An-Chieh Cheng",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de4c",
                    "name": "Yang Fu",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de4d",
                    "name": "Yukang Chen",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de4e",
                    "name": "Zhijian Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de4f",
                    "name": "Xiaolong Li",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de50",
                    "name": "Subhashree Radhakrishnan",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de51",
                    "name": "Song Han",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de52",
                    "name": "Yao Lu",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de53",
                    "name": "Jan Kautz",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de54",
                    "name": "Pavlo Molchanov",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de55",
                    "name": "Hongxu Yin",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de56",
                    "name": "Xiaolong Wang",
                    "hidden": false
                },
                {
                    "_id": "68ca17616e0073c09bd1de57",
                    "name": "Sifei Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T17:59:06.000Z",
            "submittedOnDailyAt": "2025-09-17T00:35:36.289Z",
            "title": "3D Aware Region Prompted Vision Language Model",
            "submittedOnDailyBy": {
                "_id": "6039478ab3ecf716b1a5fd4d",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
                "isPro": true,
                "fullname": "taesiri",
                "user": "taesiri",
                "type": "user"
            },
            "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.",
            "upvotes": 8,
            "discussionId": "68ca17616e0073c09bd1de58",
            "projectPage": "https://www.anjiecheng.me/sr3d",
            "ai_summary": "A Spatial Region 3D (SR-3D) vision-language model unifies 2D and 3D representations by enriching 2D features with 3D positional embeddings, enabling flexible region prompting and accurate spatial reasoning across frames.",
            "ai_keywords": [
                "Spatial Region 3D",
                "SR-3D",
                "vision-language model",
                "visual token space",
                "region prompting",
                "bounding boxes",
                "segmentation masks",
                "3D positional embeddings",
                "spatial reasoning",
                "scene understanding",
                "in-the-wild videos",
                "spatial relationships",
                "metric measurements"
            ]
        },
        "publishedAt": "2025-09-16T13:59:06.000Z",
        "title": "3D Aware Region Prompted Vision Language Model",
        "summary": "We present Spatial Region 3D (SR-3D) aware vision-language model that\nconnects single-view 2D images and multi-view 3D data through a shared visual\ntoken space. SR-3D supports flexible region prompting, allowing users to\nannotate regions with bounding boxes, segmentation masks on any frame, or\ndirectly in 3D, without the need for exhaustive multi-frame labeling. We\nachieve this by enriching 2D visual features with 3D positional embeddings,\nwhich allows the 3D model to draw upon strong 2D priors for more accurate\nspatial reasoning across frames, even when objects of interest do not co-occur\nwithin the same view. Extensive experiments on both general 2D vision language\nand specialized 3D spatial benchmarks demonstrate that SR-3D achieves\nstate-of-the-art performance, underscoring its effectiveness for unifying 2D\nand 3D representation space on scene understanding. Moreover, we observe\napplicability to in-the-wild videos without sensory 3D inputs or ground-truth\n3D annotations, where SR-3D accurately infers spatial relationships and metric\nmeasurements.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13317.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6039478ab3ecf716b1a5fd4d",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/6039478ab3ecf716b1a5fd4d/_Thy4E7taiSYBLKxEKJbT.jpeg",
            "fullname": "taesiri",
            "name": "taesiri",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 106
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.12603",
            "authors": [
                {
                    "_id": "68ca2bd86e0073c09bd1def6",
                    "user": {
                        "_id": "624561c1939c9acec2103534",
                        "avatarUrl": "/avatars/3efdf797c53d153cea7415213fb5afc7.svg",
                        "isPro": false,
                        "fullname": "Mukai Li",
                        "user": "kiaia",
                        "type": "user"
                    },
                    "name": "Mukai Li",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:47:47.440Z",
                    "hidden": false
                },
                {
                    "_id": "68ca2bd86e0073c09bd1def7",
                    "name": "Linfeng Song",
                    "hidden": false
                },
                {
                    "_id": "68ca2bd86e0073c09bd1def8",
                    "name": "Zhenwen Liang",
                    "hidden": false
                },
                {
                    "_id": "68ca2bd86e0073c09bd1def9",
                    "name": "Jiahao Xu",
                    "hidden": false
                },
                {
                    "_id": "68ca2bd86e0073c09bd1defa",
                    "name": "Shansan Gong",
                    "hidden": false
                },
                {
                    "_id": "68ca2bd86e0073c09bd1defb",
                    "name": "Qi Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca2bd86e0073c09bd1defc",
                    "name": "Haitao Mi",
                    "hidden": false
                },
                {
                    "_id": "68ca2bd86e0073c09bd1defd",
                    "name": "Dong Yu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T03:00:13.000Z",
            "submittedOnDailyAt": "2025-09-17T02:05:58.503Z",
            "title": "EconProver: Towards More Economical Test-Time Scaling for Automated\n  Theorem Proving",
            "submittedOnDailyBy": {
                "_id": "624561c1939c9acec2103534",
                "avatarUrl": "/avatars/3efdf797c53d153cea7415213fb5afc7.svg",
                "isPro": false,
                "fullname": "Mukai Li",
                "user": "kiaia",
                "type": "user"
            },
            "summary": "Large Language Models (LLMs) have recently advanced the field of Automated\nTheorem Proving (ATP), attaining substantial performance gains through widely\nadopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)\nreasoning and increased sampling passes. However, they both introduce\nsignificant computational overhead for inference. Moreover, existing cost\nanalyses typically regulate only the number of sampling passes, while\nneglecting the substantial disparities in sampling costs introduced by\ndifferent scaling strategies. In this paper, we systematically compare the\nefficiency of different test-time scaling strategies for ATP models and\ndemonstrate the inefficiency of the current state-of-the-art (SOTA) open-source\napproaches. We then investigate approaches to significantly reduce token usage\nand sample passes while maintaining the original performance. Specifically, we\npropose two complementary methods that can be integrated into a unified EconRL\npipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching\nmechanism designed to mitigate unnecessary token consumption, and (2) Diverse\nparallel-scaled reinforcement learning (RL) with trainable prefixes to enhance\npass rates under constrained sampling passes. Experiments on miniF2F and\nProofNet demonstrate that our EconProver achieves comparable performance to\nbaseline methods with only 12% of the computational cost. This work provides\nactionable insights for deploying lightweight ATP models without sacrificing\nperformance.",
            "upvotes": 6,
            "discussionId": "68ca2bd96e0073c09bd1defe",
            "ai_summary": "Two methods, dynamic CoT switching and Diverse parallel-scaled RL, reduce computational cost in ATP models while maintaining performance.",
            "ai_keywords": [
                "Chain-of-Thought (CoT)",
                "sampling passes",
                "token usage",
                "reinforcement learning (RL)",
                "EconRL",
                "miniF2F",
                "ProofNet",
                "EconProver"
            ]
        },
        "publishedAt": "2025-09-15T23:00:13.000Z",
        "title": "EconProver: Towards More Economical Test-Time Scaling for Automated\n  Theorem Proving",
        "summary": "Large Language Models (LLMs) have recently advanced the field of Automated\nTheorem Proving (ATP), attaining substantial performance gains through widely\nadopted test-time scaling strategies, notably reflective Chain-of-Thought (CoT)\nreasoning and increased sampling passes. However, they both introduce\nsignificant computational overhead for inference. Moreover, existing cost\nanalyses typically regulate only the number of sampling passes, while\nneglecting the substantial disparities in sampling costs introduced by\ndifferent scaling strategies. In this paper, we systematically compare the\nefficiency of different test-time scaling strategies for ATP models and\ndemonstrate the inefficiency of the current state-of-the-art (SOTA) open-source\napproaches. We then investigate approaches to significantly reduce token usage\nand sample passes while maintaining the original performance. Specifically, we\npropose two complementary methods that can be integrated into a unified EconRL\npipeline for amplified benefits: (1) a dynamic Chain-of-Thought (CoT) switching\nmechanism designed to mitigate unnecessary token consumption, and (2) Diverse\nparallel-scaled reinforcement learning (RL) with trainable prefixes to enhance\npass rates under constrained sampling passes. Experiments on miniF2F and\nProofNet demonstrate that our EconProver achieves comparable performance to\nbaseline methods with only 12% of the computational cost. This work provides\nactionable insights for deploying lightweight ATP models without sacrificing\nperformance.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12603.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "624561c1939c9acec2103534",
            "avatarUrl": "/avatars/3efdf797c53d153cea7415213fb5afc7.svg",
            "fullname": "Mukai Li",
            "name": "kiaia",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.12341",
            "authors": [
                {
                    "_id": "68c9fc8a6e0073c09bd1ddc2",
                    "user": {
                        "_id": "647bf082aba7062fe5c51ca9",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                        "isPro": false,
                        "fullname": "Yifan Zhang",
                        "user": "yifAI",
                        "type": "user"
                    },
                    "name": "Yifan Zhang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:57:20.391Z",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T18:10:28.000Z",
            "submittedOnDailyAt": "2025-09-17T01:28:39.129Z",
            "title": "Exact Coset Sampling for Quantum Lattice Algorithms",
            "submittedOnDailyBy": {
                "_id": "647bf082aba7062fe5c51ca9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
                "isPro": false,
                "fullname": "Yifan Zhang",
                "user": "yifAI",
                "type": "user"
            },
            "summary": "We give a simple, fully correct, and assumption-light replacement for the\ncontested \"domain-extension\" in Step 9 of a recent windowed-QFT lattice\nalgorithm with complex-Gaussian windows~chen2024quantum. The published\nStep~9 suffers from a periodicity/support mismatch. We present a pair-shift\ndifference construction that coherently cancels all unknown offsets, produces\nan exact uniform CRT-coset state over Z_{P}, and then uses the QFT\nto enforce the intended modular linear relation. The unitary is reversible,\nuses poly(log M_2) gates, and preserves the algorithm's\nasymptotics. Project Page: https://github.com/yifanzhang-pro/quantum-lattice.",
            "upvotes": 3,
            "discussionId": "68c9fc8b6e0073c09bd1ddc3",
            "projectPage": "https://github.com/yifanzhang-pro/quantum-lattice",
            "githubRepo": "https://github.com/yifanzhang-pro/quantum-lattice",
            "ai_summary": "A replacement for the domain-extension step in a quantum lattice algorithm uses a pair-shift difference construction to correct periodicity issues and enforce modular linear relations efficiently.",
            "ai_keywords": [
                "domain-extension",
                "windowed-QFT",
                "complex-Gaussian windows",
                "periodicity/support mismatch",
                "pair-shift difference",
                "CRT-coset state",
                "QFT",
                "modular linear relation",
                "unitary",
                "gates",
                "asymptotics"
            ],
            "githubStars": 4
        },
        "publishedAt": "2025-09-15T14:10:28.000Z",
        "title": "Exact Coset Sampling for Quantum Lattice Algorithms",
        "summary": "We give a simple, fully correct, and assumption-light replacement for the\ncontested \"domain-extension\" in Step 9 of a recent windowed-QFT lattice\nalgorithm with complex-Gaussian windows~chen2024quantum. The published\nStep~9 suffers from a periodicity/support mismatch. We present a pair-shift\ndifference construction that coherently cancels all unknown offsets, produces\nan exact uniform CRT-coset state over Z_{P}, and then uses the QFT\nto enforce the intended modular linear relation. The unitary is reversible,\nuses poly(log M_2) gates, and preserves the algorithm's\nasymptotics. Project Page: https://github.com/yifanzhang-pro/quantum-lattice.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12341.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "647bf082aba7062fe5c51ca9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/647bf082aba7062fe5c51ca9/p4lY9IjHiWZETKmFq1mtU.jpeg",
            "fullname": "Yifan Zhang",
            "name": "yifAI",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 16
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.06079",
            "authors": [
                {
                    "_id": "68ca1af76e0073c09bd1dec9",
                    "name": "Hao Liang",
                    "hidden": false
                },
                {
                    "_id": "68ca1af76e0073c09bd1deca",
                    "name": "Ruitao Wu",
                    "hidden": false
                },
                {
                    "_id": "68ca1af76e0073c09bd1decb",
                    "user": {
                        "_id": "6671214c92412fd4640714eb",
                        "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                        "isPro": false,
                        "fullname": "bohan zeng",
                        "user": "zbhpku",
                        "type": "user"
                    },
                    "name": "Bohan Zeng",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:47:49.930Z",
                    "hidden": false
                },
                {
                    "_id": "68ca1af76e0073c09bd1decc",
                    "name": "Junbo Niu",
                    "hidden": false
                },
                {
                    "_id": "68ca1af76e0073c09bd1decd",
                    "name": "Wentao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68ca1af76e0073c09bd1dece",
                    "name": "Bin Dong",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-07T14:47:32.000Z",
            "submittedOnDailyAt": "2025-09-17T00:51:56.380Z",
            "title": "Multimodal Reasoning for Science: Technical Report and 1st Place\n  Solution to the ICML 2025 SeePhys Challenge",
            "submittedOnDailyBy": {
                "_id": "6671214c92412fd4640714eb",
                "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
                "isPro": false,
                "fullname": "bohan zeng",
                "user": "zbhpku",
                "type": "user"
            },
            "summary": "Multimodal reasoning remains a fundamental challenge in artificial\nintelligence. Despite substantial advances in text-based reasoning, even\nstate-of-the-art models such as GPT-o3 struggle to maintain strong performance\nin multimodal scenarios. To address this gap, we introduce a caption-assisted\nreasoning framework that effectively bridges visual and textual modalities. Our\napproach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge\n2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we\nvalidate its generalization on the MathVerse benchmark for geometric reasoning,\ndemonstrating the versatility of our method. Our code is publicly available at\nhttps://github.com/OpenDCAI/SciReasoner.",
            "upvotes": 3,
            "discussionId": "68ca1af86e0073c09bd1decf",
            "githubRepo": "https://github.com/OpenDCAI/SciReasoner",
            "ai_summary": "A caption-assisted reasoning framework bridges visual and textual modalities, achieving top performance in multimodal reasoning tasks like SeePhys and MathVerse.",
            "ai_keywords": [
                "multimodal reasoning",
                "caption-assisted reasoning",
                "visual modalities",
                "textual modalities",
                "ICML 2025 AI for Math Workshop & Challenge 2: SeePhys",
                "MathVerse benchmark",
                "geometric reasoning"
            ],
            "githubStars": 3
        },
        "publishedAt": "2025-09-07T10:47:32.000Z",
        "title": "Multimodal Reasoning for Science: Technical Report and 1st Place\n  Solution to the ICML 2025 SeePhys Challenge",
        "summary": "Multimodal reasoning remains a fundamental challenge in artificial\nintelligence. Despite substantial advances in text-based reasoning, even\nstate-of-the-art models such as GPT-o3 struggle to maintain strong performance\nin multimodal scenarios. To address this gap, we introduce a caption-assisted\nreasoning framework that effectively bridges visual and textual modalities. Our\napproach achieved 1st place in the ICML 2025 AI for Math Workshop \\& Challenge\n2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we\nvalidate its generalization on the MathVerse benchmark for geometric reasoning,\ndemonstrating the versatility of our method. Our code is publicly available at\nhttps://github.com/OpenDCAI/SciReasoner.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.06079.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "6671214c92412fd4640714eb",
            "avatarUrl": "/avatars/48fa84e7bc3bb92ad0192aa26b32de10.svg",
            "fullname": "bohan zeng",
            "name": "zbhpku",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 3
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.12521",
            "authors": [
                {
                    "_id": "68cabaa55a7803ff3be42c04",
                    "user": {
                        "_id": "677cc30a38114a126d10b6d2",
                        "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Izaxu-ihgWADuVfA1ZAH3.png",
                        "isPro": false,
                        "fullname": "Yifan Lan",
                        "user": "yflantmy",
                        "type": "user"
                    },
                    "name": "Yifan Lan",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T15:29:11.234Z",
                    "hidden": false
                },
                {
                    "_id": "68cabaa55a7803ff3be42c05",
                    "name": "Yuanpu Cao",
                    "hidden": false
                },
                {
                    "_id": "68cabaa55a7803ff3be42c06",
                    "name": "Weitong Zhang",
                    "hidden": false
                },
                {
                    "_id": "68cabaa55a7803ff3be42c07",
                    "name": "Lu Lin",
                    "hidden": false
                },
                {
                    "_id": "68cabaa55a7803ff3be42c08",
                    "name": "Jinghui Chen",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/677cc30a38114a126d10b6d2/fLA0iZ1gQBnJWofNJCy2Z.png",
                "https://cdn-uploads.huggingface.co/production/uploads/677cc30a38114a126d10b6d2/nr0YODMIkixe2uwoXluM2.png"
            ],
            "publishedAt": "2025-09-15T23:55:57.000Z",
            "submittedOnDailyAt": "2025-09-17T14:47:29.864Z",
            "title": "Phi: Preference Hijacking in Multi-modal Large Language Models at\n  Inference Time",
            "submittedOnDailyBy": {
                "_id": "677cc30a38114a126d10b6d2",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Izaxu-ihgWADuVfA1ZAH3.png",
                "isPro": false,
                "fullname": "Yifan Lan",
                "user": "yflantmy",
                "type": "user"
            },
            "summary": "Recently, Multimodal Large Language Models (MLLMs) have gained significant\nattention across various domains. However, their widespread adoption has also\nraised serious safety concerns. In this paper, we uncover a new safety risk of\nMLLMs: the output preference of MLLMs can be arbitrarily manipulated by\ncarefully optimized images. Such attacks often generate contextually relevant\nyet biased responses that are neither overtly harmful nor unethical, making\nthem difficult to detect. Specifically, we introduce a novel method, Preference\nHijacking (Phi), for manipulating the MLLM response preferences using a\npreference hijacked image. Our method works at inference time and requires no\nmodel modifications. Additionally, we introduce a universal hijacking\nperturbation -- a transferable component that can be embedded into different\nimages to hijack MLLM responses toward any attacker-specified preferences.\nExperimental results across various tasks demonstrate the effectiveness of our\napproach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.",
            "upvotes": 2,
            "discussionId": "68cabaa65a7803ff3be42c09",
            "githubRepo": "https://github.com/Yifan-Lan/Phi",
            "ai_summary": "A novel method, Preference Hijacking (Phi), manipulates Multimodal Large Language Model (MLLM) response preferences using specially crafted images, demonstrating significant effectiveness across various tasks.",
            "ai_keywords": [
                "Multimodal Large Language Models",
                "MLLMs",
                "Preference Hijacking",
                "Phi",
                "preference hijacked image",
                "universal hijacking perturbation"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-15T19:55:57.000Z",
        "title": "Phi: Preference Hijacking in Multi-modal Large Language Models at\n  Inference Time",
        "summary": "Recently, Multimodal Large Language Models (MLLMs) have gained significant\nattention across various domains. However, their widespread adoption has also\nraised serious safety concerns. In this paper, we uncover a new safety risk of\nMLLMs: the output preference of MLLMs can be arbitrarily manipulated by\ncarefully optimized images. Such attacks often generate contextually relevant\nyet biased responses that are neither overtly harmful nor unethical, making\nthem difficult to detect. Specifically, we introduce a novel method, Preference\nHijacking (Phi), for manipulating the MLLM response preferences using a\npreference hijacked image. Our method works at inference time and requires no\nmodel modifications. Additionally, we introduce a universal hijacking\nperturbation -- a transferable component that can be embedded into different\nimages to hijack MLLM responses toward any attacker-specified preferences.\nExperimental results across various tasks demonstrate the effectiveness of our\napproach. The code for Phi is accessible at https://github.com/Yifan-Lan/Phi.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/677cc30a38114a126d10b6d2/fLA0iZ1gQBnJWofNJCy2Z.png",
            "https://cdn-uploads.huggingface.co/production/uploads/677cc30a38114a126d10b6d2/nr0YODMIkixe2uwoXluM2.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12521.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "677cc30a38114a126d10b6d2",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/Izaxu-ihgWADuVfA1ZAH3.png",
            "fullname": "Yifan Lan",
            "name": "yflantmy",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.11526",
            "authors": [
                {
                    "_id": "68ca148b6e0073c09bd1de00",
                    "user": {
                        "_id": "649010619852e20da842e84c",
                        "avatarUrl": "/avatars/5016c566d804d1a00fccbba69059e2ef.svg",
                        "isPro": false,
                        "fullname": "Wenhao",
                        "user": "Dearcat",
                        "type": "user"
                    },
                    "name": "Wenhao Tang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:57:05.211Z",
                    "hidden": false
                },
                {
                    "_id": "68ca148b6e0073c09bd1de01",
                    "name": "Sheng Huang",
                    "hidden": false
                },
                {
                    "_id": "68ca148b6e0073c09bd1de02",
                    "name": "Heng Fang",
                    "hidden": false
                },
                {
                    "_id": "68ca148b6e0073c09bd1de03",
                    "name": "Fengtao Zhou",
                    "hidden": false
                },
                {
                    "_id": "68ca148b6e0073c09bd1de04",
                    "name": "Bo Liu",
                    "hidden": false
                },
                {
                    "_id": "68ca148b6e0073c09bd1de05",
                    "name": "Qingshan Liu",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-15T02:31:33.000Z",
            "submittedOnDailyAt": "2025-09-17T00:25:06.245Z",
            "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining\n  for Gigapixel Histopathology Image Analysis",
            "submittedOnDailyBy": {
                "_id": "649010619852e20da842e84c",
                "avatarUrl": "/avatars/5016c566d804d1a00fccbba69059e2ef.svg",
                "isPro": false,
                "fullname": "Wenhao",
                "user": "Dearcat",
                "type": "user"
            },
            "summary": "Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has\nopened new avenues for Computational Pathology (CPath). As positive tissue\ncomprises only a small fraction of gigapixel WSIs, existing Multiple Instance\nLearning (MIL) methods typically focus on identifying salient instances via\nattention mechanisms. However, this leads to a bias towards easy-to-classify\ninstances while neglecting challenging ones. Recent studies have shown that\nhard examples are crucial for accurately modeling discriminative boundaries.\nApplying such an idea at the instance level, we elaborate a novel MIL framework\nwith masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure\nwith a consistency constraint to explore the hard instances. Using a\nclass-aware instance probability, MHIM-MIL employs a momentum teacher to mask\nsalient instances and implicitly mine hard instances for training the student\nmodel. To obtain diverse, non-redundant hard instances, we adopt large-scale\nrandom masking while utilizing a global recycle network to mitigate the risk of\nlosing key features. Furthermore, the student updates the teacher using an\nexponential moving average, which identifies new hard instances for subsequent\ntraining iterations and stabilizes optimization. Experimental results on cancer\ndiagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate\nthat MHIM-MIL outperforms the latest methods in both performance and\nefficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.",
            "upvotes": 1,
            "discussionId": "68ca148b6e0073c09bd1de06",
            "githubRepo": "https://github.com/DearCaat/MHIM-MIL",
            "ai_summary": "A novel MIL framework, MHIM-MIL, uses masked hard instance mining with a Siamese structure and momentum teacher to improve cancer diagnosis and subtyping accuracy.",
            "ai_keywords": [
                "Multiple Instance Learning",
                "MIL",
                "masked hard instance mining",
                "MHIM-MIL",
                "Siamese structure",
                "consistency constraint",
                "class-aware instance probability",
                "momentum teacher",
                "large-scale random masking",
                "global recycle network",
                "exponential moving average"
            ],
            "githubStars": 82
        },
        "publishedAt": "2025-09-14T22:31:33.000Z",
        "title": "Multiple Instance Learning Framework with Masked Hard Instance Mining\n  for Gigapixel Histopathology Image Analysis",
        "summary": "Digitizing pathological images into gigapixel Whole Slide Images (WSIs) has\nopened new avenues for Computational Pathology (CPath). As positive tissue\ncomprises only a small fraction of gigapixel WSIs, existing Multiple Instance\nLearning (MIL) methods typically focus on identifying salient instances via\nattention mechanisms. However, this leads to a bias towards easy-to-classify\ninstances while neglecting challenging ones. Recent studies have shown that\nhard examples are crucial for accurately modeling discriminative boundaries.\nApplying such an idea at the instance level, we elaborate a novel MIL framework\nwith masked hard instance mining (MHIM-MIL), which utilizes a Siamese structure\nwith a consistency constraint to explore the hard instances. Using a\nclass-aware instance probability, MHIM-MIL employs a momentum teacher to mask\nsalient instances and implicitly mine hard instances for training the student\nmodel. To obtain diverse, non-redundant hard instances, we adopt large-scale\nrandom masking while utilizing a global recycle network to mitigate the risk of\nlosing key features. Furthermore, the student updates the teacher using an\nexponential moving average, which identifies new hard instances for subsequent\ntraining iterations and stabilizes optimization. Experimental results on cancer\ndiagnosis, subtyping, survival analysis tasks, and 12 benchmarks demonstrate\nthat MHIM-MIL outperforms the latest methods in both performance and\nefficiency. The code is available at: https://github.com/DearCaat/MHIM-MIL.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11526.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "649010619852e20da842e84c",
            "avatarUrl": "/avatars/5016c566d804d1a00fccbba69059e2ef.svg",
            "fullname": "Wenhao",
            "name": "Dearcat",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.11481",
            "authors": [
                {
                    "_id": "68caf6d95a7803ff3be42c7c",
                    "name": "Jonas Eschmann",
                    "hidden": false
                },
                {
                    "_id": "68caf6d95a7803ff3be42c7d",
                    "name": "Dario Albani",
                    "hidden": false
                },
                {
                    "_id": "68caf6d95a7803ff3be42c7e",
                    "name": "Giuseppe Loianno",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/64a011bd343399711c7057ad/7V5u8bggdmCzLUkH_OJ3A.mp4"
            ],
            "publishedAt": "2025-09-15T00:05:40.000Z",
            "submittedOnDailyAt": "2025-09-17T16:43:39.693Z",
            "title": "RAPTOR: A Foundation Policy for Quadrotor Control",
            "submittedOnDailyBy": {
                "_id": "64a011bd343399711c7057ad",
                "avatarUrl": "/avatars/b511abb202bab05f884c6a8f54e4a388.svg",
                "isPro": false,
                "fullname": "Jonas Eschmann",
                "user": "jonas-eschmann",
                "type": "user"
            },
            "summary": "Humans are remarkably data-efficient when adapting to new unseen conditions,\nlike driving a new car. In contrast, modern robotic control systems, like\nneural network policies trained using Reinforcement Learning (RL), are highly\nspecialized for single environments. Because of this overfitting, they are\nknown to break down even under small differences like the Simulation-to-Reality\n(Sim2Real) gap and require system identification and retraining for even\nminimal changes to the system. In this work, we present RAPTOR, a method for\ntraining a highly adaptive foundation policy for quadrotor control. Our method\nenables training a single, end-to-end neural-network policy to control a wide\nvariety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg\nthat also differ in motor type (brushed vs. brushless), frame type (soft vs.\nrigid), propeller type (2/3/4-blade), and flight controller\n(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy\nwith only 2084 parameters is sufficient for zero-shot adaptation to a wide\nvariety of platforms. The adaptation through In-Context Learning is made\npossible by using a recurrence in the hidden layer. The policy is trained\nthrough a novel Meta-Imitation Learning algorithm, where we sample 1000\nquadrotors and train a teacher policy for each of them using Reinforcement\nLearning. Subsequently, the 1000 teachers are distilled into a single, adaptive\nstudent policy. We find that within milliseconds, the resulting foundation\npolicy adapts zero-shot to unseen quadrotors. We extensively test the\ncapabilities of the foundation policy under numerous conditions (trajectory\ntracking, indoor/outdoor, wind disturbance, poking, different propellers).",
            "upvotes": 1,
            "discussionId": "68caf6d95a7803ff3be42c7f",
            "ai_summary": "A method called RAPTOR enables a single neural network policy to adapt zero-shot to various quadrotors using Meta-Imitation Learning and In-Context Learning with recurrence in the hidden layer.",
            "ai_keywords": [
                "Reinforcement Learning",
                "RL",
                "Simulation-to-Reality",
                "Sim2Real",
                "RAPTOR",
                "In-Context Learning",
                "Meta-Imitation Learning",
                "trajectory tracking",
                "wind disturbance"
            ]
        },
        "publishedAt": "2025-09-14T20:05:40.000Z",
        "title": "RAPTOR: A Foundation Policy for Quadrotor Control",
        "summary": "Humans are remarkably data-efficient when adapting to new unseen conditions,\nlike driving a new car. In contrast, modern robotic control systems, like\nneural network policies trained using Reinforcement Learning (RL), are highly\nspecialized for single environments. Because of this overfitting, they are\nknown to break down even under small differences like the Simulation-to-Reality\n(Sim2Real) gap and require system identification and retraining for even\nminimal changes to the system. In this work, we present RAPTOR, a method for\ntraining a highly adaptive foundation policy for quadrotor control. Our method\nenables training a single, end-to-end neural-network policy to control a wide\nvariety of quadrotors. We test 10 different real quadrotors from 32 g to 2.4 kg\nthat also differ in motor type (brushed vs. brushless), frame type (soft vs.\nrigid), propeller type (2/3/4-blade), and flight controller\n(PX4/Betaflight/Crazyflie/M5StampFly). We find that a tiny, three-layer policy\nwith only 2084 parameters is sufficient for zero-shot adaptation to a wide\nvariety of platforms. The adaptation through In-Context Learning is made\npossible by using a recurrence in the hidden layer. The policy is trained\nthrough a novel Meta-Imitation Learning algorithm, where we sample 1000\nquadrotors and train a teacher policy for each of them using Reinforcement\nLearning. Subsequently, the 1000 teachers are distilled into a single, adaptive\nstudent policy. We find that within milliseconds, the resulting foundation\npolicy adapts zero-shot to unseen quadrotors. We extensively test the\ncapabilities of the foundation policy under numerous conditions (trajectory\ntracking, indoor/outdoor, wind disturbance, poking, different propellers).",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/64a011bd343399711c7057ad/7V5u8bggdmCzLUkH_OJ3A.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11481.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "64a011bd343399711c7057ad",
            "avatarUrl": "/avatars/b511abb202bab05f884c6a8f54e4a388.svg",
            "fullname": "Jonas Eschmann",
            "name": "jonas-eschmann",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.11177",
            "authors": [
                {
                    "_id": "68ca5a906e0073c09bd1df7d",
                    "user": {
                        "_id": "67d6bb22eab66ce9cb4e3662",
                        "avatarUrl": "/avatars/814704eef9e6907d9d4ab407b605566b.svg",
                        "isPro": false,
                        "fullname": "Hang Guo",
                        "user": "HangGuo",
                        "type": "user"
                    },
                    "name": "Hang Guo",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:47:25.533Z",
                    "hidden": false
                },
                {
                    "_id": "68ca5a906e0073c09bd1df7e",
                    "name": "Yawei Li",
                    "hidden": false
                },
                {
                    "_id": "68ca5a906e0073c09bd1df7f",
                    "name": "Luca Benini",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-14T09:17:19.000Z",
            "submittedOnDailyAt": "2025-09-17T05:22:49.836Z",
            "title": "Optimal Brain Restoration for Joint Quantization and Sparsification of\n  LLMs",
            "submittedOnDailyBy": {
                "_id": "67d6bb22eab66ce9cb4e3662",
                "avatarUrl": "/avatars/814704eef9e6907d9d4ab407b605566b.svg",
                "isPro": false,
                "fullname": "Hang Guo",
                "user": "HangGuo",
                "type": "user"
            },
            "summary": "Recent advances in Large Language Model (LLM) compression, such as\nquantization and pruning, have achieved notable success. However, as these\ntechniques gradually approach their respective limits, relying on a single\nmethod for further compression has become increasingly challenging. In this\nwork, we explore an alternative solution by combining quantization and\nsparsity. This joint approach, though promising, introduces new difficulties\ndue to the inherently conflicting requirements on weight distributions:\nquantization favors compact ranges, while pruning benefits from high variance.\nTo attack this problem, we propose Optimal Brain Restoration (OBR), a general\nand training-free framework that aligns pruning and quantization by error\ncompensation between both. OBR minimizes performance degradation on downstream\ntasks by building on a second-order Hessian objective, which is then\nreformulated into a tractable problem through surrogate approximation and\nultimately reaches a closed-form solution via group error compensation.\nExperiments show that OBR enables aggressive W4A4KV4 quantization with 50%\nsparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory\nreduction compared to the FP16-dense baseline.",
            "upvotes": 1,
            "discussionId": "68ca5a956e0073c09bd1df80",
            "githubRepo": "https://github.com/csguoh/OBR",
            "ai_summary": "A framework combining quantization and pruning in LLMs through error compensation achieves significant speedup and memory reduction.",
            "ai_keywords": [
                "quantization",
                "pruning",
                "Optimal Brain Restoration (OBR)",
                "second-order Hessian objective",
                "surrogate approximation",
                "group error compensation",
                "W4A4KV4 quantization",
                "speedup",
                "memory reduction",
                "FP16-dense baseline"
            ],
            "githubStars": 1
        },
        "publishedAt": "2025-09-14T05:17:19.000Z",
        "title": "Optimal Brain Restoration for Joint Quantization and Sparsification of\n  LLMs",
        "summary": "Recent advances in Large Language Model (LLM) compression, such as\nquantization and pruning, have achieved notable success. However, as these\ntechniques gradually approach their respective limits, relying on a single\nmethod for further compression has become increasingly challenging. In this\nwork, we explore an alternative solution by combining quantization and\nsparsity. This joint approach, though promising, introduces new difficulties\ndue to the inherently conflicting requirements on weight distributions:\nquantization favors compact ranges, while pruning benefits from high variance.\nTo attack this problem, we propose Optimal Brain Restoration (OBR), a general\nand training-free framework that aligns pruning and quantization by error\ncompensation between both. OBR minimizes performance degradation on downstream\ntasks by building on a second-order Hessian objective, which is then\nreformulated into a tractable problem through surrogate approximation and\nultimately reaches a closed-form solution via group error compensation.\nExperiments show that OBR enables aggressive W4A4KV4 quantization with 50%\nsparsity on existing LLMs, and delivers up to 4.72x speedup and 6.4x memory\nreduction compared to the FP16-dense baseline.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.11177.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67d6bb22eab66ce9cb4e3662",
            "avatarUrl": "/avatars/814704eef9e6907d9d4ab407b605566b.svg",
            "fullname": "Hang Guo",
            "name": "HangGuo",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.10687",
            "authors": [
                {
                    "_id": "68caf3075a7803ff3be42c6f",
                    "name": "Hao Zhang",
                    "hidden": false
                },
                {
                    "_id": "68caf3075a7803ff3be42c70",
                    "name": "Chun-Han Yao",
                    "hidden": false
                },
                {
                    "_id": "68caf3075a7803ff3be42c71",
                    "name": "Simon Donn",
                    "hidden": false
                },
                {
                    "_id": "68caf3075a7803ff3be42c72",
                    "name": "Narendra Ahuja",
                    "hidden": false
                },
                {
                    "_id": "68caf3075a7803ff3be42c73",
                    "name": "Varun Jampani",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/65712f530ea91e592a1e8126/r_n5iJmbWFvSyAcIYqHY2.mp4"
            ],
            "publishedAt": "2025-09-12T20:39:43.000Z",
            "submittedOnDailyAt": "2025-09-17T16:27:32.207Z",
            "title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video\n  Generation",
            "submittedOnDailyBy": {
                "_id": "65712f530ea91e592a1e8126",
                "avatarUrl": "/avatars/2ed00ca40b7ff2b88e4a75ef04807447.svg",
                "isPro": true,
                "fullname": "Hao Zhang",
                "user": "haoz19",
                "type": "user"
            },
            "summary": "We present Stable Part Diffusion 4D (SP4D), a framework for generating paired\nRGB and kinematic part videos from monocular inputs. Unlike conventional part\nsegmentation methods that rely on appearance-based semantic cues, SP4D learns\nto produce kinematic parts - structural components aligned with object\narticulation and consistent across views and time. SP4D adopts a dual-branch\ndiffusion model that jointly synthesizes RGB frames and corresponding part\nsegmentation maps. To simplify the architecture and flexibly enable different\npart counts, we introduce a spatial color encoding scheme that maps part masks\nto continuous RGB-like images. This encoding allows the segmentation branch to\nshare the latent VAE from the RGB branch, while enabling part segmentation to\nbe recovered via straightforward post-processing. A Bidirectional Diffusion\nFusion (BiDiFuse) module enhances cross-branch consistency, supported by a\ncontrastive part consistency loss to promote spatial and temporal alignment of\npart predictions. We demonstrate that the generated 2D part maps can be lifted\nto 3D to derive skeletal structures and harmonic skinning weights with few\nmanual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,\na curated dataset of over 20K rigged objects selected and processed from\nObjaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part\nvideo sequences. Experiments show that SP4D generalizes strongly to diverse\nscenarios, including real-world videos, novel generated objects, and rare\narticulated poses, producing kinematic-aware outputs suitable for downstream\nanimation and motion-related tasks.",
            "upvotes": 1,
            "discussionId": "68caf3075a7803ff3be42c74",
            "projectPage": "https://stablepartdiffusion4d.github.io/",
            "ai_summary": "SP4D generates paired RGB and kinematic part videos from monocular inputs using a dual-branch diffusion model with spatial color encoding and BiDiFuse module, demonstrating strong generalization to diverse scenarios.",
            "ai_keywords": [
                "dual-branch diffusion model",
                "spatial color encoding",
                "latent VAE",
                "Bidirectional Diffusion Fusion (BiDiFuse)",
                "contrastive part consistency loss",
                "skeletal structures",
                "harmonic skinning weights",
                "KinematicParts20K",
                "Objaverse XL"
            ]
        },
        "publishedAt": "2025-09-12T16:39:43.000Z",
        "title": "Stable Part Diffusion 4D: Multi-View RGB and Kinematic Parts Video\n  Generation",
        "summary": "We present Stable Part Diffusion 4D (SP4D), a framework for generating paired\nRGB and kinematic part videos from monocular inputs. Unlike conventional part\nsegmentation methods that rely on appearance-based semantic cues, SP4D learns\nto produce kinematic parts - structural components aligned with object\narticulation and consistent across views and time. SP4D adopts a dual-branch\ndiffusion model that jointly synthesizes RGB frames and corresponding part\nsegmentation maps. To simplify the architecture and flexibly enable different\npart counts, we introduce a spatial color encoding scheme that maps part masks\nto continuous RGB-like images. This encoding allows the segmentation branch to\nshare the latent VAE from the RGB branch, while enabling part segmentation to\nbe recovered via straightforward post-processing. A Bidirectional Diffusion\nFusion (BiDiFuse) module enhances cross-branch consistency, supported by a\ncontrastive part consistency loss to promote spatial and temporal alignment of\npart predictions. We demonstrate that the generated 2D part maps can be lifted\nto 3D to derive skeletal structures and harmonic skinning weights with few\nmanual adjustments. To train and evaluate SP4D, we construct KinematicParts20K,\na curated dataset of over 20K rigged objects selected and processed from\nObjaverse XL (Deitke et al., 2023), each paired with multi-view RGB and part\nvideo sequences. Experiments show that SP4D generalizes strongly to diverse\nscenarios, including real-world videos, novel generated objects, and rare\narticulated poses, producing kinematic-aware outputs suitable for downstream\nanimation and motion-related tasks.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/65712f530ea91e592a1e8126/r_n5iJmbWFvSyAcIYqHY2.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10687.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "65712f530ea91e592a1e8126",
            "avatarUrl": "/avatars/2ed00ca40b7ff2b88e4a75ef04807447.svg",
            "fullname": "Hao Zhang",
            "name": "haoz19",
            "type": "user",
            "isPro": true,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.13177",
            "authors": [
                {
                    "_id": "68caaee95a7803ff3be42bae",
                    "user": {
                        "_id": "67a659fad1284b551f110ce2",
                        "avatarUrl": "/avatars/763ad24b3d6a6285e01fd3f07c4519b3.svg",
                        "isPro": false,
                        "fullname": "Salvatore Esposito",
                        "user": "iamsalvatore",
                        "type": "user"
                    },
                    "name": "Salvatore Esposito",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T12:55:36.197Z",
                    "hidden": false
                },
                {
                    "_id": "68caaee95a7803ff3be42baf",
                    "name": "Matas Mattamala",
                    "hidden": false
                },
                {
                    "_id": "68caaee95a7803ff3be42bb0",
                    "name": "Daniel Rebain",
                    "hidden": false
                },
                {
                    "_id": "68caaee95a7803ff3be42bb1",
                    "name": "Francis Xiatian Zhang",
                    "hidden": false
                },
                {
                    "_id": "68caaee95a7803ff3be42bb2",
                    "name": "Kevin Dhaliwal",
                    "hidden": false
                },
                {
                    "_id": "68caaee95a7803ff3be42bb3",
                    "name": "Mohsen Khadem",
                    "hidden": false
                },
                {
                    "_id": "68caaee95a7803ff3be42bb4",
                    "name": "Subramanian Ramamoorthy",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/67a659fad1284b551f110ce2/MSMdhZ7x4zS7RJpkPf4OU.mp4"
            ],
            "publishedAt": "2025-09-16T15:30:02.000Z",
            "submittedOnDailyAt": "2025-09-17T14:09:11.625Z",
            "title": "ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic\n  Medical Datasets Generation",
            "submittedOnDailyBy": {
                "_id": "67a659fad1284b551f110ce2",
                "avatarUrl": "/avatars/763ad24b3d6a6285e01fd3f07c4519b3.svg",
                "isPro": false,
                "fullname": "Salvatore Esposito",
                "user": "iamsalvatore",
                "type": "user"
            },
            "summary": "Continuum robots are advancing bronchoscopy procedures by accessing complex\nlung airways and enabling targeted interventions. However, their development is\nlimited by the lack of realistic training and test environments: Real data is\ndifficult to collect due to ethical constraints and patient safety concerns,\nand developing autonomy algorithms requires realistic imaging and physical\nfeedback. We present ROOM (Realistic Optical Observation in Medicine), a\ncomprehensive simulation framework designed for generating photorealistic\nbronchoscopy training data. By leveraging patient CT scans, our pipeline\nrenders multi-modal sensor data including RGB images with realistic noise and\nlight specularities, metric depth maps, surface normals, optical flow and point\nclouds at medically relevant scales. We validate the data generated by ROOM in\ntwo canonical tasks for medical robotics -- multi-view pose estimation and\nmonocular depth estimation, demonstrating diverse challenges that\nstate-of-the-art methods must overcome to transfer to these medical settings.\nFurthermore, we show that the data produced by ROOM can be used to fine-tune\nexisting depth estimation models to overcome these challenges, also enabling\nother downstream applications such as navigation. We expect that ROOM will\nenable large-scale data generation across diverse patient anatomies and\nprocedural scenarios that are challenging to capture in clinical settings. Code\nand data: https://github.com/iamsalvatore/room.",
            "upvotes": 0,
            "discussionId": "68caaee95a7803ff3be42bb5",
            "projectPage": "https://iamsalvatore.io/room/",
            "githubRepo": "https://github.com/iamsalvatore/room",
            "ai_summary": "ROOM is a simulation framework that generates photorealistic bronchoscopy training data using patient CT scans, enabling the development and validation of autonomy algorithms in medical robotics.",
            "ai_keywords": [
                "photorealistic bronchoscopy training data",
                "multi-modal sensor data",
                "RGB images",
                "realistic noise",
                "light specularities",
                "metric depth maps",
                "surface normals",
                "optical flow",
                "point clouds",
                "multi-view pose estimation",
                "monocular depth estimation",
                "depth estimation models",
                "navigation"
            ],
            "githubStars": 0
        },
        "publishedAt": "2025-09-16T11:30:02.000Z",
        "title": "ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic\n  Medical Datasets Generation",
        "summary": "Continuum robots are advancing bronchoscopy procedures by accessing complex\nlung airways and enabling targeted interventions. However, their development is\nlimited by the lack of realistic training and test environments: Real data is\ndifficult to collect due to ethical constraints and patient safety concerns,\nand developing autonomy algorithms requires realistic imaging and physical\nfeedback. We present ROOM (Realistic Optical Observation in Medicine), a\ncomprehensive simulation framework designed for generating photorealistic\nbronchoscopy training data. By leveraging patient CT scans, our pipeline\nrenders multi-modal sensor data including RGB images with realistic noise and\nlight specularities, metric depth maps, surface normals, optical flow and point\nclouds at medically relevant scales. We validate the data generated by ROOM in\ntwo canonical tasks for medical robotics -- multi-view pose estimation and\nmonocular depth estimation, demonstrating diverse challenges that\nstate-of-the-art methods must overcome to transfer to these medical settings.\nFurthermore, we show that the data produced by ROOM can be used to fine-tune\nexisting depth estimation models to overcome these challenges, also enabling\nother downstream applications such as navigation. We expect that ROOM will\nenable large-scale data generation across diverse patient anatomies and\nprocedural scenarios that are challenging to capture in clinical settings. Code\nand data: https://github.com/iamsalvatore/room.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/67a659fad1284b551f110ce2/MSMdhZ7x4zS7RJpkPf4OU.mp4"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.13177.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "67a659fad1284b551f110ce2",
            "avatarUrl": "/avatars/763ad24b3d6a6285e01fd3f07c4519b3.svg",
            "fullname": "Salvatore Esposito",
            "name": "iamsalvatore",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    },
    {
        "paper": {
            "id": "2509.12541",
            "authors": [
                {
                    "_id": "68cb07e05a7803ff3be42c9e",
                    "name": "Nicholas Pipitone",
                    "hidden": false
                },
                {
                    "_id": "68cb07e05a7803ff3be42c9f",
                    "name": "Ghita Houir Alami",
                    "hidden": false
                },
                {
                    "_id": "68cb07e05a7803ff3be42ca0",
                    "name": "Advaith Avadhanam",
                    "hidden": false
                },
                {
                    "_id": "68cb07e05a7803ff3be42ca1",
                    "name": "Anton Kaminskyi",
                    "hidden": false
                },
                {
                    "_id": "68cb07e05a7803ff3be42ca2",
                    "name": "Ashley Khoo",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-16T00:44:08.000Z",
            "submittedOnDailyAt": "2025-09-17T17:45:52.810Z",
            "title": "zELO: ELO-inspired Training Method for Rerankers and Embedding Models",
            "submittedOnDailyBy": {
                "_id": "5df7e9e5da6d0311fd3d53f9",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg",
                "isPro": true,
                "fullname": "Thomas Wolf",
                "user": "thomwolf",
                "type": "user"
            },
            "summary": "We introduce a novel training methodology named zELO, which optimizes\nretrieval performance via the analysis that ranking tasks are statically\nequivalent to a Thurstone model. Based on the zELO method, we use unsupervised\ndata in order train a suite of state-of-the-art open-weight reranker models:\nzerank-1 and zerank-1-small. These models achieve the highest retrieval scores\nin multiple domains, including finance, legal, code, and STEM, outperforming\nclosed-source proprietary rerankers on both NDCG@10 and Recall. These models\nalso demonstrate great versatility, maintaining their 0-shot performance on\nout-of-domain and private customer datasets. The training data included 112,000\nqueries and 100 documents per query, and was trained end-to-end from\nunannotated queries and documents in less than 10,000 H100-hours.",
            "upvotes": 0,
            "discussionId": "68cb07e15a7803ff3be42ca3",
            "ai_summary": "A novel training methodology named zELO optimizes retrieval performance by treating ranking tasks as equivalent to a Thurstone model, resulting in state-of-the-art open-weight reranker models that outperform proprietary models across various domains.",
            "ai_keywords": [
                "zELO",
                "Thurstone model",
                "reranker models",
                "NDCG@10",
                "Recall",
                "0-shot performance",
                "unannotated queries",
                "documents",
                "H100-hours"
            ]
        },
        "publishedAt": "2025-09-15T20:44:08.000Z",
        "title": "zELO: ELO-inspired Training Method for Rerankers and Embedding Models",
        "summary": "We introduce a novel training methodology named zELO, which optimizes\nretrieval performance via the analysis that ranking tasks are statically\nequivalent to a Thurstone model. Based on the zELO method, we use unsupervised\ndata in order train a suite of state-of-the-art open-weight reranker models:\nzerank-1 and zerank-1-small. These models achieve the highest retrieval scores\nin multiple domains, including finance, legal, code, and STEM, outperforming\nclosed-source proprietary rerankers on both NDCG@10 and Recall. These models\nalso demonstrate great versatility, maintaining their 0-shot performance on\nout-of-domain and private customer datasets. The training data included 112,000\nqueries and 100 documents per query, and was trained end-to-end from\nunannotated queries and documents in less than 10,000 H100-hours.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.12541.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "5df7e9e5da6d0311fd3d53f9",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/1583857746553-5df7e9e5da6d0311fd3d53f9.jpeg",
            "fullname": "Thomas Wolf",
            "name": "thomwolf",
            "type": "user",
            "isPro": true,
            "isHf": true,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 1461
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.10706",
            "authors": [
                {
                    "_id": "68cb2c245a7803ff3be42cc3",
                    "name": "Chin-Yun Yu",
                    "hidden": false
                },
                {
                    "_id": "68cb2c245a7803ff3be42cc4",
                    "name": "Gyrgy Fazekas",
                    "hidden": false
                }
            ],
            "mediaUrls": [
                "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/ryJ_9LplVKuAsRGXgol25.png"
            ],
            "publishedAt": "2025-09-12T21:48:30.000Z",
            "submittedOnDailyAt": "2025-09-17T20:18:33.114Z",
            "title": "Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson\n  Method",
            "submittedOnDailyBy": {
                "_id": "621d85a10e35b2fbbf3e6196",
                "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
                "isPro": false,
                "fullname": "Chin-Yun Yu",
                "user": "yoyolicoris",
                "type": "user"
            },
            "summary": "Automatic differentiation through digital signal processing algorithms for\nvirtual analogue modelling has recently gained popularity. These algorithms are\ntypically more computationally efficient than black-box neural networks that\nrely on dense matrix multiplications. Due to their differentiable nature, they\ncan be integrated with neural networks and jointly trained using gradient\ndescent algorithms, resulting in more efficient systems. Furthermore, signal\nprocessing algorithms have significantly fewer parameters than neural networks,\nallowing the application of the Newton-Raphson method. This method offers\nfaster and more robust convergence than gradient descent at the cost of\nquadratic storage. This paper presents a method to emulate analogue levelling\namplifiers using a feed-forward digital compressor with parameters optimised\nvia the Newton-Raphson method. We demonstrate that a digital compressor can\nsuccessfully approximate the behaviour of our target unit, the Teletronix\nLA-2A. Different strategies for computing the Hessian matrix are benchmarked.\nWe leverage parallel algorithms for recursive filters to achieve efficient\ntraining on modern GPUs. The resulting model is made into a VST plugin and is\nopen-sourced at https://github.com/aim-qmul/4a2a.",
            "upvotes": 0,
            "discussionId": "68cb2c245a7803ff3be42cc5",
            "githubRepo": "https://github.com/aim-qmul/4a2a",
            "ai_summary": "A digital compressor is optimized using the Newton-Raphson method to emulate the behavior of an analogue levelling amplifier, demonstrating efficient training and approximation using parallel recursive filters.",
            "ai_keywords": [
                "automatic differentiation",
                "digital signal processing",
                "virtual analogue modelling",
                "feed-forward digital compressor",
                "Newton-Raphson method",
                "Hessian matrix",
                "recursive filters",
                "VST plugin"
            ],
            "githubStars": 11
        },
        "publishedAt": "2025-09-12T17:48:30.000Z",
        "title": "Sound Matching an Analogue Levelling Amplifier Using the Newton-Raphson\n  Method",
        "summary": "Automatic differentiation through digital signal processing algorithms for\nvirtual analogue modelling has recently gained popularity. These algorithms are\ntypically more computationally efficient than black-box neural networks that\nrely on dense matrix multiplications. Due to their differentiable nature, they\ncan be integrated with neural networks and jointly trained using gradient\ndescent algorithms, resulting in more efficient systems. Furthermore, signal\nprocessing algorithms have significantly fewer parameters than neural networks,\nallowing the application of the Newton-Raphson method. This method offers\nfaster and more robust convergence than gradient descent at the cost of\nquadratic storage. This paper presents a method to emulate analogue levelling\namplifiers using a feed-forward digital compressor with parameters optimised\nvia the Newton-Raphson method. We demonstrate that a digital compressor can\nsuccessfully approximate the behaviour of our target unit, the Teletronix\nLA-2A. Different strategies for computing the Hessian matrix are benchmarked.\nWe leverage parallel algorithms for recursive filters to achieve efficient\ntraining on modern GPUs. The resulting model is made into a VST plugin and is\nopen-sourced at https://github.com/aim-qmul/4a2a.",
        "mediaUrls": [
            "https://cdn-uploads.huggingface.co/production/uploads/621d85a10e35b2fbbf3e6196/ryJ_9LplVKuAsRGXgol25.png"
        ],
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10706.png",
        "numComments": 2,
        "submittedBy": {
            "_id": "621d85a10e35b2fbbf3e6196",
            "avatarUrl": "https://cdn-avatars.huggingface.co/v1/production/uploads/621d85a10e35b2fbbf3e6196/D6DMntYh-eV5fVef_Mx4Z.png",
            "fullname": "Chin-Yun Yu",
            "name": "yoyolicoris",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false,
            "followerCount": 8
        },
        "isAuthorParticipating": false
    },
    {
        "paper": {
            "id": "2509.10696",
            "authors": [
                {
                    "_id": "68c984f5fbbc67c7b9472b30",
                    "user": {
                        "_id": "66ef2897e4b4a1c1cf34af7e",
                        "avatarUrl": "/avatars/ddd78c7ce336b20be878835ffc8536b9.svg",
                        "isPro": false,
                        "fullname": "Shuaiqi Wang",
                        "user": "Shuaiqiw",
                        "type": "user"
                    },
                    "name": "Shuaiqi Wang",
                    "status": "claimed_verified",
                    "statusLastChangedAt": "2025-09-17T13:05:31.782Z",
                    "hidden": false
                },
                {
                    "_id": "68c984f5fbbc67c7b9472b31",
                    "name": "Vikas Raunak",
                    "hidden": false
                },
                {
                    "_id": "68c984f5fbbc67c7b9472b32",
                    "name": "Arturs Backurs",
                    "hidden": false
                },
                {
                    "_id": "68c984f5fbbc67c7b9472b33",
                    "name": "Victor Reis",
                    "hidden": false
                },
                {
                    "_id": "68c984f5fbbc67c7b9472b34",
                    "name": "Pei Zhou",
                    "hidden": false
                },
                {
                    "_id": "68c984f5fbbc67c7b9472b35",
                    "name": "Sihao Chen",
                    "hidden": false
                },
                {
                    "_id": "68c984f5fbbc67c7b9472b36",
                    "name": "Longqi Yang",
                    "hidden": false
                },
                {
                    "_id": "68c984f5fbbc67c7b9472b37",
                    "name": "Zinan Lin",
                    "hidden": false
                },
                {
                    "_id": "68c984f5fbbc67c7b9472b38",
                    "name": "Sergey Yekhanin",
                    "hidden": false
                },
                {
                    "_id": "68c984f5fbbc67c7b9472b39",
                    "name": "Giulia Fanti",
                    "hidden": false
                }
            ],
            "publishedAt": "2025-09-12T21:18:13.000Z",
            "submittedOnDailyAt": "2025-09-17T18:22:24.184Z",
            "title": "Struct-Bench: A Benchmark for Differentially Private Structured Text\n  Generation",
            "submittedOnDailyBy": {
                "_id": "66ef2897e4b4a1c1cf34af7e",
                "avatarUrl": "/avatars/ddd78c7ce336b20be878835ffc8536b9.svg",
                "isPro": false,
                "fullname": "Shuaiqi Wang",
                "user": "Shuaiqiw",
                "type": "user"
            },
            "summary": "Differentially private (DP) synthetic data generation is a promising\ntechnique for utilizing private datasets that otherwise cannot be exposed for\nmodel training or other analytics. While much research literature has focused\non generating private unstructured text and image data, in enterprise settings,\nstructured data (e.g., tabular) is more common, often including natural\nlanguage fields or components. Existing synthetic data evaluation techniques\n(e.g., FID) struggle to capture the structural properties and correlations of\nsuch datasets. In this work, we propose Struct-Bench, a framework and benchmark\nfor evaluating synthetic datasets derived from structured datasets that contain\nnatural language data. The Struct-Bench framework requires users to provide a\nrepresentation of their dataset structure as a Context-Free Grammar (CFG). Our\nbenchmark comprises 5 real-world and 2 synthetically generated datasets, each\nannotated with CFGs. We show that these datasets demonstrably present a great\nchallenge even for state-of-the-art DP synthetic data generation methods.\nStruct-Bench also includes reference implementations of different metrics and a\nleaderboard, thereby providing researchers a standardized evaluation platform\nto benchmark and investigate privacy-preserving synthetic data generation\nmethods. Further, we also present a case study showing how to use Struct-Bench\nto improve the synthetic data quality of Private Evolution (PE) on structured\ndata. The benchmark and the leaderboard have been publicly made available at\nhttps://struct-bench.github.io.",
            "upvotes": 0,
            "discussionId": "68c984f6fbbc67c7b9472b3a",
            "ai_summary": "Struct-Bench is a framework and benchmark for evaluating synthetic structured datasets with natural language components, addressing challenges in differentially private synthetic data generation.",
            "ai_keywords": [
                "differentially private",
                "synthetic data generation",
                "structured data",
                "tabular data",
                "natural language data",
                "Context-Free Grammar",
                "CFG",
                "Struct-Bench",
                "benchmark",
                "privacy-preserving",
                "Private Evolution",
                "PE"
            ]
        },
        "publishedAt": "2025-09-12T17:18:13.000Z",
        "title": "Struct-Bench: A Benchmark for Differentially Private Structured Text\n  Generation",
        "summary": "Differentially private (DP) synthetic data generation is a promising\ntechnique for utilizing private datasets that otherwise cannot be exposed for\nmodel training or other analytics. While much research literature has focused\non generating private unstructured text and image data, in enterprise settings,\nstructured data (e.g., tabular) is more common, often including natural\nlanguage fields or components. Existing synthetic data evaluation techniques\n(e.g., FID) struggle to capture the structural properties and correlations of\nsuch datasets. In this work, we propose Struct-Bench, a framework and benchmark\nfor evaluating synthetic datasets derived from structured datasets that contain\nnatural language data. The Struct-Bench framework requires users to provide a\nrepresentation of their dataset structure as a Context-Free Grammar (CFG). Our\nbenchmark comprises 5 real-world and 2 synthetically generated datasets, each\nannotated with CFGs. We show that these datasets demonstrably present a great\nchallenge even for state-of-the-art DP synthetic data generation methods.\nStruct-Bench also includes reference implementations of different metrics and a\nleaderboard, thereby providing researchers a standardized evaluation platform\nto benchmark and investigate privacy-preserving synthetic data generation\nmethods. Further, we also present a case study showing how to use Struct-Bench\nto improve the synthetic data quality of Private Evolution (PE) on structured\ndata. The benchmark and the leaderboard have been publicly made available at\nhttps://struct-bench.github.io.",
        "thumbnail": "https://cdn-thumbnails.huggingface.co/social-thumbnails/papers/2509.10696.png",
        "numComments": 3,
        "submittedBy": {
            "_id": "66ef2897e4b4a1c1cf34af7e",
            "avatarUrl": "/avatars/ddd78c7ce336b20be878835ffc8536b9.svg",
            "fullname": "Shuaiqi Wang",
            "name": "Shuaiqiw",
            "type": "user",
            "isPro": false,
            "isHf": false,
            "isHfAdmin": false,
            "isMod": false
        },
        "isAuthorParticipating": true
    }
]